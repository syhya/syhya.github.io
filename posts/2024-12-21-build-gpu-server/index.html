<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building a Home Deep Learning Rig with Dual RTX 4090 GPUs | Yue Shui Blog</title>
<meta name=keywords content="Deep Learning,AI,LLM,RTX 4090,AI Hardware,PC Assembly"><meta name=description content="Rent a GPU or Buy Your Own?
Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2024-12-21-build-gpu-server/><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://syhya.github.io/apple-touch-icon.png><link rel=mask-icon href=https://syhya.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2024-12-21-build-gpu-server/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://syhya.github.io/posts/2024-12-21-build-gpu-server/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Building a Home Deep Learning Rig with Dual RTX 4090 GPUs"><meta property="og:description" content="Rent a GPU or Buy Your Own? Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-21T12:00:00+08:00"><meta property="article:modified_time" content="2024-12-21T12:00:00+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="article:tag" content="RTX 4090"><meta property="article:tag" content="AI Hardware"><meta property="article:tag" content="PC Assembly"><meta name=twitter:card content="summary"><meta name=twitter:title content="Building a Home Deep Learning Rig with Dual RTX 4090 GPUs"><meta name=twitter:description content="Rent a GPU or Buy Your Own?
Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs","item":"https://syhya.github.io/posts/2024-12-21-build-gpu-server/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs","name":"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs","description":"Rent a GPU or Buy Your Own? Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\n","keywords":["Deep Learning","AI","LLM","RTX 4090","AI Hardware","PC Assembly"],"articleBody":"Rent a GPU or Buy Your Own? Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\nAdvantages of Renting GPUs:\nNo high upfront hardware costs Elastic scalability according to project needs Cloud vendors typically provide data compliance and security assurances, reducing hardware maintenance concerns Advantages of Buying GPUs:\nLower total cost if used extensively over the long term Higher privacy and control for in-house data and models Hardware can be upgraded or adjusted at any time, offering more flexible deployment Personal Suggestions\nIf you have a limited budget or are just starting out, use Colab, Kaggle, or cloud-based GPU solutions first. Once computing needs and privacy requirements increase, consider building your own multi-GPU server or renting a multi-node, multi-GPU cluster. Background In September 2023, to continue my exploration and research on Large Language Models (LLMs) outside of work, I assembled a dual RTX 4090 personal AI server. It has been running for nearly a year, and here are some observations:\nNoise: Placed under my desk, the fans can get quite loud under full load. However, noise levels are acceptable during inference or moderate loads. Inference Performance: With a total of 48GB of VRAM, and by using 4-bit quantization, it can handle 70B-level models (e.g., Llama 70B, Qwen 72B). Training Performance: By using DeepSpeed with distributed and offload techniques (ZeRO-3 + CPU offload), I can finetune around 34B parameter models (e.g., CodeLlama 34B). Cost-Effectiveness: For personal or small-team experiments and small-to-medium scale model training, this configuration is quite practical. However, full-scale training of very large models (hundreds of billions of parameters) still requires more professional GPUs (e.g., multi-GPU A100 or H100 clusters). Below is an illustration of VRAM requirements for various model sizes and training approaches (referenced from LLaMA-Factory):\nAssembly Strategy \u0026 Configuration Details The total budget is around 40,000 RMB (~6,000 USD). The final build list is as follows (for reference only):\nComponent Model Price (RMB) GPU RTX 4090 * 2 25098 Motherboard + CPU AMD R9 7900X + MSI MPG X670E CARBON 5157.55 Memory Corsair 48GB * 2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + Samsung 990PRO 4TB 4587 PSU Corsair AX1600i 2699 Fans Phanteks T30 120mm P * 6 1066.76 CPU Cooler Thermalright FC140 BLACK 419 Chassis Phanteks 620PC Full Tower 897.99 GPU Riser Cable Phanteks FL60 PCI-E4.0 *16 399 Total: ~ 42,723.3 RMB\nGPU Selection For large-scale model research, floating-point performance (TFLOPS) and VRAM capacity are the most critical metrics. Professional GPUs (A100, H100, etc.) boast higher VRAM and NVLink support but can easily cost hundreds of thousands of RMB, which is prohibitive for most individual users. According to Tim Dettmers, the RTX 4090 offers excellent performance-per-dollar and supports features like BF16 and Flash Attention, making it a high-value option.\nCooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling Cooling Method Advantages Disadvantages Best For Blower Fan Compact form factor; good for multi-GPU setups Loud noise, generally weaker cooling Server racks, dense multi-GPU deployments Air-Cooling Good balance of performance and noise; easy upkeep Cards are often large, require space Home or personal research (with enough space) Liquid-Cooling Excellent cooling, quieter under full load Risk of leaks, higher cost Extreme quiet needs or heavy overclocking Home Setup Recommendation: Air-cooled GPUs are typically the best balance of cooling efficiency, noise, and maintenance. They are generally more user-friendly compared to blower or liquid-cooled alternatives.\nCPU \u0026 Motherboard In deep learning scenarios, the CPU handles data preprocessing, pipeline scheduling, and multi-process/thread management, ensuring high-throughput, low-latency data delivery to the GPUs. Thus, the CPU’s main requirements include sufficient PCIe lanes and robust multi-threaded performance.\nIntel: 13th/14th Generation i9 (e.g., 13900K) offers 20 primary PCIe lanes, enough for dual GPUs at x8 + x8. AMD: The Ryzen 7000/9000 series (e.g., 7950X) provides 28 (24 usable) PCIe lanes and also supports dual GPUs at x8 + x8, with enough bandwidth for multiple M.2 SSDs. MSI MPG X670E CARBON Motherboard Expandability: Supports PCIe 5.0 and DDR5 memory, offering ample future-proofing. Stability: High-quality power delivery, ensuring stable operation for both CPU and multiple GPUs. Rich I/O: Supports multiple M.2 SSDs and USB4 for various usage scenarios. AMD Ryzen 9 7900X Highlights Cores \u0026 Threads: 12 cores, 24 threads, excellent for data preprocessing and multitasking in deep learning workloads. PCIe Bandwidth: Provides 28 (24 usable) PCIe 5.0 lanes to support dual GPUs at x8 + x8 while also powering high-speed M.2 SSDs. Power Efficiency: Built on the Zen 4 architecture, delivering outstanding performance-to-power ratio for high-performance computing tasks. Key Motherboard Considerations Physical Layout RTX 4090 cards are huge, often occupying multiple slots. Confirm the board can hold two such cards simultaneously. If space or thermal conflicts arise, use a riser cable for vertical placement of the second GPU. PCIe Lane Splitting Ensure the motherboard can run two PCIe 4.0 x8 + x8 slots. Avoid a setup ending up as x16 + x2, which severely limits the second GPU’s bandwidth and can lead to a significant performance bottleneck, especially critical in large model training. Expandability With two GPUs installed, you still need enough M.2 slots and external ports for additional storage or peripherals. After evaluating expandability, performance, and cost-effectiveness, I chose the AMD Ryzen 9 7900X paired with the MSI MPG X670E CARBON. A GPU riser cable resolved the slot conflicts caused by the thickness of dual RTX 4090s.\nBIOS Setup Recommendations Memory Optimization Enable XMP/EXPO (Intel/AMD) to boost memory clock speeds and bandwidth. Overclocking If additional performance is needed, enable PBO (Precision Boost Overdrive) or Intel Performance Tuning and monitor system stability. Thermals \u0026 Stability Avoid extreme overclocking and keep temperatures under control to maintain system stability. Memory During deep learning training, large amounts of system memory are used for data loading and optimizer states (especially in multi-GPU scenarios with Zero-stage parallelism). Aim for at least 2× the total GPU VRAM capacity. This build uses 48GB * 2 (96GB in total), sufficient for everyday multitasking and distributed training without frequent swapping.\nStorage Prefer M.2 NVMe SSDs: They offer superior read/write performance, critical for loading large model weights, caching intermediate files, and storing training logs. Capacity ≥ 2TB: Model files continue to grow in size. 2TB can fill up quickly; consider 4TB or more depending on your needs. SSD Brands: Samsung, SK Hynix, and Western Digital have reliable high-end product lines. Power Supply Dual RTX 4090s can draw 900W–1000W under full load, and you also need overhead for the CPU, motherboard, and storage. Generally, 1,500W+ Platinum or Titanium PSUs are recommended to ensure stable power delivery and mitigate voltage fluctuations.\nI opted for a Corsair AX1600i (digital PSU), which provides real-time power monitoring and sufficient headroom.\nCooling \u0026 Fans I chose an air-cooling setup:\nCPU Cooler: Thermalright FC140, a dual-tower air cooler offering solid thermal performance with relatively low noise. Case Fans: Phanteks T30 120mm * 6, maintaining slightly positive internal pressure to ensure proper airflow across the GPUs and power delivery modules. For prolonged high-load training (such as distributed training of large models), chassis airflow management and adequate fans are crucial. Monitor CPU, GPU, and VRM temperatures regularly and adjust fan curves as needed.\nAdvanced Cooling\nFor even quieter operation, consider a Hybrid or partial liquid-cooling solution, along with finely tuned fan curves. Regularly cleaning dust filters, using dust guards, and refreshing thermal compound can also help improve thermals and stability. Chassis Because the RTX 4090 is massive, and two of them stacked requires ample internal space and airflow, a full-tower chassis is recommended for better cable routing and thermal design. I went with the Phanteks 620PC, which offers ample space and built-in cable management.\nBelow is a sample photo of the completed build:\nSystem \u0026 Software Environment Operating System: Linux (e.g., Ubuntu 22.04 LTS) is highly recommended due to better compatibility and support for CUDA, NVIDIA drivers, and popular deep learning frameworks. The general setup flow:\nOS Installation: Ubuntu or another Linux distribution. NVIDIA Driver Installation: Make sure nvidia-smi detects both 4090 GPUs correctly:\nCUDA Toolkit: Verify via nvcc -V:\ncuDNN: Ensure your deep learning frameworks can leverage GPU-accelerated kernels for convolution, RNN, etc. Framework Testing: Use PyTorch, TensorFlow, or JAX to confirm basic inference and training functionality. Docker Containerization: With nvidia-container-toolkit, containers can directly access GPU resources, eliminating host-environment conflicts. For multi-node, multi-GPU setups, consider Kubernetes, Ray, or Slurm for cluster scheduling and resource management. Recommended Tools \u0026 Frameworks Training Frameworks\nLLaMA-Factory: Offers user-friendly packaging for large language model training and inference. Great for beginners. DeepSpeed: Provides distributed training for large models, with multiple parallelization strategies and optimizations. Megatron-LM: NVIDIA’s official large-scale language model training framework, ideal for multi-node, multi-GPU scenarios. Monitoring \u0026 Visualization\nWeights \u0026 Biases or TensorBoard: Real-time monitoring of loss, learning rate, etc., with remote-friendly UI. Inference Tools\nollama: Based on llama.cpp, easy local inference setup. vLLM: Optimized for high-concurrency, multi-user scenarios, delivering superior throughput. Framework ollama vLLM Function Simple local LLM deployment High-concurrency / high-throughput LLM Concurrent Performance drops as concurrency increases Handles higher concurrency with better TPS 16 Threads ~17s/req ~9s/req Throughput Slower token generation speeds ~2× faster token generation Max Concur. Performance deteriorates over 32 threads Remains stable under large concurrency Use Cases Personal or low-traffic apps Enterprise or multi-user high concurrency WebUI\nOpen-WebUI: A user-friendly, web-based solution that integrates multiple AI backends (ollama, OpenAI API, etc.), handy for rapid prototyping and visualization. Advanced Suggestions Development \u0026 Debugging Efficiency\nUse SSH for remote development, and create custom Docker images to reduce setup overhead. Quantization \u0026 Pruning\nTechniques like 4-bit or 8-bit quantization and pruning can reduce model size and VRAM usage, thereby improving inference performance. Mixed-Precision Training\nSwitch to BF16 or FP16 training to accelerate training speed, combined with gradient scaling (GradScaler) to maintain numerical stability. CPU Coordination\nEnhance data loading using multi-threading, multi-processing, or RAM disks for streaming large pretraining datasets. Multi-Node Cluster Deployment\nConnect nodes via InfiniBand or high-speed Ethernet and use Kubernetes for efficient resource scheduling. Summary With the above configuration and methodology, I successfully built a dual RTX 4090 deep learning workstation. It excels at inference and small to medium scale fine-tuning scenarios—ideal for individuals or small teams focusing on LLM research or application development. This setup strikes a balance between cost-effectiveness and flexibility. However, if you plan to train massive models (hundreds of billions of parameters) in a full-parameter regime, you will still need more GPUs (e.g., multi-GPU A100 or H100 clusters).\nFrom personal experience, dual 4090 GPUs provide sufficient performance within a reasonable budget, meeting the majority of small-to-medium-scale R\u0026D needs—a solid option for qualified individuals or teams to consider.\nReferences Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe lane specs AMD R5 7600X PCIe lane specs MSI MPG X670E CARBON Specifications nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI Copyright \u0026 Citation Disclaimer: The configurations, prices, and recommendations in this article are for technical discussion and research purposes only. Actual purchases and deployments should be carefully evaluated based on individual budgets and requirements. The author assumes no liability for any direct or indirect consequences resulting from following or adapting the information provided herein.\nCitation: When reposting or referencing this content, please credit the original author and source.\n","wordCount":"1923","inLanguage":"en","datePublished":"2024-12-21T12:00:00+08:00","dateModified":"2024-12-21T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2024-12-21-build-gpu-server/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Yue Shui Blog (Alt + H)">Yue Shui Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Building a Home Deep Learning Rig with Dual RTX 4090 GPUs</h1><div class=post-meta><p>Date: December 21, 2024
| Estimated Reading Time: 10 min
| Author: Yue Shui</p>&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#rent-a-gpu-or-buy-your-own aria-label="Rent a GPU or Buy Your Own?">Rent a GPU or Buy Your Own?</a></li><li><a href=#background aria-label=Background>Background</a></li><li><a href=#assembly-strategy--configuration-details aria-label="Assembly Strategy & Configuration Details">Assembly Strategy & Configuration Details</a><ul><li><a href=#gpu-selection aria-label="GPU Selection">GPU Selection</a><ul><li><a href=#cooling-options-blower-vs-air-cooling-vs-liquid-cooling aria-label="Cooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling">Cooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling</a></li></ul></li><li><a href=#cpu--motherboard aria-label="CPU & Motherboard">CPU & Motherboard</a><ul><li><a href=#msi-mpg-x670e-carbon-motherboard aria-label="MSI MPG X670E CARBON Motherboard">MSI MPG X670E CARBON Motherboard</a></li><li><a href=#amd-ryzen-9-7900x-highlights aria-label="AMD Ryzen 9 7900X Highlights">AMD Ryzen 9 7900X Highlights</a></li><li><a href=#key-motherboard-considerations aria-label="Key Motherboard Considerations">Key Motherboard Considerations</a></li><li><a href=#bios-setup-recommendations aria-label="BIOS Setup Recommendations">BIOS Setup Recommendations</a></li></ul></li><li><a href=#memory aria-label=Memory>Memory</a></li><li><a href=#storage aria-label=Storage>Storage</a></li><li><a href=#power-supply aria-label="Power Supply">Power Supply</a></li><li><a href=#cooling--fans aria-label="Cooling & Fans">Cooling & Fans</a></li><li><a href=#chassis aria-label=Chassis>Chassis</a></li></ul></li><li><a href=#system--software-environment aria-label="System & Software Environment">System & Software Environment</a></li><li><a href=#recommended-tools--frameworks aria-label="Recommended Tools & Frameworks">Recommended Tools & Frameworks</a></li><li><a href=#advanced-suggestions aria-label="Advanced Suggestions">Advanced Suggestions</a></li><li><a href=#summary aria-label=Summary>Summary</a></li><li><a href=#references aria-label=References>References</a></li><li><a href=#copyright--citation aria-label="Copyright & Citation">Copyright & Citation</a></li></ul></div></details></div><div class=post-content><h2 id=rent-a-gpu-or-buy-your-own>Rent a GPU or Buy Your Own?<a hidden class=anchor aria-hidden=true href=#rent-a-gpu-or-buy-your-own>#</a></h2><p>Before setting up a deep learning environment, consider <strong>usage duration</strong>, <strong>budget</strong>, <strong>data privacy</strong>, and <strong>maintenance overhead</strong>. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.</p><ul><li><p><strong>Advantages of Renting GPUs</strong>:</p><ul><li>No high upfront hardware costs</li><li>Elastic scalability according to project needs</li><li>Cloud vendors typically provide data compliance and security assurances, reducing hardware maintenance concerns</li></ul></li><li><p><strong>Advantages of Buying GPUs</strong>:</p><ul><li>Lower total cost if used extensively over the long term</li><li>Higher privacy and control for in-house data and models</li><li>Hardware can be upgraded or adjusted at any time, offering more flexible deployment</li></ul></li></ul><blockquote><p><strong>Personal Suggestions</strong></p><ol><li>If you have a limited budget or are just starting out, use Colab, Kaggle, or cloud-based GPU solutions first.</li><li>Once computing needs and privacy requirements increase, consider building your own multi-GPU server or renting a multi-node, multi-GPU cluster.</li></ol></blockquote><hr><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>In September 2023, to continue my exploration and research on Large Language Models (LLMs) outside of work, I assembled a <strong>dual RTX 4090</strong> personal AI server. It has been running for nearly a year, and here are some observations:</p><ul><li><strong>Noise</strong>: Placed under my desk, the fans can get quite loud under full load. However, noise levels are acceptable during inference or moderate loads.</li><li><strong>Inference Performance</strong>: With a total of 48GB of VRAM, and by using 4-bit quantization, it can handle 70B-level models (e.g., Llama 70B, Qwen 72B).</li><li><strong>Training Performance</strong>: By using <a href=https://github.com/microsoft/DeepSpeed>DeepSpeed</a> with distributed and offload techniques (ZeRO-3 + CPU offload), I can finetune around 34B parameter models (e.g., CodeLlama 34B).</li><li><strong>Cost-Effectiveness</strong>: For personal or small-team experiments and small-to-medium scale model training, this configuration is quite practical. However, full-scale training of very large models (hundreds of billions of parameters) still requires more professional GPUs (e.g., multi-GPU A100 or H100 clusters).</li></ul><p>Below is an illustration of VRAM requirements for various model sizes and training approaches (referenced from <a href=https://github.com/hiyouga/LLaMA-Factory#hardware-requirement>LLaMA-Factory</a>):</p><p><img alt="Hardware Requirements Illustration" loading=lazy src=/posts/2024-12-21-build-gpu-server/hardware_requirement.png></p><hr><h2 id=assembly-strategy--configuration-details>Assembly Strategy & Configuration Details<a hidden class=anchor aria-hidden=true href=#assembly-strategy--configuration-details>#</a></h2><p>The total budget is around <strong>40,000 RMB (~6,000 USD)</strong>. The final build list is as follows (for reference only):</p><table><thead><tr><th style=text-align:center>Component</th><th style=text-align:left>Model</th><th style=text-align:center>Price (RMB)</th></tr></thead><tbody><tr><td style=text-align:center><strong>GPU</strong></td><td style=text-align:left>RTX 4090 * 2</td><td style=text-align:center>25098</td></tr><tr><td style=text-align:center><strong>Motherboard + CPU</strong></td><td style=text-align:left>AMD R9 7900X + MSI MPG X670E CARBON</td><td style=text-align:center>5157.55</td></tr><tr><td style=text-align:center><strong>Memory</strong></td><td style=text-align:left>Corsair 48GB * 2 (DDR5 5600)</td><td style=text-align:center>2399</td></tr><tr><td style=text-align:center><strong>SSD</strong></td><td style=text-align:left>SOLIDIGM 944 PRO 2TB *2 + Samsung 990PRO 4TB</td><td style=text-align:center>4587</td></tr><tr><td style=text-align:center><strong>PSU</strong></td><td style=text-align:left>Corsair AX1600i</td><td style=text-align:center>2699</td></tr><tr><td style=text-align:center><strong>Fans</strong></td><td style=text-align:left>Phanteks T30 120mm P * 6</td><td style=text-align:center>1066.76</td></tr><tr><td style=text-align:center><strong>CPU Cooler</strong></td><td style=text-align:left>Thermalright FC140 BLACK</td><td style=text-align:center>419</td></tr><tr><td style=text-align:center><strong>Chassis</strong></td><td style=text-align:left>Phanteks 620PC Full Tower</td><td style=text-align:center>897.99</td></tr><tr><td style=text-align:center><strong>GPU Riser Cable</strong></td><td style=text-align:left>Phanteks FL60 PCI-E4.0 *16</td><td style=text-align:center>399</td></tr></tbody></table><p><strong>Total</strong>: ~ 42,723.3 RMB</p><h3 id=gpu-selection>GPU Selection<a hidden class=anchor aria-hidden=true href=#gpu-selection>#</a></h3><p>For large-scale model research, <strong>floating-point performance (TFLOPS)</strong> and <strong>VRAM capacity</strong> are the most critical metrics. Professional GPUs (A100, H100, etc.) boast higher VRAM and NVLink support but can easily cost hundreds of thousands of RMB, which is prohibitive for most individual users. According to <a href=https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/>Tim Dettmers</a>, the RTX 4090 offers excellent performance-per-dollar and supports features like BF16 and Flash Attention, making it a high-value option.</p><h4 id=cooling-options-blower-vs-air-cooling-vs-liquid-cooling>Cooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling<a hidden class=anchor aria-hidden=true href=#cooling-options-blower-vs-air-cooling-vs-liquid-cooling>#</a></h4><table><thead><tr><th style=text-align:center><strong>Cooling Method</strong></th><th style=text-align:left><strong>Advantages</strong></th><th style=text-align:center><strong>Disadvantages</strong></th><th style=text-align:center><strong>Best For</strong></th></tr></thead><tbody><tr><td style=text-align:center><strong>Blower Fan</strong></td><td style=text-align:left>Compact form factor; good for multi-GPU setups</td><td style=text-align:center>Loud noise, generally weaker cooling</td><td style=text-align:center>Server racks, dense multi-GPU deployments</td></tr><tr><td style=text-align:center><strong>Air-Cooling</strong></td><td style=text-align:left>Good balance of performance and noise; easy upkeep</td><td style=text-align:center>Cards are often large, require space</td><td style=text-align:center>Home or personal research (with enough space)</td></tr><tr><td style=text-align:center><strong>Liquid-Cooling</strong></td><td style=text-align:left>Excellent cooling, quieter under full load</td><td style=text-align:center>Risk of leaks, higher cost</td><td style=text-align:center>Extreme quiet needs or heavy overclocking</td></tr></tbody></table><blockquote><p><strong>Home Setup Recommendation</strong>: <strong>Air-cooled GPUs</strong> are typically the best balance of cooling efficiency, noise, and maintenance. They are generally more user-friendly compared to blower or liquid-cooled alternatives.</p></blockquote><h3 id=cpu--motherboard>CPU & Motherboard<a hidden class=anchor aria-hidden=true href=#cpu--motherboard>#</a></h3><p>In deep learning scenarios, the CPU handles data preprocessing, pipeline scheduling, and multi-process/thread management, ensuring high-throughput, low-latency data delivery to the GPUs. Thus, the CPU’s main requirements include <strong>sufficient PCIe lanes</strong> and <strong>robust multi-threaded performance</strong>.</p><ul><li><strong>Intel</strong>: 13th/14th Generation i9 (e.g., 13900K) offers 20 primary PCIe lanes, enough for dual GPUs at x8 + x8.</li><li><strong>AMD</strong>: The Ryzen 7000/9000 series (e.g., 7950X) provides 28 (24 usable) PCIe lanes and also supports dual GPUs at x8 + x8, with enough bandwidth for multiple M.2 SSDs.</li></ul><hr><h4 id=msi-mpg-x670e-carbon-motherboard>MSI MPG X670E CARBON Motherboard<a hidden class=anchor aria-hidden=true href=#msi-mpg-x670e-carbon-motherboard>#</a></h4><ul><li><strong>Expandability</strong>: Supports PCIe 5.0 and DDR5 memory, offering ample future-proofing.</li><li><strong>Stability</strong>: High-quality power delivery, ensuring stable operation for both CPU and multiple GPUs.</li><li><strong>Rich I/O</strong>: Supports multiple M.2 SSDs and USB4 for various usage scenarios.</li></ul><hr><h4 id=amd-ryzen-9-7900x-highlights>AMD Ryzen 9 7900X Highlights<a hidden class=anchor aria-hidden=true href=#amd-ryzen-9-7900x-highlights>#</a></h4><ul><li><strong>Cores & Threads</strong>: 12 cores, 24 threads, excellent for data preprocessing and multitasking in deep learning workloads.</li><li><strong>PCIe Bandwidth</strong>: Provides 28 (24 usable) PCIe 5.0 lanes to support dual GPUs at x8 + x8 while also powering high-speed M.2 SSDs.</li><li><strong>Power Efficiency</strong>: Built on the Zen 4 architecture, delivering outstanding performance-to-power ratio for high-performance computing tasks.</li></ul><hr><h4 id=key-motherboard-considerations>Key Motherboard Considerations<a hidden class=anchor aria-hidden=true href=#key-motherboard-considerations>#</a></h4><ol><li><strong>Physical Layout</strong><ul><li>RTX 4090 cards are huge, often occupying multiple slots. Confirm the board can hold two such cards simultaneously. If space or thermal conflicts arise, use a riser cable for vertical placement of the second GPU.</li></ul></li><li><strong>PCIe Lane Splitting</strong><ul><li>Ensure the motherboard can run two PCIe 4.0 x8 + x8 slots. Avoid a setup ending up as x16 + x2, which severely limits the second GPU’s bandwidth and can lead to a significant performance bottleneck, especially critical in large model training.</li></ul></li><li><strong>Expandability</strong><ul><li>With two GPUs installed, you still need enough M.2 slots and external ports for additional storage or peripherals.</li></ul></li></ol><p>After evaluating expandability, performance, and cost-effectiveness, I chose the <strong>AMD Ryzen 9 7900X paired with the MSI MPG X670E CARBON</strong>. A GPU riser cable resolved the slot conflicts caused by the thickness of dual RTX 4090s.</p><hr><h4 id=bios-setup-recommendations>BIOS Setup Recommendations<a hidden class=anchor aria-hidden=true href=#bios-setup-recommendations>#</a></h4><ol><li><strong>Memory Optimization</strong><ul><li>Enable <strong>XMP/EXPO</strong> (Intel/AMD) to boost memory clock speeds and bandwidth.</li></ul></li><li><strong>Overclocking</strong><ul><li>If additional performance is needed, enable <strong>PBO (Precision Boost Overdrive)</strong> or Intel Performance Tuning and monitor system stability.</li></ul></li><li><strong>Thermals & Stability</strong><ul><li>Avoid extreme overclocking and keep temperatures under control to maintain system stability.</li></ul></li></ol><h3 id=memory>Memory<a hidden class=anchor aria-hidden=true href=#memory>#</a></h3><p>During deep learning training, large amounts of system memory are used for data loading and optimizer states (especially in multi-GPU scenarios with Zero-stage parallelism). <strong>Aim for at least 2× the total GPU VRAM capacity</strong>. This build uses 48GB * 2 (96GB in total), sufficient for everyday multitasking and distributed training without frequent swapping.</p><h3 id=storage>Storage<a hidden class=anchor aria-hidden=true href=#storage>#</a></h3><ul><li><strong>Prefer M.2 NVMe SSDs</strong>: They offer superior read/write performance, critical for loading large model weights, caching intermediate files, and storing training logs.</li><li><strong>Capacity ≥ 2TB</strong>: Model files continue to grow in size. 2TB can fill up quickly; consider 4TB or more depending on your needs.</li><li><strong>SSD Brands</strong>: Samsung, SK Hynix, and Western Digital have reliable high-end product lines.</li></ul><h3 id=power-supply>Power Supply<a hidden class=anchor aria-hidden=true href=#power-supply>#</a></h3><p>Dual RTX 4090s can draw <strong>900W–1000W</strong> under full load, and you also need overhead for the CPU, motherboard, and storage. Generally, <strong>1,500W+</strong> Platinum or Titanium PSUs are recommended to ensure stable power delivery and mitigate voltage fluctuations.<br>I opted for a Corsair AX1600i (digital PSU), which provides real-time power monitoring and sufficient headroom.</p><h3 id=cooling--fans>Cooling & Fans<a hidden class=anchor aria-hidden=true href=#cooling--fans>#</a></h3><p>I chose an <strong>air-cooling</strong> setup:</p><ul><li><strong>CPU Cooler</strong>: Thermalright FC140, a dual-tower air cooler offering solid thermal performance with relatively low noise.</li><li><strong>Case Fans</strong>: Phanteks T30 120mm * 6, maintaining slightly positive internal pressure to ensure proper airflow across the GPUs and power delivery modules.</li></ul><p>For prolonged high-load training (such as distributed training of large models), chassis airflow management and adequate fans are crucial. Monitor CPU, GPU, and VRM temperatures regularly and adjust fan curves as needed.</p><blockquote><p><strong>Advanced Cooling</strong></p><ul><li>For even quieter operation, consider a <em>Hybrid</em> or partial liquid-cooling solution, along with finely tuned fan curves.</li><li>Regularly cleaning dust filters, using dust guards, and refreshing thermal compound can also help improve thermals and stability.</li></ul></blockquote><h3 id=chassis>Chassis<a hidden class=anchor aria-hidden=true href=#chassis>#</a></h3><p>Because the RTX 4090 is massive, and two of them stacked requires ample internal space and airflow, a full-tower chassis is recommended for better cable routing and thermal design. I went with the Phanteks 620PC, which offers ample space and built-in cable management.</p><p>Below is a sample photo of the completed build:<br><img alt="Actual Rig Photo" loading=lazy src=/posts/2024-12-21-build-gpu-server/computer.jpeg></p><hr><h2 id=system--software-environment>System & Software Environment<a hidden class=anchor aria-hidden=true href=#system--software-environment>#</a></h2><p><strong>Operating System</strong>: Linux (e.g., <strong>Ubuntu 22.04 LTS</strong>) is highly recommended due to better compatibility and support for CUDA, NVIDIA drivers, and popular deep learning frameworks. The general setup flow:</p><ol><li><strong>OS Installation</strong>: Ubuntu or another Linux distribution.</li><li><strong>NVIDIA Driver Installation</strong>: Make sure <code>nvidia-smi</code> detects both 4090 GPUs correctly:<br><img alt="nvidia-smi output example" loading=lazy src=/posts/2024-12-21-build-gpu-server/nvidia_smi.png></li><li><strong>CUDA Toolkit</strong>: Verify via <code>nvcc -V</code>:<br><img alt="nvcc -V output example" loading=lazy src=/posts/2024-12-21-build-gpu-server/nvcc.png></li><li><strong>cuDNN</strong>: Ensure your deep learning frameworks can leverage GPU-accelerated kernels for convolution, RNN, etc.</li><li><strong>Framework Testing</strong>: Use <a href=https://pytorch.org/>PyTorch</a>, <a href=https://www.tensorflow.org/>TensorFlow</a>, or <a href=https://github.com/google/jax>JAX</a> to confirm basic inference and training functionality.</li><li><strong>Docker Containerization</strong>:<ul><li>With <a href=https://github.com/NVIDIA/nvidia-container-toolkit>nvidia-container-toolkit</a>, containers can directly access GPU resources, eliminating host-environment conflicts.</li><li>For multi-node, multi-GPU setups, consider <strong>Kubernetes</strong>, <strong>Ray</strong>, or <strong>Slurm</strong> for cluster scheduling and resource management.</li></ul></li></ol><hr><h2 id=recommended-tools--frameworks>Recommended Tools & Frameworks<a hidden class=anchor aria-hidden=true href=#recommended-tools--frameworks>#</a></h2><ol><li><p><strong>Training Frameworks</strong></p><ul><li><a href=https://github.com/hiyouga/LLaMA-Factory><strong>LLaMA-Factory</strong></a>: Offers user-friendly packaging for large language model training and inference. Great for beginners.</li><li><a href=https://github.com/microsoft/DeepSpeed><strong>DeepSpeed</strong></a>: Provides distributed training for large models, with multiple parallelization strategies and optimizations.</li><li><a href=https://github.com/NVIDIA/Megatron-LM><strong>Megatron-LM</strong></a>: NVIDIA’s official large-scale language model training framework, ideal for multi-node, multi-GPU scenarios.</li></ul></li><li><p><strong>Monitoring & Visualization</strong></p><ul><li><a href=https://wandb.ai/><strong>Weights & Biases</strong></a> or <a href=https://www.tensorflow.org/tensorboard><strong>TensorBoard</strong></a>: Real-time monitoring of loss, learning rate, etc., with remote-friendly UI.</li></ul></li><li><p><strong>Inference Tools</strong></p><ul><li><a href=https://github.com/jmorganca/ollama><strong>ollama</strong></a>: Based on <a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a>, easy local inference setup.</li><li><a href=https://github.com/vllm-project/vllm><strong>vLLM</strong></a>: Optimized for high-concurrency, multi-user scenarios, delivering superior throughput.</li></ul><table><thead><tr><th style=text-align:center><strong>Framework</strong></th><th style=text-align:center><strong>ollama</strong></th><th style=text-align:center><strong>vLLM</strong></th></tr></thead><tbody><tr><td style=text-align:center><strong>Function</strong></td><td style=text-align:center>Simple local LLM deployment</td><td style=text-align:center>High-concurrency / high-throughput LLM</td></tr><tr><td style=text-align:center><strong>Concurrent</strong></td><td style=text-align:center>Performance drops as concurrency increases</td><td style=text-align:center>Handles higher concurrency with better TPS</td></tr><tr><td style=text-align:center><strong>16 Threads</strong></td><td style=text-align:center>~17s/req</td><td style=text-align:center>~9s/req</td></tr><tr><td style=text-align:center><strong>Throughput</strong></td><td style=text-align:center>Slower token generation speeds</td><td style=text-align:center>~2× faster token generation</td></tr><tr><td style=text-align:center><strong>Max Concur.</strong></td><td style=text-align:center>Performance deteriorates over 32 threads</td><td style=text-align:center>Remains stable under large concurrency</td></tr><tr><td style=text-align:center><strong>Use Cases</strong></td><td style=text-align:center>Personal or low-traffic apps</td><td style=text-align:center>Enterprise or multi-user high concurrency</td></tr></tbody></table></li><li><p><strong>WebUI</strong></p><ul><li><a href=https://github.com/open-webui/open-webui><strong>Open-WebUI</strong></a>: A user-friendly, web-based solution that integrates multiple AI backends (ollama, OpenAI API, etc.), handy for rapid prototyping and visualization.</li></ul></li></ol><hr><h2 id=advanced-suggestions>Advanced Suggestions<a hidden class=anchor aria-hidden=true href=#advanced-suggestions>#</a></h2><ol><li><p><strong>Development & Debugging Efficiency</strong></p><ul><li>Use SSH for remote development, and create custom Docker images to reduce setup overhead.</li></ul></li><li><p><strong>Quantization & Pruning</strong></p><ul><li>Techniques like 4-bit or 8-bit quantization and pruning can reduce model size and VRAM usage, thereby improving inference performance.</li></ul></li><li><p><strong>Mixed-Precision Training</strong></p><ul><li>Switch to BF16 or FP16 training to accelerate training speed, combined with gradient scaling (GradScaler) to maintain numerical stability.</li></ul></li><li><p><strong>CPU Coordination</strong></p><ul><li>Enhance data loading using multi-threading, multi-processing, or RAM disks for streaming large pretraining datasets.</li></ul></li><li><p><strong>Multi-Node Cluster Deployment</strong></p><ul><li>Connect nodes via InfiniBand or high-speed Ethernet and use Kubernetes for efficient resource scheduling.</li></ul></li></ol><hr><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>With the above configuration and methodology, I successfully built a <strong>dual RTX 4090</strong> deep learning workstation. It excels at <strong>inference</strong> and <strong>small to medium scale fine-tuning</strong> scenarios—ideal for individuals or small teams focusing on LLM research or application development. This setup strikes a balance between <strong>cost-effectiveness</strong> and <strong>flexibility</strong>. However, if you plan to train massive models (hundreds of billions of parameters) in a full-parameter regime, you will still need more GPUs (e.g., multi-GPU A100 or H100 clusters).</p><p>From personal experience, dual 4090 GPUs provide sufficient performance within a reasonable budget, meeting the majority of small-to-medium-scale R&amp;D needs—a solid option for qualified individuals or teams to consider.</p><hr><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/>Tim Dettmers: Which GPU for Deep Learning? (2023)</a></li><li><a href=https://www.intel.com/content/www/us/en/products/sku/236773/intel-core-i9-processor-14900k-36m-cache-up-to-6-00-ghz/specifications.html>Intel 14900K PCIe lane specs</a></li><li><a href=https://www.amd.com/en/products/processors/desktops/ryzen/7000-series/amd-ryzen-5-7600.html>AMD R5 7600X PCIe lane specs</a></li><li><a href=https://www.msi.com/Motherboard/MPG-X670E-CARBON-WIFI/Specification>MSI MPG X670E CARBON Specifications</a></li><li><a href=https://github.com/NVIDIA/nvidia-container-toolkit>nvidia-container-toolkit</a></li><li><a href=https://github.com/hiyouga/LLaMA-Factory>LLaMA-Factory</a></li><li><a href=https://github.com/microsoft/DeepSpeed>DeepSpeed</a></li><li><a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a></li><li><a href=https://github.com/jmorganca/ollama>ollama</a></li><li><a href=https://github.com/vllm-project/vllm>vLLM</a></li><li><a href=https://medium.com/@naman1011/ollama-vs-vllm-which-tool-handles-ai-models-better-a93345b911e6>Ollama vs VLLM: Which Tool Handles AI Models Better?</a></li><li><a href=https://github.com/open-webui/open-webui>Open-WebUI</a></li></ol><hr><h2 id=copyright--citation>Copyright & Citation<a hidden class=anchor aria-hidden=true href=#copyright--citation>#</a></h2><blockquote><p><strong>Disclaimer</strong>: The configurations, prices, and recommendations in this article are for technical discussion and research purposes only. Actual purchases and deployments should be carefully evaluated based on individual budgets and requirements. The author assumes no liability for any direct or indirect consequences resulting from following or adapting the information provided herein.<br><strong>Citation</strong>: When reposting or referencing this content, please credit the original author and source.</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/tags/rtx-4090/>RTX 4090</a></li><li><a href=https://syhya.github.io/tags/ai-hardware/>AI Hardware</a></li><li><a href=https://syhya.github.io/tags/pc-assembly/>PC Assembly</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>