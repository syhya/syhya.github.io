<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Stock Price Prediction and Quantitative Strategy Based on Deep Learning | Yue Shui Blog</title><meta name=keywords content="Deep learning,AI,RNN,LSTM,BiLSTM,GRU,LightGBM,Neural Networks,Stock Prediction,Financial Modeling,Machine Learning,Quantitative Investment,Portfolio Management,Financial Engineering,Algorithmic Trading,Time Series"><meta name=description content="Abstract
The stock market is a crucial component of the financial market. In recent years, with its vigorous development, research on stock price prediction and quantitative investment strategies has attracted scholars from various fields.  With the advancement of Artificial Intelligence (AI) and Machine Learning (ML) in recent years, researchers have shifted from traditional statistical models to AI algorithms. Particularly after the deep learning boom, neural networks have achieved remarkable results in stock price prediction and quantitative investment strategy research. The objective of deep learning is to learn multi-level features, constructing abstract high-level features by combining low-level ones, thereby mining the distributed feature representations of data. This approach enables complex nonlinear modeling to accomplish prediction tasks. Recurrent Neural Networks (RNNs) have been widely applied to sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, leading many researchers to use RNNs for stock price prediction. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The advent of Long Short-Term Memory (LSTM) networks addressed this problem, followed by variants such as Gated Recurrent Units (GRUs), Peephole LSTMs, and Bidirectional LSTMs (BiLSTMs). Traditional stock prediction models often overlook temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully leverages the contextual relationships in both forward and backward temporal directions of time series data. It also avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://syhya.github.io/apple-touch-icon.png><link rel=mask-icon href=https://syhya.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2021-04-21-deep-learning-stock-prediction/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Stock Price Prediction and Quantitative Strategy Based on Deep Learning"><meta property="og:description" content="Abstract The stock market is a crucial component of the financial market. In recent years, with its vigorous development, research on stock price prediction and quantitative investment strategies has attracted scholars from various fields. With the advancement of Artificial Intelligence (AI) and Machine Learning (ML) in recent years, researchers have shifted from traditional statistical models to AI algorithms. Particularly after the deep learning boom, neural networks have achieved remarkable results in stock price prediction and quantitative investment strategy research. The objective of deep learning is to learn multi-level features, constructing abstract high-level features by combining low-level ones, thereby mining the distributed feature representations of data. This approach enables complex nonlinear modeling to accomplish prediction tasks. Recurrent Neural Networks (RNNs) have been widely applied to sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, leading many researchers to use RNNs for stock price prediction. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The advent of Long Short-Term Memory (LSTM) networks addressed this problem, followed by variants such as Gated Recurrent Units (GRUs), Peephole LSTMs, and Bidirectional LSTMs (BiLSTMs). Traditional stock prediction models often overlook temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully leverages the contextual relationships in both forward and backward temporal directions of time series data. It also avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-04-21T12:00:00+08:00"><meta property="article:modified_time" content="2021-04-21T12:00:00+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="AI"><meta property="article:tag" content="RNN"><meta property="article:tag" content="LSTM"><meta property="article:tag" content="BiLSTM"><meta property="article:tag" content="GRU"><meta name=twitter:card content="summary"><meta name=twitter:title content="Stock Price Prediction and Quantitative Strategy Based on Deep Learning"><meta name=twitter:description content="Abstract
The stock market is a crucial component of the financial market. In recent years, with its vigorous development, research on stock price prediction and quantitative investment strategies has attracted scholars from various fields.  With the advancement of Artificial Intelligence (AI) and Machine Learning (ML) in recent years, researchers have shifted from traditional statistical models to AI algorithms. Particularly after the deep learning boom, neural networks have achieved remarkable results in stock price prediction and quantitative investment strategy research. The objective of deep learning is to learn multi-level features, constructing abstract high-level features by combining low-level ones, thereby mining the distributed feature representations of data. This approach enables complex nonlinear modeling to accomplish prediction tasks. Recurrent Neural Networks (RNNs) have been widely applied to sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, leading many researchers to use RNNs for stock price prediction. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The advent of Long Short-Term Memory (LSTM) networks addressed this problem, followed by variants such as Gated Recurrent Units (GRUs), Peephole LSTMs, and Bidirectional LSTMs (BiLSTMs). Traditional stock prediction models often overlook temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully leverages the contextual relationships in both forward and backward temporal directions of time series data. It also avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Stock Price Prediction and Quantitative Strategy Based on Deep Learning","item":"https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Stock Price Prediction and Quantitative Strategy Based on Deep Learning","name":"Stock Price Prediction and Quantitative Strategy Based on Deep Learning","description":"Abstract The stock market is a crucial component of the financial market. In recent years, with its vigorous development, research on stock price prediction and quantitative investment strategies has attracted scholars from various fields. With the advancement of Artificial Intelligence (AI) and Machine Learning (ML) in recent years, researchers have shifted from traditional statistical models to AI algorithms. Particularly after the deep learning boom, neural networks have achieved remarkable results in stock price prediction and quantitative investment strategy research. The objective of deep learning is to learn multi-level features, constructing abstract high-level features by combining low-level ones, thereby mining the distributed feature representations of data. This approach enables complex nonlinear modeling to accomplish prediction tasks. Recurrent Neural Networks (RNNs) have been widely applied to sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, leading many researchers to use RNNs for stock price prediction. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The advent of Long Short-Term Memory (LSTM) networks addressed this problem, followed by variants such as Gated Recurrent Units (GRUs), Peephole LSTMs, and Bidirectional LSTMs (BiLSTMs). Traditional stock prediction models often overlook temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully leverages the contextual relationships in both forward and backward temporal directions of time series data. It also avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies.\n","keywords":["Deep learning","AI","RNN","LSTM","BiLSTM","GRU","LightGBM","Neural Networks","Stock Prediction","Financial Modeling","Machine Learning","Quantitative Investment","Portfolio Management","Financial Engineering","Algorithmic Trading","Time Series"],"articleBody":"Abstract The stock market is a crucial component of the financial market. In recent years, with its vigorous development, research on stock price prediction and quantitative investment strategies has attracted scholars from various fields. With the advancement of Artificial Intelligence (AI) and Machine Learning (ML) in recent years, researchers have shifted from traditional statistical models to AI algorithms. Particularly after the deep learning boom, neural networks have achieved remarkable results in stock price prediction and quantitative investment strategy research. The objective of deep learning is to learn multi-level features, constructing abstract high-level features by combining low-level ones, thereby mining the distributed feature representations of data. This approach enables complex nonlinear modeling to accomplish prediction tasks. Recurrent Neural Networks (RNNs) have been widely applied to sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, leading many researchers to use RNNs for stock price prediction. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The advent of Long Short-Term Memory (LSTM) networks addressed this problem, followed by variants such as Gated Recurrent Units (GRUs), Peephole LSTMs, and Bidirectional LSTMs (BiLSTMs). Traditional stock prediction models often overlook temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully leverages the contextual relationships in both forward and backward temporal directions of time series data. It also avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies.\nThe first part of this paper’s experiments utilizes stock data from China’s Shanghai Pudong Development Bank and the US’s IBM to establish stock prediction models using LSTM, GRU, and BiLSTM. By comparing the prediction results of these three deep learning models, it is found that the BiLSTM model outperforms the others for both datasets, demonstrating better prediction accuracy. The second part uses A-share market-wide stock data and first employs a LightGBM model to screen 50 factors, selecting the top 10 most important factors. Subsequently, a BiLSTM model is used to select and combine these factors to establish a quantitative investment strategy. Empirical analysis and backtesting of this strategy reveal that it outperforms the market benchmark index, indicating the practical application value of the BiLSTM model in stock price prediction and quantitative investment.\nKeywords: Quantitative Investment; Deep Learning; Neural Network Model; Multi-Factor Stock Selection; BiLSTM\nChapter 1 Introduction 1.1 Research Background and Significance 1.1.1 Research Background Emerging in the 1970s, quantitative investment gradually entered the vision of investors, initiating a new revolution that changed the landscape of portfolio management previously dominated by passive management and the efficient market hypothesis. The efficient market hypothesis posits that under market efficiency, stock prices reflect all market information, and no excess returns exist. Passive investment management, based on the belief that markets are efficient, focuses more on asset classes, with the most common approach being purchasing index funds and tracking published index performance. In contrast, active investment management relies primarily on investors’ subjective judgments of the market and individual stocks. By applying mathematical models to the financial domain and using available public data to evaluate stocks, active managers construct portfolios to achieve returns. Quantitative investment, through statistical processing of vast historical data to uncover investment opportunities and avoid subjective factors, has gained increasing popularity among investors. Since the rise of quantitative investment, people have gradually utilized various technologies for stock price prediction to better establish quantitative investment strategies. Early domestic and international scholars adopted statistical methods for modeling and predicting stock prices, such as exponential smoothing, multiple regression, and Autoregressive Moving Average (ARMA) models. However, due to the multitude of factors influencing the stock market and the large volume of data, stock prediction is highly challenging, and the prediction effectiveness of various statistical models has been unsatisfactory.\nIn recent years, the continuous development of machine learning, deep learning, and neural network technologies has provided technical support for stock price prediction and the construction of quantitative strategies. Many scholars have achieved new breakthroughs using methods like Random Forest, Neural Networks, Support Vector Machines, and Convolutional Neural Networks. The ample historical data in the stock market, coupled with diverse technological support, provides favorable conditions for stock price prediction and the construction of quantitative strategies.\n1.1.2 Research Significance From the perspective of the long-term development of the national economic system and financial markets, research on stock price prediction models and quantitative investment strategies is indispensable. China started relatively late, with a less mature financial market, fewer financial instruments, and lower market efficiency. However, in recent years, the country has gradually relaxed policies and vigorously developed the financial market, providing a “breeding ground” for the development of quantitative investment. Developing quantitative investment and emerging financial technologies can offer China’s financial market an opportunity for a “curve overtaking”. Furthermore, the stock price index, as a crucial economic indicator, serves as a barometer for China’s economic development.\nFrom the perspective of individual and institutional investors, constructing stock price prediction models and quantitative investment strategy models enhances market efficiency. Individual investors often lack comprehensive professional knowledge, and their investment behaviors can be somewhat blind. Developing relevant models to provide references can reduce the probability of judgment errors and change the relatively disadvantaged position of individual investors in the capital market. For institutional investors, rational and objective models, combined with personal experience, improve the accuracy of decision-making and provide new directions for investment behaviors.\nIn summary, considering China’s current development status, this paper’s selection of individual stocks for stock price prediction models and A-share market-wide stocks for quantitative strategy research holds significant practical research value.\n1.2 Literature Review White (1988)$^{[1]}$ used a Backpropagation (BP) neural network to predict the daily returns of IBM stock. However, due to the BP neural network model’s susceptibility to gradient explosion, the model could not converge to a global minimum, thus failing to achieve accurate predictions.\nKimoto (1990)$^{[2]}$ developed a system for predicting the Tokyo Stock Exchange Prices Index (TOPIX) using modular neural network technology. This system not only successfully predicted TOPIX but also achieved a certain level of profitability through stock trading simulations based on the prediction results.\nG. Peter Zhang (2003)$^{[3]}$ conducted a comparative study on the performance of Autoregressive Integrated Moving Average (ARIMA) models and Artificial Neural Network (ANN) models in time series forecasting. The results showed that ANN models significantly outperformed ARIMA models in terms of time series prediction accuracy.\nRyo Akita (2016)$^{[4]}$ selected the Consumer Price Index (CPI), Price-to-Earnings ratio (P/E ratio), and various events reported in newspapers as features, and constructed a financial time series prediction model using paragraph vectors and LSTM networks. Using actual data from fifty listed companies on the Tokyo Stock Exchange, the effectiveness of this model in predicting stock opening prices was verified.\nKunihiro Miyazaki (2017)$^{[5]}$ constructed a model for predicting the rise and fall of the Topix Core 30 index and its constituent stocks by extracting daily stock chart images and 30-minute stock price data. The study compared multiple models, including Logistic Regression (LR), Random Forest (RF), Multilayer Perceptron (MLP), LSTM, CNN, PCA-CNN, and CNN-LSTM. The results indicated that LSTM had the best prediction performance, CNN performed generally, but hybrid models combining CNN and LSTM could improve prediction accuracy.\nTaewook Kim (2019)$^{[6]}$ proposed an LSTM-CNN hybrid model to combine features from both stock price time series and stock price image data representations to predict the stock price of the S\u0026P 500 index. The study showed that the LSTM-CNN model outperformed single models in stock price prediction, and this prediction had practical significance for constructing quantitative investment strategies.\n1.3 Innovations of the Paper This paper has the following innovations in stock price prediction:\nData from both the domestic A-share market (Shanghai Pudong Development Bank) and the international US stock market (IBM) are used for research, avoiding the limitations of single-market studies. Traditional BP models have never considered temporal factors, or like LSTM models, only consider unidirectional temporal relationships. Therefore, this paper uses the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions of time series data and avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies. Empirical evidence, compared with LSTM and GRU models, demonstrates its ability to improve prediction accuracy. The stock price prediction model is trained using multiple stock features, including opening price, closing price, highest price, and trading volume. Compared to single-feature prediction of stock closing prices, this approach is theoretically more accurate and can better compare the prediction effectiveness of LSTM, GRU, and BiLSTM for stocks. This paper has the following innovations in quantitative strategy research:\nInstead of using common market factors, this paper uses multiple price-volume factors obtained through Genetic Programming (GP) algorithms and artificial data mining. LightGBM model is used to screen 50 factors, selecting the top 10 most important factors. Traditional quantitative investment models generally use LSTM and CNN models to construct quantitative investment strategies. This paper uses A-share market-wide data and employs a BiLSTM model to select and combine factors to establish a quantitative investment strategy. Backtesting and empirical analysis of this strategy show that it outperforms the market benchmark index (CSI All Share), demonstrating the practical application value of the BiLSTM model in stock price prediction and quantitative investment. 1.4 Research Framework of the Paper This paper conducts research on stock price prediction and quantitative strategies based on deep learning algorithms. The specific research framework of this paper is shown in Fig. 1:\nFig. 1. Research Framework.\nThe specific research framework of this paper is as follows:\nChapter 1 is the introduction. This chapter first introduces the research significance and background of stock price prediction and quantitative strategy research. Then, it reviews the current research status, followed by an explanation of the innovations of this paper compared to existing research, and finally, a brief description of the research framework of this paper.\nChapter 2 is about related theoretical foundations. This chapter introduces the basic theories of deep learning models and quantitative stock selection involved in this research. The deep learning model section sequentially introduces four deep learning models: RNN, LSTM, GRU, and BiLSTM, with a focus on the internal structure of the LSTM model. The quantitative stock selection theory section sequentially introduces the Mean-Variance Model, Capital Asset Pricing Model, Arbitrage Pricing Theory, Multi-Factor Model, Fama-French Three-Factor Model, and Five-Factor Model. This section introduces the history of multi-factor quantitative stock selection from the development context of various financial theories and models.\nChapter 3 is a comparative study of LSTM, GRU, and BiLSTM in stock price prediction. This chapter first introduces the datasets of domestic and international stocks used in the experiment, and then performs data preprocessing steps of normalization and data partitioning. It then describes the network structures, model compilation, and hyperparameter settings of the LSTM, GRU, and BiLSTM models used in this chapter, and conducts experiments to obtain experimental results. Finally, the experimental results are analyzed, and a summary of this chapter is provided.\nChapter 4 is a study on a quantitative investment model based on LightGBM-BiLSTM. This chapter first outlines the experimental steps, and then introduces the stock data and factor data used in the experiment. Subsequently, factors are processed sequentially through missing value handling, outlier removal, factor standardization, and factor neutralization to obtain cleaned factors. Then, LightGBM and BiLSTM are used for factor selection and factor combination, respectively. Finally, a quantitative strategy is constructed based on the obtained model, and backtesting is performed on the quantitative strategy.\nChapter 5 is the conclusion and future directions. This chapter summarizes the main research content of this paper on stock price prediction and quantitative investment strategies. Based on the existing shortcomings of the current research, future research directions are proposed.\nChapter 2 Related Theoretical Foundations 2.1 Deep Learning Models 2.1.1 RNN Recurrent Neural Networks (RNNs) are widely used for sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, hence many previous studies have used RNNs to predict stock prices. RNNs employ a very simple chain structure of repeating modules, such as a single tanh layer. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The emergence of LSTM solved this problem. Fig. 2 is an RNN structure diagram.\nFig. 2. RNN Structure Diagram. (Image source: Understanding LSTM Networks)\n2.1.2 LSTM Long Short-Term Memory (LSTM) networks are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter \u0026 Schmidhuber (1997)$^{[7]}$ and have been refined and popularized by many in subsequent work. Due to their unique design structure, LSTMs are relatively insensitive to gap lengths and solve the gradient vanishing and explosion problems of traditional RNNs. Compared to traditional RNNs and other time series models like Hidden Markov Models (HMMs), LSTMs can better handle and predict important events in time series with very long intervals and delays. Therefore, LSTMs are widely used in machine translation and speech recognition.\nLSTMs are explicitly designed to avoid long-term dependency problems. All recurrent neural networks have the form of a chain of repeating modules of neural networks, but LSTM improves the structure of RNN. Instead of a single neural network layer, LSTM uses a four-layer structure that interacts in a special way.\nFig. 3. LSTM Structure Diagram 1. (Image source: Understanding LSTM Networks)\nFig. 4. LSTM Structure Diagram 2. (Image source: Understanding LSTM Networks)\nAs shown in Fig. 3, black lines are used to represent the transmission of an output vector from one node to the input vector of another node. A neural network layer is a processing module with a $\\sigma$ activation function or a tanh activation function; pointwise operation represents element-wise multiplication between vectors; vector transfer indicates the direction of information flow; concatenate and copy are represented by two black lines merging together and two black lines separating, respectively, to indicate information merging and information copying.\nBelow, we take LSTM as an example to explain its structure in detail.\nForget Gate Fig. 5. Forget Gate Calculation (Image source: Understanding LSTM Networks)\n$$ f_{t} = \\sigma\\left(W_{f} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{f}\\right) $$Parameter Description:\n$h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $\\sigma$: Sigmoid activation function $W_{f}$: Weight matrix for the forget gate $b_{f}$: Bias vector parameter for the forget gate The first step, as shown in Fig. 5, is to decide what information to discard from the cell state. This process is calculated by the sigmoid function to obtain the value of $f_{t}$ (the range of $f_{t}$ is between 0 and 1, where 0 means completely block, and 1 means completely pass through) to determine whether the cell state $C_{t-1}$ is passed through or partially passed through.\nInput Gate Fig. 6. Input Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} i_{t} \u0026= \\sigma\\left(W_{i} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{i}\\right) \\\\ \\tilde{C}_{t} \u0026= \\tanh\\left(W_{C} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{C}\\right) \\end{aligned} $$Parameter Description:\n$h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $\\sigma$: Sigmoid activation function $W_{i}$: Weight matrix for the input gate $b_{i}$: Bias vector parameter for the input gate $W_{C}$: Weight matrix for the cell state $b_{C}$: Bias vector parameter for the cell state $\\tanh$: tanh activation function The second step, as shown in Fig. 6, uses a sigmoid function to calculate what information we want to store in the cell state. Next, a $\\tanh$ layer creates a candidate vector $\\tilde{C}_{t}$, which will be added to the cell state.\nFig. 7. Current Cell State Calculation (Image source: Understanding LSTM Networks)\n$$ C_{t} = f_{t} * C_{t-1} + i_{t} * \\tilde{C}_{t} $$Parameter Description:\n$C_{t-1}$: Cell state from the previous time step $\\tilde{C}_{t}$: Temporary cell state $i_{t}$: Value of the input gate $f_{t}$: Value of the forget gate The third step, as shown in Fig. 7, calculates the current cell state $C_t$ by combining the effects of the forget gate and the input gate.\nThe forget gate $f_t$ weights the previous cell state $C_{t-1}$ to discard unnecessary information. The input gate $i_t$ weights the candidate cell state $\\tilde{C}_t$ to decide how much new information to introduce. Finally, the two parts are added together to update and derive the current cell state $C_t$. Output Gate Fig. 8. Output Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} o_{t} \u0026= \\sigma\\left(W_{o} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{o}\\right) \\\\ h_{t} \u0026= o_{t} * \\tanh\\left(C_{t}\\right) \\end{aligned} $$Parameter Description:\n$o_{t}$: Value of the output gate $\\sigma$: Sigmoid activation function $W_{o}$: Weight matrix for the output gate $h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $b_{o}$: Bias vector parameter for the output gate $h_{t}$: Output at the current time step $\\tanh$: tanh activation function $C_{t}$: Cell state at the current time step The fourth step, as shown in Fig. 8, uses a sigmoid function to calculate the value of the output gate. Finally, the cell state $C_{t}$ at this time step is processed by a tanh activation function and multiplied by the value of the output gate $o_{t}$ to obtain the output $h_{t}$ at the current time step.\n2.1.3 GRU K. Cho (2014)$^{[8]}$ proposed the Gated Recurrent Unit (GRU). GRU is mainly simplified and adjusted based on LSTM, merging the original forget gate, input gate, and output gate of LSTM into an update gate and a reset gate. In addition, GRU also merges the cell state and hidden state, thereby reducing the complexity of the model while still achieving performance comparable to LSTM in some tasks.\nThis model can save a lot of time when the training dataset is relatively large and shows better performance on some smaller and less frequent datasets$^{[9][10]}$.\nFig. 9. GRU Structure Diagram (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} z_{t} \u0026= \\sigma\\left(W_{z} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ r_{t} \u0026= \\sigma\\left(W_{r} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ \\tilde{h}_{t} \u0026= \\tanh\\left(W \\cdot \\left[r_{t} * h_{t-1}, x_{t}\\right]\\right) \\\\ h_{t} \u0026= \\left(1 - z_{t}\\right) * h_{t-1} + z_{t} * \\tilde{h}_{t} \\end{aligned} $$Parameter Description:\n$z_{t}$: Value of the update gate $r_{t}$: Value of the reset gate $W_{z}$: Weight matrix for the update gate $W_{r}$: Weight matrix for the reset gate $\\tilde{h}_{t}$: Temporary output 2.1.4 BiLSTM Bidirectional Long Short-Term Memory (BiLSTM) networks are formed by combining a forward LSTM and a backward LSTM. The BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions of time series data, enabling it to learn information with long-term dependencies. Compared to unidirectional LSTM, it can better consider the reverse impact of future data. Fig. 10 is a BiLSTM structure diagram.\nFig. 10. BiLSTM Structure Diagram. (Image source: Baeldung)\n2.2 Quantitative Stock Selection Theory 2.2.1 Mean-Variance Model Quantitative stock selection strategies originated in the 1950s. Markowitz (1952)$^{[11]}$ proposed the Mean-Variance Model. This model not only laid the foundation for modern portfolio theory, quantifying investment risk, but also established a specific model describing risk and expected return. It broke away from the previous situation of qualitative analysis of investment portfolios without quantitative analysis, successfully introducing mathematical models into the field of financial investment.\n$$ \\begin{aligned} \\mathrm{E}\\left(R_{p}\\right) \u0026= \\sum_{i=1}^{n} w_{i} \\mathrm{E}\\left(R_{i}\\right) \\\\ \\sigma_{p}^{2} \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\operatorname{Cov}\\left(R_{i}, R_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\sigma_{i} \\sigma_{j} \\rho_{ij} \\\\ \\sigma_{i} \u0026= \\sqrt{\\operatorname{Var}\\left(R_{i}\\right)}, \\quad \\rho_{ij} = \\operatorname{Corr}\\left(R_{i}, R_{j}\\right) \\end{aligned} $$$$ \\min \\sigma_{p}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} \\mathrm{E}\\left(R_{i}\\right) w_{i} = \\mu_{p}, \\quad \\sum_{i=1}^{n} w_{i} = 1 $$$$ \\begin{aligned} \\Omega \u0026= \\begin{pmatrix} \\sigma_{11} \u0026 \\cdots \u0026 \\sigma_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\sigma_{n1} \u0026 \\cdots \u0026 \\sigma_{nn} \\end{pmatrix} = \\begin{pmatrix} \\operatorname{Var}\\left(R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Cov}\\left(R_{1}, R_{n}\\right) \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\operatorname{Cov}\\left(R_{n}, R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Var}\\left(R_{n}\\right) \\end{pmatrix} \\\\ \\Omega^{-1} \u0026= \\begin{pmatrix} v_{11} \u0026 \\cdots \u0026 v_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ v_{n1} \u0026 \\cdots \u0026 v_{nn} \\end{pmatrix} \\\\ w_{i} \u0026= \\frac{1}{D}\\left(\\mu_{p} \\sum_{j=1}^{n} v_{ij}\\left(C \\mathrm{E}\\left(R_{j}\\right) - A\\right) + \\sum_{j=1}^{n} v_{ij}\\left(B - A \\mathrm{E}\\left(R_{j}\\right)\\right)\\right), \\quad i = 1, \\ldots, n \\end{aligned} $$$$ \\begin{aligned} A \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{j}\\right), \\quad B = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{i}\\right) \\mathrm{E}\\left(R_{j}\\right), \\quad C = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij}, \\quad D = BC - A^{2} \u003e 0 \\\\ \\sigma_{p}^{2} \u0026= \\frac{C \\mu_{p}^{2} - 2A \\mu_{p} + B}{D} \\end{aligned} $$Where:\n$\\mathrm{E}\\left(R_{p}\\right)$ and $\\mu_{p}$ are the expected return of portfolio $p$ $\\mathrm{E}\\left(R_{i}\\right)$ is the expected return of asset $i$ $\\sigma_{i}$ is the standard deviation of asset $i$ $\\sigma_{j}$ is the standard deviation of asset $j$ $w_{i}$ is the proportion of asset $i$ in the portfolio $\\sigma_{p}^{2}$ is the variance of portfolio $p$ $\\rho_{ij}$ is the correlation coefficient between asset $i$ and asset $j$ Using the above formulas$^{[12]}$, we can construct an investment portfolio that minimizes non-systematic risk under a certain expected rate of return.\n2.2.2 Capital Asset Pricing Model William Sharpe (1964)$^{[13]}$, John Lintner (1965)$^{[14]}$, and Jan Mossin (1966)$^{[15]}$ proposed the Capital Asset Pricing Model (CAPM). This model posits that the expected return of an asset is related to its risk measure, the $\\beta$ value. Through a simple linear relationship, this model links the expected return of an asset to market risk, making Markowitz (1952)$^{[11]}$’s portfolio selection theory more relevant to the real world, and laying a theoretical foundation for the establishment of multi-factor stock selection models.\nAccording to the Capital Asset Pricing Model, for a given asset $i$, the relationship between its expected return and the expected return of the market portfolio can be expressed as:\n$$ E\\left(r_{i}\\right) = r_{f} + \\beta_{im}\\left[E\\left(r_{m}\\right) - r_{f}\\right] $$Where:\n$E\\left(r_{i}\\right)$ is the expected return of asset $i$ $r_{f}$ is the risk-free rate $\\beta_{im}$ (Beta) is the systematic risk coefficient of asset $i$, $\\beta_{im} = \\frac{\\operatorname{Cov}\\left(r_{i}, r_{m}\\right)}{\\operatorname{Var}\\left(r_{m}\\right)}$ $E\\left(r_{m}\\right)$ is the expected return of the market portfolio $m$ $E\\left(r_{m}\\right) - r_{f}$ is the market risk premium 2.2.3 Arbitrage Pricing Theory and Multi-Factor Model Ross (1976)$^{[16]}$ proposed the Arbitrage Pricing Theory (APT). This theory argues that arbitrage behavior is the decisive factor in forming market equilibrium prices. By introducing a series of factors in the return formation process to construct linear correlations, it overcomes the limitations of the Capital Asset Pricing Model (CAPM) and provides important theoretical guidance for subsequent researchers.\nArbitrage Pricing Theory is considered the theoretical basis of the Multi-Factor Model (MFM), an important component of asset pricing models, and one of the cornerstones of asset pricing theory. The general form of the multi-factor model is:\n$$ r_{j} = a_{j} + \\lambda_{j1} f_{1} + \\lambda_{j2} f_{2} + \\cdots + \\lambda_{jn} f_{n} + \\delta_{j} $$Where:\n$r_{j}$ is the return of asset $j$ $a_{j}$ is a constant for asset $j$ $f_{n}$ is the systematic factor $\\lambda_{jn}$ is the factor loading $\\delta_{j}$ is the random error 2.2.4 Fama-French Three-Factor Model and Five-Factor Model Fama (1992) and French (1992)$^{[17]}$ used a combination of cross-sectional regression and time series methods and found that the $\\beta$ value of the stock market could not explain the differences in returns of different stocks, while market capitalization, book-to-market ratio, and price-to-earnings ratio of listed companies could significantly explain the differences in stock returns. They argued that excess returns are compensation for risk factors not reflected by $\\beta$ in CAPM, and thus proposed the Fama-French Three-Factor Model. The three factors are:\nMarket Risk Premium Factor (Market Risk Premium)\nRepresents the overall systematic risk of the market, i.e., the difference between the expected return of the market portfolio and the risk-free rate. Measures the excess return investors expect for bearing systematic risk (risk that cannot be eliminated through diversification). Calculated as: $$ \\text{Market Risk Premium} = E(R_m) - R_f $$ where $E(R_m)$ is the expected market return, and $R_f$ is the risk-free rate. Size Factor (SMB: Small Minus Big)\nRepresents the return difference between small-cap stocks and large-cap stocks. Small-cap stocks are generally riskier, but historical data shows that their expected returns tend to be higher than those of large-cap stocks. Calculated as: $$ SMB = R_{\\text{Small}} - R_{\\text{Big}} $$ reflecting the market’s compensation for the additional risk premium of small-cap stocks. Value Factor (HML: High Minus Low)\nReflects the return difference between high book-to-market ratio stocks (i.e., “value stocks”) and low book-to-market ratio stocks (i.e., “growth stocks”). Stocks with high book-to-market ratios are usually priced lower (undervalued by the market), but may achieve higher returns in the long run. Calculated as: $$ HML = R_{\\text{High}} - R_{\\text{Low}} $$ Stocks with low book-to-market ratios may be overvalued due to overly optimistic market expectations. This model concretizes the factors in the APT model and concludes that investing in small-cap, high-growth stocks has the characteristics of high risk and high return. The Fama-French Three-Factor Model is widely used in the analysis and practice of modern investment behavior.\nSubsequently, Fama (2015) and French (2015)$^{[18]}$ extended the three-factor model, adding the following two factors:\nProfitability Factor (RMW: Robust Minus Weak)\nReflects the return difference between highly profitable companies and less profitable companies. Companies with strong profitability (high ROE, net profit margin) are more likely to provide stable and higher returns. Calculated as: $$ RMW = R_{\\text{Robust}} - R_{\\text{Weak}} $$ Investment Factor (CMA: Conservative Minus Aggressive)\nReflects the return difference between conservative investment companies and aggressive investment companies. Aggressive companies (rapidly expanding, high capital expenditure) are usually accompanied by greater operational risks, while conservative companies (relatively stable capital expenditure) show higher stability and returns. Calculated as: $$ CMA = R_{\\text{Conservative}} - R_{\\text{Aggressive}} $$ The Fama-French Three-Factor Model formula is:\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\epsilon_i $$The Fama-French Five-Factor Model formula is:\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\beta_{i,RMW} \\cdot RMW + \\beta_{i,CMA} \\cdot CMA + \\epsilon_i $$Where:\n$R_i$: Expected return of stock $i$ $R_f$: Risk-free rate of return $R_m$: Expected return of the market portfolio $R_m - R_f$: Market risk premium factor $SMB$: Return of small-cap stocks minus large-cap stocks $HML$: Return of high book-to-market ratio stocks minus low book-to-market ratio stocks $RMW$: Return of high profitability stocks minus low profitability stocks $CMA$: Return of conservative investment stocks minus aggressive investment stocks $\\beta_{i,*}$: Sensitivity of stock $i$ to the corresponding factor $\\epsilon_i$: Regression residual 2.2.5 Model Comparison Table The following table summarizes the core content and factor sources of the Mean-Variance Model, Capital Asset Pricing Model (CAPM), Arbitrage Pricing Theory (APT), and Fama-French Models:\nModel Core Content Factor Source Mean-Variance Model Foundation of portfolio theory, optimizes portfolio through expected returns and covariance matrix. Expected returns and covariance matrix of assets in portfolio Capital Asset Pricing Model (CAPM) Explains asset returns through market risk factor ($\\beta$), laying the theoretical foundation for multi-factor models. Market factor $\\beta$ Arbitrage Pricing Theory (APT) Multi-factor framework, allows multiple economic variables to explain asset returns, e.g., inflation rate, interest rate. Multiple factors (macroeconomic variables, e.g., inflation rate, interest rate) Fama-French Three-Factor Model Adds size factor and book-to-market ratio factor, improving the explanatory power of asset returns. Market factor, SMB (size factor), HML (book-to-market ratio factor) Fama-French Five-Factor Model Adds profitability factor and investment factor on the basis of the three-factor model, further improving asset pricing model. Market factor, SMB, HML, RMW (profitability factor), CMA (investment factor) The following table summarizes the advantages and disadvantages of these models:\nModel Advantages Disadvantages Mean-Variance Model Provides a systematic portfolio optimization method, laying the foundation for modern investment theory. Only optimizes for return and variance, does not explicitly specify the source of risk compensation. Capital Asset Pricing Model (CAPM) Simple and easy to use, explains return differences through market risk, provides a theoretical basis for multi-factor models. Assumes a single factor (market risk) determines returns, ignores other systematic risk factors. Arbitrage Pricing Theory (APT) Allows multiple factors to explain asset returns, reduces reliance on single-factor assumptions, more flexible. Does not specify concrete factors, lower operability, only provides a framework. Fama-French Three-Factor Model Significantly improves the explanatory power of asset returns by adding size factor and book-to-market ratio factor. Ignores other factors such as profitability and investment behavior. Fama-French Five-Factor Model More comprehensively captures key variables affecting asset returns by adding profitability factor and investment factor on the basis of the three-factor model. Higher model complexity, high data requirements, may still miss some potential factors. Chapter 3 Comparative Study of LSTM, GRU, and BiLSTM in Stock Price Prediction 3.1 Introduction to Experimental Data Many scholars, both domestically and internationally, focus their research on their own country’s stock indices, with relatively less research on individual stocks in different markets. Furthermore, few studies compare LSTM, GRU, and BiLSTM models directly. Therefore, this paper selects Shanghai Pudong Development Bank (SPD Bank, code 600000) in the domestic A-share market and International Business Machines Corporation (IBM) in the US stock market for research. This approach allows for a more accurate comparison of the three models used. For SPD Bank, stock data from January 1, 2008, to December 31, 2020, is used, totaling 3114 valid data points, sourced from the Tushare financial big data platform. We select six features from this dataset for the experiment: date, open price, close price, high price, low price, and volume. For the SPD Bank dataset, all five features except date (used as a time series index) are used as independent variables. For IBM, stock data from January 2, 1990, to November 15, 2018, is used, totaling 7278 valid data points, sourced from Yahoo Finance. We select seven features from this dataset for the experiment: date, open price, high price, low price, close price, adjusted close price (Adj Close), and volume. For the IBM dataset, all six features except date (used as a time series index) are used as independent variables. In this experiment, the closing price (close) is chosen as the variable to be predicted. Tables 3.1.1 and 3.1.2 show partial data from the two datasets.\n3.1.1 Partial Display of SPD Bank Dataset date open close high low volume code 2008-01-02 9.007 9.101 9.356 8.805 131583.90 600000 2008-01-03 9.007 8.645 9.101 8.426 211346.56 600000 2008-01-04 8659 9.009 9.111 8.501 139249.67 600000 2008-01-07 8.970 9.515 9.593 8.953 228043.01 600000 2008-01-08 9.551 9.583 9.719 9.517 161255.31 600000 2008-01-09 9.583 9.663 9.772 9.432 102510.92 600000 2008-01-10 9.701 9.680 9.836 9.602 217966.25 600000 2008-01-11 9.670 10.467 10.532 9.670 231544.21 600000 2008-01-14 10.367 10.059 10.433 10.027 142918.39 600000 2008-01-15 10.142 10.051 10.389 10.006 161221.52 600000 Data Source: Tushare\n3.1.2 Partial Display of IBM Dataset Date Open High Low Close Adj Close Volume 1990-01-02 23.6875 24.5313 23.6250 24.5000 6.590755 7041600 1990-01-03 24.6875 24.8750 24.5938 24.7188 6.649599 9464000 1990-01-04 24.7500 25.0938 24.7188 25.0000 6.725261 9674800 1990-01-05 24.9688 25.4063 24.8750 24.9375 6.708448 7570000 1990-01-08 24.8125 25.2188 24.8125 25.0938 6.750481 4625200 1990-01-09 25.1250 25.3125 24.8438 24.8438 6.683229 7048000 1990-01-10 24.8750 25.0000 24.6563 24.7500 6.658009 5945600 1990-01-11 24.8750 25.0938 24.8438 24.9688 6.716855 5905600 1990-01-12 24.6563 24.8125 24.4063 24.4688 6.582347 5390800 1990-01-15 24.4063 24.5938 24.3125 24.5313 6.599163 4035600 Data Source: Yahoo Finance\n3.2 Experimental Data Preprocessing 3.2.1 Data Normalization In the experiment, there are differences in units and magnitudes among various features. For example, the magnitude difference between stock prices and trading volume is huge, which will affect the final prediction results of our experiment. Therefore, we use the MinMaxScaler method from the sklearn.preprocessing library to scale the features of the data to between 0 and 1. This can not only improve the model accuracy but also increase the model convergence speed. The normalization formula is:\n$$ x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)} $$where $x^{\\prime}$ is the normalized data, $x$ is the original data, $\\min (x)$ is the minimum value of the original dataset, and $\\max (x)$ is the maximum value of the original dataset. After obtaining the prediction results in our experimental process, we also need to denormalize the data before we can perform stock price prediction and model evaluation.\n3.2.2 Data Partitioning Here, the entire experimental datasets of SPD Bank and IBM are input respectively, and the timestep of the recurrent kernel is set to 60 for both, with the number of input features per timestep being 5 and 6, respectively. This allows inputting data from the previous 60 trading days to predict the closing price on the 61st day. This makes our dataset meet the input requirements of the three neural network models to be compared later, which are the number of samples, the number of recurrent kernel unfolding steps, and the number of input features per timestep. Then, we divide the normalized SPD Bank dataset into training, validation, and test sets in a ratio of 2488:311:255. The normalized IBM dataset is divided into training, validation, and test sets in a ratio of 6550:364:304. We partition out a validation set here to facilitate adjusting the hyperparameters of the models to optimize each model before comparison.\n3.3 Model Network Structure The network structures of each model set in this paper through a large number of repeated experiments are shown in the table below. The default tanh and linear activation functions of recurrent neural networks are used between layers, and Dropout is added to prevent overfitting. The dropout rate is set to 0.2. The number of neurons in each recurrent layer of LSTM and GRU is 50, and the number of neurons in the recurrent layer of BiLSTM is 100. Each model of LSTM, GRU, and BiLSTM adopts four layers of LSTM, GRU, BiLSTM, and one fully connected layer, with a Dropout set between each network layer.\n3.3.1 LSTM Network Structure for IBM Layer(type) Output Shape Param# lstm_1 (LSTM) (None, 60, 50) 11400 dropout_1 (Dropout) (None, 60, 50) 0 lstm_2 (LSTM) (None, 60, 50) 20200 dropout_2 (Dropout) (None, 60, 50) 0 lstm_3 (LSTM) (None, 60, 50) 20200 dropout_3 (Dropout) (None, 60, 50) 0 lstm_4 (LSTM) (None, 50) 20200 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params: 72,051\nTrainable params: 72,051\nNon-trainable params: 0\n3.3.2 GRU Network Structure for IBM Layer(type) Output Shape Param# gru_1 (GRU) (None, 60, 50) 8550 dropout_1 (Dropout) (None, 60, 50) 0 gru_2 (GRU) (None, 60, 50) 15150 dropout_2 (Dropout) (None, 60, 50) 0 gru_3 (GRU) (None, 60, 50) 15150 dropout_3 (Dropout) (None, 60, 50) 0 gru_4 (GRU) (None, 50) 15150 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params: 54,051\nTrainable params: 54,051\nNon-trainable params: 0\n3.3.3 BiLSTM Network Structure for IBM Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 60, 100) 22800 dropout_1 (Dropout) (None, 60, 100) 0 bidirectional_2 (Bidirection) (None, 60, 100) 60400 dropout_2 (Dropout) (None, 60, 100) 0 bidirectional_3 (Bidirection) (None, 60, 100) 60400 dropout_3 (Dropout) (None, 60, 100) 0 bidirectional_4 (Bidirection) (None, 100) 60400 dropout_4 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 1) 101 Total params: 204,101\nTrainable params: 204,101\nNon-trainable params: 0\n3.4 Model Compilation and Hyperparameter Settings In this paper, after continuous hyperparameter tuning with the goal of minimizing the loss function on the validation set, the following hyperparameters are selected for the three models of SPD Bank: epochs=100, batch_size=32; and for the three models of IBM: epochs=50, batch_size=32. The optimizer used is Adaptive Moment Estimation (Adam)$^{[19]}$. The default values in its keras package are used, i.e., lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, and decay=0.0. The loss function is Mean Squared Error (MSE).\nParameter Explanation:\nlr: Learning rate beta_1: Exponential decay rate for the first moment estimate beta_2: Exponential decay rate for the second moment estimate epsilon: Fuzz factor decay: Learning rate decay value after each update 3.5 Experimental Results and Analysis First, let’s briefly introduce the evaluation metrics used for the models. The calculation formulas are as follows:\nMean Squared Error (MSE): $$ M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2} $$ Root Mean Squared Error (RMSE): $$ R M S E=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}} $$ Mean Absolute Error (MAE): $$ M A E=\\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-\\hat{Y}_{i}\\right| $$ \\( R^2 \\) (R Squared): $$ \\begin{gathered} \\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} \\\\ R^{2}=1-\\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\end{gathered} $$Where: $n$ is the number of samples, $Y_{i}$ is the actual closing price of the stock, $\\hat{Y}_{i}$ is the predicted closing price of the stock, and $\\bar{Y}$ is the average closing price of the stock. The smaller the MSE, RMSE, and MAE, the more accurate the model. The larger the \\( R^2 \\), the better the goodness of fit of the model coefficients.\n3.5.1 Experimental Results for SPD Bank LSTM GRU BiLSTM MSE 0.059781 0.069323 0.056454 RMSE 0.244501 0.263292 0.237601 MAE 0.186541 0.202665 0.154289 R-squared 0.91788 0.896214 0.929643 Comparing the evaluation metrics of the three models, we can find that on the SPD Bank test set, the MSE, RMSE, and MAE of the BiLSTM model are smaller than those of the LSTM and GRU models, while the R-Squared is larger than those of the LSTM and GRU models. By comparing RMSE, we find that BiLSTM has a 2.90% performance improvement over LSTM and a 10.81% performance improvement over GRU on the validation set.\n3.5.2 Experimental Results for IBM LSTM GRU BiLSTM MSE 18.01311 12.938584 11.057501 RMSE 4.244186 3.597024 3.325282 MAE 3.793223 3.069033 2.732075 R-squared 0.789453 0.851939 0.883334 Comparing the evaluation metrics of the three models, we can find that on the IBM test set, the MSE, RMSE, and MAE of the BiLSTM model are smaller than those of the LSTM and GRU models, while the R-Squared is larger than those of the LSTM and GRU models. By comparing RMSE, we find that BiLSTM has a 27.63% performance improvement over LSTM and an 8.17% performance improvement over GRU on the validation set.\n3.6 Chapter Summary This chapter first introduced the SPD Bank and IBM datasets and the features used in the experiment. Then, it performed preprocessing steps of data normalization and data partitioning on the datasets. It also detailed the network structures and hyperparameters of the LSTM, GRU, and BiLSTM models used in the experiment. Finally, it obtained the loss function images and a series of fitting graphs for each model. By comparing multiple evaluation metrics and fitting images of the models, it is concluded that the BiLSTM model can better predict stock prices, laying a foundation for our next chapter’s research on the LightGBM-BiLSTM quantitative investment strategy.\nChapter 4 Research on Quantitative Investment Model Based on LightGBM-BiLSTM 4.1 Experimental Steps Fig. 11. LightGBM-BiLSTM Diagram.\nAs shown in Fig. 11, this experiment first selects 50 factors from the factor library. Then, it performs factor cleaning steps of outlier removal, standardization, and missing value imputation on the factors sequentially. Next, the LightGBM model is used for factor selection, and the top ten factors with the highest importance are selected as the factors for this cross-sectional selection. Subsequently, a BiLSTM model is used to establish a multi-factor model, and finally, backtesting analysis is performed.\n4.2 Experimental Data The market data used in this paper comes from Tushare. The specific features of the dataset are shown in the table below.\n4.2.1 Features Included in the Stock Dataset Name Type Description ts_code str Stock code trade_date str Trading date open float Open price high float High price low float Low price close float Close price pre_close float Previous close price change float Change amount pct_chg float Change percentage (unadjusted) vol float Volume (in hands) amount float Turnover (in thousands of CNY) The A-share market-wide daily dataset contains 5,872,309 rows of data, i.e., 5,872,309 samples. As shown in Table 4.2.1, the A-share market-wide daily dataset has the following 11 features, in order: stock code (ts_code), trading date (trade_date), open price (open), high price (high), low price (low), close price (close), previous close price (pre_close), change amount (change), turnover rate (turnover_rate), turnover amount (amount), total market value (total_mv), and adjustment factor (adj_factor).\n4.2.2 Partial Display of A-Share Market-Wide Daily Dataset ts_code trade_date open high low close pre_close change vol amount 600613.SH 20120104 8.20 8.20 7.84 7.86 8.16 -0.30 4762.98 3854.1000 600690.SH 20120104 9.00 9.17 8.78 8.78 8.93 -0.15 142288.41 127992.6050 300277.SZ 20120104 22.90 22.98 20.81 20.88 22.68 -1.80 12212.39 26797.1370 002403.SZ 20120104 8.87 8.90 8.40 8.40 8.84 -0.441 10331.97 9013.4317 300179.SZ 20120104 19.99 20.32 19.20 19.50 19.96 -0.46 1532.31 3008.0594 600000.SH 20120104 8.54 8.56 8.39 8.41 8.49 -0.08 342013.79 290229.5510 300282.SZ 20120104 22.90 23.33 21.02 21.02 23.35 -2.33 38408.60 86216.2356 002319.SZ 20120104 9.74 9.95 9.38 9.41 9.73 -0.32 4809.74 4671.4803 601991.SH 20120104 5.17 5.39 5.12 5.25 5.16 0.09 145268.38 76547.7490 000780.SZ 20120104 10.42 10.49 10.00 10.00 10.30 -0.30 20362.30 20830.1761 [5872309 rows x 11 columns]\nThe CSI All Share daily dataset contains 5,057 rows of data, i.e., 5,057 samples. As shown in Table 4.2.2, the CSI All Share daily dataset has the following 7 features, in order: trading date (trade_date), open price (open), high price (high), low price (low), close price (close), volume (volume), and previous close price (pre_close).\n4.2.3 Partial Display of CSI All Share Daily Dataset trade_date open high low close volume pre_close 2006-11-24 1564.3560 1579.3470 1549.9790 1576.1530 7.521819e+09 1567.0910 2006-11-27 1574.1130 1598.7440 1574.1130 1598.7440 7.212786e+09 1581.1530 2006-11-28 1597.7200 1604.7190 1585.3620 1596.8400 7.025637e+09 1598.7440 2006-11-29 1575.3030 1620.2870 1575.3030 1617.9880 7.250354e+09 1596.8400 2006-11-30 1621.4280 1657.3230 1621.4280 1657.3230 9.656888e+09 1617.9880 … … … … … … … 2020-11-11 5477.8870 5493.5867 5422.9110 5425.8017 5.604086e+10 5494.1042 2020-11-12 5439.2296 5454.3452 5413.9659 5435.1379 4.594251e+10 5425.8017 2020-11-13 5418.2953 5418.3523 5364.2031 5402.7702 4.688916e+10 5435.1379 2020-11-16 5422.3565 5456.7264 5391.9232 5456.7264 5.593672e+10 5402.7702 2020-11-17 5454.0696 5454.0696 5395.6052 5428.0765 5.857009e+10 5456.7264 [5057 rows x 7 columns]\nTable 4.2.4 below shows partial data of the original factors. After sequentially going through the four factor cleaning steps of missing value imputation, outlier removal, factor standardization, and factor neutralization mentioned above, partial data of the cleaned factors are obtained as shown in Table 4.2.5.\n4.2.4 Original Factor Data trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 … 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 … 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 … 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 … 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 … 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 … 4.2.5 Cleaned Factor Data sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 … 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 … 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 … 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 … 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 … 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 … 4.2.6 Factor Data Construction of Price-Volume Factors This paper uses the following method to construct price-volume factors. There are two basic elements for constructing price-volume factors: first, basic fields, and second, operators. As shown in Table 4.2.1, basic fields include daily frequency high price, low price, open price, close price, previous day’s close price, volume, change percentage, turnover rate, turnover amount, total market value, and adjustment factor.\n4.2.7 Basic Field Table No. Field Name Meaning high High Price Highest price in intraday transactions low Low Price Lowest price in intraday transactions open Open Price Price at which the call auction concludes close Close Price Price of the last transaction of the day pre_close Previous Close Price Price of the last transaction of the previous day vol Volume Number of shares traded throughout the day pct_chg Change Percentage Percentage change of the security for the day turnover_rate Turnover Rate Turnover rate of the security for the day amount Turnover Amount Total value of transactions for the day total_mv Total Market Value Total value of the stock, calculated by total shares outstanding multiplied by the current stock price adj_factor Adjustment Factor Ratio for adjusting for dividends and splits This paper obtains the operator list shown in the table below through the basic operator set provided by gplearn and some self-defined special operators.\n4.2.8 Operator List Operator Name Definition add(x, y) Sum \\( x + y\\); element-wise operation \\(\\operatorname{div}(x, y)\\) Division \\( x / y\\); element-wise operation \\(\\operatorname{mul}(x, y)\\) Multiplication \\( x \\cdot y\\); element-wise operation \\(\\operatorname{sub}(x, y)\\) Subtraction \\( x - y\\); element-wise operation neg(x) Negative \\(-x\\); element-wise operation \\(\\log(x)\\) Logarithm \\(\\log(x)\\); element-wise operation max(x, y) Maximum Larger value between \\(x\\) and \\(y\\); element-wise operation \\(\\min(x, y)\\) Minimum Smaller value between \\(x\\) and \\(y\\); element-wise operation delta_d(x) d-day Difference Current day’s \\(x\\) value minus \\(x\\) value \\(d\\) days ago; time series operation delay_d(x) d-day Delay \\(x\\) value \\(d\\) days ago; time series operation Corr_d(x, y) d-day Correlation Correlation between \\(x\\) values and \\(y\\) values over \\(d\\) days; time series operation Max_d(x) d-day Maximum Maximum value of \\(x\\) over \\(d\\) days; time series operation Min_d(x) d-day Minimum Minimum value of \\(x\\) over \\(d\\) days; time series operation sort_d(x) d-day Rank Rank of \\(x\\) values over \\(d\\) days; time series operation Argsortmin_d(x) d-day Minimum Position Position of the minimum value of \\(x\\) over \\(d\\) days; time series operation Argsortmax_d(x) d-day Maximum Position Position of the maximum value of \\(x\\) over \\(d\\) days; time series operation \\(\\operatorname{inv}(x)\\) Inverse \\( 1 / x\\); element-wise operation Std_d(x) d-day Standard Deviation Standard deviation of \\(x\\) values over \\(d\\) days; time series operation abs(x) Absolute Value \\(\\lvert x\\rvert\\); element-wise operation 4.2.9 Genetic Programming The core idea of Genetic Programming (GP) is to use evolutionary algorithms to automatically “evolve” factor expressions with strong predictive power in the vast search space composed of operators and basic fields. For factor mining in this paper, the main goal of GP is to search and find those factors that can better predict future stock returns from all possible expressions that can be combined from the basic fields in Table 4.2.7 and the operators in Table 4.2.8. The core process of GP can be divided into the following steps:\nInitialization Define Operator Set and Basic Fields\nOperator set (operators) as shown in Table 4.2.8, including operations such as addition, subtraction, multiplication, division, logarithm, absolute value, delay, moving maximum/minimum, moving correlation coefficient, etc. Basic fields (terminals) as shown in Table 4.2.7, including open price, close price, high price, low price, volume, adjustment factor, etc. These operators and basic fields can be regarded as “nodes” in the factor expression tree, where basic fields are leaf nodes (terminal nodes), and operators are internal nodes. Randomly Generate Initial Population\nIn the initialization phase, based on the given operator set and field set, a series of factor expressions (which can be represented as several syntax trees or expression trees) are randomly “spliced” to form an initial population. For example, it may randomly generate \\[ \\text{Factor 1}: \\mathrm{Max\\_5}\\bigl(\\mathrm{add}(\\mathrm{vol}, \\mathrm{close})\\bigr), \\quad \\text{Factor 2}: \\mathrm{sub}\\bigl(\\mathrm{adj\\_factor}, \\mathrm{neg}(\\mathrm{turnover\\_rate})\\bigr), \\dots \\] Each factor expression will correspond to an individual. Fitness Function Measure Factor’s Predictive Ability\nFor each expression (individual), we need to evaluate its predictive ability for future returns or other objectives. Specifically, we can calculate the correlation coefficient (IC) or a more comprehensive indicator IR (Information Ratio) between the next period’s stock return \\( r^{T+1} \\) and the current period’s factor exposure \\( x_k^T \\) to measure it. Set Objective\nIf we want the factor to have a higher correlation (IC), we can set the fitness function to \\(\\lvert \\rho(x_k^T, r^{T+1})\\rvert\\); If we want the factor to have a higher IR, we can set the fitness function to the IR value. The higher the factor IC or IR, the higher the “fitness” of the expression. Therefore, we usually set: \\[ \\text{Fitness} \\bigl(F(x)\\bigr) \\;=\\; \\begin{cases} \\lvert \\rho(x_k^T, r^{T+1})\\rvert \\quad \u0026\\text{(Maximize IC)},\\\\[6pt] \\mathrm{IR}(x_k^T) \\quad \u0026\\text{(Maximize IR)}. \\end{cases} \\] where \\(\\rho(\\cdot)\\) represents the correlation coefficient, and \\(\\mathrm{IR}(\\cdot)\\) is the IR indicator.\nSelection, Crossover, and Mutation Selection\nBased on the results of the fitness function, expressions with high factor fitness are “retained” or “bred”, while expressions with lower fitness are eliminated. This is similar to “survival of the fittest” in biological evolution. Crossover\nRandomly select a part of the “nodes” of several expressions with higher fitness (parents) for exchange, so as to obtain new expressions (offspring). In the expression tree structure, subtree A and subtree B can be interchanged to generate new offspring expressions. For example, if a subtree of expression tree \\(\\mathrm{FactorA}\\) is exchanged with the corresponding subtree of expression tree \\(\\mathrm{FactorB}\\), two new expressions are generated. Mutation\nRandomly change some nodes of the expression with a certain probability, such as: Replacing the operator of the node (for example, changing \\(\\mathrm{add}\\) to \\(\\mathrm{sub}\\)), Replacing the basic field of the terminal node (for example, changing \\(\\mathrm{vol}\\) to \\(\\mathrm{close}\\)), Or randomly changing operation parameters (such as moving window length, smoothing factor, etc.). Mutation can increase the diversity of the population and avoid premature convergence or falling into local optima. Iterative Evolution Iterative Execution\nRepeatedly execute selection, crossover, and mutation operations for multiple generations. Each generation produces a new population of factor expressions and evaluates their fitness. Convergence and Termination\nWhen evolution reaches a predetermined stopping condition (such as the number of iterations, fitness threshold, etc.), the algorithm terminates. Usually, we will select several factor expressions with higher final fitness and regard them as the evolution results. Mathematical Representation: Searching for Optimal Factor Expressions Abstracting the above process into the following formula, the factor search objective can be simply expressed as:\n\\[ F(x) \\;=\\; \\mathrm{GP}\\bigl(\\{\\text{operators}\\}, \\{\\text{terminals}\\}\\bigr), \\] indicating that a function \\(F(x)\\) is searched through the GP algorithm on a given operator set (operators) and basic field set (terminals). From the perspective of optimization, we hope to find:\n\\[ \\max_{F} \\bigl\\lvert \\rho(F^T, r^{T+1}) \\bigr\\rvert \\quad \\text{or} \\quad \\max_{F} \\; \\mathrm{IR}\\bigl(F\\bigr), \\] where\n\\(\\rho(\\cdot)\\) represents the correlation coefficient (IC) between the factor and the next period’s return, \\(\\mathrm{IR}(\\cdot)\\) represents the IR indicator of the factor. In practical applications, we will give a backtesting period, score the candidate factors of each generation (IC/IR evaluation), and continuously “evolve” better factors through the iterative process of selection, crossover, and mutation.\nThrough the above steps, we can finally automatically mine a batch of factor expressions that have strong predictive power for future returns and good robustness (such as higher IR) in the vast search space of operator combinations and basic field combinations.\n4.2.10 Partially Mined Factors Factor Name Definition 0 Max＿25(add(turnover_rate, vol)) 1 Max＿30(vol) 2 Max＿25(turnover_rate) 3 Max＿35(add(vol, close)) 4 Max＿30(turnover_rate) 5 sub(Min＿20(neg(pre_close)), div(vol, adj_factor)) 6 Max＿60(max(vol, adj_factor)) 7 Max＿50(amount) 8 div(vol, neg(close)) 9 min(ArgSortMin＿25(pre_close), neg(vol)) 10 neg(max(vol, turnover_rate)) 11 mul(amount, neg(turnover_rate)) 12 inv(add(ArgSortMax＿40(change), inv(pct_chg))) 13 Std＿40(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))) 14 div(log(total_mv),amount) 15 div(neg(Max＿5(amount)), Min＿20(ArgSort＿60(high))) 16 Corr＿30(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))), add(log(Max＿10(pre_close)), high)) 17 ArgSort＿60(neg(turnover_rate)) … … These factors are all obtained by combining from the operator list (Table 4.2.8) and the basic field list (Table 4.2.7) through genetic programming and have different mathematical expressions.\nFactor Validity Test After we get the mined factors, we need to test the validity of the factors. Common test indicators are Information Coefficient (IC) and Information Ratio (IR).\nInformation Coefficient (IC) describes the linear correlation between the next period’s return rate of the selected stocks and the current period’s factor exposure, which can reflect the robustness of the factor in predicting returns. Information Ratio (IR) is the ratio of the mean of excess returns to the standard deviation of excess returns. The information ratio is similar to the Sharpe ratio. The main difference is that the Sharpe ratio uses the risk-free return as a benchmark, while the information ratio uses a risk index as a benchmark. The Sharpe ratio helps to determine the absolute return of a portfolio, and the information ratio helps to determine the relative return of a portfolio. After we calculate the IC, we can calculate the IR based on the IC value. When the IR is greater than 0.5, the factor has a strong ability to stably obtain excess returns. In actual calculation, the \\( \\mathrm{IC} \\) value of factor \\(k\\) generally refers to the correlation coefficient between the exposure \\( x_k^T \\) of factor \\(k\\) in period \\(T\\) of the selected stocks and the return rate \\( r^{T+1} \\) of the selected stocks in period \\(T+1\\); the \\( \\mathrm{IR} \\) value of factor \\(k\\) is the mean of the \\( \\mathrm{IC} \\) of factor \\(k\\) divided by the standard deviation of the \\( \\mathrm{IC} \\) of factor \\(k\\). The calculation formulas are as follows:\n$$ \\begin{gathered} I C=\\rho_{x_{k}^{T}, r^{T+1}}=\\frac{\\operatorname{cov}\\left(x_{k}^{T}, r^{T+1}\\right)}{\\sigma_{x_{k}^{T}} \\sigma_{r^{T+1}}}=\\frac{\\mathrm{E}\\left(x_{k}^{T} * r^{T+1}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right) \\mathrm{E}\\left(r^{T+1}\\right)}{\\sqrt{\\mathrm{E}\\left(\\left(x_{k}^{T}\\right)^{2}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right)^{2}} \\cdot \\sqrt{\\mathrm{E}\\left(\\left(r^{T+1}\\right)^{2}\\right)-\\mathrm{E}\\left(r^{T+1}\\right)^{2}}} \\\\ I R=\\frac{\\overline{I C}}{\\sigma_{I C}} \\end{gathered} $$Where:\n$x_{k}^{T}$: the exposure of the selected stock to factor $k$ in period $T$ $r^{T+1}$: the return of the selected stock in period $T+1$ $\\overline{IC}$: the mean of the Information Coefficient (IC) This paper uses IR to judge the quality of factors. Through “screening” a large number of different combinations of operators and basic data and IC and IR, this paper obtains the 50 price-volume factors selected in this paper. After IR testing, the table shown in the figure below is obtained by sorting IR from high to low. From the table below, we can see that the IRs of the selected 50 price-volume factors are all greater than 0.5, indicating that these factors have a strong ability to stably obtain excess returns.\n4.2.11 Factor IR Test Table Factor Name IR Factor Name IR 0 3.11 25 2.73 1 2.95 26 2.71 2 2.95 27 2.70 3 2.95 28 2.69 4 2.95 29 2.69 5 2.94 30 2.69 6 2.94 31 2.68 7 2.94 32 2.68 8 2.93 33 2.68 9 2.93 34 2.68 10 2.93 35 2.67 11 2.92 36 2.67 12 2.91 37 2.66 13 2.89 38 2.65 14 2.86 39 2.65 15 2.83 40 2.65 16 2.83 41 2.65 17 2.83 42 2.64 18 2.79 43 2.63 19 2.78 44 2.63 20 2.78 45 2.62 21 2.76 46 2.62 22 2.75 47 2.62 It can be seen from this table that among the screened factors, the IRs of all factors are greater than 0.5, which has a strong and stable ability to obtain excess returns.\n4.3 Factor Cleaning 4.3.1 Factor Missing Value Handling and Outlier Removal Methods for handling missing values of factors include case deletion, mean imputation, regression imputation, and other methods. This paper adopts a relatively simple mean imputation method to handle missing values, that is, using the average value of the factor to replace the missing data. Methods for factor outlier removal include median outlier removal, percentile outlier removal, and $3 \\sigma$ outlier removal. This paper uses the $3 \\sigma$ outlier removal method. This method uses the $3 \\sigma$ principle in statistics to convert outlier factors that are more than three standard deviations away from the mean of the factor to a position that is just three standard deviations away from the mean. The specific calculation formula is as follows:\n$$ X_i^{\\prime}= \\begin{cases} \\bar{X}+3 \\sigma \u0026 \\text{if } X_i \u003e \\bar{X} + 3 \\sigma \\\\ \\bar{X}-3 \\sigma \u0026 \\text{if } X_i \u003c \\bar{X} - 3 \\sigma \\\\ X_i \u0026 \\text{if } \\bar{X} - 3 \\sigma \u003c X_i \u003c \\bar{X} + 3 \\sigma \\end{cases} $$Where:\n$X_{i}$: Value of the factor before processing $\\bar{X}$: Mean of the factor sequence $\\sigma$: Standard deviation of the factor sequence $X_{i}^{\\prime}$: Value of the factor after outlier removal 4.3.2 Factor Standardization In this experiment, multiple factors are selected, and the dimensions of each factor are not completely the same. For the convenience of comparison and regression, we also need to standardize the factors. Currently, common specific standardization methods include Min-Max standardization, Z-score standardization, and Decimal scaling standardization. This paper chooses the Z-score standardization method. The data is standardized through the mean and standard deviation of the original data. The processed data conforms to the standard normal distribution, that is, the mean is 0 and the standard deviation is 1. The standardized numerical value is positive or negative, and a standard normal distribution curve is obtained.\nThe Z-score standardization formula used in this paper is as follows:\n$$ \\tilde{x}=\\frac{x_{i}-u}{\\sigma} $$Where:\n$x_{i}$: Original value of the factor $u$: Mean of the factor sequence $\\sigma$: Standard deviation of the factor sequence $\\tilde{x}$: Standardized factor value 4.3.3 Factor Neutralization Factor neutralization is to eliminate the influence of other factors on our selected factors, so that the stocks selected by our quantitative investment strategy portfolio are more dispersed, rather than concentrated in specific industries or market capitalization stocks. It can better share the risk of the investment portfolio and solve the problem of factor multicollinearity. Market capitalization and industry are the two main independent variables that affect stock returns. Therefore, in the process of factor cleaning, the influence of market capitalization and industry must also be considered. In this empirical study, we adopt the method of only including industry factors and including market factors in industry factors. The single-factor regression model for factors is shown in formula (31). We take the residual term of the following regression model as the new factor value after factor neutralization.\n$$ \\tilde{r}_{j}^{t}=\\sum_{s=1}^{s} X_{j s}^{t} \\tilde{f}_{s}^{t}+X_{j k}^{t} \\tilde{f}_{k}^{t}+\\tilde{u}_{j}^{t} $$Where:\n$\\tilde{r}_{j}^{t}$: Return rate of stock $j$ in period $t$ $X_{j s}^{t}$: Exposure of stock $j$ in industry $s$ in period $t$ $\\tilde{f}_{s}^{t}$: Return rate of the industry in period $t$ $X_{j k}^{t}$: Exposure of stock $j$ on factor $k$ in period $t$ $\\tilde{f}_{k}^{t}$: Return rate of factor $k$ in period $t$ $\\tilde{u}_j^t$: A $0-1$ dummy variable, that is, if stock $j$ belongs to industry $s$, the exposure is 1, otherwise it is 0 In this paper, the industry to which a company belongs is not proportionally split, that is, stock $j$ can only belong to a specific industry $s$, the exposure in industry $s$ is 1, and the exposure in all other industries is 0. This paper uses the Shenwan Hongyuan industry classification standard. The specific classifications are sequentially: agriculture, forestry, animal husbandry and fishery, mining, chemical industry, steel, nonferrous metals, electronic components, household appliances, food and beverage, textile and apparel, light industry manufacturing, pharmaceutical and biological, public utilities, transportation, real estate, commercial trade, catering and tourism, comprehensive, building materials, building decoration, electrical equipment, national defense and military industry, computer, media, communication, banking, non-banking finance, automobile, and mechanical equipment, a total of 28 categories. The table below shows the historical market chart of Shenwan Index Level 1 industries on February 5, 2021.\n4.3.3.1 Historical Market Chart of Shenwan Index Level 1 Industries on February 5, 2021 Index Code Index Name Release Date Open Index High Index Low Index Close Index Volume (100 Million Hands) Turnover (100 Million CNY) Change (%) 801010 Agriculture, Forestry, Animal Husbandry and Fishery 2021/2/5 0:00 4111.43 4271.09 4072.53 4081.81 15.81 307.82 -0.3 801020 Mining 2021/2/5 0:00 2344.62 2357.33 2288.97 2289.41 18.06 115.6 -2.25 801030 Chemical Industry 2021/2/5 0:00 4087.77 4097.59 3910.67 3910.67 55.78 778.85 -3.95 801040 Steel 2021/2/5 0:00 2253.78 2268.17 2243.48 2250.81 11.61 48.39 -1.02 801050 Nonferrous Metals 2021/2/5 0:00 4212.1 4250.59 4035.99 4036.74 45.41 593.92 -4.43 801080 Electronic Components 2021/2/5 0:00 4694.8 4694.8 4561.95 4561.95 52.67 850.79 -2.78 801110 Household Appliances 2021/2/5 0:00 10033.82 10171.26 9968.93 10096.83 8.55 149.18 0.83 801120 Food and Beverage 2021/2/5 0:00 30876.33 31545.02 30649.57 30931.69 11.32 657.11 0.47 801130 Textile and Apparel 2021/2/5 0:00 1614.48 1633.89 1604.68 1607.63 6.28 57.47 -0.39 801140 Light Industry Manufacturing 2021/2/5 0:00 2782.07 2791.88 2735.48 2737.24 15.28 176.16 -1.35 … … … … … … … … … … Data Source: Shenwan Hongyuan\nThe table below is partial data of the original factors. After sequentially going through the four factor cleaning steps of missing value imputation, factor outlier removal, factor standardization, and factor neutralization mentioned above, partial data of the cleaned factors are obtained as shown in the table.\n4.3.3.2 Original Factor Data trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 … 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 … 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 … 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 … 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 … 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 … 4.3.3.3 Cleaned Factor Data sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 … 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 … 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 … 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 … 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 … 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 … 4.4 Factor Selection Based on LightGBM 4.4.1 GBDT Gradient Boosting Decision Tree (GBDT), proposed by Friedman (2001)$^{[20]}$, is an iterative regression decision tree. Its main idea is to optimize the model by gradually adding weak classifiers (usually decision trees), so that the overall model can minimize the loss function. The GBDT model can be expressed as:\n$$ \\hat{y} = \\sum_{m=1}^{M} \\gamma_m h_m(\\mathbf{x}) $$Where:\n\\( M \\) is the number of iterations, \\( \\gamma_m \\) is the weight of the \\( m \\)-th weak classifier, \\( h_m(\\mathbf{x}) \\) is the \\( m \\)-th decision tree model. The training process of GBDT minimizes the loss function by gradually fitting the negative gradient direction. The specific update formula is:\n$$ \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{N} L\\left(y_i, \\hat{y}_{i}^{(m-1)} + \\gamma h_m(\\mathbf{x}_i)\\right) $$Where, \\( L \\) is the loss function, \\( y_i \\) is the true value, and \\( \\hat{y}_{i}^{(m-1)} \\) is the predicted value after the \\( (m-1) \\)-th iteration.\n4.4.2 LightGBM Light Gradient Boosting Machine (LightGBM)$^{[21]}$ is an efficient framework for implementing the GBDT algorithm, initially developed by Microsoft as a free and open-source distributed gradient boosting framework. LightGBM is based on decision tree algorithms and is widely used in ranking, classification, and other machine learning tasks. Its development focuses on performance and scalability. Its main advantages include high-efficiency parallel training, faster training speed, lower memory consumption, better accuracy, and support for distributed computing and fast processing of massive data$^{[22]}$.\nThe core algorithm of LightGBM is based on the following optimization objective:\n$$ L = \\sum_{i=1}^{N} l(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(h_m) $$Where, \\( l \\) is the loss function, and \\( \\Omega \\) is the regularization term, used to control model complexity, usually expressed as:\n$$ \\Omega(h_m) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 $$Here, \\( T \\) is the number of leaves in the tree, \\( w_j \\) is the weight of the \\( j \\)-th leaf, and \\( \\gamma \\) and \\( \\lambda \\) are regularization parameters.\nLightGBM uses technologies such as Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB), which significantly improve training efficiency and model performance.\nIn this study, the loss function used during training is Mean Squared Error (MSE), which is defined as:\n$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$Where, \\( y \\) is the true return rate, \\( \\hat{y} \\) is the return rate predicted by the model, and \\( N \\) is the number of samples.\n4.4.3 Algorithm Flow The specific algorithm flow in this section is as follows:\nData Preparation: Use one year’s worth of 50 factor data for each stock (A-share market-wide data) and historical future one-month returns as features.\nModel Training: Use Grid Search to optimize the hyperparameters of the LightGBM model and train the model to predict the future one-month return rate. The model training flow is shown in Fig. 4.12.\n$$ \\text{Parameter Optimization:} \\quad \\theta^* = \\arg\\min_\\theta \\sum_{i=1}^{N} L(y_i, \\hat{y}_i(\\theta)) $$Where, \\( \\theta \\) represents the set of model hyperparameters, and \\( \\theta^* \\) is the optimal parameter.\nFactor Importance Calculation: Use LightGBM’s feature_importances_ method to calculate the feature importance of each factor. Feature importance is mainly measured by two indicators:\nSplit: The number of times the feature is used for splitting in all trees. Gain: The total gain brought by the feature in all splits (i.e., the amount of reduction in the loss function). The feature importance of a factor can be expressed as:\n$$ \\text{Importance}_{\\text{split}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\mathbb{I}(f \\text{ is used for splitting the } j \\text{-th leaf node}) $$$$ \\text{Importance}_{\\text{gain}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\Delta L_{m,j} \\cdot \\mathbb{I}(f \\text{ is used for splitting the } j \\text{-th leaf node}) $$Where, \\( \\mathbb{I} \\) is the indicator function, and \\( \\Delta L_{m,j} \\) is the reduction in loss brought by factor \\( f \\) in the \\( j \\)-th split of the \\( m \\)-th tree.\nFactor Screening: Sort according to the factor importance calculated by the model, and select the top ten factors with the highest importance as the factors used in this cross-sectional analysis. The importance of the selected factors is shown in Table 4.4.4.\n4.4.4 Partial Ranking of Selected Factor Importance importance feature_name trade_date 35 factor_35 2010-08-11 27 factor_27 2010-08-11 33 factor_33 2010-08-11 20 factor_20 2010-08-11 24 factor_24 2010-08-11 45 factor_45 2010-08-11 37 factor_37 2010-08-11 49 factor_49 2010-08-11 19 factor_19 2010-08-11 47 factor_47 2010-08-11 22 factor_22 2010-09-09 20 factor_20 2010-09-09 30 factor_30 2010-09-09 24 factor_24 2010-09-09 4.4.5 Code Implementation Snippet The following is a code snippet used in the training process for factor selection.\nfeature_choice def feature_choice( self, days=21, is_local=False ): if is_local: feature_info = pd.read_hdf(os.path.join(RESULTS, Feature_Info + '.h5')) else: factors = self.get_env().query_data(Factors_Data) factors = factors[ factors[COM_DATE] \u003e= '2010-01-01' ] trade_list = list(set(factors[COM_DATE])) trade_list.sort() if len(trade_list) % days == 0: n = int(len(trade_list) / days) - 7 else: n = int(len(trade_list) / days) - 6 feature_info = pd.DataFrame() begin_index = 147 feature = list(factors.columns) feature.remove(COM_SEC) feature.remove(COM_DATE) feature.remove(Ret) for i in range(n): end_date = days * i + begin_index - 21 begin_date = days * i trade_date = days * i + begin_index print(trade_list[trade_date]) train_data = factors[ (factors[COM_DATE] \u003c= trade_list[end_date]) \u0026 (factors[COM_DATE] \u003e= trade_list[begin_date]) ] model = lgb.LGBMRegressor() model.fit(train_data[feature], train_data[Ret]) feature_info_cell = pd.DataFrame(columns=Info_Fields) feature_info_cell[Importance] = model.feature_importances_ feature_info_cell[Feature_Name] = model.feature_name_ feature_info_cell = feature_info_cell.sort_values(by=Importance).tail(10) feature_info_cell[COM_DATE] = trade_list[trade_date] feature_info = pd.concat( [feature_info, feature_info_cell], axis=0 ) h = pd.HDFStore(os.path.join(RESULTS, Feature_Info + '.h5'), 'w') h['data'] = feature_info h.close() self.get_env().add_data(feature_info, Feature_Info) pass Through the above process, LightGBM is used to efficiently screen out the factors that have the greatest impact on predicting future returns, thereby improving the predictive ability and interpretability of the model.\n4.5 Factor Combination Based on BiLSTM This section uses BiLSTM for factor combination. The specific principle of BiLSTM has been introduced in Chapter 2, and will not be repeated here. First, let’s introduce the specific network structure of the model. The network structure of BiLSTM set in this paper through a large number of repeated experiments is shown in Table 4.5.1. The default tanh and linear activation functions of recurrent neural networks are used between layers. Dropout is added to prevent overfitting, but if Dropout uses an excessively large dropout rate, underfitting will occur. Therefore, the dropout rate of Dropout is set to 0.01. The number of neurons in the BiLSTM recurrent layer of the final model is 100. A BiLSTM layer and three fully connected layers are used, and a Dropout is set between the BiLSTM layer and the first fully connected layer.\n4.5.1 BiLSTM Network Structure Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 100) 24400 dropout_1 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 256) 25856 dropout_2 (Dropout) (None, 256) 0 dense_2 (Dense) (None, 64) 16448 dense_3 (Dense) (None, 1) 0 Total params: 66,769\nTrainable params: 66,769\nNon-trainable params: 0\nBecause the amount of data used in this experiment is large, epochs=400 and batch_size=1024 are selected. The loss function of the model is Mean Squared Error (MSE). The optimizer used is Stochastic Gradient Descent (SGD). Stochastic gradient descent has three advantages over gradient descent (GD): it can more effectively use information when information is redundant, and the early iteration effect is excellent, which is suitable for processing large-sample data $^{[23]}$. Since the amount of training data in this experiment is large, if SGD is used, only one sample is used for iteration each time, and the training speed is very fast, which can greatly reduce the time spent on training. The default values in its keras package are used, i.e., lr=0.01, momentum=0.0, decay=0.0, and nesterov=False.\nParameter Explanation:\nlr: Learning rate momentum: Momentum parameter decay: Learning rate decay value after each update nesterov: Determine whether to use Nesterov momentum 4.5.2 Algorithm Flow The specific algorithm flow in this section is as follows:\nUse A-share market-wide data of 10 factors (factors selected by LightGBM) and historical future one-month returns for each stock for one year as features. Take the future one-month return rate of each stock per year as the prediction target, and use BiLSTM for training, as shown in Fig. 12. Fig. 12. Rolling Window\nThe real-time factor data of out-of-sample data for one month is passed through the trained BiLSTM model to obtain the real-time expected return rate of each stock for the next month. The return rate is shown in Table 4.11. 4.5.3 Partial Stock Predicted Return Rate Table sec_code trade_date y_hat 000001.SZ 2011/5/26 0.0424621 000002.SZ 2011/5/26 -0.1632174 000004.SZ 2011/5/26 -0.0642319 000005.SZ 2011/5/26 0.08154649 000006.SZ 2011/5/26 0.00093213 000007.SZ 2011/5/26 -0.073218 000008.SZ 2011/5/26 -0.0464256 000009.SZ 2011/5/26 -0.091549 000010.SZ 2011/5/26 0.08154649 000011.SZ 2011/5/26 -0.1219943 000012.SZ 2011/5/26 -0.1448984 000014.SZ 2011/5/26 0.09038845 000016.SZ 2011/5/26 -0.11225 4.5.4 Code Implementation Snippet The following is a code snippet used in the training process for building the BiLSTM training network.\nbuild_net_blstm def build_net_blstm(self): model = ks.Sequential() model.add( ks.layers.Bidirectional(ks.layers.LSTM( 50 ),input_shape=(11,10)) ) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(256)) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(64)) model.add(ks.layers.Dense(1)) model.compile(optimizer='sgd', loss='mse') model.summary() self.set_model(model) 4.6 Quantitative Strategy and Strategy Backtesting 4.6.1 Backtesting Metrics First, let’s introduce some common backtesting metrics for strategies. Evaluation metrics include Total Rate of Return, Annualized Rate of Return, Annualized volatility, Sharpe ratio, Maximum Drawdown (MDD), Annualized turnover rate, and Annualized transaction cost rate. It is assumed that the stock market is open for 252 days a year, the risk-free rate is defaulted to 0.035, and the commission fee is defaulted to 0.002.\nTotal Rate of Return: Under the same other indicators, the larger the cumulative rate of return, the better the strategy, and the more it can bring greater returns. The formula is as follows: $$ \\text{Total Rate of Return} = r_{p} = \\frac{P_{1} - P_{0}}{P_{0}} $$$P_{1}$: Total value of final stocks and cash\n$P_{0}$: Total value of initial stocks and cash\nAnnualized Rate of Return: It is to convert the cumulative total rate of return into a geometric average rate of return on an annual basis. Under the same other indicators, the larger the annualized rate of return, the better the strategy. The formula is as follows: $$ \\text{Annualized Rate of Return} = R_{p} = \\left(1 + r_{p}\\right)^{\\frac{252}{t}} - 1 $$$r_{p}$: Cumulative rate of return\n$t$: Number of days the investment strategy is executed\nAnnualized volatility: Defined as the standard deviation of the logarithmic value of the annual return rate of the object asset. Annualized volatility is used to measure the risk of a strategy. The greater the volatility, the higher the risk of the strategy. The formula is as follows: $$ \\begin{aligned} \\text{Annualized volatility} = \\sigma_{p} \u0026= \\sqrt{\\frac{252}{t-1} \\sum_{i=1}^{t}\\left(r_{d} - \\bar{r}_{d}\\right)^{2}} \\\\ \\bar{r}_{d} \u0026= \\frac{1}{t} \\sum_{i=1}^{t} r_{d_{i}} \\end{aligned} $$$r_{d_{i}}$: Daily return rate on the $i$-th day\n$\\bar{r}_{d}$: Average daily return rate\n$t$: Number of days the investment strategy is executed\nSharpe ratio: Proposed by Sharpe (1966)$^{[24]}$. It represents the excess return obtained by investors for bearing an extra unit of risk$^{[25]}$. Here is the calculation formula for the annualized Sharpe ratio: $$ S = \\frac{R_{p} - R_{f}}{\\sigma_{p}} $$$R_{p}$: Annualized rate of return\n$R_{f}$: Risk-free rate of return\n$\\sigma_{p}$: Annualized volatility\nMaximum Drawdown (MDD): Indicates the maximum value of the return rate drawdown when the total value of stocks and cash of our strategy portfolio reaches the lowest point during the operation period. Maximum drawdown is used to measure the most extreme possible loss situation of the strategy. $$ MDD = \\frac{\\max \\left(V_{x} - V_{y}\\right)}{V_{x}} $$$V_{x}$ and $V_{y}$ are the total value of stocks and cash of the strategy portfolio on day $x$ and day $y$ respectively, and $x \u003c y$.\nAnnualized turnover rate: Used to measure the frequency of buying and selling stocks in the investment portfolio. The larger the value, the more frequent the portfolio turnover and the greater the transaction cost. $$ \\text{change} = \\frac{N \\times 252}{t} $$$t$: Number of days the investment strategy is executed\n$N$: Total number of buy and sell transactions\nAnnualized transaction cost rate: Used to measure the transaction cost of the investment portfolio strategy. The larger the value, the higher the transaction cost. $$ c = \\left(1 + \\text{commison}\\right)^{\\text{change}} - 1 $$change: Annualized turnover rate\ncommison: Commission fee\n4.6.2 Strategy and Backtesting Results The quantitative trading strategy in this paper adopts position switching every month (i.e., the rebalancing period is 28 trading days). Each time, the strategy adopts an equal-weight stock holding method to buy the 25 stocks with the highest expected return rate predicted by BiLSTM and sell the originally held stocks. The backtesting time and rules in this paper are as follows:\nBacktesting Time: From January 2012 to October 2020. Backtesting Stock Pool: All A-shares, excluding Special Treatment (ST) stocks. Transaction Fee: A brokerage commission of 0.2% is paid when buying, and a brokerage commission of 0.2% is paid when selling. If the commission for a single transaction is less than 5 CNY, the brokerage charges 5 CNY. Buying and Selling Rules: Stocks that hit the upper limit on the opening day cannot be bought, and stocks that hit the lower limit cannot be sold. 4.6.2.1 Strategy Backtesting Results Cumulative Return Annualized Return Annualized Volatility Sharpe Ratio Max Drawdown Annualized Turnover Rate Annualized Transaction Cost Rate Strategy 701.00% 29.18% 33.44% 0.77 51.10% 51.10% 11.35% Benchmark 110.40% 9.70% 26.01% 0.24 58.49% 58.49% 0.00% Fig. 22. Net Profit Curve\nThe backtesting results are shown in the table and Fig. 22 above. My strategy adopts the LightGBM-BiLSTM quantitative strategy introduced in this chapter. The benchmark uses the CSI All Share (000985). From the results above, it can be seen that the cumulative return of this strategy is 701.00%, which is much higher than the benchmark’s 110.40%; the annualized return is 29.18%, which is much higher than the benchmark’s 9.70%; and the Sharpe ratio is 0.77, which is higher than the benchmark’s 0.24. These three backtesting indicators show that the LightGBM-BiLSTM quantitative strategy can indeed bring greater returns to investors. The annualized volatility of this strategy is 33.44%, which is greater than the benchmark’s 26.01%, and the maximum drawdown is 51.10%, which is less than the benchmark’s 58.49%. These two backtesting indicators show that the LightGBM-BiLSTM quantitative strategy has certain risks, especially it is difficult to resist the impact of systemic risks. The annualized turnover rate is 11.35%, and the annualized transaction cost rate is 2.29%, indicating that our strategy is not a high-frequency trading strategy and the transaction cost is small. It can be seen from the return curve chart that the return rate of the LightGBM-BiLSTM quantitative strategy in the first two years is not much different from the benchmark, and there is no special advantage. However, from around April 2015, the return rate of the LightGBM-BiLSTM quantitative strategy is significantly better than the benchmark’s return rate. Overall, the return rate of this LightGBM-BiLSTM quantitative strategy is very considerable, but there are still certain risks.\nChapter 5 Conclusion and Future Directions 5.1 Conclusion This paper first introduced the research background and significance of stock price prediction and quantitative strategy research based on deep learning, and then introduced the domestic and international research status of stock price prediction and quantitative investment strategies respectively. Then, the innovations and research framework of this paper were explained. Then, in the chapter on related theoretical foundations, this paper briefly introduced the deep learning models and the development history of quantitative investment used in this paper. The basic structure, basic principles, and characteristics of the three models LSTM, GRU, and BiLSTM are mainly introduced.\nSubsequently, this paper used the daily frequency data of SPD Bank and IBM, and preprocessed the data through a series of data processing processes and feature extraction. Then, the specific network structure and hyperparameter settings of the three models LSTM, GRU, and BiLSTM were introduced. Then, we used LSTM, GRU, and BiLSTM to predict the closing prices of the two stocks and compare the model evaluations. The experimental results show that for both stocks, the BiLSTM prediction effect is more accurate.\nFinally, in order to further illustrate the application value of BiLSTM in finance, this paper constructed a quantitative investment model based on LightGBM-BiLSTM. Stocks in the entire A-share market and multiple factors were selected for factor cleaning, factor selection based on LightGBM, and factor combination based on LSTM. Then, we constructed a certain investment strategy and compared it with the benchmark holding CSI All Share through evaluation indicators such as cumulative return rate, annualized return rate, annualized volatility, and Sharpe ratio. Through comparison, it was found that the LightGBM-BiLSTM quantitative investment model can bring better returns, indicating the effectiveness of using deep learning to build quantitative investment strategies.\n5.2 Future Directions Although this paper compares the effects of LSTM, GRU, and BiLSTM models in predicting stock closing prices and achieves certain results based on the LightGBM-BiLSTM quantitative investment strategy, there are still some shortcomings in this paper’s research. Combining the research results of this paper, the following research and improvements can be further carried out:\nDiversification of Prediction Targets: In terms of predicting stock prices, this paper selects the stock closing price as the prediction target. Although this result is the most intuitive, the Random Walk Hypothesis (RWH) proposed by Bachelier (1900)$^{[26]}$ believes that stock prices follow a random walk and are unpredictable. Although many behavioral economists have since proved that this view is not entirely correct, it also shows that simply predicting stock closing prices is not so strong in terms of difficulty and interpretability $^{[27][28]}$. Therefore, stock volatility prediction, stock price increase/decrease judgment, and stock return rate prediction can be selected as future research directions. Diversified Model Comparison: In terms of predicting stock prices, this paper compares the three recurrent neural network models LSTM, GRU, and BiLSTM and shows that BiLSTM has better prediction effect, but there is still a lack of comparative research with more different models. Therefore, future in-depth research can be conducted on comparisons with Autoregressive Integrated Moving Average (ARIMA), Convolutional Neural Networks (CNNs), Deep Neural Networks (DNNs), CNN-LSTM, Transformer, TimeGPT, and other single or composite models. Factor Diversification: The factors used in this paper to construct quantitative investment strategies are all technical price-volume factors, and the types of factors are single. In the future, different types of factors such as financial factors, sentiment factors, and growth factors can be selected to improve the performance of the strategy. At the same time, future research can also appropriately add timing strategies to increase positions when predicting that the market will rise and reduce positions when predicting that the market will fall to earn beta (\\(\\beta\\)) returns. Investment Portfolio Optimization: The factor combination process in this paper is still imperfect. In the future, quadratic programming methods can be used to optimize the investment portfolio. High-Frequency Trading Strategy Research: The quantitative investment strategy method in this paper adopts a low-frequency trading strategy. In the future, stock tick data can be used to study high-frequency strategies and ultra-high-frequency strategies. References [1] White, H. “Economic prediction using neural networks: The case of IBM daily stock returns.” Proc. of ICNN. 1988, 2: 451-458.\n[2] Kimoto, T., Asakawa, K., Yoda, M., et al. “Stock market prediction system with modular neural networks.” Proc. of 1990 IJCNN International Joint Conference on Neural Networks. IEEE, 1990: 1-6.\n[3] Zhang, G. P. “Time series forecasting using a hybrid ARIMA and neural network model.” Neurocomputing. 2003, 50: 159-175.\n[4] Akita, R., Yoshihara, A., Matsubara, T., et al. “Deep learning for stock prediction using numerical and textual information.” Proc. of 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS). IEEE, 2016: 1-6.\n[5] 宮崎邦洋, 松尾豊. “Deep Learning を用いた株価予測の分析.” 人工知能学会全国大会論文集 第31回全国大会. 一般社団法人 人工知能学会, 2017: 2D3OS19a3-2D3OS19a3.\n[6] Kim, T., Kim, H. Y. “Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data.” PLoS ONE. 2019, 14(2): e0212320.\n[7] Hochreiter, S., Schmidhuber, J. “Long short-term memory.” Neural Computation. 1997, 9(8): 1735-1780.\n[8] Cho, K., Van Merriënboer, B., Gulcehre, C., et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078. 2014.\n[9] Chung, J., Gulcehre, C., Cho, K. H., et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” arXiv preprint arXiv:1412.3555. 2014.\n[10] Gruber, N., Jockisch, A. “Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?” Frontiers in Artificial Intelligence. 2020, 3(40): 1-6.\n[11] Markowitz, H. “Portfolio Selection.” The Journal of Finance. 1952, 7(1): 77-91. doi:10.2307/2975974.\n[12] Merton, R. C. “An analytic derivation of the efficient portfolio frontier.” Journal of Financial and Quantitative Analysis. 1972: 1851-1872.\n[13] Sharpe, W. F. “Capital asset prices: A theory of market equilibrium under conditions of risk.” The Journal of Finance. 1964, 19(3): 425-442.\n[14] Lintner, J. “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.” Review of Economics and Statistics. 1965, 47(1): 13-37.\n[15] Mossin, J. “Equilibrium in a capital asset market.” Econometrica: Journal of the Econometric Society. 1966: 768-783.\n[16] Ross, S. A. “The arbitrage theory of capital asset pricing.” Journal of Economic Theory. 1976, 13(3): 341-60.\n[17] Fama, E. F., French, K. R. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics. 1993, 33(1): 3-56.\n[18] Fama, E. F., French, K. R. “A five-factor asset pricing model.” Journal of Financial Economics. 2015, 116(1): 1-22.\n[19] Kingma, D. P., Ba, J. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980. 2014.\n[20] Friedman, J. H. “Greedy function approximation: A gradient boosting machine.” Annals of Statistics. 2001: 1189-1232.\n[21] Kopitar, L., Kocbek, P., Cilar, L., et al. “Early detection of type 2 diabetes mellitus using machine learning-based prediction models.” Scientific Reports. 2020, 10(1): 1-12.\n[22] Ke, G., Meng, Q., Finley, T., et al. “Lightgbm: A highly efficient gradient boosting decision tree.” Advances in Neural Information Processing Systems. 2017, 30: 3146-3154.\n[23] Bottou, L., Curtis, F. E., Nocedal, J. “Optimization methods for large-scale machine learning.” SIAM Review. 2018, 60(2): 223-311.\n[24] Sharpe, W. F. “Mutual fund performance.” The Journal of Business. 1966, 39(1): 119-138.\n[25] Sharpe, W. F. “The sharpe ratio.” Journal of Portfolio Management. 1994, 21(1): 49-58.\n[26] Bachelier, L. “Théorie de la spéculation.” Annales Scientifiques de l’École Normale Supérieure. 1900, 17: 21-86.\n[27] Fromlet, H. “Behavioral finance-theory and practical application: Systematic analysis of departures from the homo oeconomicus paradigm are essential for realistic financial research and analysis.” Business Economics. 2001: 63-69.\n[28] Lo, A. W. “The adaptive markets hypothesis.” The Journal of Portfolio Management. 2004, 30(5): 15-29.\nReference Blog Colah’s Blog. (2015, August 27). Understanding LSTM Networks. Citation Citation: For reprint or citation of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Apr 2021). Stock Price Prediction and Quantitative Strategy Based on Deep Learning. https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\nOr\n@article{syhya2021stockprediction, title = \"Stock Price Prediction and Quantitative Strategy Based on Deep Learning\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2021\", month = \"Apr\", url = \"https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\" } ","wordCount":"13710","inLanguage":"en","datePublished":"2021-04-21T12:00:00+08:00","dateModified":"2021-04-21T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Stock Price Prediction and Quantitative Strategy Based on Deep Learning</h1><div class=post-meta><span title='2021-04-21 12:00:00 +0800 +0800'>Created:&nbsp;2021-04-21</span>&nbsp;·&nbsp;Updated:&nbsp;2021-04-21&nbsp;·&nbsp;65 min&nbsp;·&nbsp;13710 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2021-04-21-deep-learning-stock-prediction/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#chapter-1-introduction>Chapter 1 Introduction</a><ul><li><a href=#11-research-background-and-significance>1.1 Research Background and Significance</a><ul><li><a href=#111-research-background>1.1.1 Research Background</a></li><li><a href=#112-research-significance>1.1.2 Research Significance</a></li></ul></li><li><a href=#12-literature-review>1.2 Literature Review</a></li><li><a href=#13-innovations-of-the-paper>1.3 Innovations of the Paper</a></li><li><a href=#14-research-framework-of-the-paper>1.4 Research Framework of the Paper</a></li></ul></li><li><a href=#chapter-2-related-theoretical-foundations>Chapter 2 Related Theoretical Foundations</a><ul><li><a href=#21-deep-learning-models>2.1 Deep Learning Models</a><ul><li><a href=#211-rnn>2.1.1 RNN</a></li><li><a href=#212-lstm>2.1.2 LSTM</a></li><li><a href=#213-gru>2.1.3 GRU</a></li><li><a href=#214-bilstm>2.1.4 BiLSTM</a></li></ul></li><li><a href=#22-quantitative-stock-selection-theory>2.2 Quantitative Stock Selection Theory</a><ul><li><a href=#221-mean-variance-model>2.2.1 Mean-Variance Model</a></li><li><a href=#222-capital-asset-pricing-model>2.2.2 Capital Asset Pricing Model</a></li><li><a href=#223-arbitrage-pricing-theory-and-multi-factor-model>2.2.3 Arbitrage Pricing Theory and Multi-Factor Model</a></li><li><a href=#224-fama-french-three-factor-model-and-five-factor-model>2.2.4 Fama-French Three-Factor Model and Five-Factor Model</a></li><li><a href=#225-model-comparison-table>2.2.5 Model Comparison Table</a></li></ul></li></ul></li><li><a href=#chapter-3-comparative-study-of-lstm-gru-and-bilstm-in-stock-price-prediction>Chapter 3 Comparative Study of LSTM, GRU, and BiLSTM in Stock Price Prediction</a><ul><li><a href=#31-introduction-to-experimental-data>3.1 Introduction to Experimental Data</a><ul><li><a href=#311-partial-display-of-spd-bank-dataset>3.1.1 Partial Display of SPD Bank Dataset</a></li><li><a href=#312-partial-display-of-ibm-dataset>3.1.2 Partial Display of IBM Dataset</a></li></ul></li><li><a href=#32-experimental-data-preprocessing>3.2 Experimental Data Preprocessing</a><ul><li><a href=#321-data-normalization>3.2.1 Data Normalization</a></li><li><a href=#322-data-partitioning>3.2.2 Data Partitioning</a></li></ul></li><li><a href=#33-model-network-structure>3.3 Model Network Structure</a><ul><li><a href=#331-lstm-network-structure-for-ibm>3.3.1 LSTM Network Structure for IBM</a></li><li><a href=#332-gru-network-structure-for-ibm>3.3.2 GRU Network Structure for IBM</a></li><li><a href=#333-bilstm-network-structure-for-ibm>3.3.3 BiLSTM Network Structure for IBM</a></li></ul></li><li><a href=#34-model-compilation-and-hyperparameter-settings>3.4 Model Compilation and Hyperparameter Settings</a></li><li><a href=#35-experimental-results-and-analysis>3.5 Experimental Results and Analysis</a><ul><li><a href=#351-experimental-results-for-spd-bank>3.5.1 Experimental Results for SPD Bank</a></li><li><a href=#352-experimental-results-for-ibm>3.5.2 Experimental Results for IBM</a></li></ul></li><li><a href=#36-chapter-summary>3.6 Chapter Summary</a></li></ul></li><li><a href=#chapter-4-research-on-quantitative-investment-model-based-on-lightgbm-bilstm>Chapter 4 Research on Quantitative Investment Model Based on LightGBM-BiLSTM</a><ul><li><a href=#41-experimental-steps>4.1 Experimental Steps</a></li><li><a href=#42-experimental-data>4.2 Experimental Data</a><ul><li><a href=#421-features-included-in-the-stock-dataset>4.2.1 Features Included in the Stock Dataset</a></li><li><a href=#422-partial-display-of-a-share-market-wide-daily-dataset>4.2.2 Partial Display of A-Share Market-Wide Daily Dataset</a></li><li><a href=#423-partial-display-of-csi-all-share-daily-dataset>4.2.3 Partial Display of CSI All Share Daily Dataset</a></li><li><a href=#424-original-factor-data>4.2.4 Original Factor Data</a></li><li><a href=#425-cleaned-factor-data>4.2.5 Cleaned Factor Data</a></li><li><a href=#426-factor-data>4.2.6 Factor Data</a></li><li><a href=#427-basic-field-table>4.2.7 Basic Field Table</a></li><li><a href=#428-operator-list>4.2.8 Operator List</a></li><li><a href=#429-genetic-programming>4.2.9 Genetic Programming</a></li><li><a href=#4210-partially-mined-factors>4.2.10 Partially Mined Factors</a></li><li><a href=#4211-factor-ir-test-table>4.2.11 Factor IR Test Table</a></li></ul></li><li><a href=#43-factor-cleaning>4.3 Factor Cleaning</a><ul><li><a href=#431-factor-missing-value-handling-and-outlier-removal>4.3.1 Factor Missing Value Handling and Outlier Removal</a></li><li><a href=#432-factor-standardization>4.3.2 Factor Standardization</a></li><li><a href=#433-factor-neutralization>4.3.3 Factor Neutralization</a></li></ul></li><li><a href=#44-factor-selection-based-on-lightgbm>4.4 Factor Selection Based on LightGBM</a><ul><li><a href=#441-gbdt>4.4.1 GBDT</a></li><li><a href=#442-lightgbm>4.4.2 LightGBM</a></li><li><a href=#443-algorithm-flow>4.4.3 Algorithm Flow</a></li><li><a href=#444-partial-ranking-of-selected-factor-importance>4.4.4 Partial Ranking of Selected Factor Importance</a></li><li><a href=#445-code-implementation-snippet>4.4.5 Code Implementation Snippet</a></li></ul></li><li><a href=#45-factor-combination-based-on-bilstm>4.5 Factor Combination Based on BiLSTM</a><ul><li><a href=#451-bilstm-network-structure>4.5.1 BiLSTM Network Structure</a></li><li><a href=#452-algorithm-flow>4.5.2 Algorithm Flow</a></li><li><a href=#453-partial-stock-predicted-return-rate-table>4.5.3 Partial Stock Predicted Return Rate Table</a></li><li><a href=#454-code-implementation-snippet>4.5.4 Code Implementation Snippet</a></li></ul></li><li><a href=#46-quantitative-strategy-and-strategy-backtesting>4.6 Quantitative Strategy and Strategy Backtesting</a><ul><li><a href=#461-backtesting-metrics>4.6.1 Backtesting Metrics</a></li><li><a href=#462-strategy-and-backtesting-results>4.6.2 Strategy and Backtesting Results</a></li></ul></li></ul></li><li><a href=#chapter-5-conclusion-and-future-directions>Chapter 5 Conclusion and Future Directions</a><ul><li><a href=#51-conclusion>5.1 Conclusion</a></li><li><a href=#52-future-directions>5.2 Future Directions</a></li></ul></li><li><a href=#references>References</a><ul><li><a href=#reference-blog>Reference Blog</a></li></ul></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><h2 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h2><p>The stock market is a crucial component of the financial market. In recent years, with its vigorous development, research on stock price prediction and quantitative investment strategies has attracted scholars from various fields. With the advancement of Artificial Intelligence (AI) and Machine Learning (ML) in recent years, researchers have shifted from traditional statistical models to AI algorithms. Particularly after the deep learning boom, neural networks have achieved remarkable results in stock price prediction and quantitative investment strategy research. The objective of deep learning is to learn multi-level features, constructing abstract high-level features by combining low-level ones, thereby mining the distributed feature representations of data. This approach enables complex nonlinear modeling to accomplish prediction tasks. Recurrent Neural Networks (RNNs) have been widely applied to sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, leading many researchers to use RNNs for stock price prediction. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The advent of Long Short-Term Memory (LSTM) networks addressed this problem, followed by variants such as Gated Recurrent Units (GRUs), Peephole LSTMs, and Bidirectional LSTMs (BiLSTMs). Traditional stock prediction models often overlook temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully leverages the contextual relationships in both forward and backward temporal directions of time series data. It also avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies.</p><p>The first part of this paper&rsquo;s experiments utilizes stock data from China&rsquo;s Shanghai Pudong Development Bank and the US&rsquo;s IBM to establish stock prediction models using LSTM, GRU, and BiLSTM. By comparing the prediction results of these three deep learning models, it is found that the BiLSTM model outperforms the others for both datasets, demonstrating better prediction accuracy. The second part uses A-share market-wide stock data and first employs a LightGBM model to screen 50 factors, selecting the top 10 most important factors. Subsequently, a BiLSTM model is used to select and combine these factors to establish a quantitative investment strategy. Empirical analysis and backtesting of this strategy reveal that it outperforms the market benchmark index, indicating the practical application value of the BiLSTM model in stock price prediction and quantitative investment.</p><p><strong>Keywords</strong>: Quantitative Investment; Deep Learning; Neural Network Model; Multi-Factor Stock Selection; BiLSTM</p><h2 id=chapter-1-introduction>Chapter 1 Introduction<a hidden class=anchor aria-hidden=true href=#chapter-1-introduction>#</a></h2><h3 id=11-research-background-and-significance>1.1 Research Background and Significance<a hidden class=anchor aria-hidden=true href=#11-research-background-and-significance>#</a></h3><h4 id=111-research-background>1.1.1 Research Background<a hidden class=anchor aria-hidden=true href=#111-research-background>#</a></h4><p>Emerging in the 1970s, quantitative investment gradually entered the vision of investors, initiating a new revolution that changed the landscape of portfolio management previously dominated by passive management and the efficient market hypothesis. The efficient market hypothesis posits that under market efficiency, stock prices reflect all market information, and no excess returns exist. Passive investment management, based on the belief that markets are efficient, focuses more on asset classes, with the most common approach being purchasing index funds and tracking published index performance. In contrast, active investment management relies primarily on investors&rsquo; subjective judgments of the market and individual stocks. By applying mathematical models to the financial domain and using available public data to evaluate stocks, active managers construct portfolios to achieve returns. Quantitative investment, through statistical processing of vast historical data to uncover investment opportunities and avoid subjective factors, has gained increasing popularity among investors. Since the rise of quantitative investment, people have gradually utilized various technologies for stock price prediction to better establish quantitative investment strategies. Early domestic and international scholars adopted statistical methods for modeling and predicting stock prices, such as exponential smoothing, multiple regression, and Autoregressive Moving Average (ARMA) models. However, due to the multitude of factors influencing the stock market and the large volume of data, stock prediction is highly challenging, and the prediction effectiveness of various statistical models has been unsatisfactory.</p><p>In recent years, the continuous development of machine learning, deep learning, and neural network technologies has provided technical support for stock price prediction and the construction of quantitative strategies. Many scholars have achieved new breakthroughs using methods like Random Forest, Neural Networks, Support Vector Machines, and Convolutional Neural Networks. The ample historical data in the stock market, coupled with diverse technological support, provides favorable conditions for stock price prediction and the construction of quantitative strategies.</p><h4 id=112-research-significance>1.1.2 Research Significance<a hidden class=anchor aria-hidden=true href=#112-research-significance>#</a></h4><p>From the perspective of the long-term development of the national economic system and financial markets, research on stock price prediction models and quantitative investment strategies is indispensable. China started relatively late, with a less mature financial market, fewer financial instruments, and lower market efficiency. However, in recent years, the country has gradually relaxed policies and vigorously developed the financial market, providing a &ldquo;breeding ground&rdquo; for the development of quantitative investment. Developing quantitative investment and emerging financial technologies can offer China&rsquo;s financial market an opportunity for a &ldquo;curve overtaking&rdquo;. Furthermore, the stock price index, as a crucial economic indicator, serves as a barometer for China&rsquo;s economic development.</p><p>From the perspective of individual and institutional investors, constructing stock price prediction models and quantitative investment strategy models enhances market efficiency. Individual investors often lack comprehensive professional knowledge, and their investment behaviors can be somewhat blind. Developing relevant models to provide references can reduce the probability of judgment errors and change the relatively disadvantaged position of individual investors in the capital market. For institutional investors, rational and objective models, combined with personal experience, improve the accuracy of decision-making and provide new directions for investment behaviors.</p><p>In summary, considering China&rsquo;s current development status, this paper&rsquo;s selection of individual stocks for stock price prediction models and A-share market-wide stocks for quantitative strategy research holds significant practical research value.</p><h3 id=12-literature-review>1.2 Literature Review<a hidden class=anchor aria-hidden=true href=#12-literature-review>#</a></h3><p><a href=https://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf>White (1988)</a>$^{[1]}$ used a Backpropagation (BP) neural network to predict the daily returns of IBM stock. However, due to the BP neural network model&rsquo;s susceptibility to gradient explosion, the model could not converge to a global minimum, thus failing to achieve accurate predictions.</p><p><a href=https://web.ist.utl.pt/adriano.simoes/tese/referencias/Papers%20-%20Adriano/NN.pdf>Kimoto (1990)</a>$^{[2]}$ developed a system for predicting the Tokyo Stock Exchange Prices Index (TOPIX) using modular neural network technology. This system not only successfully predicted TOPIX but also achieved a certain level of profitability through stock trading simulations based on the prediction results.</p><p><a href=https://dl.icdst.org/pdfs/files/2c442c738bd6bc178e715f400bec5d5f.pdf>G. Peter Zhang (2003)</a>$^{[3]}$ conducted a comparative study on the performance of Autoregressive Integrated Moving Average (ARIMA) models and Artificial Neural Network (ANN) models in time series forecasting. The results showed that ANN models significantly outperformed ARIMA models in terms of time series prediction accuracy.</p><p><a href=https://ieeexplore.ieee.org/document/7550882>Ryo Akita (2016)</a>$^{[4]}$ selected the Consumer Price Index (CPI), Price-to-Earnings ratio (P/E ratio), and various events reported in newspapers as features, and constructed a financial time series prediction model using paragraph vectors and LSTM networks. Using actual data from fifty listed companies on the Tokyo Stock Exchange, the effectiveness of this model in predicting stock opening prices was verified.</p><p><a href=https://www.ai-gakkai.or.jp/jsai2017/webprogram/2017/pdf/1112.pdf>Kunihiro Miyazaki (2017)</a>$^{[5]}$ constructed a model for predicting the rise and fall of the Topix Core 30 index and its constituent stocks by extracting daily stock chart images and 30-minute stock price data. The study compared multiple models, including Logistic Regression (LR), Random Forest (RF), Multilayer Perceptron (MLP), LSTM, CNN, PCA-CNN, and CNN-LSTM. The results indicated that LSTM had the best prediction performance, CNN performed generally, but hybrid models combining CNN and LSTM could improve prediction accuracy.</p><p><a href="https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0212320&amp;type=printable">Taewook Kim (2019)</a>$^{[6]}$ proposed an LSTM-CNN hybrid model to combine features from both stock price time series and stock price image data representations to predict the stock price of the S&amp;P 500 index. The study showed that the LSTM-CNN model outperformed single models in stock price prediction, and this prediction had practical significance for constructing quantitative investment strategies.</p><h3 id=13-innovations-of-the-paper>1.3 Innovations of the Paper<a hidden class=anchor aria-hidden=true href=#13-innovations-of-the-paper>#</a></h3><p>This paper has the following innovations in stock price prediction:</p><ol><li>Data from both the domestic A-share market (Shanghai Pudong Development Bank) and the international US stock market (IBM) are used for research, avoiding the limitations of single-market studies. Traditional BP models have never considered temporal factors, or like LSTM models, only consider unidirectional temporal relationships. Therefore, this paper uses the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions of time series data and avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies. Empirical evidence, compared with LSTM and GRU models, demonstrates its ability to improve prediction accuracy.</li><li>The stock price prediction model is trained using multiple stock features, including opening price, closing price, highest price, and trading volume. Compared to single-feature prediction of stock closing prices, this approach is theoretically more accurate and can better compare the prediction effectiveness of LSTM, GRU, and BiLSTM for stocks.</li></ol><p>This paper has the following innovations in quantitative strategy research:</p><ol><li>Instead of using common market factors, this paper uses multiple price-volume factors obtained through Genetic Programming (GP) algorithms and artificial data mining. LightGBM model is used to screen 50 factors, selecting the top 10 most important factors.</li><li>Traditional quantitative investment models generally use LSTM and CNN models to construct quantitative investment strategies. This paper uses A-share market-wide data and employs a BiLSTM model to select and combine factors to establish a quantitative investment strategy. Backtesting and empirical analysis of this strategy show that it outperforms the market benchmark index (CSI All Share), demonstrating the practical application value of the BiLSTM model in stock price prediction and quantitative investment.</li></ol><h3 id=14-research-framework-of-the-paper>1.4 Research Framework of the Paper<a hidden class=anchor aria-hidden=true href=#14-research-framework-of-the-paper>#</a></h3><p>This paper conducts research on stock price prediction and quantitative strategies based on deep learning algorithms. The specific research framework of this paper is shown in Fig. 1:</p><figure class=align-center><a href=Research%20Framework.svg data-fancybox=gallery><img loading=lazy src=Research%20Framework.svg#center alt="Fig. 1. Research Framework."></a><figcaption><p>Fig. 1. Research Framework.</p></figcaption></figure><p>The specific research framework of this paper is as follows:</p><p>Chapter 1 is the introduction. This chapter first introduces the research significance and background of stock price prediction and quantitative strategy research. Then, it reviews the current research status, followed by an explanation of the innovations of this paper compared to existing research, and finally, a brief description of the research framework of this paper.</p><p>Chapter 2 is about related theoretical foundations. This chapter introduces the basic theories of deep learning models and quantitative stock selection involved in this research. The deep learning model section sequentially introduces four deep learning models: RNN, LSTM, GRU, and BiLSTM, with a focus on the internal structure of the LSTM model. The quantitative stock selection theory section sequentially introduces the Mean-Variance Model, Capital Asset Pricing Model, Arbitrage Pricing Theory, Multi-Factor Model, Fama-French Three-Factor Model, and Five-Factor Model. This section introduces the history of multi-factor quantitative stock selection from the development context of various financial theories and models.</p><p>Chapter 3 is a comparative study of LSTM, GRU, and BiLSTM in stock price prediction. This chapter first introduces the datasets of domestic and international stocks used in the experiment, and then performs data preprocessing steps of normalization and data partitioning. It then describes the network structures, model compilation, and hyperparameter settings of the LSTM, GRU, and BiLSTM models used in this chapter, and conducts experiments to obtain experimental results. Finally, the experimental results are analyzed, and a summary of this chapter is provided.</p><p>Chapter 4 is a study on a quantitative investment model based on LightGBM-BiLSTM. This chapter first outlines the experimental steps, and then introduces the stock data and factor data used in the experiment. Subsequently, factors are processed sequentially through missing value handling, outlier removal, factor standardization, and factor neutralization to obtain cleaned factors. Then, LightGBM and BiLSTM are used for factor selection and factor combination, respectively. Finally, a quantitative strategy is constructed based on the obtained model, and backtesting is performed on the quantitative strategy.</p><p>Chapter 5 is the conclusion and future directions. This chapter summarizes the main research content of this paper on stock price prediction and quantitative investment strategies. Based on the existing shortcomings of the current research, future research directions are proposed.</p><h2 id=chapter-2-related-theoretical-foundations>Chapter 2 Related Theoretical Foundations<a hidden class=anchor aria-hidden=true href=#chapter-2-related-theoretical-foundations>#</a></h2><h3 id=21-deep-learning-models>2.1 Deep Learning Models<a hidden class=anchor aria-hidden=true href=#21-deep-learning-models>#</a></h3><h4 id=211-rnn>2.1.1 RNN<a hidden class=anchor aria-hidden=true href=#211-rnn>#</a></h4><p>Recurrent Neural Networks (RNNs) are widely used for sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, hence many previous studies have used RNNs to predict stock prices. RNNs employ a very simple chain structure of repeating modules, such as a single tanh layer. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The emergence of LSTM solved this problem. Fig. 2 is an RNN structure diagram.</p><figure class=align-center><a href=RNN.png data-fancybox=gallery><img loading=lazy src=RNN.png#center alt="Fig. 2. RNN Structure Diagram. (Image source: Understanding LSTM Networks)"></a><figcaption><p>Fig. 2. RNN Structure Diagram. (Image source: <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>Understanding LSTM Networks</a>)</p></figcaption></figure><h4 id=212-lstm>2.1.2 LSTM<a hidden class=anchor aria-hidden=true href=#212-lstm>#</a></h4><p>Long Short-Term Memory (LSTM) networks are a special kind of RNN, capable of learning long-term dependencies. They were introduced by <a href=https://www.bioinf.jku.at/publications/older/2604.pdf>Hochreiter & Schmidhuber (1997)</a>$^{[7]}$ and have been refined and popularized by many in subsequent work. Due to their unique design structure, LSTMs are relatively insensitive to gap lengths and solve the gradient vanishing and explosion problems of traditional RNNs. Compared to traditional RNNs and other time series models like Hidden Markov Models (HMMs), LSTMs can better handle and predict important events in time series with very long intervals and delays. Therefore, LSTMs are widely used in machine translation and speech recognition.</p><p>LSTMs are explicitly designed to avoid long-term dependency problems. All recurrent neural networks have the form of a chain of repeating modules of neural networks, but LSTM improves the structure of RNN. Instead of a single neural network layer, LSTM uses a four-layer structure that interacts in a special way.</p><figure class=align-center><a href=LSTM.png data-fancybox=gallery><img loading=lazy src=LSTM.png#center alt="Fig. 3. LSTM Structure Diagram 1. (Image source: Understanding LSTM Networks)"></a><figcaption><p>Fig. 3. LSTM Structure Diagram 1. (Image source: <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>Understanding LSTM Networks</a>)</p></figcaption></figure><figure class=align-center><a href=LSTM2.png data-fancybox=gallery><img loading=lazy src=LSTM2.png#center alt="Fig. 4. LSTM Structure Diagram 2. (Image source: Understanding LSTM Networks)"></a><figcaption><p>Fig. 4. LSTM Structure Diagram 2. (Image source: <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>Understanding LSTM Networks</a>)</p></figcaption></figure><p>As shown in Fig. 3, black lines are used to represent the transmission of an output vector from one node to the input vector of another node. A neural network layer is a processing module with a $\sigma$ activation function or a tanh activation function; pointwise operation represents element-wise multiplication between vectors; vector transfer indicates the direction of information flow; concatenate and copy are represented by two black lines merging together and two black lines separating, respectively, to indicate information merging and information copying.</p><p>Below, we take LSTM as an example to explain its structure in detail.</p><ol><li><strong>Forget Gate</strong></li></ol><figure class=align-center><a href=forget_gate.png data-fancybox=gallery><img loading=lazy src=forget_gate.png#center alt="Fig. 5. Forget Gate Calculation (Image source: Understanding LSTM Networks)" width=70%></a><figcaption><p>Fig. 5. Forget Gate Calculation (Image source: <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>Understanding LSTM Networks</a>)</p></figcaption></figure>$$
f_{t} = \sigma\left(W_{f} \cdot \left[h_{t-1}, x_{t}\right] + b_{f}\right)
$$<p><strong>Parameter Description:</strong></p><ul><li>$h_{t-1}$: Output from the previous time step</li><li>$x_{t}$: Input at the current time step</li><li>$\sigma$: Sigmoid activation function</li><li>$W_{f}$: Weight matrix for the forget gate</li><li>$b_{f}$: Bias vector parameter for the forget gate</li></ul><p>The first step, as shown in Fig. 5, is to decide what information to discard from the cell state. This process is calculated by the sigmoid function to obtain the value of $f_{t}$ (the range of $f_{t}$ is between 0 and 1, where 0 means completely block, and 1 means completely pass through) to determine whether the cell state $C_{t-1}$ is passed through or partially passed through.</p><ol start=2><li><strong>Input Gate</strong></li></ol><figure class=align-center><a href=input_gate1.png data-fancybox=gallery><img loading=lazy src=input_gate1.png#center alt="Fig. 6. Input Gate Calculation (Image source: Understanding LSTM Networks)" width=70%></a><figcaption><p>Fig. 6. Input Gate Calculation (Image source: <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>Understanding LSTM Networks</a>)</p></figcaption></figure>$$
\begin{aligned}
i_{t} &= \sigma\left(W_{i} \cdot \left[h_{t-1}, x_{t}\right] + b_{i}\right) \\
\tilde{C}_{t} &= \tanh\left(W_{C} \cdot \left[h_{t-1}, x_{t}\right] + b_{C}\right)
\end{aligned}
$$<p><strong>Parameter Description:</strong></p><ul><li>$h_{t-1}$: Output from the previous time step</li><li>$x_{t}$: Input at the current time step</li><li>$\sigma$: Sigmoid activation function</li><li>$W_{i}$: Weight matrix for the input gate</li><li>$b_{i}$: Bias vector parameter for the input gate</li><li>$W_{C}$: Weight matrix for the cell state</li><li>$b_{C}$: Bias vector parameter for the cell state</li><li>$\tanh$: tanh activation function</li></ul><p>The second step, as shown in Fig. 6, uses a sigmoid function to calculate what information we want to store in the cell state. Next, a $\tanh$ layer creates a candidate vector $\tilde{C}_{t}$, which will be added to the cell state.</p><figure class=align-center><a href=input_gate2.png data-fancybox=gallery><img loading=lazy src=input_gate2.png#center alt="Fig. 7. Current Cell State Calculation (Image source: Understanding LSTM Networks)" width=70%></a><figcaption><p>Fig. 7. Current Cell State Calculation (Image source: <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>Understanding LSTM Networks</a>)</p></figcaption></figure>$$
C_{t} = f_{t} * C_{t-1} + i_{t} * \tilde{C}_{t}
$$<p><strong>Parameter Description:</strong></p><ul><li>$C_{t-1}$: Cell state from the previous time step</li><li>$\tilde{C}_{t}$: Temporary cell state</li><li>$i_{t}$: Value of the input gate</li><li>$f_{t}$: Value of the forget gate</li></ul><p>The third step, as shown in Fig. 7, calculates the current cell state $C_t$ by combining the effects of the forget gate and the input gate.</p><ul><li>The forget gate $f_t$ weights the previous cell state $C_{t-1}$ to discard unnecessary information.</li><li>The input gate $i_t$ weights the candidate cell state $\tilde{C}_t$ to decide how much new information to introduce.
Finally, the two parts are added together to update and derive the current cell state $C_t$.</li></ul><ol start=3><li><strong>Output Gate</strong></li></ol><figure class=align-center><a href=output_gate.png data-fancybox=gallery><img loading=lazy src=output_gate.png#center alt="Fig. 8. Output Gate Calculation (Image source: Understanding LSTM Networks)" width=70%></a><figcaption><p>Fig. 8. Output Gate Calculation (Image source: <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>Understanding LSTM Networks</a>)</p></figcaption></figure>$$
\begin{aligned}
o_{t} &= \sigma\left(W_{o} \cdot \left[h_{t-1}, x_{t}\right] + b_{o}\right) \\
h_{t} &= o_{t} * \tanh\left(C_{t}\right)
\end{aligned}
$$<p><strong>Parameter Description:</strong></p><ul><li>$o_{t}$: Value of the output gate</li><li>$\sigma$: Sigmoid activation function</li><li>$W_{o}$: Weight matrix for the output gate</li><li>$h_{t-1}$: Output from the previous time step</li><li>$x_{t}$: Input at the current time step</li><li>$b_{o}$: Bias vector parameter for the output gate</li><li>$h_{t}$: Output at the current time step</li><li>$\tanh$: tanh activation function</li><li>$C_{t}$: Cell state at the current time step</li></ul><p>The fourth step, as shown in Fig. 8, uses a sigmoid function to calculate the value of the output gate. Finally, the cell state $C_{t}$ at this time step is processed by a tanh activation function and multiplied by the value of the output gate $o_{t}$ to obtain the output $h_{t}$ at the current time step.</p><h4 id=213-gru>2.1.3 GRU<a hidden class=anchor aria-hidden=true href=#213-gru>#</a></h4><p><a href=https://arxiv.org/abs/1406.1078>K. Cho (2014)</a>$^{[8]}$ proposed the Gated Recurrent Unit (GRU). GRU is mainly simplified and adjusted based on LSTM, merging the original forget gate, input gate, and output gate of LSTM into an update gate and a reset gate. In addition, GRU also merges the cell state and hidden state, thereby reducing the complexity of the model while still achieving performance comparable to LSTM in some tasks.</p><p>This model can save a lot of time when the training dataset is relatively large and shows better performance on some smaller and less frequent datasets$^{[9][10]}$.</p><figure class=align-center><a href=GRU.png data-fancybox=gallery><img loading=lazy src=GRU.png#center alt="Fig. 9. GRU Structure Diagram (Image source: Understanding LSTM Networks)" width=70%></a><figcaption><p>Fig. 9. GRU Structure Diagram (Image source: <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>Understanding LSTM Networks</a>)</p></figcaption></figure>$$
\begin{aligned}
z_{t} &= \sigma\left(W_{z} \cdot \left[h_{t-1}, x_{t}\right]\right) \\
r_{t} &= \sigma\left(W_{r} \cdot \left[h_{t-1}, x_{t}\right]\right) \\
\tilde{h}_{t} &= \tanh\left(W \cdot \left[r_{t} * h_{t-1}, x_{t}\right]\right) \\
h_{t} &= \left(1 - z_{t}\right) * h_{t-1} + z_{t} * \tilde{h}_{t}
\end{aligned}
$$<p><strong>Parameter Description:</strong></p><ul><li>$z_{t}$: Value of the update gate</li><li>$r_{t}$: Value of the reset gate</li><li>$W_{z}$: Weight matrix for the update gate</li><li>$W_{r}$: Weight matrix for the reset gate</li><li>$\tilde{h}_{t}$: Temporary output</li></ul><h4 id=214-bilstm>2.1.4 BiLSTM<a hidden class=anchor aria-hidden=true href=#214-bilstm>#</a></h4><p>Bidirectional Long Short-Term Memory (BiLSTM) networks are formed by combining a forward LSTM and a backward LSTM. The BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions of time series data, enabling it to learn information with long-term dependencies. Compared to unidirectional LSTM, it can better consider the reverse impact of future data. Fig. 10 is a BiLSTM structure diagram.</p><figure class=align-center><a href=BiLSTM.png data-fancybox=gallery><img loading=lazy src=BiLSTM.png#center alt="Fig. 10. BiLSTM Structure Diagram. (Image source: Baeldung)"></a><figcaption><p>Fig. 10. BiLSTM Structure Diagram. (Image source: <a href=https://www.baeldung.com/cs/bidirectional-vs-unidirectional-lstm>Baeldung</a>)</p></figcaption></figure><h3 id=22-quantitative-stock-selection-theory>2.2 Quantitative Stock Selection Theory<a hidden class=anchor aria-hidden=true href=#22-quantitative-stock-selection-theory>#</a></h3><h4 id=221-mean-variance-model>2.2.1 Mean-Variance Model<a hidden class=anchor aria-hidden=true href=#221-mean-variance-model>#</a></h4><p>Quantitative stock selection strategies originated in the 1950s. <a href=https://www.jstor.org/stable/2975974>Markowitz (1952)</a>$^{[11]}$ proposed the Mean-Variance Model. This model not only laid the foundation for modern portfolio theory, quantifying investment risk, but also established a specific model describing risk and expected return. It broke away from the previous situation of qualitative analysis of investment portfolios without quantitative analysis, successfully introducing mathematical models into the field of financial investment.</p>$$
\begin{aligned}
\mathrm{E}\left(R_{p}\right) &= \sum_{i=1}^{n} w_{i} \mathrm{E}\left(R_{i}\right) \\
\sigma_{p}^{2} &= \sum_{i=1}^{n} \sum_{j=1}^{n} w_{i} w_{j} \operatorname{Cov}\left(R_{i}, R_{j}\right) = \sum_{i=1}^{n} \sum_{j=1}^{n} w_{i} w_{j} \sigma_{i} \sigma_{j} \rho_{ij} \\
\sigma_{i} &= \sqrt{\operatorname{Var}\left(R_{i}\right)}, \quad \rho_{ij} = \operatorname{Corr}\left(R_{i}, R_{j}\right)
\end{aligned}
$$$$
\min \sigma_{p}^{2} \quad \text{subject to} \quad \sum_{i=1}^{n} \mathrm{E}\left(R_{i}\right) w_{i} = \mu_{p}, \quad \sum_{i=1}^{n} w_{i} = 1
$$$$
\begin{aligned}
\Omega &= \begin{pmatrix}
\sigma_{11} & \cdots & \sigma_{1n} \\
\vdots & \ddots & \vdots \\
\sigma_{n1} & \cdots & \sigma_{nn}
\end{pmatrix} = \begin{pmatrix}
\operatorname{Var}\left(R_{1}\right) & \cdots & \operatorname{Cov}\left(R_{1}, R_{n}\right) \\
\vdots & \ddots & \vdots \\
\operatorname{Cov}\left(R_{n}, R_{1}\right) & \cdots & \operatorname{Var}\left(R_{n}\right)
\end{pmatrix} \\
\Omega^{-1} &= \begin{pmatrix}
v_{11} & \cdots & v_{1n} \\
\vdots & \ddots & \vdots \\
v_{n1} & \cdots & v_{nn}
\end{pmatrix} \\
w_{i} &= \frac{1}{D}\left(\mu_{p} \sum_{j=1}^{n} v_{ij}\left(C \mathrm{E}\left(R_{j}\right) - A\right) + \sum_{j=1}^{n} v_{ij}\left(B - A \mathrm{E}\left(R_{j}\right)\right)\right), \quad i = 1, \ldots, n
\end{aligned}
$$$$
\begin{aligned}
A &= \sum_{i=1}^{n} \sum_{j=1}^{n} v_{ij} \mathrm{E}\left(R_{j}\right), \quad B = \sum_{i=1}^{n} \sum_{j=1}^{n} v_{ij} \mathrm{E}\left(R_{i}\right) \mathrm{E}\left(R_{j}\right), \quad C = \sum_{i=1}^{n} \sum_{j=1}^{n} v_{ij}, \quad D = BC - A^{2} > 0 \\
\sigma_{p}^{2} &= \frac{C \mu_{p}^{2} - 2A \mu_{p} + B}{D}
\end{aligned}
$$<p><strong>Where:</strong></p><ul><li>$\mathrm{E}\left(R_{p}\right)$ and $\mu_{p}$ are the expected return of portfolio $p$</li><li>$\mathrm{E}\left(R_{i}\right)$ is the expected return of asset $i$</li><li>$\sigma_{i}$ is the standard deviation of asset $i$</li><li>$\sigma_{j}$ is the standard deviation of asset $j$</li><li>$w_{i}$ is the proportion of asset $i$ in the portfolio</li><li>$\sigma_{p}^{2}$ is the variance of portfolio $p$</li><li>$\rho_{ij}$ is the correlation coefficient between asset $i$ and asset $j$</li></ul><p>Using the above formulas$^{[12]}$, we can construct an investment portfolio that minimizes non-systematic risk under a certain expected rate of return.</p><h4 id=222-capital-asset-pricing-model>2.2.2 Capital Asset Pricing Model<a hidden class=anchor aria-hidden=true href=#222-capital-asset-pricing-model>#</a></h4><p><a href=https://www.jstor.org/stable/2977928>William Sharpe (1964)</a>$^{[13]}$, <a href=https://www.jstor.org/stable/1924119>John Lintner (1965)</a>$^{[14]}$, and <a href=https://www.jstor.org/stable/1910098>Jan Mossin (1966)</a>$^{[15]}$ proposed the Capital Asset Pricing Model (CAPM). This model posits that the expected return of an asset is related to its risk measure, the $\beta$ value. Through a simple linear relationship, this model links the expected return of an asset to market risk, making <a href=https://www.jstor.org/stable/2975974>Markowitz (1952)</a>$^{[11]}$&rsquo;s portfolio selection theory more relevant to the real world, and laying a theoretical foundation for the establishment of multi-factor stock selection models.</p><p>According to the Capital Asset Pricing Model, for a given asset $i$, the relationship between its expected return and the expected return of the market portfolio can be expressed as:</p>$$
E\left(r_{i}\right) = r_{f} + \beta_{im}\left[E\left(r_{m}\right) - r_{f}\right]
$$<p><strong>Where:</strong></p><ul><li>$E\left(r_{i}\right)$ is the expected return of asset $i$</li><li>$r_{f}$ is the risk-free rate</li><li>$\beta_{im}$ (Beta) is the systematic risk coefficient of asset $i$, $\beta_{im} = \frac{\operatorname{Cov}\left(r_{i}, r_{m}\right)}{\operatorname{Var}\left(r_{m}\right)}$</li><li>$E\left(r_{m}\right)$ is the expected return of the market portfolio $m$</li><li>$E\left(r_{m}\right) - r_{f}$ is the market risk premium</li></ul><h4 id=223-arbitrage-pricing-theory-and-multi-factor-model>2.2.3 Arbitrage Pricing Theory and Multi-Factor Model<a hidden class=anchor aria-hidden=true href=#223-arbitrage-pricing-theory-and-multi-factor-model>#</a></h4><p><a href=https://www.top1000funds.com/wp-content/uploads/2014/05/The-Arbitrage-Theory-of-Capital-Asset-Pricing.pdf>Ross (1976)</a>$^{[16]}$ proposed the Arbitrage Pricing Theory (APT). This theory argues that arbitrage behavior is the decisive factor in forming market equilibrium prices. By introducing a series of factors in the return formation process to construct linear correlations, it overcomes the limitations of the Capital Asset Pricing Model (CAPM) and provides important theoretical guidance for subsequent researchers.</p><p>Arbitrage Pricing Theory is considered the theoretical basis of the Multi-Factor Model (MFM), an important component of asset pricing models, and one of the cornerstones of asset pricing theory. The general form of the multi-factor model is:</p>$$
r_{j} = a_{j} + \lambda_{j1} f_{1} + \lambda_{j2} f_{2} + \cdots + \lambda_{jn} f_{n} + \delta_{j}
$$<p><strong>Where:</strong></p><ul><li>$r_{j}$ is the return of asset $j$</li><li>$a_{j}$ is a constant for asset $j$</li><li>$f_{n}$ is the systematic factor</li><li>$\lambda_{jn}$ is the factor loading</li><li>$\delta_{j}$ is the random error</li></ul><h4 id=224-fama-french-three-factor-model-and-five-factor-model>2.2.4 Fama-French Three-Factor Model and Five-Factor Model<a hidden class=anchor aria-hidden=true href=#224-fama-french-three-factor-model-and-five-factor-model>#</a></h4><p><a href=https://www.bauer.uh.edu/rsusmel/phd/Fama-French_JFE93.pdf>Fama (1992) and French (1992)</a>$^{[17]}$ used a combination of cross-sectional regression and time series methods and found that the $\beta$ value of the stock market could not explain the differences in returns of different stocks, while market capitalization, book-to-market ratio, and price-to-earnings ratio of listed companies could significantly explain the differences in stock returns. They argued that excess returns are compensation for risk factors not reflected by $\beta$ in CAPM, and thus proposed the Fama-French Three-Factor Model. The three factors are:</p><ul><li><p><strong>Market Risk Premium Factor</strong> (Market Risk Premium)</p><ul><li>Represents the overall systematic risk of the market, i.e., the difference between the expected return of the market portfolio and the risk-free rate.</li><li>Measures the excess return investors expect for bearing systematic risk (risk that cannot be eliminated through diversification).</li><li>Calculated as:
$$
\text{Market Risk Premium} = E(R_m) - R_f
$$
where $E(R_m)$ is the expected market return, and $R_f$ is the risk-free rate.</li></ul></li><li><p><strong>Size Factor</strong> (SMB: Small Minus Big)</p><ul><li>Represents the return difference between small-cap stocks and large-cap stocks.</li><li>Small-cap stocks are generally riskier, but historical data shows that their expected returns tend to be higher than those of large-cap stocks.</li><li>Calculated as:
$$
SMB = R_{\text{Small}} - R_{\text{Big}}
$$
reflecting the market&rsquo;s compensation for the additional risk premium of small-cap stocks.</li></ul></li><li><p><strong>Value Factor</strong> (HML: High Minus Low)</p><ul><li>Reflects the return difference between high book-to-market ratio stocks (i.e., &ldquo;value stocks&rdquo;) and low book-to-market ratio stocks (i.e., &ldquo;growth stocks&rdquo;).</li><li>Stocks with high book-to-market ratios are usually priced lower (undervalued by the market), but may achieve higher returns in the long run.</li><li>Calculated as:
$$
HML = R_{\text{High}} - R_{\text{Low}}
$$
Stocks with low book-to-market ratios may be overvalued due to overly optimistic market expectations.</li></ul></li></ul><p>This model concretizes the factors in the APT model and concludes that investing in small-cap, high-growth stocks has the characteristics of high risk and high return. The Fama-French Three-Factor Model is widely used in the analysis and practice of modern investment behavior.</p><p>Subsequently, <a href=https://tevgeniou.github.io/EquityRiskFactors/bibliography/FiveFactor.pdf>Fama (2015) and French (2015)</a>$^{[18]}$ extended the three-factor model, adding the following two factors:</p><ul><li><p><strong>Profitability Factor</strong> (RMW: Robust Minus Weak)</p><ul><li>Reflects the return difference between highly profitable companies and less profitable companies.</li><li>Companies with strong profitability (high ROE, net profit margin) are more likely to provide stable and higher returns.</li><li>Calculated as:
$$
RMW = R_{\text{Robust}} - R_{\text{Weak}}
$$</li></ul></li><li><p><strong>Investment Factor</strong> (CMA: Conservative Minus Aggressive)</p><ul><li>Reflects the return difference between conservative investment companies and aggressive investment companies.</li><li>Aggressive companies (rapidly expanding, high capital expenditure) are usually accompanied by greater operational risks, while conservative companies (relatively stable capital expenditure) show higher stability and returns.</li><li>Calculated as:
$$
CMA = R_{\text{Conservative}} - R_{\text{Aggressive}}
$$</li></ul></li></ul><p>The Fama-French Three-Factor Model formula is:</p>$$
R_i - R_f = \alpha_i + \beta_{i,m} \cdot (R_m - R_f) + \beta_{i,SMB} \cdot SMB + \beta_{i,HML} \cdot HML + \epsilon_i
$$<p>The Fama-French Five-Factor Model formula is:</p>$$
R_i - R_f = \alpha_i + \beta_{i,m} \cdot (R_m - R_f) + \beta_{i,SMB} \cdot SMB + \beta_{i,HML} \cdot HML + \beta_{i,RMW} \cdot RMW + \beta_{i,CMA} \cdot CMA + \epsilon_i
$$<p><strong>Where:</strong></p><ul><li>$R_i$: Expected return of stock $i$</li><li>$R_f$: Risk-free rate of return</li><li>$R_m$: Expected return of the market portfolio</li><li>$R_m - R_f$: Market risk premium factor</li><li>$SMB$: Return of small-cap stocks minus large-cap stocks</li><li>$HML$: Return of high book-to-market ratio stocks minus low book-to-market ratio stocks</li><li>$RMW$: Return of high profitability stocks minus low profitability stocks</li><li>$CMA$: Return of conservative investment stocks minus aggressive investment stocks</li><li>$\beta_{i,*}$: Sensitivity of stock $i$ to the corresponding factor</li><li>$\epsilon_i$: Regression residual</li></ul><h4 id=225-model-comparison-table>2.2.5 Model Comparison Table<a hidden class=anchor aria-hidden=true href=#225-model-comparison-table>#</a></h4><p>The following table summarizes the core content and factor sources of the <strong>Mean-Variance Model</strong>, <strong>Capital Asset Pricing Model (CAPM)</strong>, <strong>Arbitrage Pricing Theory (APT)</strong>, and <strong>Fama-French Models</strong>:</p><table><thead><tr><th style=text-align:left><strong>Model</strong></th><th style=text-align:left><strong>Core Content</strong></th><th style=text-align:left><strong>Factor Source</strong></th></tr></thead><tbody><tr><td style=text-align:left><strong>Mean-Variance Model</strong></td><td style=text-align:left>Foundation of portfolio theory, optimizes portfolio through expected returns and covariance matrix.</td><td style=text-align:left>Expected returns and covariance matrix of assets in portfolio</td></tr><tr><td style=text-align:left><strong>Capital Asset Pricing Model (CAPM)</strong></td><td style=text-align:left>Explains asset returns through market risk factor ($\beta$), laying the theoretical foundation for multi-factor models.</td><td style=text-align:left>Market factor $\beta$</td></tr><tr><td style=text-align:left><strong>Arbitrage Pricing Theory (APT)</strong></td><td style=text-align:left>Multi-factor framework, allows multiple economic variables to explain asset returns, e.g., inflation rate, interest rate.</td><td style=text-align:left>Multiple factors (macroeconomic variables, e.g., inflation rate, interest rate)</td></tr><tr><td style=text-align:left><strong>Fama-French Three-Factor Model</strong></td><td style=text-align:left>Adds size factor and book-to-market ratio factor, improving the explanatory power of asset returns.</td><td style=text-align:left>Market factor, SMB (size factor), HML (book-to-market ratio factor)</td></tr><tr><td style=text-align:left><strong>Fama-French Five-Factor Model</strong></td><td style=text-align:left>Adds profitability factor and investment factor on the basis of the three-factor model, further improving asset pricing model.</td><td style=text-align:left>Market factor, SMB, HML, RMW (profitability factor), CMA (investment factor)</td></tr></tbody></table><p>The following table summarizes the advantages and disadvantages of these models:</p><table><thead><tr><th style=text-align:left><strong>Model</strong></th><th style=text-align:left><strong>Advantages</strong></th><th style=text-align:left><strong>Disadvantages</strong></th></tr></thead><tbody><tr><td style=text-align:left><strong>Mean-Variance Model</strong></td><td style=text-align:left>Provides a systematic portfolio optimization method, laying the foundation for modern investment theory.</td><td style=text-align:left>Only optimizes for return and variance, does not explicitly specify the source of risk compensation.</td></tr><tr><td style=text-align:left><strong>Capital Asset Pricing Model (CAPM)</strong></td><td style=text-align:left>Simple and easy to use, explains return differences through market risk, provides a theoretical basis for multi-factor models.</td><td style=text-align:left>Assumes a single factor (market risk) determines returns, ignores other systematic risk factors.</td></tr><tr><td style=text-align:left><strong>Arbitrage Pricing Theory (APT)</strong></td><td style=text-align:left>Allows multiple factors to explain asset returns, reduces reliance on single-factor assumptions, more flexible.</td><td style=text-align:left>Does not specify concrete factors, lower operability, only provides a framework.</td></tr><tr><td style=text-align:left><strong>Fama-French Three-Factor Model</strong></td><td style=text-align:left>Significantly improves the explanatory power of asset returns by adding size factor and book-to-market ratio factor.</td><td style=text-align:left>Ignores other factors such as profitability and investment behavior.</td></tr><tr><td style=text-align:left><strong>Fama-French Five-Factor Model</strong></td><td style=text-align:left>More comprehensively captures key variables affecting asset returns by adding profitability factor and investment factor on the basis of the three-factor model.</td><td style=text-align:left>Higher model complexity, high data requirements, may still miss some potential factors.</td></tr></tbody></table><h2 id=chapter-3-comparative-study-of-lstm-gru-and-bilstm-in-stock-price-prediction>Chapter 3 Comparative Study of LSTM, GRU, and BiLSTM in Stock Price Prediction<a hidden class=anchor aria-hidden=true href=#chapter-3-comparative-study-of-lstm-gru-and-bilstm-in-stock-price-prediction>#</a></h2><h3 id=31-introduction-to-experimental-data>3.1 Introduction to Experimental Data<a hidden class=anchor aria-hidden=true href=#31-introduction-to-experimental-data>#</a></h3><p>Many scholars, both domestically and internationally, focus their research on their own country&rsquo;s stock indices, with relatively less research on individual stocks in different markets. Furthermore, few studies compare LSTM, GRU, and BiLSTM models directly. Therefore, this paper selects Shanghai Pudong Development Bank (SPD Bank, code 600000) in the domestic A-share market and International Business Machines Corporation (IBM) in the US stock market for research. This approach allows for a more accurate comparison of the three models used. For SPD Bank, stock data from January 1, 2008, to December 31, 2020, is used, totaling 3114 valid data points, sourced from the Tushare financial big data platform. We select six features from this dataset for the experiment: date, open price, close price, high price, low price, and volume. For the SPD Bank dataset, all five features except date (used as a time series index) are used as independent variables. For IBM, stock data from January 2, 1990, to November 15, 2018, is used, totaling 7278 valid data points, sourced from Yahoo Finance. We select seven features from this dataset for the experiment: date, open price, high price, low price, close price, adjusted close price (Adj Close), and volume. For the IBM dataset, all six features except date (used as a time series index) are used as independent variables. In this experiment, the closing price (close) is chosen as the variable to be predicted. Tables 3.1.1 and 3.1.2 show partial data from the two datasets.</p><h4 id=311-partial-display-of-spd-bank-dataset>3.1.1 Partial Display of SPD Bank Dataset<a hidden class=anchor aria-hidden=true href=#311-partial-display-of-spd-bank-dataset>#</a></h4><table><thead><tr><th>date</th><th>open</th><th>close</th><th>high</th><th>low</th><th>volume</th><th>code</th></tr></thead><tbody><tr><td>2008-01-02</td><td>9.007</td><td>9.101</td><td>9.356</td><td>8.805</td><td>131583.90</td><td>600000</td></tr><tr><td>2008-01-03</td><td>9.007</td><td>8.645</td><td>9.101</td><td>8.426</td><td>211346.56</td><td>600000</td></tr><tr><td>2008-01-04</td><td>8659</td><td>9.009</td><td>9.111</td><td>8.501</td><td>139249.67</td><td>600000</td></tr><tr><td>2008-01-07</td><td>8.970</td><td>9.515</td><td>9.593</td><td>8.953</td><td>228043.01</td><td>600000</td></tr><tr><td>2008-01-08</td><td>9.551</td><td>9.583</td><td>9.719</td><td>9.517</td><td>161255.31</td><td>600000</td></tr><tr><td>2008-01-09</td><td>9.583</td><td>9.663</td><td>9.772</td><td>9.432</td><td>102510.92</td><td>600000</td></tr><tr><td>2008-01-10</td><td>9.701</td><td>9.680</td><td>9.836</td><td>9.602</td><td>217966.25</td><td>600000</td></tr><tr><td>2008-01-11</td><td>9.670</td><td>10.467</td><td>10.532</td><td>9.670</td><td>231544.21</td><td>600000</td></tr><tr><td>2008-01-14</td><td>10.367</td><td>10.059</td><td>10.433</td><td>10.027</td><td>142918.39</td><td>600000</td></tr><tr><td>2008-01-15</td><td>10.142</td><td>10.051</td><td>10.389</td><td>10.006</td><td>161221.52</td><td>600000</td></tr></tbody></table><p><strong>Data Source</strong>: <a href=https://github.com/waditu/tushare>Tushare</a></p><h4 id=312-partial-display-of-ibm-dataset>3.1.2 Partial Display of IBM Dataset<a hidden class=anchor aria-hidden=true href=#312-partial-display-of-ibm-dataset>#</a></h4><table><thead><tr><th>Date</th><th>Open</th><th>High</th><th>Low</th><th>Close</th><th>Adj Close</th><th>Volume</th></tr></thead><tbody><tr><td>1990-01-02</td><td>23.6875</td><td>24.5313</td><td>23.6250</td><td>24.5000</td><td>6.590755</td><td>7041600</td></tr><tr><td>1990-01-03</td><td>24.6875</td><td>24.8750</td><td>24.5938</td><td>24.7188</td><td>6.649599</td><td>9464000</td></tr><tr><td>1990-01-04</td><td>24.7500</td><td>25.0938</td><td>24.7188</td><td>25.0000</td><td>6.725261</td><td>9674800</td></tr><tr><td>1990-01-05</td><td>24.9688</td><td>25.4063</td><td>24.8750</td><td>24.9375</td><td>6.708448</td><td>7570000</td></tr><tr><td>1990-01-08</td><td>24.8125</td><td>25.2188</td><td>24.8125</td><td>25.0938</td><td>6.750481</td><td>4625200</td></tr><tr><td>1990-01-09</td><td>25.1250</td><td>25.3125</td><td>24.8438</td><td>24.8438</td><td>6.683229</td><td>7048000</td></tr><tr><td>1990-01-10</td><td>24.8750</td><td>25.0000</td><td>24.6563</td><td>24.7500</td><td>6.658009</td><td>5945600</td></tr><tr><td>1990-01-11</td><td>24.8750</td><td>25.0938</td><td>24.8438</td><td>24.9688</td><td>6.716855</td><td>5905600</td></tr><tr><td>1990-01-12</td><td>24.6563</td><td>24.8125</td><td>24.4063</td><td>24.4688</td><td>6.582347</td><td>5390800</td></tr><tr><td>1990-01-15</td><td>24.4063</td><td>24.5938</td><td>24.3125</td><td>24.5313</td><td>6.599163</td><td>4035600</td></tr></tbody></table><p><strong>Data Source</strong>: <a href=https://finance.yahoo.com/quote/IBM/history/>Yahoo Finance</a></p><h3 id=32-experimental-data-preprocessing>3.2 Experimental Data Preprocessing<a hidden class=anchor aria-hidden=true href=#32-experimental-data-preprocessing>#</a></h3><h4 id=321-data-normalization>3.2.1 Data Normalization<a hidden class=anchor aria-hidden=true href=#321-data-normalization>#</a></h4><p>In the experiment, there are differences in units and magnitudes among various features. For example, the magnitude difference between stock prices and trading volume is huge, which will affect the final prediction results of our experiment. Therefore, we use the <code>MinMaxScaler</code> method from the <code>sklearn.preprocessing</code> library to scale the features of the data to between 0 and 1. This can not only improve the model accuracy but also increase the model convergence speed. The normalization formula is:</p>$$
x^{\prime}=\frac{x-\min (x)}{\max (x)-\min (x)}
$$<p>where $x^{\prime}$ is the normalized data, $x$ is the original data, $\min (x)$ is the minimum value of the original dataset, and $\max (x)$ is the maximum value of the original dataset. After obtaining the prediction results in our experimental process, we also need to denormalize the data before we can perform stock price prediction and model evaluation.</p><h4 id=322-data-partitioning>3.2.2 Data Partitioning<a hidden class=anchor aria-hidden=true href=#322-data-partitioning>#</a></h4><p>Here, the entire experimental datasets of SPD Bank and IBM are input respectively, and the timestep of the recurrent kernel is set to 60 for both, with the number of input features per timestep being 5 and 6, respectively. This allows inputting data from the previous 60 trading days to predict the closing price on the 61st day. This makes our dataset meet the input requirements of the three neural network models to be compared later, which are the number of samples, the number of recurrent kernel unfolding steps, and the number of input features per timestep. Then, we divide the normalized SPD Bank dataset into training, validation, and test sets in a ratio of 2488:311:255. The normalized IBM dataset is divided into training, validation, and test sets in a ratio of 6550:364:304. We partition out a validation set here to facilitate adjusting the hyperparameters of the models to optimize each model before comparison.</p><h3 id=33-model-network-structure>3.3 Model Network Structure<a hidden class=anchor aria-hidden=true href=#33-model-network-structure>#</a></h3><p>The network structures of each model set in this paper through a large number of repeated experiments are shown in the table below. The default tanh and linear activation functions of recurrent neural networks are used between layers, and Dropout is added to prevent overfitting. The dropout rate is set to 0.2. The number of neurons in each recurrent layer of LSTM and GRU is 50, and the number of neurons in the recurrent layer of BiLSTM is 100. Each model of LSTM, GRU, and BiLSTM adopts four layers of LSTM, GRU, BiLSTM, and one fully connected layer, with a Dropout set between each network layer.</p><h4 id=331-lstm-network-structure-for-ibm>3.3.1 LSTM Network Structure for IBM<a hidden class=anchor aria-hidden=true href=#331-lstm-network-structure-for-ibm>#</a></h4><table><thead><tr><th>Layer(type)</th><th>Output Shape</th><th>Param#</th></tr></thead><tbody><tr><td>lstm_1 (LSTM)</td><td>(None, 60, 50)</td><td>11400</td></tr><tr><td>dropout_1 (Dropout)</td><td>(None, 60, 50)</td><td>0</td></tr><tr><td>lstm_2 (LSTM)</td><td>(None, 60, 50)</td><td>20200</td></tr><tr><td>dropout_2 (Dropout)</td><td>(None, 60, 50)</td><td>0</td></tr><tr><td>lstm_3 (LSTM)</td><td>(None, 60, 50)</td><td>20200</td></tr><tr><td>dropout_3 (Dropout)</td><td>(None, 60, 50)</td><td>0</td></tr><tr><td>lstm_4 (LSTM)</td><td>(None, 50)</td><td>20200</td></tr><tr><td>dropout_4 (Dropout)</td><td>(None, 50)</td><td>0</td></tr><tr><td>dense_1 (Dense)</td><td>(None, 1)</td><td>51</td></tr></tbody></table><p><strong>Total params</strong>: 72,051<br><strong>Trainable params</strong>: 72,051<br><strong>Non-trainable params</strong>: 0</p><hr><h4 id=332-gru-network-structure-for-ibm>3.3.2 GRU Network Structure for IBM<a hidden class=anchor aria-hidden=true href=#332-gru-network-structure-for-ibm>#</a></h4><table><thead><tr><th>Layer(type)</th><th>Output Shape</th><th>Param#</th></tr></thead><tbody><tr><td>gru_1 (GRU)</td><td>(None, 60, 50)</td><td>8550</td></tr><tr><td>dropout_1 (Dropout)</td><td>(None, 60, 50)</td><td>0</td></tr><tr><td>gru_2 (GRU)</td><td>(None, 60, 50)</td><td>15150</td></tr><tr><td>dropout_2 (Dropout)</td><td>(None, 60, 50)</td><td>0</td></tr><tr><td>gru_3 (GRU)</td><td>(None, 60, 50)</td><td>15150</td></tr><tr><td>dropout_3 (Dropout)</td><td>(None, 60, 50)</td><td>0</td></tr><tr><td>gru_4 (GRU)</td><td>(None, 50)</td><td>15150</td></tr><tr><td>dropout_4 (Dropout)</td><td>(None, 50)</td><td>0</td></tr><tr><td>dense_1 (Dense)</td><td>(None, 1)</td><td>51</td></tr></tbody></table><p><strong>Total params</strong>: 54,051<br><strong>Trainable params</strong>: 54,051<br><strong>Non-trainable params</strong>: 0</p><hr><h4 id=333-bilstm-network-structure-for-ibm>3.3.3 BiLSTM Network Structure for IBM<a hidden class=anchor aria-hidden=true href=#333-bilstm-network-structure-for-ibm>#</a></h4><table><thead><tr><th>Layer(type)</th><th>Output Shape</th><th>Param#</th></tr></thead><tbody><tr><td>bidirectional_1 (Bidirection)</td><td>(None, 60, 100)</td><td>22800</td></tr><tr><td>dropout_1 (Dropout)</td><td>(None, 60, 100)</td><td>0</td></tr><tr><td>bidirectional_2 (Bidirection)</td><td>(None, 60, 100)</td><td>60400</td></tr><tr><td>dropout_2 (Dropout)</td><td>(None, 60, 100)</td><td>0</td></tr><tr><td>bidirectional_3 (Bidirection)</td><td>(None, 60, 100)</td><td>60400</td></tr><tr><td>dropout_3 (Dropout)</td><td>(None, 60, 100)</td><td>0</td></tr><tr><td>bidirectional_4 (Bidirection)</td><td>(None, 100)</td><td>60400</td></tr><tr><td>dropout_4 (Dropout)</td><td>(None, 100)</td><td>0</td></tr><tr><td>dense_1 (Dense)</td><td>(None, 1)</td><td>101</td></tr></tbody></table><p><strong>Total params</strong>: 204,101<br><strong>Trainable params</strong>: 204,101<br><strong>Non-trainable params</strong>: 0</p><h3 id=34-model-compilation-and-hyperparameter-settings>3.4 Model Compilation and Hyperparameter Settings<a hidden class=anchor aria-hidden=true href=#34-model-compilation-and-hyperparameter-settings>#</a></h3><p>In this paper, after continuous hyperparameter tuning with the goal of minimizing the loss function on the validation set, the following hyperparameters are selected for the three models of SPD Bank: <code>epochs=100</code>, <code>batch_size=32</code>; and for the three models of IBM: <code>epochs=50</code>, <code>batch_size=32</code>. The optimizer used is Adaptive Moment Estimation <a href=https://arxiv.org/abs/1412.6980>(Adam)</a>$^{[19]}$. The default values in its <code>keras</code> package are used, i.e., <code>lr=0.001</code>, <code>beta_1=0.9</code>, <code>beta_2=0.999</code>, <code>epsilon=1e-08</code>, and <code>decay=0.0</code>. The loss function is Mean Squared Error (MSE).</p><p><strong>Parameter Explanation:</strong></p><ul><li><code>lr</code>: Learning rate</li><li><code>beta_1</code>: Exponential decay rate for the first moment estimate</li><li><code>beta_2</code>: Exponential decay rate for the second moment estimate</li><li><code>epsilon</code>: Fuzz factor</li><li><code>decay</code>: Learning rate decay value after each update</li></ul><h3 id=35-experimental-results-and-analysis>3.5 Experimental Results and Analysis<a hidden class=anchor aria-hidden=true href=#35-experimental-results-and-analysis>#</a></h3><p>First, let&rsquo;s briefly introduce the evaluation metrics used for the models. The calculation formulas are as follows:</p><ol><li><strong>Mean Squared Error (MSE)</strong>:</li></ol>$$
M S E=\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-\hat{Y}_{i}\right)^{2}
$$<ol start=2><li><strong>Root Mean Squared Error (RMSE)</strong>:</li></ol>$$
R M S E=\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-\hat{Y}_{i}\right)^{2}}
$$<ol start=3><li><strong>Mean Absolute Error (MAE)</strong>:</li></ol>$$
M A E=\frac{1}{n} \sum_{i=1}^{n}\left|Y_{i}-\hat{Y}_{i}\right|
$$<ol start=4><li><strong>\( R^2 \) (R Squared)</strong>:</li></ol>$$
\begin{gathered}
\bar{Y}=\frac{1}{n} \sum_{i=1}^{n} Y_{i} \\
R^{2}=1-\frac{\sum_{i=1}^{n}\left(Y_{i}-\hat{Y}_{i}\right)^{2}}{\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}
\end{gathered}
$$<p>Where: $n$ is the number of samples, $Y_{i}$ is the actual closing price of the stock, $\hat{Y}_{i}$ is the predicted closing price of the stock, and $\bar{Y}$ is the average closing price of the stock. The smaller the MSE, RMSE, and MAE, the more accurate the model. The larger the \( R^2 \), the better the goodness of fit of the model coefficients.</p><h4 id=351-experimental-results-for-spd-bank>3.5.1 Experimental Results for SPD Bank<a hidden class=anchor aria-hidden=true href=#351-experimental-results-for-spd-bank>#</a></h4><table><thead><tr><th></th><th>LSTM</th><th>GRU</th><th>BiLSTM</th></tr></thead><tbody><tr><td><strong>MSE</strong></td><td>0.059781</td><td>0.069323</td><td>0.056454</td></tr><tr><td><strong>RMSE</strong></td><td>0.244501</td><td>0.263292</td><td>0.237601</td></tr><tr><td><strong>MAE</strong></td><td>0.186541</td><td>0.202665</td><td>0.154289</td></tr><tr><td><strong>R-squared</strong></td><td>0.91788</td><td>0.896214</td><td>0.929643</td></tr></tbody></table><p>Comparing the evaluation metrics of the three models, we can find that on the SPD Bank test set, the MSE, RMSE, and MAE of the BiLSTM model are smaller than those of the LSTM and GRU models, while the R-Squared is larger than those of the LSTM and GRU models. By comparing RMSE, we find that BiLSTM has a 2.90% performance improvement over LSTM and a 10.81% performance improvement over GRU on the validation set.</p><h4 id=352-experimental-results-for-ibm>3.5.2 Experimental Results for IBM<a hidden class=anchor aria-hidden=true href=#352-experimental-results-for-ibm>#</a></h4><table><thead><tr><th></th><th>LSTM</th><th>GRU</th><th>BiLSTM</th></tr></thead><tbody><tr><td><strong>MSE</strong></td><td>18.01311</td><td>12.938584</td><td>11.057501</td></tr><tr><td><strong>RMSE</strong></td><td>4.244186</td><td>3.597024</td><td>3.325282</td></tr><tr><td><strong>MAE</strong></td><td>3.793223</td><td>3.069033</td><td>2.732075</td></tr><tr><td><strong>R-squared</strong></td><td>0.789453</td><td>0.851939</td><td>0.883334</td></tr></tbody></table><p>Comparing the evaluation metrics of the three models, we can find that on the IBM test set, the MSE, RMSE, and MAE of the BiLSTM model are smaller than those of the LSTM and GRU models, while the R-Squared is larger than those of the LSTM and GRU models. By comparing RMSE, we find that BiLSTM has a 27.63% performance improvement over LSTM and an 8.17% performance improvement over GRU on the validation set.</p><h3 id=36-chapter-summary>3.6 Chapter Summary<a hidden class=anchor aria-hidden=true href=#36-chapter-summary>#</a></h3><p>This chapter first introduced the SPD Bank and IBM datasets and the features used in the experiment. Then, it performed preprocessing steps of data normalization and data partitioning on the datasets. It also detailed the network structures and hyperparameters of the LSTM, GRU, and BiLSTM models used in the experiment. Finally, it obtained the loss function images and a series of fitting graphs for each model. By comparing multiple evaluation metrics and fitting images of the models, it is concluded that the BiLSTM model can better predict stock prices, laying a foundation for our next chapter&rsquo;s research on the LightGBM-BiLSTM quantitative investment strategy.</p><hr><h2 id=chapter-4-research-on-quantitative-investment-model-based-on-lightgbm-bilstm>Chapter 4 Research on Quantitative Investment Model Based on LightGBM-BiLSTM<a hidden class=anchor aria-hidden=true href=#chapter-4-research-on-quantitative-investment-model-based-on-lightgbm-bilstm>#</a></h2><h3 id=41-experimental-steps>4.1 Experimental Steps<a hidden class=anchor aria-hidden=true href=#41-experimental-steps>#</a></h3><figure class=align-center><a href=LightGBM_BiLSTM_Flow.png data-fancybox=gallery><img loading=lazy src=LightGBM_BiLSTM_Flow.png#center alt="Fig. 11. LightGBM-BiLSTM Diagram."></a><figcaption><p>Fig. 11. LightGBM-BiLSTM Diagram.</p></figcaption></figure><p>As shown in Fig. 11, this experiment first selects 50 factors from the factor library. Then, it performs factor cleaning steps of outlier removal, standardization, and missing value imputation on the factors sequentially. Next, the LightGBM model is used for factor selection, and the top ten factors with the highest importance are selected as the factors for this cross-sectional selection. Subsequently, a BiLSTM model is used to establish a multi-factor model, and finally, backtesting analysis is performed.</p><h3 id=42-experimental-data>4.2 Experimental Data<a hidden class=anchor aria-hidden=true href=#42-experimental-data>#</a></h3><p>The market data used in this paper comes from <a href=https://github.com/waditu/tushare>Tushare</a>. The specific features of the dataset are shown in the table below.</p><h4 id=421-features-included-in-the-stock-dataset>4.2.1 Features Included in the Stock Dataset<a hidden class=anchor aria-hidden=true href=#421-features-included-in-the-stock-dataset>#</a></h4><table><thead><tr><th style=text-align:left>Name</th><th style=text-align:left>Type</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left>ts_code</td><td style=text-align:left>str</td><td style=text-align:left>Stock code</td></tr><tr><td style=text-align:left>trade_date</td><td style=text-align:left>str</td><td style=text-align:left>Trading date</td></tr><tr><td style=text-align:left>open</td><td style=text-align:left>float</td><td style=text-align:left>Open price</td></tr><tr><td style=text-align:left>high</td><td style=text-align:left>float</td><td style=text-align:left>High price</td></tr><tr><td style=text-align:left>low</td><td style=text-align:left>float</td><td style=text-align:left>Low price</td></tr><tr><td style=text-align:left>close</td><td style=text-align:left>float</td><td style=text-align:left>Close price</td></tr><tr><td style=text-align:left>pre_close</td><td style=text-align:left>float</td><td style=text-align:left>Previous close price</td></tr><tr><td style=text-align:left>change</td><td style=text-align:left>float</td><td style=text-align:left>Change amount</td></tr><tr><td style=text-align:left>pct_chg</td><td style=text-align:left>float</td><td style=text-align:left>Change percentage (unadjusted)</td></tr><tr><td style=text-align:left>vol</td><td style=text-align:left>float</td><td style=text-align:left>Volume (in hands)</td></tr><tr><td style=text-align:left>amount</td><td style=text-align:left>float</td><td style=text-align:left>Turnover (in thousands of CNY)</td></tr></tbody></table><p>The A-share market-wide daily dataset contains 5,872,309 rows of data, i.e., 5,872,309 samples. As shown in Table 4.2.1, the A-share market-wide daily dataset has the following 11 features, in order: stock code (ts_code), trading date (trade_date), open price (open), high price (high), low price (low), close price (close), previous close price (pre_close), change amount (change), turnover rate (turnover_rate), turnover amount (amount), total market value (total_mv), and adjustment factor (adj_factor).</p><h4 id=422-partial-display-of-a-share-market-wide-daily-dataset>4.2.2 Partial Display of A-Share Market-Wide Daily Dataset<a hidden class=anchor aria-hidden=true href=#422-partial-display-of-a-share-market-wide-daily-dataset>#</a></h4><table><thead><tr><th style=text-align:left>ts_code</th><th style=text-align:left>trade_date</th><th style=text-align:left>open</th><th style=text-align:left>high</th><th style=text-align:left>low</th><th style=text-align:left>close</th><th style=text-align:left>pre_close</th><th style=text-align:left>change</th><th style=text-align:left>vol</th><th style=text-align:left>amount</th></tr></thead><tbody><tr><td style=text-align:left>600613.SH</td><td style=text-align:left>20120104</td><td style=text-align:left>8.20</td><td style=text-align:left>8.20</td><td style=text-align:left>7.84</td><td style=text-align:left>7.86</td><td style=text-align:left>8.16</td><td style=text-align:left>-0.30</td><td style=text-align:left>4762.98</td><td style=text-align:left>3854.1000</td></tr><tr><td style=text-align:left>600690.SH</td><td style=text-align:left>20120104</td><td style=text-align:left>9.00</td><td style=text-align:left>9.17</td><td style=text-align:left>8.78</td><td style=text-align:left>8.78</td><td style=text-align:left>8.93</td><td style=text-align:left>-0.15</td><td style=text-align:left>142288.41</td><td style=text-align:left>127992.6050</td></tr><tr><td style=text-align:left>300277.SZ</td><td style=text-align:left>20120104</td><td style=text-align:left>22.90</td><td style=text-align:left>22.98</td><td style=text-align:left>20.81</td><td style=text-align:left>20.88</td><td style=text-align:left>22.68</td><td style=text-align:left>-1.80</td><td style=text-align:left>12212.39</td><td style=text-align:left>26797.1370</td></tr><tr><td style=text-align:left>002403.SZ</td><td style=text-align:left>20120104</td><td style=text-align:left>8.87</td><td style=text-align:left>8.90</td><td style=text-align:left>8.40</td><td style=text-align:left>8.40</td><td style=text-align:left>8.84</td><td style=text-align:left>-0.441</td><td style=text-align:left>10331.97</td><td style=text-align:left>9013.4317</td></tr><tr><td style=text-align:left>300179.SZ</td><td style=text-align:left>20120104</td><td style=text-align:left>19.99</td><td style=text-align:left>20.32</td><td style=text-align:left>19.20</td><td style=text-align:left>19.50</td><td style=text-align:left>19.96</td><td style=text-align:left>-0.46</td><td style=text-align:left>1532.31</td><td style=text-align:left>3008.0594</td></tr><tr><td style=text-align:left>600000.SH</td><td style=text-align:left>20120104</td><td style=text-align:left>8.54</td><td style=text-align:left>8.56</td><td style=text-align:left>8.39</td><td style=text-align:left>8.41</td><td style=text-align:left>8.49</td><td style=text-align:left>-0.08</td><td style=text-align:left>342013.79</td><td style=text-align:left>290229.5510</td></tr><tr><td style=text-align:left>300282.SZ</td><td style=text-align:left>20120104</td><td style=text-align:left>22.90</td><td style=text-align:left>23.33</td><td style=text-align:left>21.02</td><td style=text-align:left>21.02</td><td style=text-align:left>23.35</td><td style=text-align:left>-2.33</td><td style=text-align:left>38408.60</td><td style=text-align:left>86216.2356</td></tr><tr><td style=text-align:left>002319.SZ</td><td style=text-align:left>20120104</td><td style=text-align:left>9.74</td><td style=text-align:left>9.95</td><td style=text-align:left>9.38</td><td style=text-align:left>9.41</td><td style=text-align:left>9.73</td><td style=text-align:left>-0.32</td><td style=text-align:left>4809.74</td><td style=text-align:left>4671.4803</td></tr><tr><td style=text-align:left>601991.SH</td><td style=text-align:left>20120104</td><td style=text-align:left>5.17</td><td style=text-align:left>5.39</td><td style=text-align:left>5.12</td><td style=text-align:left>5.25</td><td style=text-align:left>5.16</td><td style=text-align:left>0.09</td><td style=text-align:left>145268.38</td><td style=text-align:left>76547.7490</td></tr><tr><td style=text-align:left>000780.SZ</td><td style=text-align:left>20120104</td><td style=text-align:left>10.42</td><td style=text-align:left>10.49</td><td style=text-align:left>10.00</td><td style=text-align:left>10.00</td><td style=text-align:left>10.30</td><td style=text-align:left>-0.30</td><td style=text-align:left>20362.30</td><td style=text-align:left>20830.1761</td></tr></tbody></table><p><strong>[5872309 rows x 11 columns]</strong></p><p>The CSI All Share daily dataset contains 5,057 rows of data, i.e., 5,057 samples. As shown in Table 4.2.2, the CSI All Share daily dataset has the following 7 features, in order: trading date (trade_date), open price (open), high price (high), low price (low), close price (close), volume (volume), and previous close price (pre_close).</p><h4 id=423-partial-display-of-csi-all-share-daily-dataset>4.2.3 Partial Display of CSI All Share Daily Dataset<a hidden class=anchor aria-hidden=true href=#423-partial-display-of-csi-all-share-daily-dataset>#</a></h4><table><thead><tr><th style=text-align:left>trade_date</th><th style=text-align:left>open</th><th style=text-align:left>high</th><th style=text-align:left>low</th><th style=text-align:left>close</th><th style=text-align:left>volume</th><th style=text-align:left>pre_close</th></tr></thead><tbody><tr><td style=text-align:left>2006-11-24</td><td style=text-align:left>1564.3560</td><td style=text-align:left>1579.3470</td><td style=text-align:left>1549.9790</td><td style=text-align:left>1576.1530</td><td style=text-align:left>7.521819e+09</td><td style=text-align:left>1567.0910</td></tr><tr><td style=text-align:left>2006-11-27</td><td style=text-align:left>1574.1130</td><td style=text-align:left>1598.7440</td><td style=text-align:left>1574.1130</td><td style=text-align:left>1598.7440</td><td style=text-align:left>7.212786e+09</td><td style=text-align:left>1581.1530</td></tr><tr><td style=text-align:left>2006-11-28</td><td style=text-align:left>1597.7200</td><td style=text-align:left>1604.7190</td><td style=text-align:left>1585.3620</td><td style=text-align:left>1596.8400</td><td style=text-align:left>7.025637e+09</td><td style=text-align:left>1598.7440</td></tr><tr><td style=text-align:left>2006-11-29</td><td style=text-align:left>1575.3030</td><td style=text-align:left>1620.2870</td><td style=text-align:left>1575.3030</td><td style=text-align:left>1617.9880</td><td style=text-align:left>7.250354e+09</td><td style=text-align:left>1596.8400</td></tr><tr><td style=text-align:left>2006-11-30</td><td style=text-align:left>1621.4280</td><td style=text-align:left>1657.3230</td><td style=text-align:left>1621.4280</td><td style=text-align:left>1657.3230</td><td style=text-align:left>9.656888e+09</td><td style=text-align:left>1617.9880</td></tr><tr><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>2020-11-11</td><td style=text-align:left>5477.8870</td><td style=text-align:left>5493.5867</td><td style=text-align:left>5422.9110</td><td style=text-align:left>5425.8017</td><td style=text-align:left>5.604086e+10</td><td style=text-align:left>5494.1042</td></tr><tr><td style=text-align:left>2020-11-12</td><td style=text-align:left>5439.2296</td><td style=text-align:left>5454.3452</td><td style=text-align:left>5413.9659</td><td style=text-align:left>5435.1379</td><td style=text-align:left>4.594251e+10</td><td style=text-align:left>5425.8017</td></tr><tr><td style=text-align:left>2020-11-13</td><td style=text-align:left>5418.2953</td><td style=text-align:left>5418.3523</td><td style=text-align:left>5364.2031</td><td style=text-align:left>5402.7702</td><td style=text-align:left>4.688916e+10</td><td style=text-align:left>5435.1379</td></tr><tr><td style=text-align:left>2020-11-16</td><td style=text-align:left>5422.3565</td><td style=text-align:left>5456.7264</td><td style=text-align:left>5391.9232</td><td style=text-align:left>5456.7264</td><td style=text-align:left>5.593672e+10</td><td style=text-align:left>5402.7702</td></tr><tr><td style=text-align:left>2020-11-17</td><td style=text-align:left>5454.0696</td><td style=text-align:left>5454.0696</td><td style=text-align:left>5395.6052</td><td style=text-align:left>5428.0765</td><td style=text-align:left>5.857009e+10</td><td style=text-align:left>5456.7264</td></tr></tbody></table><p><strong>[5057 rows x 7 columns]</strong></p><p>Table 4.2.4 below shows partial data of the original factors. After sequentially going through the four factor cleaning steps of missing value imputation, outlier removal, factor standardization, and factor neutralization mentioned above, partial data of the cleaned factors are obtained as shown in Table 4.2.5.</p><h4 id=424-original-factor-data>4.2.4 Original Factor Data<a hidden class=anchor aria-hidden=true href=#424-original-factor-data>#</a></h4><table><thead><tr><th style=text-align:left>trade_date</th><th style=text-align:left>sec_code</th><th style=text-align:left>ret</th><th style=text-align:left>factor_0</th><th style=text-align:left>factor_1</th><th style=text-align:left>factor_2</th><th style=text-align:left>factor_3</th><th style=text-align:left>factor_4</th><th style=text-align:left>factor_5</th><th style=text-align:left>factor_6</th><th style=text-align:left>&mldr;</th></tr></thead><tbody><tr><td style=text-align:left>2005-01-04</td><td style=text-align:left>600874.SH</td><td style=text-align:left>0.001684</td><td style=text-align:left>NaN</td><td style=text-align:left>9.445412</td><td style=text-align:left>9.445412</td><td style=text-align:left>9.445408</td><td style=text-align:left>-1.00</td><td style=text-align:left>NaN</td><td style=text-align:left>12651.124023</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>2005-01-04</td><td style=text-align:left>000411.SZ</td><td style=text-align:left>0.021073</td><td style=text-align:left>NaN</td><td style=text-align:left>5.971262</td><td style=text-align:left>5.971262</td><td style=text-align:left>5.971313</td><td style=text-align:left>0.38</td><td style=text-align:left>NaN</td><td style=text-align:left>392.124298</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>2005-01-04</td><td style=text-align:left>000979.SZ</td><td style=text-align:left>0.021207</td><td style=text-align:left>NaN</td><td style=text-align:left>6.768918</td><td style=text-align:left>6.768918</td><td style=text-align:left>6.768815</td><td style=text-align:left>-1.45</td><td style=text-align:left>NaN</td><td style=text-align:left>870.587585</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>2005-01-04</td><td style=text-align:left>000498.SZ</td><td style=text-align:left>0.030220</td><td style=text-align:left>NaN</td><td style=text-align:left>8.852752</td><td style=text-align:left>8.852752</td><td style=text-align:left>8.852755</td><td style=text-align:left>0.55</td><td style=text-align:left>NaN</td><td style=text-align:left>6994.011719</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>2005-01-04</td><td style=text-align:left>600631.SH</td><td style=text-align:left>0.015699</td><td style=text-align:left>NaN</td><td style=text-align:left>9.589897</td><td style=text-align:left>9.589897</td><td style=text-align:left>9.589889</td><td style=text-align:left>-1.70</td><td style=text-align:left>NaN</td><td style=text-align:left>14616.806641</td><td style=text-align:left>&mldr;</td></tr></tbody></table><h4 id=425-cleaned-factor-data>4.2.5 Cleaned Factor Data<a hidden class=anchor aria-hidden=true href=#425-cleaned-factor-data>#</a></h4><table><thead><tr><th style=text-align:left>sec_code</th><th style=text-align:left>trade_date</th><th style=text-align:left>ret</th><th style=text-align:left>factor_0</th><th style=text-align:left>factor_1</th><th style=text-align:left>factor_2</th><th style=text-align:left>factor_3</th><th style=text-align:left>factor_4</th><th style=text-align:left>factor_5</th><th style=text-align:left>factor_6</th><th style=text-align:left>&mldr;</th></tr></thead><tbody><tr><td style=text-align:left>000001.SZ</td><td style=text-align:left>2005-01-04</td><td style=text-align:left>-1.58653</td><td style=text-align:left>0.01545</td><td style=text-align:left>1.38306</td><td style=text-align:left>1.38306</td><td style=text-align:left>1.38306</td><td style=text-align:left>0.13392</td><td style=text-align:left>0.01545</td><td style=text-align:left>1.38564</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>000002.SZ</td><td style=text-align:left>2005-01-04</td><td style=text-align:left>1.36761</td><td style=text-align:left>-0.44814</td><td style=text-align:left>1.69728</td><td style=text-align:left>1.69728</td><td style=text-align:left>1.69728</td><td style=text-align:left>1.04567</td><td style=text-align:left>-0.44814</td><td style=text-align:left>1.69728</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>000004.SZ</td><td style=text-align:left>2005-01-04</td><td style=text-align:left>0.32966</td><td style=text-align:left>-1.41654</td><td style=text-align:left>-0.13907</td><td style=text-align:left>-0.13907</td><td style=text-align:left>-0.13907</td><td style=text-align:left>-0.34769</td><td style=text-align:left>-1.41654</td><td style=text-align:left>-0.13650</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>000005.SZ</td><td style=text-align:left>2005-01-04</td><td style=text-align:left>0.61297</td><td style=text-align:left>-1.13066</td><td style=text-align:left>1.05339</td><td style=text-align:left>1.05339</td><td style=text-align:left>1.05339</td><td style=text-align:left>-1.20020</td><td style=text-align:left>-1.13066</td><td style=text-align:left>1.05597</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>000006.SZ</td><td style=text-align:left>2005-01-04</td><td style=text-align:left>-0.35542</td><td style=text-align:left>1.67667</td><td style=text-align:left>-0.07726</td><td style=text-align:left>-0.07726</td><td style=text-align:left>-0.07726</td><td style=text-align:left>1.55820</td><td style=text-align:left>1.67667</td><td style=text-align:left>-0.07469</td><td style=text-align:left>&mldr;</td></tr></tbody></table><h4 id=426-factor-data>4.2.6 Factor Data<a hidden class=anchor aria-hidden=true href=#426-factor-data>#</a></h4><ul><li><strong>Construction of Price-Volume Factors</strong></li></ul><p>This paper uses the following method to construct price-volume factors. There are two basic elements for constructing price-volume factors: first, <strong>basic fields</strong>, and second, <strong>operators</strong>. As shown in Table 4.2.1, basic fields include daily frequency high price, low price, open price, close price, previous day&rsquo;s close price, volume, change percentage, turnover rate, turnover amount, total market value, and adjustment factor.</p><h4 id=427-basic-field-table>4.2.7 Basic Field Table<a hidden class=anchor aria-hidden=true href=#427-basic-field-table>#</a></h4><table><thead><tr><th style=text-align:center>No.</th><th style=text-align:left>Field Name</th><th style=text-align:left>Meaning</th></tr></thead><tbody><tr><td style=text-align:center>high</td><td style=text-align:left>High Price</td><td style=text-align:left>Highest price in intraday transactions</td></tr><tr><td style=text-align:center>low</td><td style=text-align:left>Low Price</td><td style=text-align:left>Lowest price in intraday transactions</td></tr><tr><td style=text-align:center>open</td><td style=text-align:left>Open Price</td><td style=text-align:left>Price at which the call auction concludes</td></tr><tr><td style=text-align:center>close</td><td style=text-align:left>Close Price</td><td style=text-align:left>Price of the last transaction of the day</td></tr><tr><td style=text-align:center>pre_close</td><td style=text-align:left>Previous Close Price</td><td style=text-align:left>Price of the last transaction of the previous day</td></tr><tr><td style=text-align:center>vol</td><td style=text-align:left>Volume</td><td style=text-align:left>Number of shares traded throughout the day</td></tr><tr><td style=text-align:center>pct_chg</td><td style=text-align:left>Change Percentage</td><td style=text-align:left>Percentage change of the security for the day</td></tr><tr><td style=text-align:center>turnover_rate</td><td style=text-align:left>Turnover Rate</td><td style=text-align:left>Turnover rate of the security for the day</td></tr><tr><td style=text-align:center>amount</td><td style=text-align:left>Turnover Amount</td><td style=text-align:left>Total value of transactions for the day</td></tr><tr><td style=text-align:center>total_mv</td><td style=text-align:left>Total Market Value</td><td style=text-align:left>Total value of the stock, calculated by total shares outstanding multiplied by the current stock price</td></tr><tr><td style=text-align:center>adj_factor</td><td style=text-align:left>Adjustment Factor</td><td style=text-align:left>Ratio for adjusting for dividends and splits</td></tr></tbody></table><p>This paper obtains the operator list shown in the table below through the basic operator set provided by <a href=https://github.com/trevorstephens/gplearn>gplearn</a> and some self-defined special operators.</p><h4 id=428-operator-list>4.2.8 Operator List<a hidden class=anchor aria-hidden=true href=#428-operator-list>#</a></h4><table><thead><tr><th style=text-align:left>Operator</th><th style=text-align:left>Name</th><th style=text-align:left>Definition</th></tr></thead><tbody><tr><td style=text-align:left>add(x, y)</td><td style=text-align:left>Sum</td><td style=text-align:left>\( x + y\); element-wise operation</td></tr><tr><td style=text-align:left>\(\operatorname{div}(x, y)\)</td><td style=text-align:left>Division</td><td style=text-align:left>\( x / y\); element-wise operation</td></tr><tr><td style=text-align:left>\(\operatorname{mul}(x, y)\)</td><td style=text-align:left>Multiplication</td><td style=text-align:left>\( x \cdot y\); element-wise operation</td></tr><tr><td style=text-align:left>\(\operatorname{sub}(x, y)\)</td><td style=text-align:left>Subtraction</td><td style=text-align:left>\( x - y\); element-wise operation</td></tr><tr><td style=text-align:left>neg(x)</td><td style=text-align:left>Negative</td><td style=text-align:left>\(-x\); element-wise operation</td></tr><tr><td style=text-align:left>\(\log(x)\)</td><td style=text-align:left>Logarithm</td><td style=text-align:left>\(\log(x)\); element-wise operation</td></tr><tr><td style=text-align:left>max(x, y)</td><td style=text-align:left>Maximum</td><td style=text-align:left>Larger value between \(x\) and \(y\); element-wise operation</td></tr><tr><td style=text-align:left>\(\min(x, y)\)</td><td style=text-align:left>Minimum</td><td style=text-align:left>Smaller value between \(x\) and \(y\); element-wise operation</td></tr><tr><td style=text-align:left>delta_d(x)</td><td style=text-align:left>d-day Difference</td><td style=text-align:left>Current day&rsquo;s \(x\) value minus \(x\) value \(d\) days ago; <strong>time series operation</strong></td></tr><tr><td style=text-align:left>delay_d(x)</td><td style=text-align:left>d-day Delay</td><td style=text-align:left>\(x\) value \(d\) days ago; <strong>time series operation</strong></td></tr><tr><td style=text-align:left>Corr_d(x, y)</td><td style=text-align:left>d-day Correlation</td><td style=text-align:left>Correlation between \(x\) values and \(y\) values over \(d\) days; <strong>time series operation</strong></td></tr><tr><td style=text-align:left>Max_d(x)</td><td style=text-align:left>d-day Maximum</td><td style=text-align:left>Maximum value of \(x\) over \(d\) days; <strong>time series operation</strong></td></tr><tr><td style=text-align:left>Min_d(x)</td><td style=text-align:left>d-day Minimum</td><td style=text-align:left>Minimum value of \(x\) over \(d\) days; <strong>time series operation</strong></td></tr><tr><td style=text-align:left>sort_d(x)</td><td style=text-align:left>d-day Rank</td><td style=text-align:left>Rank of \(x\) values over \(d\) days; <strong>time series operation</strong></td></tr><tr><td style=text-align:left>Argsortmin_d(x)</td><td style=text-align:left>d-day Minimum Position</td><td style=text-align:left>Position of the minimum value of \(x\) over \(d\) days; <strong>time series operation</strong></td></tr><tr><td style=text-align:left>Argsortmax_d(x)</td><td style=text-align:left>d-day Maximum Position</td><td style=text-align:left>Position of the maximum value of \(x\) over \(d\) days; <strong>time series operation</strong></td></tr><tr><td style=text-align:left>\(\operatorname{inv}(x)\)</td><td style=text-align:left>Inverse</td><td style=text-align:left>\( 1 / x\); element-wise operation</td></tr><tr><td style=text-align:left>Std_d(x)</td><td style=text-align:left>d-day Standard Deviation</td><td style=text-align:left>Standard deviation of \(x\) values over \(d\) days; <strong>time series operation</strong></td></tr><tr><td style=text-align:left>abs(x)</td><td style=text-align:left>Absolute Value</td><td style=text-align:left>\(\lvert x\rvert\); element-wise operation</td></tr></tbody></table><h4 id=429-genetic-programming>4.2.9 Genetic Programming<a hidden class=anchor aria-hidden=true href=#429-genetic-programming>#</a></h4><p>The core idea of <strong>Genetic Programming (GP)</strong> is to use evolutionary algorithms to automatically &ldquo;evolve&rdquo; factor expressions with strong predictive power in the vast search space composed of operators and basic fields. For factor mining in this paper, the main goal of GP is to <strong>search</strong> and find those factors that can better predict future stock returns from all possible expressions that can be combined from the <strong>basic fields</strong> in Table 4.2.7 and the <strong>operators</strong> in Table 4.2.8. The core process of GP can be divided into the following steps:</p><h5 id=initialization>Initialization<a hidden class=anchor aria-hidden=true href=#initialization>#</a></h5><ol><li><p><strong>Define Operator Set and Basic Fields</strong></p><ul><li>Operator set (operators) as shown in Table 4.2.8, including operations such as addition, subtraction, multiplication, division, logarithm, absolute value, delay, moving maximum/minimum, moving correlation coefficient, etc.</li><li>Basic fields (terminals) as shown in Table 4.2.7, including open price, close price, high price, low price, volume, adjustment factor, etc.
These operators and basic fields can be regarded as &ldquo;nodes&rdquo; in the factor expression tree, where basic fields are leaf nodes (terminal nodes), and operators are internal nodes.</li></ul></li><li><p><strong>Randomly Generate Initial Population</strong></p><ul><li>In the initialization phase, based on the given operator set and field set, a series of factor expressions (which can be represented as several syntax trees or expression trees) are randomly &ldquo;spliced&rdquo; to form an initial population.</li><li>For example, it may randomly generate
\[
\text{Factor 1}: \mathrm{Max\_5}\bigl(\mathrm{add}(\mathrm{vol}, \mathrm{close})\bigr), \quad
\text{Factor 2}: \mathrm{sub}\bigl(\mathrm{adj\_factor}, \mathrm{neg}(\mathrm{turnover\_rate})\bigr),
\dots
\]</li><li>Each factor expression will correspond to an individual.</li></ul></li></ol><h5 id=fitness-function>Fitness Function<a hidden class=anchor aria-hidden=true href=#fitness-function>#</a></h5><ol><li><p><strong>Measure Factor&rsquo;s Predictive Ability</strong></p><ul><li>For each expression (individual), we need to evaluate its predictive ability for future returns or other objectives. Specifically, we can calculate the <strong>correlation coefficient</strong> (IC) or a more comprehensive indicator IR (Information Ratio) between the <strong>next period&rsquo;s stock return</strong> \( r^{T+1} \) and the current period&rsquo;s factor exposure \( x_k^T \) to measure it.</li></ul></li><li><p><strong>Set Objective</strong></p><ul><li>If we want the factor to have a higher correlation (IC), we can set the fitness function to \(\lvert \rho(x_k^T, r^{T+1})\rvert\);</li><li>If we want the factor to have a higher IR, we can set the fitness function to the IR value.</li><li>The higher the factor IC or IR, the higher the &ldquo;fitness&rdquo; of the expression.</li></ul></li></ol><p>Therefore, we usually set:</p>\[
\text{Fitness} \bigl(F(x)\bigr) \;=\;
\begin{cases}
\lvert \rho(x_k^T, r^{T+1})\rvert \quad &\text{(Maximize IC)},\\[6pt]
\mathrm{IR}(x_k^T) \quad &\text{(Maximize IR)}.
\end{cases}
\]<p>where \(\rho(\cdot)\) represents the correlation coefficient, and \(\mathrm{IR}(\cdot)\) is the IR indicator.</p><h5 id=selection-crossover-and-mutation>Selection, Crossover, and Mutation<a hidden class=anchor aria-hidden=true href=#selection-crossover-and-mutation>#</a></h5><ol><li><p><strong>Selection</strong></p><ul><li>Based on the results of the fitness function, expressions with high factor fitness are &ldquo;retained&rdquo; or &ldquo;bred&rdquo;, while expressions with lower fitness are eliminated.</li><li>This is similar to &ldquo;survival of the fittest&rdquo; in biological evolution.</li></ul></li><li><p><strong>Crossover</strong></p><ul><li>Randomly select a part of the &ldquo;nodes&rdquo; of several expressions with higher fitness (parents) for exchange, so as to obtain new expressions (offspring).</li><li>In the expression tree structure, subtree A and subtree B can be interchanged to generate new offspring expressions.</li><li>For example, if a subtree of expression tree \(\mathrm{FactorA}\) is exchanged with the corresponding subtree of expression tree \(\mathrm{FactorB}\), two new expressions are generated.</li></ul></li><li><p><strong>Mutation</strong></p><ul><li>Randomly change some nodes of the expression with a certain probability, such as:<ul><li>Replacing the operator of the node (for example, changing \(\mathrm{add}\) to \(\mathrm{sub}\)),</li><li>Replacing the basic field of the terminal node (for example, changing \(\mathrm{vol}\) to \(\mathrm{close}\)),</li><li>Or randomly changing operation parameters (such as moving window length, smoothing factor, etc.).</li></ul></li><li>Mutation can increase the diversity of the population and avoid premature convergence or falling into local optima.</li></ul></li></ol><h5 id=iterative-evolution>Iterative Evolution<a hidden class=anchor aria-hidden=true href=#iterative-evolution>#</a></h5><ol><li><p><strong>Iterative Execution</strong></p><ul><li>Repeatedly execute selection, crossover, and mutation operations for multiple generations.</li><li>Each generation produces a new population of factor expressions and evaluates their fitness.</li></ul></li><li><p><strong>Convergence and Termination</strong></p><ul><li>When evolution reaches a predetermined stopping condition (such as the number of iterations, fitness threshold, etc.), the algorithm terminates.</li><li>Usually, we will select <strong>several</strong> factor expressions with higher final fitness and regard them as the evolution results.</li></ul></li></ol><h5 id=mathematical-representation-searching-for-optimal-factor-expressions>Mathematical Representation: Searching for Optimal Factor Expressions<a hidden class=anchor aria-hidden=true href=#mathematical-representation-searching-for-optimal-factor-expressions>#</a></h5><p>Abstracting the above process into the following formula, the factor search objective can be simply expressed as:</p>\[
F(x) \;=\; \mathrm{GP}\bigl(\{\text{operators}\}, \{\text{terminals}\}\bigr),
\]<p>indicating that a function \(F(x)\) is searched through the GP algorithm on a given operator set (operators) and basic field set (terminals). From the perspective of optimization, we hope to find:</p>\[
\max_{F} \bigl\lvert \rho(F^T, r^{T+1}) \bigr\rvert
\quad \text{or} \quad
\max_{F} \; \mathrm{IR}\bigl(F\bigr),
\]<p>where</p><ul><li>\(\rho(\cdot)\) represents the correlation coefficient (IC) between the factor and the next period&rsquo;s return,</li><li>\(\mathrm{IR}(\cdot)\) represents the IR indicator of the factor.</li></ul><p>In practical applications, we will give a backtesting period, score the candidate factors of each generation (IC/IR evaluation), and continuously &ldquo;evolve&rdquo; better factors through the iterative process of selection, crossover, and mutation.</p><p><strong>Through the above steps, we can finally automatically mine a batch of factor expressions that have strong predictive power for future returns and good robustness (such as higher IR) in the vast search space of operator combinations and basic field combinations.</strong></p><h4 id=4210-partially-mined-factors>4.2.10 Partially Mined Factors<a hidden class=anchor aria-hidden=true href=#4210-partially-mined-factors>#</a></h4><table><thead><tr><th style=text-align:left>Factor Name</th><th style=text-align:left>Definition</th></tr></thead><tbody><tr><td style=text-align:left>0</td><td style=text-align:left>Max＿25(add(turnover_rate, vol))</td></tr><tr><td style=text-align:left>1</td><td style=text-align:left>Max＿30(vol)</td></tr><tr><td style=text-align:left>2</td><td style=text-align:left>Max＿25(turnover_rate)</td></tr><tr><td style=text-align:left>3</td><td style=text-align:left>Max＿35(add(vol, close))</td></tr><tr><td style=text-align:left>4</td><td style=text-align:left>Max＿30(turnover_rate)</td></tr><tr><td style=text-align:left>5</td><td style=text-align:left>sub(Min＿20(neg(pre_close)), div(vol, adj_factor))</td></tr><tr><td style=text-align:left>6</td><td style=text-align:left>Max＿60(max(vol, adj_factor))</td></tr><tr><td style=text-align:left>7</td><td style=text-align:left>Max＿50(amount)</td></tr><tr><td style=text-align:left>8</td><td style=text-align:left>div(vol, neg(close))</td></tr><tr><td style=text-align:left>9</td><td style=text-align:left>min(ArgSortMin＿25(pre_close), neg(vol))</td></tr><tr><td style=text-align:left>10</td><td style=text-align:left>neg(max(vol, turnover_rate))</td></tr><tr><td style=text-align:left>11</td><td style=text-align:left>mul(amount, neg(turnover_rate))</td></tr><tr><td style=text-align:left>12</td><td style=text-align:left>inv(add(ArgSortMax＿40(change), inv(pct_chg)))</td></tr><tr><td style=text-align:left>13</td><td style=text-align:left>Std＿40(inv(abs(sub(mul(total_mv, change), min(adj_factor, high))))</td></tr><tr><td style=text-align:left>14</td><td style=text-align:left>div(log(total_mv),amount)</td></tr><tr><td style=text-align:left>15</td><td style=text-align:left>div(neg(Max＿5(amount)), Min＿20(ArgSort＿60(high)))</td></tr><tr><td style=text-align:left>16</td><td style=text-align:left>Corr＿30(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))), add(log(Max＿10(pre_close)), high))</td></tr><tr><td style=text-align:left>17</td><td style=text-align:left>ArgSort＿60(neg(turnover_rate))</td></tr><tr><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td></tr></tbody></table><p>These factors are all obtained by combining from the operator list (Table 4.2.8) and the basic field list (Table 4.2.7) through genetic programming and have different mathematical expressions.</p><ul><li><strong>Factor Validity Test</strong></li></ul><p>After we get the mined factors, we need to test the validity of the factors. Common test indicators are <strong>Information Coefficient (IC)</strong> and <strong>Information Ratio (IR)</strong>.</p><ul><li><strong>Information Coefficient (IC)</strong> describes the <strong>linear correlation</strong> between the <strong>next period&rsquo;s return rate</strong> of the selected stocks and the current period&rsquo;s factor exposure, which can reflect the robustness of the factor in predicting returns.</li><li><strong>Information Ratio (IR)</strong> is the ratio of the mean of excess returns to the standard deviation of excess returns. The information ratio is similar to the Sharpe ratio. The main difference is that the Sharpe ratio uses the risk-free return as a benchmark, while the information ratio uses a risk index as a benchmark. The Sharpe ratio helps to determine the absolute return of a portfolio, and the information ratio helps to determine the relative return of a portfolio. After we calculate the IC, we can calculate the IR based on the IC value. When the IR is greater than 0.5, the factor has a strong ability to stably obtain excess returns.</li></ul><p>In actual calculation, the \( \mathrm{IC} \) value of factor \(k\) generally refers to the correlation coefficient between the exposure \( x_k^T \) of factor \(k\) in period \(T\) of the selected stocks and the return rate \( r^{T+1} \) of the selected stocks in period \(T+1\); the \( \mathrm{IR} \) value of factor \(k\) is the mean of the \( \mathrm{IC} \) of factor \(k\) divided by the standard deviation of the \( \mathrm{IC} \) of factor \(k\). The calculation formulas are as follows:</p>$$
\begin{gathered}
I C=\rho_{x_{k}^{T}, r^{T+1}}=\frac{\operatorname{cov}\left(x_{k}^{T}, r^{T+1}\right)}{\sigma_{x_{k}^{T}} \sigma_{r^{T+1}}}=\frac{\mathrm{E}\left(x_{k}^{T} * r^{T+1}\right)-\mathrm{E}\left(x_{k}^{T}\right) \mathrm{E}\left(r^{T+1}\right)}{\sqrt{\mathrm{E}\left(\left(x_{k}^{T}\right)^{2}\right)-\mathrm{E}\left(x_{k}^{T}\right)^{2}} \cdot \sqrt{\mathrm{E}\left(\left(r^{T+1}\right)^{2}\right)-\mathrm{E}\left(r^{T+1}\right)^{2}}} \\
I R=\frac{\overline{I C}}{\sigma_{I C}}
\end{gathered}
$$<p>Where:</p><ul><li>$x_{k}^{T}$: the exposure of the selected stock to factor $k$ in period $T$</li><li>$r^{T+1}$: the return of the selected stock in period $T+1$</li><li>$\overline{IC}$: the mean of the Information Coefficient (IC)</li></ul><p>This paper uses IR to judge the quality of factors. Through &ldquo;screening&rdquo; a large number of different combinations of operators and basic data and IC and IR, this paper obtains the 50 price-volume factors selected in this paper. After IR testing, the table shown in the figure below is obtained by sorting IR from high to low. From the table below, we can see that the IRs of the selected 50 price-volume factors are all greater than 0.5, indicating that these factors have a strong ability to stably obtain excess returns.</p><h4 id=4211-factor-ir-test-table>4.2.11 Factor IR Test Table<a hidden class=anchor aria-hidden=true href=#4211-factor-ir-test-table>#</a></h4><table><thead><tr><th style=text-align:left>Factor Name</th><th style=text-align:left>IR</th><th style=text-align:left>Factor Name</th><th style=text-align:left>IR</th></tr></thead><tbody><tr><td style=text-align:left>0</td><td style=text-align:left>3.11</td><td style=text-align:left>25</td><td style=text-align:left>2.73</td></tr><tr><td style=text-align:left>1</td><td style=text-align:left>2.95</td><td style=text-align:left>26</td><td style=text-align:left>2.71</td></tr><tr><td style=text-align:left>2</td><td style=text-align:left>2.95</td><td style=text-align:left>27</td><td style=text-align:left>2.70</td></tr><tr><td style=text-align:left>3</td><td style=text-align:left>2.95</td><td style=text-align:left>28</td><td style=text-align:left>2.69</td></tr><tr><td style=text-align:left>4</td><td style=text-align:left>2.95</td><td style=text-align:left>29</td><td style=text-align:left>2.69</td></tr><tr><td style=text-align:left>5</td><td style=text-align:left>2.94</td><td style=text-align:left>30</td><td style=text-align:left>2.69</td></tr><tr><td style=text-align:left>6</td><td style=text-align:left>2.94</td><td style=text-align:left>31</td><td style=text-align:left>2.68</td></tr><tr><td style=text-align:left>7</td><td style=text-align:left>2.94</td><td style=text-align:left>32</td><td style=text-align:left>2.68</td></tr><tr><td style=text-align:left>8</td><td style=text-align:left>2.93</td><td style=text-align:left>33</td><td style=text-align:left>2.68</td></tr><tr><td style=text-align:left>9</td><td style=text-align:left>2.93</td><td style=text-align:left>34</td><td style=text-align:left>2.68</td></tr><tr><td style=text-align:left>10</td><td style=text-align:left>2.93</td><td style=text-align:left>35</td><td style=text-align:left>2.67</td></tr><tr><td style=text-align:left>11</td><td style=text-align:left>2.92</td><td style=text-align:left>36</td><td style=text-align:left>2.67</td></tr><tr><td style=text-align:left>12</td><td style=text-align:left>2.91</td><td style=text-align:left>37</td><td style=text-align:left>2.66</td></tr><tr><td style=text-align:left>13</td><td style=text-align:left>2.89</td><td style=text-align:left>38</td><td style=text-align:left>2.65</td></tr><tr><td style=text-align:left>14</td><td style=text-align:left>2.86</td><td style=text-align:left>39</td><td style=text-align:left>2.65</td></tr><tr><td style=text-align:left>15</td><td style=text-align:left>2.83</td><td style=text-align:left>40</td><td style=text-align:left>2.65</td></tr><tr><td style=text-align:left>16</td><td style=text-align:left>2.83</td><td style=text-align:left>41</td><td style=text-align:left>2.65</td></tr><tr><td style=text-align:left>17</td><td style=text-align:left>2.83</td><td style=text-align:left>42</td><td style=text-align:left>2.64</td></tr><tr><td style=text-align:left>18</td><td style=text-align:left>2.79</td><td style=text-align:left>43</td><td style=text-align:left>2.63</td></tr><tr><td style=text-align:left>19</td><td style=text-align:left>2.78</td><td style=text-align:left>44</td><td style=text-align:left>2.63</td></tr><tr><td style=text-align:left>20</td><td style=text-align:left>2.78</td><td style=text-align:left>45</td><td style=text-align:left>2.62</td></tr><tr><td style=text-align:left>21</td><td style=text-align:left>2.76</td><td style=text-align:left>46</td><td style=text-align:left>2.62</td></tr><tr><td style=text-align:left>22</td><td style=text-align:left>2.75</td><td style=text-align:left>47</td><td style=text-align:left>2.62</td></tr></tbody></table><p>It can be seen from this table that among the screened factors, the IRs of all factors are greater than 0.5, which has a strong and stable ability to obtain excess returns.</p><h3 id=43-factor-cleaning>4.3 Factor Cleaning<a hidden class=anchor aria-hidden=true href=#43-factor-cleaning>#</a></h3><h4 id=431-factor-missing-value-handling-and-outlier-removal>4.3.1 Factor Missing Value Handling and Outlier Removal<a hidden class=anchor aria-hidden=true href=#431-factor-missing-value-handling-and-outlier-removal>#</a></h4><p>Methods for handling missing values of factors include case deletion, mean imputation, regression imputation, and other methods. This paper adopts a relatively simple mean imputation method to handle missing values, that is, using the average value of the factor to replace the missing data. Methods for factor outlier removal include median outlier removal, percentile outlier removal, and $3 \sigma$ outlier removal. This paper uses the $3 \sigma$ outlier removal method. This method uses the $3 \sigma$ principle in statistics to convert outlier factors that are more than three standard deviations away from the mean of the factor to a position that is just three standard deviations away from the mean. The specific calculation formula is as follows:</p>$$
X_i^{\prime}= \begin{cases} \bar{X}+3 \sigma & \text{if } X_i > \bar{X} + 3 \sigma \\ \bar{X}-3 \sigma & \text{if } X_i < \bar{X} - 3 \sigma \\ X_i & \text{if } \bar{X} - 3 \sigma < X_i < \bar{X} + 3 \sigma \end{cases}
$$<p>Where:</p><ul><li>$X_{i}$: Value of the factor before processing</li><li>$\bar{X}$: Mean of the factor sequence</li><li>$\sigma$: Standard deviation of the factor sequence</li><li>$X_{i}^{\prime}$: Value of the factor after outlier removal</li></ul><h4 id=432-factor-standardization>4.3.2 Factor Standardization<a hidden class=anchor aria-hidden=true href=#432-factor-standardization>#</a></h4><p>In this experiment, multiple factors are selected, and the dimensions of each factor are not completely the same. For the convenience of comparison and regression, we also need to standardize the factors. Currently, common specific standardization methods include Min-Max standardization, Z-score standardization, and Decimal scaling standardization. This paper chooses the Z-score standardization method. The data is standardized through the mean and standard deviation of the original data. The processed data conforms to the standard normal distribution, that is, the mean is 0 and the standard deviation is 1. The standardized numerical value is positive or negative, and a standard normal distribution curve is obtained.</p><p>The Z-score standardization formula used in this paper is as follows:</p>$$
\tilde{x}=\frac{x_{i}-u}{\sigma}
$$<p>Where:</p><ul><li>$x_{i}$: Original value of the factor</li><li>$u$: Mean of the factor sequence</li><li>$\sigma$: Standard deviation of the factor sequence</li><li>$\tilde{x}$: Standardized factor value</li></ul><h4 id=433-factor-neutralization>4.3.3 Factor Neutralization<a hidden class=anchor aria-hidden=true href=#433-factor-neutralization>#</a></h4><p>Factor neutralization is to eliminate the influence of other factors on our selected factors, so that the stocks selected by our quantitative investment strategy portfolio are more dispersed, rather than concentrated in specific industries or market capitalization stocks. It can better share the risk of the investment portfolio and solve the problem of factor multicollinearity. Market capitalization and industry are the two main independent variables that affect stock returns. Therefore, in the process of factor cleaning, the influence of market capitalization and industry must also be considered. In this empirical study, we adopt the method of only including industry factors and including market factors in industry factors. The single-factor regression model for factors is shown in formula (31). We take the residual term of the following regression model as the new factor value after factor neutralization.</p>$$
\tilde{r}_{j}^{t}=\sum_{s=1}^{s} X_{j s}^{t} \tilde{f}_{s}^{t}+X_{j k}^{t} \tilde{f}_{k}^{t}+\tilde{u}_{j}^{t}
$$<p>Where:</p><ul><li>$\tilde{r}_{j}^{t}$: Return rate of stock $j$ in period $t$</li><li>$X_{j s}^{t}$: Exposure of stock $j$ in industry $s$ in period $t$</li><li>$\tilde{f}_{s}^{t}$: Return rate of the industry in period $t$</li><li>$X_{j k}^{t}$: Exposure of stock $j$ on factor $k$ in period $t$</li><li>$\tilde{f}_{k}^{t}$: Return rate of factor $k$ in period $t$</li><li>$\tilde{u}_j^t$: A $0-1$ dummy variable, that is, if stock $j$ belongs to industry $s$, the exposure is 1, otherwise it is 0</li></ul><p>In this paper, the industry to which a company belongs is not proportionally split, that is, stock $j$ can only belong to a specific industry $s$, the exposure in industry $s$ is 1, and the exposure in all other industries is 0. This paper uses the Shenwan Hongyuan industry classification standard. The specific classifications are sequentially: agriculture, forestry, animal husbandry and fishery, mining, chemical industry, steel, nonferrous metals, electronic components, household appliances, food and beverage, textile and apparel, light industry manufacturing, pharmaceutical and biological, public utilities, transportation, real estate, commercial trade, catering and tourism, comprehensive, building materials, building decoration, electrical equipment, national defense and military industry, computer, media, communication, banking, non-banking finance, automobile, and mechanical equipment, a total of 28 categories. The table below shows the historical market chart of Shenwan Index Level 1 industries on February 5, 2021.</p><h5 id=4331-historical-market-chart-of-shenwan-index-level-1-industries-on-february-5-2021>4.3.3.1 Historical Market Chart of Shenwan Index Level 1 Industries on February 5, 2021<a hidden class=anchor aria-hidden=true href=#4331-historical-market-chart-of-shenwan-index-level-1-industries-on-february-5-2021>#</a></h5><table><thead><tr><th style=text-align:left>Index Code</th><th style=text-align:left>Index Name</th><th style=text-align:left>Release Date</th><th style=text-align:left>Open Index</th><th style=text-align:left>High Index</th><th style=text-align:left>Low Index</th><th style=text-align:left>Close Index</th><th style=text-align:left>Volume (100 Million Hands)</th><th style=text-align:left>Turnover (100 Million CNY)</th><th style=text-align:left>Change (%)</th></tr></thead><tbody><tr><td style=text-align:left>801010</td><td style=text-align:left>Agriculture, Forestry, Animal Husbandry and Fishery</td><td style=text-align:left>2021/2/5 0:00</td><td style=text-align:left>4111.43</td><td style=text-align:left>4271.09</td><td style=text-align:left>4072.53</td><td style=text-align:left>4081.81</td><td style=text-align:left>15.81</td><td style=text-align:left>307.82</td><td style=text-align:left>-0.3</td></tr><tr><td style=text-align:left>801020</td><td style=text-align:left>Mining</td><td style=text-align:left>2021/2/5 0:00</td><td style=text-align:left>2344.62</td><td style=text-align:left>2357.33</td><td style=text-align:left>2288.97</td><td style=text-align:left>2289.41</td><td style=text-align:left>18.06</td><td style=text-align:left>115.6</td><td style=text-align:left>-2.25</td></tr><tr><td style=text-align:left>801030</td><td style=text-align:left>Chemical Industry</td><td style=text-align:left>2021/2/5 0:00</td><td style=text-align:left>4087.77</td><td style=text-align:left>4097.59</td><td style=text-align:left>3910.67</td><td style=text-align:left>3910.67</td><td style=text-align:left>55.78</td><td style=text-align:left>778.85</td><td style=text-align:left>-3.95</td></tr><tr><td style=text-align:left>801040</td><td style=text-align:left>Steel</td><td style=text-align:left>2021/2/5 0:00</td><td style=text-align:left>2253.78</td><td style=text-align:left>2268.17</td><td style=text-align:left>2243.48</td><td style=text-align:left>2250.81</td><td style=text-align:left>11.61</td><td style=text-align:left>48.39</td><td style=text-align:left>-1.02</td></tr><tr><td style=text-align:left>801050</td><td style=text-align:left>Nonferrous Metals</td><td style=text-align:left>2021/2/5 0:00</td><td style=text-align:left>4212.1</td><td style=text-align:left>4250.59</td><td style=text-align:left>4035.99</td><td style=text-align:left>4036.74</td><td style=text-align:left>45.41</td><td style=text-align:left>593.92</td><td style=text-align:left>-4.43</td></tr><tr><td style=text-align:left>801080</td><td style=text-align:left>Electronic Components</td><td style=text-align:left>2021/2/5 0:00</td><td style=text-align:left>4694.8</td><td style=text-align:left>4694.8</td><td style=text-align:left>4561.95</td><td style=text-align:left>4561.95</td><td style=text-align:left>52.67</td><td style=text-align:left>850.79</td><td style=text-align:left>-2.78</td></tr><tr><td style=text-align:left>801110</td><td style=text-align:left>Household Appliances</td><td style=text-align:left>2021/2/5 0:00</td><td style=text-align:left>10033.82</td><td style=text-align:left>10171.26</td><td style=text-align:left>9968.93</td><td style=text-align:left>10096.83</td><td style=text-align:left>8.55</td><td style=text-align:left>149.18</td><td style=text-align:left>0.83</td></tr><tr><td style=text-align:left>801120</td><td style=text-align:left>Food and Beverage</td><td style=text-align:left>2021/2/5 0:00</td><td style=text-align:left>30876.33</td><td style=text-align:left>31545.02</td><td style=text-align:left>30649.57</td><td style=text-align:left>30931.69</td><td style=text-align:left>11.32</td><td style=text-align:left>657.11</td><td style=text-align:left>0.47</td></tr><tr><td style=text-align:left>801130</td><td style=text-align:left>Textile and Apparel</td><td style=text-align:left>2021/2/5 0:00</td><td style=text-align:left>1614.48</td><td style=text-align:left>1633.89</td><td style=text-align:left>1604.68</td><td style=text-align:left>1607.63</td><td style=text-align:left>6.28</td><td style=text-align:left>57.47</td><td style=text-align:left>-0.39</td></tr><tr><td style=text-align:left>801140</td><td style=text-align:left>Light Industry Manufacturing</td><td style=text-align:left>2021/2/5 0:00</td><td style=text-align:left>2782.07</td><td style=text-align:left>2791.88</td><td style=text-align:left>2735.48</td><td style=text-align:left>2737.24</td><td style=text-align:left>15.28</td><td style=text-align:left>176.16</td><td style=text-align:left>-1.35</td></tr><tr><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td><td style=text-align:left>&mldr;</td></tr></tbody></table><p><strong>Data Source</strong>: Shenwan Hongyuan</p><p>The table below is partial data of the original factors. After sequentially going through the four factor cleaning steps of missing value imputation, factor outlier removal, factor standardization, and factor neutralization mentioned above, partial data of the cleaned factors are obtained as shown in the table.</p><h5 id=4332-original-factor-data>4.3.3.2 Original Factor Data<a hidden class=anchor aria-hidden=true href=#4332-original-factor-data>#</a></h5><table><thead><tr><th style=text-align:left>trade_date</th><th style=text-align:left>sec_code</th><th style=text-align:left>ret</th><th style=text-align:left>factor_0</th><th style=text-align:left>factor_1</th><th style=text-align:left>factor_2</th><th style=text-align:left>factor_3</th><th style=text-align:left>factor_4</th><th style=text-align:left>factor_5</th><th style=text-align:left>factor_6</th><th style=text-align:left>&mldr;</th></tr></thead><tbody><tr><td style=text-align:left>2005-01-04</td><td style=text-align:left>600874.SH</td><td style=text-align:left>0.001684</td><td style=text-align:left>NaN</td><td style=text-align:left>9.445412</td><td style=text-align:left>9.445412</td><td style=text-align:left>9.445408</td><td style=text-align:left>-1.00</td><td style=text-align:left>NaN</td><td style=text-align:left>12651.124023</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>2005-01-04</td><td style=text-align:left>000411.SZ</td><td style=text-align:left>0.021073</td><td style=text-align:left>NaN</td><td style=text-align:left>5.971262</td><td style=text-align:left>5.971262</td><td style=text-align:left>5.971313</td><td style=text-align:left>0.38</td><td style=text-align:left>NaN</td><td style=text-align:left>392.124298</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>2005-01-04</td><td style=text-align:left>000979.SZ</td><td style=text-align:left>0.021207</td><td style=text-align:left>NaN</td><td style=text-align:left>6.768918</td><td style=text-align:left>6.768918</td><td style=text-align:left>6.768815</td><td style=text-align:left>-1.45</td><td style=text-align:left>NaN</td><td style=text-align:left>870.587585</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>2005-01-04</td><td style=text-align:left>000498.SZ</td><td style=text-align:left>0.030220</td><td style=text-align:left>NaN</td><td style=text-align:left>8.852752</td><td style=text-align:left>8.852752</td><td style=text-align:left>8.852755</td><td style=text-align:left>0.55</td><td style=text-align:left>NaN</td><td style=text-align:left>6994.011719</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>2005-01-04</td><td style=text-align:left>600631.SH</td><td style=text-align:left>0.015699</td><td style=text-align:left>NaN</td><td style=text-align:left>9.589897</td><td style=text-align:left>9.589897</td><td style=text-align:left>9.589889</td><td style=text-align:left>-1.70</td><td style=text-align:left>NaN</td><td style=text-align:left>14616.806641</td><td style=text-align:left>&mldr;</td></tr></tbody></table><h5 id=4333-cleaned-factor-data>4.3.3.3 Cleaned Factor Data<a hidden class=anchor aria-hidden=true href=#4333-cleaned-factor-data>#</a></h5><table><thead><tr><th style=text-align:left>sec_code</th><th style=text-align:left>trade_date</th><th style=text-align:left>ret</th><th style=text-align:left>factor_0</th><th style=text-align:left>factor_1</th><th style=text-align:left>factor_2</th><th style=text-align:left>factor_3</th><th style=text-align:left>factor_4</th><th style=text-align:left>factor_5</th><th style=text-align:left>factor_6</th><th style=text-align:left>&mldr;</th></tr></thead><tbody><tr><td style=text-align:left>000001.SZ</td><td style=text-align:left>2005-01-04</td><td style=text-align:left>-1.58653</td><td style=text-align:left>0.01545</td><td style=text-align:left>1.38306</td><td style=text-align:left>1.38306</td><td style=text-align:left>1.38306</td><td style=text-align:left>0.13392</td><td style=text-align:left>0.01545</td><td style=text-align:left>1.38564</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>000002.SZ</td><td style=text-align:left>2005-01-04</td><td style=text-align:left>1.36761</td><td style=text-align:left>-0.44814</td><td style=text-align:left>1.69728</td><td style=text-align:left>1.69728</td><td style=text-align:left>1.69728</td><td style=text-align:left>1.04567</td><td style=text-align:left>-0.44814</td><td style=text-align:left>1.69728</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>000004.SZ</td><td style=text-align:left>2005-01-04</td><td style=text-align:left>0.32966</td><td style=text-align:left>-1.41654</td><td style=text-align:left>-0.13907</td><td style=text-align:left>-0.13907</td><td style=text-align:left>-0.13907</td><td style=text-align:left>-0.34769</td><td style=text-align:left>-1.41654</td><td style=text-align:left>-0.13650</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>000005.SZ</td><td style=text-align:left>2005-01-04</td><td style=text-align:left>0.61297</td><td style=text-align:left>-1.13066</td><td style=text-align:left>1.05339</td><td style=text-align:left>1.05339</td><td style=text-align:left>1.05339</td><td style=text-align:left>-1.20020</td><td style=text-align:left>-1.13066</td><td style=text-align:left>1.05597</td><td style=text-align:left>&mldr;</td></tr><tr><td style=text-align:left>000006.SZ</td><td style=text-align:left>2005-01-04</td><td style=text-align:left>-0.35542</td><td style=text-align:left>1.67667</td><td style=text-align:left>-0.07726</td><td style=text-align:left>-0.07726</td><td style=text-align:left>-0.07726</td><td style=text-align:left>1.55820</td><td style=text-align:left>1.67667</td><td style=text-align:left>-0.07469</td><td style=text-align:left>&mldr;</td></tr></tbody></table><h3 id=44-factor-selection-based-on-lightgbm>4.4 Factor Selection Based on LightGBM<a hidden class=anchor aria-hidden=true href=#44-factor-selection-based-on-lightgbm>#</a></h3><h4 id=441-gbdt>4.4.1 GBDT<a hidden class=anchor aria-hidden=true href=#441-gbdt>#</a></h4><p>Gradient Boosting Decision Tree (GBDT), proposed by <a href=https://www.jstor.org/stable/2699986>Friedman (2001)</a>$^{[20]}$, is an iterative regression decision tree. Its main idea is to optimize the model by gradually adding weak classifiers (usually decision trees), so that the overall model can minimize the loss function. The GBDT model can be expressed as:</p>$$
\hat{y} = \sum_{m=1}^{M} \gamma_m h_m(\mathbf{x})
$$<p>Where:</p><ul><li>\( M \) is the number of iterations,</li><li>\( \gamma_m \) is the weight of the \( m \)-th weak classifier,</li><li>\( h_m(\mathbf{x}) \) is the \( m \)-th decision tree model.</li></ul><p>The training process of GBDT minimizes the loss function by gradually fitting the negative gradient direction. The specific update formula is:</p>$$
\gamma_m = \arg\min_\gamma \sum_{i=1}^{N} L\left(y_i, \hat{y}_{i}^{(m-1)} + \gamma h_m(\mathbf{x}_i)\right)
$$<p>Where, \( L \) is the loss function, \( y_i \) is the true value, and \( \hat{y}_{i}^{(m-1)} \) is the predicted value after the \( (m-1) \)-th iteration.</p><h4 id=442-lightgbm>4.4.2 LightGBM<a hidden class=anchor aria-hidden=true href=#442-lightgbm>#</a></h4><p><a href=https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf>Light Gradient Boosting Machine (LightGBM)</a>$^{[21]}$ is an efficient framework for implementing the GBDT algorithm, initially developed by Microsoft as a free and open-source distributed gradient boosting framework. LightGBM is based on decision tree algorithms and is widely used in ranking, classification, and other machine learning tasks. Its development focuses on performance and scalability. Its main advantages include high-efficiency parallel training, faster training speed, lower memory consumption, better accuracy, and support for distributed computing and fast processing of massive data$^{[22]}$.</p><p>The core algorithm of LightGBM is based on the following optimization objective:</p>$$
L = \sum_{i=1}^{N} l(y_i, \hat{y}_i) + \sum_{m=1}^{M} \Omega(h_m)
$$<p>Where, \( l \) is the loss function, and \( \Omega \) is the regularization term, used to control model complexity, usually expressed as:</p>$$
\Omega(h_m) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2
$$<p>Here, \( T \) is the number of leaves in the tree, \( w_j \) is the weight of the \( j \)-th leaf, and \( \gamma \) and \( \lambda \) are regularization parameters.</p><p>LightGBM uses technologies such as Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB), which significantly improve training efficiency and model performance.</p><p>In this study, the loss function used during training is Mean Squared Error (MSE), which is defined as:</p>$$
L(y, \hat{y}) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
$$<p>Where, \( y \) is the true return rate, \( \hat{y} \) is the return rate predicted by the model, and \( N \) is the number of samples.</p><h4 id=443-algorithm-flow>4.4.3 Algorithm Flow<a hidden class=anchor aria-hidden=true href=#443-algorithm-flow>#</a></h4><p>The specific algorithm flow in this section is as follows:</p><ol><li><p><strong>Data Preparation</strong>: Use one year&rsquo;s worth of 50 factor data for each stock (A-share market-wide data) and historical future one-month returns as features.</p></li><li><p><strong>Model Training</strong>: Use Grid Search to optimize the hyperparameters of the LightGBM model and train the model to predict the future one-month return rate. The model training flow is shown in Fig. 4.12.</p>$$
\text{Parameter Optimization:} \quad \theta^* = \arg\min_\theta \sum_{i=1}^{N} L(y_i, \hat{y}_i(\theta))
$$<p>Where, \( \theta \) represents the set of model hyperparameters, and \( \theta^* \) is the optimal parameter.</p></li><li><p><strong>Factor Importance Calculation</strong>: Use LightGBM&rsquo;s <code>feature_importances_</code> method to calculate the feature importance of each factor. Feature importance is mainly measured by two indicators:</p><ul><li><strong>Split</strong>: The number of times the feature is used for splitting in all trees.</li><li><strong>Gain</strong>: The total gain brought by the feature in all splits (i.e., the amount of reduction in the loss function).</li></ul><p>The feature importance of a factor can be expressed as:</p>$$
\text{Importance}_{\text{split}}(f) = \sum_{m=1}^{M} \sum_{j=1}^{T_m} \mathbb{I}(f \text{ is used for splitting the } j \text{-th leaf node})
$$$$
\text{Importance}_{\text{gain}}(f) = \sum_{m=1}^{M} \sum_{j=1}^{T_m} \Delta L_{m,j} \cdot \mathbb{I}(f \text{ is used for splitting the } j \text{-th leaf node})
$$<p>Where, \( \mathbb{I} \) is the indicator function, and \( \Delta L_{m,j} \) is the reduction in loss brought by factor \( f \) in the \( j \)-th split of the \( m \)-th tree.</p></li><li><p><strong>Factor Screening</strong>: Sort according to the factor importance calculated by the model, and select the top ten factors with the highest importance as the factors used in this cross-sectional analysis. The importance of the selected factors is shown in Table 4.4.4.</p></li></ol><h4 id=444-partial-ranking-of-selected-factor-importance>4.4.4 Partial Ranking of Selected Factor Importance<a hidden class=anchor aria-hidden=true href=#444-partial-ranking-of-selected-factor-importance>#</a></h4><table><thead><tr><th style=text-align:left>importance</th><th style=text-align:left>feature_name</th><th style=text-align:left>trade_date</th></tr></thead><tbody><tr><td style=text-align:left>35</td><td style=text-align:left>factor_35</td><td style=text-align:left>2010-08-11</td></tr><tr><td style=text-align:left>27</td><td style=text-align:left>factor_27</td><td style=text-align:left>2010-08-11</td></tr><tr><td style=text-align:left>33</td><td style=text-align:left>factor_33</td><td style=text-align:left>2010-08-11</td></tr><tr><td style=text-align:left>20</td><td style=text-align:left>factor_20</td><td style=text-align:left>2010-08-11</td></tr><tr><td style=text-align:left>24</td><td style=text-align:left>factor_24</td><td style=text-align:left>2010-08-11</td></tr><tr><td style=text-align:left>45</td><td style=text-align:left>factor_45</td><td style=text-align:left>2010-08-11</td></tr><tr><td style=text-align:left>37</td><td style=text-align:left>factor_37</td><td style=text-align:left>2010-08-11</td></tr><tr><td style=text-align:left>49</td><td style=text-align:left>factor_49</td><td style=text-align:left>2010-08-11</td></tr><tr><td style=text-align:left>19</td><td style=text-align:left>factor_19</td><td style=text-align:left>2010-08-11</td></tr><tr><td style=text-align:left>47</td><td style=text-align:left>factor_47</td><td style=text-align:left>2010-08-11</td></tr><tr><td style=text-align:left>22</td><td style=text-align:left>factor_22</td><td style=text-align:left>2010-09-09</td></tr><tr><td style=text-align:left>20</td><td style=text-align:left>factor_20</td><td style=text-align:left>2010-09-09</td></tr><tr><td style=text-align:left>30</td><td style=text-align:left>factor_30</td><td style=text-align:left>2010-09-09</td></tr><tr><td style=text-align:left>24</td><td style=text-align:left>factor_24</td><td style=text-align:left>2010-09-09</td></tr></tbody></table><h4 id=445-code-implementation-snippet>4.4.5 Code Implementation Snippet<a hidden class=anchor aria-hidden=true href=#445-code-implementation-snippet>#</a></h4><p>The following is a code snippet used in the training process for factor selection.</p><p><details><summary markdown=span>feature_choice</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>feature_choice</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>days</span><span class=o>=</span><span class=mi>21</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>is_local</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>is_local</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>feature_info</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_hdf</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>RESULTS</span><span class=p>,</span> <span class=n>Feature_Info</span> <span class=o>+</span> <span class=s1>&#39;.h5&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>factors</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>get_env</span><span class=p>()</span><span class=o>.</span><span class=n>query_data</span><span class=p>(</span><span class=n>Factors_Data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>factors</span> <span class=o>=</span> <span class=n>factors</span><span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>factors</span><span class=p>[</span><span class=n>COM_DATE</span><span class=p>]</span> <span class=o>&gt;=</span> <span class=s1>&#39;2010-01-01&#39;</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>trade_list</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>set</span><span class=p>(</span><span class=n>factors</span><span class=p>[</span><span class=n>COM_DATE</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=n>trade_list</span><span class=o>.</span><span class=n>sort</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>trade_list</span><span class=p>)</span> <span class=o>%</span> <span class=n>days</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>n</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>trade_list</span><span class=p>)</span> <span class=o>/</span> <span class=n>days</span><span class=p>)</span> <span class=o>-</span> <span class=mi>7</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>n</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>trade_list</span><span class=p>)</span> <span class=o>/</span> <span class=n>days</span><span class=p>)</span> <span class=o>-</span> <span class=mi>6</span>
</span></span><span class=line><span class=cl>        <span class=n>feature_info</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>begin_index</span> <span class=o>=</span> <span class=mi>147</span>
</span></span><span class=line><span class=cl>        <span class=n>feature</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>factors</span><span class=o>.</span><span class=n>columns</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>feature</span><span class=o>.</span><span class=n>remove</span><span class=p>(</span><span class=n>COM_SEC</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>feature</span><span class=o>.</span><span class=n>remove</span><span class=p>(</span><span class=n>COM_DATE</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>feature</span><span class=o>.</span><span class=n>remove</span><span class=p>(</span><span class=n>Ret</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>end_date</span> <span class=o>=</span> <span class=n>days</span> <span class=o>*</span> <span class=n>i</span> <span class=o>+</span> <span class=n>begin_index</span> <span class=o>-</span> <span class=mi>21</span>
</span></span><span class=line><span class=cl>            <span class=n>begin_date</span> <span class=o>=</span> <span class=n>days</span> <span class=o>*</span> <span class=n>i</span>
</span></span><span class=line><span class=cl>            <span class=n>trade_date</span> <span class=o>=</span> <span class=n>days</span> <span class=o>*</span> <span class=n>i</span> <span class=o>+</span> <span class=n>begin_index</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=n>trade_list</span><span class=p>[</span><span class=n>trade_date</span><span class=p>])</span>
</span></span><span class=line><span class=cl>            <span class=n>train_data</span> <span class=o>=</span> <span class=n>factors</span><span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=p>(</span><span class=n>factors</span><span class=p>[</span><span class=n>COM_DATE</span><span class=p>]</span> <span class=o>&lt;=</span> <span class=n>trade_list</span><span class=p>[</span><span class=n>end_date</span><span class=p>])</span> <span class=o>&amp;</span>
</span></span><span class=line><span class=cl>                <span class=p>(</span><span class=n>factors</span><span class=p>[</span><span class=n>COM_DATE</span><span class=p>]</span> <span class=o>&gt;=</span> <span class=n>trade_list</span><span class=p>[</span><span class=n>begin_date</span><span class=p>])</span>
</span></span><span class=line><span class=cl>            <span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>model</span> <span class=o>=</span> <span class=n>lgb</span><span class=o>.</span><span class=n>LGBMRegressor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_data</span><span class=p>[</span><span class=n>feature</span><span class=p>],</span> <span class=n>train_data</span><span class=p>[</span><span class=n>Ret</span><span class=p>])</span>
</span></span><span class=line><span class=cl>            <span class=n>feature_info_cell</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>columns</span><span class=o>=</span><span class=n>Info_Fields</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>feature_info_cell</span><span class=p>[</span><span class=n>Importance</span><span class=p>]</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>feature_importances_</span>
</span></span><span class=line><span class=cl>            <span class=n>feature_info_cell</span><span class=p>[</span><span class=n>Feature_Name</span><span class=p>]</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>feature_name_</span>
</span></span><span class=line><span class=cl>            <span class=n>feature_info_cell</span> <span class=o>=</span> <span class=n>feature_info_cell</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=n>by</span><span class=o>=</span><span class=n>Importance</span><span class=p>)</span><span class=o>.</span><span class=n>tail</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>feature_info_cell</span><span class=p>[</span><span class=n>COM_DATE</span><span class=p>]</span> <span class=o>=</span> <span class=n>trade_list</span><span class=p>[</span><span class=n>trade_date</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>feature_info</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>concat</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=p>[</span><span class=n>feature_info</span><span class=p>,</span> <span class=n>feature_info_cell</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>axis</span><span class=o>=</span><span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>HDFStore</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>RESULTS</span><span class=p>,</span> <span class=n>Feature_Info</span> <span class=o>+</span> <span class=s1>&#39;.h5&#39;</span><span class=p>),</span> <span class=s1>&#39;w&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span><span class=p>[</span><span class=s1>&#39;data&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>feature_info</span>
</span></span><span class=line><span class=cl>        <span class=n>h</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>get_env</span><span class=p>()</span><span class=o>.</span><span class=n>add_data</span><span class=p>(</span><span class=n>feature_info</span><span class=p>,</span> <span class=n>Feature_Info</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>pass</span>
</span></span></code></pre></div></details></p><p>Through the above process, LightGBM is used to efficiently screen out the factors that have the greatest impact on predicting future returns, thereby improving the predictive ability and interpretability of the model.</p><h3 id=45-factor-combination-based-on-bilstm>4.5 Factor Combination Based on BiLSTM<a hidden class=anchor aria-hidden=true href=#45-factor-combination-based-on-bilstm>#</a></h3><p>This section uses BiLSTM for factor combination. The specific principle of BiLSTM has been introduced in Chapter 2, and will not be repeated here. First, let&rsquo;s introduce the specific network structure of the model. The network structure of BiLSTM set in this paper through a large number of repeated experiments is shown in Table 4.5.1. The default tanh and linear activation functions of recurrent neural networks are used between layers. Dropout is added to prevent overfitting, but if Dropout uses an excessively large dropout rate, underfitting will occur. Therefore, the dropout rate of Dropout is set to 0.01. The number of neurons in the BiLSTM recurrent layer of the final model is 100. A BiLSTM layer and three fully connected layers are used, and a Dropout is set between the BiLSTM layer and the first fully connected layer.</p><h4 id=451-bilstm-network-structure>4.5.1 BiLSTM Network Structure<a hidden class=anchor aria-hidden=true href=#451-bilstm-network-structure>#</a></h4><table><thead><tr><th style=text-align:left>Layer(type)</th><th style=text-align:left>Output Shape</th><th style=text-align:left>Param#</th></tr></thead><tbody><tr><td style=text-align:left>bidirectional_1 (Bidirection)</td><td style=text-align:left>(None, 100)</td><td style=text-align:left>24400</td></tr><tr><td style=text-align:left>dropout_1 (Dropout)</td><td style=text-align:left>(None, 100)</td><td style=text-align:left>0</td></tr><tr><td style=text-align:left>dense_1 (Dense)</td><td style=text-align:left>(None, 256)</td><td style=text-align:left>25856</td></tr><tr><td style=text-align:left>dropout_2 (Dropout)</td><td style=text-align:left>(None, 256)</td><td style=text-align:left>0</td></tr><tr><td style=text-align:left>dense_2 (Dense)</td><td style=text-align:left>(None, 64)</td><td style=text-align:left>16448</td></tr><tr><td style=text-align:left>dense_3 (Dense)</td><td style=text-align:left>(None, 1)</td><td style=text-align:left>0</td></tr></tbody></table><p><strong>Total params</strong>: 66,769<br><strong>Trainable params</strong>: 66,769<br><strong>Non-trainable params</strong>: 0</p><p>Because the amount of data used in this experiment is large, <code>epochs=400</code> and <code>batch_size=1024</code> are selected. The loss function of the model is Mean Squared Error (MSE). The optimizer used is Stochastic Gradient Descent (SGD). Stochastic gradient descent has three advantages over gradient descent (GD): it can more effectively use information when information is redundant, and the early iteration effect is excellent, which is suitable for processing large-sample data $^{[23]}$. Since the amount of training data in this experiment is large, if SGD is used, only one sample is used for iteration each time, and the training speed is very fast, which can greatly reduce the time spent on training. The default values in its <code>keras</code> package are used, i.e., <code>lr=0.01</code>, <code>momentum=0.0</code>, <code>decay=0.0</code>, and <code>nesterov=False</code>.</p><p><strong>Parameter Explanation:</strong></p><ul><li><code>lr</code>: Learning rate</li><li><code>momentum</code>: Momentum parameter</li><li><code>decay</code>: Learning rate decay value after each update</li><li><code>nesterov</code>: Determine whether to use Nesterov momentum</li></ul><h4 id=452-algorithm-flow>4.5.2 Algorithm Flow<a hidden class=anchor aria-hidden=true href=#452-algorithm-flow>#</a></h4><p>The specific algorithm flow in this section is as follows:</p><ol><li>Use A-share market-wide data of 10 factors (factors selected by LightGBM) and historical future one-month returns for each stock for one year as features.</li><li>Take the future one-month return rate of each stock per year as the prediction target, and use BiLSTM for training, as shown in Fig. 12.</li></ol><figure class=align-center><a href=Rolling_Window.png data-fancybox=gallery><img loading=lazy src=Rolling_Window.png#center alt="Fig. 12. Rolling Window"></a><figcaption><p>Fig. 12. Rolling Window</p></figcaption></figure><ol start=3><li>The real-time factor data of out-of-sample data for one month is passed through the trained BiLSTM model to obtain the real-time expected return rate of each stock for the next month. The return rate is shown in Table 4.11.</li></ol><h4 id=453-partial-stock-predicted-return-rate-table>4.5.3 Partial Stock Predicted Return Rate Table<a hidden class=anchor aria-hidden=true href=#453-partial-stock-predicted-return-rate-table>#</a></h4><table><thead><tr><th style=text-align:left>sec_code</th><th style=text-align:left>trade_date</th><th style=text-align:left>y_hat</th></tr></thead><tbody><tr><td style=text-align:left>000001.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>0.0424621</td></tr><tr><td style=text-align:left>000002.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>-0.1632174</td></tr><tr><td style=text-align:left>000004.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>-0.0642319</td></tr><tr><td style=text-align:left>000005.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>0.08154649</td></tr><tr><td style=text-align:left>000006.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>0.00093213</td></tr><tr><td style=text-align:left>000007.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>-0.073218</td></tr><tr><td style=text-align:left>000008.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>-0.0464256</td></tr><tr><td style=text-align:left>000009.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>-0.091549</td></tr><tr><td style=text-align:left>000010.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>0.08154649</td></tr><tr><td style=text-align:left>000011.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>-0.1219943</td></tr><tr><td style=text-align:left>000012.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>-0.1448984</td></tr><tr><td style=text-align:left>000014.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>0.09038845</td></tr><tr><td style=text-align:left>000016.SZ</td><td style=text-align:left>2011/5/26</td><td style=text-align:left>-0.11225</td></tr></tbody></table><h4 id=454-code-implementation-snippet>4.5.4 Code Implementation Snippet<a hidden class=anchor aria-hidden=true href=#454-code-implementation-snippet>#</a></h4><p>The following is a code snippet used in the training process for building the BiLSTM training network.</p><p><details><summary markdown=span>build_net_blstm</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_net_blstm</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>ks</span><span class=o>.</span><span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>ks</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Bidirectional</span><span class=p>(</span><span class=n>ks</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=mi>50</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span><span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>11</span><span class=p>,</span><span class=mi>10</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>ks</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>ks</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>256</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>ks</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>ks</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>64</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>ks</span><span class=o>.</span><span class=n>layers</span><span class=o>.</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;sgd&#39;</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s1>&#39;mse&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>set_model</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
</span></span></code></pre></div></details></p><h3 id=46-quantitative-strategy-and-strategy-backtesting>4.6 Quantitative Strategy and Strategy Backtesting<a hidden class=anchor aria-hidden=true href=#46-quantitative-strategy-and-strategy-backtesting>#</a></h3><h4 id=461-backtesting-metrics>4.6.1 Backtesting Metrics<a hidden class=anchor aria-hidden=true href=#461-backtesting-metrics>#</a></h4><p>First, let&rsquo;s introduce some common backtesting metrics for strategies. Evaluation metrics include Total Rate of Return, Annualized Rate of Return, Annualized volatility, Sharpe ratio, Maximum Drawdown (MDD), Annualized turnover rate, and Annualized transaction cost rate. It is assumed that the stock market is open for 252 days a year, the risk-free rate is defaulted to 0.035, and the commission fee is defaulted to 0.002.</p><ol><li><strong>Total Rate of Return</strong>: Under the same other indicators, the larger the cumulative rate of return, the better the strategy, and the more it can bring greater returns. The formula is as follows:</li></ol>$$
\text{Total Rate of Return} = r_{p} = \frac{P_{1} - P_{0}}{P_{0}}
$$<p>$P_{1}$: Total value of final stocks and cash<br>$P_{0}$: Total value of initial stocks and cash</p><ol start=2><li><strong>Annualized Rate of Return</strong>: It is to convert the cumulative total rate of return into a geometric average rate of return on an annual basis. Under the same other indicators, the larger the annualized rate of return, the better the strategy. The formula is as follows:</li></ol>$$
\text{Annualized Rate of Return} = R_{p} = \left(1 + r_{p}\right)^{\frac{252}{t}} - 1
$$<p>$r_{p}$: Cumulative rate of return<br>$t$: Number of days the investment strategy is executed</p><ol start=3><li><strong>Annualized volatility</strong>: Defined as the standard deviation of the logarithmic value of the annual return rate of the object asset. Annualized volatility is used to measure the risk of a strategy. The greater the volatility, the higher the risk of the strategy. The formula is as follows:</li></ol>$$
\begin{aligned}
\text{Annualized volatility} = \sigma_{p} &= \sqrt{\frac{252}{t-1} \sum_{i=1}^{t}\left(r_{d} - \bar{r}_{d}\right)^{2}} \\
\bar{r}_{d} &= \frac{1}{t} \sum_{i=1}^{t} r_{d_{i}}
\end{aligned}
$$<p>$r_{d_{i}}$: Daily return rate on the $i$-th day<br>$\bar{r}_{d}$: Average daily return rate<br>$t$: Number of days the investment strategy is executed</p><ol start=4><li><strong>Sharpe ratio</strong>: Proposed by <a href=https://doi.org/10.2307/2328485>Sharpe (1966)</a>$^{[24]}$. It represents the excess return obtained by investors for bearing an extra unit of risk$^{[25]}$. Here is the calculation formula for the annualized Sharpe ratio:</li></ol>$$
S = \frac{R_{p} - R_{f}}{\sigma_{p}}
$$<p>$R_{p}$: Annualized rate of return<br>$R_{f}$: Risk-free rate of return<br>$\sigma_{p}$: Annualized volatility</p><ol start=5><li><strong>Maximum Drawdown (MDD)</strong>: Indicates the maximum value of the return rate drawdown when the total value of stocks and cash of our strategy portfolio reaches the lowest point during the operation period. Maximum drawdown is used to measure the most extreme possible loss situation of the strategy.</li></ol>$$
MDD = \frac{\max \left(V_{x} - V_{y}\right)}{V_{x}}
$$<p>$V_{x}$ and $V_{y}$ are the total value of stocks and cash of the strategy portfolio on day $x$ and day $y$ respectively, and $x &lt; y$.</p><ol start=6><li><strong>Annualized turnover rate</strong>: Used to measure the frequency of buying and selling stocks in the investment portfolio. The larger the value, the more frequent the portfolio turnover and the greater the transaction cost.</li></ol>$$
\text{change} = \frac{N \times 252}{t}
$$<p>$t$: Number of days the investment strategy is executed<br>$N$: Total number of buy and sell transactions</p><ol start=7><li><strong>Annualized transaction cost rate</strong>: Used to measure the transaction cost of the investment portfolio strategy. The larger the value, the higher the transaction cost.</li></ol>$$
c = \left(1 + \text{commison}\right)^{\text{change}} - 1
$$<p>change: Annualized turnover rate<br>commison: Commission fee</p><h4 id=462-strategy-and-backtesting-results>4.6.2 Strategy and Backtesting Results<a hidden class=anchor aria-hidden=true href=#462-strategy-and-backtesting-results>#</a></h4><p>The quantitative trading strategy in this paper adopts position switching every month (i.e., the rebalancing period is 28 trading days). Each time, the strategy adopts an equal-weight stock holding method to buy the 25 stocks with the highest expected return rate predicted by BiLSTM and sell the originally held stocks. The backtesting time and rules in this paper are as follows:</p><ol><li><strong>Backtesting Time</strong>: From January 2012 to October 2020.</li><li><strong>Backtesting Stock Pool</strong>: All A-shares, excluding Special Treatment (ST) stocks.</li><li><strong>Transaction Fee</strong>: A brokerage commission of 0.2% is paid when buying, and a brokerage commission of 0.2% is paid when selling. If the commission for a single transaction is less than 5 CNY, the brokerage charges 5 CNY.</li><li><strong>Buying and Selling Rules</strong>: Stocks that hit the upper limit on the opening day cannot be bought, and stocks that hit the lower limit cannot be sold.</li></ol><h5 id=4621-strategy-backtesting-results>4.6.2.1 Strategy Backtesting Results<a hidden class=anchor aria-hidden=true href=#4621-strategy-backtesting-results>#</a></h5><table><thead><tr><th style=text-align:left></th><th style=text-align:left>Cumulative Return</th><th style=text-align:left>Annualized Return</th><th style=text-align:left>Annualized Volatility</th><th style=text-align:left>Sharpe Ratio</th><th style=text-align:left>Max Drawdown</th><th style=text-align:left>Annualized Turnover Rate</th><th style=text-align:left>Annualized Transaction Cost Rate</th></tr></thead><tbody><tr><td style=text-align:left><strong>Strategy</strong></td><td style=text-align:left>701.00%</td><td style=text-align:left>29.18%</td><td style=text-align:left>33.44%</td><td style=text-align:left>0.77</td><td style=text-align:left>51.10%</td><td style=text-align:left>51.10%</td><td style=text-align:left>11.35%</td></tr><tr><td style=text-align:left><strong>Benchmark</strong></td><td style=text-align:left>110.40%</td><td style=text-align:left>9.70%</td><td style=text-align:left>26.01%</td><td style=text-align:left>0.24</td><td style=text-align:left>58.49%</td><td style=text-align:left>58.49%</td><td style=text-align:left>0.00%</td></tr></tbody></table><figure class=align-center><a href=res.png data-fancybox=gallery><img loading=lazy src=res.png#center alt="Fig. 22. Net Profit Curve"></a><figcaption><p>Fig. 22. Net Profit Curve</p></figcaption></figure><p>The backtesting results are shown in the table and Fig. 22 above. My strategy adopts the LightGBM-BiLSTM quantitative strategy introduced in this chapter. The benchmark uses the CSI All Share (000985). From the results above, it can be seen that the cumulative return of this strategy is 701.00%, which is much higher than the benchmark&rsquo;s 110.40%; the annualized return is 29.18%, which is much higher than the benchmark&rsquo;s 9.70%; and the Sharpe ratio is 0.77, which is higher than the benchmark&rsquo;s 0.24. These three backtesting indicators show that the LightGBM-BiLSTM quantitative strategy can indeed bring greater returns to investors. The annualized volatility of this strategy is 33.44%, which is greater than the benchmark&rsquo;s 26.01%, and the maximum drawdown is 51.10%, which is less than the benchmark&rsquo;s 58.49%. These two backtesting indicators show that the LightGBM-BiLSTM quantitative strategy has certain risks, especially it is difficult to resist the impact of systemic risks. The annualized turnover rate is 11.35%, and the annualized transaction cost rate is 2.29%, indicating that our strategy is not a high-frequency trading strategy and the transaction cost is small. It can be seen from the return curve chart that the return rate of the LightGBM-BiLSTM quantitative strategy in the first two years is not much different from the benchmark, and there is no special advantage. However, from around April 2015, the return rate of the LightGBM-BiLSTM quantitative strategy is significantly better than the benchmark&rsquo;s return rate. Overall, the return rate of this LightGBM-BiLSTM quantitative strategy is very considerable, but there are still certain risks.</p><h2 id=chapter-5-conclusion-and-future-directions>Chapter 5 Conclusion and Future Directions<a hidden class=anchor aria-hidden=true href=#chapter-5-conclusion-and-future-directions>#</a></h2><h3 id=51-conclusion>5.1 Conclusion<a hidden class=anchor aria-hidden=true href=#51-conclusion>#</a></h3><p>This paper first introduced the research background and significance of stock price prediction and quantitative strategy research based on deep learning, and then introduced the domestic and international research status of stock price prediction and quantitative investment strategies respectively. Then, the innovations and research framework of this paper were explained. Then, in the chapter on related theoretical foundations, this paper briefly introduced the deep learning models and the development history of quantitative investment used in this paper. The basic structure, basic principles, and characteristics of the three models LSTM, GRU, and BiLSTM are mainly introduced.</p><p>Subsequently, this paper used the daily frequency data of SPD Bank and IBM, and preprocessed the data through a series of data processing processes and feature extraction. Then, the specific network structure and hyperparameter settings of the three models LSTM, GRU, and BiLSTM were introduced. Then, we used LSTM, GRU, and BiLSTM to predict the closing prices of the two stocks and compare the model evaluations. The experimental results show that for both stocks, the BiLSTM prediction effect is more accurate.</p><p>Finally, in order to further illustrate the application value of BiLSTM in finance, this paper constructed a quantitative investment model based on LightGBM-BiLSTM. Stocks in the entire A-share market and multiple factors were selected for factor cleaning, factor selection based on LightGBM, and factor combination based on LSTM. Then, we constructed a certain investment strategy and compared it with the benchmark holding CSI All Share through evaluation indicators such as cumulative return rate, annualized return rate, annualized volatility, and Sharpe ratio. Through comparison, it was found that the LightGBM-BiLSTM quantitative investment model can bring better returns, indicating the effectiveness of using deep learning to build quantitative investment strategies.</p><h3 id=52-future-directions>5.2 Future Directions<a hidden class=anchor aria-hidden=true href=#52-future-directions>#</a></h3><p>Although this paper compares the effects of LSTM, GRU, and BiLSTM models in predicting stock closing prices and achieves certain results based on the LightGBM-BiLSTM quantitative investment strategy, there are still some shortcomings in this paper&rsquo;s research. Combining the research results of this paper, the following research and improvements can be further carried out:</p><ol><li><strong>Diversification of Prediction Targets</strong>: In terms of predicting stock prices, this paper selects the stock closing price as the prediction target. Although this result is the most intuitive, the Random Walk Hypothesis (RWH) proposed by <a href=http://www.numdam.org/item/ASENS_1900_3_17__21_0/>Bachelier (1900)</a>$^{[26]}$ believes that stock prices follow a random walk and are unpredictable. Although many behavioral economists have since proved that this view is not entirely correct, it also shows that simply predicting stock closing prices is not so strong in terms of difficulty and interpretability $^{[27][28]}$. Therefore, stock volatility prediction, stock price increase/decrease judgment, and stock return rate prediction can be selected as future research directions.</li><li><strong>Diversified Model Comparison</strong>: In terms of predicting stock prices, this paper compares the three recurrent neural network models LSTM, GRU, and BiLSTM and shows that BiLSTM has better prediction effect, but there is still a lack of comparative research with more different models. Therefore, future in-depth research can be conducted on comparisons with Autoregressive Integrated Moving Average (ARIMA), Convolutional Neural Networks (CNNs), Deep Neural Networks (DNNs), CNN-LSTM, Transformer, TimeGPT, and other single or composite models.</li><li><strong>Factor Diversification</strong>: The factors used in this paper to construct quantitative investment strategies are all technical price-volume factors, and the types of factors are single. In the future, different types of factors such as financial factors, sentiment factors, and growth factors can be selected to improve the performance of the strategy. At the same time, future research can also appropriately add timing strategies to increase positions when predicting that the market will rise and reduce positions when predicting that the market will fall to earn beta (\(\beta\)) returns.</li><li><strong>Investment Portfolio Optimization</strong>: The factor combination process in this paper is still imperfect. In the future, quadratic programming methods can be used to optimize the investment portfolio.</li><li><strong>High-Frequency Trading Strategy Research</strong>: The quantitative investment strategy method in this paper adopts a low-frequency trading strategy. In the future, stock tick data can be used to study high-frequency strategies and ultra-high-frequency strategies.</li></ol><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] White, H. <a href=https://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf>“Economic prediction using neural networks: The case of IBM daily stock returns.”</a> <em>Proc. of ICNN</em>. 1988, 2: 451-458.</p><p>[2] Kimoto, T., Asakawa, K., Yoda, M., et al. <a href=https://web.ist.utl.pt/adriano.simoes/tese/referencias/Papers%20-%20Adriano/NN.pdf>“Stock market prediction system with modular neural networks.”</a> <em>Proc. of 1990 IJCNN International Joint Conference on Neural Networks</em>. IEEE, 1990: 1-6.</p><p>[3] Zhang, G. P. <a href=https://dl.icdst.org/pdfs/files/2c442c738bd6bc178e715f400bec5d5f.pdf>“Time series forecasting using a hybrid ARIMA and neural network model.”</a> <em>Neurocomputing</em>. 2003, 50: 159-175.</p><p>[4] Akita, R., Yoshihara, A., Matsubara, T., et al. <a href=https://ieeexplore.ieee.org/document/7550882>“Deep learning for stock prediction using numerical and textual information.”</a> <em>Proc. of 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)</em>. IEEE, 2016: 1-6.</p><p>[5] 宮崎邦洋, 松尾豊. <a href=https://www.ai-gakkai.or.jp/jsai2017/webprogram/2017/pdf/1112.pdf>“Deep Learning を用いた株価予測の分析.”</a> <em>人工知能学会全国大会論文集 第31回全国大会</em>. 一般社団法人 人工知能学会, 2017: 2D3OS19a3-2D3OS19a3.</p><p>[6] Kim, T., Kim, H. Y. <a href="https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0212320&amp;type=printable">“Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data.”</a> <em>PLoS ONE</em>. 2019, 14(2): e0212320.</p><p>[7] Hochreiter, S., Schmidhuber, J. <a href=https://www.bioinf.jku.at/publications/older/2604.pdf>“Long short-term memory.”</a> <em>Neural Computation</em>. 1997, 9(8): 1735-1780.</p><p>[8] Cho, K., Van Merriënboer, B., Gulcehre, C., et al. <a href=https://arxiv.org/abs/1406.1078>“Learning phrase representations using RNN encoder-decoder for statistical machine translation.”</a> <em>arXiv preprint arXiv:1406.1078</em>. 2014.</p><p>[9] Chung, J., Gulcehre, C., Cho, K. H., et al. <a href=https://arxiv.org/abs/1412.3555>“Empirical evaluation of gated recurrent neural networks on sequence modeling.”</a> <em>arXiv preprint arXiv:1412.3555</em>. 2014.</p><p>[10] Gruber, N., Jockisch, A. <a href=https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2020.00040/full>“Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?”</a> <em>Frontiers in Artificial Intelligence</em>. 2020, 3(40): 1-6.</p><p>[11] Markowitz, H. <a href=https://www.jstor.org/stable/2975974>“Portfolio Selection.”</a> <em>The Journal of Finance</em>. 1952, 7(1): 77-91. doi:10.2307/2975974.</p><p>[12] Merton, R. C. <a href=https://www.jstor.org/stable/2329621>“An analytic derivation of the efficient portfolio frontier.”</a> <em>Journal of Financial and Quantitative Analysis</em>. 1972: 1851-1872.</p><p>[13] Sharpe, W. F. <a href=https://www.jstor.org/stable/2977928>“Capital asset prices: A theory of market equilibrium under conditions of risk.”</a> <em>The Journal of Finance</em>. 1964, 19(3): 425-442.</p><p>[14] Lintner, J. <a href=https://www.jstor.org/stable/1924119>“The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.”</a> <em>Review of Economics and Statistics</em>. 1965, 47(1): 13-37.</p><p>[15] Mossin, J. <a href=https://www.jstor.org/stable/1910098>“Equilibrium in a capital asset market.”</a> <em>Econometrica: Journal of the Econometric Society</em>. 1966: 768-783.</p><p>[16] Ross, S. A. <a href=https://www.top1000funds.com/wp-content/uploads/2014/05/The-Arbitrage-Theory-of-Capital-Asset-Pricing.pdf>“The arbitrage theory of capital asset pricing.”</a> <em>Journal of Economic Theory</em>. 1976, 13(3): 341-60.</p><p>[17] Fama, E. F., French, K. R. <a href=https://www.bauer.uh.edu/rsusmel/phd/Fama-French_JFE93.pdf>“Common risk factors in the returns on stocks and bonds.”</a> <em>Journal of Financial Economics</em>. 1993, 33(1): 3-56.</p><p>[18] Fama, E. F., French, K. R. <a href=https://tevgeniou.github.io/EquityRiskFactors/bibliography/FiveFactor.pdf>“A five-factor asset pricing model.”</a> <em>Journal of Financial Economics</em>. 2015, 116(1): 1-22.</p><p>[19] Kingma, D. P., Ba, J. <a href=https://arxiv.org/abs/1412.6980>“Adam: A method for stochastic optimization.”</a> <em>arXiv preprint arXiv:1412.6980</em>. 2014.</p><p>[20] Friedman, J. H. <a href=https://www.jstor.org/stable/2699986>“Greedy function approximation: A gradient boosting machine.”</a> <em>Annals of Statistics</em>. 2001: 1189-1232.</p><p>[21] Kopitar, L., Kocbek, P., Cilar, L., et al. <a href=https://www.nature.com/articles/s41598-020-68771-z>“Early detection of type 2 diabetes mellitus using machine learning-based prediction models.”</a> <em>Scientific Reports</em>. 2020, 10(1): 1-12.</p><p>[22] Ke, G., Meng, Q., Finley, T., et al. <a href=https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf>“Lightgbm: A highly efficient gradient boosting decision tree.”</a> <em>Advances in Neural Information Processing Systems</em>. 2017, 30: 3146-3154.</p><p>[23] Bottou, L., Curtis, F. E., Nocedal, J. <a href=https://coral.ise.lehigh.edu/frankecurtis/files/papers/BottCurtNoce18.pdf>“Optimization methods for large-scale machine learning.”</a> <em>SIAM Review</em>. 2018, 60(2): 223-311.</p><p>[24] Sharpe, W. F. <a href=https://finance.martinsewell.com/fund-performance/Sharpe1966.pdf>“Mutual fund performance.”</a> <em>The Journal of Business</em>. 1966, 39(1): 119-138.</p><p>[25] Sharpe, W. F. <a href=https://web.stanford.edu/~wfsharpe/art/sr/sr.htm>“The sharpe ratio.”</a> <em>Journal of Portfolio Management</em>. 1994, 21(1): 49-58.</p><p>[26] Bachelier, L. <a href=http://www.numdam.org/item/ASENS_1900_3_17__21_0/>“Théorie de la spéculation.”</a> <em>Annales Scientifiques de l&rsquo;École Normale Supérieure</em>. 1900, 17: 21-86.</p><p>[27] Fromlet, H. <a href=https://www.jstor.org/stable/23488166>“Behavioral finance-theory and practical application: Systematic analysis of departures from the homo oeconomicus paradigm are essential for realistic financial research and analysis.”</a> <em>Business Economics</em>. 2001: 63-69.</p><p>[28] Lo, A. W. <a href=https://www.pm-research.com/content/iijpormgmt/30/5/15>“The adaptive markets hypothesis.”</a> <em>The Journal of Portfolio Management</em>. 2004, 30(5): 15-29.</p><h3 id=reference-blog>Reference Blog<a hidden class=anchor aria-hidden=true href=#reference-blog>#</a></h3><ul><li>Colah&rsquo;s Blog. (2015, August 27). <a href=https://colah.github.io/posts/2015-08-Understanding-LSTMs/><em>Understanding LSTM Networks.</em></a></li></ul><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: For reprint or citation of this article, please indicate the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Apr 2021). Stock Price Prediction and Quantitative Strategy Based on Deep Learning.
<a href=https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/>https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2021stockprediction</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;Stock Price Prediction and Quantitative Strategy Based on Deep Learning&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2021&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Apr&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/rnn/>RNN</a></li><li><a href=https://syhya.github.io/tags/lstm/>LSTM</a></li><li><a href=https://syhya.github.io/tags/bilstm/>BiLSTM</a></li><li><a href=https://syhya.github.io/tags/gru/>GRU</a></li><li><a href=https://syhya.github.io/tags/lightgbm/>LightGBM</a></li><li><a href=https://syhya.github.io/tags/neural-networks/>Neural Networks</a></li><li><a href=https://syhya.github.io/tags/stock-prediction/>Stock Prediction</a></li><li><a href=https://syhya.github.io/tags/financial-modeling/>Financial Modeling</a></li><li><a href=https://syhya.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://syhya.github.io/tags/quantitative-investment/>Quantitative Investment</a></li><li><a href=https://syhya.github.io/tags/portfolio-management/>Portfolio Management</a></li><li><a href=https://syhya.github.io/tags/financial-engineering/>Financial Engineering</a></li><li><a href=https://syhya.github.io/tags/algorithmic-trading/>Algorithmic Trading</a></li><li><a href=https://syhya.github.io/tags/time-series/>Time Series</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2024-12-21-build-gpu-server/><span class=title>« Prev</span><br><span>Building a Home Deep Learning Rig with Dual RTX 4090 GPUs</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Stock Price Prediction and Quantitative Strategy Based on Deep Learning on x" href="https://x.com/intent/tweet/?text=Stock%20Price%20Prediction%20and%20Quantitative%20Strategy%20Based%20on%20Deep%20Learning&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2021-04-21-deep-learning-stock-prediction%2f&amp;hashtags=Deeplearning%2cAI%2cRNN%2cLSTM%2cBiLSTM%2cGRU%2cLightGBM%2cNeuralNetworks%2cStockPrediction%2cFinancialModeling%2cMachineLearning%2cQuantitativeInvestment%2cPortfolioManagement%2cFinancialEngineering%2cAlgorithmicTrading%2cTimeSeries"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Stock Price Prediction and Quantitative Strategy Based on Deep Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2021-04-21-deep-learning-stock-prediction%2f&amp;title=Stock%20Price%20Prediction%20and%20Quantitative%20Strategy%20Based%20on%20Deep%20Learning&amp;summary=Stock%20Price%20Prediction%20and%20Quantitative%20Strategy%20Based%20on%20Deep%20Learning&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2021-04-21-deep-learning-stock-prediction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Stock Price Prediction and Quantitative Strategy Based on Deep Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2021-04-21-deep-learning-stock-prediction%2f&title=Stock%20Price%20Prediction%20and%20Quantitative%20Strategy%20Based%20on%20Deep%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Stock Price Prediction and Quantitative Strategy Based on Deep Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2021-04-21-deep-learning-stock-prediction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Stock Price Prediction and Quantitative Strategy Based on Deep Learning on whatsapp" href="https://api.whatsapp.com/send?text=Stock%20Price%20Prediction%20and%20Quantitative%20Strategy%20Based%20on%20Deep%20Learning%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2021-04-21-deep-learning-stock-prediction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Stock Price Prediction and Quantitative Strategy Based on Deep Learning on telegram" href="https://telegram.me/share/url?text=Stock%20Price%20Prediction%20and%20Quantitative%20Strategy%20Based%20on%20Deep%20Learning&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2021-04-21-deep-learning-stock-prediction%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Stock Price Prediction and Quantitative Strategy Based on Deep Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=Stock%20Price%20Prediction%20and%20Quantitative%20Strategy%20Based%20on%20Deep%20Learning&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2021-04-21-deep-learning-stock-prediction%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>