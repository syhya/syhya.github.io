---
title: llama系列
date: 2025-01-05T12:00:00+08:00
author: "Yue Shui"
tags: ["AI", "NLP", "LLM", "Pre-training", "Post-training", "DPO", "领域模型", "DeepSpeed"]
categories: ["技术博客"]
readingTime: 25
toc: true
ShowToc: true
TocOpen: false
draft: True
type: "posts"
---
## 引言

2025年1月，Meta正式发布了其最新的开源大语言模型——LLaMA 3（Large Language Model Meta AI 3）。作为LLaMA系列的第三代模型，LLaMA3在多个基准测试中实现了全面领先，性能超越了业界同类最先进的模型。这一发布不仅标志着LLaMA系列的技术成熟，也预示着开源大语言模型在未来产业中的重要地位。

本篇文章将系统梳理LLaMA系列模型从LLaMA1到LLaMA3的发展历程，深入解析其模型架构、训练数据和训练方法，并通过对比表格揭示各版本的核心差异。此外，还将探讨LLaMA模型对开源社区和产业的深远影响，帮助读者全面理解这一系列模型的技术进化与应用潜力。

## LLaMA模型演进

### LLaMA1

#### 概述
LLaMA1（[Touvron et al., 2023](https://arxiv.org/abs/2302.13971)）是Meta于2023年2月发布的一款大型语言模型。作为当时性能出色的开源模型之一，LLaMA1迅速在学术界和工业界获得了广泛关注。

#### 特点
- **参数规模**：提供7B、13B、30B和65B四种参数版本。
- **训练数据量**：超过1.4万亿token。
- **训练资源**：最大的65B参数模型在2048张80G显存的A100 GPU上训练了近21天。
- **性能优势**：在大多数基准测试中超越了拥有175B参数的GPT-3。

#### 技术细节
LLaMA1采用了Decoder-only架构，通过以下技术优化模型结构和训练方法：
- **Pre-normalization**：[GPT-3](https://arxiv.org/abs/2005.14165) 引入的预归一化技术，使用RMSNorm归一化函数，提高训练稳定性，避免计算样本均值，速度提升约40%。
- **FFN_SWiGLU**：[PaLM](https://arxiv.org/abs/2204.02311) 中的门控线性单元，调整隐藏单元数量至 \( \frac{2}{3}d \) 以保持FFN层参数量不变，并将ReLU替换为SiLU激活函数，提升性能。
- **Rotary Embeddings (RoPE)**：引入的位置嵌入方法，在每一层网络中添加位置嵌入，替代传统的positional embeddings，增强模型对位置信息的处理能力。

#### 性能评估
LLaMA1在多个自然语言处理基准测试中表现优异，特别是在语言理解和生成任务中，显著优于同期的闭源模型如GPT-3。

#### 应用场景
- **研究与开发**：作为基础模型，支持各种自然语言处理任务的研究。
- **商业应用**：为企业提供强大的语言理解和生成能力，优化客户服务、内容生成等领域。
- **教育与培训**：帮助学术机构和培训机构开发先进的语言处理工具。

---

### LLaMA2

#### 概述
LLaMA2（[Touvron et al., 2023](https://arxiv.org/abs/2307.09288)）是LLaMA系列的第二代模型，进一步提升了模型的规模和性能，巩固了Meta在开源大语言模型领域的领先地位。

#### 特点
- **参数规模**：提供7B、13B、34B和70B四种参数版本。
- **训练数据量**：从1.4万亿token增加到2万亿token，提升约40%。
- **上下文长度**：扩展至4096 tokens，是LLaMA1的两倍。
- **注意力机制**：引入分组查询注意力机制（Group Query Attention, GQA），优化kv-cache性能与内存效率。

#### 技术细节
LLaMA2在架构上进行了多项优化，具体包括：
- **分组查询注意力机制（GQA）**：通过分组查询优化自注意力计算的效率，减少内存占用，提高计算性能。
- **有监督微调（SFT）**：使用标注数据对模型进行微调，提升其在特定任务上的表现。
- **基于人类反馈的强化学习（RLHF）**：通过人类反馈指导模型学习，增强其生成内容的质量和安全性。

#### 性能评估
LLaMA2的Chat版本在多个基准测试中表现出色，超越了前代模型，并在推理和对话生成任务中与顶尖闭源模型相匹配。

#### 应用场景
- **对话系统**：提供更加自然和准确的对话生成，提升用户体验。
- **内容生成**：支持高质量的文本生成，应用于新闻、营销等领域。
- **安全与合规**：通过增强的安全机制，确保生成内容符合伦理和法律标准。

---

### Code Llama

#### 概述
Code Llama（[Rozière et al., 2023](https://arxiv.org/abs/2307.09288)）是Meta于2023年8月24日推出的专门针对编程任务的扩展模型，基于LLaMA2架构，旨在提升代码生成和理解能力。

#### 特点
- **版本**：
  1. **Code Llama - 70B**：基础代码模型。
  2. **Code Llama - 70B - Python**：专门针对Python优化。
  3. **Code Llama - 70B - Instruct**：经过指令微调，增强理解和执行自然语言指令的能力。
- **训练数据量**：
  - **70B模型**：1万亿token的代码及相关数据。
  - **7B和13B模型**：各5000亿token的代码及相关数据。
- **上下文长度**：支持长达100,000 tokens的上下文。

#### 技术细节
Code Llama基于LLaMA2架构，进行了进一步优化以增强代码生成和理解能力，具体包括：
- **填充中间（Fill-in-the-Middle, FIM）**：允许模型在现有代码中插入代码片段，实现即时代码补全。
- **优化的训练方法**：结合有监督微调和指令微调，提升模型在代码生成任务中的表现。

#### 性能评估
在HumanEval和Mostly Basic Python Programming (MBPP)等代码生成基准测试中，**Code Llama 34B**分别获得53.7%和56.2%的得分，超越了其他开源及代码专用的LLM，并与ChatGPT表现持平。

#### 应用场景
- **开发者工具**：提升代码编写效率，支持实时代码补全和调试。
- **教育领域**：帮助学习编程的初学者，通过示例和代码生成提供教学支持。
- **商业应用**：集成到IDE和代码管理工具中，优化开发流程。

---

### Llama Guard

#### 概述
Llama Guard（[Inan et al., 2023](https://arxiv.org/abs/2312.06674)）是Meta为LLaMA2和LLaMA3系列开发的安全性增强模块，专注于内容安全评估和过滤，确保模型输出符合预定的安全标准。

#### 特点
- **版本**：
  1. **Llama Guard 3 1B**：文本专用，适用于基本的内容安全评估。
  2. **Llama Guard 3 8B**：文本专用，适用于更复杂的内容安全评估，特别是在代码解释器滥用（S14）场景下。
  3. **Llama Guard 3 Vision**（[Chi et al., 2024](https://arxiv.org/abs/2411.10414)）：增强了多模态评估能力，支持图像与文本的联合评估。

#### 技术细节
- **多模态评估**：使用特定token（如 `<|image|>`）标识图像输入，结合文本进行安全性评估。
- **安全性分类**：基于ML Commons consortium定义的13个安全类别（S1-S13），在3.2版本中新增S14：代码解释器滥用。
- **评估流程**：用户提供包含安全类别和对话内容的提示，模型输出安全评估分类结果（安全或不安全）及相关违规类别。

#### 性能评估
Llama Guard在内容过滤和安全评估任务中表现出色，能够有效识别和过滤潜在的有害内容，确保模型输出的合规性和安全性。

#### 应用场景
- **内容审核**：自动化审核生成内容，确保符合平台和法律规定。
- **安全监控**：实时监控和过滤有害信息，保护用户和系统安全。
- **多模态应用**：在需要同时处理文本和图像内容的应用中，提供综合的安全评估。

---

### LLaMA3

#### 概述
LLaMA3（[Grattafiori et al., 2024](https://arxiv.org/abs/2411.10414)）是LLaMA系列的第三代模型，提供1B、3B、11B、70B、90B和405B六种参数规模，带来了多项重大升级和优化。

#### 特点
- **参数规模**：1B、3B、11B、70B、90B和405B六种版本。
- **训练数据量**：15万亿token，是LLaMA2的7.5倍。
- **Tokenizer更新**：采用更高效的`tiktoken` tokenizer，词汇表大小从32k扩展至128k。
- **上下文长度**：扩展至8192 tokens。
- **多语言支持**：覆盖30多种语言，显著提升多语言处理能力。
- **多模态支持**：推出视觉语言模型（11B和90B），支持图像理解和生成任务。
- **轻量级模型**：1B和3B参数量的模型，采用剪枝和知识蒸馏技术，适用于边缘设备和移动设备。

#### 技术细节
LLaMA3全面采用GQA（Grouped Query Attention）机制，优化自注意力计算的内存和计算效率。训练方法结合有监督微调、拒绝采样、近端策略优化（PPO）和直接策略优化（DPO），提升了模型的推理和编码能力。此外，视觉语言模型的引入，使其在处理图像和文本结合的任务中表现出色。

#### 性能评估
LLaMA3在预训练模型的各项基准测试中表现领先，尤其在推理、代码生成和指令跟随等应用场景中表现出色。最大的405B模型预计将与GPT-4和Claude 3.5相媲美，展示了强大的综合能力。

#### 应用场景
- **高级对话系统**：支持更复杂和多样化的对话场景，提升用户交互体验。
- **跨语言应用**：在多语言环境中提供高效的语言理解和生成，支持全球化应用。
- **多模态任务**：处理图像与文本结合的任务，如图像描述生成、视觉问答等。
- **边缘计算**：轻量级模型适用于资源受限的设备，如移动设备和物联网设备，拓展了应用范围。

## 关键技术解析

### LLaMA3的核心技术

LLaMA3作为系列最新版本，集成了LLaMA1和LLaMA2的核心技术，并在此基础上进行了多项创新和优化。以下是LLaMA3所采用的所有关键技术的详细解析，包括数学公式和代码说明。

#### RMSNorm

**RMSNorm**（Root Mean Square Layer Normalization）是LLaMA3中用于归一化的关键技术之一，取代了传统的LayerNorm。RMSNorm通过仅计算输入向量的均方根（RMS）来进行归一化，简化了计算过程，提升了效率。

数学表达式：
$$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d}x_i^2 + \epsilon}} \cdot \gamma
$$
其中：
- \( x \) 为输入向量。
- \( d \) 为特征维度。
- \( \epsilon \) 为一个小常数，防止除零错误。
- \( \gamma \) 为可学习的缩放参数。

**优势**：
- **计算效率高**：相比LayerNorm需要计算均值和方差，RMSNorm仅需计算均方根，减少了计算开销。
- **训练稳定性**：通过归一化输入，提升了模型的训练稳定性，使其在更大的学习率下仍能稳定训练。

#### FFN_SwiGLU

**FFN_SwiGLU**（Swish-Gated Linear Unit）是LLaMA3中用于增强前馈网络（Feed-Forward Network, FFN）非线性表达能力的关键技术。SwiGLU结合了Swish激活函数和门控机制，提升了模型的表现力。

数学表达式：
$$
\text{FFN}_{\text{SwiGLU}}(x) = (\text{SiLU}(xW_1) \otimes xW_3)W_2
$$
其中：
- \( \text{SiLU}(x) = x \cdot \sigma(x) \)（Swish激活函数）。
- \( \otimes \) 表示逐元素相乘。
- \( W_1, W_2, W_3 \) 为线性变换矩阵。

**优势**：
- **增强非线性表达**：结合Swish激活和门控机制，使得FFN层能够更好地捕捉复杂的模式和关系。
- **参数效率**：通过调整隐藏层的维度比例，保持FFN层的参数量不变，同时提升性能。
- **性能提升**：在多个基准测试中，FFN_SwiGLU显著提升了模型的性能，尤其在处理复杂任务和长文本时表现更为出色。

#### Rotary Positional Embeddings (RoPE)

**Rotary Positional Embeddings (RoPE)** 是LLaMA3中用于表示序列中位置关系的技术，通过对Query和Key向量应用旋转变换，增强了模型对相对位置信息的感知能力。

数学表达式：
$$
\text{RoPE}(x) = x \cdot e^{i\theta}
$$
其中：
- \( x \) 为输入向量。
- \( \theta \) 为与位置和维度相关的旋转角度。

**优势**：
- **相对位置感知**：RoPE能够自然地捕捉词汇之间的相对位置关系，提升了长距离依赖的建模效果。
- **计算效率高**：无需额外的计算，位置编码与词向量的结合在计算上是高效的，适用于大规模模型。
- **适应不同长度的序列**：RoPE可以灵活处理不同长度的输入序列，不受固定位置编码的限制。

#### Grouped Query Attention (GQA)

**Grouped Query Attention (GQA)** 是LLaMA3中用于优化自注意力计算的关键技术，通过将多个查询头分组，共享一组键值头，显著减少了内存占用和计算复杂度。

数学表达式：
$$
\text{memory\_kv-cache} = 4nhb(s + o)
$$
其中：
- \( nh \) 为头数。
- \( b \) 为批量大小。
- \( s \) 为序列长度。
- \( o \) 为输出长度。

**机制**：
1. **查询头分组**：将多个查询头划分为若干组，每组共享一组键值头。
2. **共享键值头**：每组查询头使用相同的键值头进行注意力计算，减少了键值缓存的内存占用。
3. **优化计算**：通过分组减少了重复计算，提高了推理效率，同时保持了与Multi-Head Attention (MHA)相近的模型性能。

**优势**：
- **内存优化**：显著减少了自注意力计算中的键值缓存内存占用。
- **计算效率**：优化了注意力计算流程，提升了推理速度，尤其在处理大规模模型时表现出色。
- **性能保持**：在减少内存和计算开销的同时，保持了与MHA相近的模型性能，确保生成质量不受影响。

#### 高效Tokenizer

**tiktoken** tokenizer是LLaMA3采用的新一代分词器，相较于LLaMA2使用的SentencePiece BPE，tiktoken在以下方面有所改进：

1. **词汇表扩展**：词汇表从32k扩展至128k，覆盖更多语言和专业术语，减少了分词次数，提升了生成质量。
2. **编码效率**：优化了编码算法，减少了分词时间，提高了处理速度。
3. **生成质量**：通过更细粒度的词汇表示，提升了模型生成文本的连贯性和准确性。

数学表达式（简化版）：
$$
\text{Tokenize}(w) = \text{BPE}(w) \quad \text{vs} \quad \text{Tokenize}(w) = \text{tiktoken\_BPE}(w)
$$
- 其中，\( w \) 为输入词汇，tiktoken\_BPE通过更大词汇表减少了分词次数。

**优势**：
- **减少分词次数**：更大的词汇表使得更多词汇能作为单一token处理，减少了分词次数，提高了生成效率和质量。
- **提升生成质量**：更细粒度的词汇表示，使模型在生成文本时能够更准确地表达复杂语义。
- **编码速度快**：优化的编码算法提升了分词速度，适用于大规模模型的高效训练和推理。

#### 多模态支持

LLaMA3引入了**视觉语言模型**（11B和90B），具备以下特点：

1. **图像理解与生成**：支持多模态任务，如图像描述、图像生成、视觉问答等，拓展了模型的应用场景。
2. **跨模态学习**：通过联合训练图像和文本数据，提升了模型在跨模态任务中的表现。
3. **高效推理**：优化了多模态模型的推理过程，确保在处理图像和文本时的高效性和准确性。

数学表达式（简化版）：
$$
\text{Multimodal\_Attention}(x, y) = \text{Attention}(x) + \text{Attention}(y) + \text{Cross-Attention}(x, y)
$$
- 其中，\( x \) 为文本输入，\( y \) 为图像输入，结合多模态注意力机制进行信息融合。

**优势**：
- **拓展应用场景**：支持图像与文本的结合应用，如自动图像描述、图像生成、视觉问答等。
- **提升理解能力**：通过跨模态学习，模型能够更全面地理解和生成复杂信息。
- **高效融合**：优化的多模态注意力机制确保了信息融合的高效性和准确性。

#### 轻量级模型

为了适应边缘设备和移动设备的需求，LLaMA3推出了**1B和3B参数量的轻量级模型**，采用以下技术：

1. **剪枝技术**：通过系统性地移除网络中的冗余参数，减小模型规模，同时保持核心性能。
2. **知识蒸馏**：让小模型从大模型中学习，提升其在特定任务上的表现。
3. **优化部署**：针对移动设备的硬件架构进行优化，如针对Arm处理器的性能调优，确保模型在资源受限环境中的高效运行。

数学表达式（简化版）：
$$
\text{Pruned\_Model} = \text{Prune}(\text{Original\_Model}, \text{Pruning\_Rate})
$$
$$
\text{Distilled\_Model} = \text{Distill}(\text{Large\_Model}, \text{Small\_Model})
$$
- 其中，Prune表示剪枝操作，Distill表示知识蒸馏过程。

**优势**：
- **适应资源受限设备**：减小模型规模，使其适用于边缘设备和移动设备，推动了大语言模型的普及。
- **保持性能**：通过剪枝和知识蒸馏技术，保持了模型的核心性能和表现。
- **高效运行**：优化的模型结构和权重格式（如BFloat16）提升了计算效率，确保在移动设备上的高效运行。

#### 训练方法

**LLaMA3**在训练数据和方法上进行了全面升级，采用了更大规模的数据和更先进的训练技术：

1. **预训练阶段**：
   - **大规模数据扩展**：训练数据量达到15万亿token，覆盖更多语言、专业领域和多模态数据，提升了模型的泛化能力和多语言支持。
   - **扩展法则（Scaling Laws）**：
     - 根据Chinchilla扩展法则，优化模型的训练数据量和参数规模平衡，确保模型在关键任务上的最佳性能。
     - 数学表达式：
       $$
       \text{Optimal Data} \propto \text{Model Size}^{4/3}
       $$
       这一公式指导了数据和模型规模的平衡，确保随着模型规模的增加，训练数据量也按比例增长，避免模型过拟合或欠拟合。

2. **并行训练策略**：
   - **数据并行**：将训练数据分布到多个GPU上，提升数据处理速度。
   - **模型并行**：将模型的不同部分分布到多个GPU上，支持更大规模的模型训练。
   - **流水并行**：分阶段处理模型的不同部分，提高训练效率。
   
   数学表达式：
   $$
   \text{Total Throughput} = \text{Data Parallelism} \times \text{Model Parallelism} \times \text{Pipeline Parallelism}
   $$
   - 其中，总吞吐量（Total Throughput）是数据并行、模型并行和流水并行的乘积，显著提升了训练效率。

3. **硬件优化**：
   - **高效利用GPU**：在16K GPU上实现每GPU超过400 TFLOPS的计算利用率，通过定制的24K GPU集群进行训练，确保训练过程的高效性和稳定性。
   - **错误处理与存储优化**：
     - **自动错误检测与处理**：确保训练过程的连续性和高效性。
     - **可扩展存储系统**：减少检查点和回滚的开销，提高数据存储效率。

4. **微调阶段**：
   - **多轮对齐步骤**：
     - **监督微调（SFT）**：使用高质量的标注数据进一步优化模型性能。
     - **拒绝采样（Rejection Sampling）**：通过拒绝低质量内容，提升生成文本的质量。
     - **近端策略优化（Proximal Policy Optimization, PPO）和直接策略优化（Direct Policy Optimization, DPO）**：结合两者的优势，优化模型的生成策略，使其更符合人类偏好。
   
   数学表达式：
   $$
   \mathcal{L}_{\text{RLHF}} = \mathbb{E}_{\theta \sim \pi_{\theta}} \left[ r(s, a) \right]
   $$
   - 其中，\( \mathcal{L}_{\text{RLHF}} \)为RLHF的损失函数，\( \pi_{\theta} \)为策略分布，\( r(s, a) \)为奖励函数。

5. **多模态训练**：
   - **视觉语言模型**：结合图像和文本数据，提升模型在多模态任务中的表现。
   - **代码数据扩展**：增加代码token数量，提升模型在编程任务中的表现。

6. **模型安全与质量控制**：
   - **数据过滤pipeline**：
     - **启发式过滤器**：基于规则的过滤，提高数据质量。
     - **NSFW过滤器**：去除不适内容，确保数据的安全性。
     - **语义重复数据删除**：使用语义分析技术，删除内容高度相似的数据。
     - **文本分类器**：预测数据质量，进一步优化数据集。

7. **优化训练堆栈**：
   - **高级训练堆栈**：自动检测和处理训练过程中的错误，提升硬件可靠性。
   - **性能调优**：针对不同硬件平台进行优化，确保训练过程的高效性。

**LLaMA3**通过这些先进的训练方法和优化策略，显著提升了模型的性能和适应性，成为开源大语言模型领域的领先者。

## 模型性能与应用对比

### LLaMA1 vs LLaMA2

**LLaMA2**在LLaMA1的基础上，通过扩大训练数据量、扩展上下文长度和引入GQA机制，显著提升了模型的性能和推理效率。在多语言支持和安全性方面，LLaMA2也比LLaMA1有了明显的进步。

**性能提升**：

- 在同等参数量下，LLaMA2在多项基准测试中均优于LLaMA1。
- 引入GQA机制后，LLaMA2在内存使用和计算效率上有了显著改善，尤其在长序列处理和多任务学习中表现更为出色。

**多语言支持**：

- LLaMA2扩展了多语言训练数据，覆盖30多种语言，相比LLaMA1仅侧重于英语，提升了模型在全球范围内的适用性。

**安全性和质量控制**：

- LLaMA2在数据过滤和毒性分析方面做了更为全面的考量，确保了模型输出的安全性和可靠性。

### LLaMA2 vs LLaMA3

**LLaMA3**在LLaMA2的基础上，进一步扩展了训练数据量、上下文长度和多模态能力，提升了模型的综合性能和应用范围。

**性能提升**：

- **LLaMA3**在1B、3B、11B、70B和405B版本上，表现超越了同规模的Gemma 7B和Mistral 7B Instruct，405B版本甚至战胜了闭源模型Claude 3 Sonnet。
- **LLaMA3**在多项基准测试中，尤其是在多语言和代码生成任务中，表现出色，接近或超过GPT-4。

**多模态能力**：

- **LLaMA3**引入了视觉语言模型（11B和90B），支持图像理解和生成，扩展了模型的应用场景。

**上下文长度和编码效率**：

- 将上下文长度从4096 tokens扩展至8192 tokens，**LLaMA3**在处理长文本和复杂任务时展现了更强的能力。
- 使用更高效的`tiktoken` tokenizer，词汇表扩展至128k，提升了编码效率和生成质量。

**多语言支持**：

- **LLaMA3**在多语言训练数据上进行了大幅扩展，支持30多种语言，显著提升了模型在全球多语言环境中的表现。

**代码生成能力**：

- **LLaMA3**通过扩充代码数据量，提升了模型在编程任务中的表现，尤其在Python代码生成和理解方面表现优异。

**模型安全与质量控制**：

- **LLaMA3**进一步优化了数据过滤pipeline，采用了更先进的技术手段，确保了训练数据的高质量和安全性。

# LLaMA 系列模型特性对比

| 特性               | LLaMA1               | LLaMA2                       | LLaMA3                              |
|--------------------|----------------------|------------------------------|-------------------------------------|
| **发布时间**        | 2023年2月            | 2023年7月                    | 2024年4月                           |
| **模型规模**        | 7B、13B、30B、65B    | 7B、13B、34B、70B             | 1B、3B、11B、70B、90B、405B         |
| **训练数据量**      | 1.4 万亿+ tokens     | 2 万亿 +tokens               | 15 万亿+ tokens                    |
| **上下文长度**      | 2048 tokens          | 4096 tokens                  | 128k tokens                        |
| **Tokenizer**      | SentencePiece BPE，32k 词汇表 | SentencePiece BPE，32k 词汇表 | tiktoken BPE，128k 词汇表          |
| **位置编码**        | RoPE                 | RoPE                         | RoPE                               |
| **注意力机制和推理优化** | Multi-Head Attention (MHA) | Grouped Query Attention (GQA)+ kv cache | Grouped Query Attention (GQA) + kv cache |
| **归一化方法**      | RMSNorm              | RMSNorm                      | RMSNorm                            |
| **激活函数**        | SwiGLU               | SwiGLU                       | SwiGLU                             |
| **训练资源**        | 2048 * A100 80GB     | 3.3M GPU hours on A100-80GB  | 16K H100 80GB                      |
| **应用场景**        | 通用语言理解与生成    | 通用语言理解与生成，推理效率进一步提升 | 多模态应用（图像、语言）、轻量级部署、边缘设备适配 |



## 总结

**LLaMA**系列模型从LLaMA1到LLaMA3，体现了大规模预训练语言模型的技术进化与产业影响。通过不断扩展训练数据量、优化模型架构和引入先进的训练方法，LLaMA系列在性能、多语言支持和多模态能力等方面取得了显著的提升。其开源策略不仅推动了全球AI社区的创新和发展，也为产业应用提供了强大的技术支持。


## 参考资料

1. [Hendrycks and Gimpel, 2016](https://arxiv.org/pdf/1606.08415.pdf)
2. [GLU Variants Improve Transformer](https://arxiv.org/pdf/2002.05202.pdf)
3. [LLaMA: Open and Efficient Foundation Language Models](https://openreview.net/pdf?id=SygkZ3MTJE)
4. [LLaMA2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288)
5. [详解三种常用标准化 Batch Norm & Layer Norm & RMSNorm](https://blog.csdn.net/wxc971231/article/details/139925707)
6. [Open-LLM-Components](https://github.com/jeff52415/open-llm-components/tree/main)
7. [Build Your Own Llama 3 Architecture from Scratch Using PyTorch](https://pub.towardsai.net/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c)
8. [LLaMA2 介绍 (模型结构 + 参数计算)](https://zhuanlan.zhihu.com/p/647862867)
9. [知乎 - LLaMA2 详解](https://zhuanlan.zhihu.com/p/649756898)
10. [Building LLaMA 3 From Scratch](https://github.com/meta-llama/llama/blob/main/llama/model.py)

