<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building Domain-Specific LLMs | Yue Shui Blog</title>
<meta name=keywords content="AI,NLP,LLM,Pre-training,Post-training,DPO,Domain Models,DeepSpeed"><meta name=description content="Background
With the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-01-05-domain-llm-training/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-01-05-domain-llm-training/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.19/dist/katex.min.css integrity=sha384-7lU0muIg/i1plk7MgygDUp3/bNRA65orrBub4/OSWHECgwEsY83HaS1x3bljA/XV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.19/dist/katex.min.js integrity=sha384-RdymN7NRJ+XoyeRY4185zXaxq9QWOOx3O7beyyrRK4KQZrPlCDQQpCu95FoCGPAE crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.19/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-01-05-domain-llm-training/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Building Domain-Specific LLMs"><meta property="og:description" content="Background With the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-05T12:00:00+08:00"><meta property="article:modified_time" content="2025-01-05T12:00:00+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Pre-Training"><meta property="article:tag" content="Post-Training"><meta property="article:tag" content="DPO"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Building Domain-Specific LLMs"><meta name=twitter:description content="Background
With the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Building Domain-Specific LLMs","item":"https://syhya.github.io/posts/2025-01-05-domain-llm-training/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building Domain-Specific LLMs","name":"Building Domain-Specific LLMs","description":"Background With the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.\n","keywords":["AI","NLP","LLM","Pre-training","Post-training","DPO","Domain Models","DeepSpeed"],"articleBody":"Background With the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.\nBased on my work experience, this article summarizes how to build LLMs equipped with specific domain knowledge by leveraging data preparation, model training, deployment, evaluation, and continuous iteration on top of existing general models.\nWhy Inject Domain Knowledge into the Foundational LLMs? Challenge 1: Limited Domain Knowledge Existing pre-trained models (such as GPT-4 and Llama 3) are primarily trained on general-purpose corpora, lacking in-depth understanding of niche languages or proprietary domains. This deficiency leads to subpar performance when the models handle programming code.\nChallenge 2: Data Security and Compliance When enterprises handle sensitive data, they must adhere to strict data sovereignty and compliance requirements. Uploading proprietary business data to third-party cloud services poses security risks, necessitating data processing and model training within local environments.\nChallenge 3: Limitations of OpenAI Fine-Tuning Mainstream commercial APIs for fine-tuning are typically basic and struggle to achieve deep alignment and optimization. For highly customized domain models, such approaches often fail to meet the required specifications.\nTwo Approaches of Injecting Knowledge In practical projects, the common methods for injecting domain knowledge into base models include Fine-Tuning and Retrieval-Augmented Generation (RAG). The following sections provide a detailed comparison of these methods to aid in selecting the most suitable strategy.\nMethod Comparison Fine-Tuning Core Concept\nThrough continued pre-training, supervised fine-tuning, and preference alignment, directly update the model parameters to enable it to master domain-specific knowledge and task patterns.\nTechnical Details\nContinued Pre-Training (CPT): Continue pre-training the base model on a large volume of domain-specific unsupervised data. Supervised Fine-Tuning (SFT): Perform supervised fine-tuning using high-quality labeled data. Preference Alignment (DPO): Optimize model outputs based on user feedback. Parameter Tuning Methods: Utilize full-parameter fine-tuning or combine with PEFT methods like LoRA to freeze some parameters and add adapters. Advantages\nDeep Customization: Updating the internal weights of the model enables a profound understanding of domain knowledge. No External Retrieval Dependency: Inference does not require additional knowledge bases, reducing latency and total token consumption. Enhanced Overall Performance: Significantly outperforms general models in domain-specific tasks. Disadvantages\nHigh Computational Cost: Requires substantial computational resources for training, especially during the CPT phase. Long Training Cycles: From data preparation to model training and optimization, the process is time-consuming. Catastrophic Forgetting: The model may forget its original general capabilities while learning new knowledge. Retrieval-Augmented Generation (RAG) Core Concept\nBuild a domain-specific knowledge base and retrieve relevant documents during inference to assist the model in generating more accurate responses without directly altering model parameters.\nTechnical Details\nData Processing: Preprocess domain documents by chunking them based on size and overlap. Vectorization: Embedding text chunks as vectors using embedding models and storing them in a Vector Store for retrieval. Retrieval: During inference, retrieve relevant documents through similarity search to provide contextual information or few-shot examples to the base model. Advantages\nPreserves General Capabilities: Model parameters remain unchanged, retaining general language abilities. Quick Updates: The knowledge base can be dynamically updated without retraining the model. Computational Efficiency: Avoids large-scale training, saving computational resources. Disadvantages\nDependence on Knowledge Base Quality: The quality of retrieved documents directly impacts response quality. Inference Speed: The retrieval process may increase inference latency and require more tokens. Limited Knowledge Coverage: The model’s internal knowledge is still restricted by the base model’s pre-training data. Models and Training Resources Base Models Taking the Llama 3 series as an example, it features the following characteristics:\nParameter Scale\nThe Llama 3 series includes models ranging from 1B to 405B parameters, widely supporting multilingual processing, code generation, reasoning, as well as visual and textual tasks. Smaller models (1B and 3B) are specially optimized for edge and mobile devices, supporting up to 128K context windows, efficiently handling local tasks such as summary generation, instruction execution, and text rewriting.\nMultimodal Capabilities\nLlama 3’s visual models (11B and 90B parameters) outperform many closed models in image understanding tasks and support multimodal processing of images, videos, and audio. All models support fine-tuning, facilitating customized development for specific domains.\nOpen Source and Community Support\nLlama 3 series models and their weights are released in open-source form and can be accessed via llama.com and the Hugging Face platform, providing convenient access and application support for developers.\nDataset Restrictions\nAlthough the Llama 3 models are released as open-source, the datasets used for their training are not open-sourced. Therefore, strictly speaking, Llama 3 is not entirely open-source. This limitation may pose challenges in addressing catastrophic forgetting, as obtaining data sets identical to the original training data is difficult.\nTraining Resources Training large language models requires robust computational resources and efficient distributed training frameworks.\nHardware Resources\nGPU Clusters: NVIDIA A100 or H100 GPUs are recommended, with configurations of 4 or 8 GPUs connected via NVLink or InfiniBand to enhance communication bandwidth. Storage Resources: High-performance SSDs (e.g., NVMe) to support fast data read and write operations. Software Frameworks\nDistributed Training Frameworks: DeepSpeed, Megatron-LM, among others, support large-scale model training. Inference Frameworks: vLLM, ollama, etc., optimize inference speed and resource utilization. Parallel Strategies\nData Parallelism (DP): Suitable when the model fits on a single GPU, implemented via DeepSpeed’s ZeRO Stage 0. Model Parallelism (MP), Pipeline Parallelism (PP), and Tensor Parallelism (TP): When the model cannot fit on a single GPU, optimize using ZeRO Stage 1, 2, or 3, or employ ZeRO-Infinity to offload parts of parameters and optimizer states to CPU or NVMe. DeepSpeed ZeRO Sharding Strategies Comparison ZeRO Stage Sharding Strategies ZeRO Stage Description GPU Memory Usage Training Speed ZeRO-0 Pure data parallelism without any sharding. All optimizer states, gradients, and parameters are fully replicated on each GPU. Highest Fastest ZeRO-1 Shards optimizer states (e.g., momentum and second moments), reducing GPU memory usage, but gradients and parameters remain data parallel. High Slightly slower than ZeRO-0 ZeRO-2 Shards optimizer states and gradients, further reducing GPU memory usage based on ZeRO-1. Medium Slower than ZeRO-1 ZeRO-3 Shards optimizer states, gradients, and model parameters, achieving the lowest GPU memory usage, suitable for extremely large models. Requires parameter broadcasting (All-Gather/All-Reduce) during forward/backward passes, significantly increasing communication overhead. Low Significantly slower than ZeRO-2, depends on model size and network bandwidth Offload Strategies Offload Type Description GPU Memory Usage Training Speed ZeRO-1 + CPU Offload Extends ZeRO-1 by offloading optimizer states to CPU memory, further reducing GPU memory usage but necessitating CPU-GPU data transfer, relying on PCIe bandwidth, and occupying CPU memory. Medium-low Slower than ZeRO-1, affected by CPU performance and PCIe bandwidth ZeRO-2 + CPU Offload Extends ZeRO-2 by offloading optimizer states to CPU memory, further reducing GPU memory usage for larger models but increasing CPU-GPU data transfer overhead. Lower Slower than ZeRO-2, affected by CPU performance and PCIe bandwidth ZeRO-3 + CPU Offload Extends ZeRO-3 by offloading optimizer states and model parameters to CPU, achieving minimal GPU memory usage but with extremely high CPU-GPU communication volume and CPU bandwidth significantly lower than GPU-GPU communication. Extremely Low Very Slow ZeRO-Infinity (NVMe Offload) Based on ZeRO-3, offloads optimizer states, gradients, and parameters to NVMe, breaking CPU memory limits and suitable for ultra-large-scale models; performance highly depends on NVMe parallel read/write speeds. Extremely LowRequires NVMe support Slower than ZeRO-3 but generally faster than ZeRO-3 + CPU Offload, can achieve better throughput if NVMe bandwidth is sufficient Communication Volume and Performance Impact ZeRO-0/1/2:\nCommunication is primarily gradient synchronization using All-Reduce operations, resulting in relatively low communication volume.\nZeRO-3:\nRequires All-Gather/All-Reduce operations for model parameters, significantly increasing communication volume. Network bandwidth becomes a critical bottleneck, and parameter broadcasting during forward/backward passes further exacerbates communication load.\nCPU Offload (ZeRO-1/2/3 + CPU):\nOffloads optimizer states or parameters to CPU, reducing GPU memory usage. Communication volume mainly arises from CPU \u003c-\u003e GPU data transfers, which have much lower bandwidth compared to GPU-GPU communication, easily causing performance bottlenecks, especially in ZeRO-3 scenarios. NVMe Offload (ZeRO-Infinity):\nFurther offloads to NVMe based on ZeRO-3, overcoming CPU memory limitations to support ultra-large-scale models. Performance heavily relies on NVMe I/O bandwidth and parallelism. If NVMe speed is sufficiently high, it typically outperforms CPU Offload; however, performance may suffer in scenarios with weak I/O performance or high latency. Hardware and Configuration Impact Hardware Constraints:\nPCIe Bandwidth, Network Bandwidth, NVMe I/O, etc., significantly impact Offload performance. Optimal strategies should be selected based on the hardware environment. Additional Notes:\nCPU Offload utilizes CPU memory and transfers data via PCIe; NVMe Offload saves states on NVMe devices. NVMe Offload generally outperforms CPU Offload when NVMe I/O performance is adequate, but care must be taken to avoid performance bottlenecks caused by insufficient I/O performance. Reference to Official Documentation:\nIt is recommended to consult the DeepSpeed official documentation for the latest and most accurate configuration parameters and performance tuning advice. Data Preparation: The Core of Training Success Data quality directly determines model performance. Data preparation includes data collection, cleaning, deduplication, categorization and balancing, anonymization, and other steps.\nPre-Training Data Data Sources Public Datasets: Such as the-stack-v2, Common Crawl, etc. Enterprise Proprietary Data: Internal documents, code repositories, business logs, etc. Web Crawlers: Collect domain-relevant web content using crawling technologies. Data Scale It is recommended to use at least hundreds of millions to billions of tokens to ensure the model can thoroughly learn domain knowledge. When data volume is insufficient, model performance may be limited. Data augmentation methods are suggested to supplement the data. Data Processing Data Preprocessing\nUniform Formatting: Process large volumes of unlabeled corpora from multiple data sources to ensure consistent formatting. It is recommended to use efficient storage formats like Parquet to improve data reading and processing efficiency. Data Deduplication\nDetection Methods: Use algorithms such as MinHash, SimHash, or cosine similarity for approximate duplicate detection. Granularity of Processing: Choose to deduplicate at the sentence, paragraph, or document level, adjusting flexibly based on task requirements. Similarity Threshold: Set a reasonable similarity threshold (e.g., 0.9) to remove texts with duplication above the threshold, ensuring data diversity. Data Cleaning\nText Filtering: Remove garbled text, spelling errors, and low-quality text by combining rule-based methods and model scorers (e.g., BERT/RoBERTa). Formatting Processing: Prefer using JSON format to handle data, ensuring the accuracy of special formats like code, Markdown, and LaTeX. Data Anonymization\nPrivacy Protection: Anonymize or remove sensitive information such as names, phone numbers, emails, passwords, etc., to ensure data compliance. Filtering Non-Compliant Content: Remove data blocks containing illegal, pornographic, or racially discriminatory content. Data Mixing and Balancing\nProportion Control: For example, combine 70% domain-specific data with 30% general data to prevent the model from forgetting general capabilities. Task Types: Ensure the data includes various task types such as code generation, Q\u0026A dialogue, document summarization, multi-turn conversations, and mathematical reasoning. Data Sequencing\nProgressive Guidance: Use Curriculum Learning to start training with simple, clean data and gradually introduce more complex or noisy data, optimizing the model’s learning efficiency and convergence path. Semantic Coherence: Utilize In-Context Pretraining techniques to concatenate semantically similar documents, enhancing contextual consistency and improving the model’s depth of semantic understanding and generalization ability. Supervised Fine-Tuning Data Data Format Adopt Alpaca or Vicuna styles, such as single-turn and multi-turn dialogues structured as [instruction, input, output].\nScale: From thousands to hundreds of thousands, depending on project requirements and computational resources. Quality: Ensure high-quality and diverse data to prevent the model from learning errors or biases. Data Construction During the data construction process, we first collect daily business data and collaboratively build foundational questions with business experts. Subsequently, we use large language models for data augmentation to enhance data diversity and robustness. The specific data augmentation strategies are as follows:\nData Augmentation Strategies Diverse Expressions\nRewrite existing data using large language models through synonym replacement and syntactic transformations to increase data diversity.\nRobustness Enhancement\nCreate prompts containing spelling errors, mixed languages, and other input variations to simulate real-world scenarios while ensuring high-quality generated answers.\nKnowledge Distillation\nUtilize large language models like GPT-4 and Claude for knowledge distillation to generate Q\u0026A pairs that meet requirements.\nComplex Task Design\nManually design high-quality data for complex scenarios (e.g., multi-turn dialogues, logical reasoning) to cover the model’s capability boundaries.\nData Generation Pipeline\nBuild an automated data generation pipeline that integrates data generation, filtering, formatting, and validation to improve overall efficiency.\nKey Points Task Type Annotation: Clearly annotate each data entry with its task type to facilitate subsequent fine-grained analysis and tuning. Multi-Turn Dialogues and Topic Switching: Construct data that captures contextual coherence and topic transitions in multi-turn dialogues to ensure the model learns the ability to handle topic switching and maintain contextual relevance. Chain of Thought (CoT) Strategy: For classification and reasoning tasks, generate procedural answers using CoT to improve accuracy. Data Flywheel: Continuously collect real user queries after deployment, iterating data based on real needs; regularly clean the data to ensure quality and diversity. Preference Data Data Format Triple Structure: [prompt, chosen answer, rejected answer] Annotation Details: Multi-Model Sampling: Generate answers using multiple models at different training stages or with different data ratios to increase data diversity. Editing and Optimization: Annotators can make slight modifications to the chosen answers to ensure answer quality. Sampling Strategies Multi-Model Sampling: Deploy multiple versions of the model to generate diverse answers for the same prompt. Comparative Annotation: Use manual or automated systems to compare generated answers and select superior answer pairs. Key Points Data Diversity and Coverage: Ensure preference data covers various scenarios and tasks to prevent the model from underperforming in specific contexts. High-Quality Annotation: The quality of preference data directly affects the model’s alignment, requiring accurate and consistent annotations. Training Process A complete training process for a domain-specific large language model typically includes Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO) as the three main steps, ultimately achieving model deployment and continuous optimization.\nComparison of Three Methods Training Method Overview Training Method Main Objective Data Requirements Typical Application Scenarios Continued Pre-Training (CPT) Continue pre-training on large-scale unsupervised corpora to inject new domain knowledge Large amounts of unlabeled text (at least hundreds of millions to billions of tokens) Supplementing domain knowledge, such as specialized texts in law, medicine, finance, etc. Supervised Fine-Tuning (SFT) Fine-tune on supervised labeled data to strengthen specific tasks and instruction execution capabilities Customized labeled data (instruction/dialog pairs), ranging from thousands to hundreds of thousands Various specific tasks, such as code generation, Q\u0026A, text rewriting, complex instruction execution, etc. Direct Preference Optimization (DPO) Optimize model outputs to align with human preferences using preference data (chosen vs. rejected) Preference data: [prompt, chosen, rejected](relatively smaller scale) Aligning with human feedback, such as response style, compliance, safety, etc. Advantages and Challenges Continued Pre-Training (CPT) Advantages:\nBetter domain coverage, comprehensively enhancing the model’s understanding and generation capabilities in specific domains. No need for additional manual annotation. Challenges/Limitations:\nRequires a large volume of high-quality domain data. High training costs, necessitating massive computational power and time. May introduce domain biases, necessitating careful handling of data quality and distribution. Supervised Fine-Tuning (SFT) Advantages:\nQuickly acquires task execution capabilities. Significantly improves accuracy in specific scenarios. Challenges/Limitations:\nHigh data annotation costs. Requires careful selection of labeled data to avoid overfitting. Fine-tuning may weaken the model’s generality. Direct Preference Optimization (DPO) Advantages:\nNo need to train a separate Reward Model. Requires less data and computational resources to achieve similar or better results compared to PPO. Challenges/Limitations:\nRequires reliable preference annotations. Continues to need more preference data for complex and diverse scenarios. Easily constrained by the distribution of preference data. General Training Tips and Technical Details When performing CPT, SFT, and DPO, there are numerous general training tips and technical details. The following sections uniformly describe these general aspects for better understanding and application.\nData Processing and Preparation Data Quality: Regardless of CPT, SFT, or DPO, data quality is crucial. Ensure data accuracy, unambiguity, and diversity. Data Formatting: Consistent data formats simplify the training process. For example, using JSON or other structured formats to store training data. Data Augmentation: Increase data diversity and improve the model’s generalization ability through methods like LLM rewriting and optimization. Learning Rate and Optimization Learning Rate Settings: Typically use a smaller learning rate than during pre-training, such as reducing from 3e-4 to 3e-5, depending on the task and data volume. Learning Rate Scheduling: Use warm-up strategies (e.g., linearly increasing for the first 10% of steps), followed by linear decay or cosine annealing to ensure a smooth training process. Optimizer Selection: Choose suitable optimizers based on model size and hardware resources, such as AdamW. Training Strategies Full-Parameter Fine-Tuning: When resources permit, prioritize full-parameter fine-tuning to ensure the model fully captures new knowledge. Parameter-Efficient Fine-Tuning (PEFT): Methods like LoRA are suitable for scenarios with limited computational resources by freezing some parameters and adding adapters for efficient fine-tuning. Mixed Precision Training: Use bf16 or fp16 on supported GPUs to reduce memory usage and increase training speed. Training Stability: Employ techniques such as gradient clipping, regularization, dropout, and weight decay to prevent gradient explosion and model overfitting. Flash Attention: Utilize Flash Attention to optimize the computation efficiency of the attention mechanism, enhancing training speed and reducing memory usage. Monitoring and Tuning Convergence Monitoring: Continuously monitor loss curves on training and validation sets to ensure the model is converging properly. Adjust learning rates and other hyperparameters as needed. Checkpoint: Regularly save checkpoints to prevent loss of all training progress due to unexpected interruptions. Early Stopping: Prevent model overfitting by stopping training at an appropriate time and saving the best model state. Model Evaluation: Conduct periodic evaluations during training to ensure model performance meets expectations. Continued Pre-Training (CPT) Objective Inject new domain knowledge into the base model by continuing pre-training on a large volume of domain-specific unsupervised data, enhancing the model’s understanding and generation capabilities in the specific domain.\nTraining Tips Streaming Data Loading\nImplement streaming data loading to dynamically read data during training, preventing memory overflows and training interruptions. Full-Parameter Fine-Tuning\nTypically, update all model parameters during training to ensure comprehensive knowledge acquisition. Compared to parameter-efficient fine-tuning methods (e.g., LoRA), full-parameter fine-tuning offers better domain knowledge injection, especially when computational resources are abundant. It is recommended to prioritize full-parameter fine-tuning under such conditions. Supervised Fine-Tuning (SFT) Objective Enhance the model’s practicality and accuracy by training it on high-quality labeled data to perform specific tasks such as code generation, code repair, and complex instruction execution.\nTraining Tips Number of Epochs\nTypically, 1 to 4 epochs are sufficient to observe significant effects when data volume is adequate. If data volume is insufficient, consider increasing the number of epochs while being mindful of overfitting risks. Data augmentation is recommended in such cases. Data Augmentation and Diversity\nEnsure training data covers a variety of task types and instruction expressions to improve the model’s generalization ability. Include multi-turn dialogues and robustness data to enhance the model’s capability to handle real user scenarios. Direct Preference Optimization (DPO) Objective Optimize model outputs to better align with human expectations and needs, including response style, safety, and readability, by leveraging user feedback and preference data.\nCharacteristics of DPO Direct Optimization\nDoes not require training a separate Reward Model. Instead, directly performs contrastive learning on (chosen, rejected) data pairs.\nEfficiency\nCompared to PPO, DPO requires less data and computational resources to achieve similar or better results.\nDynamic Adaptation\nThe model can immediately adapt whenever new data is available without the need to retrain a Reward Model.\nTraining Tips Collecting Preference Data\nDeploy multiple models at different training stages or with different data ratios to generate diverse responses. Annotate chosen and rejected answer pairs through manual or automated means to ensure data diversity and quality. Contrastive Learning\nOptimize model parameters by maximizing the probability of chosen answers and minimizing the probability of rejected answers. Iterative Optimization\nContinuously collect user feedback, generate new preference data, and perform iterative training to gradually enhance model performance. Implement a data flywheel mechanism to achieve ongoing model evolution and optimization. Common Issues and Solutions Repetitive Outputs\nIssue: The model generates repetitive content, continuously printing without stopping.\nSolutions:\nData Deduplication and Cleaning: Ensure training data does not contain a large amount of repetitive content. Check EOT (End-of-Token) Settings: Prevent the model from continuously generating without stopping. Align via SFT/DPO: Optimize model output quality. Adjust Decoding Strategies: Increase parameters like top_k, repetition penalty, and temperature. Catastrophic Forgetting\nIssue: The model forgets its original general capabilities during fine-tuning, effectively overfitting to the new dataset and causing excessive changes to the original model parameter space.\nSolutions:\nMix in Some General Data: Maintain the model’s general capabilities. Lower the Learning Rate: Reduce the impact on existing knowledge. Increase Dropout Rate and Weight Decay: Prevent overfitting. Use Parameter-Efficient Fine-Tuning Methods like LoRA: Avoid large-scale parameter updates. Utilize RAG Assistance: Combine with external knowledge bases to enhance model performance. Chat Vector: Quickly inject conversational and general capabilities into the model through simple arithmetic operations on model weights. Insufficient Understanding of Entity Relationships and Reasoning Paths\nIssue: The model struggles to correctly understand complex entity relationships and reasoning paths.\nSolutions:\nIntroduce Chain-of-Thought (CoT) Data and Enhanced Reasoning Training: Improve the model’s capabilities through step-by-step reasoning training, combined with Reinforcement Fine-Tuning and o1/o3 training methods. Expand Training Data Coverage: Incorporate more diverse scenarios containing complex entity relationships and reasoning paths. Combine with Knowledge Graph Modeling: Use GraphRAG to strengthen the model’s understanding and reasoning abilities regarding entity relationships. Model Deployment and Evaluation Deployment Inference Frameworks\nollama: Local inference deployment based on llama.cpp, enabling quick startups. vLLM: Optimized for high concurrency and inference throughput in multi-user scenarios. Quantization: Quantize the model to 8-bit or 4-bit to further reduce inference costs and improve deployment efficiency. Integrate RAG \u0026 Agents\nRAG: Combine with a vector knowledge base to retrieve relevant documents or code snippets in real-time, assisting the model in generating more accurate responses. Agents: Utilize Function Calls or multi-turn dialogue mechanisms to enable the model to invoke external tools or perform complex reasoning, enhancing interactivity and practicality. Langgraph: Encapsulate RAG and multi-agent workflows to build customized dialogue systems or automated code generation platforms. Evaluation Evaluation Metrics\nCPT Phase: Use domain-specific test sets to evaluate perplexity (PPL) or cross-entropy, measuring the model’s mastery of new knowledge. SFT/DPO Phase: Human or Model Evaluation: Assess the accuracy, coherence, readability, and safety of responses through human ratings or automated tools. Code Generation: Build a large-scale unit test set to evaluate the pass@k metric, measuring the correctness rate of code generation. General Capabilities: Test the model on common benchmarks (e.g., MMLU, CMMLU) to ensure minimal performance degradation on general tasks. Decoding Hyperparameters\nConsistency: Maintain consistent decoding parameters such as top_k, top_p, temperature, and max_new_tokens during evaluation to ensure comparability of results. Grid Search: When computational resources permit, evaluate different combinations of decoding parameters to select the optimal configuration. Data Flywheel and Continuous Iteration Data Flywheel Mechanism\nReal-Time Collection of User Logs\nCollect real user prompts and generated responses online, covering diverse usage scenarios and task types. Automated or Manual Annotation\nAnnotate collected user prompts and responses with preferences, generating new (chosen, rejected) data pairs. Iterative Training\nIncorporate newly generated preference data into the next round of SFT/DPO training to continuously optimize response quality and user experience. Robustness Data\nInclude data with spelling errors, mixed languages, vague instructions, etc., to enhance the model’s robustness and ability to handle real-world scenarios. Continuous Optimization\nFeedback Loop: Utilize user feedback to continuously improve training data and model performance, achieving self-optimization and evolution of the model. Multi-Model Collaboration: Deploy multiple versions of the model to generate diverse responses, enhancing the model’s comprehensive capabilities through contrastive learning. Integrating Intent Recognition and Multi-Agent Reasoning Use an intent classification model to allow the large model to determine the category of user input intent. Based on the mapping between intent categories and context types, supervise the reasoning path, and then perform multi-way retrieval based on the reasoning path. Provide this information to the trained model to generate the final result.\nConclusion Through the combination of Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO), it is possible to effectively inject domain-specific knowledge into base large models, constructing closed-source LLMs capable of efficiently solving business problems. The key steps are as follows:\nData Preparation\nHigh-quality data collection, cleaning, deduplication, and categorization to ensure data diversity and accuracy. Implement data anonymization strategies to protect privacy and ensure compliance. Model Training\nUse CPT to inject domain knowledge, SFT to learn specific task patterns, and DPO to optimize model outputs to align with human preferences and safety. Leverage efficient parallel training frameworks and hyperparameter tuning techniques to enhance training efficiency and resource utilization. Deployment and Evaluation\nEmploy efficient inference frameworks, integrating RAG and Agents for knowledge enhancement and functional extension. Conduct multi-dimensional evaluations to ensure the model performs as expected at each stage. Continuous Iteration\nBuild a data flywheel to continuously collect user feedback and optimize training data and model performance. Integrate RAG and Agents to achieve ongoing improvement and expansion of model capabilities. Ultimately, through a systematic process and technical measures, it is possible to construct an AI system with not only profound domain knowledge but also the flexibility to handle complex business requirements over its lifecycle.\nReferences DeepSpeed Megatron-LM ollama vLLM GraphRAG The Llama 3 Herd of Models ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages Evaluating Large Language Models Trained on Code Direct Preference Optimization: Your Language Model is Secretly a Reward Model Citation Citation: To reproduce or cite the content of this article, please acknowledge the original author and source.\nCited as:\nYue Shui. (Jan 2025). Building Domain-Specific LLMs.\nhttps://syhya.github.io/posts/2025-01-05-build-domain-llm\nOr\n@article{syhya2024domainllm, title = \"Building Domain-Specific LLMs\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Jan\", url = \"https://syhya.github.io/posts/2025-01-05-build-domain-llm/\" } ","wordCount":"4340","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-01-05T12:00:00+08:00","dateModified":"2025-01-05T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-01-05-domain-llm-training/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Building Domain-Specific LLMs</h1><div class=post-meta><span title='2025-01-05 12:00:00 +0800 +0800'>2025-01-05</span>&nbsp;·&nbsp;21 min&nbsp;·&nbsp;4340 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#background>Background</a></li><li><a href=#why-inject-domain-knowledge-into-the-foundational-llms>Why Inject Domain Knowledge into the Foundational LLMs?</a><ul><li><a href=#challenge-1-limited-domain-knowledge>Challenge 1: Limited Domain Knowledge</a></li><li><a href=#challenge-2-data-security-and-compliance>Challenge 2: Data Security and Compliance</a></li><li><a href=#challenge-3-limitations-of-openai-fine-tuning>Challenge 3: Limitations of OpenAI Fine-Tuning</a></li></ul></li><li><a href=#two-approaches-of-injecting-knowledge>Two Approaches of Injecting Knowledge</a><ul><li><a href=#method-comparison>Method Comparison</a></li></ul></li><li><a href=#models-and-training-resources>Models and Training Resources</a><ul><li><a href=#base-models>Base Models</a></li><li><a href=#training-resources>Training Resources</a></li></ul></li><li><a href=#deepspeed-zero-sharding-strategies-comparison>DeepSpeed ZeRO Sharding Strategies Comparison</a><ul><li><a href=#zero-stage-sharding-strategies>ZeRO Stage Sharding Strategies</a></li><li><a href=#offload-strategies>Offload Strategies</a></li></ul></li><li><a href=#communication-volume-and-performance-impact>Communication Volume and Performance Impact</a><ul><li><a href=#hardware-and-configuration-impact>Hardware and Configuration Impact</a></li></ul></li><li><a href=#data-preparation-the-core-of-training-success>Data Preparation: The Core of Training Success</a><ul><li><a href=#pre-training-data>Pre-Training Data</a></li><li><a href=#supervised-fine-tuning-data>Supervised Fine-Tuning Data</a></li><li><a href=#preference-data>Preference Data</a></li></ul></li><li><a href=#training-process>Training Process</a><ul><li><a href=#comparison-of-three-methods>Comparison of Three Methods</a></li><li><a href=#general-training-tips-and-technical-details>General Training Tips and Technical Details</a></li><li><a href=#continued-pre-training-cpt-1>Continued Pre-Training (CPT)</a></li><li><a href=#supervised-fine-tuning-sft-1>Supervised Fine-Tuning (SFT)</a></li><li><a href=#direct-preference-optimization-dpo-1>Direct Preference Optimization (DPO)</a></li><li><a href=#common-issues-and-solutions>Common Issues and Solutions</a></li></ul></li><li><a href=#model-deployment-and-evaluation>Model Deployment and Evaluation</a><ul><li><a href=#deployment>Deployment</a></li><li><a href=#evaluation>Evaluation</a></li></ul></li><li><a href=#data-flywheel-and-continuous-iteration>Data Flywheel and Continuous Iteration</a></li><li><a href=#integrating-intent-recognition-and-multi-agent-reasoning>Integrating Intent Recognition and Multi-Agent Reasoning</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>With the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.</p><p>Based on my work experience, this article summarizes how to build LLMs equipped with specific domain knowledge by leveraging data preparation, model training, deployment, evaluation, and continuous iteration on top of existing general models.</p><h2 id=why-inject-domain-knowledge-into-the-foundational-llms>Why Inject Domain Knowledge into the Foundational LLMs?<a hidden class=anchor aria-hidden=true href=#why-inject-domain-knowledge-into-the-foundational-llms>#</a></h2><h3 id=challenge-1-limited-domain-knowledge>Challenge 1: Limited Domain Knowledge<a hidden class=anchor aria-hidden=true href=#challenge-1-limited-domain-knowledge>#</a></h3><p>Existing pre-trained models (such as GPT-4 and Llama 3) are primarily trained on general-purpose corpora, lacking in-depth understanding of niche languages or proprietary domains. This deficiency leads to subpar performance when the models handle programming code.</p><h3 id=challenge-2-data-security-and-compliance>Challenge 2: Data Security and Compliance<a hidden class=anchor aria-hidden=true href=#challenge-2-data-security-and-compliance>#</a></h3><p>When enterprises handle sensitive data, they must adhere to strict data sovereignty and compliance requirements. Uploading proprietary business data to third-party cloud services poses security risks, necessitating data processing and model training within local environments.</p><h3 id=challenge-3-limitations-of-openai-fine-tuning>Challenge 3: Limitations of OpenAI Fine-Tuning<a hidden class=anchor aria-hidden=true href=#challenge-3-limitations-of-openai-fine-tuning>#</a></h3><p>Mainstream commercial APIs for fine-tuning are typically basic and struggle to achieve deep alignment and optimization. For highly customized domain models, such approaches often fail to meet the required specifications.</p><hr><h2 id=two-approaches-of-injecting-knowledge>Two Approaches of Injecting Knowledge<a hidden class=anchor aria-hidden=true href=#two-approaches-of-injecting-knowledge>#</a></h2><p>In practical projects, the common methods for injecting domain knowledge into base models include <strong>Fine-Tuning</strong> and <strong>Retrieval-Augmented Generation (RAG)</strong>. The following sections provide a detailed comparison of these methods to aid in selecting the most suitable strategy.</p><h3 id=method-comparison>Method Comparison<a hidden class=anchor aria-hidden=true href=#method-comparison>#</a></h3><h4 id=fine-tuning>Fine-Tuning<a hidden class=anchor aria-hidden=true href=#fine-tuning>#</a></h4><p><strong>Core Concept</strong><br>Through continued pre-training, supervised fine-tuning, and preference alignment, directly update the model parameters to enable it to master domain-specific knowledge and task patterns.</p><p><strong>Technical Details</strong></p><ul><li><strong>Continued Pre-Training (CPT)</strong>: Continue pre-training the base model on a large volume of domain-specific unsupervised data.</li><li><strong>Supervised Fine-Tuning (SFT)</strong>: Perform supervised fine-tuning using high-quality labeled data.</li><li><strong>Preference Alignment (DPO)</strong>: Optimize model outputs based on user feedback.</li><li><strong>Parameter Tuning Methods</strong>: Utilize full-parameter fine-tuning or combine with PEFT methods like LoRA to freeze some parameters and add adapters.</li></ul><p><strong>Advantages</strong></p><ul><li><strong>Deep Customization</strong>: Updating the internal weights of the model enables a profound understanding of domain knowledge.</li><li><strong>No External Retrieval Dependency</strong>: Inference does not require additional knowledge bases, reducing latency and total token consumption.</li><li><strong>Enhanced Overall Performance</strong>: Significantly outperforms general models in domain-specific tasks.</li></ul><p><strong>Disadvantages</strong></p><ul><li><strong>High Computational Cost</strong>: Requires substantial computational resources for training, especially during the CPT phase.</li><li><strong>Long Training Cycles</strong>: From data preparation to model training and optimization, the process is time-consuming.</li><li><strong>Catastrophic Forgetting</strong>: The model may forget its original general capabilities while learning new knowledge.</li></ul><h4 id=retrieval-augmented-generation-rag>Retrieval-Augmented Generation (RAG)<a hidden class=anchor aria-hidden=true href=#retrieval-augmented-generation-rag>#</a></h4><p><strong>Core Concept</strong><br>Build a domain-specific knowledge base and retrieve relevant documents during inference to assist the model in generating more accurate responses without directly altering model parameters.</p><p><strong>Technical Details</strong></p><ul><li><strong>Data Processing</strong>: Preprocess domain documents by chunking them based on size and overlap.</li><li><strong>Vectorization</strong>: Embedding text chunks as vectors using embedding models and storing them in a Vector Store for retrieval.</li><li><strong>Retrieval</strong>: During inference, retrieve relevant documents through similarity search to provide contextual information or few-shot examples to the base model.</li></ul><p><strong>Advantages</strong></p><ul><li><strong>Preserves General Capabilities</strong>: Model parameters remain unchanged, retaining general language abilities.</li><li><strong>Quick Updates</strong>: The knowledge base can be dynamically updated without retraining the model.</li><li><strong>Computational Efficiency</strong>: Avoids large-scale training, saving computational resources.</li></ul><p><strong>Disadvantages</strong></p><ul><li><strong>Dependence on Knowledge Base Quality</strong>: The quality of retrieved documents directly impacts response quality.</li><li><strong>Inference Speed</strong>: The retrieval process may increase inference latency and require more tokens.</li><li><strong>Limited Knowledge Coverage</strong>: The model’s internal knowledge is still restricted by the base model’s pre-training data.</li></ul><hr><h2 id=models-and-training-resources>Models and Training Resources<a hidden class=anchor aria-hidden=true href=#models-and-training-resources>#</a></h2><h3 id=base-models>Base Models<a hidden class=anchor aria-hidden=true href=#base-models>#</a></h3><p>Taking the <a href=https://arxiv.org/pdf/2407.21783>Llama 3 series</a> as an example, it features the following characteristics:</p><ul><li><p><strong>Parameter Scale</strong><br>The Llama 3 series includes models ranging from 1B to 405B parameters, widely supporting multilingual processing, code generation, reasoning, as well as visual and textual tasks. Smaller models (1B and 3B) are specially optimized for edge and mobile devices, supporting up to 128K context windows, efficiently handling local tasks such as summary generation, instruction execution, and text rewriting.</p></li><li><p><strong>Multimodal Capabilities</strong><br>Llama 3&rsquo;s visual models (11B and 90B parameters) outperform many closed models in image understanding tasks and support multimodal processing of images, videos, and audio. All models support fine-tuning, facilitating customized development for specific domains.</p></li><li><p><strong>Open Source and Community Support</strong><br>Llama 3 series models and their weights are released in open-source form and can be accessed via <a href=https://llama.com>llama.com</a> and the <a href=https://huggingface.co/meta-llama>Hugging Face platform</a>, providing convenient access and application support for developers.</p></li><li><p><strong>Dataset Restrictions</strong><br>Although the Llama 3 models are released as open-source, the datasets used for their training are not open-sourced. Therefore, strictly speaking, Llama 3 is not entirely open-source. This limitation may pose challenges in addressing catastrophic forgetting, as obtaining data sets identical to the original training data is difficult.</p></li></ul><h3 id=training-resources>Training Resources<a hidden class=anchor aria-hidden=true href=#training-resources>#</a></h3><p>Training large language models requires robust computational resources and efficient distributed training frameworks.</p><ul><li><p><strong>Hardware Resources</strong></p><ul><li><strong>GPU Clusters</strong>: NVIDIA A100 or H100 GPUs are recommended, with configurations of 4 or 8 GPUs connected via NVLink or InfiniBand to enhance communication bandwidth.</li><li><strong>Storage Resources</strong>: High-performance SSDs (e.g., NVMe) to support fast data read and write operations.</li></ul></li><li><p><strong>Software Frameworks</strong></p><ul><li><strong>Distributed Training Frameworks</strong>: <a href=https://github.com/microsoft/DeepSpeed>DeepSpeed</a>, <a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a>, among others, support large-scale model training.</li><li><strong>Inference Frameworks</strong>: <a href=https://github.com/vllm-project/vllm>vLLM</a>, <a href=https://github.com/jmorganca/ollama>ollama</a>, etc., optimize inference speed and resource utilization.</li></ul></li><li><p><strong>Parallel Strategies</strong></p><ul><li><strong>Data Parallelism (DP)</strong>: Suitable when the model fits on a single GPU, implemented via DeepSpeed&rsquo;s ZeRO Stage 0.</li><li><strong>Model Parallelism (MP), Pipeline Parallelism (PP), and Tensor Parallelism (TP)</strong>: When the model cannot fit on a single GPU, optimize using ZeRO Stage 1, 2, or 3, or employ ZeRO-Infinity to offload parts of parameters and optimizer states to CPU or NVMe.</li></ul></li></ul><h2 id=deepspeed-zero-sharding-strategies-comparison>DeepSpeed ZeRO Sharding Strategies Comparison<a hidden class=anchor aria-hidden=true href=#deepspeed-zero-sharding-strategies-comparison>#</a></h2><h3 id=zero-stage-sharding-strategies>ZeRO Stage Sharding Strategies<a hidden class=anchor aria-hidden=true href=#zero-stage-sharding-strategies>#</a></h3><table><thead><tr><th><strong>ZeRO Stage</strong></th><th><strong>Description</strong></th><th><strong>GPU Memory Usage</strong></th><th><strong>Training Speed</strong></th></tr></thead><tbody><tr><td><strong>ZeRO-0</strong></td><td>Pure data parallelism without any sharding. All optimizer states, gradients, and parameters are fully replicated on each GPU.</td><td>Highest</td><td><strong>Fastest</strong></td></tr><tr><td><strong>ZeRO-1</strong></td><td>Shards optimizer states (e.g., momentum and second moments), reducing GPU memory usage, but gradients and parameters remain data parallel.</td><td>High</td><td>Slightly slower than ZeRO-0</td></tr><tr><td><strong>ZeRO-2</strong></td><td>Shards optimizer states and gradients, further reducing GPU memory usage based on ZeRO-1.</td><td>Medium</td><td>Slower than ZeRO-1</td></tr><tr><td><strong>ZeRO-3</strong></td><td>Shards optimizer states, gradients, and model parameters, achieving the lowest GPU memory usage, suitable for extremely large models. Requires parameter broadcasting (All-Gather/All-Reduce) during forward/backward passes, significantly increasing communication overhead.</td><td>Low</td><td>Significantly slower than ZeRO-2, depends on model size and network bandwidth</td></tr></tbody></table><h3 id=offload-strategies>Offload Strategies<a hidden class=anchor aria-hidden=true href=#offload-strategies>#</a></h3><table><thead><tr><th><strong>Offload Type</strong></th><th><strong>Description</strong></th><th><strong>GPU Memory Usage</strong></th><th><strong>Training Speed</strong></th></tr></thead><tbody><tr><td><strong>ZeRO-1 + CPU Offload</strong></td><td>Extends ZeRO-1 by offloading optimizer states to CPU memory, further reducing GPU memory usage but necessitating CPU-GPU data transfer, relying on PCIe bandwidth, and occupying CPU memory.</td><td>Medium-low</td><td>Slower than ZeRO-1, affected by CPU performance and PCIe bandwidth</td></tr><tr><td><strong>ZeRO-2 + CPU Offload</strong></td><td>Extends ZeRO-2 by offloading optimizer states to CPU memory, further reducing GPU memory usage for larger models but increasing CPU-GPU data transfer overhead.</td><td>Lower</td><td>Slower than ZeRO-2, affected by CPU performance and PCIe bandwidth</td></tr><tr><td><strong>ZeRO-3 + CPU Offload</strong></td><td>Extends ZeRO-3 by offloading optimizer states and model parameters to CPU, achieving minimal GPU memory usage but with extremely high CPU-GPU communication volume and CPU bandwidth significantly lower than GPU-GPU communication.</td><td>Extremely Low</td><td>Very Slow</td></tr><tr><td><strong>ZeRO-Infinity (NVMe Offload)</strong></td><td>Based on ZeRO-3, offloads optimizer states, gradients, and parameters to NVMe, breaking CPU memory limits and suitable for ultra-large-scale models; performance highly depends on NVMe parallel read/write speeds.</td><td>Extremely LowRequires NVMe support</td><td>Slower than ZeRO-3 but generally faster than ZeRO-3 + CPU Offload, can achieve better throughput if NVMe bandwidth is sufficient</td></tr></tbody></table><hr><h2 id=communication-volume-and-performance-impact>Communication Volume and Performance Impact<a hidden class=anchor aria-hidden=true href=#communication-volume-and-performance-impact>#</a></h2><ul><li><p><strong>ZeRO-0/1/2</strong>:<br>Communication is primarily <strong>gradient synchronization</strong> using All-Reduce operations, resulting in relatively low communication volume.</p></li><li><p><strong>ZeRO-3</strong>:<br>Requires <strong>All-Gather/All-Reduce</strong> operations for model parameters, significantly increasing communication volume. Network bandwidth becomes a critical bottleneck, and parameter broadcasting during forward/backward passes further exacerbates communication load.</p></li><li><p><strong>CPU Offload</strong> (ZeRO-1/2/3 + CPU):</p><ul><li>Offloads optimizer states or parameters to CPU, reducing GPU memory usage.</li><li>Communication volume mainly arises from <strong>CPU &lt;-> GPU</strong> data transfers, which have much lower bandwidth compared to GPU-GPU communication, easily causing performance bottlenecks, especially in <strong>ZeRO-3</strong> scenarios.</li></ul></li><li><p><strong>NVMe Offload</strong> (ZeRO-Infinity):</p><ul><li>Further offloads to NVMe based on <strong>ZeRO-3</strong>, overcoming CPU memory limitations to support ultra-large-scale models.</li><li>Performance heavily relies on <strong>NVMe I/O bandwidth</strong> and parallelism. If NVMe speed is sufficiently high, it typically outperforms CPU Offload; however, performance may suffer in scenarios with weak I/O performance or high latency.</li></ul></li></ul><h3 id=hardware-and-configuration-impact>Hardware and Configuration Impact<a hidden class=anchor aria-hidden=true href=#hardware-and-configuration-impact>#</a></h3><ul><li><p><strong>Hardware Constraints</strong>:</p><ul><li><strong>PCIe Bandwidth</strong>, <strong>Network Bandwidth</strong>, <strong>NVMe I/O</strong>, etc., significantly impact Offload performance. Optimal strategies should be selected based on the hardware environment.</li></ul></li><li><p><strong>Additional Notes</strong>:</p><ul><li><strong>CPU Offload</strong> utilizes CPU memory and transfers data via PCIe; <strong>NVMe Offload</strong> saves states on NVMe devices.</li><li>NVMe Offload generally outperforms CPU Offload when <strong>NVMe I/O performance is adequate</strong>, but care must be taken to avoid performance bottlenecks caused by insufficient I/O performance.</li></ul></li><li><p><strong>Reference to Official Documentation</strong>:</p><ul><li>It is recommended to consult the <a href=https://www.deepspeed.ai/>DeepSpeed official documentation</a> for the latest and most accurate configuration parameters and performance tuning advice.</li></ul></li></ul><hr><h2 id=data-preparation-the-core-of-training-success>Data Preparation: The Core of Training Success<a hidden class=anchor aria-hidden=true href=#data-preparation-the-core-of-training-success>#</a></h2><p>Data quality directly determines model performance. Data preparation includes data collection, cleaning, deduplication, categorization and balancing, anonymization, and other steps.</p><h3 id=pre-training-data>Pre-Training Data<a hidden class=anchor aria-hidden=true href=#pre-training-data>#</a></h3><h4 id=data-sources>Data Sources<a hidden class=anchor aria-hidden=true href=#data-sources>#</a></h4><ul><li><strong>Public Datasets</strong>: Such as <a href=https://huggingface.co/datasets/bigcode/the-stack-v2>the-stack-v2</a>, Common Crawl, etc.</li><li><strong>Enterprise Proprietary Data</strong>: Internal documents, code repositories, business logs, etc.</li><li><strong>Web Crawlers</strong>: Collect domain-relevant web content using crawling technologies.</li></ul><h4 id=data-scale>Data Scale<a hidden class=anchor aria-hidden=true href=#data-scale>#</a></h4><ul><li>It is recommended to use at least hundreds of millions to billions of tokens to ensure the model can thoroughly learn domain knowledge.</li><li>When data volume is insufficient, model performance may be limited. Data augmentation methods are suggested to supplement the data.</li></ul><h4 id=data-processing>Data Processing<a hidden class=anchor aria-hidden=true href=#data-processing>#</a></h4><ol><li><p><strong>Data Preprocessing</strong></p><ul><li><strong>Uniform Formatting</strong>: Process large volumes of unlabeled corpora from multiple data sources to ensure consistent formatting. It is recommended to use efficient storage formats like Parquet to improve data reading and processing efficiency.</li></ul></li><li><p><strong>Data Deduplication</strong></p><ul><li><strong>Detection Methods</strong>: Use algorithms such as MinHash, SimHash, or cosine similarity for approximate duplicate detection.</li><li><strong>Granularity of Processing</strong>: Choose to deduplicate at the sentence, paragraph, or document level, adjusting flexibly based on task requirements.</li><li><strong>Similarity Threshold</strong>: Set a reasonable similarity threshold (e.g., 0.9) to remove texts with duplication above the threshold, ensuring data diversity.</li></ul></li><li><p><strong>Data Cleaning</strong></p><ul><li><strong>Text Filtering</strong>: Remove garbled text, spelling errors, and low-quality text by combining rule-based methods and model scorers (e.g., BERT/RoBERTa).</li><li><strong>Formatting Processing</strong>: Prefer using JSON format to handle data, ensuring the accuracy of special formats like code, Markdown, and LaTeX.</li></ul></li><li><p><strong>Data Anonymization</strong></p><ul><li><strong>Privacy Protection</strong>: Anonymize or remove sensitive information such as names, phone numbers, emails, passwords, etc., to ensure data compliance.</li><li><strong>Filtering Non-Compliant Content</strong>: Remove data blocks containing illegal, pornographic, or racially discriminatory content.</li></ul></li><li><p><strong>Data Mixing and Balancing</strong></p><ul><li><strong>Proportion Control</strong>: For example, combine 70% domain-specific data with 30% general data to prevent the model from forgetting general capabilities.</li><li><strong>Task Types</strong>: Ensure the data includes various task types such as code generation, Q&amp;A dialogue, document summarization, multi-turn conversations, and mathematical reasoning.</li></ul></li><li><p><strong>Data Sequencing</strong></p><ul><li><strong>Progressive Guidance</strong>: Use Curriculum Learning to start training with simple, clean data and gradually introduce more complex or noisy data, optimizing the model&rsquo;s learning efficiency and convergence path.</li><li><strong>Semantic Coherence</strong>: Utilize In-Context Pretraining techniques to concatenate semantically similar documents, enhancing contextual consistency and improving the model&rsquo;s depth of semantic understanding and generalization ability.</li></ul></li></ol><h3 id=supervised-fine-tuning-data>Supervised Fine-Tuning Data<a hidden class=anchor aria-hidden=true href=#supervised-fine-tuning-data>#</a></h3><h4 id=data-format>Data Format<a hidden class=anchor aria-hidden=true href=#data-format>#</a></h4><p>Adopt Alpaca or Vicuna styles, such as single-turn and multi-turn dialogues structured as [instruction, input, output].</p><ul><li><strong>Scale</strong>: From thousands to hundreds of thousands, depending on project requirements and computational resources.</li><li><strong>Quality</strong>: Ensure high-quality and diverse data to prevent the model from learning errors or biases.</li></ul><h4 id=data-construction>Data Construction<a hidden class=anchor aria-hidden=true href=#data-construction>#</a></h4><p>During the data construction process, we first collect daily business data and collaboratively build foundational questions with business experts. Subsequently, we use large language models for data augmentation to enhance data diversity and robustness. The specific data augmentation strategies are as follows:</p><h4 id=data-augmentation-strategies>Data Augmentation Strategies<a hidden class=anchor aria-hidden=true href=#data-augmentation-strategies>#</a></h4><ul><li><p><strong>Diverse Expressions</strong><br>Rewrite existing data using large language models through synonym replacement and syntactic transformations to increase data diversity.</p></li><li><p><strong>Robustness Enhancement</strong><br>Create prompts containing spelling errors, mixed languages, and other input variations to simulate real-world scenarios while ensuring high-quality generated answers.</p></li><li><p><strong>Knowledge Distillation</strong><br>Utilize large language models like GPT-4 and Claude for knowledge distillation to generate Q&amp;A pairs that meet requirements.</p></li><li><p><strong>Complex Task Design</strong><br>Manually design high-quality data for complex scenarios (e.g., multi-turn dialogues, logical reasoning) to cover the model&rsquo;s capability boundaries.</p></li><li><p><strong>Data Generation Pipeline</strong><br>Build an automated data generation pipeline that integrates data generation, filtering, formatting, and validation to improve overall efficiency.</p></li></ul><h4 id=key-points>Key Points<a hidden class=anchor aria-hidden=true href=#key-points>#</a></h4><ul><li><strong>Task Type Annotation</strong>: Clearly annotate each data entry with its task type to facilitate subsequent fine-grained analysis and tuning.</li><li><strong>Multi-Turn Dialogues and Topic Switching</strong>: Construct data that captures contextual coherence and topic transitions in multi-turn dialogues to ensure the model learns the ability to handle topic switching and maintain contextual relevance.</li><li><strong>Chain of Thought (CoT) Strategy</strong>: For classification and reasoning tasks, generate procedural answers using CoT to improve accuracy.</li><li><strong>Data Flywheel</strong>: Continuously collect real user queries after deployment, iterating data based on real needs; regularly clean the data to ensure quality and diversity.</li></ul><h3 id=preference-data>Preference Data<a hidden class=anchor aria-hidden=true href=#preference-data>#</a></h3><h4 id=data-format-1>Data Format<a hidden class=anchor aria-hidden=true href=#data-format-1>#</a></h4><ul><li><strong>Triple Structure</strong>: [prompt, chosen answer, rejected answer]</li><li><strong>Annotation Details</strong>:<ul><li><strong>Multi-Model Sampling</strong>: Generate answers using multiple models at different training stages or with different data ratios to increase data diversity.</li><li><strong>Editing and Optimization</strong>: Annotators can make slight modifications to the chosen answers to ensure answer quality.</li></ul></li></ul><h4 id=sampling-strategies>Sampling Strategies<a hidden class=anchor aria-hidden=true href=#sampling-strategies>#</a></h4><ul><li><strong>Multi-Model Sampling</strong>: Deploy multiple versions of the model to generate diverse answers for the same prompt.</li><li><strong>Comparative Annotation</strong>: Use manual or automated systems to compare generated answers and select superior answer pairs.</li></ul><h4 id=key-points-1>Key Points<a hidden class=anchor aria-hidden=true href=#key-points-1>#</a></h4><ul><li><strong>Data Diversity and Coverage</strong>: Ensure preference data covers various scenarios and tasks to prevent the model from underperforming in specific contexts.</li><li><strong>High-Quality Annotation</strong>: The quality of preference data directly affects the model&rsquo;s alignment, requiring accurate and consistent annotations.</li></ul><hr><h2 id=training-process>Training Process<a hidden class=anchor aria-hidden=true href=#training-process>#</a></h2><p>A complete training process for a domain-specific large language model typically includes <strong>Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO)</strong> as the three main steps, ultimately achieving model deployment and continuous optimization.</p><h3 id=comparison-of-three-methods>Comparison of Three Methods<a hidden class=anchor aria-hidden=true href=#comparison-of-three-methods>#</a></h3><h4 id=training-method-overview>Training Method Overview<a hidden class=anchor aria-hidden=true href=#training-method-overview>#</a></h4><table><thead><tr><th><strong>Training Method</strong></th><th><strong>Main Objective</strong></th><th><strong>Data Requirements</strong></th><th><strong>Typical Application Scenarios</strong></th></tr></thead><tbody><tr><td><strong>Continued Pre-Training (CPT)</strong></td><td>Continue pre-training on large-scale unsupervised corpora to inject new domain knowledge</td><td>Large amounts of unlabeled text (at least hundreds of millions to billions of tokens)</td><td>Supplementing domain knowledge, such as specialized texts in law, medicine, finance, etc.</td></tr><tr><td><strong>Supervised Fine-Tuning (SFT)</strong></td><td>Fine-tune on supervised labeled data to strengthen specific tasks and instruction execution capabilities</td><td>Customized labeled data (instruction/dialog pairs), ranging from thousands to hundreds of thousands</td><td>Various specific tasks, such as code generation, Q&amp;A, text rewriting, complex instruction execution, etc.</td></tr><tr><td><strong>Direct Preference Optimization (DPO)</strong></td><td>Optimize model outputs to align with human preferences using preference data (chosen vs. rejected)</td><td>Preference data: [prompt, chosen, rejected](relatively smaller scale)</td><td>Aligning with human feedback, such as response style, compliance, safety, etc.</td></tr></tbody></table><h4 id=advantages-and-challenges>Advantages and Challenges<a hidden class=anchor aria-hidden=true href=#advantages-and-challenges>#</a></h4><h5 id=continued-pre-training-cpt>Continued Pre-Training (CPT)<a hidden class=anchor aria-hidden=true href=#continued-pre-training-cpt>#</a></h5><p><strong>Advantages</strong>:</p><ul><li>Better domain coverage, comprehensively enhancing the model&rsquo;s understanding and generation capabilities in specific domains.</li><li>No need for additional manual annotation.</li></ul><p><strong>Challenges/Limitations</strong>:</p><ul><li>Requires a large volume of high-quality domain data.</li><li>High training costs, necessitating massive computational power and time.</li><li>May introduce domain biases, necessitating careful handling of data quality and distribution.</li></ul><h5 id=supervised-fine-tuning-sft>Supervised Fine-Tuning (SFT)<a hidden class=anchor aria-hidden=true href=#supervised-fine-tuning-sft>#</a></h5><p><strong>Advantages</strong>:</p><ul><li>Quickly acquires task execution capabilities.</li><li>Significantly improves accuracy in specific scenarios.</li></ul><p><strong>Challenges/Limitations</strong>:</p><ul><li>High data annotation costs.</li><li>Requires careful selection of labeled data to avoid overfitting.</li><li>Fine-tuning may weaken the model&rsquo;s generality.</li></ul><h5 id=direct-preference-optimization-dpo>Direct Preference Optimization (DPO)<a hidden class=anchor aria-hidden=true href=#direct-preference-optimization-dpo>#</a></h5><p><strong>Advantages</strong>:</p><ul><li>No need to train a separate Reward Model.</li><li>Requires less data and computational resources to achieve similar or better results compared to PPO.</li></ul><p><strong>Challenges/Limitations</strong>:</p><ul><li>Requires reliable preference annotations.</li><li>Continues to need more preference data for complex and diverse scenarios.</li><li>Easily constrained by the distribution of preference data.</li></ul><hr><h3 id=general-training-tips-and-technical-details>General Training Tips and Technical Details<a hidden class=anchor aria-hidden=true href=#general-training-tips-and-technical-details>#</a></h3><p>When performing <strong>CPT, SFT, and DPO</strong>, there are numerous general training tips and technical details. The following sections uniformly describe these general aspects for better understanding and application.</p><h4 id=data-processing-and-preparation>Data Processing and Preparation<a hidden class=anchor aria-hidden=true href=#data-processing-and-preparation>#</a></h4><ul><li><strong>Data Quality</strong>: Regardless of CPT, SFT, or DPO, data quality is crucial. Ensure data accuracy, unambiguity, and diversity.</li><li><strong>Data Formatting</strong>: Consistent data formats simplify the training process. For example, using JSON or other structured formats to store training data.</li><li><strong>Data Augmentation</strong>: Increase data diversity and improve the model&rsquo;s generalization ability through methods like LLM rewriting and optimization.</li></ul><h4 id=learning-rate-and-optimization>Learning Rate and Optimization<a hidden class=anchor aria-hidden=true href=#learning-rate-and-optimization>#</a></h4><ul><li><strong>Learning Rate Settings</strong>: Typically use a smaller learning rate than during pre-training, such as reducing from 3e-4 to 3e-5, depending on the task and data volume.</li><li><strong>Learning Rate Scheduling</strong>: Use warm-up strategies (e.g., linearly increasing for the first 10% of steps), followed by linear decay or cosine annealing to ensure a smooth training process.</li><li><strong>Optimizer Selection</strong>: Choose suitable optimizers based on model size and hardware resources, such as AdamW.</li></ul><h4 id=training-strategies>Training Strategies<a hidden class=anchor aria-hidden=true href=#training-strategies>#</a></h4><ul><li><strong>Full-Parameter Fine-Tuning</strong>: When resources permit, prioritize full-parameter fine-tuning to ensure the model fully captures new knowledge.</li><li><strong>Parameter-Efficient Fine-Tuning (PEFT)</strong>: Methods like LoRA are suitable for scenarios with limited computational resources by freezing some parameters and adding adapters for efficient fine-tuning.</li><li><strong>Mixed Precision Training</strong>: Use bf16 or fp16 on supported GPUs to reduce memory usage and increase training speed.</li><li><strong>Training Stability</strong>: Employ techniques such as gradient clipping, regularization, dropout, and weight decay to prevent gradient explosion and model overfitting.</li><li><strong>Flash Attention</strong>: Utilize <a href=https://github.com/Dao-AILab/flash-attention>Flash Attention</a> to optimize the computation efficiency of the attention mechanism, enhancing training speed and reducing memory usage.</li></ul><h4 id=monitoring-and-tuning>Monitoring and Tuning<a hidden class=anchor aria-hidden=true href=#monitoring-and-tuning>#</a></h4><ul><li><strong>Convergence Monitoring</strong>: Continuously monitor loss curves on training and validation sets to ensure the model is converging properly. Adjust learning rates and other hyperparameters as needed.</li><li><strong>Checkpoint</strong>: Regularly save checkpoints to prevent loss of all training progress due to unexpected interruptions.</li><li><strong>Early Stopping</strong>: Prevent model overfitting by stopping training at an appropriate time and saving the best model state.</li><li><strong>Model Evaluation</strong>: Conduct periodic evaluations during training to ensure model performance meets expectations.</li></ul><h3 id=continued-pre-training-cpt-1>Continued Pre-Training (CPT)<a hidden class=anchor aria-hidden=true href=#continued-pre-training-cpt-1>#</a></h3><h4 id=objective>Objective<a hidden class=anchor aria-hidden=true href=#objective>#</a></h4><p>Inject new domain knowledge into the base model by continuing pre-training on a large volume of domain-specific unsupervised data, enhancing the model&rsquo;s understanding and generation capabilities in the specific domain.</p><h4 id=training-tips>Training Tips<a hidden class=anchor aria-hidden=true href=#training-tips>#</a></h4><ol><li><p><strong>Streaming Data Loading</strong></p><ul><li>Implement streaming data loading to dynamically read data during training, preventing memory overflows and training interruptions.</li></ul></li><li><p><strong>Full-Parameter Fine-Tuning</strong></p><ul><li>Typically, update all model parameters during training to ensure comprehensive knowledge acquisition.</li><li>Compared to parameter-efficient fine-tuning methods (e.g., LoRA), full-parameter fine-tuning offers better domain knowledge injection, especially when computational resources are abundant. It is recommended to prioritize full-parameter fine-tuning under such conditions.</li></ul></li></ol><h3 id=supervised-fine-tuning-sft-1>Supervised Fine-Tuning (SFT)<a hidden class=anchor aria-hidden=true href=#supervised-fine-tuning-sft-1>#</a></h3><h4 id=objective-1>Objective<a hidden class=anchor aria-hidden=true href=#objective-1>#</a></h4><p>Enhance the model&rsquo;s practicality and accuracy by training it on high-quality labeled data to perform specific tasks such as code generation, code repair, and complex instruction execution.</p><h4 id=training-tips-1>Training Tips<a hidden class=anchor aria-hidden=true href=#training-tips-1>#</a></h4><ol><li><p><strong>Number of Epochs</strong></p><ul><li>Typically, 1 to 4 epochs are sufficient to observe significant effects when data volume is adequate.</li><li>If data volume is insufficient, consider increasing the number of epochs while being mindful of overfitting risks. Data augmentation is recommended in such cases.</li></ul></li><li><p><strong>Data Augmentation and Diversity</strong></p><ul><li>Ensure training data covers a variety of task types and instruction expressions to improve the model&rsquo;s generalization ability.</li><li>Include multi-turn dialogues and robustness data to enhance the model&rsquo;s capability to handle real user scenarios.</li></ul></li></ol><h3 id=direct-preference-optimization-dpo-1>Direct Preference Optimization (DPO)<a hidden class=anchor aria-hidden=true href=#direct-preference-optimization-dpo-1>#</a></h3><h4 id=objective-2>Objective<a hidden class=anchor aria-hidden=true href=#objective-2>#</a></h4><p>Optimize model outputs to better align with human expectations and needs, including response style, safety, and readability, by leveraging user feedback and preference data.</p><h4 id=characteristics-of-dpo>Characteristics of DPO<a hidden class=anchor aria-hidden=true href=#characteristics-of-dpo>#</a></h4><ul><li><p><strong>Direct Optimization</strong><br>Does not require training a separate Reward Model. Instead, directly performs contrastive learning on (chosen, rejected) data pairs.</p></li><li><p><strong>Efficiency</strong><br>Compared to PPO, DPO requires less data and computational resources to achieve similar or better results.</p></li><li><p><strong>Dynamic Adaptation</strong><br>The model can immediately adapt whenever new data is available without the need to retrain a Reward Model.</p></li></ul><h4 id=training-tips-2>Training Tips<a hidden class=anchor aria-hidden=true href=#training-tips-2>#</a></h4><ol><li><p><strong>Collecting Preference Data</strong></p><ul><li>Deploy multiple models at different training stages or with different data ratios to generate diverse responses.</li><li>Annotate chosen and rejected answer pairs through manual or automated means to ensure data diversity and quality.</li></ul></li><li><p><strong>Contrastive Learning</strong></p><ul><li>Optimize model parameters by maximizing the probability of chosen answers and minimizing the probability of rejected answers.</li></ul></li><li><p><strong>Iterative Optimization</strong></p><ul><li>Continuously collect user feedback, generate new preference data, and perform iterative training to gradually enhance model performance.</li><li>Implement a data flywheel mechanism to achieve ongoing model evolution and optimization.</li></ul></li></ol><hr><h3 id=common-issues-and-solutions>Common Issues and Solutions<a hidden class=anchor aria-hidden=true href=#common-issues-and-solutions>#</a></h3><ol><li><p><strong>Repetitive Outputs</strong><br><strong>Issue</strong>: The model generates repetitive content, continuously printing without stopping.<br><strong>Solutions</strong>:</p><ul><li><strong>Data Deduplication and Cleaning</strong>: Ensure training data does not contain a large amount of repetitive content.</li><li><strong>Check EOT (End-of-Token) Settings</strong>: Prevent the model from continuously generating without stopping.</li><li><strong>Align via SFT/DPO</strong>: Optimize model output quality.</li><li><strong>Adjust Decoding Strategies</strong>: Increase parameters like top_k, repetition penalty, and temperature.</li></ul></li><li><p><strong>Catastrophic Forgetting</strong><br><strong>Issue</strong>: The model forgets its original general capabilities during fine-tuning, effectively overfitting to the new dataset and causing excessive changes to the original model parameter space.<br><strong>Solutions</strong>:</p><ul><li><strong>Mix in Some General Data</strong>: Maintain the model’s general capabilities.</li><li><strong>Lower the Learning Rate</strong>: Reduce the impact on existing knowledge.</li><li><strong>Increase Dropout Rate and Weight Decay</strong>: Prevent overfitting.</li><li><strong>Use Parameter-Efficient Fine-Tuning Methods like LoRA</strong>: Avoid large-scale parameter updates.</li><li><strong>Utilize RAG Assistance</strong>: Combine with external knowledge bases to enhance model performance.</li><li><strong><a href=https://arxiv.org/pdf/2310.04799>Chat Vector</a></strong>: Quickly inject conversational and general capabilities into the model through simple arithmetic operations on model weights.</li></ul></li><li><p><strong>Insufficient Understanding of Entity Relationships and Reasoning Paths</strong><br><strong>Issue</strong>: The model struggles to correctly understand complex entity relationships and reasoning paths.<br><strong>Solutions</strong>:</p><ul><li><strong>Introduce Chain-of-Thought (CoT) Data and Enhanced Reasoning Training</strong>: Improve the model&rsquo;s capabilities through step-by-step reasoning training, combined with <a href=https://openai.com/form/rft-research-program/>Reinforcement Fine-Tuning</a> and <a href=https://openai.com/o1/>o1/o3</a> training methods.</li><li><strong>Expand Training Data Coverage</strong>: Incorporate more diverse scenarios containing complex entity relationships and reasoning paths.</li><li><strong>Combine with Knowledge Graph Modeling</strong>: Use <a href=https://github.com/microsoft/graphrag>GraphRAG</a> to strengthen the model&rsquo;s understanding and reasoning abilities regarding entity relationships.</li></ul></li></ol><hr><h2 id=model-deployment-and-evaluation>Model Deployment and Evaluation<a hidden class=anchor aria-hidden=true href=#model-deployment-and-evaluation>#</a></h2><h3 id=deployment>Deployment<a hidden class=anchor aria-hidden=true href=#deployment>#</a></h3><p><strong>Inference Frameworks</strong></p><ul><li><a href=https://github.com/jmorganca/ollama><strong>ollama</strong></a>: Local inference deployment based on <a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a>, enabling quick startups.</li><li><a href=https://github.com/vllm-project/vllm><strong>vLLM</strong></a>: Optimized for high concurrency and inference throughput in multi-user scenarios.</li><li><strong>Quantization</strong>: Quantize the model to 8-bit or 4-bit to further reduce inference costs and improve deployment efficiency.</li></ul><p><strong>Integrate RAG & Agents</strong></p><ul><li><strong>RAG</strong>: Combine with a vector knowledge base to retrieve relevant documents or code snippets in real-time, assisting the model in generating more accurate responses.</li><li><strong>Agents</strong>: Utilize Function Calls or multi-turn dialogue mechanisms to enable the model to invoke external tools or perform complex reasoning, enhancing interactivity and practicality.</li><li><strong>Langgraph</strong>: Encapsulate RAG and multi-agent workflows to build customized dialogue systems or automated code generation platforms.</li></ul><h3 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h3><p><strong>Evaluation Metrics</strong></p><ul><li><strong>CPT Phase</strong>: Use domain-specific test sets to evaluate perplexity (PPL) or cross-entropy, measuring the model&rsquo;s mastery of new knowledge.</li><li><strong>SFT/DPO Phase</strong>:<ul><li><strong>Human or Model Evaluation</strong>: Assess the accuracy, coherence, readability, and safety of responses through human ratings or automated tools.</li><li><strong>Code Generation</strong>: Build a large-scale unit test set to evaluate the pass@k metric, measuring the correctness rate of code generation.</li><li><strong>General Capabilities</strong>: Test the model on common benchmarks (e.g., MMLU, CMMLU) to ensure minimal performance degradation on general tasks.</li></ul></li></ul><p><strong>Decoding Hyperparameters</strong></p><ul><li><strong>Consistency</strong>: Maintain consistent decoding parameters such as top_k, top_p, temperature, and max_new_tokens during evaluation to ensure comparability of results.</li><li><strong>Grid Search</strong>: When computational resources permit, evaluate different combinations of decoding parameters to select the optimal configuration.</li></ul><hr><h2 id=data-flywheel-and-continuous-iteration>Data Flywheel and Continuous Iteration<a hidden class=anchor aria-hidden=true href=#data-flywheel-and-continuous-iteration>#</a></h2><p><strong>Data Flywheel Mechanism</strong></p><ol><li><p><strong>Real-Time Collection of User Logs</strong></p><ul><li>Collect real user prompts and generated responses online, covering diverse usage scenarios and task types.</li></ul></li><li><p><strong>Automated or Manual Annotation</strong></p><ul><li>Annotate collected user prompts and responses with preferences, generating new (chosen, rejected) data pairs.</li></ul></li><li><p><strong>Iterative Training</strong></p><ul><li>Incorporate newly generated preference data into the next round of SFT/DPO training to continuously optimize response quality and user experience.</li></ul></li><li><p><strong>Robustness Data</strong></p><ul><li>Include data with spelling errors, mixed languages, vague instructions, etc., to enhance the model’s robustness and ability to handle real-world scenarios.</li></ul></li></ol><p><strong>Continuous Optimization</strong></p><ul><li><strong>Feedback Loop</strong>: Utilize user feedback to continuously improve training data and model performance, achieving self-optimization and evolution of the model.</li><li><strong>Multi-Model Collaboration</strong>: Deploy multiple versions of the model to generate diverse responses, enhancing the model&rsquo;s comprehensive capabilities through contrastive learning.</li></ul><hr><h2 id=integrating-intent-recognition-and-multi-agent-reasoning>Integrating Intent Recognition and Multi-Agent Reasoning<a hidden class=anchor aria-hidden=true href=#integrating-intent-recognition-and-multi-agent-reasoning>#</a></h2><p>Use an intent classification model to allow the large model to determine the category of user input intent. Based on the mapping between intent categories and context types, supervise the reasoning path, and then perform multi-way retrieval based on the reasoning path. Provide this information to the trained model to generate the final result.</p><hr><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Through the combination of <strong>Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO)</strong>, it is possible to effectively inject domain-specific knowledge into base large models, constructing closed-source LLMs capable of efficiently solving business problems. The key steps are as follows:</p><ol><li><p><strong>Data Preparation</strong></p><ul><li>High-quality data collection, cleaning, deduplication, and categorization to ensure data diversity and accuracy.</li><li>Implement data anonymization strategies to protect privacy and ensure compliance.</li></ul></li><li><p><strong>Model Training</strong></p><ul><li>Use CPT to inject domain knowledge, SFT to learn specific task patterns, and DPO to optimize model outputs to align with human preferences and safety.</li><li>Leverage efficient parallel training frameworks and hyperparameter tuning techniques to enhance training efficiency and resource utilization.</li></ul></li><li><p><strong>Deployment and Evaluation</strong></p><ul><li>Employ efficient inference frameworks, integrating RAG and Agents for knowledge enhancement and functional extension.</li><li>Conduct multi-dimensional evaluations to ensure the model performs as expected at each stage.</li></ul></li><li><p><strong>Continuous Iteration</strong></p><ul><li>Build a data flywheel to continuously collect user feedback and optimize training data and model performance.</li><li>Integrate RAG and Agents to achieve ongoing improvement and expansion of model capabilities.</li></ul></li></ol><p>Ultimately, through a systematic process and technical measures, it is possible to construct an AI system with not only profound domain knowledge but also the flexibility to handle complex business requirements over its lifecycle.</p><hr><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://github.com/microsoft/DeepSpeed>DeepSpeed</a></li><li><a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a></li><li><a href=https://github.com/jmorganca/ollama>ollama</a></li><li><a href=https://github.com/vllm-project/vllm>vLLM</a></li><li><a href=https://microsoft.github.io/graphrag/>GraphRAG</a></li><li><a href=https://arxiv.org/pdf/2407.21783>The Llama 3 Herd of Models</a></li><li><a href=https://arxiv.org/abs/2104.07857>ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a></li><li><a href=https://arxiv.org/pdf/2310.04799>Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages</a></li><li><a href=https://arxiv.org/pdf/2107.03374>Evaluating Large Language Models Trained on Code</a></li><li><a href=https://arxiv.org/abs/2305.18290>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></li></ol><hr><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: To reproduce or cite the content of this article, please acknowledge the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Jan 2025). Building Domain-Specific LLMs.<br><a href=https://syhya.github.io/posts/2025-01-05-build-domain-llm>https://syhya.github.io/posts/2025-01-05-build-domain-llm</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2024domainllm</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;Building Domain-Specific LLMs&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Jan&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-01-05-build-domain-llm/&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/nlp/>NLP</a></li><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/tags/pre-training/>Pre-Training</a></li><li><a href=https://syhya.github.io/tags/post-training/>Post-Training</a></li><li><a href=https://syhya.github.io/tags/dpo/>DPO</a></li><li><a href=https://syhya.github.io/tags/domain-models/>Domain Models</a></li><li><a href=https://syhya.github.io/tags/deepspeed/>DeepSpeed</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-01-16-gqa-attention/><span class=title>« Prev</span><br><span>Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA</span>
</a><a class=next href=https://syhya.github.io/posts/2024-12-21-build-gpu-server/><span class=title>Next »</span><br><span>Building a Home Deep Learning Rig with Dual RTX 4090 GPUs</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Domain-Specific LLMs on x" href="https://x.com/intent/tweet/?text=Building%20Domain-Specific%20LLMs&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-05-domain-llm-training%2f&amp;hashtags=AI%2cNLP%2cLLM%2cPre-training%2cPost-training%2cDPO%2cDomainModels%2cDeepSpeed"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Domain-Specific LLMs on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-05-domain-llm-training%2f&amp;title=Building%20Domain-Specific%20LLMs&amp;summary=Building%20Domain-Specific%20LLMs&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-05-domain-llm-training%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Domain-Specific LLMs on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-05-domain-llm-training%2f&title=Building%20Domain-Specific%20LLMs"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Domain-Specific LLMs on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-05-domain-llm-training%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Domain-Specific LLMs on whatsapp" href="https://api.whatsapp.com/send?text=Building%20Domain-Specific%20LLMs%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-05-domain-llm-training%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Domain-Specific LLMs on telegram" href="https://telegram.me/share/url?text=Building%20Domain-Specific%20LLMs&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-05-domain-llm-training%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building Domain-Specific LLMs on ycombinator" href="https://news.ycombinator.com/submitlink?t=Building%20Domain-Specific%20LLMs&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-05-domain-llm-training%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>