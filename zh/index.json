[{"content":"大语言模型（LLMs）目前应用场景不断扩展，但也暴露出知识截止、幻觉以及复杂计算与逻辑推理不足等局限。为应对这些挑战，将智能体（Agent）与强化学习（Reinforcement Learning, RL）相结合的 Agentic RL 正逐渐成为关键研究方向。\nAgentic RL 通过让模型与外部世界（如搜索引擎、代码解释器、数据库、浏览器等）形成闭环交互，并借助奖励信号持续优化，使 LLM 拥有自主规划、决策制定、工具使用与环境交互等能力。在实际业务中，它不仅能理解需求并自主规划，还能在执行与反馈循环中不断修正与优化。\n其核心价值主要体现在两方面：\n减少提示依赖: 让模型摆脱对 prompt 的过度依赖，具备自适应问题求解能力； 强化自主探索: 借助多轮强化学习，提升探索与推理能力，从而弥补静态数据分布稀疏或重复带来的不足。 Agentic RL 与 LLM-RL 区别 Fig. 1. Paradigm shift from LLM-RL to Agentic RL. (Image source: Zhang et al., 2025)\n以 RLHF 为代表的对齐式 LLM-RL 在实践中常被近似为单步（序列级）决策的马尔可夫决策过程（Markov Decision Process, MDP）；而 Agentic RL 则在部分可观测环境中展开，涉及多步、长时程的序列决策，更适合用部分可观测马尔可夫决策过程（Partially Observable Markov Decision Process, POMDP）进行刻画。下面表格也总结了两者之间的差异。\n特性 传统 LLM-RL (如 RLHF) Agentic RL 决策过程 单步退化 MDP：输入 prompt → 输出完整 response → 一次性奖励，类似“单轮映射”。 多步长时程 POMDP：在部分可观测环境中持续交互，每一步都会更新状态并获得反馈。 状态空间 \\(\\mathcal{S}\\) 静态，仅由输入 prompt 决定，不随过程演化。 动态，包含历史交互、工具返回结果、外部环境状态等，随交互不断更新。 动作空间 \\(\\mathcal{A}\\) 单一动作：生成文本序列（response）。 复合动作：生成思考链（Thought）、调用工具（Tool Call）、更新状态、生成最终答案。 奖励 \\(\\mathcal{R}\\) 稀疏 Outcome Reward：多在生成完成后，由人工偏好或模型裁判给出。 混合奖励机制：既包括稀疏 Outcome Reward，也结合稠密 Process Reward（如工具调用成败、子任务完成度）。 核心挑战 对齐人类偏好，保证安全性与有用性；提升整体生成质量。 长时程信用分配、复杂任务规划、探索效率、工具的稳健使用，以及探索—利用的平衡。 评估 科学、全面且贴近真实的评估基准是衡量和提升 LLM Agent 能力的关键。\u0026ldquo;Successful language model evals\u0026rdquo; (Wei, 2024)总结了成功评估基准的几大关键特质，这些特质共同决定了一个评测集能否被社区广泛接受并经受住时间的考验：\n足够的样本量：评测集需要包含足够多的样本（通常建议至少 1000 个），以减少评估结果的随机波动。样本量过少会导致评测分数在不同模型检查点之间剧烈震荡，给研究者带来困扰，使其难以判断模型性能的真实变化。 高质量的数据：评测集中的数据（问题、答案、测试用例等）必须准确无误。如果评测集自身存在大量错误，当一个强大的模型（如 GPT-4）给出与标准答案不符的结果时，研究者会更倾向于质疑评测集的正确性，从而丧失对该评测集的信任。 简洁的单一度量指标：一个成功的评测集必须有一个核心的、易于理解的单一度量指标（如准确率）。过于复杂的评估体系，如 HELM (Liang et al., 2022) 的早期版本，虽然全面但因指标过多，反而让研究者难以聚焦，不利于快速比较和传播。 易于运行和复现：评估流程应尽可能简单、高效。如果运行一次评测需要复杂的环境配置和漫长的等待时间如 BIG-Bench (Srivastava et al., 2022) 的部分子集，会极大地阻碍其推广和应用。 任务富有意义：评测的任务应该与智能的核心能力（如语言理解、数学推理、代码生成）紧密相关。一些虽然有挑战性但意义不大的任务（如正确闭合括号），即使模型表现优异，也难以得出关于其“智能水平”的实质性结论。 评分机制准确可靠：自动化评分脚本必须极其健壮和准确。如果研究者在调试时发现模型的正确输出被评分脚本误判，会迅速削弱他们对整个评测集的信心。 避免在评测集上过拟合：评测集的难度应具有前瞻性，确保在未来一段时间内，模型性能仍有足够的提升空间。像 GLUE (Wang et al., 2018) 和 SuperGLUE (Wang et al., 2019)这样很快被模型性能刷满的评测集，会迅速失去作为衡量技术进步标尺的作用。 验证的不对称性(Wei, 2024)这个概念指出，许多任务验证一个解的正确性远比从零开始找到这个解要容易得多。\nFig. 2. Improving Verification with Privileged Information. (Image source: Wei, 2024)\n例如，解决一个复杂的数独谜题可能需要数小时，但验证一个填好的数独是否正确只需几分钟；编写一个大型网站的后端代码需要团队数年的努力，但任何用户都能快速判断网站是否正常工作。类似地，很多信息检索或开放型任务，生成答案可能要经过大量尝试，但一旦有候选结果，验证它是否符合约束往往只需很少的时间。\n如果我们能为任务提前准备先验信息或验证机制，就能显著降低验证成本。比如：\nSWE-bench：验证代码正确性本来需要人工逐行阅读，但如果提前准备好覆盖率足够的测试用例，那么只需运行测试，就能在几秒钟内判断模型生成的补丁是否有效。 AIME 数学竞赛：数学题目的推导过程往往复杂且耗时，但一旦公布标准答案，任何解答都可以通过对照答案在几秒钟内完成验证。 这种不对称性对于 AI 训练至关重要，因为它直接关系到构建 RL 环境或奖励模型的可行性。由此，Jason Wei 提出了验证者法则：\n训练 AI 解决一个任务的难易程度，与该任务的可验证性成正比。所有可解且易于验证的任务，终将被 AI 解决。\n一个任务是否易于验证，通常取决于它是否满足以下五个关键属性：\n客观真理：对于什么是好的解答存在广泛共识（如数学题的唯一正确答案）。 快速验证：单个解答能在秒级完成核验（如运行一组测试用例）。 可扩展验证：可以并行验证大量候选解答（例如批量运行代码测试）。 低噪声：验证信号与解答质量高度相关，误判率低。 连续奖励：不仅能判定对错，还能对多个解答进行排序，从而形成更平滑的优化信号。 这套法则解释了为何像 SWE-bench 编程任务 和 AIME 数学解题这样的场景，成为检验 AI 能力的理想试金石。它们天然符合上述大部分条件，使得我们能够高效构建自动化评测体系，并通过大规模“生成–验证”循环不断优化模型表现。\nSWE-bench Fig. 3. SWE-bench links real GitHub issues with their merged pull request fixes. Given an issue description and a codebase snapshot, models generate a patch that is tested against actual project test cases. (Image source: Jimenez et al., 2024)\nSWE-bench (Jimenez et al., 2024) 从 12 个流行的 Python 开源项目中收集了 2294 个真实的开发任务，这些任务直接来源于 GitHub 的 Issues 和 Pull Requests。\n为了保证实验的可重复性和环境独立性，SWE-Bench 为每个任务构建了隔离的 Docker 环境，避免因 Python 版本或依赖库不一致导致运行失败。同时，这种设计也迫使模型学会针对不同环境生成兼容的补丁代码。\n验证机制上，SWE-Bench 巧妙地利用了项目自带的单元测试来自动化评估 LLM 的补丁是否正确。它包含两类测试：\nFail-to-Pass (F2P)：原先失败的测试，合入正确的 PR 后应当通过，用于确认 LLM 是否修复了目标问题。 Pass-to-Pass (P2P)：原先能通过的测试，在合入 PR 后也必须继续通过，用于保证 LLM 没有破坏已有功能或引入新 bug。 这种“真实任务 + 隔离环境 + 自动化测试”的组合，使 SWE-bench 成为一个高可信度、可扩展的基准，大幅降低了验证编程任务正确性的成本。但原始 SWE-bench 存在测试用例不公平、问题描述模糊和环境复杂等缺陷，导致模型能力被低估，因此 OpenAI 构建了经人工筛选的高质量子集 SWE-bench Verified(OpenAI, 2024)，用以更准确评估模型水平。\nBrowseComp 与软件工程任务不同，网页浏览任务的目标是在浩瀚的互联网中找到特定信息。BrowseComp (Wei et al., 2025) 是一个专为此类任务设计的简单而具挑战性的基准。\n设计理念：BrowseComp 遵循难于解决，易于验证的原则。问题被设计得需要持久、创造性地浏览大量网页才能找到答案，但答案本身通常是一个简短的、无可争议的字符串，可以轻松与参考答案进行比对。\n数据构建：出题者采用逆向提问的方式。他们先找到一个冷门的事实（如一篇特定会议论文），然后围绕这个事实构建一个包含多个复杂约束条件的查询。例如：“请找出 EMNLP 2018-2023 年间发表的一篇论文，其第一作者本科毕业于达特茅斯学院，第四作者本科毕业于宾夕法尼亚大学。” 验证这个答案很简单，但要从数千篇论文中找到它则极为困难。\nFig. 4. BrowseComp performance of an early version of OpenAI Deep Research scales smoothly with test-time compute. (Image source: Wei et al., 2025)\nBrowseComp 衡量的是 Agent 的核心浏览能力：事实推理、持久导航和创造性搜索。如图所示，强大的浏览 Agent (如 OpenAI Deep Research) 在该基准上的性能会随着测试时计算量（即浏览努力程度）的增加而平滑提升，这表明该评测集能有效衡量 Agent 的深度搜索和信息整合能力。\n数据 高质量的数据是训练强大智能体的基石。然而，人工标注 agent 在复杂任务中的完整决策轨迹成本极高且难以规模化。因此，合成数据成为了该领域的主流解决方案。各种创新的数据生成管线被提出，旨在构建一个能持续产生高质量训练数据的“数据飞轮”。\nAgentFounder Fig. 5. The Agentic Training Pipeline proposed by AgentFounder, incorporating an Agentic CPT stage. (Image source: Su et al., 2025)\nAgentFounder (Su et al., 2025) 在传统的预训练与后训练之间，提出了一个新的流程智能体持续预训练 (Agentic Continual Pre-training, Agentic CPT)，整个训练流程包含三个阶段：\nGeneral Pre-training：与标准流程一致，先训练一个具备通用知识的基础模型。 Agentic CPT：在通用基础模型之上，利用大规模、多样化的合成智能体行为数据，继续进行“下一个词预测”式训练。其目标并非解决具体任务，而是让模型内化通用的 Agent 行为模式，形成 agentic 能力。 Post-training：在已经具备基础智能体能力的模型上，再进行 SFT 或 RL，使其对齐到具体任务，从而避免后训练阶段“能力习得”和“任务对齐”同时进行所带来的优化冲突。 Agentic CPT 的关键是如何以低成本合成大规模类智能体数据。为此，AgentFounder 提出了两种无需外部工具调用的高效数据生成方法：一阶动作合成 (First-order Action Synthesis, FAS) 和 高阶动作合成 (Higher-order Action Synthesis, HAS)。\nFAS 的核心思想是通过推演的方式，低成本地生成关于如何思考和如何规划第一步的数据。 Fig. 6. Illustration of First-order Action Synthesis (FAS). (Image source: Su et al., 2025)\n规划动作：给定一个问题，让 LLM 生成多种可能的分析与第一个工具调用计划，帮助模型学习任务分解与初步规划。 推理动作：在提供问题和相关知识片段的情况下，让 LLM 生成完整的逻辑推理链，并得出最终答案，锻炼其逻辑演绎与信息综合能力。 FAS 仅生成思考过程与计划动作，不涉及真实工具调用或环境交互，因而生成成本极低，非常适合进行大规模数据合成（可达亿级）。\nHAS 目标是将已有的（即使是次优的）agent 轨迹转化为高价值的决策学习数据。 Fig. 7. Illustration of Higher-order Action Synthesis (HAS). (Image source: Su et al., 2025)\n步骤级扩展：针对轨迹中的任意一步，利用 LLM 生成多个替代动作，构建一个局部决策空间。 对比学习：将原始选择与扩展出的候选动作，重构为一个带反馈的多选题，要求模型识别更优决策。 因果监督：在轨迹末尾附上最终结果（成功或失败），帮助模型学习决策与结果之间的因果联系。 这种方法将传统的轨迹模仿，升级为步骤级决策学习。模型不仅会“走过一条成功路径”，更能理解在每个关键节点如何选择，提升了数据的信噪比与利用效率。\nAgentic CPT 是否真的缓解了“优化冲突”？论文通过训练损失曲线与性能对比实验出了结论。\nFig. 8. Training loss evolution showing superior convergence of AgentFounder models compared to baseline. (Image source: Su et al., 2025)\n实验结果表明经过 Agentic CPT 的模型在 SFT 阶段收敛速度更快，训练损失显著低于未经过该阶段的基线模型。这说明模型在进入后训练阶段之前，已经形成了一定 agentic 能力，从而在后续学习特定任务时更加高效。\nWebShaper 传统的数据合成方法通常是信息驱动的：先从网络爬取信息，再根据信息生成问题。WebShaper (Tao et al., 2025) 指出，这种方式可能导致生成的问题中的推理结构与原始信息结构不一致，模型可能通过“逻辑捷径”而非真正的多步推理来找到答案。为此，它提出了一种形式化驱动 范式, 这里主要用了知识图谱和集合论来形式化问题。\nFig. 9. The formalism-driven data synthesis pipeline of WebShaper. (Image source: Tao et al., 2025)\n知识投影(Knowledge Projections, KP)：WebShaper 首先基于集合论将信息寻求任务进行形式化。一个 知识投影 \\(R(V)\\) 被定义为与实体集合 \\(V\\) 存在关系 \\(R\\) 的所有实体的集合。例如，bornIn({1990s}) 表示所有在 1990 年代出生的实体集合。 任务形式化：复杂的查询可以被严谨地表示为多个 KP 的交集 和 并集 运算。例如，“查找在 2004-05 赛季效力于一支成立于 1966 年的东德球队，并且出生于 90 年代的球员”可以被形式化为多个 KP 的交集。 Expander 智能体：WebShaper 使用一个名为 Expander 的智能体，它首先生成一个形式化的查询结构（如三个 KP 的交集），然后通过调用工具（搜索、摘要）来逐步填充这个结构中的具体内容，并以“层级扩展”策略逐步增加问题的复杂度，从而有效避免了逻辑捷径和信息冗余。 奖励设计 奖励函数是 RL 的灵魂，它定义了智能体的优化目标。\n可验证奖励：对于数学、代码等有明确答案的任务，这是最可靠和可扩展的奖励来源。奖励信号可以直接来自单元测试通过率、代码编译器反馈或最终答案的正确性。这种基于规则的奖励有效避免了奖励模型可能带来的 Reward hacking 问题。\n生成式奖励：对于开放式、没有唯一答案的任务（如生成研究报告），采用 LLM-as-a-Judge(Zheng et al., 2023) 的方法使用一个强大的 LLM 作为裁判来评估生成结果的质量，并给出分数或自然语言反馈作为奖励信号。\n稠密奖励：与只在任务结束时提供一次性奖励的结果奖励模型 不同，过程奖励模型 为智能体在任务过程中的每一步或每一个中间环节提供反馈。这有助于解决长时程任务中的信用分配难题，但同时也增加了标注成本和被模型利用的风险。\n无监督奖励：为了摆脱对外部标注的依赖，研究者探索了从模型自身行为中提取奖励信号的方法，例如基于输出一致性（多次生成结果是否一致）或内部置信度（如生成概率的熵）来构建奖励。\n优化算法 近年来，围绕 PPO、DPO、GRPO 等方法，衍生出了大量改进算法，下面简单介绍三个比较有代表性的算法。\nPPO 近端策略优化 (Proximal Policy Optimization, PPO) (Schulman et al., 2017) 是一种经典的 Actor-Critic 算法，因其在 InstructGPT (Ouyang et al., 2022) 中的成功应用而成为 LLM 强化学习微调的主流方法。PPO 的核心思想是在更新策略时，限制新旧策略之间的变化幅度，从而保证训练的稳定性。用 token 级重要性比率 和 clip 裁剪 限制新旧策略偏移，并借助一个 Critic 模型估计优势（将 sequence-level 奖励分解到 token-level），从而提升稳定性，但也带来额外的模型与计算开销。\n$$ \\mathcal{J}_{\\mathrm{PPO}}(\\theta)=\\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta_{\\text {old }}}(\\cdot \\mid x)}\\left[\\frac{1}{|y|} \\sum_{t=1}^{|y|} \\min \\left(w_t(\\theta) \\widehat{A}_t, \\operatorname{clip}\\left(w_t(\\theta), 1-\\varepsilon, 1+\\varepsilon\\right) \\widehat{A}_t\\right)\\right] $$其中，token $y_{t}$ 的重要性比率定义为 $w_t(\\theta)=\\frac{\\pi_\\theta\\left(y_t \\mid x, y_{\u0026lt;t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(y_t \\mid x, y_{\u0026lt;t}\\right)}$，$y_{t}$ 的优势 $\\widehat{A}_{t}$ 由另一个 Critic 模型估计得到，$\\varepsilon$ 是重要性比率的裁剪范围。\nGRPO 组相对策略优化 (Group Relative Policy Optimization, GRPO) (Shao, et al. 2024) 巧妙地移除了 Critic 模型。对于每个问题，它会采样一组 $G$ 输出来，通过计算每个输出在该组内的相对优势（即奖励值减去组内均值再除以标准差）来作为优势函数（优势是 sequence-level 的，但仍然在 token-level 上更新），降低了计算成本并提升了训练稳定性，下面公式省略了 KL 散度惩罚项，完整的可以参考博主之前写的 GRPO。\n$$ \\mathcal{J}_{\\mathrm{GRPO}}(\\theta)=\\mathbb{E}_{x \\sim \\mathcal{D},\\left\\{y_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text {old }}}(\\cdot \\mid x)}\\left[\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|y_i\\right|} \\sum_{t=1}^{\\left|y_i\\right|} \\min \\left(w_{i, t}(\\theta) \\widehat{A}_{i, t}, \\operatorname{clip}\\left(w_{i, t}(\\theta), 1-\\varepsilon, 1+\\varepsilon\\right) \\widehat{A}_{i, t}\\right)\\right] $$token $y_{i, t}$ 的重要性比率和优势分别为：\n$$ w_{i, t}(\\theta)=\\frac{\\pi_{\\theta}\\left(y_{i, t} \\mid x, y_{i,\u0026lt;t}\\right)}{\\pi_{\\theta_{\\text{old}}}\\left(y_{i, t} \\mid x, y_{i,\u0026lt;t}\\right)}, \\quad \\widehat{A}_{i, t}=\\widehat{A}_{i}=\\frac{r\\left(x, y_{i}\\right)-\\operatorname{mean}\\left(\\left\\{r\\left(x, y_{i}\\right)\\right\\}_{i=1}^{G}\\right)}{\\operatorname{std}\\left(\\left\\{r\\left(x, y_{i}\\right)\\right\\}_{i=1}^{G}\\right)}, $$$y_{i}$ 内的所有 token 共享相同的优势 $\\widehat{A}_{i}$, $G$ 表示每个查询 $x$ 生成的输出数量（即组大小）。\nGSPO 组序列策略优化 (Group Sequence Policy Optimization, GSPO) (Zheng et al., 2025) 将优化的基本单位从 token level 提升到 sequence-level。与 GRPO 使用 token level 重要性比率不同，GSPO 引入 sequence-level 重要性比率来对齐序列级奖励，从而避免长序列中 token 比率累积带来的噪声，降低方差并提升稳定性。Qwen 团队指出这样不仅能够在 MoE 中缓解由局部路由抖动导致的概率大幅波动，还能与 Agent 任务中普遍存在序列级奖励自然对齐，适用于长序列建模和对路由敏感的场景。\nGSPO 的目标函数为：\n$$ \\mathcal{J}_{\\mathrm{GSPO}}(\\theta)=\\mathbb{E}_{x \\sim \\mathcal{D},\\left\\{y_{i}\\right\\}_{i=1}^{G} \\sim \\pi_{\\theta_{\\text {old }}}(\\cdot \\mid x)}\\left[\\frac{1}{G} \\sum_{i=1}^{G} \\min \\left(s_{i}(\\theta) \\widehat{A}_{i}, \\operatorname{clip}\\left(s_{i}(\\theta), 1-\\varepsilon, 1+\\varepsilon\\right) \\widehat{A}_{i}\\right)\\right], $$其中，组内的优势函数定义为：\n$$ \\widehat{A}_{i}=\\frac{r\\left(x, y_{i}\\right)-\\operatorname{mean}\\left(\\left\\{r\\left(x, y_{i}\\right)\\right\\}_{i=1}^{G}\\right)}{\\operatorname{std}\\left(\\left\\{r\\left(x, y_{i}\\right)\\right\\}_{i=1}^{G}\\right)} $$而序列级的重要性比率 $s_i(\\theta)$ 定义为：\n$$ s_i(\\theta)=\\left(\\frac{\\pi_\\theta\\left(y_i \\mid x\\right)}{\\pi_{\\theta_{\\text{old}}}\\left(y_i \\mid x\\right)}\\right)^{\\frac{1}{\\left|y_i\\right|}}=\\exp \\left(\\frac{1}{\\left|y_i\\right|} \\sum_{t=1}^{\\left|y_i\\right|} \\log \\frac{\\pi_\\theta\\left(y_{i, t} \\mid x, y_{i,\u0026lt;t}\\right)}{\\pi_{\\theta_{\\text{old}}}\\left(y_{i, t} \\mid x, y_{i,\u0026lt;t}\\right)}\\right) $$它对 整个序列 应用裁剪，而不是对单个 token 裁剪，使优化与序列级奖励保持一致。这里采用了 长度归一化 来降低方差并控制 $s_i(\\theta)$ 的数值范围，否则少量 token 的概率变化就可能导致比率剧烈波动，同时不同长度的响应也会导致不一致的裁剪范围。需要注意的是，由于重要性比率定义不同，GSPO 与 GRPO 的裁剪范围数量级通常并不相同。\n框架 Agentic RL 的训练流程复杂，涉及推理、训练、奖励计算等多个环节，通常需要借助分布式框架来高效协调。\nFig. 10. Dataflow graph of 3 RLHF algorithms including PPO, Safe-RLHF and ReMax. (Image source: Sheng et al., 2024)\n上图显示了不同算法训练过程中的复杂数据流。它不仅包含 Actor、Critic、Reference 和 Reward 等多种模型，还交织着数据生成、推理和训练等不同类型的计算负载。以基础的 PPO 算法为例，系统中涉及 4 个核心模型：Actor 负责根据输入 prompt 生成 response；Critic 用于评估结果；Reference 作为生成质量的基准；Reward 则提供奖励信号。从计算角度看，整个流程可分为 3 个阶段：\nGeneration：Actor 逐 token 生成 response，此过程受文本长度和生成方式影响，是推理资源和时间的主要消耗点。 Forward (Rollout)：生成结果与 query 一起输入 4 个模型进行一次前向计算，数据存入 Replay Buffer。 Training：从 Buffer 中采样数据，用于更新 Actor 与 Critic。 Fig. 11. Two representative RL framework architectures. (Image source: Zhong et al., 2025)\n如图所示，常见的分布式调度策略分为两类：\n时分共用 (Colocate)：将 Rollout 和 Training 部署在同一组 GPU 上，通过时间片轮流执行。这种方式实现简单，通信开销小，但稳定性差，且无法利用异构硬件。 训推分离 (Disaggregated)：将 Rollout 和 Training 分别部署在独立的 GPU 集群上。这种架构更灵活，稳定性更高，允许异构硬件混合部署，但可能引入流水线气泡，影响吞吐率。 verl verl (Volcano Engine Reinforcement Learning) (Sheng et al., 2024) 是字节跳动开源的一个为 LLM 设计的高效、通用的强化学习框架。\nFig. 12. The asynchronous system architecture of verl. (Image source: ByteDance Seed, 2025)\nverl 的核心是异步架构，它将 Rollout、奖励计算和模型优化等阶段解耦，通过流水线并行处理，最大化硬件利用率。其流程如下：\nPPOTrainer 发起一次 PPO 迭代，先进行 rollout，再进行 train。 AgentLoopManager 唤醒/同步推理与训练引擎权重（vLLM/SGLang ⇄ FSDP/Megatron-LM），将 batch 切成 chunk 分派给多个 AgentLoopWorker 并并发执行。 每个 AgentLoopWorker 为一个样本启动一个协程，在需要生成时，通过 AsyncLLMServerManager 将请求路由到负载最小的推理实例，天然支持多轮对话和多工具调用。 verl 原生支持对工具输出等外部信息进行损失掩码 (Loss Masking)，即在计算损失时忽略这些 token，只让模型为自己生成的内容负责。这是保证 Tool RL 训练稳定性的关键特性。 Fig. 13. The agent loop worker of verl based on React. (Image source: ByteDance Seed, 2025)\nrollout 结束后统一回收/休眠实例以释放显存；通过可插拔接口自定义奖励函数、集成新工具或替换RL 算法（如在 ReactAgentLoop 上派生自定义 Agent）。这里默认使用 LangGraph 框架实现 ReAct Agent。 案例研究 Search-R1 Search-R1 (Jin et al., 2025) 训练 LLM 在逐步推理的过程中，自主地与搜索引擎进行多轮交互，从而学会何时以及如何利用外部知识。\nFig. 14. Template for SEARCH-R1. question will be replaced with the specific question during training and inference. (Image source: Jin et al., 2025)\n在 RL 训练的轨迹中，模型与环境的交互遵循以下步骤：\n在 \u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt; 标签内进行自主推理。 当意识到知识不足时，生成 \u0026lt;search\u0026gt;query\u0026lt;/search\u0026gt; 标签来调用搜索引擎。 环境执行搜索后，将检索到的信息包裹在 \u0026lt;information\u0026gt;...\u0026lt;/information\u0026gt; 标签内反馈给模型。 模型根据新的信息继续推理，并可以进行多轮搜索，直到最终在 \u0026lt;answer\u0026gt;...\u0026lt;/answer\u0026gt; 标签内给出答案。 Fig. 15. LLM Response Rollout with Multi-Turn Search Engine Calls. (Image source: Jin et al., 2025)\n这个过程通过一个循环算法实现：模型生成文本直到遇到 \u0026lt;search\u0026gt; 或 \u0026lt;answer\u0026gt; 标签。如果检测到搜索请求，系统会暂停生成，执行搜索，并将结果用 \u0026lt;information\u0026gt; 标签包裹后注入到上下文中，供模型在下一步继续推理。这个循环会持续进行，直到模型生成最终答案或达到最大交互次数。\n论文采用 PPO 与 GRPO 算法进行训练。为保证训练稳定性，Search-R1 引入了检索 token 掩码机制：在计算 RL 损失（策略梯度与 KL 散度）时，所有由搜索引擎返回并包裹在 \u0026lt;information\u0026gt; 标签内的 retrieved token 均被masking，其损失不参与梯度更新。这一设计使模型专注于学习何时及如何进行推理和搜索，而非机械模仿外部检索内容，从而有效避免训练不稳定。下图结果显示使用该策略训练后的模型（Qwen2.5-7b-base，经 PPO 训练）在性能上始终优于未使用掩码的版本。\nFig. 16. The performance of SEARCH-R1 with and without retrieved token loss masking. (Image source: Jin et al., 2025)\nSearch-R1 采用了一个简洁的基于结果的奖励函数，仅根据最终答案的正确性进行评分，使用精确匹配作为评判标准。\n$$ R(y, \\hat{y}_{\\text{gold}}) = \\text{EM}(y_{\\text{answer}}, \\hat{y}_{\\text{gold}}) = \\begin{cases} 1, \u0026 \\text{if } y_{\\text{answer}} = \\hat{y}_{\\text{gold}} \\\\ 0, \u0026 \\text{otherwise} \\end{cases} $$这种简单的奖励设计被证明是有效的，足以引导模型学习出复杂的搜索和推理行为。\nFig. 17. The main results comparing SEARCH-R1 with baseline methods across the seven datasets. (Image source: Jin et al., 2025)\n实验表明，在 Qwen2.5-7B 和 Qwen2.5-3B 模型上，相较于传统的 RAG 基线，Search-R1 分别取得了 24% 和 20% 的平均相对性能提升。\nReTool ReTool (Luo et al., 2025) 是一个基于 verl 框架训练模型何时以及如何调用代码解释器 (Code Interpreter, CI) 来解决数学问题案例。\nFig. 18. The architecture of ReTool. (Image source: Luo et al., 2025)\nReTool 采用“冷启动 SFT → 工具增强 RL”两阶段流程：\n冷启动 SFT：首先构建包含代码增强推理轨迹的数据集，通过 SFT 使模型掌握基础的工具调用能力。 工具增强 RL：在 RL 阶段，模型在解决问题的过程中可以生成 \u0026lt;code\u0026gt;...\u0026lt;/code\u0026gt; 代码片段，这些代码会在沙盒环境（如 SandboxFusion）中执行，执行结果（包括输出或错误栈）会被包裹在 \u0026lt;interpreter\u0026gt;...\u0026lt;/interpreter\u0026gt; 标签中反馈给模型，模型可以根据反馈继续推理或进行自我纠错。 Fig. 19. The case of “aha moment” about code self-correction. (Image source: Luo et al., 2025)\n其奖励函数仅依据最终答案的正确性打分，鼓励模型自主探索稳健的推理-执行策略。\n$$ R(a,\\hat a)= \\begin{cases} 1, \u0026 \\text{is_equivalent}(a,\\hat a) \\\\ -1, \u0026 \\text{otherwise} \\end{cases} $$基于 PPO 算法进行训练，类似于 Search-R1 对解释器反馈 \u0026lt;interpreter\u0026gt;...\u0026lt;/interpreter\u0026gt; 进行全量 loss mask，只更新模型思考与代码，避免梯度被外部环境噪声污染。下图结果显示在 AIME 2024/2025 评测中，ReTool 仅用 400 步 RL 就使 Qwen2.5-32B-Instruct 的准确率达到 67.0% / 49.3%，超过纯文本 RL 基线，同时平均响应长度缩短了约 40%。\nFig. 20. AIME 2024 \u0026amp; 2025 scores of ReTool and text-based RL baseline on the Qwen2.5-32B-Instruct model. (Image source: Luo et al., 2025)\n参考文献 [1] Zhang, Guibin, et al. \u0026ldquo;The landscape of agentic reinforcement learning for llms: A survey.\u0026rdquo; arXiv preprint arXiv:2509.02547 (2025).\n[2] Wei, Jason. \u0026ldquo;Successful language model evals.\u0026rdquo; Blog post, 2024.\n[3] Liang, Percy, et al. \u0026ldquo;Holistic evaluation of language models.\u0026rdquo; arXiv preprint arXiv:2211.09110 (2022).\n[4] Srivastava, Aarohi, et al. \u0026ldquo;Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\u0026rdquo; Transactions on machine learning research (2023).\n[5] Wang, Alex, et al. \u0026ldquo;GLUE: A multi-task benchmark and analysis platform for natural language understanding.\u0026rdquo; arXiv preprint arXiv:1804.07461 (2018).\n[6] Wang, Alex, et al. \u0026ldquo;SuperGLUE: A stickier benchmark for general-purpose language understanding systems.\u0026rdquo; Advances in neural information processing systems 32 (2019).\n[7] Wei, Jason. \u0026ldquo;Asymmetry of verification and verifier’s rule.\u0026rdquo; Blog post, 2025.\n[8] Jimenez, Carlos E., et al. \u0026ldquo;SWE-bench: Can language models resolve real-world github issues?.\u0026rdquo; arXiv preprint arXiv:2310.06770 (2023).\n[9] OpenAI. \u0026ldquo;Introducing SWE-bench Verified.\u0026rdquo; OpenAI, 2024 (updated 2025).\n[10] Wei, Jason, et al. \u0026ldquo;Browsecomp: A simple yet challenging benchmark for browsing agents.\u0026rdquo; arXiv preprint arXiv:2504.12516 (2025).\n[11] Su, Liangcai, et al. \u0026ldquo;Scaling Agents via Continual Pre-training.\u0026rdquo; arXiv preprint arXiv:2509.13310 (2025).\n[12] Tao, Zhengwei, et al. \u0026ldquo;Webshaper: Agentically data synthesizing via information-seeking formalization.\u0026rdquo; arXiv preprint arXiv:2507.15061 (2025).\n[13] Zheng, Lianmin, et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.\u0026quot; Advances in neural information processing systems 36 (2023): 46595-46623.\n[14] Schulman, John, et al. \u0026ldquo;Proximal policy optimization algorithms.\u0026rdquo; arXiv preprint arXiv:1707.06347 (2017).\n[15] Shao, Zhihong, et al. \u0026ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\u0026rdquo; arXiv preprint arXiv:2402.03300 (2024).\n[16] Zheng, Chujie, et al. \u0026ldquo;Group sequence policy optimization.\u0026rdquo; arXiv preprint arXiv:2507.18071 (2025).\n[17] Sheng, Guangming, et al. \u0026ldquo;Hybridflow: A flexible and efficient rlhf framework.\u0026rdquo; Proceedings of the Twentieth European Conference on Computer Systems. 2025.\n[18] Zhong, Yinmin, et al. \u0026ldquo;StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation.\u0026rdquo; arXiv preprint arXiv:2504.15930 (2025).\n[19] Jin, Bowen, et al. \u0026ldquo;Search-r1: Training llms to reason and leverage search engines with reinforcement learning.\u0026rdquo; arXiv preprint arXiv:2503.09516 (2025).\n[20] Feng, Jiazhan, et al. \u0026ldquo;Retool: Reinforcement learning for strategic tool use in llms.\u0026rdquo; arXiv preprint arXiv:2504.11536 (2025).\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (September 2025). Agentic RL https://syhya.github.io/zh/posts/2025-09-30-agentic-rl/\nOr\n@article{yue_shui_agentic_rl_2025, title = \u0026#34;Agentic RL\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;September\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-09-30-agentic-rl/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-09-30-agentic-rl/","summary":"\u003cp\u003e大语言模型（LLMs）目前应用场景不断扩展，但也暴露出知识截止、幻觉以及复杂计算与逻辑推理不足等局限。为应对这些挑战，将智能体（Agent）与强化学习（Reinforcement Learning, RL）相结合的 \u003cstrong\u003eAgentic RL\u003c/strong\u003e 正逐渐成为关键研究方向。\u003c/p\u003e","title":"Agentic RL"},{"content":"2025 年 8 月，AI 领域迎来了 OpenAI 的密集发布期。继 2019 年 GPT-2 (OpenAI, 2019) 之后，OpenAI 再次向开源社区贡献了其首个开放权重的大型语言模型系列 gpt-oss (OpenAI, 2025)，包含 120B 和 20B 两种规模。紧随其后，备受瞩目的下一代旗舰模型 GPT-5 (OpenAI, 2025) 也正式发布。这一系列发布不仅标志着开源模型在推理和智能体能力上达到了新的高度，也揭示了 OpenAI 在模型架构、训练方法论以及安全对齐方面的最新进展。\ngpt-oss gpt-oss (OpenAI, 2025) 是 OpenAI 自 GPT-2 以来首次发布的开放权重语言模型，旨在为开源社区提供强大的推理和工具使用能力。该系列包含 gpt-oss-120b 和 gpt-oss-20b 两个版本，均在 Apache 2.0 许可下发布。\n架构概览 Fig. 1. A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B. (Image source: Raschka, 2025)\ngpt-oss 建立在 GPT 系列架构之上，并融合了近年来多项主流技术包括 RMSNorm、SwiGLU、GQA、RoPE、YaRN 和 MoE 等。\n下表直观对比了 GPT-OSS 20B vs GPT-2 XL 1.5B 模型差异。\n特性 GPT-OSS 20B (2025) GPT-2 XL 1.5B (2019) 发布时间 2025年 2019年 模型大小 20B 参数 1.5B 参数 活跃参数 3.5B (每次推理) 1.5B (全部激活) 词汇表大小 200k tokens 50k tokens 嵌入维度 2,880 1,600 Transformer层数 24层 48层 注意力头数 64个 25个 支持上下文长度 131k tokens 1,024 tokens 位置编码 RoPE (旋转位置编码) 绝对位置嵌入 注意力机制 Grouped Query Attention Multi-head Attention 前馈网络 SwiGLU激活 + MoE GELU激活 MoE架构 32个专家，4个激活 无 归一化方法 RMSNorm (2处) LayerNorm (2处) Dropout 无 有 滑动窗口注意力 每隔一层使用(窗口128 tokens) 无 训练特点 包含监督微调+强化学习 仅预训练 量化支持 MXFP4 (可在单GPU运行) 无特殊量化 许可证 Apache 2.0 MIT 高效注意力机制 为了在支持 128k 长上下文的同时保持高效率，gpt-oss 采用了多种先进的注意力机制。\n分组查询注意力 (Grouped-Query Attention, GQA): gpt-oss 中有 64 个查询头和 8 个键值头，意味着每 8 个查询头共享一对 K/V，这显著减少了 KV 缓存的大小和内存带宽需求，从而在几乎不损失模型性能的情况下，大幅提升了推理吞吐量。\n滑动窗口注意力 (Sliding Window Attention): 为了进一步降低计算复杂度，gpt-oss 借鉴了 Longformer(Jiang et al., 2020) 和 Mistral(Jiang et al., 2023)的思想，采用了交替的注意力模式。其 Transformer 层在全注意力 (Dense Attention)和局部带状稀疏注意力 (Locally Banded Sparse Attention)之间交替。后者即滑动窗口注意力，它将每个 Token 的注意力范围限制在一个固定大小的局部窗口内。\nFig. 2. Comparison between regular attention (left) and sliding window attention (right). (Image source: Jiang et al., 2023)\n在 gpt-oss 中，这个窗口大小被设置为 128 个 Token。这意味着，在一个局部注意力层中，一个 Token 只能关注其前面 128 个 Token，而不是整个上下文。这种设计使得注意力的计算复杂度从 \\( O(L^2) \\) 降低到 \\( O(L \\cdot W) \\)，其中 \\( L \\) 是序列长度，\\( W \\) 是窗口大小。通过与全注意力层交替，模型既能高效处理局部信息，又能通过全注意力层整合全局信息。\n注意力池: 模型引入了注意力池（Attention Sinks） (Xiao et al., 2023)，通过学习一个附加到注意力分数上的偏置 \\( \\mathbf{s}_h \\)，使得初始 Token 能够被持续关注，这有助于在长上下文场景下稳定注意力分布，防止信息丢失。 \\[ \\text{Attention}(Q, K, V)_h = \\text{softmax}\\left(\\frac{Q_h K_h^T}{\\sqrt{d_k}} + \\mathbf{s}_h\\right)V_h \\] Fig. 3. Illustration of StreamingLLM vs existing methods. (Image source: Xiao et al., 2023)\n上图比较了 StreamingLLM 与三种常见长文本处理方法在性能和效率上的差异。假设语言模型在预训练时只见过长度为 $L$ 的文本，而推理时需要预测第 $T$ 个 token（其中 $T \\gg L$）：\n密集注意力（Dense Attention）：保留所有历史 token 的键值（KV）并计算全量注意力，时间复杂度为 $O(T^2)$，缓存规模持续增长。当输入长度超过预训练长度 $L$ 时，性能明显下降。 窗口注意力（Window Attention）：只缓存最近 $L$ 个 token 的 KV，推理效率高，但一旦早期 token 的信息被替换，性能会急剧下降。 滑动窗口重计算（Sliding Window with Re-computation）：在每次生成新 token 时，从最近 $L$ 个 token 重建 KV 状态。尽管在长文本上性能较好，但因重计算涉及二次注意力，时间复杂度高达 $O(TL^2)$，推理速度很慢。 这种方法在计算时结合了注意力池和最近的 token，不仅保持了推理效率，还能在长文本场景下维持稳定的注意力分布和较低的困惑度。\nMXFP4 量化 Fig. 4. Faster MXFP4 Backpropagation via Stochastic Rounding and Hadamard Transform. (Image source: Tseng et al., 2025)\n为了让大模型能在消费级硬件上运行，gpt-oss 采用了 MXFP4 (Tseng et al., 2025) 格式对 MoE 权重进行量化。MXFP4 是一种微缩放浮点格式，可以将权重有效量化到约 4.25 bit。由于 MoE 权重占模型总参数的 90% 以上，此方法极大地压缩了模型大小，使得 120B 模型能装入单个 80GB GPU，20B 模型能在 16GB 显存的设备上运行。\n训练 预训练: 模型在数万亿 Token 的文本数据集上进行训练，数据侧重于 STEM、编码和通用知识。为了提升安全性，预训练数据复用了 GPT-4o 的 CBRN 内容过滤器。\n后训练 (推理与工具使用): 预训练后，模型采用与 OpenAI o3 类似的 CoT RL 技术进行后训练。这一阶段的目标是教会模型：\n推理: 生成详细的思维链（Chain-of-Thought, CoT）来解决复杂问题。 工具使用: 学会调用外部工具（如网页搜索、代码执行）来增强自身能力。 为了实现这些高级智能体功能，OpenAI 设计了Harmony Chat Format。该格式引入了“通道（channels）”概念（如 analysis 用于 CoT，commentary 用于工具调用，final 用于最终答案），并建立了严格的指令层级（System \u0026gt; Developer \u0026gt; User \u0026gt; Assistant \u0026gt; Tool），确保模型行为的可控性。\n此外，模型还支持可变努力推理（Variable Effort Reasoning）。用户可以在系统提示中设置 Reasoning: low/medium/high，从而在延迟和性能之间进行权衡。高努力度会生成更长的 CoT，通常带来更高的准确率，但延迟也相应增加。\nFig. 5. Accuracy vs. average CoT length for different reasoning levels on AIME and GPQA benchmarks. (Image source: OpenAI, 2025)\n评估 Fig. 6. Main capabilities evaluations for gpt-oss series. (Image source: OpenAI, 2025)\n评测基准结果显示 gpt-oss-120b 的准确率已超过 OpenAI 的 o3-mini，并接近 o4-mini；而规模仅为其六分之一的 gpt-oss-20b 也展现出一定竞争力。\nGPT-5 GPT-5 (OpenAI, 2025) 并非单个模型，而是一个统一的智能系统。它并非一个单一的庞大模型，而是一个由多个专业模型和智能路由机制协同工作的复杂系统，旨在平衡性能、速度与成本。\n系统架构 Fig. 7. GPT-5 Unified System Architecture. (Image source: Latent Space, 2025)\nGPT-5 系统由以三个核心部分组成：\ngpt-5-main: 作为系统的默认模型，它快速、高效，负责处理绝大多数用户查询。可视为 GPT-4o 的继任者。 gpt-5-thinking: 用于处理更复杂、需要深度思考的问题。当用户明确要求（如“think hard about this”）或系统判断任务需要时，该模型会被激活。可视为 OpenAI o3 的继任者。 实时路由器 (Real-time Router): 这是一个持续训练的决策模型，它能根据多种信号快速判断应将用户请求分配给哪个模型处理。其决策依据包括： 对话类型 (Conversation Type): 是闲聊、问答还是任务导向型对话。 复杂性 (Complexity): 问题的难度和所需的推理深度。 工具需求 (Tool Needs): 是否需要调用网页搜索、代码解释器等外部工具。 用户意图 (Explicit Intent): 用户可以通过明确的指令（如“think hard about this”）来引导路由器选择深度推理模型。 该路由器通过持续学习真实用户信号（如用户切换模型的行为、响应的偏好率、答案的实测正确率）来不断优化其决策能力。\n安全补全 传统的安全训练范式是硬性拒绝 (Hard Refusals)，即模型根据用户意图的二元分类（安全或不安全）来决定是完全回答还是直接拒绝。这种方法在处理意图明确的恶意提示时有效，但在面对意图模糊或涉及双重用途 (Dual-use)（如生物、网络安全）的领域时，显得非常脆弱，常常导致过度拒绝。\n安全补全 (Safe Completions)(Baker et al., 2025)不再对用户意图进行二元分类，而是致力于在遵守安全策略的前提下，最大化模型的帮助性。\n对于明确有害的请求：模型依然会拒绝。 对于双重用途的请求（如生物、化学、网络安全领域）：模型会提供安全的、高层次的、不包含可直接执行细节的回答，而不是完全拒绝。 对于模糊意图的请求：模型会尝试以安全的方式完成任务，或提供安全的替代方案。 这种方法显著提升了模型在双重用途领域的安全性和实用性，并减少了不必要的过度拒绝。\nFig. 8. Left: Overall structure of the safe-completion training stack. Right: Details of the safecompletion reward design. (Image source: OpenAI, 2025)\n思维链监控 OpenAI 采用 思维链监控 (CoT Monitoring)(Baker et al., 2025)技术来确保推理模型的可靠性、安全性以及避免出现 reward hacking。与一些试图通过 SFT 来优化 CoT 的做法不同，GPT-5 的 CoT 训练没有施加直接对齐。这使得 CoT 能够更真实地反映模型的“思考”过程，成为一个有效的窗口，用于检测模型的错误行为、欺骗意图或潜在的风险。\nFig. 9. Monitoring Frontier Reasoning Models for Reward Hacking. (Image source: Baker et al., 2025)\n通过 CoT 监控，OpenAI 发现 gpt-5-thinking 的欺骗行为发生率从 o3 的 4.8% 降低到了 2.1%。这项技术对于理解和缓解高级 AI 系统的风险至关重要。\n评估 GPT-5 在多个基准测试中表现出色，尤其在推理、编码和多模态能力上树立了新的标杆。与前代模型相比，它不仅在准确率上实现了飞跃，还在效率上取得了显著进步，通常能用 50-80% 更少的输出来达到甚至超越 o3 的性能。\nFig. 10. GPT-5 performance in SWE-bench Verified Software Engineering. (Image source: OpenAI, 2025)\n🧠 智慧 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano AIME ’25 94.6% 91.1% 85.2% 88.9% 92.7% 46.4% 40.2% - FrontierMath (with python tool only) 26.3% 22.1% 9.6% 15.8% 15.4% - - - GPQA diamond 85.7% 82.3% 71.2% 83.3% 81.4% 66.3% 65.0% 50.3% HLE 24.8% 16.7% 8.7% 20.2% 14.7% 5.4% 3.7% - HMMT 2025 93.3% 87.8% 75.6% 81.7% 85.0% 28.9% 35.0% - 🖼️ 多模态 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano MMMU 84.2% 81.6% 75.6% 82.9% 81.6% 74.8% 72.7% 55.4% MMMU-Pro (avg across standard and vision sets) 78.4% 74.1% 62.6% 76.4% 73.4% 60.3% 58.9% 33.0% CharXiv reasoning (python enabled) 81.1% 75.5% 62.7% 78.6% 72.0% 56.7% 56.8% 40.5% VideoMMMU (max frame 256) 84.6% 82.5% 66.8% 83.3% 79.4% 60.9% 55.1% 30.2% ERQA 65.7% 62.9% 50.1% 64.0% 56.5% 44.3% 42.3% 26.5% 💻 编码 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano SWE-Lancer: IC SWE Diamond Freelance Coding Tasks $112K $75K $49K $86K $66K $34K $31K $9K SWE-bench Verified 74.9% 71.0% 54.7% 69.1% 68.1% 54.6% 23.6% - Aider polyglot (diff) 88.0% 71.6% 48.4% 79.6% 58.2% 52.9% 31.6% 6.2% 📋 指令遵循 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano Scale multichallenge (o3-mini grader) 69.6% 62.3% 54.9% 60.4% 57.5% 46.2% 42.2% 31.1% Internal API instruction following eval (hard) 64.0% 65.8% 56.1% 47.4% 44.7% 49.1% 45.1% 31.6% COLLIE 99.0% 98.5% 96.9% 98.4% 96.1% 65.8% 54.6% 42.5% 🔧 工具调用 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano Tau²-bench airline 62.6% 60.0% 41.0% 64.8% 60.2% 56.0% 51.0% 14.0% Tau²-bench retail 81.1% 78.3% 62.3% 80.2% 70.5% 74.0% 66.0% 21.5% Tau²-bench telecom 96.7% 74.1% 35.5% 58.2% 40.5% 34.0% 44.0% 12.1% 📚 长上下文 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano OpenAI-MRCR: 2 needle 128k 95.2% 84.3% 43.2% 55.0% 56.4% 57.2% 47.2% 36.6% OpenAI-MRCR: 2 needle 256k 86.8% 58.8% 34.9% - - 56.2% 45.5% 22.6% Graphwalks bfs \u0026lt;128k 78.3% 73.4% 64.0% 77.3% 62.3% 61.7% 61.7% 25.0% Graphwalks parents \u0026lt;128k 73.3% 64.3% 43.8% 72.9% 51.1% 58.0% 60.5% 9.4% BrowseComp Long Context 128k 90.0% 89.4% 80.4% 88.3% 80.0% 85.9% 89.0% 89.4% BrowseComp Long Context 256k 88.8% 86.0% 68.4% - - 75.5% 81.6% 19.1% VideoMME (long, with subtitle category) 86.7% 78.5% 65.7% 84.9% 79.5% 78.7% 68.4% 55.2% 🚨 幻觉 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini LongFact-Concepts hallucination rate (no tools) 1.0% 0.7% 1.0% 5.2% 3.0% 0.7% 1.1% LongFact-Objects hallucination rate (no tools) 1.2% 1.3% 2.8% 6.8% 8.9% 1.1% 1.8% FActScore hallucination rate (no tools) 2.8% 3.5% 7.3% 23.5% 38.7% 6.7% 10.9% 这些结果表明，GPT-5 在需要深度推理的复杂任务（如 GPQA、AIME）和需要与外部环境交互的智能体任务（如 SWE-bench、τ²-bench）上取得了较大提升。同时，事实准确性的大幅提升（幻觉率降低近 8 倍）也使其在实际应用中更加可靠。\n参考文献 [1] Raschka, S. (2025). \u0026ldquo;From GPT-2 to gpt-oss: Analyzing the Architectural Advances.\u0026rdquo; Ahead of AI.\n[2] Radford, Alec, et al. \u0026ldquo;Language models are unsupervised multitask learners.\u0026rdquo; OpenAI blog 1.8 (2019): 9.\n[3] OpenAI. (2025). \u0026ldquo;Introducing gpt-oss.\u0026rdquo; OpenAI Blog.\n[4] OpenAI. (2025). \u0026ldquo;Introducing GPT-5.\u0026rdquo; OpenAI Blog.\n[5] OpenAI. (2025). \u0026ldquo;gpt-oss-120b \u0026amp; gpt-oss-20b Model Card.\u0026rdquo;\n[6] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. \u0026ldquo;Longformer: The long-document transformer.\u0026rdquo; arXiv preprint arXiv:2004.05150 (2020).\n[7] Jiang, Dongsheng, et al. \u0026ldquo;Mistral 7B.\u0026rdquo; arXiv preprint arXiv:2310.08825 (2023).\n[8] Xiao, G., et al. (2023). \u0026ldquo;Efficient Streaming Language Models with Attention Sinks.\u0026rdquo; arXiv preprint arXiv:2309.17453.\n[9] Tseng, Albert, Tao Yu, and Youngsuk Park. \u0026ldquo;Training llms with mxfp4.\u0026rdquo; arXiv preprint arXiv:2502.20586 (2025).\n[10] Yuan, Yuan, et al. \u0026ldquo;From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training.\u0026rdquo; arXiv preprint arXiv:2508.09224 (2025).\n[11] B. Baker, J. Huizinga, L. Gao, Z. Dou, M. Y. Guan, A. Madry, W. Zaremba, J. Pachocki, and D. Farhi, \u0026ldquo;Monitoring reasoning models for misbehavior and the risks of promoting obfuscation.\u0026rdquo; arXiv preprint arXiv:2503.11926, 2025. Submitted on 14 March 2025.\n[12] OpenAI. (2025). \u0026ldquo;GPT-5 System Card.\u0026rdquo;\n[13] OpenAI. (2025). \u0026ldquo;Introducing GPT-5 for developers.\u0026rdquo; OpenAI Blog.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Aug 2025). gpt-oss \u0026amp; GPT-5. https://syhya.github.io/zh/posts/2025-08-24-gpt5\nOr\n@article{yue_shui_gpt5_2025 title = \u0026#34;gpt-oss \u0026amp; GPT-5\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Aug\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-08-24-gpt5\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-08-24-gpt5/","summary":"\u003cp\u003e2025 年 8 月，AI 领域迎来了 OpenAI 的密集发布期。继 2019 年 \u003cstrong\u003eGPT-2\u003c/strong\u003e (\u003ca href=\"https://openai.com/index/better-language-models/\"\u003eOpenAI, 2019\u003c/a\u003e) 之后，OpenAI 再次向开源社区贡献了其首个开放权重的大型语言模型系列 \u003cstrong\u003egpt-oss\u003c/strong\u003e (\u003ca href=\"https://openai.com/index/introducing-gpt-oss/\"\u003eOpenAI, 2025\u003c/a\u003e)，包含 120B 和 20B 两种规模。紧随其后，备受瞩目的下一代旗舰模型 \u003cstrong\u003eGPT-5\u003c/strong\u003e (\u003ca href=\"https://openai.com/index/introducing-gpt-5/\"\u003eOpenAI, 2025\u003c/a\u003e) 也正式发布。这一系列发布不仅标志着开源模型在推理和智能体能力上达到了新的高度，也揭示了 OpenAI 在模型架构、训练方法论以及安全对齐方面的最新进展。\u003c/p\u003e","title":"gpt-oss \u0026 GPT-5"},{"content":"近年来，大语言模型 (Large Language Models, LLMs) 在自然语言处理、代码生成乃至多模态交互等领域取得了革命性的突破。然而，这些模型强大的能力背后是巨大的计算和内存开销，尤其是在推理 (Inference) 阶段。高效地部署和运行这些数十亿甚至数万亿参数的模型，已成为将 LLM 技术规模化应用到实际产品中的核心挑战。\nLLM 推理的挑战主要源于两个方面：\n巨大的内存占用：除了模型参数本身，推理过程中还需要存储大量的中间状态，尤其是 KV Cache。例如，对于一个批处理大小为 512、序列长度为 2048 的请求，其 KV 缓存可能高达 3TB，数倍于模型本身的体积。此外，自注意力计算复杂度随序列长度呈二次增长。 低并行性：LLM 的文本生成本质上是一个自回归 (Autoregressive) 过程，即逐个 Token 生成，下一个 Token 的生成依赖于之前所有已生成的 Tokens。这种串行特性使得解码过程难以高效并行。 Token 生成原理 为了更好地理解后续的优化技术，我们首先需要了解大模型是如何生成文本的，以及其推理过程中的关键瓶颈。\n自回归生成 目前主流的大语言模型如 GPT 采用 Decoder-Only Transformers 的架构，使用自回归 (Autoregressive) 的方式生成文本。其基本思想是，文本序列的概率可以被分解为一系列条件概率的乘积。给定一个初始的上下文词序列 $W_0$（通常是用户的输入 Prompt），模型逐个预测下一个词 (Token)，并将新生成的词加入到上下文，作为下一步预测的输入。这个过程可以用以下公式表示：\n$$ P(w_{1:T} | W_0) = \\prod_{t=1}^{T} P(w_t | w_{1:t-1}, W_0), \\text{ with } w_{1:0} = \\emptyset $$其中，$w_t$ 是在时间步 $t$ 生成的词，$w_{1:t-1}$ 是在时间步 $t$ 之前已经生成的所有词的序列。整个生成过程持续进行，直到模型生成一个特殊的终止符 (EOS, End-of-Sequence) 或者达到预设的最大长度 $T$。\nPrefilling 与 Decoding 自回归的生成方式决定了 LLM 的推理过程可以被清晰地划分为两个阶段：Prefilling (预填充) 阶段和Decoding (解码) 阶段。\nFig. 1. The Prefilling and Decoding Stages of LLM Inference. (Image source: Zhou et al., 2024)\nPrefilling 阶段：在此阶段，模型并行处理输入的整个 Prompt (例如，上图 1 中的 \u0026ldquo;I, like, natural, language\u0026rdquo;)，并计算出第一个输出 Token (\u0026ldquo;Processing\u0026rdquo;) 的概率分布。这个阶段的计算特点是并行度高，因为输入的所有 Token 可以一次性送入 Transformer 模型进行计算，这使得计算密集型操作 (如矩阵乘法) 可以充分利用 GPU 的并行计算能力，属于计算密集型 (Compute-bound)。\nDecoding 阶段：在此阶段，模型逐个生成后续的 Token。每生成一个 Token，它就会被添加到现有序列的末尾，作为下一次预测的输入。这个过程是串行的，因为下一个 Token 的生成依赖于前一个 Token。因此，这个阶段的计算特点是内存访问密集 (Memory-bound)，其主要瓶颈在于从 GPU 显存中加载庞大的模型权重，而不是计算本身。\nFig. 2. Illustration of the memory variation through time (latency) during one generation process. Note that author ignore the activation size in this figure for a simplification. (Image source: Zhou et al., 2024)\n为了加速解码过程，现代 LLM 推理框架普遍采用 KV Cache 技术。在 Transformer 的自注意力机制中，每个 Token 都需要与它之前的所有 Token 进行交互。为了避免在生成每个新 Token 时都重新计算前面所有 Token 的 Key (K) 和 Value (V) 向量，系统会将这些计算好的 K 和 V 值缓存起来。这个缓存就是 KV Cache。\n如图 2 所示，随着生成序列的增长，KV Cache 的体积会线性增大。对于一个拥有数十亿参数的模型和长序列，KV Cache 可能占用数 GB 甚至数十 GB 的显存。这使得显存成为 LLM 推理中最稀缺的资源，极大地限制了系统能够同时处理的请求数量 (即批处理大小，Batch Size)，从而直接影响了推理的吞吐量。因此，如何高效地管理和优化 KV Cache 是 LLM 推理优化的核心问题之一。\n解码策略 在每个解码步骤，模型会输出一个覆盖整个词汇表的概率分布。如何从这个分布中选择下一个 Token，是由解码策略 (或称 Token 生成策略) 决定的。不同的策略会显著影响生成文本的质量、创造性和连贯性。\n贪心搜索 贪心搜索是最简单的解码策略。在每个时间步 $t$，它总是选择概率最高的那个词作为输出：\n$$ w_t = \\underset{w}{\\operatorname{argmax}} P(w | w_{1:t-1}) $$通过这样的方式，它可以极大地减少计算复杂度，快速地产生结果，但这种方法存在明显的局限性：因为每一步只做局部的最优选择，贪心搜索很容易陷入局部最优，从而忽略整体更优的可能性，导致生成的文本常常显得枯燥、重复，缺乏多样性和创造性。\nFig. 3. At each time step, greedy search selects the token with the highest conditional probability. (Image source: d2l-en, 2019)\n代码实现:\nimport torch import torch.nn.functional as F def greedy_search(model, input_ids, max_len=20, eos_token_id=2): \u0026#34;\u0026#34;\u0026#34; A simple implementation of Greedy Search. `model` should be a function that takes input_ids and returns logits. \u0026#34;\u0026#34;\u0026#34; generated_sequence = input_ids for _ in range(max_len): # Get logits for the last token logits = model(generated_sequence) next_token_logits = logits[:, -1, :] # Select the token with the highest probability next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1) # Append the new token to the sequence generated_sequence = torch.cat([generated_sequence, next_token], dim=1) # Stop if EOS token is generated if next_token.item() == eos_token_id: break return generated_sequence 束搜索 为了克服贪心搜索的局部最优问题，束搜索在每个解码步骤保留 $k$ 个 (num_beams 或束宽度) 最可能的候选序列 (称为“束”)。在下一步，它会基于这 $k$ 个序列扩展，并再次选出总概率最高的 $k$ 个新序列。最后，算法会从所有完成的候选序列中选择一个整体概率最高的作为最终输出。\nFig. 4. The process of beam search (beam size $=2$; maximum length of an output sequence $=3$ ). The candidate output sequences are $A, C, A B, C E, A B D$, and $C E D$. (Image source: d2l-en, 2019)\n这种方式扩大了搜索空间，有效地减少了局部最优的影响，通常能够生成更高质量、更连贯的文本。然而，束搜索的本质依然是选择整体概率最高的路径，这使得它在处理开放式生成任务时仍会倾向于产生高频、常见的表达，可能缺乏创造性和多样化的输出。\n温度采样 Fig. 5. Illustration of Temperature Sampling. (Image source: Big Hummingbird Blogs, 2024)\n与确定性的搜索方法不同，采样方法引入了随机性，使得生成的文本更加多样和富有创造力。最基础的采样方法是直接根据模型的概率分布进行随机抽样。温度采样通过一个温度系数 $T$ 来调节原始概率分布的形状，加在 Softmax 上。温度系数用来调节大模型输出 Token 的概率分布的平坦程度，越大概率分布越平坦，输出越随机，越小概率分布越极端，输出越稳定。\n$$ P_T(w_i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)} $$其中 $z_i$ 是模型对词 $w_i$ 输出的 logit。\n当 $T \\to 1$ 时，概率分布不变。 当 $T \u0026lt; 1$ 时 (降温)，分布会变得更“尖锐”，高概率的词更容易被选中，生成结果更接近贪心搜索。 当 $T \u0026gt; 1$ 时 (升温)，分布会变得更“平坦”，低概率的词也有机会被选中，生成结果更具多样性和随机性。 Top-K 采样 Top-K 采样 (Fan et al., 2018) 在采样前，只保留概率最高的 $K$ 个候选词，然后在这 $K$ 个词中重新进行归一化和采样。这有效防止了模型从概率极低的词中采样，避免生成不连贯的文本。但其缺点是 $K$ 的取值是固定的，无法动态适应不同的概率分布。\nFig. 6. Illustration of Top-K Sampling. (Image source: Big Hummingbird Blogs, 2024)\nTop-p 采样 Top-p 采样 (Holtzman et al., 2019) 采用动态选择候选词集合的方法。它从概率最高的词开始，累加它们的概率，直到总和超过一个预设的阈值 $p$ (例如 0.9)。然后，模型只在这个动态生成的、最小的候选词集合 $V_{\\text{top-p}}$ 中进行采样。这种方法兼顾了文本的连贯性和创造性，是目前开放式文本生成中最常用且效果最好的策略之一。\nFig. 7. Illustration of Top-p Sampling. (Image source: Big Hummingbird Blogs, 2024)\n联合采样代码实现 (Top-K, Top-p, Temperature):\nimport torch import torch.nn.functional as F @torch.no_grad() def generate_with_sampling(model, idx, max_new_tokens, temperature=1.0, top_k=None, top_p=None, eos_token_id=2): for _ in range(max_new_tokens): # Crop context if it\u0026#39;s too long idx_cond = idx if idx.size(1) \u0026lt;= model.config.max_position_embeddings else idx[:, -model.config.max_position_embeddings:] # Forward pass to get logits logits = model(idx_cond).logits[:, -1, :] # Apply temperature if temperature \u0026gt; 0: logits = logits / temperature # Apply Top-K filtering if top_k is not None: v, _ = torch.topk(logits, min(top_k, logits.size(-1))) logits[logits \u0026lt; v[:, [-1]]] = -float(\u0026#39;Inf\u0026#39;) # Apply Top-p (Nucleus) filtering if top_p is not None: sorted_logits, sorted_indices = torch.sort(logits, descending=True) cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) # Remove tokens with cumulative probability above the threshold sorted_indices_to_remove = cumulative_probs \u0026gt; top_p # Shift the indices to the right to keep the first token above the threshold sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 indices_to_remove = sorted_indices[sorted_indices_to_remove] logits.scatter_(1, indices_to_remove, -float(\u0026#39;Inf\u0026#39;)) # Convert logits to probabilities and sample probs = F.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) # Append sampled index and check for EOS idx = torch.cat((idx, idx_next), dim=1) if idx_next.item() == eos_token_id: break return idx 推测解码 推测解码 (Speculative Decoding) (Leviathan et al., 2023) 是一种创新的加速技术，旨在用小模型的速度实现大模型的生成质量，从而在不牺牲生成质量的前提下降低延迟。\n它使用一个小型、快速的 Draft Model 一次性生成多个 (例如 $k$ 个) 候选 Token。然后，大型的 Target Model 并行地对这 $k$ 个 Token 进行一次前向传播验证。如果草稿模型预测的 Token 与目标模型一致，那么这些 Token 就被接受，从而实现了一次前向传播生成多个 Token 的效果。如果不一致，则丢弃草稿模型的后续预测，并使用目标模型的预测进行修正。\nFig. 8. Overview of online speculative decoding (OSD) framework: For each prompt, the draft model suggests multiple tokens and the target model performs the verification. (Image source: Liu et al., 2024)\n只要草稿模型与目标模型的预测有一定的一致性，推测解码就能显著降低生成延迟。其变体包括使用模型自身的早期层作为草稿模型的自推测解码 (Self-speculative decoding) 等。\n启发式策略 Best-of-N / Majority Vote: 这些方法通过生成多个候选答案来提升最终结果的质量和鲁棒性。 Best-of-N: 让大模型输出 N 个回答，然后通过一个独立的评估模型 (Verifier) 或基于奖励模型 (Reward Model) 打分，选择分数最高的 (Best) 作为最终回答。 Majority Vote / Self-Consistency: 让大模型针对同一问题生成多个不同的推理路径 (Chain-of-Thought) 和答案，然后通过多数投票的方式选出最一致的答案作为最终结果。这种方法在需要复杂推理的任务上尤其有效。 优化方法概述 了解了推理的基本原理后，我们来深入探讨如何优化这个过程。推理优化的目标主要有三个：降低延迟 (Latency)、提高吞吐量 (Throughput) 和 减少内存占用 (Memory Footprint)。现有技术可以大致分为三大类：模型压缩、内存与计算优化、以及高效模型架构。\n通常，模型推理优化的目标包括：\n通过使用更少的 GPU 设备和显存来降低模型的内存占用； 通过减少所需的浮点运算次数 (FLOPs) 来降低计算复杂度； 降低推理延迟，让模型运行得更快。 为了在内存和时间上降低推理成本，可以采用多种方法：\n应用各种并行技术，将模型扩展到大量 GPU 上。通过对模型组件和数据的智能并行化，可以运行数万亿参数的模型。 内存卸载，将暂时不用的数据卸载到 CPU，在需要时再读回。这有助于减少内存使用，但会增加延迟。 智能批处理策略；例如，EffectiveTransformer 将连续的序列打包在一起，以消除批处理中的填充 (padding)。 网络压缩技术，如剪枝 (pruning)、量化 (quantization)、蒸馏 (distillation)。参数数量或位宽更小的模型，自然需要更少的内存和更快的运行速度。 针对特定模型架构的改进。许多架构上的改变，特别是针对注意力层的改进，有助于加快 Transformer 的解码速度。 可以查阅之前关于大型模型训练的文章，了解不同类型的训练并行化和内存节省设计，包括 CPU 内存卸载。本文将重点关注网络压缩技术和针对 Transformer 模型的架构改进。\n知识蒸馏 知识蒸馏 (Knowledge Distillation, KD) (Hinton et al., 2015) 是一种直接的方法，通过将一个预训练好的昂贵模型 (“教师模型”) 的知识迁移到一个更小、更廉价的模型 (“学生模型”) 中，来构建一个更小的模型以加速推理。除了要求学生模型的输出空间与教师模型匹配以便构建合适的学习目标外，对于学生模型的架构没有太多限制。\nFig. 9. The generic framework of teacher-student knowledge distillation training. (Image source: Gou et al., 2020)\n给定一个数据集，学生模型通过蒸馏损失函数来学习模仿教师模型的输出。通常神经网络会有一个 softmax 层；例如，一个 LLM 会输出一个关于 Token 的概率分布。我们用 $\\mathbf{z}_t$ 和 $\\mathbf{z}_s$ 分别表示教师模型和学生模型在 softmax 之前的 logits 层。蒸馏损失通过最小化两个带有较高温度 $T$ 的 softmax 输出之间的差异来实现。当存在真实标签 $\\mathbf{y}$ 时，我们可以将其与一个监督学习目标 (例如交叉熵) 结合起来，该目标作用于真实标签和学生的软 logits 之间。\n$$ \\mathcal{L}_{\\mathrm{KD}}=\\mathcal{L}_{\\text {distill }}\\left(\\operatorname{softmax}\\left(\\mathbf{z}_t, T\\right), \\operatorname{softmax}\\left(\\mathbf{z}_s, T\\right)\\right)+\\lambda \\mathcal{L}_{\\mathrm{CE}}\\left(\\mathbf{y}, \\mathbf{z}_s\\right) $$其中 $\\lambda$ 是一个超参数，用于平衡软目标和硬目标的学习。$\\mathcal{L}_{\\text {distill}}$ 的一个常见选择是 KL 散度或交叉熵。\n一个早期的成功案例是 DistilBERT (Sanh et al. 2019)，它能够将 BERT 的参数减少 40%，同时在下游微调任务上保持 BERT 97% 的性能，并且运行速度快 71%。DistilBERT 的预训练损失是软蒸馏损失、监督训练损失 (在 BERT 中即掩码语言建模损失 $\\mathcal{L}_{\\text{MLM}}$) 以及一个特殊的余弦嵌入损失的组合，后者用于对齐教师和学生模型之间的隐藏状态向量。\nFig. 10. The performance of DistilBERT (Image source: Sanh et al., 2019)\n蒸馏可以很容易地与量化、剪枝或稀疏化技术相结合，其中教师模型是原始的全精度、密集模型，而学生模型则被量化、剪枝或修剪以达到更高的稀疏度。\n量化 为了在推理过程中进一步提升模型性能，我们可以超越低精度浮点数，转而使用量化 (Quantization) 技术。量化将模型的浮点权重转换为低位宽的整数表示，例如 8 位整数 (INT8)，甚至 4 位整数 (INT4)。\n在深度神经网络上应用量化通常有两种方法：\n训练后量化 (Post-Training Quantization, PTQ)：首先将模型训练至收敛，然后在不进行更多训练的情况下将其权重转换为较低的精度。与训练相比，这种方法的实现成本通常很低。 量化感知训练 (Quantization-Aware Training, QAT)：在预训练或进一步微调的过程中应用量化。QAT 能够获得更好的性能，但需要额外的计算资源和对代表性训练数据的访问。 精度对比 在深度学习领域，数值精度决定着计算速度和模型性能之间的微妙平衡。理解不同浮点数和整数格式的优缺点，是优化大规模模型性能的关键。浮点数在计算机中以三部分表示：\n符号位 (Sign)：表示数值的正负。 指数位 (Exponent)：决定数值的动态范围。 尾数位 (Mantissa 或 Significand)：决定数值的精确度，为了方便通常我们将尾数位称为小数 (fraction)。 Fig. 11. fp32 vs fp16 vs bf16 (Image source: Raschka, 2023)\n类型 总位数 符号位 指数位 尾数位 特性 FP64 (双精度) 64 1 11 52 极高精度，广泛用于科学计算，但计算昂贵，内存占用大，深度学习中少用 FP32 (单精度) 32 1 8 23 深度学习训练标准格式，速度适中，内存占用较大 FP16 (半精度) 16 1 5 10 更快计算，内存占用是 FP32 的一半，但动态范围受限，易发生数值溢出 BF16 (Brain Floating Point) 16 1 8 7 动态范围与 FP32 相同，避免溢出，更适合大语言模型，精度略低于 FP16 纯粹的 FP16 精度虽速度快、内存小，但由于动态范围有限，极易出现数值溢出 (overflow) 和下溢 (underflow)，使训练不稳定甚至无法收敛。因此，采用混合精度训练 (Mixed-Precision)至关重要。\n量化将浮点数映射为整数，进一步降低计算复杂度与内存占用。具体来说：\nINT8：占用内存仅为 FP32 的 1/4，显著加速推理速度，但可能会略微降低模型精度。 INT4：更极端的压缩方案，更适合资源极其受限的设备，或需要极高吞吐量的推理场景。 Transformer 量化挑战 许多关于 Transformer 模型量化的研究都有一个共同的发现：简单的低精度 (例如 8-bit) 训练后量化会导致显著的性能下降，这主要是由于激活值存在高动态范围，而一个简单的激活值量化策略无法保持模型的性能。\nFig. 12. Only quantizing model weights to 8-bit while keeping activation at full precision (W8A32) achieves much better results when activations are quantized to 8-bit irrespective of whether weights are in lower precision (W8A8 and W32A8). (Image source: Bondarenko et al. 2021)\nBondarenko et al. (2021) 在小型 BERT 模型进行实验发现由于输出张量中存在强烈的离群值 (outliers)，FFN (前馈网络) 的输入和输出具有非常不同的动态范围。因此，对 FFN 的残差和进行逐张量 (per-tensor) 量化可能会导致显著的误差。\n随着模型规模增长到数十亿参数，所有 Transformer 层中都开始出现大幅度的离群特征，这导致简单的低比特量化失败。研究人员在大于 6.7B 参数大小的 OPT (Zhang et al. 2022) 模型中观察到了这种现象。更大的模型有更多的层带有极端的离群值，而这些离群特征对模型性能有显著影响。在少数维度上，激活值离群点的规模可以比其他大多数值大约 100 倍。\nFig. 13. The mean zero-shot accuracy over a set of language tasks (WinoGrande, HellaSwag, PIQA, LAMBADA) of OPT models of increasing sizes. (Image source: Dettmers et al. 2022)\n训练后量化 混合精度量化 解决上述量化挑战最直接的方法是为权重和激活值实现不同精度的量化。\nGOBO (Zadeh et al. 2020) 是最早在 BERT 上应用训练后量化的模型之一。它假设每层的模型权重服从高斯分布，因此通过跟踪每层的均值和标准差来检测离群值。离群特征保持原始形式，而其他值被分成多个桶 (bin)，只存储相应的桶索引和质心值。\nFig. 14. The pseudocode for GOBO algorithm. (Image source: Zadeh et al. 2020)\n基于在 BERT 中只有某些激活层 (例如 FFN 后的残差连接) 会导致大的性能下降的观察，Bondarenko et al. (2021) 采用了混合精度量化，对有问题的激活值使用 16 位量化，而对其他部分使用 8 位量化。\nLLM.int8() (Dettmers et al. 2022) 中的混合精度量化通过两种混合精度分解实现：\n因为矩阵乘法包含一系列行向量和列向量之间的独立内积，我们可以对每个内积施加独立的量化：每一行和每一列都通过其绝对值最大值进行缩放，然后量化到 INT8。 离群的激活特征 (例如比其他维度大 20 倍) 保持在 FP16 格式，但它们只占总权重的一小部分。如何识别离群值是经验性的。 Fig. 15. Two mixed-precision decompositions of LLM.int8(). (Image source: Dettmers et al. 2022)\n细粒度量化 Fig. 16. Comparison of quantization at different granularity. $d$ is the model size / hidden state dimension and $h$ is the number of heads in one MHSA (multi-head self-attention) component. (Image source: Lilian, 2023)\n简单地将一层中的整个权重矩阵进行量化 (“逐张量”或“逐层”量化) 最容易实现，但无法达到良好的量化粒度。\nQ-BERT (Shen, et al. 2020) 对一个微调过的 BERT 模型应用了分组量化 (group-wise quantization)，将 MHSA (多头自注意力) 中每个头对应的单个矩阵 $W$ 视为一个组，然后应用基于 Hessian 矩阵的混合精度量化。\n逐嵌入组 (Per-embedding group, PEG) (Bondarenko et al. 2021) 激活值量化的动机是观察到离群值只出现在 $d$ (隐藏状态/模型大小) 维度中的少数几个维度上。逐嵌入量化计算成本相当高。相比之下，PEG 量化将激活张量沿着嵌入维度分成几个大小均匀的组，其中同一组中的元素共享量化参数。为了确保所有离群值被分到同一组，他们应用了一种确定性的基于范围的嵌入维度排列，其中维度按其值范围排序。\nZeroQuant (Yao et al. 2022) 对权重使用分组量化 (与 Q-BERT 相同)，对激活值使用逐 Token 量化 (token-wise quantization)。为了避免昂贵的量化和反量化计算，ZeroQuant 构建了定制化的内核，将量化操作与其前一个操作符合并。\n二阶信息用于量化 Q-BERT (Shen, et al. 2020) 为其混合精度量化开发了Hessian 感知量化 (Hessian AWare Quantization, HAWQ) (Dong, et al. 2019)。其动机是具有较高 Hessian 谱 (即较大的顶层特征值) 的参数对量化更敏感，因此需要更高的精度。这实质上是一种识别离群值的方法。\n从另一个角度看，量化问题是一个优化问题。给定权重矩阵 $\\mathbf{W}$ 和输入矩阵 $\\mathbf{X}$，我们希望找到一个量化后的权重矩阵 $\\hat{\\mathbf{W}}$ 来最小化均方误差 (MSE)：\n$$ \\hat{\\mathbf{W}}^* = \\arg \\min_{\\hat{\\mathbf{W}}} |\\mathbf{W}\\mathbf{X} - \\hat{\\mathbf{W}}\\mathbf{X}| $$GPTQ (Frantar et al. 2022) 在 OBC (Optimal Brain Compression) (Frantar et al. 2022) 方法基础上进行优化，将权重矩阵 $\\mathbf{W}$ 视为行向量 $\\mathbf{w}$ 的集合，并独立地对每一行进行量化。GPTQ 迭代地量化更多的权重，这些权重是贪婪选择的，以最小化量化误差。对所选权重的更新有一个利用 Hessian 矩阵的闭式解公式。\nFig. 17. The pseudocode for GPTQ algorithm. (Image source: Frantar et al. 2022)\nGPTQ 可以将 OPT-175B 中权重的位宽降低到 3 bit 或 4 bit 而没有太多性能损失，但它只适用于模型权重，不适用于激活值。\n离群值平滑 Fig. 18. Magnitude of the input activations and weights of a linear layer in OPT-13B before and after SmoothQuant (Image source: Xiao et al. 2022)\n从上图我们可以看到在 Transformer 模型中，激活值比权重更难量化。并且有以下三个特征：\n激活比权重更难量化： 权重用 INT8/INT4 量化几乎不损准确率，但激活则更敏感。 异常值放大激活量化难度： 激活中的极端大值约为普通值的 100 倍，直接 INT8 量化会把多数小值压成 0。 异常值固定在少数通道： 这些极端值长期集中在固定通道，通道间分布高度不均。 SmoothQuant (Xiao et al. 2022) 提出了一个聪明的解决方案，通过数学上等价的变换将离群特征从激活值平滑到权重，然后对权重和激活值都进行量化 (W8A8)。因此，SmoothQuant 比混合精度量化具有更好的硬件效率。\nFig. 19. SmoothQuant migrates the scale variance from activations to weights offline to reduce the difficulty of activation quantization. Both the resulting new weight and activation matrices are easy to quantize. (Image source: Xiao et al. 2022)\n考虑一个逐通道的平滑因子 $\\mathbf{s}$，SmoothQuant 根据以下公式缩放权重：\n$$ \\mathbf{Y} = (\\mathbf{X}\\text{diag}(\\mathbf{s})^{-1}) \\cdot (\\text{diag}(\\mathbf{s})\\mathbf{W}) = \\hat{\\mathbf{X}}\\hat{\\mathbf{W}} $$平滑因子可以很容易地离线融合到前一层的参数中。一个超参数 $\\alpha$ 控制将量化难度从激活值迁移到权重的程度：$\\mathbf{s} = \\max(|\\mathbf{X}_j|)^\\alpha / \\max(|\\mathbf{W}_j|)^{1-\\alpha}$。论文发现在实验中，对于许多 LLM, $\\alpha=0.5$ 是一个最佳选择。对于激活值中具有更显著离群值的模型，可以调整 $\\alpha$ 使其更大。\n量化感知训练 量化感知训练 (Quantization-Aware Training, QAT) 将量化操作融合到预训练或微调过程中，直接学习模型权重的低比特表示，以额外的训练时间和计算资源为代价，获得更高的性能。\n常见的量化感知训练方法包括：\n直接微调法：首先对模型进行一次性量化，然后在原始预训练数据集或具有代表性的训练数据集上进一步微调模型，使模型对量化误差敏感并主动补偿，从而提升量化后模型的性能。训练目标可以与原始预训练目标一致 (如语言模型中的负对数似然 NLL 或掩蔽语言建模 MLM)，也可根据具体下游任务定义特定目标 (如分类任务的交叉熵)。一个典型的实现是 QLoRA，它通过低比特 (如 4-bit) 基座模型结合全精度 LoRA 适配器实现高效微调。\n知识蒸馏法：将全精度模型作为教师 (teacher)，低精度模型作为学生 (student)，通过蒸馏损失引导学生模型接近教师模型的性能。ZeroQuant (Yao et al. 2022) 采用的逐层知识蒸馏 (Layer-by-layer Knowledge Distillation, LKD) 技术即属于这一方法，它逐层地进行模型权重量化，每一层量化后的网络以对应的全精度层为教师，通过最小化两者之间权重计算结果的均方误差 (MSE) 来提升性能。\n$$ \\mathcal{L}_{L K D, k}=M S E\\left(L_k \\cdot L_{k-1} \\cdot L_{k-2} \\cdot \\ldots \\cdot L_1(\\boldsymbol{X})-\\widehat{L}_k \\cdot L_{k-1} \\cdot L_{k-2} \\cdot \\ldots \\cdot L_1(\\boldsymbol{X})\\right) $$剪枝 网络剪枝 (Pruning) 通过删除不重要的模型权重或连接来减小模型规模，从而实现模型压缩，同时尽可能保持模型的性能。根据实现方式不同，剪枝可分为非结构化剪枝和结构化剪枝。\n非结构化剪枝：不受限于特定模式，可随意丢弃网络中任意位置的权重或连接，因此破坏了原始网络的结构规律。由于这种方式产生的稀疏模式难以适配现代硬件架构，通常无法有效提升实际推理效率。\n结构化剪枝：通过裁剪整个结构 (如卷积核、通道或层) 来保持网络的结构性，使剪枝后的网络仍然适用于现有硬件优化的密集矩阵计算，从而显著提升实际推理性能。我们在本文中特别关注结构化剪枝，以实现 Transformer 模型中的高效稀疏结构。\n网络剪枝的典型工作流程包括以下三个步骤：\n训练一个完整的密集网络，直至收敛； 对网络进行剪枝，移除冗余的结构或权重； (可选) 进一步微调网络，以恢复剪枝后模型的性能。 彩票假设 剪枝的理论基础之一是彩票假设 (Lottery Ticket Hypothesis, LTH) (Frankle \u0026amp; Carbin, 2019)。该假设认为，一个随机初始化的密集神经网络中蕴含着某些稀疏子网络 (即“中奖彩票”)，这些子网络在单独训练时即可达到与完整网络接近甚至更好的性能。\nLTH 的核心观点是，并非所有参数都同等重要。模型中只有一小部分参数真正发挥了关键作用，这表明大量参数并非用于解决过拟合问题，而主要是提供了充足的初始化搜索空间，使得高性能子网络能够被发现。\n为了验证这一假设，Frankle 与 Carbin 提出了以下实验步骤：\n随机初始化一个密集的神经网络，初始权重记为 $\\theta_0$； 完整训练该网络，使参数达到良好性能，记为 $\\theta$； 对训练后的参数 $\\theta$ 进行剪枝，生成稀疏掩码 $m$； 选取“中奖彩票”子网络，初始参数定义为 $m \\odot \\theta_0$。 实验发现，仅使用步骤 1 中选定的少量“中奖彩票”参数，并保持它们原始的随机初始化值进行训练，模型仍可达到几乎与原网络相同的准确率。\n这一结果表明，庞大的初始参数空间并非在最终模型部署时是必需的，而是在训练阶段提供了大量初始可能性，使得网络可以发掘出性能优异的稀疏结构。这也解释了为什么虽然经过剪枝的模型规模显著减小，但直接从零开始训练相同的稀疏结构却难以成功。\n剪枝策略 幅度剪枝 (Magnitude pruning) 是最简单但相当有效的剪枝方法——绝对值最小的权重被修剪掉。事实上，一些研究 (Gale et al. 2019) 发现，简单的幅度剪枝方法可以达到与复杂剪枝方法 (如变分丢弃 Molchanov et al. 2017 和 $l_0$ 正则化 Louizos et al. 2017) 相当或更好的结果。幅度剪枝易于应用于大型模型，并在广泛的超参数范围内实现相当一致的性能。\n渐进幅度剪枝 (Gradual Magnitude Pruning, GMP) (Zhu \u0026amp; Gupta, 2017) 方法基于大型稀疏模型能够比小型但密集的模型取得更好的性能，提出了在训练过程中逐渐增加网络的稀疏度。在每个训练步骤，绝对值最小的权重被掩码为零以达到期望的稀疏度水平 $s$，并且被掩码的权重在反向传播期间不接收梯度更新。期望的稀疏度水平 $s$ 随着训练步骤的增加而增加。GMP 的过程对学习率调度敏感，学习率应该高于密集网络训练中使用的学习率，但又不能太高以至于无法收敛。\n迭代剪枝 (Iterative pruning) (Renda et al. 2020) 则是多次迭代步骤 2 (剪枝) 和步骤 3 (重训练)：在每次迭代中只剪枝一小部分权重，然后重新训练模型。这个过程重复进行，直到达到期望的稀疏度水平。\n重训练 重训练步骤可以是简单的微调，使用相同的预训练数据或其他任务特定的数据集。\n彩票假设 提出了一种 权重回溯 (weight rewinding) 的重训练技术：剪枝后，未剪枝的权重被重新初始化回训练早期的原始值，然后使用相同的学习率调度进行重训练。\n学习率回溯 (Renda et al. 2020) 只将学习率重置回其早期值，而未剪枝的权重自上一个训练阶段结束以来保持不变。他们观察到 (1) 在各种网络和数据集上，使用权重回卷的重训练优于使用微调的重训练；(2) 在所有测试场景中，学习率回卷与权重回卷相当或更优。\n稀疏性 稀疏性是扩展模型容量同时保持模型推理计算效率的有效方法。这里我们考虑两种用于 Transformer 的稀疏性类型：\n稀疏化的密集层，包括自注意力和 FFN 层。 稀疏模型架构；即通过引入专家混合 (Mixture-of-Experts, MoE) 组件。 通过剪枝实现 N:M 稀疏性 N:M 稀疏性是一种结构化的稀疏模式，与现代 GPU 硬件优化配合良好，其中每 $M$ 个连续元素中有 $N$ 个为零。例如，Nvidia A100 的稀疏张量核心支持 2:4 稀疏性以实现更快的推理。\nFig. 20. The illustration of achieving N:M structure sparsity. (Image source: Zhou et al. 2021)\n为了使密集神经网络稀疏化以遵循 N:M 结构化稀疏模式，Nvidia 建议使用三步常规工作流程来训练剪枝网络：训练 -\u0026gt; 剪枝以满足 2:4 稀疏性 -\u0026gt; 重训练。\n通过置换可以在剪枝过程中提供更多选择，以保持大幅度参数或满足像 N:M 稀疏性这样的特殊限制。只要两个矩阵的配对轴以相同的顺序进行置换，矩阵乘法的结果就不会改变。例如：\n(1) 在自注意力模块内，如果对查询嵌入矩阵 $\\mathbf{Q}$ 的轴 1 和键嵌入矩阵 $\\mathbf{K}^\\top$ 的轴 0 应用相同的置换顺序，$\\mathbf{Q}\\mathbf{K}^\\top$ 的最终矩阵乘法结果将保持不变。\nFig. 21. Illustration of same permutation on $\\mathbf{Q}$ (axis 1) and $\\mathbf{K}^\\top$ (axis 0) to keep the results of a self-attention module unchanged. (Image source: Lilian, 2023)\n(2) 在包含两个 MLP 层和一个 ReLU 非线性层的 FFN 层内，我们可以以相同的顺序置换第一个线性权重矩阵 $\\mathbf{W}_1$ 的轴 1 和第二个线性权重矩阵 $\\mathbf{W}_2$ 的轴 0。\nFig. 22. Illustration of the same permutation on $\\mathbf{W}_1$ (axis 1) and $\\mathbf{W}_2$ (axis 0) to keep the FFN layer\u0026rsquo;s output unchanged. For simplicity, the bias terms are skipped but the same permutation should be applied on them too. (Image source: Lilian, 2023)\n为了强制实现 N:M 结构化稀疏性，我们将一个矩阵的列分成多个 $M$ 列的片段 (称为“条带”)，我们可以很容易地观察到，每个条带内的列顺序和条带的顺序对 N:M 稀疏性限制没有影响。\nChannel Permutations Channel Permutations (Pool \u0026amp; Yu, 2021) 采用了迭代贪心算法来寻找最优置换，以最大化 N:M 稀疏性的权重幅度。所有通道对都被推测性地交换，只有导致幅度最大增加的交换被采纳，从而生成一个新的置换并结束单次迭代。贪心算法可能只找到局部最优解，因此他们引入了两种技术来跳出局部最优：\n有界回归 (Bounded regressions)：在实践中，随机交换两个通道，最多固定次数。解决方案搜索被限制在只有一个通道交换的深度，以保持搜索空间的广度和浅度。 窄而深的搜索 (Narrow, deep search)：选择多个条带并同时优化它们。 Fig. 23. Algorithm of finding the best permutation for N:M sparsity greedily and iteratively. (Image source: Pool \u0026amp; Yu, 2021)\n如果网络在剪枝前进行了置换，相比于在其默认通道顺序下剪枝，可以获得更好的性能。\nSTE \u0026amp; SR-STE STE (Straight-Through Estimator) (Bengio et al. 2013) 计算密集参数相对于剪枝后网络 $\\widetilde{W}$ 的梯度 $\\partial\\mathcal{L}/\\partial\\widetilde{W}$，并将其应用于密集网络 $W$ 作为近似： $$ W_{t+1} \\leftarrow W_t - \\gamma \\frac{\\partial\\mathcal{L}}{\\partial\\widetilde{W}} $$SR-STE (Sparse-refined STE) (Zhou et al. 2021) 对 STE 方法进行了扩展, 采用从头开始训练一个具有 N:M 稀疏性的模型，它通常用于模型量化中的反向传播更新，以适用于幅度剪枝和稀疏参数更新。通过以下方式更新密集权重 $W$： $$ W_{t+1} \\leftarrow W_t - \\gamma \\frac{\\partial\\mathcal{L}}{\\partial\\widetilde{W}} + \\lambda_W(\\overline{\\mathcal{E}} \\odot W_t) $$ 其中 $\\overline{\\mathcal{E}}$ 是 $\\widetilde{W}$ 的掩码矩阵，$\\odot$ 是逐元素乘法。SR-STE 旨在通过 (1) 限制在 $\\widetilde{W}_t$ 中被剪枝的权重的值，和 (2) 提升在 $\\widetilde{W}_t$ 中未被剪枝的权重，来防止二元掩码发生大的变化。\nFig. 24. Comparison of STE and SR-STE. $\\odot$ is element-wise product; $\\otimes$ is matrix multiplication. (Image source: Zhou et al. 2021)\nTop-KAST Top-KAST (Top-K Always Sparse Training) (Jayakumar et al. 2021) 方法与 STE 或 SR-STE 不同，可以在前向和后向传播中都保持恒定的稀疏性，而不需要使用密集参数或密集梯度进行前向传播。\n在一个训练步骤 $t$，Top-KAST 处理如下：\n稀疏前向传播：选择一个参数子集 $A^t \\subset \\Theta$，包含每层按幅度排序的前 $K$ 个参数，限制为权重的前 $D$ 比例。在时间 $t$ 的参数化 $\\alpha^t$ 中，如果参数不在 $A^t$ (活动权重) 中，则其值为零。 $$ \\alpha_i^t = \\begin{cases} \\theta_i^t \u0026 \\text{if } i \\in A^t = \\{i \\mid \\theta_i^t \\in \\text{TopK}(\\theta^t, D)\\} \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$ 其中 $\\text{TopK}(\\theta, x)$ 从 $\\theta$ 中根据幅度选择前 $x$ 比例的权重。\n稀疏后向传播：然后将梯度应用于一个更大的参数子集 $B \\subset \\Theta$，其中 $B$ 包含 $(D+M)$ 比例的权重且 $A \\subset B$。更新更大比例的权重可以更有效地探索不同的剪枝掩码，使其更有可能在顶部的 $D$ 比例活动权重中引起置换。 $$ \\Delta\\theta_i^t = \\begin{cases} -\\eta \\nabla_{\\alpha_t} \\mathcal{L}(y, x, \\alpha^t)_i \u0026 \\text{if } i \\in B^t = \\{i \\mid \\theta_i^t \\in \\text{TopK}(\\theta^t, D+M)\\} \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$ 训练分为两个阶段，集合 $B \\setminus A$ 中的附加坐标控制了引入多少探索。探索量预计会随着训练过程逐渐减少，掩码最终会稳定下来。\nFig. 25. The pruning mask of Top-KAST stabilizes in time. (Image source: Jayakumar et al. 2021)\n为了防止“富者愈富”现象，Top-KAST 通过 L2 正则化损失惩罚活动权重的大小，以鼓励探索新项目。在 $B \\setminus A$ 中的参数比 $A$ 中的参数受到更多的惩罚，以便在更新期间为稳定掩码设置更高的选择门槛。 $$ L_{\\text{penalty}}(\\alpha_i^t) = \\begin{cases} |\\theta_i^t| \u0026 \\text{if } i \\in A^t \\\\ |\\theta_i^t|/D \u0026 \\text{if } i \\in B^t \\setminus A^t \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$稀疏化 Transformer 稀疏化 Transformer (Jaszczur et al. 2021) 在 Transformer 架构中稀疏化了自注意力和 FFN 层，实现了单样本推理 37 倍的加速。\nFig. 26. Decoding speed of a single token for Terraformer with 17B parameters is 37x faster than a dense baseline model, (Image source: Jaszczur et al. 2021)\n稀疏 FFN 层：每个 FFN 层包含 2 个 MLP 和一个 ReLU。因为 ReLU 会引入大量零值，他们对激活值实施了一个固定结构，强制在一个 $N$ 个元素的块中只有一个非零值。稀疏模式是动态的，对每个 Token 都不同。 $$ \\begin{aligned} Y_{\\text{sparse}} \u0026= \\max(0, xW_1 + b_1) \\odot \\text{Controller}(x) \\\\ \\text{SparseFFN}(x) \u0026= Y_{\\text{sparse}} W_2 + b_2 \\\\ \\text{Controller}(x) \u0026= \\arg\\max(\\text{Reshape}(xC_1C_2, (-1, N))) \\end{aligned} $$ 其中 $Y_{\\text{sparse}}$ 中的每个激活对应于 $W_1$ 中的一列和 $W_2$ 中的一行。控制器是一个低秩瓶颈密集层，$C_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{lowrank}}}, C_2 \\in \\mathbb{R}^{d_{\\text{lowrank}} \\times d_{\\text{ff}}}$ 且 $d_{\\text{lowrank}} = d_{\\text{model}}/N$。它在推理时使用 arg max 来选择哪些列应该为非零，在训练时使用 Gumbel-softmax 技巧。因为我们可以在加载 FFN 权重矩阵之前计算 $\\text{Controller}(x)$，我们知道哪些列将被置零，因此选择不将它们加载到内存中以加速推理。\nFig. 27. (a) Sparse FFN layer; columns in red are not loaded in memory for faster inference. (b) Sparse FFN controller for 1:4 sparsity. (Image source: Jaszczur et al. 2021)\n稀疏 QKV (注意力) 层：在注意力层中，维度 $d_{\\text{model}}$ 被分成 $S$ 个模块，每个模块的大小为 $M = d_{\\text{model}}/S$。为了确保每个子部分可以访问嵌入的任何部分，Scaling Transformer 引入了一个乘法层 (即，一个将来自多个神经网络层的输入逐元素相乘的层)，它可以表示任意置换，但比密集层包含更少的参数。\n给定一个输入向量 $x \\in \\mathbb{R}^{d_{\\text{model}}}$，乘法层输出 $y \\in \\mathbb{R}^{S \\times M}$： $$ y_{s,m} = \\sum_i x_i D_{i,s} E_{i,m} \\quad \\text{where } D \\in \\mathbb{R}^{d_{\\text{model}} \\times S}, E \\in \\mathbb{R}^{d_{\\text{model}} \\times M} $$ 乘法层的输出是一个大小为 $\\mathbb{R}^{\\text{batch size} \\times \\text{length} \\times S \\times M}$ 的张量。然后它由一个二维卷积层处理，其中 length 和 $S$ 被视为图像的高度和宽度。这样的卷积层进一步减少了注意力层的参数数量和计算时间。\nFig. 28. (a) A multiplicative layer is introduced to enable partitions to access any part of an embedding. (b) Combination of multiplicative dense layer and 2-D convolutional layer reduces the number of parameters and computation time of the attention layer. (Image source: Jaszczur et al. 2021)\n为了更好地处理长序列，Scaling Transformer 进一步配备了来自 Reformer (Kitaev, et al. 2020) 的 LSH (局部敏感哈希) 注意力和 FFN 块循环。\n混合专家 混合专家 (Mixture-of-Experts, MoE) 模型由多个“专家”网络组成，每个输入样本只激活部分专家网络来进行计算。\nFig. 29. Dense Transformer vs Sparse Expert Transformer. (Image source: Fedus et al. 2022)\nDense Model：所有输入令牌 (token) 使用相同的前馈网络 (FFN) 参数进行处理。虽然结构简单易于训练，但随着模型规模的增长，其计算成本也快速增加。\nSparse Expert Model：每个输入令牌独立地路由到多个专家网络中的少数专家进行处理。这种稀疏路由机制使得模型可以拥有更多独特的参数，同时整体计算成本不显著增加，因而提高了参数效率和扩展能力，有效降低了推理阶段的计算成本。\n路由策略改进 MoE 层有一个路由网络，为每个输入 Token 分配一个专家子集。在传统的 MoE 模型中，路由策略是按照 Token 在自然顺序中出现的顺序，将每个 Token 路由到其偏好的专家。如果一个 Token 被路由到已经达到容量的专家，该 Token 将被标记为“溢出”并被跳过。\nVision MoE (V-MoE) (Riquelme et al. 2021) 将 MoE 层添加到 ViT (Vision Transformer) 中。它达到了之前 SOTA 的性能，但只需要一半的推理计算量。V-MoE 可以扩展到 15B 参数。他们的实验使用了 $k=2$，32 个专家，并且每隔 2 层放置一个专家 (意味着 MoE 被放置在每隔一层)。\n由于每个专家的容量有限，一些重要和信息丰富的 Token 如果出现得太晚 (例如，句子中单词的顺序，或图像块的顺序)，可能会被丢弃。为了避免传统路由方案的这种缺点，V-MoE 采用了批量优先路由 (BPR, Batch Priority Routing)，首先为具有高优先级的 Token 分配专家。BPR 在专家分配前为每个 Token 计算一个优先级分数 (top-k 路由器分数的最大值或总和)，并相应地改变 Token 的顺序。这保证了专家容量缓冲区将首先被关键 Token 填充。\nFig. 30. How image patches are discarded according to priority scores when $C \u0026lt; 1$. (Image source: Riquelme et al. 2021)\n当 $C \\le 0.5$ 时，BPR 比传统路由效果好得多，此时模型开始丢弃大量 Token。它使模型即使在相当低的容量下也能与密集网络竞争。\n在研究如何解释图像类别与专家的关联时，他们观察到早期的 MoE 层更通用，而后期的 MoE 层可能专门用于少数几个图像类别。\nTask MoE (Task-level Mixture-of-Experts) (Kudugunta et al. 2021) 考虑了任务信息，并在机器翻译中以任务级别而不是单词或 Token 级别路由 Token。他们使用 MNMT (多语言神经机器翻译) 作为例子，并根据目标语言或语言对对翻译任务进行分组。\nToken 级别的路由是动态的，每个 Token 的路由决策是独立做出的。因此，在推理时，服务器需要预加载所有专家。相比之下，任务级别的路由在给定固定任务时是静态的，因此一个任务的推理服务器只需要预加载 $k$ 个专家 (假设是 top-k 路由)。根据他们的实验，与密集模型基线相比，Task MoE 可以实现与 Token MoE 相似的性能增益，峰值吞吐量高 2.6 倍，解码器大小仅为 1.6%。\n任务级别的 MoE 本质上是根据预定义的启发式规则对任务分布进行分类，并将这些人类知识融入到路由器中。当这种启发式规则不存在时 (例如，考虑一个通用的句子续写任务)，如何利用 Task MoE 就不那么直接了。\nPR-MoE (Pyramid residual MoE) (Rajbhandari et al. 2022) 让每个 Token 通过一个固定的 MLP 和一个选择的专家。由于观察到 MoE 在后期层更有益，PR-MoE 在后期层采用更多的专家。DeepSpeed 库实现了一个灵活的多专家、多数据并行，以支持训练具有不同专家数量的 PR-MoE。\nFig. 31. Illustration of PR-MoE architecture in comparison with a standard MoE. (Image source: Rajbhandari et al. 2022)\n内核改进 专家网络可以托管在不同的设备上。然而，当 GPU 数量增加时，每个 GPU 的专家数量减少，专家之间的通信 (“All-to-all”) 变得更加昂贵。跨多个 GPU 的专家之间的 All-to-all 通信依赖于 NCCL 的 P2P API，在大规模下无法饱和高速链路 (如 NVLink, HDR InfiniBand) 的带宽，因为随着使用更多节点，单个数据块变得更小。现有的 all-to-all 算法在小工作负载的大规模场景下表现不佳。有多种内核改进可以实现更高效的 MoE 计算，例如使 all-to-all 通信更便宜/更快。\nDeepSpeed (Rajbhandari et al. 2022) 和 TUTEL (Hwang et al. 2022) 都实现了一种基于树的分层 all-to-all 算法，该算法先运行一个节点内 all-to-all，然后是一个节点间 all-to-all。它将通信跳数从 $O(G)$ 减少到 $O(G_{\\text{node}} + G/G_{\\text{node}})$，其中 $G$ 是 GPU 节点的总数，$G_{\\text{node}}$ 是每个节点的 GPU 核心数。尽管在这种实现中通信量增加了一倍，但它在小批量大规模场景下实现了更好的扩展性，因为瓶颈在于延迟而不是通信带宽。\nDynaMoE (Kossmann et al. 2022) 使用动态重编译来使计算资源适应专家之间的动态工作负载。RECOMPILE 机制从头开始编译计算图，并且只在需要时重新分配资源。它测量分配给每个专家的样本数量，并动态调整它们的容量因子 $C$，以减少运行时的内存和计算需求。基于样本-专家分配在训练早期收敛的观察，收敛后引入样本分配缓存，然后使用 RECOMPILE 来消除门控网络和专家之间的依赖关系。\n架构优化 Efficient Transformers Efficient Transformers (Tay et al. 2020) 的综述论文回顾了一系列 Transformer 架构，这些架构在计算和内存效率方面有所改进。对此感兴趣的读者可以阅读原文。\nFig. 32. Categorization of efficient transformer models. (Image source: Tay et al. 2020)\nKV Cache 优化 Multi-Query Attention (MQA) \u0026amp; Grouped-Query Attention (GQA)：标准的多头注意力 (Multi-Head Attention, MHA) 中，每个头都有一套独立的 Key 和 Value 投影矩阵。MQA (Shazeer, 2019) 提出让所有的查询头 (Query heads) 共享同一套 Key 和 Value 头，极大地减小了 KV Cache 的体积。GQA (Ainslie et al., 2023) 则是 MHA 和 MQA 的折中，它将查询头分组，组内的头共享一套 K/V，在性能和效果上取得了很好的平衡。\nvLLM(Kwon et al., 2023) 借鉴了操作系统中虚拟内存和分页的思想，提出了 PagedAttention。它将 KV Cache 分割成固定大小的块 (Block)，这些块在物理显存中可以不连续存储，通过一个“块表”来管理逻辑块到物理块的映射。关于 vLLM 的详细介绍可以参考我之前的博客vLLM：高吞吐、有效内存的LLM服务引擎。这种方法几乎完全消除了内存碎片 (内部和外部)，使得显存利用率接近 100%。更重要的是，它通过写时复制 (Copy-on-Write) 机制，可以非常高效地实现跨请求的 KV Cache 共享，极大地提升了并行采样、Beam Search 等复杂解码场景下的吞吐量。\nFlashAttention FlashAttention(Dao et al., 2022) 是一种 IO 感知的精确注意力算法。它认识到标准 Attention 实现的主要瓶颈在于 GPU HBM (高带宽内存) 和 SRAM (片上高速缓存) 之间的数据读写。FlashAttention 通过 Tiling (分块) 和 Recomputation (重计算) 技术，将整个 Attention 计算融合到一个 CUDA 核中，避免了将巨大的 $N \\times N$ 注意力矩阵写入和读出 HBM。这极大地减少了内存访问量，从而在不牺牲精度的情况下，将 Attention 的计算速度提升了数倍。FlashAttention-2(Dao, 2023) 进一步优化了并行度和硬件利用率。 Fig. 33. FlashAttention uses tiling to avoid materializing the large N × N attention matrix on slow GPU HBM, achieving up to 7.6× speedup over PyTorch. (Image source: Dao et al., 2022)\n参考文献 [1] Zhou, Zixuan, et al. “A survey on efficient inference for large language models.” arXiv preprint arXiv:2404.14294 (2024).\n[2] Zhang, Aston, et al. “Dive into Deep Learning.”. Cambridge University Press, 2023.\n[3] Big Hummingbird Blogs. (2024). “A Visual Explanation of LLM Hyperparameters.” Blog post.\n[4] Fan, Angela, Mike Lewis, and Yann Dauphin. “Hierarchical neural story generation.” arXiv preprint arXiv:1805.04833 (2018). int arXiv:1805.04832.\n[5] Holtzman, Ari, et al. “The curious case of neural text degeneration.” arXiv preprint arXiv:1904.09751 (2019).\n[6] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. “Fast inference from transformers via speculative decoding.” International Conference on Machine Learning. PMLR, 2023.\n[7] Liu, Xiaoxuan, et al. “Online speculative decoding.” arXiv preprint arXiv:2310.07177 (2023).\n[8] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015).\n[9] Gou, Jianping, et al. “Knowledge distillation: A survey.” International Journal of Computer Vision 129.6 (2021): 1789-1819.\n[10] Sanh, Victor, et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.” arXiv preprint arXiv:1910.01108 (2019).\n[11] Raschka, S. (2023). “Accelerating Large Language Models with Mixed-Precision Techniques.” Blog post.\n[12] Bondarenko, Yelysei, Markus Nagel, and Tijmen Blankevoort. \u0026ldquo;Understanding and overcoming the challenges of efficient transformer quantization.\u0026rdquo; arXiv preprint arXiv:2109.12948 (2021).\n[13] Zhang, S., Roller, S., Goyal, N., et al. (2022). “OPT: Open pre-trained transformer language models.” arXiv preprint arXiv:2205.01068.\n[14] Dettmers, T., et al. (2022). “LLM.int8(): 8-bit matrix multiplication for transformers at scale.” arXiv preprint arXiv:2208.07339.\n[15] Zadeh, V. K., et al. (2020). “GOBO: Quantizing attention-based NLP models for low latency and energy efficient inference.” arXiv preprint arXiv:2005.03842.\n[16] Weng, L. (2023). “Large Transformer Model Inference Optimization.” Lil’Log blog post.\n[17] Shen, Z., Dong, Z., Ye, J., et al. (2020). “Q-BERT: Hessian-based ultra-low-precision quantization of BERT.” arXiv preprint arXiv:1909.05840.\n[18] Dong, Z., Yao, Z., Gholami, A., et al. (2019). “HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision” arXiv preprint arXiv:1905.03696.\n[19] Yao, Z., et al. (2022). “ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers.” arXiv preprint arXiv:2206.01861.\n[20] Frantar, E., et al. (2022). “GPTQ: Accurate post-training quantization for generative pre-trained transformers.” arXiv preprint arXiv:2210.17323.\n[21] Xiao, G., \u0026amp; Lin, J. (2022). “SmoothQuant: Accurate and efficient post-training quantization for large language models.” arXiv preprint arXiv:2211.10438.\n[22] Frankle, J., \u0026amp; Carbin, M. (2019). “The lottery ticket hypothesis: Finding sparse, trainable neural networks.” arXiv preprint arXiv:1803.03635.\n[23] Gale, T., Elsen, E., \u0026amp; Hooker, S. (2019). “The state of sparsity in deep neural networks.” arXiv preprint arXiv:1902.09574.\n[24] Molchanov, D., Ashukha, A., \u0026amp; Vetrov, D. (2017). “Variational dropout sparsifies deep neural networks.” arXiv preprint arXiv:1701.05369.\n[25] Louizos, Christos, Max Welling, and Diederik P. Kingma. \u0026ldquo;Learning sparse neural networks through $ L_0 $ regularization.\u0026rdquo; arXiv preprint arXiv:1712.01312 (2017).\n[26] Zhu, M., \u0026amp; Gupta, S. (2017). “To prune, or not to prune: Exploring the efficacy of pruning for model compression.” arXiv preprint arXiv:1710.01878.\n[27] Renda, A., Frankle, J., \u0026amp; Carbin, M. (2020). “Comparing rewinding and fine-tuning in neural network pruning.” arXiv preprint arXiv:2003.02389.\n[28] Nvidia. (2020). “NVIDIA A100 Tensor Core GPU.” Nvidia Blog.\n[29] Zhou, A., \u0026amp; Ma, X. (2021). “Learning N:M fine-grained structured sparse neural networks from scratch.” arXiv preprint arXiv:2102.04010.\n[30] Pool, J., \u0026amp; Yu, F. (2021). “Channel permutations for N:M structured sparsity.” Advances in Neural Information Processing Systems 34.\n[31] Bengio, Y., Léonard, N., \u0026amp; Courville, A. (2013). “Estimating or propagating gradients through stochastic neurons for conditional computation.” arXiv preprint arXiv:1308.3432.\n[32] Jayakumar, S. M., Pascanu, R., Rae, J., et al. (2021). “Top-KAST: Top-K always sparse training.” arXiv preprint arXiv:2106.03517.\n[33] Jaszczur, S., et al. (2021). “Sparse is enough in scaling transformers.” Advances in Neural Information Processing Systems 34.\n[34] Kitaev, N., Kaiser, Ł., \u0026amp; Levskaya, A. (2020). “Reformer: The efficient transformer.” arXiv preprint arXiv:2001.04451.\n[35] Fedus, W., et al. (2022). “A review of sparse expert models in deep learning.” arXiv preprint arXiv:2209.01667.\n[36] Riquelme, C., et al. (2021). “Scaling vision with sparse mixture of experts.” Advances in Neural Information Processing Systems 34: 8583-8595.\n[37] Kudugunta, S., Lepikhin, D., Heafield, K., et al. (2021). “Beyond domain adaptation: Multi-task mixture-of-experts for zero-shot generalization.” arXiv preprint arXiv:2110.03742.\n[38] Rajbhandari, S., et al. (2022). “DeepSpeed-MoE: Advancing mixture-of-experts inference and training to power next-generation AI scale.” arXiv preprint arXiv:2201.05596.\n[39] Hwang, I., et al. (2022). “Tutel: Adaptive mixture-of-experts at scale.” arXiv preprint arXiv:2206.03382.\n[40] Kossmann, F., et al. (2022). “Optimizing Mixture of Experts using Dynamic Recompilations” arXiv preprint arXiv:2205.01848.\n[41] Tay, Y., et al. (2020). “Efficient transformers: A survey.” arXiv preprint arXiv:2009.06732.\n[42] Shazeer, N. (2019). “Fast transformer decoding: One write-head is all you need.” arXiv preprint arXiv:1911.02150.\n[43] Ainslie, J., et al. (2023). “GQA: Training generalized multi-query transformer models from multi-head checkpoints.” arXiv preprint arXiv:2305.13245.\n[44] Kwon, W., et al. (2023). “Efficient memory management for large language model serving with PagedAttention.” Proceedings of the 29th Symposium on Operating Systems Principles.\n[45] Dao, T., et al. (2022). “FlashAttention: Fast and memory-efficient exact attention with IO-awareness.” Advances in Neural Information Processing Systems 35: 16344-16359.\n[46] Dao, T. (2023). “FlashAttention-2: Faster attention with better parallelism and work partitioning.” arXiv preprint arXiv:2307.08691.\n[47] Pope, R., et al. (2022). “Efficiently scaling transformer inference.” arXiv preprint arXiv:2211.05102.\n[48] von Platen, P. (2020). “How to generate text: Using different decoding methods for language generation with Transformers.” Hugging Face Blog.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Jun 2025). 大语言模型推理. https://syhya.github.io/zh/posts/2025-06-29-llm-inference\nOr\n@article{syhya2025llminferencesurvey, title = \u0026#34;大语言模型推理\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jun\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-06-29-llm-inference\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-06-29-llm-inference/","summary":"\u003cp\u003e近年来，大语言模型 (Large Language Models, LLMs) 在自然语言处理、代码生成乃至多模态交互等领域取得了革命性的突破。然而，这些模型强大的能力背后是巨大的计算和内存开销，尤其是在推理 (Inference) 阶段。高效地部署和运行这些数十亿甚至数万亿参数的模型，已成为将 LLM 技术规模化应用到实际产品中的核心挑战。\u003c/p\u003e","title":"大语言模型推理"},{"content":"随着大语言模型 (Large Language Models, LLMs) 参数不断增大，实际部署和提供这些模型的服务也面临挑战。vLLM 是一个开源库，旨在实现快速、便捷且经济高效的 LLM 推理和在线服务。其核心是利用 PagedAttention 算法高效地管理注意力机制中的键和值的缓存（KV Cache）。\n评价指标 为了评估 LLM 推理与服务引擎的性能，我们主要关注以下几个指标：\n首 token 生成时间 首 token 生成时间（Time To First Token, TTFT） 是指模型从接收到用户输入到生成第一个输出 token 所花费的时间。TTFT 越短，用户等待响应的时间就越少，这对于实时交互场景尤为重要；而在离线场景中，TTFT 的重要性相对较低。\n每个输出 token 的生成时间 每个输出 token 的生成时间（Time Per Output Token, TPOT） 指模型平均生成一个新 token 所需的时间，它直接决定了用户感知到的响应“速度”。为提升体验，实际应用中通常采用 Streaming 方式。例如，如果 TPOT 为 0.1 秒/token，意味着模型每秒可生成约 10 个 token，折合每分钟约 450 个单词，已超过多数人的阅读速度。\n总体延迟 总体延迟（Latency） 指模型为用户生成完整响应所需的总时间。总体延迟可由 TTFT 和 TPOT 计算得出，公式如下：\n$$ \\text{Latency} = \\text{TTFT} + \\text{TPOT} \\times (\\text{Number of Output Tokens}) $$吞吐量 吞吐量（Throughput） 衡量模型推理服务器单位时间能为所有用户请求生成的总 token 数量（包括输入与输出 token），体现了服务器的处理效率与并发能力。具体计算公式如下：\n$$ \\text{Throughput} = \\frac{\\text{Batch Size} \\times (\\text{Number of Input Tokens} + \\text{Number of Output Tokens})}{\\text{End-to-End Latency}} $$Token 间延迟 Token 间延迟（Inter Token Latency, ITL） 表示生成连续 token 时每两个 token 间的平均时间间隔。它体现了模型在生成首个 token 后，每个后续 token 的生成速度，计算公式为：\n$$ \\text{ITL} = \\frac{\\text{End-to-End Latency} - \\text{TTFT}}{\\text{Batch Size} \\times (\\text{Number of Output Tokens} - 1)} $$这些指标反映了推理引擎的响应速度、处理效率和并发能力，是评估和优化 LLM 推理性能的重要依据。\nvLLM V0 自 2023 年 6 月首次发布以来，配备 PagedAttention 的 vLLM 显著提升了 LLM 服务的性能标杆，相较于 HuggingFace Transformers (HF) 和 Text Generation Inference (TGI) 具有显著的吞吐量优势, 且无需修改任何模型架构。\nFig. 1. Throughput comparison (single output completion) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)\n单输出推理：图中显示了 vLLM 吞吐量比 HF 高 14x-24x，比 TGI 高 2.2x-2.5x。 Fig. 2. Throughput comparison (three parallel output completions) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)\n三路并行推理：图中显示了 vLLM 吞吐量比 HF 高 8.5x-15x，比 TGI 高 3.3x-3.5x。 Batching 传统的动态批处理 (Dynamic Batching) 会等待一批请求全部完成后再处理下一批，如果某些请求提前结束，会导致 GPU 空闲，资源利用率降低。\n而 vLLM 采用的 连续批处理 (Continuous Batching) 则允许在批次执行过程中动态插入新的请求序列，一旦某个序列完成，就可以立即用新的序列替换，从而显著提高 GPU 利用率和吞吐量。\nFig. 3. Dynamic Batching vs Continuous Batching. (Image source: NYC vLLM Meetup, 2025)\nDynamic Batching: 如图左侧所示，T1-T4 时刻，S₁-S₄ 四个序列并行处理。在 T5 时刻，S₁ 和 S₃ 提前完成，但由于 S₂ 和 S₄ 仍在运行，新的序列无法立即加入，导致 GPU 部分空闲。直到 T6 时刻 S₂ 结束、T7 时刻 S₄ 结束后，新的序列才能开始。\nContinuous Batching: 如图右侧所示，T1-T4 时刻与动态批处理类似。但在 T5 时刻，S₁ 和 S₃ 完成后，新的序列 S₅ 和 S₆ 可以立即加入并开始处理，而 S₂ 和 S₄ 继续运行。当 S₂ 在 T6 结束时，S₇ 可以即时加入。这种方式使得 GPU 几乎总是满负荷运行，极大提高了效率。\nKV 缓存 LLM 服务性能的主要瓶颈在于内存管理，在自回归解码过程中，LLM 为输入序列中的每个 token 生成注意力键和值张量，这些 KV 缓存必须保留在 GPU 内存中以生成后续的 token。KV 缓存具有以下特点：\n占用空间大: 对于 LLaMA-13B 模型，单个序列的 KV 缓存可能高达 1.7 GB。 动态性: KV 缓存的大小取决于序列长度，而序列长度是高度可变且不可预测的。 管理效率低下: 现有推理框架比如 FasterTransformer, Orca (Yu et al. 2022) 通常将 KV 缓存存储在连续的内存块中。为了应对动态性，它们需要预先分配足够容纳最大可能序列长度的内存块。这导致了严重的内存浪费： 内部碎片: 预留空间远大于实际需要。 外部碎片: 不同大小的预留块导致内存空间难以有效利用。 过度预留: 为未来 token 预留的空间在当前无法被其他请求利用。 下图展示了现有推理系统中 KV 缓存管理导致的内存浪费类型：\nFig. 4. KV cache memory management in existing systems, showing reserved waste, internal fragmentation, and external fragmentation. (Image source: Kwon et al. 2023)\n以下左图展示了在 NVIDIA A100 GPU 上运行 13B 参数 LLM 的内存分布：约 65% 的内存用于静态模型权重（灰色），约 30% 的内存按需动态分配给 KV 缓存（红色），用于存储前序 token 的注意力上下文，而少量内存（黄色）则用于临时激活计算；右图则表明 vLLM 通过平滑 KV 缓存内存使用的快速增长，有效缓解了内存瓶颈，从而大幅提升了批量请求处理能力和整体服务吞吐量。\nFig. 5. Left: Memory layout for a 13B LLM on an NVIDIA A100—gray is persistent parameters, red is per-request KV cache, and yellow is temporary activation memory. Right: vLLM limits rapid KV cache growth, improving throughput. (Image source: Kwon et al. 2023)\nPagedAttention PagedAttention (Kwon et al. 2023) 的灵感来源于操作系统中的虚拟内存 (Virtual Memory) 和 分页 (Paging)。它允许将逻辑上连续的 KV Cache 存储在物理上非连续的显存空间中。\n具体来说，PagedAttention 将每个序列的 KV Cache 分割成固定大小的 块 (Blocks)。每个块包含固定数量 token 的 Key 和 Value 向量。系统维护一个 块表 (Block Table)，用于记录每个序列的逻辑块到物理块的映射关系。\nFig. 6. Illustration of the PagedAttention algorithm, where KV vectors are stored in non-contiguous blocks. (Image source: Kwon et al. 2023)\nPagedAttention 的核心思想借鉴了操作系统的虚拟内存和分页机制来管理 KV 缓存。\n具体来说，PagedAttention 的设计理念可以总结为以下几点：\n类比关系:\nKV 缓存的 块 (Blocks) 类比于操作系统内存管理的 页 (Pages)。 Token 类比于 字节 (Bytes)。 序列 (Sequences) 类比于 进程 (Processes)。 映射机制: PagedAttention 使用 块表 来维护从序列的连续 逻辑块 到 物理块 的映射。这些物理块在内存中可以是非连续的，就像操作系统的页表将虚拟地址映射到物理页帧一样。\n按需分配: 最关键的一点是，物理块 不是预先为整个序列最大长度分配好的，而是在需要存储新的 Key-Value（即生成新 Token）时 按需分配。\n这种按需、非连续的内存管理方式，使得 PagedAttention 能更有效地利用内存，避免了因预分配大量连续空间而造成的浪费和内部碎片，从而提高了 GPU 内存的利用率。\n数学上，PagedAttention 将注意力计算转化为块级计算。设块大小为 $B$，第 $j$ 个 Key 块为 $K_{j}=\\left(k_{(j-1) B+1}, \\ldots, k_{j B}\\right)$，Value 块为 $V_{j}=\\left(v_{(j-1) B+1}, \\ldots, v_{j B}\\right)$。对于查询向量 $q_i$，注意力计算变为：\n\\[ A_{i j}=\\frac{\\exp \\left(q_{i}^{\\top} K_{j} / \\sqrt{d}\\right)}{\\sum_{t=1}^{\\lceil i / B\\rceil} \\exp \\left(q_{i}^{\\top} K_{t} \\mathbf{1} / \\sqrt{d}\\right)}, \\quad o_{i}=\\sum_{j=1}^{\\lceil i / B\\rceil} V_{j} A_{i j}^{\\top} \\]其中 $A_{i j}=\\left(a_{i,(j-1) B+1}, \\ldots, a_{i, j B}\\right)$ 是第 $i$ 个查询对第 $j$ 个 KV 块的注意力得分行向量。在计算过程中，PagedAttention 内核会高效地识别并获取所需的物理块。\nKV 缓存管理器 vLLM 的内存管理器借鉴了操作系统的虚拟内存机制：\n逻辑块与物理块: 每个请求的 KV 缓存被表示为一系列逻辑块。GPU 工作节点上的块引擎分配物理内存并将其划分为物理块。 块表: 维护每个请求的逻辑块到物理块的映射。每个条目记录物理块地址和块内已填充的 token 数量。 动态分配: 物理块按需分配，无需预先保留最大长度的空间，从而消除了大部分内存浪费。 Fig. 7. Block table translation in vLLM. Logical blocks are mapped to non-contiguous physical blocks. (Image source: Kwon et al. 2023)\n结合上图7的例子：\n预填充阶段: 输入 prompt 有 7 个 token。假设块大小为 4。vLLM 分配 2 个物理块（例如物理块 7 和 1）并更新块表，将逻辑块 0 映射到物理块 7，逻辑块 1 映射到物理块 1。计算 prompt 的 KV 缓存并填充到这两个物理块中。逻辑块 0 填满 4 个 token，逻辑块 1 填充 3 个 token，剩余 1 个 slot 备用。 解码阶段 : 第 1 步: 使用 PagedAttention 计算下一个 token。由于逻辑块 1 还有空位，新的 KV 缓存直接存入物理块 1，并更新块表中逻辑块 1 的填充计数。 第 2 步: 逻辑块 1 已满。vLLM 分配一个新的物理块（例如物理块 3），更新块表将新的逻辑块 2 映射到物理块 3，并将新生成的 KV 缓存存入物理块 3。 这种按需分配的方式将内存浪费限制在每个序列的最后一个块内，实现了接近最优的内存利用率（浪费低于 4%），从而可以批处理更多请求，提高吞吐量。\n图8中展示了 vLLM 如何管理两个序列的内存空间。两个序列的逻辑块被映射到 GPU 工作节点（GPU worker）上由区块引擎预留的不同物理块中。这意味着，即使在逻辑层面相邻的块在物理 GPU 内存中也无需连续，从而两个序列可以有效地共享和利用物理内存空间。\nFig. 8. Storing the KV cache of two requests concurrently in vLLM using paged memory. (Image source: Kwon et al. 2023)\n内存共享 PagedAttention 的另一个关键优势是高效的内存共享，尤其适用于复杂的解码策略。\n并行采样 当一个请求需要从同一个 prompt 生成多个输出序列时（例如代码补全建议），prompt 部分的 KV 缓存可以共享。\nFig. 9. Parallel sampling example. Logical blocks for the shared prompt map to the same physical blocks. (Image source: Kwon et al. 2023)\nvLLM 通过块表实现共享：\n共享映射: 不同序列的逻辑块可以映射到同一个物理块。 引用计数: 每个物理块维护一个引用计数。 写时复制 (Copy-on-Write, CoW): 当一个共享块（引用计数 \u0026gt; 1）需要被写入时，vLLM 会分配一个新物理块，复制原块内容，更新写入序列的块表映射，并将原物理块的引用计数减 1。后续对该物理块的写入（当引用计数为 1 时）则直接进行。 这种机制显著减少了并行采样 (Parallel Sampling) 的内存开销，论文实验显示可节省高达 55% 的内存。\n束搜索 束搜索 (Beam Search) 在解码过程中，不同的候选序列（beam）不仅共享 prompt 部分，还可能共享后续生成的 token 的 KV 缓存，且共享模式是动态变化的。\nFig. 10. Beam search example ($k=4$). Blocks are dynamically shared and freed based on candidate survival. (Image source: Kwon et al. 2023)\nvLLM 通过引用计数和 CoW 机制，高效地管理这种动态共享，避免了传统实现中频繁且昂贵的内存拷贝操作。大部分块可以共享，只有当新生成的 token 落入旧的共享块时才需要 CoW（仅拷贝一个块）。\n共享前缀 对于许多 prompt 共享相同前缀（如系统指令、few-shot 示例）的应用场景，vLLM 可以预先计算并缓存这些 共享前缀 (Shared Prefix) 的 KV 缓存到一组物理块中。\nFig. 11. Shared prompt example for machine translation using few-shot examples. (Image source: Kwon et al. 2023)\n当处理包含该前缀的请求时，只需将其逻辑块映射到缓存的物理块（最后一个块标记为 CoW），从而避免了对前缀部分的重复计算。\n调度与抢占 vLLM 采用 FCFS 调度策略。当 GPU 内存不足以容纳新生成的 KV 缓存时，需要进行抢占：\n抢占单位: 以序列组 为单位进行抢占（例如，一个 beam search 请求的所有候选序列）。确保最早到达的请求优先服务，最晚到达的请求优先被抢占。 恢复机制: 换出: 将被抢占序列的 KV 块拷贝到 CPU 内存。当资源可用时再换回 GPU。适用于 PCIe 带宽较高且块较大的情况。 重计算: 丢弃被抢占序列的 KV 缓存。当资源可用时，将原始 prompt 和已生成的 token 拼接起来，通过一次高效的 prompt phase 重新计算 KV 缓存。适用于 PCIe 带宽较低或块较小的情况。 分布式执行 vLLM 支持 Megatron-LM 风格的张量模型并行。\nFig. 12. vLLM system overview showing centralized scheduler and distributed workers. (Image source: Kwon et al. 2023)\n集中式调度器: 包含 KV 缓存管理器，维护全局的逻辑块到物理块的映射。 共享映射: 所有 GPU worker 共享块表。 本地存储: 每个 worker 只存储其负责的注意力头对应的 KV 缓存部分。 执行流程: 调度器广播输入 token ID 和块表给所有 worker -\u0026gt; worker 执行模型计算（包括 PagedAttention）-\u0026gt; worker 间通过 All-Reduce 同步中间结果 -\u0026gt; worker 将采样结果返回给调度器。内存管理信息在每步开始时一次性广播，无需 worker 间同步。 内核优化 为了高效实现 PagedAttention，vLLM 开发了定制 CUDA 核：\n融合 Reshape 和块写入: 将新 KV 缓存分块、重塑布局、按块表写入融合为单个核。 融合块读取和注意力计算: 修改 FasterTransformer 的注意力核，使其能根据块表读取非连续块并即时计算注意力，优化内存访问模式。 融合块拷贝: 将 CoW 触发的多个小块拷贝操作批量化到单个核中执行。 vLLM V1 2025 年 1 月，vLLM 团队发布了 vLLM V1 的 alpha 版本，这是对其核心架构的一次重大升级。基于过去一年半的开发经验，V1 版本重新审视了关键设计决策，整合了各种特性，并简化了代码库。\n基于 vLLM V0 的成功和经验教训，vLLM V1 对核心架构进行了重大升级，旨在提供更简洁、模块化、易于扩展且性能更高的代码库。\nV1 的动机与目标 V0 的挑战: 随着功能和硬件支持的扩展，V0 的代码复杂度增加，特性难以有效组合，技术债务累积。 V1 的目标: 简洁、模块化、易于修改的代码库。 接近零 CPU 开销的高性能。 将关键优化统一到架构中。 默认启用优化，实现零配置。 优化的执行循环与 API 服务器 Fig. 13. vLLM V1\u0026rsquo;s multiprocessing architecture with an isolated EngineCore. (Image source: vLLM Blog, 2025)\n随着 GPU 计算速度加快（例如 H100 上 Llama-8B 推理时间仅约 5ms），CPU 开销（API 服务、调度、输入准备、解码、流式响应）成为瓶颈。V1 采用了多进程架构：\n隔离的 EngineCore: 将调度器和模型执行器隔离在核心引擎循环中。 CPU 任务卸载: 将 Tokenization、多模态输入处理、Detokenization、流式传输等 CPU 密集型任务移至独立进程，与 EngineCore 并行执行，最大化模型吞吐量。 简洁灵活的调度器 Fig. 14. vLLM V1\u0026rsquo;s scheduler treats prompt and generated tokens uniformly, enabling features like chunked prefill. (Image source: vLLM Blog, 2025)\n统一处理: 不再区分 \u0026ldquo;prefill\u0026rdquo; 和 \u0026ldquo;decode\u0026rdquo; 阶段，统一处理用户输入 token 和模型生成 token。 简单表示: 调度决策用字典表示，如 {request_id: num_tokens}，指定每步为每个请求处理多少 token。 通用性: 这种表示足以支持块状预填充 (Chunked Prefills)、前缀缓存 (Prefix Caching)、投机解码 (Speculative Decoding) 等特性。例如，块状预填充只需在固定 token 预算下动态分配各请求的处理数量。 零开销前缀缓存 Fig. 15. Performance comparison of prefix caching in vLLM V0 and V1. V1 achieves near-zero overhead even at 0% hit rate. (Image source: vLLM Blog, 2025)\nV1 优化了前缀缓存（基于哈希匹配和 LRU 驱逐）的实现：\n优化数据结构: 实现常数时间缓存驱逐。 减少 Python 对象开销: 最小化对象创建。 结果: 即使缓存命中率为 0%，性能下降也小于 1%。而在高命中率时，性能提升数倍。因此，V1 默认启用前缀缓存。 清晰的张量并行推理架构 Fig. 16. vLLM V1\u0026rsquo;s symmetric tensor-parallel architecture using diff-based updates. (Image source: vLLM Blog, 2025)\nV1 解决了 V0 中调度器和 Worker 0 耦合导致的非对称架构问题：\nWorker 端状态缓存: 请求状态缓存在 Worker 端。 增量更新: 每步只传输状态的增量变化 (diffs)，极大减少了进程间通信。 对称架构: 调度器和 Worker 0 可以运行在不同进程中，架构更清晰、对称。 抽象分布式逻辑: Worker 在单 GPU 和多 GPU 设置下行为一致。 高效的输入准备 Fig. 17. vLLM V1 uses Persistent Batch to cache input tensors and apply diffs. (Image source: vLLM Blog, 2025)\nV0 每步重新创建模型输入张量和元数据，CPU 开销大。V1 采用 Persistent Batch 技术：\n缓存输入张量: 缓存输入张量。 应用 Diffs: 每步只应用增量变化。 Numpy 优化: 大量使用 Numpy 操作替代 Python 原生操作，减少更新张量的 CPU 开销。 综合优化 torch.compile 与分段 CUDA 图\n集成 torch.compile: V1 充分利用 vLLM 的 torch.compile 集成功能，自动优化模型，支持多种模型的高效运行，显著减少了手动编写 CUDA 核的需求。 分段 CUDA 图 (Piecewise CUDA Graphs): 通过引入分段 CUDA 图，成功克服了原生 CUDA 图的局限性，提高了模型的灵活性和性能。 增强的多模态 LLM 支持\nV1 针对多模态大语言模型 (MLLMs) 推出多项关键改进： 优化预处理: 将图像解码、裁剪、转换等 CPU 密集型的预处理任务移至非阻塞的独立进程，防止阻塞 GPU 工作。同时引入预处理缓存，以便缓存已处理的输入，供之后的请求复用，尤其适用于相同的多模态输入。 多模态前缀缓存: 除了使用 token ID 的哈希，V1 还引入图像哈希来标识包含图像输入的 KV 缓存。此改进在包含图像输入的多轮对话场景中尤其有利。 编码器缓存: 针对需要视觉编码器输出的应用，V1 临时缓存视觉嵌入，允许调度器将文本输入分块处理，避免在每一步都重新计算视觉嵌入，从而支持 MLLM 的块状填充调度。 FlashAttention 3 集成\n由于 V1 的高度动态性（如在同一批处理内结合预填充和解码），需要一种灵活且高性能的注意力核。FlashAttention 3 完美符合这一需求，提供了强大的功能支持，同时在各种使用场景中保持优异的性能表现。 性能对比 得益于架构改进和 CPU 开销的大幅降低，V1 相比 V0（未开启多步调度）实现了高达 1.7 倍的吞吐量提升，同时在多模态模型上的性能提升显著。\nFig. 18. Performance comparison between vLLM V0 and V1 on Llama 3.1 8B \u0026amp; Llama 3.3 70B (1xH100). (Image source: vLLM Blog, 2025)\nFig. 19. Performance comparison between vLLM V0 and V1 on Qwen2-VL 7B (1xH100). (Image source: vLLM Blog, 2025)\n表格对比:\n特性 vLLM V0 vLLM V1 改进点 核心技术 PagedAttention PagedAttention + 全面架构重构 保持 PagedAttention 优势，优化整体架构 内存效率 极高 (浪费 \u0026lt; 4%) 极高 (浪费 \u0026lt; 4%) 维持高内存效率 内存共享 支持 (CoW) 支持 (CoW) 维持高效共享 CPU 开销 相对较高，尤其在复杂场景或低命中率前缀缓存时 显著降低，接近零开销 多进程、Persistent Batch、优化数据结构等 执行循环 单进程，API 服务器与引擎耦合较紧 多进程，API 服务器与 EngineCore 解耦，高度并行 提升 CPU/GPU 并行度，减少阻塞 调度器 区分 Prefill/Decode 统一处理 Token，字典式调度表示 更简洁、灵活，易于支持高级特性 前缀缓存 默认禁用 (低命中率时有开销) 默认启用 (零开销设计) 优化后无惧低命中率，默认开启提升易用性 张量并行 不对称架构 (Scheduler+Worker0 同进程) 对称架构 (Scheduler 与 Worker 分离) 架构更清晰，IPC 开销通过状态缓存和 Diffs 传输控制 多模态支持 基本支持 增强支持 (非阻塞预处理、图像前缀缓存、编码器缓存等) 提升 VLM 性能和易用性 编译器集成 有限 集成 torch.compile 自动化模型优化，减少手写 Kernel Attention Kernel 定制 Kernel (基于 FasterTransformer) 集成 FlashAttention 3 拥抱业界标准，获得更好的性能和特性支持 性能 (vs V0) 基线 吞吐量提升高达 1.7x (文本), 多模态模型提升更显著 全面优化 CPU 开销带来的提升 代码复杂度 随功能增加而提高 更简洁、模块化 降低维护成本，方便社区贡献和二次开发 其他推理框架 LightLLM：一个基于 Python 的轻量级推理与服务框架，以轻量级设计、可扩展性和高速性能著称，汲取了 vLLM 等其他开源项目的优势。 LMDeploy：用于压缩、部署和服务 LLM 的工具包，内置 TurboMind 推理引擎，强调高请求吞吐量和高效量化。 SGLang：通过前端语言与后端执行引擎协同设计，高效执行复杂（尤其涉及结构化生成）的 LLM 程序的框架。 TGI：Hugging Face 的生产级 LLM 服务方案，广泛应用并支持多种硬件后端，借助 vLLM 的 PagedAttention 内核，提供高并发、低延迟的推理服务。 TensorRT-LLM：NVIDIA 推出的开源库，用于在 NVIDIA GPU 上优化并加速 LLM 推理，利用 TensorRT 的提前编译和深度硬件优化能力。 总结 vLLM 通过其核心技术 PagedAttention，极大地缓解了 LLM 服务中 KV 缓存管理带来的内存瓶颈，显著提高了内存利用率和吞吐量。PagedAttention 借鉴操作系统分页机制，实现了 KV 缓存的非连续存储、动态分配和高效共享（支持并行采样、束搜索、共享前缀等）。\nvLLM V1 在 V0 的基础上，对核心架构进行了全面重构和优化，通过多进程架构、灵活调度器、零开销前缀缓存、对称张量并行架构、高效输入准备、torch.compile 集成、增强 MLLMs 支持以及 FlashAttention 3 集成等一系列改进，进一步降低了 CPU 开销，提升了系统整体性能、灵活性和可扩展性，为未来快速迭代新功能奠定了坚实基础。\n参考文献 [1] Kwon, Woosuk, et al. \u0026ldquo;Efficient memory management for large language model serving with pagedattention.\u0026rdquo; Proceedings of the 29th Symposium on Operating Systems Principles. 2023.\n[2] vLLM Team. \u0026ldquo;vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention.\u0026rdquo; vLLM Blog, June 20, 2023.\n[3] vLLM Team. \u0026ldquo;vLLM V1: A Major Upgrade to vLLM\u0026rsquo;s Core Architecture.\u0026rdquo; vLLM Blog, Jan 27, 2025.\n[4] NVIDIA. \u0026ldquo;FasterTransformer.\u0026rdquo; GitHub Repository, 2023.\n[5] Yu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. \u0026ldquo;Orca: A Distributed Serving System for Transformer-Based Generative Models.\u0026rdquo; In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 2022.\n[6] OpenAI. \u0026ldquo;API Reference - Streaming.\u0026rdquo; OpenAI Platform Documentation, 2025.\n[7] Wolf, Thomas, et al. \u0026ldquo;Transformers: State-of-the-Art Natural Language Processing.\u0026rdquo; In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.\n[8] Hugging Face. \u0026ldquo;Text Generation Inference.\u0026rdquo; GitHub Repository, 2025.\n[9] Shoeybi, Mohammad, et al. \u0026ldquo;Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.\u0026rdquo; arXiv preprint arXiv:1909.08053 (2019).\n[10] InternLM Team. \u0026ldquo;LMDeploy.\u0026rdquo; GitHub Repository, 2025.\n[12] Shah, Jay, et al. \u0026ldquo;Flashattention-3: Fast and accurate attention with asynchrony and low-precision.\u0026rdquo; Advances in Neural Information Processing Systems 37 (2024): 68658-68685.\n[13] ModelTC. \u0026ldquo;LightLLM.\u0026rdquo; GitHub Repository, 2025.\n[14] Zheng, Lianmin, et al. \u0026ldquo;Sglang: Efficient execution of structured language model programs.\u0026rdquo; Advances in Neural Information Processing Systems 37 (2024): 62557-62583.\n[15] NVIDIA. \u0026ldquo;TensorRT-LLM.\u0026rdquo; GitHub Repository, 2025.\n[16] vLLM Team. \u0026ldquo;NYC vLLM Meetup Presentation.\u0026rdquo; Google Slides, 2025.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (May 2025). vLLM：高吞吐、有效内存的LLM服务引擎. https://syhya.github.io/zh/posts/2025-05-17-vllm\nOr\n@article{syhya2025vllm, title = \u0026#34;vLLM：高吞吐、有效内存的LLM服务引擎\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;May\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-05-17-vllm\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-05-17-vllm/","summary":"\u003cp\u003e随着大语言模型 (Large Language Models, LLMs) 参数不断增大，实际部署和提供这些模型的服务也面临挑战。\u003ca href=\"https://github.com/vllm-project/vllm\"\u003evLLM\u003c/a\u003e 是一个开源库，旨在实现快速、便捷且经济高效的 LLM 推理和在线服务。其核心是利用 \u003cstrong\u003ePagedAttention\u003c/strong\u003e 算法高效地管理注意力机制中的键和值的缓存（KV Cache）。\u003c/p\u003e","title":"vLLM：高吞吐、有效内存的LLM服务引擎"},{"content":"人类通过多种感官（视觉、听觉、触觉等）与世界互动，每种感官通道在表征和交流特定概念时都具有独特的优势。这种多模态交互促进了我们对世界的深刻理解。人工智能领域的核心目标之一便是开发能够有效遵循多模态指令（如视觉和语言）的通用助手，使其能够像人类一样完成现实世界的各种任务。近年来，随着 GPT-4o (OpenAI, 2024)、Gemini 2.5 Pro (DeepMind, 2025) 和 o3/o4-mini (OpenAI, 2025) 等模型的发布，多模态大语言模型（Multimodal Large Language Models, MLLMs） 取得了显著进展，它们不仅能理解图像、视频、音频等多种模态信息，还能进行复杂的推理和生成。\n符号表 下面列举了文章中使用的关键数学公式符号及其含义，以帮助你更轻松地阅读。\n符号 说明 \\( I, \\mathbf{X}_v \\) 图像输入， \\( I \\) 通常指原始图像矩阵 \\( \\in \\mathbb{R}^{H \\times W \\times C} \\) \\( T, \\mathbf{X}_c, \\mathbf{X}_q, \\mathbf{X}_a, \\mathbf{X}_{\\text{instruct}} \\) 文本输入，具体可能指图像标题(\\( \\mathbf{X}_c \\))、用户问题(\\( \\mathbf{X}_q \\))、模型回答(\\( \\mathbf{X}_a \\))或指令(\\( \\mathbf{X}_{\\text{instruct}} \\)) \\( V, \\mathbf{Z}_v \\) 图像编码器输出的原始图像特征或嵌入序列 \\( L, \\mathbf{H}_q, \\mathbf{H}_a \\) 文本编码器输出的文本特征或嵌入序列 \\( \\mathbf{H}_v \\) 经过投影层处理后，输入到 LLM 的视觉 Token 序列 \\( Z \\) Q-Former 输出的查询嵌入，作为视觉信息的压缩表示 \\( P_Z \\) 由 Q-Former 输出转换得到的软视觉提示 (Soft Visual Prompt) \\( I_e, T_e \\) 在 CLIP 的共享多模态嵌入空间中的图像和文本嵌入 \\( z_p \\) ViT 中单个图像块经过线性投射后的嵌入向量 \\( x_{class} \\) ViT 中用于分类任务的可学习 [class] 标记的嵌入 \\( x_i \\) 序列中的第 \\( i \\) 个元素或 Token (例如文本序列中的词 \\( w_i \\)) \\( E_{img}, g(\\cdot) \\) 图像编码器模型 (如 ViT) \\( E_{text}, f_{\\phi}(\\cdot) \\) 文本编码器或大语言模型 \\( E, \\mathbf{W}, \\mathbf{W}_i, \\mathbf{W}_t \\) 线性投影矩阵，用于特征转换或模态对齐 \\( E_{pos} \\) 位置编码向量，用于向 Transformer 提供序列的位置信息 \\( Q, K, V \\) 注意力机制中的 Query、Key、Value 矩阵 \\( W_Q, W_K, W_V \\) 用于从输入计算 Q, K, V 的可学习投影矩阵 \\( \\theta, \\phi \\) 模型整体或特定部分 (如 LLM \\( \\phi \\)) 的可训练参数集合 \\( P \\) ViT 模型中定义的图像块 (Patch) 的边长 \\( N \\) 批次大小 (Batch Size)，通常指一个批次中的样本数量 \\( N_{patches} \\) ViT 模型将图像分割成的图像块数量 \\( D \\) 模型中嵌入向量的主要维度 \\( d, d_k \\) 注意力机制中 Key向量的维度，用于缩放点积 \\( T_{turns} \\) 多轮对话数据中的总对话轮数 (LLaVA) \\( \\mathcal{L} \\) 损失函数，模型优化的目标 (如 \\( \\mathcal{L}_{ITC}, \\mathcal{L}_{ITM}, \\mathcal{L}_{LM}, \\mathcal{L}_{CLIP}, \\mathcal{L}_{siglip} \\)) \\( \\tau \\) 可学习参数，如对比损失中的温度或强化学习中的 KL 正则化权重 \\( \\lambda \\) 超参数，如不同损失项的权重或强化学习中的长度调节因子 \\( y \\) 目标标签或类别 (如 ITM 损失)；或模型生成的最终答案 (如 Kimi-VL RL) \\( x \\) 输入数据、上下文或问题 \\( z \\) 模型生成的中间推理步骤或思维链 \\( y^* \\) 参考答案或基准答案 (Ground Truth) $\\operatorname{sim}(u, v) = s(u, v)$ 向量 \\( u \\) 和 \\( v \\) 之间的相似度计算，通常是余弦相似度 \\( \\mathbb{E} \\) 数学期望 KL KL 散度 (Kullback–Leibler Divergence)，用于衡量两个概率分布的差异 \\( \\pi_{\\theta} \\) 策略模型，根据参数 \\( \\theta \\) 输出动作或文本序列 \\( r \\) 奖励函数，评估生成结果的好坏 多模态基础知识 在深入探讨具体技术之前，我们先来了解一些多模态的基础概念。\n什么是多模态？ 多模态 (Multimodality) 指的是使用多种不同类型的数据或信息通道（模态）来表示和处理信息。人类天生就是多模态的生物，我们通过视觉、听觉、触觉、嗅觉和味觉感知和理解世界。在人工智能领域，多模态学习为了构建能够处理和关联来自不同模态（如文本、图像、视频、音频等）信息的模型。\nFig. 1. Multimodality Data. (Image source: GPT-4o Image Generation)\n常见模态：\n文本 (Text): 自然语言文字，是信息传递和知识表达的主要方式。 图像 (Image): 静态视觉信息，包含丰富的场景、物体和纹理细节。 视频 (Video): 动态视觉信息，由连续的图像帧组成，通常伴随音频。视频不仅包含空间信息，还包含时间信息。 音频 (Audio): 声音信息，包括语音、音乐和环境声音。 其他: 表格数据、3D 点云、传感器数据（如雷达、激光雷达）、生物信号（如 EEG、ECG）等。 为什么需要多模态 AI？ 更全面的世界理解: 现实世界是多模态的。单一模态往往只能提供片面的信息。例如，仅凭文字描述可能难以完全理解一个复杂的场景，而结合图像或视频则能提供更直观、丰富的信息。多模态模型能够整合来自不同来源的信息，形成更全面、准确的理解。 增强的任务性能: 在许多任务中，结合多种模态的信息可以显著提升性能。例如，在视觉问答（VQA）中，模型需要同时理解图像内容和文本问题才能给出正确答案。在视频描述生成中，结合视觉帧和音频信息可以生成更生动、准确的描述。 更自然的交互方式: 多模态 AI 使得人机交互更加自然和灵活。用户可以通过语音、文字、图像等多种方式与 AI 系统交互，AI 系统也能以多种模态（如生成带有图片的文本回复，或生成语音回答）进行响应。 解锁新应用场景: 多模态能力催生了许多新的应用，如自动驾驶（融合摄像头、雷达、激光雷达数据）、医疗诊断（结合医学影像和病历文本）、内容创作（文生图、文生视频）、虚拟助手、机器人交互等。 促进可访问性: 多模态技术可以帮助有感官障碍的人士。例如，图像描述可以帮助视障人士理解图片内容，语音识别和合成可以帮助听障或语障人士交流。 常见多模态任务 以下表格列举了一些常见的多模态任务，这些任务通常需要结合多种模态的信息进行处理和生成。\n任务名称 说明 视觉问答 (VQA) 根据图像和相关问题生成文本答案。 图像/视频描述生成 (Image/Video Captioning) 为图像或视频生成自然语言文字描述。 文本到多模态生成 (Text-to-X Generation) 根据文本描述生成相应的图像、视频或音频内容。 跨模态检索 (Cross-Modal Retrieval) 使用一种模态（如文本）查询另一种模态（如图像）的相关数据。 多模态情感分析 (Multimodal Sentiment) 结合文本、音频、视频等多种信息判断情感倾向。 视觉推理 (Visual Reasoning) 基于图像或视频内容进行逻辑判断与关系推理。 视觉语言导航 (VLN) 根据自然语言指令在视觉环境中指导智能体导航。 多模态机器翻译 (MMT) 利用相关图像信息辅助文本翻译以消除歧义。 音视频语音识别 (AVSR) 结合音频信号和说话者唇动视觉信息进行语音识别。 视觉定位 (Visual Grounding) 将文本中的词语或短语与图像或视频中的对应区域或物体关联起来。 关键技术 多模态大模型的发展由一系列技术推动。下图直观展示了多模态理解和生成的相关技术，博主介绍其中的一些关键模型和方法。\nFig. 2. The general model architecture of MM-LLMs and the implementation choices for each component. (Image source: Zhang et al., 2024)\nVision Transformer (ViT) Vision Transformer (ViT) (Dosovitskiy et al., 2020) 将 Transformer 架构成功应用于计算机视觉领域，成为当前众多先进 MLLMs 的首选视觉编码器。\nFig. 3. ViT model overview. (Image source: Dosovitskiy et al., 2020)\n核心思想: ViT 将图像视为一系列 图像块 (Patches) 的序列，然后利用 Transformer 的自注意力机制来处理这些图像块，从而捕捉全局依赖关系。\n工作流程:\n图像分块: 将输入图像 \\( I \\in \\mathbb{R}^{H \\times W \\times C} \\) 分割成 \\( N_{patches} \\) 个固定大小的非重叠图像块 \\( x_p \\in \\mathbb{R}^{P^2 \\times C} \\)，其中 \\( (H, W) \\) 是图像分辨率，\\( C \\) 是通道数，\\( P \\) 是每个图像块的大小，\\( N_{patches} = HW/P^2 \\) 是图像块的数量。 线性投射: 将每个图像块 \\( x_p \\) 展平成一维向量，并通过一个可学习的线性投射矩阵 \\( E \\) 将其映射到 \\( D \\) 维的嵌入空间，得到图像块嵌入 \\( z_p = x_p E \\)。 位置编码: 为了保留图像块的空间位置信息，ViT 在图像块嵌入的基础上加入了可学习的位置编码 \\( E_{pos} \\)。 \\[ z_0 = [x_{class}; z_p^1; z_p^2; \\dots; z_p^{N_{patches}}] + E_{pos}, \\quad E \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}, E_{pos} \\in \\mathbb{R}^{(N_{patches}+1) \\times D} \\] 通常还会添加一个可学习的 [class] 标记嵌入 \\( x_{class} \\)，其在 Transformer 输出端的对应向量用于图像分类任务。 Transformer 编码器: 将添加了位置编码的图像块嵌入序列输入到标准的 Transformer 编码器中。编码器由多层 多头自注意力 (Multi-Head Self-Attention, MSA) 和 前馈网络 (Feed Forward Network, FFN) 组成。 MSA: 捕捉图像块之间的全局依赖关系。对于输入序列 \\( Z_{l-1} \\)，自注意力计算如下： \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] 其中 \\( Q = Z_{l-1}W_Q, K = Z_{l-1}W_K, V = Z_{l-1}W_V \\) 是查询、键、值矩阵，\\( d_k \\) 是键向量的维度。多头注意力将 \\( Q, K, V \\) 拆分成多个头并行计算注意力，然后拼接结果。 FFN: 通常由两个线性层和一个非线性激活函数（如 GELU）组成。 每一层的计算可以表示为： \\[ Z'_l = \\text{MSA}(\\text{LN}(Z_{l-1})) + Z_{l-1} \\] \\[ Z_l = \\text{FFN}(\\text{LN}(Z'_l)) + Z'_l \\] 其中 LN 表示层归一化。 输出: Transformer 编码器的输出 \\( Z_L \\) 即为图像的特征表示 \\( V \\)。 Fig. 4. Performance versus pre-training compute for different architectures: Vision Transformers, ResNets, and hybrids. Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models. (Image source: Dosovitskiy et al., 2020)\nViT 相比于传统的 CNN 具有以下优势：\n全局依赖建模 :自注意力直接连接任意两 patch，可显式捕捉长距离空间关系，比传统 CNN 更擅长整合整幅图像的语义信息。 大规模预训练迁移能力强 : 在诸如 JFT-300M、ImageNet-22K 等超大数据集上预训练后，可轻松迁移到分类、检测、分割等 20 多种下游任务，性能随模型/数据规模几乎线性提升。 架构简洁、易于扩展与并行: 纯 Transformer 堆叠便于按深度、宽度和输入分辨率三维扩展；计算由矩阵乘与 Softmax 组成，天然适配 GPU/TPU 的大批量并行和混合精度训练。 随着研究的深入，ViT 本身也在不断优化，以适应 MLLMs 的需求：\n原生动态分辨率: 传统 ViT 通常需要固定输入分辨率。Qwen2-VL 和 Kimi-VL 等模型引入了动态分辨率处理能力。它们通常去除 ViT 中的绝对位置编码，转而使用 2D 旋转位置编码来编码二维空间信息。这使得模型能够处理任意分辨率和长宽比的图像，并将其转换为变长的视觉 token 序列，更好地保留细节信息。Kimi-VL 的 MoonViT 还借鉴了 NaViT 的图像打包技术，将不同分辨率的图像块序列打包输入 Transformer，提高了训练效率。 窗口注意力: 为了降低处理高分辨率图像时自注意力机制带来的二次方计算复杂度，Qwen2.5-VL 在其 ViT 的大部分层中采用了窗口注意力。注意力计算被限制在局部窗口内，使得计算复杂度与图像块数量成线性关系，显著提升了效率，同时通过少数几层全注意力层来保持全局信息的交互。 架构对齐 LLM: Qwen2.5-VL 和 Kimi-VL 等模型还对其 ViT 架构进行了微调，使其更接近 LLM 的设计，例如使用 RMSNorm 进行归一化，使用 SwiGLU 作为激活函数，以提升计算效率和模态间的兼容性。 CLIP CLIP (Contrastive Language-Image Pre-training) (Radford et al., 2021) 是多模态领域具有里程碑意义的工作，它提出了一种简单而高效的方法来学习图像和文本之间的关联，为后续许多 MLLMs 奠定了基础。\n核心思想: CLIP 的目标是学习一个 多模态嵌入空间 (Multimodal Embedding Space)，使得在该空间中，匹配的图像和文本对具有高相似度，而不匹配的对具有低相似度。它通过 对比学习 (Contrastive Learning) 的方式，利用自然语言监督来实现这一目标。\n架构: CLIP 包含两个主要部分：\n图像编码器: 可以是 ResNet 或 ViT，负责将输入图像 \\( I \\) 编码为图像特征 \\( V \\)。 文本编码器: 通常是 Transformer，负责将输入文本 \\( T \\) 编码为文本特征 \\( L \\)。 线性投射层: 分别将图像特征 \\( V \\) 和文本特征 \\( L \\) 投射到共享的多模态嵌入空间，得到 \\( I_e = V W_i \\) 和 \\( T_e = L W_t \\)，其中 \\( W_i \\) 和 \\( W_t \\) 是可学习的投射矩阵。 Fig. 5. CLIP Architecture Overview. CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset\u0026rsquo;s classes. (Image source: Radford et al., 2021)\n训练数据: CLIP 的成功很大程度上归功于其大规模的预训练数据集 WIT (WebImageText)。研究团队从互联网上收集了 4 亿个 (图像, 文本) 对。他们通过搜索约 50 万个查询词（源自维基百科词汇、高频二元组、维基百科文章标题和 WordNet 同义词集）来构建数据集，并对每个查询词限制最多 2 万个样本以平衡数据分布。这种利用网络原生图文对的方式被称为 自然语言监督，它避免了昂贵的人工标注，使得数据规模可以轻松扩展。\n对比损失: CLIP 的核心是对比学习目标。给定一个包含 \\( N \\) 个 (图像, 文本) 对的批次 \\( \\{(I_1, T_1), \\dots, (I_N, T_N)\\} \\)，模型的目标是预测 \\( N \\times N \\) 个可能的配对中哪些是真实的配对。\n计算所有图像嵌入 \\( \\{I_{e,1}, \\dots, I_{e,N}\\} \\) 和文本嵌入 \\( \\{T_{e,1}, \\dots, T_{e,N}\\} \\)。通常会进行 L2 归一化 把每个图像或文本嵌入除以它自己的 L2 范数（Euclidean norm）。 计算所有 \\( N^2 \\) 对 \\( (I_{e,i}, T_{e,j}) \\) 之间的 余弦相似度。 \\[ \\text{logits}_{i,j} = \\text{sim}(I_{e,i}, T_{e,j}) \\cdot \\exp(\\tau) = \\frac{I_{e,i} \\cdot T_{e,j}}{\\|I_{e,i}\\| \\|T_{e,j}\\|} \\cdot \\exp(\\tau) \\] 其中 \\( \\tau \\) 是一个可学习的 温度参数，用于缩放 logits 的范围。 计算 对称交叉熵损失 (Symmetric Cross-Entropy Loss)。将问题视为两个分类任务： 对于每个图像 \\( I_i \\)，在 \\( N \\) 个文本中找到匹配的文本 \\( T_i \\)。损失为 \\( \\mathcal{L}_{\\text{image}} \\)。 对于每个文本 \\( T_j \\)，在 \\( N \\) 个图像中找到匹配的图像 \\( I_j \\)。损失为 \\( \\mathcal{L}_{\\text{text}} \\)。 总损失为： \\[ \\mathcal{L}_{CLIP} = \\frac{1}{2} (\\mathcal{L}_{\\text{image}} + \\mathcal{L}_{\\text{text}}) \\] 其中， \\[ \\mathcal{L}_{\\text{image}} = -\\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(\\text{sim}(I_{e,i}, T_{e,i}) / \\tau)}{\\sum_{j=1}^N \\exp(\\text{sim}(I_{e,i}, T_{e,j}) / \\tau)} \\] \\[ \\mathcal{L}_{\\text{text}} = -\\frac{1}{N} \\sum_{j=1}^N \\log \\frac{\\exp(\\text{sim}(I_{e,j}, T_{e,j}) / \\tau)}{\\sum_{i=1}^N \\exp(\\text{sim}(I_{e,i}, T_{e,j}) / \\tau)} \\] 这种损失函数鼓励正样本对（匹配的图文）的相似度高于负样本对（不匹配的图文）。 CLIP Core Pseudocode # image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter (tau in text) # extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # joint multimodal embedding [n, d_e] # l2_normalize projects the embeddings onto the unit hypersphere I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # scaled pairwise cosine similarities [n, n] # The temperature parameter t scales the logits # Note: using dot product on normalized vectors is equivalent to cosine similarity logits = np.dot(I_e, T_e.T) * np.exp(t) # symmetric loss function # labels are the indices [0, 1, ..., n-1] indicating the correct pairings labels = np.arange(n) # Calculate cross-entropy loss for image-to-text classification # (Predict correct text for each image) loss_i = cross_entropy_loss(logits, labels, axis=1) # axis=1 for softmax over columns # Calculate cross-entropy loss for text-to-image classification # (Predict correct image for each text) loss_t = cross_entropy_loss(logits, labels, axis=0) # axis=0 for softmax over rows # Final loss is the average of the two losses loss = (loss_i + loss_t)/2 零样本迁移: CLIP 强大的能力在于其零样本迁移性能。对于一个新的图像分类任务，无需任何微调，CLIP 可以通过以下方式进行预测：\n获取任务的所有类别名称（例如，“猫”，“狗”）。 使用 提示词工程 (Prompt Engineering) 将类别名称构造成句子，如 \u0026ldquo;A photo of a {label}.\u0026quot;。这有助于弥合预训练数据（通常是句子）和下游任务（通常是单词标签）之间的分布差距。CLIP 论文发现使用提示模板和集成多个提示可以显著提高性能（在 ImageNet 上提升近 5%）。 使用 CLIP 的文本编码器计算每个构造句子的文本嵌入，这些嵌入构成了零样本分类器的 权重向量。 对于一张新的待分类图像，使用 CLIP 的图像编码器计算其图像嵌入。 计算该图像嵌入与所有类别文本嵌入之间的余弦相似度。 将相似度最高的类别作为预测结果。 Fig. 6. Prompt engineering and ensembling improve zero-shot performance. Compared to the baseline of using contextless class names, prompt engineering and ensembling boost zero-shot classification performance by almost 5 points on average across 36 datasets. This improvement is similar to the gain from using 4 times more compute with the baseline zero-shot method but is \u0026lsquo;free\u0026rsquo; when amortized over many predictions. (Image source: Radford et al., 2021)\nCLIP 的影响: CLIP 证明了通过大规模自然语言监督和对比学习可以学到强大的、可迁移的视觉表示。其学习到的多模态嵌入空间和强大的图像编码器被广泛应用于后续的 MLLMs（如 Flamingo, BLIP-2, LLaVA）以及文生图模型（如 DALL-E 2, Stable Diffusion）中。\nCLIP 主要关注学习对齐的表示，但在生成任务上能力有限。后续工作开始探索能够同时进行理解和生成的统一模型架构。\nBLIP BLIP (Bootstrapping Language-Image Pre-training) (Li et al., 2022) 为了解决现有视觉语言预训练（Vision-Language Pre-training, VLP） 方法在模型和数据方面的局限性：模型常只能擅长理解或生成之一；数据则依赖于海量且噪声较大的网络图文对。\nBLIP 提出了多模态编码器-解码器（Multimodal Encoder-Decoder, MED） 架构，旨在统一理解和生成任务。它结合了 CLIP 的对比学习和自回归生成的优点，能够处理多种模态数据。\nFig. 7. BLIP Pre-training Model Architecture and Objectives (same parameters have the same color). We propose multimodal mixture of encoder-decoder (MED), a unified vision-language model which can operate in one of the three functionalities. (Image source: Li et al., 2022)\n图像编码器: 采用 ViT。 文本编码器/解码器: 基于 BERT 架构，但进行了修改以适应多模态任务和不同功能模式。 单模态编码器: 标准的 ViT 和 BERT，分别处理图像和文本。 基于图像的文本生成编码器: 在文本编码器的每个 Transformer 块的自注意力 (SA) 层和前馈网络 (FFN) 层之间插入 交叉注意力 (Cross-Attention, CA) 层，用于注入视觉信息。文本输入前会添加 [Encode] 标记，其输出嵌入作为图文对的多模态表示。 基于图像的文本生成解码器: 将编码器中的双向 SA 层替换为 因果自注意力 (Causal Self-Attention) 层，以实现自回归生成。共享编码器的 CA 层和 FFN 层。使用 [Decode] 标记作为序列开始符。 预训练目标: BLIP 联合优化三个目标：\n图文对比(Image-Text Contrastive, ITC)损失: 类似于 CLIP，使用单模态编码器对齐图像和文本的特征空间。BLIP 采用了ALBEF(Li et al., 2021)提出的动量编码器 (Momentum Encoder) 和软标签策略来改进对比学习。 $$L_{ITC} = \\frac{1}{2N} \\sum_{i=1}^{N} \\left( -\\log \\frac{\\exp(s(v_i, t_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(s(v_i, t_j)/\\tau)} -\\log \\frac{\\exp(s(v_i, t_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(s(v_j, t_i)/\\tau)} \\right)$$ 其中 $v_i, t_j$ 为图文特征，$s$ 为相似度函数，$\\tau$ 为温度参数。\n图文匹配(Image-Text Matching, ITM)损失: 使用图像接地文本编码器学习细粒度的图文对齐。这是一个二分类任务，预测图文对是匹配还是不匹配。采用难负例挖掘策略。 $$L_{ITM} = -\\mathbb{E}_{(I,T)\\sim D} [y \\log p_{match} + (1-y) \\log(1 - p_{match})]$$ 其中 $y$ 是标签，$p_{match}$ 是匹配概率。\n语言模型(Language Modeling, LM)损失: 使用图像接地文本解码器，根据图像生成文本描述。采用标准的交叉熵损失（带标签平滑）。\n$$L_{L M}=-\\mathbb{E}_{(I, T) \\sim D} \\sum_{k=1}^L \\log P\\left(w_k \\mid I, w_{\\lt k} ; \\theta\\right)$$ 其中 $w_k$ 是文本序列中的词，$\\theta$ 是模型参数。\n总损失函数: 这三个损失通常被联合优化（例如，等权重相加）： $$L_{BLIP} = L_{ITC} + L_{ITM} + L_{LM}$$参数共享: 为了效率和多任务学习的好处，文本编码器和解码器共享除 SA 层外的所有参数（嵌入层、CA 层、FFN 层）。\nCapFilt (Captioning and Filtering) 是一种创新的数据集引导方法，用于从未标注的网络图像中生成高质量的合成标题，并过滤掉噪声数据（包括原始网络文本和合成文本）。\nFig. 8. BLIP Learning Framework. We introduce a captioner to produce synthetic captions for web images, and a filter to remove noisy image-text pairs. (Image source: Li et al., 2022)\n初始化: 使用预训练好的 MED 模型初始化两个模块：Captioner（图像接地文本解码器）和 Filter（图像接地文本编码器）。 微调: 在高质量的人工标注数据集（如 COCO）上分别微调 Captioner (使用 LM 损失) 和 Filter (使用 ITC 和 ITM 损失)。这是一个轻量级过程。 生成与过滤: Captioner 为网络图像 \\( I_w \\) 生成合成标题 \\( T_s \\)。 Filter 判断原始网络文本 \\( T_w \\) 和合成文本 \\( T_s \\) 是否与图像 \\( I_w \\) 匹配。预测为不匹配的文本被视为噪声并移除。 引导数据集: 将过滤后的高质量图文对（来自原始网络数据和合成数据）与人工标注数据结合，形成新的引导数据集。 重新预训练: 使用引导数据集从头预训练一个新的 BLIP 模型。 效果: CapFilt 显著提升了模型在各项下游任务（如检索、描述生成、VQA）上的性能，证明了通过引导方式改善噪声数据质量的有效性。BLIP 也展示了统一模型在理解和生成任务上的灵活性。\nBLIP-2 BLIP-2 (Li et al., 2023) 针对高昂 VLP 训练成本，提出高效预训练策略：冻结预训练图像编码器与大语言模型，只训练轻量桥接模块 Q‑Former。\n核心贡献:\n利用冻结模型: 无需端到端训练整个大型模型，显著降低了计算成本，并利用了强大的预训练单模态模型的能力。 Q-Former (Querying Transformer): 提出了一种轻量级的 Transformer 结构作为可训练的桥梁，连接冻结的图像编码器和冻结的 LLM。 两阶段预训练: 设计了两阶段策略来有效弥合模态鸿沟： 阶段一：视觉-语言表示学习 (Vision-Language Representation Learning): 从冻结的图像编码器引导学习。 阶段二：视觉到语言生成学习 (Vision-to-Language Generative Learning): 从冻结的 LLM 引导学习。 架构 (Q-Former):\nQ-Former 是一个轻量级 Transformer，包含 188M 参数。 它使用一组 可学习的查询向量（例如 32 个 768 维向量）作为输入。 这些查询向量通过 自注意力层 相互交互。 通过 交叉注意力层 与冻结的图像编码器输出的图像特征进行交互，提取视觉信息。 查询向量的输出 \\( Z \\) (例如 \\( 32 \\times 768 \\) 维) 维度远小于原始图像特征，充当了信息瓶颈，迫使 Q-Former 提取对语言模型最有用的视觉信息。 Q-Former 内部包含图像 Transformer 和文本 Transformer 两个共享自注意力层的子模块。 Fig. 9. (Left) Model architecture of Q-Former and BLIP-2\u0026rsquo;s first-stage vision-language representation learning objectives. (Right) The self-attention masking strategy for each objective to control query-text interaction. (Image source: Li et al., 2023)\n两阶段预训练:\n阶段一 (表示学习):\n将 Q-Former 连接到 冻结的图像编码器 (如 CLIP ViT-L/14, EVA-CLIP ViT-g/14)。 使用图文对进行预训练，目标是让 Q-Former 的查询向量学会提取与文本最相关的视觉表示。 联合优化三个与 BLIP 类似的目标 (共享输入格式和模型参数，但冻结图像编码器，只训练 Q-Former)： 图文对比(Image-Text Contrastive, ITC)损失: 对齐 Q-Former 输出的查询表示 \\( z \\) 和文本表示 \\( t \\)。使用 In-batch Negatives。 $$L_{ITC} = \\frac{1}{2N} \\sum_{i=1}^{N} \\left( -\\log \\frac{\\exp(s(z_i, t_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(s(z_i, t_j)/\\tau)} -\\log \\frac{\\exp(s(z_i, t_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(s(z_j, t_i)/\\tau)} \\right)$$ 图文匹配(Image-Text Matching, ITM)损失: 预测图文对是否匹配。使用 Q-Former 输出的多模态查询表示进行分类。 $$L_{ITM} = -\\mathbb{E}_{(I,T)\\sim D} [y \\log p_{match} + (1-y) \\log(1 - p_{match})]$$ 基于图像的文本生成(Image-grounded Text Generation, ITG)损失: 训练 Q-Former 生成文本。查询向量需要捕获所有生成文本所需的信息，并通过自注意力层传递给文本 token。 $$L_{ITG} = -\\mathbb{E}_{(I,T)\\sim D} \\sum_{k=1}^{L} \\log P(w_k | Z_q, w_{\\lt k}; \\theta_{Q-Former})$$ 其中 $Z_q$ 是 Q-Former 的查询输出。 通过不同的自注意力掩码控制查询-文本交互来实现不同目标。 第一阶段总损失函数: $$L_{Stage1} = L_{ITC} + L_{ITM} + L_{ITG}$$ 阶段二 (生成学习):\n将 第一阶段预训练好的 Q-Former (及其连接的冻结图像编码器) 连接到 冻结的 LLM (如 OPT 系列, FlanT5 系列)。 使用一个 全连接层 将 Q-Former 的输出查询嵌入 \\( Z \\) 线性投射到与 LLM 文本嵌入相同的维度，得到软视觉提示 $P_Z$。 将投射后的查询嵌入作为 软视觉提示 (Soft Visual Prompts)，添加到 LLM 输入文本嵌入的前面。 训练目标: 训练 Q-Former (FC 层也训练)，使其输出的视觉表示能够被冻结的 LLM 理解并用于生成文本。 对于 Decoder-only LLM (如 OPT): 使用标准的语言建模损失，即根据视觉提示生成后续文本。 对于 Encoder-Decoder LLM (如 FlanT5): 使用前缀语言建模损失 (Prefix Language Modeling)，将文本分成前缀和后缀，视觉提示和前缀输入 Encoder，Decoder 生成后缀。 $$L_{Stage2} = L_{LM} = -\\mathbb{E}_{(I, T_{prompt}, T_{gen})\\sim D} \\sum_{k=1}^{M} \\log P_{LLM}(w_k | P_Z, T_{prompt}, w_{\\lt k}; \\theta_{LLM\\_frozen})$$ 其中 $\\theta_{L L M_{-} \\text {frozen }}$ 是冻结 LLM 的参数, 只用来前向传播，不参与梯度更新。 Fig. 10. BLIP-2\u0026rsquo;s second-stage vision-to-language generative pre-training, which bootstraps from frozen large language models (LLMs). (Top) Bootstrapping a decoder-based LLM (e.g. OPT). (Bottom) Bootstrapping an encoder-decoder-based LLM (e.g. FlanT5). (Image source: Li et al., 2023)\n效果与优势:\n高效: 由于只训练轻量级的 Q-Former，预训练成本远低于端到端训练的大型模型。 高性能: 在 VQA、Captioning、Retrieval 等任务上达到 SOTA 水平，甚至超越了参数量远大于它的模型（如 Flamingo）。 通用性: 可以方便地接入不同的冻结图像编码器和 LLMs，利用各自领域的最新进展。 零样本能力: 借助强大的冻结 LLM（特别是指令微调过的 FlanT5），BLIP-2 展现出令人印象深刻的零样本指令图像到文本生成能力，可以根据自然语言指令执行各种视觉语言任务（如视觉对话、视觉知识推理）。 LLaVA LLaVA (Large Language and Vision Assistant) (Liu et al., 2023) 是视觉指令微调 (Visual Instruction Tuning) 开源社区领域的重要工作，首次尝试将 NLP 领域的指令微调思想扩展到多模态领域。\n核心贡献:\n提出视觉指令微调: 探索将指令微调应用于语言-图像多模态模型，旨在构建通用的视觉助手。 GPT 辅助数据生成: 面对视觉指令数据的缺乏，创新性地使用纯语言模型 GPT-4来生成包含视觉内容的多模态语言-图像指令遵循数据。 构建 LLaVA 模型: 提出了一种连接预训练的视觉编码器 (CLIP ViT-L/14) 和大型语言模型 (LLM, Vicuna) 的端到端训练架构。 创建评估基准: 构建了 LLaVA-Bench，包含多样化和具有挑战性的任务，用于评估多模态模型的指令遵循能力。 开源贡献: 公开了 GPT-4 生成的视觉指令数据、模型代码和预训练权重，极大地推动了社区在这一方向上的研究。 GPT 辅助视觉指令数据生成:\nLLaVA 解决的关键挑战是缺乏大规模、高质量的视觉指令遵循数据。研究者提出了一种利用现有的多模态大模型如 GPT-4 基于现有的图像-文本对来生成此类数据的方法，本质上这是一种对闭源模型 GPT-4 进行知识蒸馏的过程。\n面临的挑战: 简单的将图像-标题对扩展为 (指令：描述图像，图像 -\u0026gt; 回答：标题) 的格式虽然廉价，但缺乏指令和响应的多样性及深度推理。 解决方案: 使用 GPT-4 作为“教师模型”。由于这些模型仅接受文本输入，研究者将图像内容通过符号表示 传递给它们： 图像描述: 提供图像场景的整体或多方面描述。 边界框: 提供图像中对象的类别概念及其空间位置信息 (例如 person: [0.681, 0.242, 0.774, 0.694])。 提示与上下文学习: 将图像的符号表示 (描述和边界框) 输入给 GPT-4。为了引导 GPT-4 生成特定格式和内容的输出，研究者手动设计了少量高质量的种子示例 ，利用 GPT-4 的上下文学习 能力进行 few-shot 推理。 生成三种类型数据 (基于 COCO 图像): 通过精心设计的 Prompt 引导 GPT-4 生成了三种类型的指令数据： 对话: 生成模拟人与助手之间关于图像内容的多轮对话，包含物体识别、计数、定位、动作、关系等问题。 详细描述: 根据特定指令（如“详细描述下图”）生成对图像全面、细致的描述。 复杂推理: 生成需要基于图像内容进行逻辑推理或结合背景知识的问题和答案（如“图中人物可能面临什么挑战？”）。 Fig. 11. One example to illustrate the instruction-following data. (Image source: Liu et al., 2023)\n数据集: 共收集了 158K 个独特的语言-图像指令样本，具体包括：58K 对话样本，23K 详细描述样本，77K 复杂推理样本。实验发现，GPT-4 生成的数据质量通常优于 ChatGPT。 Fig. 12. LLaVA network architecture. (Image source: Liu et al., 2023)\nLLaVA 的架构旨在有效结合预训练的视觉模型和 LLM 的能力，如上图所示。\n视觉编码器 \\( g(\\cdot) \\): 使用冻结的 CLIP ViT-L/14 模型。对于输入图像 \\( \\mathbf{X}_{\\mathrm{v}} \\)，提取其视觉特征 \\( \\mathbf{Z}_{\\mathrm{v}} = g(\\mathbf{X}_{\\mathrm{v}}) \\)。论文中提到实验考虑了最后一层 Transformer 层之前和之后的网格特征。\n投影层: 使用一个可训练的线性投影矩阵 \\( \\mathbf{W} \\) 将视觉特征 \\( \\mathbf{Z}_{\\mathrm{v}} \\) 映射到语言模型的词嵌入空间。\n$$ \\mathbf{H}_{\\mathrm{v}} = \\mathbf{W} \\cdot \\mathbf{Z}_{\\mathrm{v}} $$ 其中， \\( \\mathbf{H}_{\\mathrm{v}} \\) 是一系列视觉 Token，其维度与 LLM 的词嵌入维度相同。这种简单的线性投影方式轻量且高效，便于快速迭代以数据为中心的实验。更复杂的连接方式（如 Flamingo 中的门控交叉注意力或 BLIP-2 中的 Q-Former）可作为未来工作探索。\n大型语言模型 (LLM) \\( f_{\\phi}(\\cdot) \\): 使用 Vicuna，其参数表示为 \\( \\phi \\)。LLM 接收视觉 Token \\( \\mathbf{H}_{\\mathrm{v}} \\) 和文本指令 \\( \\mathbf{X}_{\\text{instruct}} \\)，并自回归地生成答案 \\( \\mathbf{X}_{\\mathrm{a}} \\)。 两阶段训练:\nLLaVA 采用两阶段指令微调流程。\n阶段一：特征对齐预训练 (Feature Alignment Pre-training):\n目标: 将视觉特征 \\( \\mathbf{H}_{\\mathrm{v}} \\) 与 LLM 的词嵌入空间对齐，可以理解为为冻结的 LLM 训练一个兼容的“视觉 Tokenizer”。 数据: 使用了 CC3M 数据集的一个经过滤的子集 (约 595K 图文对)。将这些图文对通过简单方式转换为指令数据：对于图像 \\( \\mathbf{X}_{\\mathrm{v}} \\)，随机选择一个简单的描述指令 \\( \\mathbf{X}_{\\mathrm{q}} \\) (如 \u0026ldquo;简要描述这张图片\u0026rdquo;)，并将原始标题 \\( \\mathbf{X}_{\\mathrm{c}} \\) 作为答案 \\( \\mathbf{X}_{\\mathrm{a}} \\)。这可以视为单轮对话。 训练: 冻结 视觉编码器 \\( g(\\cdot) \\) 和 LLM \\( f_{\\phi}(\\cdot) \\) 的权重，仅训练 投影层 \\( \\mathbf{W} \\)。训练目标是最大化答案（即图像标题）的似然概率。 阶段二：端到端微调 (Fine-tuning End-to-End):\n目标: 提升模型在多模态任务上的指令遵循和对话能力。 数据: 使用前述生成的 158K 视觉指令数据 (包含对话、详细描述、复杂推理三种类型，训练时均匀采样)。 训练: 冻结 视觉编码器 \\( g(\\cdot) \\)，同时训练 投影层 \\( \\mathbf{W} \\) 和 LLM \\( f_{\\phi}(\\cdot) \\) 的权重。 训练目标:\n对于每张图像 \\( \\mathbf{X}_{\\mathrm{v}} \\)，生成包含 \\( T_{turns} \\) 轮的多轮对话数据 \\( \\left(\\mathbf{X}_{\\mathrm{q}}^{1}, \\mathbf{X}_{\\mathrm{a}}^{1}, \\cdots, \\mathbf{X}_{\\mathrm{q}}^{T_{turns}}, \\mathbf{X}_{\\mathrm{a}}^{T_{turns}}\\right) \\)，其中 \\( T_{turns} \\) 是总对话轮数。我们将这些数据组织成一个序列，并将所有答案 \\( \\mathbf{X}_{\\mathrm{a}} \\) 视为模型的回应。其输入序列的组织形式采用了 Vicuna 格式。在第 \\( t \\) 轮对话中，指令 \\( \\mathbf{X}_{\\text{instruct}}^{t} \\) 定义为：\n$$ \\mathbf{X}_{\\text{instruct}}^{t} = \\left\\{ \\begin{array}{ll} \\text{Randomly choose } [\\mathbf{X}_{\\mathrm{q}}^{1}, \\mathbf{X}_{\\mathrm{v}}] \\text{or } [\\mathbf{X}_{\\mathrm{v}}, \\mathbf{X}_{\\mathrm{q}}^{1}], \u0026 \\text{ if } t=1 \\text{ (the first turn)} \\\\ \\mathbf{X}_{\\mathrm{q}}^{t}, \u0026 \\text{ if } t\u003e1 \\text{ (the remaining turns)} \\end{array} \\right. $$目标是预测答案序列 \\( \\mathbf{X}_{\\mathrm{a}} = (\\mathbf{X}_{\\mathrm{a}}^{1}, \\dots, \\mathbf{X}_{\\mathrm{a}}^{T_{turns}}) \\)。模型需要最大化在给定图像 \\( \\mathbf{X}_{\\mathrm{v}} \\) 和所有指令 \\( \\mathbf{X}_{\\text{instruct}} = (\\mathbf{X}_{\\text{instruct}}^{1}, \\dots, \\mathbf{X}_{\\text{instruct}}^{T_{turns}}) \\) 的条件下，生成正确答案序列的概率。对于长度为 \\( L_{seq} \\) 的完整答案序列（所有轮次的 \\( \\mathbf{X}_{\\mathrm{a}} \\) 拼接而成），其概率计算如下：\n$$ p\\left(\\mathbf{X}_{\\mathrm{a}} \\mid \\mathbf{X}_{\\mathrm{v}}, \\mathbf{X}_{\\text {instruct }}\\right)=\\prod_{i=1}^{L_{seq}} p_{\\boldsymbol{\\theta}}\\left(x_i \\mid \\mathbf{X}_{\\mathrm{v}}, \\mathbf{X}_{\\text {instruct },\\lt i}, \\mathbf{X}_{\\mathrm{a},\\lt i}\\right) $$其中：\n\\( \\boldsymbol{\\theta} \\) 是模型的可训练参数。 在阶段一，\\( \\boldsymbol{\\theta} = \\{ \\mathbf{W} \\} \\)。 在阶段二，\\( \\boldsymbol{\\theta} = \\{ \\mathbf{W}, \\phi \\} \\)。 \\( x_i \\) 是答案序列 \\( \\mathbf{X}_{\\mathrm{a}} \\) 中的第 \\( i \\) 个 token。 \\( \\mathbf{X}_{\\text{instruct},\\lt i} \\) 和 \\( \\mathbf{X}_{\\mathrm{a},\\lt i} \\) 分别代表在预测 \\( x_i \\) 时，模型已接收到的所有指令 token 和已生成的所有答案 token。 训练时的损失函数是上述概率的负对数似然 (Negative Log-Likelihood)，并且仅在答案部分的 token (即 \\( \\mathbf{X}_{\\mathrm{a}} \\) 中的 token) 上计算损失。 效果与影响:\nLLaVA 在多模态对话方面展示了令人印象深刻的能力，有时能在未见过的图像和指令上表现出类似多模态 GPT-4 的行为。在 ScienceQA 基准测试上进行微调后，LLaVA 与 GPT-4 的结合取得了当时最先进的 92.53% 准确率。\nFig. 13. Accuracy (%) on Science QA dataset. (Image source: Liu et al., 2023)\nLLaVA 的成功证明了视觉指令微调的有效性，其开源的数据、代码和模型极大地促进了后续多模态大模型的研究，为构建通用的、能够理解并遵循视觉和语言指令的 AI 助手开辟了新的途径。\nQwen-VL Qwen-VL (Bai et al., 2023)模型是 Qwen 团队研发的首个开源大型视觉语言模型，其架构由三大模块组成：\n大语言模型：采用预训练的 Qwen-7B 文本模型作为语言解码器。这部分负责理解和生成文本，与标准的 LLM 架构一致。\n视觉编码器：使用 Vision Transformer 提取图像特征。具体实现上，Qwen-VL 利用采用 OpenCLIP 的 ViT-bigG 模型初始化视觉编码部分。在训练和推理阶段，输入图像都会被调整为特定分辨率。视觉编码器通过以 14 的步幅将图像切分为多个图像块，从而提取出一组图像特征。\n位置感知视觉-语言适配器(Position-aware Vision-Language Adapter)：为高效融合长序列图像特征，引入了一个适配器将视觉特征序列压缩至固定长度。具体而言，该适配器包含一组随机初始化的可学习查询向量，通过单层的交叉注意力 模块与 ViT 输出的图像特征进行计算，将图像特征压缩为长度固定为256的序列。\n注意力计算公式如下：\n$$ \\text{CrossAttn}(Q, K, V) = \\mathrm{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V $$其中，\\(Q\\) 为适配器内部定义的可训练查询向量矩阵，而\\(K, V\\) 均直接使用视觉编码器（ViT）输出的图像特征序列作为键（Key）和值（Value）。\n通过这一机制，适配器能够根据学习到的查询向量从众多图像特征中选择并聚合最相关的信息。此外，为缓解图像特征压缩过程中可能引发的空间位置信息损失，在注意力计算的查询-键对中额外融入了二维绝对位置编码，强化了对图像空间结构的感知能力。\nFig. 14. The training pipeline of the Qwen-VL series. (Image source: Bai et al., 2023)\nQwen-VL 采用“三阶段” 逐步训练策略，将视觉感知能力注入通用大模型。第一阶段冻结 LLM 仅训练视觉模块，第二阶段解冻联合多任务训练，第三阶段指令微调得到聊天模型。上图中雪花 ❄ 表示冻结，火焰 🔥 表示参与训练。\n训练策略： Qwen-VL 系列采用分三阶段的逐步训练流程：\n纯图文预训练阶段：\n固定语言模型（7B）参数，仅训练视觉编码器和 VL 适配器； 使用约14亿对弱标注图文数据（英文占77.3%、中文占22.7%）； 图像统一缩放至较低分辨率（如 224×224）以提高效率； 采用自回归方式进行语言建模，训练模型生成图像描述文本； 训练约5万步（15亿样本）后，初步实现图文对齐能力（Qwen-VL）。 多任务联合训练阶段：\n解冻语言模型，与视觉部分端到端共同训练； 提升输入图像分辨率（如448×448以上）； 加入多种细粒度视觉任务（如图像描述、视觉问答、内容定位、OCR 识别等），共涉及7大类任务； 训练数据混合多来源数据集，并加入约 2480 万条 OCR 数据和 780 万条纯文本数据； 所有任务数据随机混合训练，每条样本带任务前缀并填充至 2048 序列长度； 模型显著提升图像理解、跨模态检索、定位、阅读等能力。 监督微调（SFT）阶段：\n在多模态指令数据（约35万条）上进行微调，得到对话增强版 Qwen-VL-Chat； 特别设计复杂的多图推理、细粒度定位、多轮交互任务数据； 微调期间再次冻结视觉编码器，仅微调语言模型和适配器； 最终模型表现出优异的多模态对话、指令跟随和复杂推理能力。 Qwen2-VL Qwen2-VL (Wang et al., 2024) 是 Qwen-VL 的升级版，在处理可变分辨率视觉输入和融合多模态位置信息方面取得了进展。\nFig. 15. Qwen2-VL is capable of accurately identifying and comprehending the content within images, regardless of their clarity, resolution, or extreme aspect ratios.: (Wang et al., 2024)\n从上图我们可以看出，Qwen2-VL 在处理不同分辨率和长宽比的图像时，能够准确识别和理解图像中的内容。主要采用了以下技术：\n原生动态分辨率 (Naive Dynamic Resolution): 借鉴 NaViT (Dehghani et al., 2023)，模型能够处理任意分辨率的图像，并将其动态地转换为变长的视觉 token 序列。\n移除 ViT 的绝对位置编码，引入 2D 旋转位置编码 (2D Rotary Position Embedding, 2D-RoPE) (Su et al., 2024; Su, 2021) 来编码二维空间信息。 推理时，可变分辨率图像被打包处理，限制总 token 长度以控制显存。 ViT 输出后，使用 MLP 压缩相邻 \\( 2 \\times 2 \\) 的 token 为一个，减少输入 LLM 的序列长度。使用 \u0026lt;|vision_start|\u0026gt; 和 \u0026lt;|vision_end|\u0026gt; 包裹视觉 token。 多模态旋转位置编码 (Multimodal Rotary Position Embedding, M-RoPE): 提出了一种新的位置编码方法，可以统一处理文本、图像和视频的位置信息。\n将 RoPE 分解为 时间 (Temporal)、高度 (Height)、宽度 (Width) 三个分量。 文本: 三个分量使用相同的位置 ID，等价于 1D-RoPE。 图像: 时间 ID 恒定，高度和宽度 ID 根据 token 在图像中的二维位置赋值。 视频: 时间 ID 随帧数递增，高度和宽度 ID 同图像。 多模态输入: 不同模态的位置 ID 依次递增。 优势: 统一编码多模态位置信息，降低了图像/视频的位置 ID 值，有利于推理时外插到更长序列。 Fig. 16. Illustration of M-RoPE. By decomposing rotary embedding into temporal, height, and width components, M-RoPE can explicitly model the positional information of text, images, and video in LLM. (Image source: Wang et al., 2024)\n统一图像与视频理解: 采用混合训练范式和特定架构设计（如 3D 卷积处理视频）来同时处理图像和视频。 混合图像和视频数据进行训练。 视频以 2 FPS 采样。 ViT 中集成 3D 卷积 处理视频输入 (处理 \\( 2 \\times 14 \\times 14 \\) 的 3D 块)，减少 token 数量。 图像被视为两帧相同的视频帧。 动态调整视频帧分辨率，限制每段视频的总 token 数（如 16384）。 训练: 沿用 Qwen-VL 的三阶段训练：ViT 预训练 -\u0026gt; 全模型预训练 -\u0026gt; LLM 指令微调。预训练数据包含图文对、OCR、图文交错文章、VQA、视频对话、图像知识等。指令微调使用 ChatML 格式。发布了 2B, 8B, 72B 三种规模的模型，探索了 MLLMs 的 scaling law。\n效果: Qwen2-VL 在多种分辨率和长宽比的图像理解、长视频理解（超过 20 分钟）以及视觉 Agent 能力方面表现出色。\nQwen2.5-VL Qwen2.5-VL (Bai et al., 2025) 在 Qwen2-VL 的基础上进一步优化了效率和时序建模能力。\nFig. 17. The Qwen2.5-VL framework demonstrates the integration of a vision encoder and a language model decoder to process multimodal inputs. The vision encoder is designed to handle inputs at their native resolution and supports dynamic FPS sampling. TMRoPE aligns time IDs with absolute time along the temporal dimension. (Image source: Bai et al., 2025)\n模型优化：\nQwen2.5-VL 在 Qwen2-VL 的基础上进行了多项优化，主要包括：\n高效 ViT 架构： 在 Vision Transformer 中引入窗口注意力(Window Attention) 机制，将大部分层的注意力计算限制在局部窗口（如 $8 \\times 8$ patch），使得计算复杂度随图像块数量呈线性增长，显著提升对高分辨率图像的处理效率。同时，仅在少数层（如每隔 8 层）执行全局注意力以保留整体上下文信息。\n动态 FPS 采样与视频处理： 引入动态帧率（Dynamic FPS）采样机制，将动态分辨率思想拓展至时间维度，提升模型对不同速率视频的适应能力。在视频处理上，保持3D 块结构（$2 \\times 14 \\times 14$）设计，并结合动态 FPS 和时间感知编码优化整体时序建模效果。\n更强的数据与任务能力支持： 模型在大规模（4.1T tokens）、高质量数据集上进行预训练与微调，重点提升了文档解析（表格、图表、公式、乐谱等）、对象定位（支持点和框标注）、长视频理解（小时级）以及Agent 多任务能力，拓宽了多模态理解的应用边界。\n数据增强:\n文档全解析数据: 构建了包含表格、图表、公式、图片、乐谱、化学式的 HTML 格式数据，包含布局框信息和坐标。 定位数据: 扩展了边界框和点的定位数据，覆盖超过 1 万个类别，并合成了包含不存在对象和多实例对象的难例。使用了 Grounding DINO 和 SAM 等工具合成数据。 OCR 数据: 增加了多语言 OCR 数据（覆盖欧洲主要语言及日韩阿越等），并包含手写体、密集文本、网页、公式、图表、表格等多种场景。 视频数据: 增加了长视频（超过半小时）的密集描述数据，并采用动态 FPS 采样训练。时间戳标注包含秒和 HMSF 两种格式。 Agent 数据: 收集了移动端、Web 端、桌面端的截图和操作轨迹，统一为函数调用格式，并合成了 CoT 推理过程。 效果: Qwen2.5-VL 在文档理解、细粒度定位、长视频理解和 Agent 任务上取得了 SOTA 性能，72B 版本在多个基准上媲美甚至超越 GPT-4o 和 Claude 3.5 Sonnet。\nQwen2.5-Omni Fig. 18. Qwen2.5-Omni is an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. (Image source: Qwen Team, 2025)\nQwen2.5-Omni (Qwen Team, 2025) 是一个类似于 GPT-4o(OpenAI, 2024) 的端到端多模态模型，支持处理包括文本、图像、音频和视频全模态的输入，并能同时 流式生成文本和自然语音 输出。\n从下图可以看出，Qwen2.5-Omni 采用了Thinker-Talker架构，其主要特点包括：\nFig. 19. Qwen2.5-Omni Overview. Adopts Thinker-Talker architecture. Thinker is tasked with text generation while Talker focuses on generating streaming speech tokens by receiving high-level representations directly from Thinker. (Image source: Qwen Team, 2025)\n统一多模态处理与时序建模：\n全模态感知: 单一模型能够同时处理文本、图像、音频、视频四种模态输入，实现多模态统一理解。 Fig. 20. An illustration of Time-aligned Multimodal RoPE (TMRoPE). (Image source: Qwen Team, 2025)\n时序对齐多模态旋转位置编码（Time-aligned Multimodal RoPE，TMRoPE）: 在Qwen2.5-VL基础上进一步优化 TMRoPE，通过时间交错 (Time-interleaving) 结构，将视频帧和音频帧每2秒切块后按时间顺序排列，块内先视频后音频。所有模态使用绝对时间戳（40ms粒度）与位置编码（TMRoPE）对齐，实现精准的音视频同步。\n输入处理细节: 文本使用Qwen tokenizer；音频为16kHz采样、128通道梅尔频谱图（25ms窗长，10ms步长），每帧约40ms，通过Qwen2-Audio编码器处理；图像/视频通过Qwen2.5-VL的ViT架构处理，视频支持动态FPS采样。\nThinker-Talker 架构设计与功能解耦：\n提出创新的Thinker-Talker架构，将文本生成和语音生成解耦，避免相互干扰，同时允许端到端联合训练。 Thinker: 基于Qwen2.5的Transformer解码器，处理多模态输入，生成高级隐层表示（包含语义和韵律信息）及文本token输出。 Talker: 双轨自回归Transformer解码器，接收Thinker输出的隐层表示和文本token，结合消除语音歧义的能力，自回归地生成离散语音token。 Thinker与Talker共享历史上下文，支持端到端训练，提升语音生成一致性与上下文保持能力。 高效流式处理能力：\n输入流式处理: 音频和视觉编码器采用分块处理 (Block-wise Processing)，支持流式输入及预填充 (Prefilling)。\n输出流式处理:\nTalker生成的离散语音token实时送入流式音频解码器 (Streaming Audio Codec)。 解码器采用基于Diffusion Transformer (DiT) 的 滑动窗口块注意力 (Sliding Window Block Attention)（回看2个块，前看1个块），控制感受野，实现流式生成。 使用 Flow Matching (Lipman et al., 2022) 将离散 token 转换为梅尔频谱图，再通过改进版 BigVGAN(Lee et al., 2022) 将频谱图流式转换为音频波形，有效降低首包延迟，提升生成实时性。 训练: 包含三个阶段：编码器与 LLM 对齐 -\u0026gt; 全模型多模态预训练 -\u0026gt; 长上下文预训练 (32k)。Talker 单独进行三阶段训练：上下文学习 -\u0026gt; DPO (优化稳定性) -\u0026gt; 多说话人指令微调 (提升自然度)。\n效果: Qwen2.5-Omni 在各项单模态基准上与同规模的 Qwen2.5-VL (视觉) 和 Qwen2-Audio (音频) 表现相当或更好。在 OmniBench 等多模态融合基准上达到 SOTA。语音指令遵循能力接近文本指令。语音生成在鲁棒性和自然度上优于多数现有模型。\nKimi-VL Kimi-VL (Kimi Team, 2025) 是一款开源的 高效混合专家 (Mixture-of-Experts, MoE) 视觉语言模型。\nFig. 21. Model architecture of Kimi-VL and Kimi-VL-Thinking, consisting of a MoonViT that allows native-resolution images, an MLP projector, and a Mixture-of-Experts (MoE) language decoder. (Image source: Kimi Team, 2025)\n架构细节:\n高效 MoE 架构: 语言模型部分采用 MoE 架构（基于 Moonlight，类 DeepSeek-V3 架构），总参数 16B，每次推理仅激活 2.8B 参数（如每层激活 2/8 experts），在保证模型性能的同时显著降低计算成本。支持最大 128K token 的上下文窗口，适用于长文档、长视频等输入场景。\n原生分辨率视觉编码器: 提出参数为 400M 的视觉编码器 MoonViT，支持图像 原生分辨率处理，无需缩放或填充，最大程度保留图像细节。架构基于 ViT，融合了以下技术：\nNaViT 图像打包 (Patch n\u0026rsquo; Pack) 策略：实现对变长图像序列的高效 batch 处理； 插值式绝对位置编码：从 SigLIP(Zhai et al. 2023) 初始化而来，提升位置感知； 二维旋转位置编码（2D-RoPE）：增强空间结构理解； 动态分辨率训练：训练阶段采样不同尺寸图像，提升泛化能力。 多模态融合模块: MoonViT 输出的图像特征通过一个包含 Pixel Shuffle 操作 的 两层 MLP Projector 进行空间压缩和格式转换，之后与文本 token 级特征拼接输入 MoE LLM，完成图文融合处理。\n长思维链推理: 基于主模型，通过长链思维训练流程，包括 长思维链监督微调 和 强化学习优化，提升模型在多轮、多步骤推理任务中的表现，支持复杂逻辑问答与场景理解。\n训练:\nFig. 22. Model architecture of Kimi-VL and Kimi-VL-Thinking, consisting of a MoonViT that allows native-resolution images, an MLP projector, and a Mixture-of-Experts (MoE) language decoder. (Image source: Kimi Team, 2025)\n预训练 (4 阶段, 共 4.4T tokens): ViT 训练 (2.1T): 单独训练 MoonViT (从 SigLIP 初始化)，使用对比损失 SigLIP 和交叉熵 caption 生成。 $$ \\mathcal{L}=\\mathcal{L}_{\\text {siglip }}+\\lambda \\mathcal{L}_{\\text {caption }}, \\text { where } \\lambda=2 $$ 联合预训练 (1.4T): 联合训练 ViT, Projector, LLM (从 Moonlight 5.2T checkpoint 初始化)，混合文本和多模态数据。 联合冷却 (0.6T): 使用高质量文本和多模态数据继续联合训练。 联合长上下文激活 (0.3T): 将上下文从 8K 扩展到 128K，使用长文本、长视频、长文档数据。 Fig. 23. The post-training stages of Kimi-VL and Kimi-VL-Thinking, including two stages of joint SFT in 32K and 128K context, and further long-CoT SFT and RL stages to activate and enhance long thinking abilities. (Image source: Kimi Team, 2025)\n后训练 : 联合 SFT: 使用 ChatML 格式，在混合文本和多模态指令数据上进行微调 (先 32K 再 128K 上下文)。 Long CoT SFT: 使用少量高质量长 CoT 数据进行 SFT，激活长链推理能力。 强化学习: 采用和 KIMI K1.5 模型(Kimi Team, 2025) 相同在线策略镜像下降（Online Policy Mirror Descent） 算法的进行训练。此阶段旨在通过强化学习进一步提升模型的复杂推理和规划能力（如错误识别、回溯、解决方案优化），使其能利用长思维链上下文进行隐式搜索，从而逼近显式规划算法的效果，同时保持自回归生成的简洁性。 核心目标: 优化策略模型 $\\pi_{\\theta}$，使其针对问题 $x \\in \\mathcal{D}$ 生成的思维链 $z$ 和最终答案 $y$ 能够最大化基于基准答案 $y^*$ 的奖励期望：\n$$ \\max _{\\theta} \\mathbb{E}_{\\left(x, y^{*}\\right) \\sim \\mathcal{D},(y, z) \\sim \\pi_{\\theta}}\\left[r\\left(x, y, y^{*}\\right)\\right] $$ 其中 $r(x, y, y^*)$ 通常为 0 或 1 的正确性奖励。\n奖励机制:\n正确性奖励 ($r$): 主要基于最终答案 $y$ 的正确性，判断方式依据任务类型：\n对于编程问题：通过运行自动生成的测试用例来判断。 对于数学问题：使用高精度的思维链奖励模型（Chain-of-Thought RM, 其准确率达 98.5%）来评估。 对于视觉问题：利用真实世界图像、合成视觉推理数据和文本渲染图像等多种数据源，根据任务目标定义奖励。 长度惩罚 (Length Penalty): 为解决“过度思考”并提升 token 效率，引入额外的长度奖励 $\\text{len_reward}(i)$。对于问题 $x$ 从当前策略采样 $k$ 个回答 $(y_i, z_i)$（$i=1, \\dots, k$），令 $\\text{len}(i)$ 为回答 $i$ 的 token 长度，$\\text{min_len} = \\min_i \\text{len}(i)$ 和 $\\text{max_len} = \\max_i \\text{len}(i)$。若 $\\text{max_len} \u0026gt; \\text{min_len}$，则长度奖励为：\n$$ \\text{len_reward}(i) = \\begin{cases} \\lambda \u0026 \\text{若 } r(x, y_i, y^*) = 1 \\\\ \\min(0, \\lambda) \u0026 \\text{若 } r(x, y_i, y^*) = 0 \\end{cases} $$ 其中，长度调节因子 $\\lambda = 0.5 - \\frac{\\text{len}(i) - \\text{min_len}}{\\text{max_len} - \\text{min_len}}$。最终用于优化的总奖励是正确性奖励和长度奖励的加权和。此惩罚采用逐步（warm-up）引入的方式。\n训练特点:\n算法: 基于在线策略镜像下降，训练过程是迭代的。在第 $i$ 轮迭代中，使用当前模型 $\\pi_{\\theta_i}$ 作为参考策略，优化以下带相对熵（KL散度）正则化的目标： $$ \\max _{\\theta} \\mathbb{E}_{\\left(x, y^{*}\\right) \\sim \\mathcal{D}}\\left[\\mathbb{E}_{(y, z) \\sim \\pi_{\\theta}}\\left[r\\left(x, y, y^{*}\\right)\\right]-\\tau \\operatorname{KL}\\left(\\pi_{\\theta}(x) \\| \\pi_{\\theta_{i}}(x)\\right)\\right] $$ 其中 $\\tau \u0026gt; 0$ 是控制正则化强度的参数。 优化: 实际更新使用离策略（off-policy）数据（即从参考策略 $\\pi_{\\theta_i}$ 采样）和近似梯度。对于每个问题 $x$，从 $\\pi_{\\theta_i}$ 采样 $k$ 个回答 $(y_j, z_j)$，计算经验平均奖励 $\\bar{r} = \\frac{1}{k}\\sum_{j=1}^{k} r(x, y_j, y^*)$ 作为基准（baseline）。模型参数 $\\theta$ 的梯度近似为： $$ \\frac{1}{k} \\sum_{j=1}^{k}\\left(\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(y_{j}, z_{j} \\mid x\\right)\\left(r\\left(x, y_{j}, y^{*}\\right)-\\bar{r}\\right)-\\frac{\\tau}{2} \\nabla_{\\theta}\\left(\\log \\frac{\\pi_{\\theta}\\left(y_{j}, z_{j} \\mid x\\right)}{\\pi_{\\theta_{i}}\\left(y_{j}, z_{j} \\mid x\\right)}\\right)^{2}\\right) $$ 该梯度形式类似于带基准的策略梯度，但加入了 $l_2$ 正则化项（最后一项的梯度）并使用离策略样本。训练中 舍弃了价值网络（value network） 以鼓励探索。 采样策略: 为提高训练效率，结合使用： 课程学习 (Curriculum Sampling): 从易到难逐步增加训练问题的难度。 优先采样 (Prioritized Sampling): 根据模型在各问题上的历史成功率 $s_i$，以 $1-s_i$ 的比例优先采样成功率较低的问题。 o3 \u0026amp; o4-mini OpenAI 的 o3 和 o4-mini (OpenAI, 2025) 是其 o 系列推理模型的最新迭代，核心特点是 更长的思考时间 (Longer Thinking Time) 和 全面的工具接入 (Full Tool Access)。\n核心贡献:\n增强推理: 模型被训练成在响应前进行更长时间、更深入的思考（类似于 CoT 或更复杂的推理过程），显著提升了在编码、数学、科学、视觉感知等复杂任务上的性能。o3 在 Codeforces, SWE-bench, MMMU 等基准上达到 SOTA。\n全工具接入: 模型可以无缝调用各种工具，如Web Search、Code Interpreter、GPT‑4o Image Generation，以及通过 API 实现的 Function Calling。模型经过训练，能够自主判断何时以及如何使用这些工具来解决问题。\n多模态推理: 模型可以将 图像直接整合进其思维链，实现视觉和文本的深度融合推理，而不仅仅是将图像作为输入。这使其在分析图表、图示等方面表现优异。\n效率与性能权衡: o3 是目前最强模型，适用于复杂查询；o4-mini 则针对速度和成本进行了优化，参数量更小，但在数学、编码和视觉任务上仍表现出色，尤其擅长利用工具（如在 AIME 竞赛中使用 Python 解释器）。\n大规模强化学习: o 系列模型的性能提升很大程度上归功于大规模强化学习 (RL) 的应用，验证了 RL 在提升推理能力方面的潜力，且性能随计算量增加而提升。\nFig. 24. o3 model demonstrates its multimodal CoT capability by analyzing a user-uploaded image, identifying the ship, and using tools (web search) to find information, ultimately answering the ship\u0026rsquo;s name and its next port of call. (Image source: OpenAI, 2025)\n工作机制:\n长时间思考: 借鉴了“计算换性能”的思想 (Snell et al., 2024)，通过在推理时增加计算量（如多次采样、使用更长的推理链、MCTS 等搜索算法）来提升复杂任务的性能，这可能比单纯增加模型参数更有效。模型内部被设计为可以进行多步推理或更复杂的计算过程，用户可以通过选择不同的 推理努力程度 (reasoning effort) 设置（如 o4-mini-high）来调整模型的思考时间。\n工具使用: 模型通过 RL 或指令微调学习工具使用的策略。当面对一个问题时，模型会：\n规划: 分析问题，判断是否需要以及需要哪些工具。 执行: 调用选定的工具（如进行网络搜索获取最新信息，运行代码进行计算）。 整合: 将工具返回的结果整合到其推理过程中，生成最终答案。 这个过程可以是多轮迭代的，模型可以根据工具返回的信息调整策略（如进行二次搜索）。 多模态思维链 (Multimodal Chain-of-Thought, MCoT) 模型可以直接在其内部推理步骤中引用和分析图像内容，例如识别图表中的数据点，理解流程图的步骤，或解释照片中的细节。感兴趣的读者可以阅读 MCoT 综述(Wang et al., 2025) 介绍其扩展到包含图像、视频、音频、3D、表格/图表等多种模态场景。\n效果:\nFig. 25. To highlight visual reasoning improvement versus our previous multimodal models, OpenAI tested o3 and o4-mini on a diverse set of human exams and ML benchmarks. These new visual reasoning models significantly outperform their predecessors on all multimodal tasks we tested. (Image source: OpenAI, 2025)\no3 和 o4-mini 在多项基准测试中展现了 SOTA 或接近 SOTA 的性能，尤其是在需要深度推理和工具辅助的任务上。专家评估显示，它们相比前代 o1/o3-mini 产生的严重错误更少，回答更实用、可验证，并且交互更自然。\n总结 多模态大语言模型正朝着更全面、更智能、更高效的方向发展。它们不仅能够理解和生成跨越文本、图像、视频、音频等多种模态的内容，还能进行复杂的推理、规划和工具调用。未来，我们可以期待 MLLMs 在效率优化、更深层次的跨模态融合与推理、更强的时序和空间理解能力、以及安全性和可控性方面取得进一步突破。\n参考文献 [1] OpenAI. \u0026ldquo;Hello gpt‑4o.\u0026rdquo; OpenAI Blog (2024).\n[2] DeepMind. \u0026ldquo;Gemini 2.5 Pro\u0026rdquo; DeepMind Blog (2025).\n[3] OpenAI. \u0026ldquo;Introducing OpenAI o3 and o4‑mini.\u0026rdquo; OpenAI Blog (2025).\n[4] Zhang, Duzhen, et al. \u0026ldquo;Mm-llms: Recent advances in multimodal large language models.\u0026rdquo; arXiv preprint arXiv:2401.13601 (2024).\n[5] Dosovitskiy, Alexey, et al. \u0026ldquo;An image is worth 16×16 words: Transformers for image recognition at scale.\u0026rdquo; arXiv preprint arXiv:2010.11929 (2020).\n[6] Radford, Alec, et al. \u0026ldquo;Learning transferable visual models from natural language supervision.\u0026rdquo; International conference on machine learning. PmLR, 2021.\n[7] Li, Junnan, et al. \u0026ldquo;Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.\u0026rdquo; International conference on machine learning. PMLR, 2022.\n[8] Li, Junnan, et al. \u0026ldquo;Align before fuse: Vision and language representation learning with momentum distillation.\u0026rdquo; Advances in neural information processing systems 34 (2021): 9694-9705.\n[9] Li, Junnan, et al. \u0026ldquo;Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\u0026rdquo; International conference on machine learning. PMLR, 2023.\n[10] Liu, Haotian, et al. \u0026ldquo;Visual instruction tuning.\u0026rdquo; arXiv preprint arXiv:2304.08485 (2023).\n[11] Bai, Jinze, et al. \u0026ldquo;Qwen-vl: A frontier large vision-language model with versatile abilities.\u0026rdquo; arXiv preprint arXiv:2308.12966 1.2 (2023): 3.\n[12] Wang, Peng, et al. \u0026ldquo;Qwen2-vl: Enhancing vision-language model\u0026rsquo;s perception of the world at any resolution.\u0026rdquo; arXiv preprint arXiv:2409.12191 (2024).\n[13] Dehghani, Mostafa, et al. \u0026ldquo;Patch n\u0026rsquo; pack: NaViT, a vision transformer for any aspect ratio and resolution.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 2252-2274.\n[14] Su, Jianlin, et al. \u0026ldquo;Roformer: Enhanced transformer with rotary position embedding.\u0026rdquo; Neurocomputing 568 (2024): 127063.\n[15] Su, Jianlin. \u0026ldquo;Transformer升级之路：4、二维位置的旋转位置编码.\u0026rdquo; 科学空间 (blog) (2021).\n[16] Bai, Shuai, et al. \u0026ldquo;Qwen2.5‑VL Technical Report.\u0026rdquo; arXiv preprint arXiv:2502.13923 (2025).\n[17] Xu, Jin, et al. \u0026ldquo;Qwen2.5‑Omni Technical Report.\u0026rdquo; arXiv preprint arXiv:2503.20215 (2025).\n[18] Lipman, Yaron, et al. \u0026ldquo;Flow matching for generative modeling.\u0026rdquo; arXiv preprint arXiv:2210.02747 (2022).\n[19] Lee, Sang-gil, et al. \u0026ldquo;Bigvgan: A universal neural vocoder with large-scale training.\u0026rdquo; arXiv preprint arXiv:2206.04658 (2022).\n[20] Kimi Team. \u0026ldquo;Kimi‑VL Technical Report.\u0026rdquo; arXiv preprint arXiv:2504.07491 (2025).\n[21] Zhai, Xiaohua, et al. \u0026ldquo;Sigmoid loss for language image pre-training.\u0026rdquo; Proceedings of the IEEE/CVF international conference on computer vision. 2023.\n[22] Kimi Team. \u0026ldquo;Kimi k1. 5: Scaling reinforcement learning with llms.\u0026rdquo; arXiv preprint arXiv:2501.12599 (2025).\n[23] Snell, Charlie, et al. \u0026ldquo;Scaling llm test-time compute optimally can be more effective than scaling model parameters.\u0026rdquo; arXiv preprint arXiv:2408.03314 (2024).\n[24] Wang, Yaoting, et al. \u0026ldquo;Multimodal chain-of-thought reasoning: A comprehensive survey.\u0026rdquo; arXiv preprint arXiv:2503.12605 (2025).\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (May 2025). 多模态大语言模型. https://syhya.github.io/zh/posts/2025-05-04-multimodal-llm/\nOr\n@article{yue_shui_multimodal_llm_2025, title = \u0026#34;多模态大语言模型\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;May\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-05-04-multimodal-llm/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-05-04-multimodal-llm/","summary":"\u003cp\u003e人类通过多种感官（视觉、听觉、触觉等）与世界互动，每种感官通道在表征和交流特定概念时都具有独特的优势。这种多模态交互促进了我们对世界的深刻理解。人工智能领域的核心目标之一便是开发能够有效遵循多模态指令（如视觉和语言）的通用助手，使其能够像人类一样完成现实世界的各种任务。近年来，随着 GPT-4o (\u003ca href=\"https://openai.com/index/hello-gpt-4o/\"\u003eOpenAI, 2024\u003c/a\u003e)、Gemini 2.5 Pro (\u003ca href=\"https://deepmind.google/technologies/gemini/pro/\"\u003eDeepMind, 2025\u003c/a\u003e) 和 o3/o4-mini (\u003ca href=\"https://openai.com/index/introducing-o3-and-o4-mini/\"\u003eOpenAI, 2025\u003c/a\u003e) 等模型的发布，\u003cstrong\u003e多模态大语言模型（Multimodal Large Language Models, MLLMs）\u003c/strong\u003e 取得了显著进展，它们不仅能理解图像、视频、音频等多种模态信息，还能进行复杂的推理和生成。\u003c/p\u003e","title":"多模态大语言模型"},{"content":"DeepSeek AI 先后发布了 DeepSeek-V2 (DeepSeek-AI, 2024) 和 DeepSeek-V3 (DeepSeek-AI, 2024)，这两款强大的混合专家（Mixture-of-Experts, MoE）语言模型在保持顶尖性能的同时，显著优化了训练成本和推理效率。DeepSeek-V2 拥有 236B 总参数，每次激活 21B；而 DeepSeek-V3 则进一步扩展至 671B 总参数，每次激活 37B。两者均支持 128K 上下文长度。\n这两个模型的核心创新在于采用了 多头隐注意力 (Multi-head Latent Attention, MLA) 和 DeepSeekMoE 架构 (Dai et al., 2024)。MLA 通过将键值（KV）缓存压缩到低维隐向量中，大幅降低了推理时的显存占用，提高了效率。DeepSeekMoE 则通过细粒度专家切分和共享专家隔离，实现了更强的专家特化能力和更经济的训练成本。DeepSeek-V3 在 V2 的基础上，进一步引入了无辅助损失的负载均衡策略 (Auxiliary-Loss-Free Load Balancing) (Wang et al., 2024) 和多 token 预测 (Multi-Token Prediction, MTP) (Gloeckle et al., 2024)训练目标 ，进一步提升了模型性能和训练效率。\nDeepSeek-V2 在 8.1T tokens 上进行预训练，而 DeepSeek-V3 则在更大规模的 14.8T tokens 上训练。两者都经过了监督微调（Supervised Fine-Tuning, SFT）和强化学习（Reinforcement Learning, RL）阶段以充分释放潜力。评估结果显示，DeepSeek-V2 和 V3 在众多基准测试中均达到了开源模型的顶尖水平，DeepSeek-V3 更是成为了目前最强的开源基础模型之一，性能可与顶尖闭源模型媲美。\nFig. 1. (a) MMLU accuracy vs. activated parameters, among different open-source models. (b) Training costs and inference efficiency of DeepSeek 67B (Dense) and DeepSeek-V2. (Image source: DeepSeek-AI, 2024)\nFig. 2. Benchmark performance of DeepSeek-V3 and its counterparts. (Image source: DeepSeek-AI, 2024)\n本文将深入探讨 DeepSeek-V2 和 DeepSeek-V3 的关键技术，包括其创新的模型架构、高效的训练基础设施、预训练和对齐过程。\n符号表 下面列举了文章所使用的数学公式，可以帮你更轻松阅读。\n符号 含义 \\( d \\) 嵌入维度 \\( n_h \\) 注意力头数量 \\( d_h \\) 每个注意力头的维度 \\( \\mathbf{h}_t \\in \\mathbb{R}^d \\) 第 \\( t \\) 个 token 在注意力层的输入 \\( \\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t \\) 查询, 键, 值向量 \\( W^Q, W^K, W^V, W^O \\) 查询、键、值、输出的投影矩阵 \\( \\mathbf{q}_{t,i}, \\mathbf{k}_{t,i}, \\mathbf{v}_{t,i} \\) 第 \\( i \\) 个注意力头的查询、键、值向量 \\( \\mathbf{o}_{t,i} \\) 第 \\( i \\) 个注意力头的输出 \\( \\mathbf{u}_t \\) 注意力层的最终输出 \\( l \\) 模型层数 \\( \\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c} \\) 键值的压缩隐向量 \\( d_c \\) KV 压缩维度 \\( W^{DKV}, W^{UK}, W^{UV} \\) KV 的下投影、键的上投影、值的上投影矩阵 \\( \\mathbf{k}_t^C, \\mathbf{v}_t^C \\) 通过上投影从隐向量恢复的键和值 \\( \\mathbf{c}_t^Q \\in \\mathbb{R}^{d_c'} \\) 查询的压缩隐向量 \\( d_c' \\) 查询压缩维度 \\( W^{DQ}, W^{UQ} \\) 查询的下投影、上投影矩阵 \\( \\mathbf{q}_t^C \\) 通过上投影从隐向量恢复的查询 \\( \\mathbf{q}_{t,i}^R, \\mathbf{k}_t^R \\) 解耦的 RoPE 查询和键 \\( d_h^R \\) 解耦 RoPE 查询/键的头维度 \\( W^{QR}, W^{KR} \\) 解耦 RoPE 查询/键的生成矩阵 \\( \\operatorname{RoPE}(\\cdot) \\) 应用旋转位置编码的操作 \\( [\\cdot ; \\cdot] \\) 拼接操作 \\( n_g \\) GQA 中的组数 \\( n \\) MoE 中的专家总数 \\( E_i \\) 第 \\( i \\) 个专家网络 \\( G(\\cdot) \\) 门控网络函数 \\( p_i \\) 门控网络输出的第 \\( i \\) 个概率 \\( H^{(i)}(x) \\) Noisy Top-k Gating 中专家 \\( i \\) 的门控分数 \\( W_g, W_{\\text{noise}} \\) MoE 门控网络和噪声网络的权重矩阵 \\( \\epsilon \\) 标准高斯噪声 \\( \\text{softplus}(\\cdot) \\) Softplus 激活函数 \\( k \\) MoE 中每个 token 选择的专家数量 \\( \\text{topk}(\\cdot, k) \\) 选择前 k 个最大值的函数 \\( \\mathcal{L}_{\\text{aux}} \\) MoE 辅助损失 \\( w_{\\text{aux}} \\) 辅助损失权重 \\( \\text{CV}(\\cdot) \\) 变异系数 \\( N_s, N_r \\) DeepSeekMoE 中共享专家和路由专家的数量 \\( \\operatorname{FFN}_i^{(s)}(\\cdot), \\operatorname{FFN}_i^{(r)}(\\cdot) \\) 第 \\( i \\) 个共享专家和路由专家函数 \\( K_r \\) DeepSeekMoE 中激活的路由专家数量 \\( g_{i,t} \\) 第 \\( i \\) 个专家对第 \\( t \\) 个 token 的门控值 \\( g_{i,t}' \\) TopK 选择后的原始门控值（V3） \\( s_{i,t} \\) 第 \\( t \\) 个 token 对第 \\( i \\) 个专家的亲和度分数 \\( \\mathbf{e}_i \\) 第 \\( i \\) 个路由专家的中心向量 \\( M \\) 设备/节点限制路由中的设备/节点数上限 \\( \\mathcal{L}_{\\text{ExpBal}}, \\mathcal{L}_{\\text{DevBal}}, \\mathcal{L}_{\\text{CommBal}} \\) 专家级、设备级、通信级负载均衡损失 \\( f_i, P_i \\) 专家 \\( i \\) 的负载分数和平均亲和度 \\( \\alpha_1, \\alpha_2, \\alpha_3 \\) 负载均衡损失的超参数 \\( T \\) 序列中的 token 数量 \\( D \\) 设备/节点组的数量 \\( \\mathcal{E}_i \\) 第 \\( i \\) 个设备/节点上的专家组 \\( f_i', P_i' \\) 设备组 \\( i \\) 的平均负载分数和总亲和度 \\( f_i'', P_i'' \\) 发送到设备 \\( i \\) 的 token 比例和设备组 \\( i \\) 的总亲和度 \\( b_i \\) 第 \\( i \\) 个专家的偏置项 (aux-loss-free balancing) \\( \\gamma \\) 偏置项更新速度 \\( \\mathcal{L}_{\\text{Bal}} \\) 序列级负载均衡损失 \\( \\alpha \\) 序列级负载均衡损失的超参数 \\( D_{MTP} \\) MTP 预测深度 \\( \\operatorname{Emb}(\\cdot), \\operatorname{OutHead}(\\cdot) \\) 共享嵌入层和输出头 (MTP) \\( \\operatorname{TRM}_k(\\cdot) \\) 第 \\( k \\) 个 MTP 模块的 Transformer 块 \\( M_k \\) 第 \\( k \\) 个 MTP 模块的投影矩阵 \\( \\mathbf{h}_i^k \\) 第 \\( i \\) 个 token 在第 \\( k \\) 个 MTP 深度的表示 \\( \\mathbf{h}_i^{\\prime k} \\) 第 \\( k \\) 个 MTP 模块的 Transformer 块的输入 \\( P_{i+k+1}^k \\) 第 \\( k \\) 个 MTP 模块对第 \\( i+k+1 \\) 个 token 的预测概率分布 \\( V \\) 词汇表大小 \\( \\mathcal{L}_{\\text{MTP}}^k \\) 第 \\( k \\) 个 MTP 深度的交叉熵损失 \\( \\mathcal{L}_{\\text{MTP}} \\) 总 MTP 损失 \\( \\lambda \\) MTP 损失的权重因子 \\( \\mathcal{J}_{GRPO}(\\theta) \\) GRPO 目标函数 \\( A_i \\) 相对优势值 (GRPO) \\( \\varepsilon \\) PPO/GRPO 中的裁剪超参数 \\( \\beta \\) KL 散度惩罚项系数 \\( \\mathbb{D}_{KL}(\\pi_\\theta \\| \\pi_{ref}) \\) KL 散度 \\( \\pi_\\theta, \\pi_{\\theta_{old}}, \\pi_{ref} \\) 当前策略、旧策略、参考策略模型 \\( r_i \\) 第 \\( i \\) 个输出的奖励值 \\( \\mathbb{1}(\\cdot) \\) 指示函数 核心架构 DeepSeek-V2 和 V3 均基于 Transformer 架构，但在注意力和前馈网络（FFN）部分采用了创新设计比如 MLA 和 DeepseekMoE，以平衡性能、训练成本和推理效率。下图展示了 DeepSeek-V2 和 V3 的架构。\nFig. 3. Illustration of the architecture of DeepSeek-V2 and DeepSeek-V3. MLA ensures efficient inference by significantly reducing the KV cache for generation, and DeepSeekMoE enables training strong models at an economical cost through the sparse architecture. (Image source: DeepSeek-AI, 2024)\n多头隐注意力 (MLA) 传统的 Transformer 模型通常采用多头注意力（Multi-Head Attention, MHA）(Vaswani et al., 2017)，但在生成过程中，其庞大的 KV cache 成为限制推理效率的瓶颈。为了解决这个问题，研究者提出了多查询注意力（Multi-Query Attention, MQA）(Shazeer, 2019) 和分组查询注意力（Grouped-Query Attention, GQA）(Ainslie et al., 2023)。这些方法虽然减少了 KV 缓存，但往往以牺牲模型性能为代价。\nDeepSeek-V2 和 V3 采用了创新的 多头隐注意力（Multi-head Latent Attention, MLA） 机制。MLA 的核心思想是 低秩键值联合压缩 (Low-Rank Key-Value Joint Compression)。\nFig. 4. Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. (Image source: DeepSeek-AI, 2024)\nMHA 回顾 标准 MHA 首先通过三个投影矩阵 \\(W^Q, W^K, W^V \\in \\mathbb{R}^{d_h n_h \\times d}\\) 将输入 \\(\\mathbf{h}_t \\in \\mathbb{R}^d\\) 转换为查询 \\(\\mathbf{q}_t\\)、键 \\(\\mathbf{k}_t\\) 和值 \\(\\mathbf{v}_t \\in \\mathbb{R}^{d_h n_h}\\)： \\[ \\begin{aligned} \\mathbf{q}_{t} \u0026= W^{Q} \\mathbf{h}_{t}, \\\\ \\mathbf{k}_{t} \u0026= W^{K} \\mathbf{h}_{t}, \\\\ \\mathbf{v}_{t} \u0026= W^{V} \\mathbf{h}_{t}. \\end{aligned} \\] 然后将 \\(\\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t\\) 切分为 \\(n_h\\) 个头，进行多头注意力计算： \\[ \\begin{aligned} \u0026 [\\mathbf{q}_{t, 1} ; \\mathbf{q}_{t, 2} ; \\ldots ; \\mathbf{q}_{t, n_{h}}] = \\mathbf{q}_{t}, \\\\ \u0026 [\\mathbf{k}_{t, 1} ; \\mathbf{k}_{t, 2} ; \\ldots ; \\mathbf{k}_{t, n_{h}}] = \\mathbf{k}_{t}, \\\\ \u0026 [\\mathbf{v}_{t, 1} ; \\mathbf{v}_{t, 2} ; \\ldots ; \\mathbf{v}_{t, n_{h}}] = \\mathbf{v}_{t}, \\\\ \u0026 \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} \\operatorname{Softmax}_{j}\\left(\\frac{\\mathbf{q}_{t, i}^{T} \\mathbf{k}_{j, i}}{\\sqrt{d_{h}}}\\right) \\mathbf{v}_{j, i}, \\\\ \u0026 \\mathbf{u}_{t} = W^{O}\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right], \\end{aligned} \\] 其中 \\(\\mathbf{q}_{t, i}, \\mathbf{k}_{t, i}, \\mathbf{v}_{t, i} \\in \\mathbb{R}^{d_h}\\) 分别是第 \\(i\\) 个头的查询、键、值，\\(W^O \\in \\mathbb{R}^{d \\times d_h n_h}\\) 是输出投影矩阵。推理时，需要缓存所有 \\(t\\) 步的键和值，每个 token 需要缓存 \\(2 n_h d_h l\\) 个元素（\\(l\\) 为层数），这构成了巨大的 KV 缓存开销。\n低秩键值联合压缩 MLA 通过引入一个低维的隐向量 \\(\\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c}\\) 来联合压缩键和值，其中 \\(d_c \\ll d_h n_h\\)： \\[ \\begin{aligned} \\boxed{\\mathbf{c}_{t}^{K V}} \u0026= W^{D K V} \\mathbf{h}_{t}, \\\\ \\mathbf{k}_{t}^{C} \u0026= W^{U K} \\mathbf{c}_{t}^{K V}, \\\\ \\mathbf{v}_{t}^{C} \u0026= W^{U V} \\mathbf{c}_{t}^{K V}. \\end{aligned} \\] 这里 \\(W^{DKV} \\in \\mathbb{R}^{d_c \\times d}\\) 是下投影矩阵，\\(W^{UK}, W^{UV} \\in \\mathbb{R}^{d_h n_h \\times d_c}\\) 分别是键和值的上投影矩阵。推理时，MLA 只需要缓存压缩后的隐向量 \\(\\mathbf{c}_t^{KV}\\)（以及后面提到的解耦 RoPE 键 \\(\\mathbf{k}_t^R\\)），极大地减少了 KV 缓存量。\n为了减少训练时的激活内存，MLA 也对查询进行了类似的低秩压缩： \\[ \\begin{aligned} \\mathbf{c}_{t}^{Q} \u0026= W^{D Q} \\mathbf{h}_{t}, \\\\ \\mathbf{q}_{t}^{C} \u0026= W^{U Q} \\mathbf{c}_{t}^{Q}, \\end{aligned} \\] 其中 \\(\\mathbf{c}_t^Q \\in \\mathbb{R}^{d_c'}\\) 是查询的压缩隐向量，\\(d_c' \\ll d_h n_h\\)，\\(W^{DQ} \\in \\mathbb{R}^{d_c' \\times d}\\) 和 \\(W^{UQ} \\in \\mathbb{R}^{d_h n_h \\times d_c'}\\) 分别是查询的下投影和上投影矩阵。\n解耦旋转位置编码 标准的旋转位置编码（RoPE）(Su et al., 2024) 直接应用于键和查询，但这与 MLA 的低秩 KV 压缩不兼容。如果在压缩后的键 \\(\\mathbf{k}_t^C\\) 上应用 RoPE，那么上投影矩阵 \\(W^{UK}\\) 会与位置相关的 RoPE 矩阵耦合，导致推理时无法将其吸收到 \\(W^Q\\) 中，必须重新计算所有前缀 token 的键，严重影响效率。\n为了解决这个问题，MLA 提出了解耦 RoPE (Decoupled RoPE) 策略。它引入了额外的多头查询 \\(\\mathbf{q}_{t, i}^R \\in \\mathbb{R}^{d_h^R}\\) 和一个共享的键 \\(\\mathbf{k}_t^R \\in \\mathbb{R}^{d_h^R}\\) 来专门承载 RoPE 信息：\n\\[ \\begin{aligned} \\left[\\mathbf{q}_{t,1}^R;\\,\\mathbf{q}_{t,2}^R;\\,\\dots;\\,\\mathbf{q}_{t,n_h}^R\\right] = \\mathbf{q}_t^R \u0026= \\operatorname{RoPE}\\bigl(W^{Q R}\\,\\mathbf{c}_t^Q\\bigr),\\\\ \\boxed{\\mathbf{k}_t^R} \u0026= \\operatorname{RoPE}\\bigl(W^{K R}\\,\\mathbf{h}_t\\bigr). \\end{aligned} \\] 这里 \\(W^{QR} \\in \\mathbb{R}^{d_h^R n_h \\times d_c'}\\) 和 \\(W^{KR} \\in \\mathbb{R}^{d_h^R \\times d}\\) 是生成解耦查询和键的矩阵。 然后将压缩得到的键/查询部分 (\\(C\\)) 与解耦的 RoPE 部分 (\\(R\\)) 拼接起来形成最终的键和查询： \\[ \\begin{aligned} \\mathbf{q}_{t, i} \u0026= [\\mathbf{q}_{t, i}^{C} ; \\mathbf{q}_{t, i}^{R}], \\\\ \\mathbf{k}_{t, i} \u0026= [\\mathbf{k}_{t, i}^{C} ; \\mathbf{k}_{t}^{R}]. \\end{aligned} \\] 最终的注意力计算变为： \\[ \\begin{aligned} \\mathbf{o}_{t, i} \u0026= \\sum_{j=1}^{t} \\operatorname{Softmax}_{j}\\left(\\frac{\\mathbf{q}_{t, i}^{T} \\mathbf{k}_{j, i}}{\\sqrt{d_{h}+d_{h}^{R}}}\\right) \\mathbf{v}_{j, i}^{C}, \\\\ \\mathbf{u}_{t} \u0026= W^{O}\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right]. \\end{aligned} \\] 推理时，除了缓存 \\(\\mathbf{c}_t^{KV}\\)，还需要缓存解耦的 RoPE 键 \\(\\mathbf{k}_t^R\\)。因此，DeepSeek-V2/V3 每个 token 总共需要缓存 \\((d_c + d_h^R)l\\) 个元素。\nMLA 推理中的矩阵吸收 MLA 的一个关键优势在于推理效率的提升，这部分得益于矩阵乘法结合律允许将上投影矩阵 \\(W^{UK}\\) 和 \\(W^{UV}\\) “吸收”掉，避免显式计算完整的键 \\(\\mathbf{k}_t^C\\) 和值 \\(\\mathbf{v}_t^C\\)。\n1. 吸收 \\(W^{UK}\\) (优化注意力分数计算):\n注意力分数计算的核心是查询和键的点积 \\(\\mathbf{q}_{t,i}^T \\mathbf{k}_{j,i}\\)。关注其中由压缩向量生成的 \\(C\\) 部分的点积： \\[ (\\mathbf{q}_{t,i}^C)^T \\mathbf{k}_{j,i}^C \\] 将 \\(\\mathbf{k}_{j,i}^C = W^{UK} \\mathbf{c}_j^{KV}\\) 代入： \\[ (\\mathbf{q}_{t,i}^C)^T (W^{UK} \\mathbf{c}_j^{KV}) \\] 根据矩阵乘法结合律 \\((AB)C = A(BC)\\) 和转置性质 \\((AB)^T = B^T A^T\\)，可以将上式改写为： \\[ (\\mathbf{q}_{t,i}^C)^T (W^{UK} \\mathbf{c}_j^{KV}) = ((W^{UK})^T \\mathbf{q}_{t,i}^C)^T \\mathbf{c}_j^{KV} \\] 这个变换的意义在于：不再需要用 \\(W^{UK}\\) 作用于缓存的 \\(\\mathbf{c}_j^{KV}\\) 来得到 \\(\\mathbf{k}_{j,i}^C\\)。相反，可以先计算一个“有效查询” \\(\\tilde{\\mathbf{q}}_{t,i}^C = (W^{UK})^T \\mathbf{q}_{t,i}^C\\)，然后直接用这个有效查询与缓存的隐向量 \\(\\mathbf{c}_j^{KV}\\) 进行点积。\n原始查询 \\(\\mathbf{q}_{t,i}^C\\) 是通过 \\(W^{UQ}\\) 和 \\(W^{DQ}\\) 从 \\(\\mathbf{h}_t\\) 计算得到的 (\\(\\mathbf{q}_{t,i}^C = (W^{UQ} W^{DQ} \\mathbf{h}_t)_i\\))。因此，从 \\(\\mathbf{h}_t\\) 到有效查询 \\(\\tilde{\\mathbf{q}}_{t,i}^C\\) 的整个计算过程可以看作是一个新的、合并了 \\(W^{UK}\\) 的有效查询投影操作。在实际实现中，这意味着计算 \\(\\mathbf{q}_{t,i}^C\\) 后，可以再左乘 \\((W^{UK})^T\\)，或者更高效地，将 \\((W^{UK})^T\\) 合并到生成查询的原始矩阵 \\(W^Q\\)（或 \\(W^{UQ}W^{DQ}\\)）中，形成一个新的查询投影矩阵 \\(\\tilde{W}^Q = (W^{UK})^T W^{UQ} W^{DQ}\\)。\n关键在于，涉及 \\(W^{UK}\\) 的计算被移到了查询侧，在计算注意力分数之前一次性完成，而无需在每次查询时都用 \\(W^{UK}\\) 从缓存的 \\(\\mathbf{c}_j^{KV}\\) 中恢复 \\(\\mathbf{k}_{j,i}^C\\)。\n2. 吸收 \\(W^{UV}\\) (优化加权求和):\n注意力头的输出 \\(\\mathbf{o}_{t,i}\\) 是注意力权重 (记作 \\(w_{ij}\\)) 与值 \\(\\mathbf{v}_{j,i}^C\\) 的加权和： \\[ \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} w_{ij} \\cdot \\mathbf{v}_{j, i}^{C} \\] 将 \\(\\mathbf{v}_{j,i}^C = (W^{UV} \\mathbf{c}_j^{KV})_i\\) 代入（这里 \\(( \\cdot )_i\\) 表示属于第 \\(i\\) 个头的部分）： \\[ \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} w_{ij} \\cdot (W^{UV} \\mathbf{c}_j^{KV})_i \\] 最终的注意力层输出 \\(\\mathbf{u}_t\\) 是所有头的输出 \\(\\mathbf{o}_{t,i}\\) 拼接后通过输出矩阵 \\(W^O\\) 投影得到的： \\[ \\mathbf{u}_{t} = W^{O}\\left[\\mathbf{o}_{t, 1} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right] = W^{O} \\begin{bmatrix} \\sum_{j} w_{1j} (W^{UV} \\mathbf{c}_j^{KV})_1 \\\\ \\vdots \\\\ \\sum_{j} w_{n_h j} (W^{UV} \\mathbf{c}_j^{KV})_{n_h} \\end{bmatrix} \\] 由于矩阵乘法的线性性质（\\(A(B+C) = AB + AC\\) 以及 \\(A(cB) = c(AB)\\)），可以将 \\(W^{UV}\\) 从求和中“提出”（这里是为了直观理解，实际操作是矩阵层面的）： \\[ \\mathbf{u}_{t} \\approx W^{O} W^{UV} \\left( \\sum_{j=1}^{t} \\begin{bmatrix} w_{1j} (\\mathbf{c}_j^{KV})_1 \\\\ \\vdots \\\\ w_{n_h j} (\\mathbf{c}_j^{KV})_{n_h} \\end{bmatrix} \\right) \\] （注意：这里的 \\((\\mathbf{c}_j^{KV})_i\\) 只是示意，实际计算中是直接对完整的 \\(\\mathbf{c}_j^{KV}\\) 操作，但原理相同，即先对 \\(\\mathbf{c}_j^{KV}\\) 进行加权求和，再应用 \\(W^{UV}\\) 和 \\(W^O\\)）。\n令有效输出矩阵 \\(\\tilde{W}^O = W^O W^{UV}\\)。这意味着可以先计算注意力权重与隐向量 \\(\\mathbf{c}_j^{KV}\\) 的加权和（得到一个维度为 \\(d_c\\) 的中间结果 \\(\\tilde{\\mathbf{o}}_t = \\sum_j w_{ij} \\mathbf{c}_j^{KV}\\)），然后直接用这个合并后的有效输出矩阵 \\(\\tilde{W}^O\\) 进行最终投影得到 \\(\\mathbf{u}_t\\)。同样，涉及 \\(W^{UV}\\) 的计算被合并到了最后的输出投影步骤，无需在计算加权和时从 \\(\\mathbf{c}_j^{KV}\\) 恢复 \\(\\mathbf{v}_{j,i}^C\\)。\n总结: 通过矩阵吸收，MLA 在推理时避免了从缓存的低维隐向量 \\(\\mathbf{c}_j^{KV}\\) 重复计算高维的键 \\(\\mathbf{k}_{j,i}^C\\) 和值 \\(\\mathbf{v}_{j,i}^C\\)，显著提高了计算效率。实际缓存的只有 \\(\\mathbf{c}_t^{KV}\\) 和 \\(\\mathbf{k}_t^R\\)。\nKV 缓存对比 下表比较了不同注意力机制的每 Token KV 缓存对比。\\(n_h\\) 是注意力头数，\\(d_h\\) 是每头维度，\\(l\\) 是层数，\\(n_g\\) 是 GQA 组数，\\(d_c\\) 和 \\(d_h^R\\) 是 MLA 的 KV 压缩维度和解耦 RoPE 维度。对于 DeepSeek-V2，\\(d_c = 4d_h\\)，\\(d_h^R = d_h/2\\)，其 KV 缓存相当于 \\(n_g=2.25\\) 的 GQA，但性能优于 MHA。DeepSeek-V3 沿用了类似配置。\n注意力机制 每 Token KV 缓存大小 (# 元素) 能力 Multi-Head Attention (MHA) \\(2 n_{h} d_{h} l\\) 强 Grouped-Query Attention (GQA) \\(2 n_{g} d_{h} l\\) 中等 Multi-Query Attention (MQA) \\(2 d_{h} l\\) 弱 Multi-head Latent Attention (MLA) \\(\\bigl(d_{c} + d_{h}^{R}\\bigr) l \\approx \\tfrac{9}{2} \\, d_{h} \\, l\\) 更强 下图表明，MLA 不仅显著减少了 KV 缓存，其性能甚至优于标准的 MHA。\nFig. 5. Comparison between MLA and MHA on hard benchmarks. DeepSeek-V2 shows better performance than MHA, but requires a significantly smaller amount of KV cache. (Image source: DeepSeek-AI, 2024)\n混合专家模型 在深入 DeepSeekMoE 之前，先回顾一下混合专家模型（MoE）的基础知识。\n混合专家模型(Mixture-of-Experts, MoE)(Shazeer et al. 2017) 是一种稀疏激活模型，它通过结合多个独立的“专家”网络和一个门控网络(Gating Network)，在不显著增加计算成本的前提下，大幅提升了模型的参数量和性能。MoE 的核心思想是稀疏激活(Sparse Activation)，即对于每个输入样本，仅激活部分专家网络，而不是整个模型。这种方法既提高了计算效率，又增强了模型的表达能力，使其在 LLMs 中表现出色。\nMoE 设计灵感来源于集成学习(Ensemble learning), 一种将复杂任务分解为多个子任务并由不同模型协作完成的技术。在 MoE 中，这些“子任务”由多个独立的专家网络处理，而门控网络则负责根据输入样本的特征动态选择最适合的专家。这种分工合作的机制类似于人类社会中的专家团队：不同领域的专家针对特定问题提供专业意见，最终综合得出结果。\nFig. 6. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)\nMoE 核心组件 一个典型的 MoE 包含以下组件：\n专家网络(Experts): 一组独立的神经网络 ${E_1, E_2, \u0026hellip;, E_n}$，每个专家网络 $E_i$ 可以是任意类型的神经网络，例如 FFN, CNN, RNN 等。专家网络的数量 $n$ 可以很大，例如几十个、几百个甚至几千个。 门控网络(Gating Network): 一个可训练的神经网络 $G$，用于根据输入样本 $x$ 学习一个概率分布，决定激活哪些专家。门控网络的输入是输入样本 $x$，输出是一个 $n$ 维的概率向量 $p = G(x) = [p_1, p_2, \u0026hellip;, p_n]$，其中 $p_i$ 表示激活专家 $E_i$ 的概率。 专家输出聚合(Expert Output Aggregation): 根据门控网络的输出概率分布，将激活的专家网络的输出进行加权求和，得到 MoE 层的最终输出 $y$。 Noisy Top-k Gating 为了实现稀疏激活并确保专家使用均衡，MoE 通常采用 Noisy Top-k Gating 作为门控机制。这种方法通过引入噪声和 top-k 选择，既保证了计算效率，又避免了专家负载不均的问题。以下是其详细工作流程：\n门控分数计算: 对于输入样本 $x$，门控网络首先计算每个专家的门控分数 $H^{(i)}(x)$。这一分数包含两部分：线性变换和噪声项，公式如下：\n$$ H^{(i)}(x) =(x W_g)^{(i)} + \\epsilon \\cdot \\text{softplus}\\left((x W_{\\text{noise}})^{(i)} \\right), \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$ 参数说明： $W_g \\in \\mathbb{R}^{d \\times n}$：门控网络的可训练权重矩阵，$d$ 是输入特征维度，$n$ 是专家数量。 $W_{\\text{noise}} \\in \\mathbb{R}^{d \\times n}$：用于生成噪声的权重矩阵。 $\\epsilon \\sim \\mathcal{N}(0, 1)$：标准高斯噪声，增加门控随机性。 $\\text{softplus}(x) = \\log(1 + e^x)$：平滑激活函数，确保噪声非负。 噪声的引入避免了门控网络总是选择固定的专家，增强了模型的鲁棒性和多样性。\nTop-k 选择: 计算出门控分数向量 $H(x) = [H^{(1)}(x), H^{(2)}(x), \\dots, H^{(n)}(x)]$ 后，门控网络选择其中值最大的前 $k$ 个专家(通常 $k \\ll n$)。这一步骤通过 $\\text{topk}(v, k)$ 函数实现：\n$$ \\text{topk}^{(i)}(v, k) = \\begin{cases} v^{(i)} \u0026 \\text{if } v^{(i)} \\text{ is in the top } k \\text{ elements of } v \\\\ -\\infty \u0026 \\text{otherwise} \\end{cases} $$将非 Top-k 专家的分数设为 $-\\infty$，确保后续 softmax 操作中这些专家的概率为 0，实现稀疏性。\nSoftmax 归一化: 对 Top-k 专家的门控分数进行 softmax 归一化，得到稀疏的概率分布 $G(x)$：\n$$ G(x) = \\text{softmax}\\left( \\text{topk}(H(x), k) \\right) $$只有 Top-k 个专家的概率非零，其余为 0。例如，若 $n=100, k=2$，则 98 个专家的概率为 0。\n加权求和: 将 Top-k 个专家的输出按概率加权求和，得到 MoE 层的输出：\n$$ y = \\sum_{i=1}^{n} G^{(i)}(x) E_i(x) $$由于只有 $k$ 个专家被激活，计算量远低于激活所有 $n$ 个专家。\n辅助损失 为了避免门控网络过度偏向少数专家，MoE 引入了辅助损失(Auxiliary Loss)(Shazeer et al. 2017)，鼓励所有专家被均匀使用。一种常用方法是基于专家使用率的变异系数(Coefficient of Variation, CV)的平方：\n$$ \\mathcal{L}_{\\text{aux}} = w_{\\text{aux}} \\cdot \\text{CV}\\left( \\sum_{x \\in X} G(x) \\right)^2 $$ 参数说明：\n$X$：一个 mini-batch 的输入样本。 $\\sum_{x \\in X} G(x)$：统计每个专家在 mini-batch 中的激活次数。 $\\text{CV}$：标准差与均值的比值，衡量专家使用分布的均匀性。 $w_{\\text{aux}}$：辅助损失的权重，需手动调整。 作用：通过最小化 $\\mathcal{L}_{\\text{aux}}$，模型优化专家选择的均衡性，避免某些专家被过度使用而其他专家闲置。\nGShard GShard(Lepikhin et al. 2020)主要对 MoE 层进行分片，将 MoE 层中的专家网络 ${E_1, E_2, \u0026hellip;, E_n}$ 分散到多个 TPU 设备上。例如，如果有 $P$ 个 TPU 设备，可以将专家网络划分为 $P$ 组，每组专家网络分配到一个 TPU 设备上。Transformer 模型的其他层(例如自注意力层、LayerNorm 层) 则在所有 TPU 设备上复制。\nGShard 的改进门控机制:\nGShard 在 Noisy Top-k Gating 的基础上，进行了一些改进，以提高门控机制的性能和稳定性：\n专家容量(Expert Capacity):\n为了避免专家过载，GShard 引入了专家容量限制。每个专家网络都有一个容量上限，表示它最多可以处理的 token 数量。如果一个 token 被路由到一个已经达到容量上限的专家网络，则该 token 会被标记为 \u0026ldquo;overflowed\u0026rdquo;，门控输出会被设置为零向量，表示该 token 不会被路由到任何专家网络。\n局部组分发(Local Group Dispatching):\n为了提高门控效率，GShard 将 token 分组，在组级别强制执行专家容量限制。例如，将 mini-batch 中的 token 划分为多个局部组，每个局部组包含一定数量的 token。门控网络为每个局部组选择 top-k 个专家网络，并确保每个专家网络在一个局部组内处理的 token 数量不超过其容量上限。\n辅助损失(Auxiliary Loss):\nGShard 也使用了辅助损失函数来平衡专家负载。与原始 MoE 模型的辅助损失不同，GShard 的辅助损失旨在最小化每个专家网络路由到的数据比例的均方误差，更加直接地衡量专家负载平衡程度。\n随机路由(Random Routing):\n为了增加路由的随机性，GShard 在选择 top-k 个专家网络时，引入了随机路由机制。除了选择最佳的 top-k 个专家网络外，GShard 还会以一定的概率随机选择次优的专家网络，增加专家网络的多样性，提高模型的泛化能力。\n下面是 GShard 的核心算法流程:\nFig. 7. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)\nSwitch Transformer Switch Transformer(Fedus et al. 2021) 是 Google 提出的一个参数量达到万亿级别的 MoE 模型。其核心创新是将 Transformer 模型中的密集前馈网络(FFN) 层替换为稀疏的 Switch FFN 层。与 GShard 的 Top-2 Gating 不同，Switch Transformer 每个输入 token 只路由到一个专家网络，具有更高的稀疏性，进一步降低了计算成本，使得训练万亿参数模型成为可能。鼓励 token 路由在 $N$ 个专家之间更加均衡。Switch Transformer 的辅助损失基于实际路由比例与预测路由概率的乘积累加，具体公式如下：\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ 参数说明： $N$：专家的总数。\n$f_i$：路由到第 $i$ 个专家的 token 比例，定义为：\n$$ f_i = \\frac{1}{T} \\sum_{x \\in B} 1\\{\\text{argmax } p(x) = i\\} $$ $P_i$：gating 网络预测的第 $i$ 个专家的路由概率，定义为：\n$$ P_i = \\frac{1}{T} \\sum_{x \\in B} p_i(x) $$ $T$：批次 $B$ 中的 token 总数。\n$\\alpha$：辅助损失的权重超参数，通常设为 $10^{-2}$。\n通过最小化 $\\text{loss}$，模型使实际路由比例 $f_i$ 与预测概率 $P_i$ 趋于一致，从而间接促进专家间的负载平衡，避免部分专家闲置。\nFig. 8. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)\nSwitch Router 机制:\n路由预测:\n对于输入 token $x$，Switch Router 预测每个专家网络的路由概率 $p_i = G^{(i)}(x)$，其中 $i = 1, 2, \u0026hellip;, n$，n 是专家网络数量。\n专家选择:\n选择路由概率最高的专家网络作为最佳专家网络。Switch Transformer 采用 Top-1 路由策略，即每个 token 只路由到路由概率最高的专家网络。\ntoken 路由:\n将输入 token $x$ 路由到选择的最佳专家网络进行处理。\nSwitch Transformer 的训练稳定性优化:\n为提升 Switch Transformer 的训练稳定性，论文提出了如下优化策略：\n选择性精度(Selective Precision)\n在路由函数内部采用 FP32 精度既能提高训练稳定性，又能避免因 FP32 张量通信而产生的额外开销。具体来说，Switch Router 的计算过程全程使用 FP32，最终结果再转换为 FP16 以兼顾效率与精度。\n更小初始化(Smaller Initialization)\n建议将 Transformer 的权重初始化尺度参数 $s$ 从 1 调整至 0.1。较小的初始化尺度有助于缓解训练初期的梯度爆炸风险，从而提升整体训练稳定性。具体实现为：从均值为 0、标准差为 $\\sqrt{s/n}$(其中 $n$ 为输入单元数) 的截断正态分布中采样。\n更高专家 Dropout(Higher Expert Dropout)\n在专家 FFN 层中采用较高的 dropout 率(例如 0.4)，而在非专家层则保持较低的 dropout 率(例如 0.1)，这种设置能有效防止过拟合，进而增强模型的泛化能力。下图实验结果显示，在 GLUE、CNNDM、SQuAD 和 SuperGLUE 等任务上，当专家层 dropout 率设为 0.4 时，模型表现最佳。\nFig. 9. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: Fedus et al. 2021)\nSwitch Transformers 论文中使用下图直观的展示了使用不同的并行技术如何分割模型权重和数据:\nFig. 10. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)\n专家选择 专家选择(Expert Choice, EC)(Zhou et al. 2022) 是一种与 token 选择路由(如 GShard 的 top-2 或 Switch Transformer 的 top-1)相反的路由策略。在 token 选择路由中，每个 token 从所有专家中选择 top-k 个进行路由；而在专家选择路由中，每个专家从所有 token 中挑选 top-k 个进行处理。这种方法旨在解决 token 选择路由中的负载不均和 token 浪费问题，同时显著提高训练效率。下面是具体的计算过程：\n计算 token-to-expert 亲和度分数\n对于输入矩阵 $X \\in \\mathbb{R}^{n \\times d}$，计算 token-to-expert 亲和度分数矩阵 $S \\in \\mathbb{R}^{n \\times e}$ 的过程为：\n$$ S = \\text{softmax}(X \\cdot W_g), \\quad \\text{where } W_g \\in \\mathbb{R}^{d \\times e}. $$ 这里，$W_g$ 为门控权重矩阵，$e$ 为专家数量。\n专家选择 token\n每个专家从所有 token 中选择 top-k 个进行处理。通过对 $S^T$ 进行 top-k 选择：\n$$ G, I = \\text{top-}k(S^T, k), $$得到：\n门控矩阵 $G \\in \\mathbb{R}^{e \\times k}$： 记录专家选择的 token 对应的路由权重，其中 $G[i, j]$ 表示专家 $i$ 选择的第 $j$ 个 token 的权重； token 索引矩阵 $I \\in \\mathbb{R}^{e \\times k}$： 表示每个专家选择的 token 在输入中的索引。 One-hot 编码\n将 token 索引矩阵 $I$ 转换为 one-hot 编码矩阵 $P \\in \\mathbb{R}^{e \\times k \\times n}$，用于后续计算：\n$$ P = \\operatorname{one}-\\operatorname{hot}(I) $$ 构造 Gated FFN 层输入\n对于每个专家 $i$，其 gated FFN 层的输入为：\n$$ (P \\cdot X) \\in \\mathbb{R}^{e \\times k \\times d}. $$ EC 通过正则化限制每个 token 被路由到的专家数量，从而控制模型的稀疏性。一个常见的正则化目标如下：\n$$ \\begin{aligned} \u0026 \\max_{A} \\langle S^{\\top}, A \\rangle + \\lambda H(A) \\\\ \u0026 \\text{s.t. } \\forall i: \\sum_{j'} A[i, j'] = k, \\quad \\forall j: \\sum_{i'} A[i', j] \\leq b, \\quad \\forall i,j: 0 \\leq A[i, j] \\leq 1, \\end{aligned} $$考虑的优化问题中定义了一个矩阵 $A$，其第 $i$ 行第 $j$ 列的元素表示第 $i$ 个专家是否选择了第 $j$ 个 token(取值 0 或 1)。由于该优化问题求解较为复杂，论文中采用 Dijkstra 算法(通过多次迭代获得近似解)来解决。\n参数 $b$ 通常由批量中 token 总数 $n$ 与容量因子决定，其中容量因子表示每个 token 平均使用的专家数量。大多数实验采用较高的容量因子，实验结果表明，即使在容量降低的情况下，EC 整体表现仍优于传统的 top-1 token 选择路由，尽管 capped expert choice 略微降低了微调性能。\nEC 的优势主要体现在以下两方面：\n完美负载均衡： 每个专家固定处理 $k$ 个 token，从而避免了部分专家过载而其他专家闲置的问题，实现了理想的负载均衡。 更高训练效率： 实验表明，EC 能将训练收敛速度提升约 2 倍，相较于传统 token 选择路由具有更高的效率。 但 EC 也存在以下局限性：\n批量大小要求： 由于 EC 对 batch size 有较高要求，因此不适用于较小 batch size 的场景。 自回归生成限制： 在自回归文本生成任务中，由于无法预知未来 token，EC 的 top-k 选择无法实现，因此不适用于此类任务。 DeepSeekMoE 混合专家（MoE）模型通过将计算路由到特定的“专家”子网络来提高效率和性能。DeepSeek-V2 和 V3 在其 FFN（前馈网络）层中采用了名为 DeepSeekMoE (Dai et al., 2024)架构 。与 GShard 等传统 MoE 架构相比，DeepSeekMoE 的核心思想是：\n细粒度专家切分 (Fine-grained Expert Segmentation): 将专家网络切分得更小。这旨在实现更高的专家特化能力和更精确的知识获取，因为每个专家可以专注于更窄的领域。 共享专家隔离 (Shared Expert Isolation): 架构中包含一部分“共享专家”，这些专家由所有 token 处理，旨在处理通用知识。这减少了需要路由的“路由专家”之间的知识冗余，让路由专家可以更专注于特定知识。 基本架构 对于 FFN 层的输入 token 表示 \\(\\mathbf{u}_t\\)，DeepSeekMoE 的输出 \\(\\mathbf{h}_t'\\) 通过结合共享专家和选定的路由专家的输出来计算： \\[ \\mathbf{h}_{t}^{\\prime} = \\mathbf{u}_{t} + \\sum_{i=1}^{N_{s}} \\operatorname{FFN}_{i}^{(s)}(\\mathbf{u}_{t}) + \\sum_{i=1}^{N_{r}} g_{i, t} \\operatorname{FFN}_{i}^{(r)}(\\mathbf{u}_{t}), \\] 其中：\n\\(N_s\\) 是共享专家的数量。 \\(N_r\\) 是路由专家的数量。 \\(\\operatorname{FFN}_i^{(s)}\\) 是第 \\(i\\) 个共享专家网络。 \\(\\operatorname{FFN}_i^{(r)}\\) 是第 \\(i\\) 个路由专家网络。 \\(g_{i, t}\\) 是第 \\(t\\) 个 token 分配给第 \\(i\\) 个路由专家的门控值（权重）。 门控值 \\(g_{i,t}\\) 的计算方式是 DeepSeek-V2 和 V3 之间的关键区别之一，它基于 token-to-expert 的亲和度分数 \\(s_{i,t}\\)，并通过 Top-K 路由机制选择激活哪些专家。\nV2 与 V3 的门控机制和负载均衡对比 MoE 模型的一个核心挑战是负载均衡：确保所有专家都能得到有效利用，避免某些专家过载而其他专家空闲，这会影响训练稳定性和计算效率。DeepSeek-V2 和 V3 在门控机制和负载均衡策略上采取了不同的方法。\n1. 亲和度计算 (\\(s_{i,t}\\)) 与 Top-K 选择:\nDeepSeek-V2: 使用 Softmax 函数计算每个 token 对每个路由专家的亲和度分数。Top-K 选择直接基于这些亲和度分数 \\(s_{i,t}\\)。 \\[ s_{i, t} = \\operatorname{Softmax}_{i}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i}) \\] 其中 \\(\\mathbf{e}_i\\) 是第 \\(i\\) 个路由专家的可学习中心向量。选择 \\(s_{i,t}\\) 最高的 \\(K_r\\) 个专家。\nDeepSeek-V3: 使用 Sigmoid 函数计算亲和度分数。更重要的是，它引入了一个可学习的偏置项 \\(b_i\\) 用于每个路由专家。Top-K 选择是基于 加偏置后的亲和度 \\(s_{i,t} + b_i\\)。 \\[ s_{i, t} = \\operatorname{Sigmoid}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i}) \\] 选择依据是 \\(s_{i,t} + b_i\\) 值最高的 \\(K_r\\) 个专家。\n2. 门控值计算 (\\(g_{i,t}\\)):\nDeepSeek-V2: 对于被 Top-K 选中的专家，其门控值 \\(g_{i,t}\\) 直接等于其原始亲和度分数 \\(s_{i,t}\\)。对于未被选中的专家，\\(g_{i,t} = 0\\)。 \\[ g_{i, t}^{\\prime} = \\begin{cases} s_{i, t}, \u0026 s_{i, t} \\in \\operatorname{Topk}(\\{s_{j, t}\\}, K_{r}), \\\\ 0, \u0026 \\text{otherwise}, \\end{cases} \\] \\[ g_{i, t} = g_{i, t}^{\\prime} \\quad (\\text{V2 中不进行额外归一化}) \\] DeepSeek-V3: 对于基于 \\(s_{i,t} + b_i\\) 被 Top-K 选中的专家，其门控值 \\(g_{i,t}\\) 是通过对这些被选中专家的 原始亲和度 \\(s_{i,t}\\) 进行归一化得到的。偏置 \\(b_i\\) 仅用于路由选择，不影响最终的加权求和。 \\[ g_{i, t}^{\\prime}= \\begin{cases} s_{i, t}, \u0026 s_{i, t}+b_{i} \\in \\operatorname{Topk}\\left(\\left\\{s_{j, t}+b_{j} \\mid 1 \\leqslant j \\leqslant N_{r}\\right\\}, K_{r}\\right) \\\\ 0, \u0026 \\text{otherwise.} \\end{cases} \\] \\[ g_{i, t} = \\frac{g_{i, t}^{\\prime}}{\\sum_{j=1}^{N_{r}} g_{j, t}^{\\prime}} \\quad (\\text{对选中的专家亲和度进行归一化}) \\] 3. 负载均衡策略:\nDeepSeek-V2:\n主要策略：辅助损失 V2 引入了多种辅助损失项来显式地鼓励负载均衡: 专家级平衡损失 (\\(\\mathcal{L}_{\\text{ExpBal}}\\)): 鼓励每个专家处理大致相等数量的 token。 \\[ \\begin{aligned} \\mathcal{L}_{\\text{ExpBal}} \u0026= \\alpha_{1} \\sum_{i=1}^{N_{r}} f_{i} P_{i} \\\\ f_{i} \u0026= \\frac{N_{r}}{K_{r} T} \\sum_{t=1}^{T} \\mathbb{1}(\\text{Token } t \\text{ selects Expert } i) \\\\ P_{i} \u0026= \\frac{1}{T} \\sum_{t=1}^{T} s_{i, t} \\end{aligned} \\] 其中 \\(T\\) 是 batch 中的 token 总数，\\(f_i\\) 是路由到专家 \\(i\\) 的 token 比例（相对于理想均衡状态），\\(P_i\\) 是专家 \\(i\\) 的平均亲和度分数，\\(\\alpha_1\\) 是超参数。 设备级平衡损失 (\\(\\mathcal{L}_{\\text{DevBal}}\\)): 鼓励将计算负载均匀分布到不同的设备组上（假设专家分布在 \\(D\\) 个设备组 \\(\\{\\mathcal{E}_1, \\dots, \\mathcal{E}_D\\}\\)）。 \\[ \\begin{aligned} \\mathcal{L}_{\\text{DevBal}} \u0026= \\alpha_{2} \\sum_{i=1}^{D} f_{i}^{\\prime} P_{i}^{\\prime} \\\\ f_{i}^{\\prime} \u0026= \\frac{1}{|\\mathcal{E}_{i}|} \\sum_{j \\in \\mathcal{E}_{i}} f_{j} \\\\ P_{i}^{\\prime} \u0026= \\sum_{j \\in \\mathcal{E}_{i}} P_{j} \\end{aligned} \\] 其中 \\(f_i'\\) 是设备组 \\(i\\) 的平均负载分数，\\(P_i'\\) 是设备组 \\(i\\) 的总亲和度，\\(\\alpha_2\\) 是超参数。 通信平衡损失 (\\(\\mathcal{L}_{\\text{CommBal}}\\)): 鼓励发送到每个设备的 token 数量大致相等，以平衡 All-to-All 通信负载。 \\[ \\begin{aligned} \\mathcal{L}_{\\text{CommBal}} \u0026= \\alpha_{3} \\sum_{i=1}^{D} f_{i}^{\\prime \\prime} P_{i}^{\\prime \\prime} \\\\ f_{i}^{\\prime \\prime} \u0026= \\frac{D}{M T} \\sum_{t=1}^{T} \\mathbb{1}(\\text{Token } t \\text{ is sent to Device } i) \\\\ P_{i}^{\\prime \\prime} \u0026= \\sum_{j \\in \\mathcal{E}_{i}} P_{j} \\end{aligned} \\] 其中 \\(f_i''\\) 是发送到设备 \\(i\\) 的 token 比例（相对于理想均衡状态），\\(P_i''\\) 是设备组 \\(i\\) 的总亲和度，\\(\\alpha_3\\) 是超参数。 路由限制：设备限制路由 限制每个 token 最多只能路由到分布在 \\(M\\) 个不同设备上的专家。V2 中设 \\(M=3\\)。 Token 丢弃: 在训练期间，如果某个设备接收到的 token 数量超过了预设的容量因子（通常略大于平均值），则会丢弃一部分具有最低路由权重（亲和度）的 token，以避免计算资源的浪费。但会保留约 10% 序列的 token 不被丢弃。 DeepSeek-V3:\n主要策略：无辅助损失的负载均衡 (Auxiliary-Loss-Free Load Balancing) V3 认为辅助损失会损害模型性能，因此采用了一种创新的无辅助损失的负载均衡(Wang et al., 2024)。它通过动态调整前面提到的可学习偏置项 \\(b_i\\) 来实现负载均衡： 偏置更新: 在每个训练步骤之后，监控每个专家 \\(i\\) 在当前 batch 中处理的 token 数量。 如果专家 \\(i\\) 过载（处理的 token 数 \u0026gt; Batch 总 token 数 / \\(N_r\\)），则降低其偏置：\\(b_i \\leftarrow b_i - \\gamma\\)。 如果专家 \\(i\\) 欠载（处理的 token 数 \u0026lt; Batch 总 token 数 / \\(N_r\\)），则增加其偏置：\\(b_i \\leftarrow b_i + \\gamma\\)。 \\(\\gamma\\) 是一个小的正步长（偏置更新速率超参数）。通过这种方式，负载高的专家在后续路由中被选中的概率会降低，负载低的专家被选中的概率会增加，从而在批处理级别上动态平衡负载。 补充策略：序列级辅助损失 (\\(\\mathcal{L}_{\\text{Bal}}\\)) V3 仍然保留了一个权重极小 (\\(\\alpha=0.0001\\)) 的辅助损失，但它作用于单个序列内部的专家选择平衡，而不是整个 batch。这主要是为了防止在单个序列中出现极端不平衡的情况。 \\[ \\begin{gathered} \\mathcal{L}_{\\text{Bal}} = \\alpha \\sum_{i=1}^{N_{r}} f_{i} P_{i}, \\\\ f_{i} = \\frac{N_{r}}{K_{r} T_{seq}} \\sum_{t=1}^{T_{seq}} \\mathbb{1}\\left(s_{i, t} \\in \\operatorname{Topk}\\left(\\left\\{s_{j, t} \\mid 1 \\leqslant j \\leqslant N_{r}\\right\\}, K_{r}\\right)\\right), \\\\ s_{i, t}^{\\prime} = \\frac{s_{i, t}}{\\sum_{j=1}^{N_{r}} s_{j, t}}, \\quad P_{i} = \\frac{1}{T_{seq}} \\sum_{t=1}^{T_{seq}} s_{i, t}^{\\prime} \\end{gathered} \\] 注意这里的 \\(f_i, P_i\\) 是在单个序列（长度为 \\(T_{seq}\\)）上计算的，并且 \\(s_{i,t}'\\) 是在序列内对原始 \\(s_{i,t}\\) 归一化后的值。 路由限制：节点限制路由 类似于 V2 的设备限制，但应用于节点级别。V3 中设 \\(M=4\\)。 无 Token 丢弃: 由于基于偏置调整的负载均衡效果良好，V3 在训练和推理过程中均不丢弃任何 token。 V3 策略的优势: V3 的无辅助损失策略旨在最小化负载均衡机制对模型最终性能的负面影响。通过动态调整偏置项进行批处理级别的负载均衡，相比 V2 中基于辅助损失的序列级均衡，约束更宽松。这允许专家在不同领域展现出更强的特化模式，因为路由决策不必在每个序列内部都严格遵循均衡分布。下图实验表明该策略在多个基准测试上优于基于辅助损失的方法。\nFig. 11. Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. (Image source: DeepSeek-AI, 2024)\n辅助损失无关的负载均衡与序列级辅助损失的关键区别在于它们的均衡范围：批次级与序列级。相比序列级辅助损失，批次级均衡施加了更灵活的约束，因为它不强制每个序列内的领域平衡。这种灵活性使得专家能够更好地在不同领域中进行专门化。为了验证这一点，图中记录并分析了在 Pile 测试集不同领域上，基于辅助损失的 16B 基线模型和无辅助损失的 16B 模型的专家负载，可以观察到无辅助损失模型表现出更明显的专家专门化模式，符合预期。\nFig. 12. Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. (Image source: DeepSeek-AI, 2024)\nDeepSeekMoE V2 vs V3 对比总结表 特性 DeepSeek-V2 DeepSeek-V3 亲和度计算 \\(s_{i,t}\\) \\(\\operatorname{Softmax}_{i}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i})\\) \\(\\operatorname{Sigmoid}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i})\\) TopK 选择依据 原始亲和度 \\(s_{i,t}\\) 加偏置后的亲和度 \\(s_{i,t} + b_i\\) 门控值计算 \\(g_{i,t}\\) 对选中的专家，\\(g_{i,t} = s_{i,t}\\) (通常无额外归一化) 对选中的专家，基于原始亲和度 \\(s_{i,t}\\) 进行归一化: \\(g_{i, t} = \\frac{s_{i, t}}{\\sum_{j \\in \\text{Selected}} s_{j, t}}\\) 主要负载均衡策略 辅助损失: - \\(\\mathcal{L}_{\\text{ExpBal}}\\) (专家级) - \\(\\mathcal{L}_{\\text{DevBal}}\\) (设备级) - \\(\\mathcal{L}_{\\text{CommBal}}\\) (通信级) 无辅助损失: - 通过动态调整可学习偏置项 \\(b_i\\) (步长 \\(\\gamma\\)) 实现批处理级均衡 补充负载均衡 无明确的补充策略 序列级辅助损失 \\(\\mathcal{L}_{\\text{Bal}}\\) (权重 \\(\\alpha\\) 极小, e.g., 0.0001)，防止单序列内极端不平衡 路由限制 设备限制: 每个 token 最多路由到 \\(M=3\\) 个设备上的专家 节点限制: 每个 token 最多路由到 \\(M=4\\) 个节点上的专家 Token 丢弃 是: 训练时，为缓解计算瓶颈，会丢弃超出设备容量的 token 中亲和度最低的部分 (保留约10%序列不丢弃) 否: 训练和推理中均不丢弃 token 均衡粒度 主要通过辅助损失在序列/Batch 级别强制均衡 主要通过偏置调整在 Batch 级别动态均衡，约束更宽松 对模型性能影响 辅助损失可能对模型性能产生负面影响 设计上旨在最小化均衡策略对性能的负面影响，允许更好的专家特化 多 token 预测 (MTP) 为了进一步提升模型性能和数据效率，DeepSeek-V3 引入了多 token 预测 (Multi-Token Prediction, MTP) 训练目标 (Gloeckle et al., 2024)。标准的语言模型只预测下一个 token，而 MTP 让模型在每个位置预测未来多个（V3 中是 \\(D_{MTP}=1\\)，即预测下下个 token）token。\nMTP 实现 MTP 通过 \\(D_{MTP}\\) 个顺序模块实现。第 \\(k\\) 个 MTP 模块 (\\(k=1, \\dots, D_{MTP}\\)) 包含：\n共享的嵌入层 \\(\\operatorname{Emb}(\\cdot)\\) 共享的输出头 \\(\\operatorname{OutHead}(\\cdot)\\) 独立的 Transformer 块 \\(\\operatorname{TRM}_k(\\cdot)\\) 独立的投影矩阵 \\(M_k \\in \\mathbb{R}^{d \\times 2d}\\) 对于输入序列中的第 \\(i\\) 个 token \\(t_i\\)，在第 \\(k\\) 个预测深度：\n将第 \\(i\\) 个 token 在第 \\(k-1\\) 深度的表示 \\(\\mathbf{h}_i^{k-1}\\)（\\(k=1\\) 时为主模型输出）与第 \\(i+k\\) 个 token 的嵌入 \\(\\operatorname{Emb}(t_{i+k})\\) 拼接，并通过投影矩阵 \\(M_k\\) 得到组合表示 \\(\\mathbf{h}_i^{\\prime k}\\)： \\[ \\mathbf{h}_{i}^{\\prime k} = M_{k}[\\operatorname{RMSNorm}(\\mathbf{h}_{i}^{k-1}) ; \\operatorname{RMSNorm}(\\operatorname{Emb}(t_{i+k}))] \\] 将组合表示输入到第 \\(k\\) 个 Transformer 块，得到当前深度的输出表示 \\(\\mathbf{h}_i^k\\)： \\[ \\mathbf{h}_{1: T-k}^{k} = \\operatorname{TRM}_{k}(\\mathbf{h}_{1: T-k}^{\\prime k}) \\] 使用共享输出头预测第 \\(i+k+1\\) 个 token 的概率分布 \\(P_{i+k+1}^k \\in \\mathbb{R}^V\\)： \\[ P_{i+k+1}^{k} = \\operatorname{OutHead}(\\mathbf{h}_{i}^{k}) \\] 关键在于，这种实现方式保持了每个预测深度的完整因果链，与并行预测多个 token 的方法不同。\nFig. 13. Illustration of our Multi-Token Prediction (MTP) implementation. They keep the complete causal chain for the prediction of each token at each depth. (Image source: DeepSeek-AI, 2024)\nMTP 训练目标 计算每个预测深度 \\(k\\) 的交叉熵损失 \\(\\mathcal{L}_{\\text{MTP}}^k\\)： \\[ \\mathcal{L}_{\\text{MTP}}^{k} = \\operatorname{CrossEntropy}(P_{2+k: T+1}^{k}, t_{2+k: T+1}) = -\\frac{1}{T} \\sum_{i=2+k}^{T+1} \\log P_{i}^{k}[t_{i}] \\] 总的 MTP 损失是所有深度损失的加权平均： \\[ \\mathcal{L}_{\\text{MTP}} = \\frac{\\lambda}{D_{MTP}} \\sum_{k=1}^{D_{MTP}} \\mathcal{L}_{\\text{MTP}}^{k} \\] 其中 \\(\\lambda\\) 是权重因子（V3 中前期为 0.3，后期为 0.1）。这个损失会加到主模型的标准 next-token prediction 损失上。\nMTP 推理 MTP 主要用于提升主模型性能。推理时，可以直接丢弃 MTP 模块，主模型可以独立工作。或者也可以利用 MTP 模块进行投机解码 (Leviathan et al., 2023; Xia et al., 2023)，以加速生成过程。V3 的实验表明，第二个 token 的接受率在 85%-90% 之间，可将解码速度提升约 1.8 倍。\n基础设施与训练效率 DeepSeek-V3 的高效训练与部署得益于算法、框架和硬件的协同设计。\n计算集群 DeepSeek-V3 在一个配备了 2048 块 NVIDIA H800 GPU 的集群上进行训练。\n节点内部: 每个节点包含 8 块 H800 GPU，通过 NVLink 和 NVSwitch 高速互联。 节点之间: 不同节点间使用 InfiniBand (IB) 网络进行通信。 训练框架 DeepSeek-V3 的训练基于自研的高效轻量级框架 HAI-LLM。整体上采用了：\n16 路流水线并行 (Pipeline Parallelism, PP) (Qi et al., 2023) 64 路专家并行 (Expert Parallelism, EP) (跨 8 节点) (Lepikhin et al., 2021) ZeRO-1 数据并行 (Data Parallelism, DP) (Rajbhandari et al., 2020) 为了实现高效训练，Deepseek 进行了细致的工程优化：\n设计了 DualPipe 算法以实现高效流水线并行，减少气泡并重叠计算与通信，解决了跨节点专家并行带来的重通信开销问题。 开发了高效的跨节点 All-to-all 通信 Kernel，充分利用 IB 和 NVLink 带宽，并节省用于通信的 SM 资源。 细致优化了训练过程中的内存占用，使得可以在不使用张量并行 (Tensor Parallelism, TP) 的情况下训练 DeepSeek-V3。 DualPipe 与计算通信重叠 挑战: 跨节点专家并行导致计算通信比接近 1:1，效率不高。 Fig. 17. Overlapping strategy for a pair of forward and backward chunks with misaligned transformer block boundaries. Orange: forward, green: backward for input, blue: backward for weights, purple: PP communication, red: barriers. Both all-to-all and PP communications are fully hidden. (Image source: DeepSeek-AI, 2024)\n核心思想: 重叠一对独立的前向和反向 chunk 内的计算和通信。将每个 chunk 分解为 Attention、All-to-all Dispatch、MLP、All-to-all Combine 四个组件（反向的 Attention 和 MLP 进一步细分为 backward for input 和 backward for weights，类似 ZeroBubble (Qi et al., 2023）。通过重排这些组件并手动调整用于通信与计算的 GPU SM 比例，实现 All-to-all 和 PP 通信的完全隐藏。 调度: 采用双向流水线调度，同时从流水线的两端输入微批次，大部分通信可以被完全重叠。 Fig. 18. Example DualPipe scheduling with 8 PP ranks and 20 micro-batches in both directions. The reverse-direction micro-batches mirror the forward ones, so their batch IDs are omitted for simplicity. Two cells within a shared black border represent mutually overlapped computation and communication. (Image source: DeepSeek-AI, 2024)\n优势: 即使在没有重通信负担的一般场景下也具有效率优势。 相比 ZB1P (Qi et al., 2023) 和 1F1B (Harlap et al., 2018)，显著减少流水线气泡，仅增加 \\(\\frac{1}{PP}\\) 倍的峰值激活内存。 虽然需要两份模型参数副本，但由于训练中使用了大的 EP size，内存增加不显著。 相比 Chimera (Li and Hoefler, 2021)，对微批次数量的要求更宽松（只需能被 2 整除），且气泡和激活内存不随微批次数量增加而增长。 方法 (Method) 气泡 (Bubble) 参数 (Parameter) 激活 (Activation) 1F1B \\((PP-1)(F+B)\\) \\(1 \\times\\) \\(PP\\) ZB1P \\((PP-1)(F+B-2W)\\) \\(1 \\times\\) \\(PP\\) DualPipe (Deepseek V3) \\(\\left(\\frac{PP}{2}-1\\right)(F\\\u0026B+B-3W)\\) \\(2 \\times\\) \\(PP+1\\) 上表对不同流水线并行方法的流水线气泡和内存使用比较。\\(F\\)：前向 chunk 执行时间；\\(B\\)：完整反向 chunk 执行时间；\\(W\\)：“权重反向” chunk 执行时间；\\(F\\\u0026B\\)：两个相互重叠的前向和反向 chunk 的执行时间。\n高效的跨节点 All-to-All 通信实现 目标: 为 DualPipe 提供足够的计算性能，定制高效的跨节点 All-to-all 通信 Kernel (dispatching \u0026amp; combining)，节省通信专用 SM。 策略: 结合 MoE 门控算法和集群网络拓扑（节点间 IB 全互联，节点内 NVLink）。 带宽利用: NVLink 带宽 (\\(160 \\mathrm{~GB} / \\mathrm{s}\\)) 约是 IB (\\(50 \\mathrm{~GB} / \\mathrm{s}\\)) 的 3.2 倍。限制每个 token 最多分发到 4 个节点以减少 IB 流量。 传输路径: Token 确定路由后，先通过 IB 传输到目标节点上具有相同节点内索引的 GPU。到达目标节点后，立即通过 NVLink 转发给托管目标专家的特定 GPU，避免被后续到达的 token 阻塞。 效果: IB 和 NVLink 通信完全重叠。每个 token 可高效选择平均 3.2 个专家/节点，无需额外 NVLink 开销。这意味着 V3 实际选择 8 个路由专家，但理论上可扩展到 13 个专家 (4 nodes × 3.2 experts/node) 而通信成本不变。 实现: 使用 Warp Specialization (Bauer et al., 2014)技术 ，将 20 个 SM 划分为 10 个通信通道。 Dispatch 过程：IB 发送、IB-to-NVLink 转发、NVLink 接收由各自的 warp 处理，warp 数量根据负载动态调整。 Combine 过程：NVLink 发送、NVLink-to-IB 转发与累加、IB 接收与累加也由动态调整的 warp 处理。 优化: Dispatch 和 Combine Kernel 与计算流重叠。使用定制 PTX 指令并自动调整通信 chunk 大小，显著减少 L2 缓存使用和对其他 SM 计算 Kernel 的干扰。 结果: 仅需 20 个 SM 即可充分利用 IB 和 NVLink 带宽。 极致内存优化与最小开销 为减少训练内存占用，采用了以下技术：\n重计算: 在反向传播中重计算所有 RMSNorm 操作和 MLA 的上投影，避免存储它们的输出激活值。以微小开销显著减少激活内存需求。 CPU 存储 EMA: 将模型参数的指数移动平均 (EMA) 保存在 CPU 内存中，并在每个训练步骤后异步更新。无需额外 GPU 内存或时间开销即可维护 EMA 参数。 共享 Embedding 和输出头: 利用 DualPipe 策略，将模型最浅层（含 Embedding 层）和最深层（含输出头）部署在同一个 PP rank 上。这使得 MTP 模块和主模型可以物理共享 Embedding 和输出头的参数及梯度，进一步提升内存效率。 效果: 这些优化使得 DeepSeek-V3 可以在不使用昂贵的张量并行 (TP) 的情况下进行训练。 FP8 训练 为了加速训练并减少显存占用，DeepSeek-V3 采用了 FP8 混合精度训练框架 (Dettmers et al., 2022; Noune et al., 2022; Peng et al., 2023)，并在超大规模模型上首次验证了其有效性。\n混合精度框架 核心计算 (GEMM): 大部分 GEMM 操作（前向、激活梯度反向、权重梯度反向）使用 FP8 输入，输出 BF16 或 FP32，理论上计算速度翻倍。 高精度保留: 对精度敏感或计算开销小的部分（如 Embedding、输出头、MoE 门控、Normalization、Attention）保留 BF16/FP32 精度。 高精度存储: 主权重、权重梯度、优化器状态（部分 BF16）使用更高精度存储，通过 ZeRO-1 分片降低显存压力。 Fig. 14. The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. (Image source: DeepSeek-AI, 2024)\n精度提升策略 细粒度量化: 为了解决 FP8 动态范围有限和对离群值敏感的问题 (Fishman et al., 2024; He et al., 2024; Sun et al., 2024)，采用更细粒度的量化： 激活: 按 \\(1 \\times 128\\) 的 tile 分组缩放。 权重: 按 \\(128 \\times 128\\) 的 block 分组缩放。 这种方法让缩放因子更适应局部数据的范围，减少量化误差。 提升累加精度: H800 的 Tensor Core 进行 FP8 GEMM 时累加精度有限（约 14 位）。为解决此问题，采用 Promotion to CUDA Cores 策略 (Thakkar et al., 2023)：Tensor Core 计算部分累加和（例如每 \\(N_C=128\\) 个元素），然后将结果传输到 CUDA Core 的 FP32 寄存器中进行全精度累加。细粒度量化的缩放因子也可以在 CUDA Core 上高效应用。通过 WGMMA 操作的并发执行，这种方法在提升精度的同时，对计算效率影响较小。 E4M3 格式: V3 在所有张量上统一使用 E4M3 格式（4 位指数，3 位尾数），而非混合使用 E5M2 (NVIDIA, 2024; Peng et al., 2023; Sun et al., 2019b)。细粒度量化策略有效缓解了 E4M3 动态范围较小的问题。 在线量化: 实时计算每个 tile/block 的最大绝对值来确定缩放因子，而非依赖历史值 (NVIDIA, 2024; Peng et al., 2023)，确保量化精度。 Fig. 15. (a) Fine-grained quantization method to mitigate quantization errors. (b) Improved FP8 GEMM precision by promoting to CUDA Cores for high-precision accumulation. (Image source: DeepSeek-AI, 2024)\n低精度存储与通信 优化器状态: AdamW (Loshchilov and Hutter, 2017) 的一阶和二阶矩使用 BF16 存储。主权重和梯度累加仍用 FP32。 激活缓存: Wgrad 操作使用 FP8 输入，因此激活值可以缓存为 FP8。对特定敏感操作（如 Attention 后 Linear 的输入）使用定制的 E5M6 格式，并进行 round scaling。MoE 中 SwiGLU 的输入也缓存为 FP8。 通信: MoE up-projection 前的激活量化为 FP8 进行分发（dispatch），MoE down-projection 前的激活梯度也量化为 FP8。Combine 操作保留 BF16 精度。 下图实验证明，FP8 训练的损失与 BF16 相比，相对误差低于0.25%，在可接受范围内。\nFig. 16. Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9 DeepSeek-AI, 2024)\n推理与部署 DeepSeek-V3 部署在 H800 集群上（节点内 NVLink，节点间 IB 全互联）。为同时保证在线服务的 SLO (服务等级目标) 和高吞吐量，采用分离 Prefilling (预填充) 和 Decoding (解码) 阶段的部署策略。\n预填充阶段 最小部署单元: 4 节点 32 GPU。 并行策略: Attention 部分: TP4 (张量并行) + SP (序列并行) 结合 DP8 (数据并行)。较小的 TP size (4) 限制了 TP 通信开销。 MoE 部分: EP32 (专家并行)，确保每个专家处理足够大的批次以提高计算效率。 浅层 Dense MLP: 使用 TP1 节省 TP 通信。 MoE All-to-All 通信: 采用与训练类似的方法：先通过 IB 跨节点传输 token，然后在节点内 GPU 间通过 NVLink 转发。 负载均衡: 采用 冗余专家 的部署策略。 根据在线部署收集的统计数据，周期性（例如每 10 分钟）检测高负载专家并进行复制部署。 确定冗余专家集后，根据观察到的负载仔细地在节点内 GPU 间重排专家，在不增加跨节点 All-to-all 通信开销的前提下尽可能平衡 GPU 负载。 DeepSeek-V3 部署中，Prefilling 阶段设置 32 个冗余专家。每个 GPU 除了托管原有的 8 个专家外，还会额外托管 1 个冗余专家。 效率优化: 为提高吞吐并隐藏 All-to-all 和 TP 通信开销，同时处理两个计算负载相似的微批次，将一个微批次的 Attention 和 MoE 与另一个微批次的 Dispatch 和 Combine 重叠。 探索方向: 动态冗余 策略，每个 GPU 托管更多专家（如 16 个），但每次推理步骤只激活 9 个。在每层 All-to-all 操作开始前动态计算全局最优路由方案。由于 Prefilling 计算量大，计算路由方案的开销几乎可忽略。 解码阶段 专家视角: 将共享专家 视为一个路由目标。从这个角度看，每个 token 在路由时会选择 9 个专家（共享专家被视为一个总会被选中的高负载专家）。 最小部署单元: 40 节点 320 GPU。 并行策略: Attention 部分: TP4 + SP 结合 DP80。 MoE 部分: EP320。每个 GPU 只托管一个专家，其中 64 个 GPU 负责托管冗余专家和共享专家。 All-to-All 通信: Dispatch 和 Combine 部分通过 IB 直接点对点传输以实现低延迟。利用 IBGDA (NVIDIA, 2022) 技术进一步最小化延迟和提升通信效率。 负载均衡: 类似 Prefilling，根据在线服务的统计专家负载，周期性确定冗余专家集。但由于每个 GPU 只托管一个专家，无需重排。 探索方向: 动态冗余策略: 但需要更仔细地优化计算全局最优路由方案的算法，并与 Dispatch Kernel 融合以减少开销。 同时处理两个微批次: 与 Prefilling 不同，解码阶段 Attention 耗时占比更大。因此，将一个微批次的 Attention 与另一个微批次的 Dispatch+MoE+Combine 重叠。解码阶段每个专家的批次大小相对较小（通常小于 256 token），瓶颈是内存访问而非计算。由于 MoE 部分只需加载一个专家的参数，访存开销小，使用较少 SM 不会显著影响整体性能。因此，可以只分配一小部分 SM 给 Dispatch+MoE+Combine，避免影响 Attention 部分的计算速度。 对硬件设计的建议 DeepSeek 团队基于 All-to-all 通信和 FP8 训练方案的实现，向 AI 硬件供应商提出以下芯片设计建议。\n通信硬件 现状: 通过计算/通信重叠隐藏了通信延迟，显著降低了对通信带宽的依赖。但当前通信实现依赖昂贵的 SM（例如 H800 上分配了 132 个 SM 中的 20 个用于此目的），限制了计算吞吐量，且 SM 用于通信导致 Tensor Core 完全闲置，效率低下。 SM 主要任务: 在 IB 和 NVLink 域之间转发数据，同时将发往同一节点内多个 GPU 的 IB 流量从单个 GPU 聚合。 在 RDMA 缓冲区和输入/输出缓冲区之间传输数据。 为 All-to-all Combine 执行 Reduce 操作。 在跨 IB 和 NVLink 域向多个专家分块传输数据时管理细粒度内存布局。 期望: 未来供应商开发硬件，将这些通信任务从宝贵的计算单元 SM 卸载，作为 GPU 协处理器或网络协处理器其类似于 NVIDIA SHARP(Graham et al., 2016)。 为降低应用编程复杂性，期望该硬件能从计算单元的角度统一 IB (scale-out) 和 NVLink (scale-up) 网络。通过这个统一接口，计算单元可以通过提交基于简单原语的通信请求，轻松完成在整个 IB-NVLink 统一域内的读、写、多播和 Reduce 等操作。 计算硬件 Tensor Core 中更高精度的 FP8 GEMM 累加: 问题: 当前 NVIDIA Hopper 架构 Tensor Core 实现中，FP8 GEMM 使用定点累加，通过右移对齐尾数乘积再相加。实验表明，符号填充右移后仅使用每个尾数乘积的最高 14 位，超出范围的位被截断。然而，例如要从 32 个 FP8×FP8 乘积累加得到精确的 FP32 结果，至少需要 34 位精度。 建议: 未来芯片设计应增加 Tensor Core 中的累加精度以支持全精度累加，或根据训练和推理算法的精度要求选择合适的累加位宽。这种方法能在保持计算效率的同时确保误差在可接受范围内。 支持 Tile 和 Block 级量化: 问题: 当前 GPU 仅支持 per-tensor 量化，缺乏对 tile-wise 和 block-wise 等细粒度量化的原生支持。当前实现中，达到 \\(N_C\\) 间隔时，部分结果需从 Tensor Core 复制到 CUDA Core，乘以缩放因子，再加到 CUDA Core 的 FP32 寄存器上。虽然结合精确 FP32 累加策略显著缓解了反量化开销，但 Tensor Core 和 CUDA Core 间的频繁数据移动仍限制计算效率。 建议: 未来芯片应通过让 Tensor Core 能够接收缩放因子并实现带分组缩放的 MMA 来支持细粒度量化。这样，整个部分和累加与反量化可以在 Tensor Core 内部直接完成，直到产生最终结果，避免频繁数据移动。 支持在线量化: 问题: 当前实现难以有效支持在线量化，尽管其有效性已在研究中得到证明。现有流程中，需要从 HBM 读取 128 个 BF16 激活值（前一计算的输出）进行量化，量化后的 FP8 值写回 HBM，然后再次读取用于 MMA。 建议: 未来芯片将 FP8 类型转换和 TMA (Tensor Memory Accelerator) 访问融合成单一操作，使得量化可以在激活从全局内存传输到共享内存的过程中完成，避免频繁的内存读写。 推荐支持 warp 级 cast 指令以加速，进一步促进 Layer Normalization 和 FP8 cast 的更好融合。 或者，采用近存计算方法，在 HBM 附近放置计算逻辑。这样，BF16 元素可以在从 HBM 读入 GPU 时直接转换为 FP8，减少约 50% 的片外内存访问。 支持转置 GEMM 操作: 问题: 当前架构将矩阵转置与 GEMM 操作融合起来很麻烦。在工作流中，前向传播的激活被量化为 \\(1 \\times 128\\) 的 FP8 tile 并存储。反向传播时，需要读出矩阵，反量化，转置，重新量化为 \\(128 \\times 1\\) tile，再存入 HBM。 建议: 未来芯片应支持在 MMA 操作前直接从共享内存读取转置后的矩阵（针对训练和推理所需的那些精度）。结合 FP8 格式转换和 TMA 访问的融合，此增强将显著简化量化工作流。 训练成本与效率 DeepSeek-V2: 相比 DeepSeek 67B (Dense)，节省 42.5% 训练成本，KV 缓存减少 93.3%，最大吞吐量提升 5.76 倍。 DeepSeek-V3: 训练效率极高，每训练 1T token 仅需 180K H800 GPU 小时。总训练成本（预训练+上下文扩展+后训练）仅 2.788M H800 GPU 小时（约 558 万美元，按 2 美元/小时计）。预训练在 2048 卡 H800 集群上耗时不到 2 个月。 训练阶段 H800 GPU 小时 预估成本 (美元) 预训练 2664 K $5.328 M 上下文扩展 119 K $0.238 M 后训练 5 K $0.01 M 总计 2788 K $5.576 M 预训练 数据构建 相较于 DeepSeek‑V2（基于 67B 模型，使用 100K 词表 Byte‑level BPE Tokenizer，8.1T tokens），DeepSeek‑V3 在预训练阶段通过以下策略，实现了更大规模和更高质量的数据构建：\n语料库扩展与精炼\n专注领域：显著增加数学与编程相关文本占比，强化模型在技术领域的理解与生成能力。 多语言覆盖：在英语、中文之外，新增多种语种语料，提升跨语言泛化性能。 去重与多样性：采用高效的数据去重和过滤流程，既最大限度减少冗余，又保证内容多样性。 规模提升：最终构建了约 14.8T 高质量 tokens，比 V2 增长近 83%。 训练策略与技术创新\n文档打包 结合 Document Packing(Ding et al., 2024)方法，将连贯文本打包为更长片段，以提升 GPU 利用率和上下文完整性；未采用跨样本注意力掩码，保持实现简洁。 Fill‑in‑Middle（FIM）策略 动机：借鉴 DeepSeekCoder‑V2(DeepSeek‑AI, 2024)的方法，旨在提升模型对中间缺失信息的填充能力。 框架：引入 Prefix‑Suffix‑Middle (PSM) 结构 ，样例如下： \u0026lt;|fim_begin|\u0026gt; f_pre \u0026lt;|fim_hole|\u0026gt; f_suf \u0026lt;|fim_end|\u0026gt; f_middle \u0026lt;|eos_token|\u0026gt; 应用比例：文档级预打包前插入 FIM，占比 10%，平衡生成与预测任务。 Tokenizer 优化\nBBPE 词表扩容：采用 Byte‑level BPE，词表由 100K 扩至 128K，提升罕见词与专有名词覆盖。 预分词器改进：针对多语言场景，调整分词规则，提升压缩效率与编码一致性。 边界偏见缓解：参考 Lundberg, 2023 的方法为减少标点符号+换行组合 token 在 few‑shot 场景下的偏倚，引入随机拆分机制，让模型接触更多边界变体。 超参数 参数 DeepSeek-V2 DeepSeek-V3 Transformer 层数 60 61 隐藏层维度 \\(d\\) 5120 7168 初始化标准差 0.006 0.006 MLA 参数 注意力头数 \\(n_h\\) 128 128 每头维度 \\(d_h\\) 128 128 KV 压缩维度 \\(d_c\\) 512 (\\(4d_h\\)) 512 (\\(4d_h\\)) 查询压缩维度 \\(d_c'\\) 1536 1536 解耦 RoPE 维度 \\(d_h^R\\) 64 (\\(d_h/2\\)) 64 (\\(d_h/2\\)) DeepSeekMoE 参数 MoE 层位置 除第 1 层外 除前 3 层外 共享专家数 \\(N_s\\) 2 1 路由专家数 \\(N_r\\) 160 256 专家中间维度 1536 2048 激活专家数 \\(K_r\\) 6 8 设备/节点限制路由 \\(M\\) 3 (设备) 4 (节点) 负载均衡策略 辅助损失 (\\(\\alpha_1=0.003, \\alpha_2=0.05, \\alpha_3=0.02\\)) + Token Dropping 无辅助损失 (\\(\\gamma=0.001\\)) + 序列级损失 (\\(\\alpha=0.0001\\)) MTP 参数 (V3 only) 预测深度 \\(D_{MTP}\\) N/A 1 MTP 损失权重 \\(\\lambda\\) N/A 0.3 (前 10T) / 0.1 (后 4.8T) 训练参数 优化器 AdamW (\\(\\beta_1=0.9, \\beta_2=0.95, wd=0.1\\)) AdamW (\\(\\beta_1=0.9, \\beta_2=0.95, wd=0.1\\)) 最大序列长度 4K 4K 训练 Tokens 8.1T 14.8T 学习率 Warmup + Step Decay (Max \\(2.4 \\times 10^{-4}\\)) Warmup + Cosine Decay + Constant (Max \\(2.2 \\times 10^{-4}\\)) Batch Size 2304 -\u0026gt; 9216 3072 -\u0026gt; 15360 梯度裁剪 1.0 1.0 精度 BF16 FP8 混合精度 长上下文扩展 两者均使用 YaRN (Peng et al., 2023) 技术扩展上下文窗口。\nDeepSeek-V2: 从 4K 扩展到 128K。使用 YaRN (scale \\(s=40, \\alpha=1, \\beta=32\\))，在 32K 序列长度上训练 1000 步。调整了长度缩放因子 \\(\\sqrt{t}=0.0707 \\ln s+1\\)。 DeepSeek-V3: 分两阶段从 4K 扩展到 32K，再到 128K。每阶段训练 1000 步。YaRN 参数与 V2 相同，长度缩放因子 \\(\\sqrt{t}=0.1 \\ln s+1\\)。第一阶段序列长度 32K，第二阶段 128K。 两模型在 NIAH 测试中均表现出良好的长上下文能力。\nFig. 19. Evaluation results on the \u0026lsquo;Needle In A Haystack\u0026rsquo; (NIAH) tests for DeepSeek-V2. (Image source: DeepSeek-AI, 2024)\nFig. 20. Evaluation results on the \u0026lsquo;Needle In A Haystack\u0026rsquo; (NIAH) tests for DeepSeek-V3. (Image source: DeepSeek-AI, 2024)\n评估 DeepSeek-V2 评估结果:\nDeepSeek-V2 与代表性开源模型对比 (部分结果)。DeepSeek-V2 以 21B 激活参数达到当时顶尖水平。\nBenchmark (Metric) # Shots DeepSeek 67B Qwen1.5 72B Mixtral 8x22B LLaMA 3 70B DeepSeek-V2 # Activated Params - 67B 72B 39B 70B 21B English MMLU (Hendrycks et al., 2020) (Acc.) 5-shot 71.3 77.2 77.6 78.9 78.5 Code HumanEval (Chen et al., 2021) (Pass@1) 0-shot 45.1 43.9 53.1 48.2 48.8 Math GSM8K (Cobbe et al., 2021) (EM) 8-shot 63.4 77.9 80.3 83.0 79.2 Chinese C-Eval (Huang et al., 2023) (Acc.) 5-shot 66.1 83.7 59.6 67.5 81.7 DeepSeek-V3 评估结果:\nDeepSeek-V3-Base 与代表性开源模型对比 (部分结果)。DeepSeek-V3-Base 在多数基准上成为最强开源模型，尤其在代码和数学方面。\nBenchmark (Metric) # Shots DeepSeek-V2 Base Qwen2.5 72B Base LLaMA-3.1 405B Base DeepSeek-V3 Base # Activated Params - 21B 72B 405B 37B English MMLU (Hendrycks et al., 2020) (EM) 5-shot 78.4 85.0 84.4 87.1 MMLU-Pro (Wang et al., 2024) (em) 5-shot 51.4 58.3 52.8 64.4 Code HumanEval (Chen et al., 2021) (Pass@1) 0-shot 43.3 53.0 54.9 65.2 LiveCodeBench-Base (Jain et al., 2024) (Pass@1) 3-shot 11.6 12.9 15.5 19.4 Math GSM8K (Cobbe et al., 2021) (Em) 8-shot 81.6 88.3 83.5 89.3 MATH (Hendrycks et al., 2021) (EM) 4-shot 43.4 54.4 49.0 61.6 Chinese C-Eval (Huang et al., 2023) (EM) 5-shot 81.4 89.2 72.5 90.1 Multilingual MMMLU-non-English (OpenAI, 2024) (em) 5-shot 64.0 74.8 73.8 79.4 总结: DeepSeek-V3-Base 凭借其架构创新、更大规模的训练数据和高效的训练方法，全面超越了 DeepSeek-V2-Base 和其他顶尖开源模型（包括参数量远超其激活参数的 LLaMA-3.1 405B）。\n对齐 为了使模型更好地理解指令、遵循人类偏好并提升特定能力（如推理），DeepSeek-V2 和 V3 都进行了监督微调 (SFT) 和强化学习 (RL)。\n监督微调 DeepSeek-V2: 使用了约 1.5M 条指令数据，包含 1.2M 帮助性数据和 0.3M 安全性数据，注重提升数据质量以减少幻觉、增强写作能力。 DeepSeek-V3: 推理数据: 利用内部的 DeepSeek-R1 模型 (Guo et al., 2025) 生成推理过程（数学、代码、逻辑等）。由于 R1 输出可能过长或格式不佳，V3 采用了知识蒸馏的思路： 训练领域专家模型（如代码专家）：结合原始 SFT 数据和 R1 生成的长 CoT 数据（带有引导反思/验证的系统提示）进行 SFT+RL 训练。 使用专家模型生成 SFT 数据：专家模型在 RL 过程中学会融合 R1 的推理模式和常规 SFT 数据的简洁性。 拒绝采样：筛选高质量 SFT 数据用于最终 V3 的 SFT。 非推理数据: 使用 DeepSeek-V2.5 生成，并由人工标注员验证。 SFT 设置: 微调 2 个 epoch，学习率从 \\(5 \\times 10^{-6}\\) 余弦衰减到 \\(1 \\times 10^{-6}\\)。采用样本打包和掩码隔离。 强化学习 两者均采用 组相对策略优化 (Group Relative Policy Optimization, GRPO) 算法 (Shao et al., 2024) 进行 RL。GRPO 是一种 Actor-Only 的方法，它通过比较一组（\\(G\\) 个）候选输出的相对好坏来估计优势 \\(A_i\\)，从而避免了训练与策略模型同样大小的 Critic 模型，节省了成本。\nGRPO 目标函数： \\[ \\begin{gathered} \\mathcal{J}_{G R P O}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_{i}\\right\\}_{i=1}^{G} \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\ \\frac{1}{G} \\sum_{i=1}^{G}\\left(\\min \\left(\\frac{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i} \\mid q\\right)} A_{i}, \\operatorname{clip}\\left(\\frac{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i} \\mid q\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A_{i}\\right)-\\beta \\mathbb{D}_{K L}\\left(\\pi_{\\theta}| | \\pi_{r e f}\\right)\\right), \\end{gathered} \\] 其中优势 \\(A_i\\) 通过组内奖励 \\(r_i\\) 标准化得到： \\[ A_{i}=\\frac{r_{i}-\\operatorname{mean}\\left(\\left\\{r_{1}, r_{2}, \\cdots, r_{G}\\right\\}\\right)}{\\operatorname{std}\\left(\\left\\{r_{1}, r_{2}, \\cdots, r_{G}\\right\\}\\right)}. \\] KL 散度惩罚项使用 Schulman 无偏估计器： \\[ \\mathbb{D}_{K L}\\left(\\pi_{\\theta}| | \\pi_{r e f}\\right)=\\frac{\\pi_{r e f}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}-1. \\]奖励模型 (RM):\nDeepSeek-V2: 采用两阶段 RL 策略。 推理对齐: 使用专门训练的 \\(RM_{\\text{reasoning}}\\) 对代码和数学推理任务进行优化。 人类偏好对齐: 使用多奖励框架，结合 \\(RM_{\\text{helpful}}\\)、\\(RM_{\\text{safety}}\\) 和基于规则的 \\(RM_{\\text{rule}}\\)。 DeepSeek-V3: 基于规则的 RM: 对于可验证的任务（如数学答案格式、LeetCode 测试用例），使用规则来提供可靠奖励。 基于模型的 RM: 对于自由格式答案或无标准答案的任务（如创意写作），使用从 V3 SFT Checkpoint 初始化的 RM。该 RM 通过学习带有 CoT 的偏好数据来提升可靠性，减少奖励 Hacking 风险。 自奖励: V3 探索使用模型自身的判断能力（通过投票增强）作为反馈来源，特别是在通用场景下，结合 Constitutional AI (Bai et al., 2022) 的思想进行优化。 RL 训练优化 (V2/V3): 针对大模型 RL 的高资源需求，进行了工程优化，如混合引擎（训练/推理并行策略不同）、使用 vLLM (Kwon et al., 2023) 加速采样、CPU Offloading 调度等。\n评估 DeepSeek-V2 Chat 评估:\nDeepSeek-V2 Chat (SFT/RL) 与代表性开源 Chat 模型在开放式生成任务上的对比。V2 Chat (RL) 在 AlpacaEval 2.0 和 AlignBench 上表现突出。\nModel MT-Bench (Zheng et al., 2023) AlpacaEval 2.0 (Dubois et al., 2024) (LC Win Rate) AlignBench (Liu et al., 2023) (中文) DeepSeek 67B Chat 8.35 16.6 6.43 Mistral 8x22B Instruct 8.66 30.9 - Qwen1.5 72B Chat 8.61 36.6 7.19 LLaMA3 70B Instruct 8.95 34.4 - DeepSeek-V2 Chat (SFT) 8.62 30.0 7.74 DeepSeek-V2 Chat (RL) 8.97 38.9 7.91 DeepSeek-V3 Chat 评估:\nDeepSeek-V3 Chat 与代表性开源及闭源 Chat 模型对比 (部分结果)。V3 在多数基准上领先开源模型，并在代码、数学、中文及开放式生成任务上与顶尖闭源模型相当。\nBenchmark (Metric) DeepSeek V2.5-0905 Qwen2.5 72B-Inst. LLaMA-3.1 405B-Inst. Claude-3.5- Sonnet-1022 GPT-4o 0513 DeepSeek V3 English MMLU (Hendrycks et al., 2020) (EM) 80.6 85.3 88.6 88.3 87.2 88.5 MMLU-Pro (Wang et al., 2024) (EM) 66.2 71.6 73.3 78.0 72.6 75.9 GPQA-Diamond (Rein et al., 2023) (Pass@1) 41.3 49.0 51.1 65.0 49.9 59.1 SimpleQA (OpenAI, 2024c) (Correct) 10.2 9.1 17.1 28.4 38.2 24.9 Code HumanEval-Mul (Pass@1) 77.4 77.3 77.2 81.7 80.5 82.6 LiveCodeBench (Jain et al., 2024) (Pass@1-COT) 29.2 31.1 28.4 36.3 33.4 40.5 SWE Verified (OpenAI, 2024d) (Resolved) 22.6 23.8 24.5 50.8 38.8 42.0 Math AIME 2024 (MAA, 2024 (Pass@1) 16.7 23.3 23.3 16.0 9.3 39.2 MATH-500 (Hendrycks et al., 2021) (ЕМ) 74.7 80.0 73.8 78.3 74.6 90.2 Chinese C-Eval (Huang et al., 2023) (EM) 79.5 86.1 61.5 76.7 76.0 86.5 C-SimpleQA (He et al., 2024) (Correct) 54.1 48.4 50.4 51.3 59.3 64.8 Open-Ended Arena-Hard (Li et al., 2024) 76.2 81.2 69.3 85.2 80.4 85.5 AlpacaEval 2.0 (Dubois et al., 2024) (LC Win Rate) 50.5 49.1 40.5 52.0 51.1 70.0 总结:\nDeepSeek-V2 Chat (RL) 在发布时已是顶尖的开源聊天模型，尤其在 AlpacaEval 和中文 AlignBench 上表现优异。 DeepSeek-V3 Chat 进一步提升了性能，成为目前最强的开源聊天模型，在代码、数学、中文知识以及 Arena-Hard (Li et al., 2024)、AlpacaEval 等开放式评估中表现极其亮眼，达到了与 GPT-4o、Claude-3.5-Sonnet 相媲美的水平。 V3 的 R1 蒸馏显著提升了推理能力，但也可能增加响应长度，需要在准确性和效率间权衡。 V3 的自奖励能力（在 RewardBench (Lambert et al., 2024) 上表现优异）为其持续对齐提供了有效途径。 讨论 负载均衡策略演进: 从 V2 的辅助损失到 V3 的无辅助损失+偏置调整，体现了在保证负载均衡的同时，尽量减少对模型性能本身干扰的趋势。批处理级别的均衡相比序列级均衡，更能促进专家特化。 MTP 的有效性: V3 的实验证明，多 token 预测作为辅助训练目标，确实能提升模型在标准评估任务上的性能，同时为推理加速（推测解码）提供了可能。 R1 蒸馏: V3 成功地将 DeepSeek-R1 的长链推理能力蒸馏到标准 LLM 中，显著提升了数学和代码能力。这是一个重要的技术方向，但也需要注意控制生成长度。 自奖励: V3 强大的判断能力（RewardBench(Lambert et al., 2024)）结果使其能有效进行自反馈和自对齐，这对于减少对人类标注的依赖、实现模型持续自我提升至关重要。 SFT 数据量: 虽然在LIMA(Zhou et al., 2024）认为少量高质量 SFT 数据即可达到不错的效果，但对于特定技能（如指令遵循 IFEval），仍需足够数据量的高质量数据才能达到满意效果。 对齐税: OpenAI 在InstructGPT (Ouyang et al., 2022)中指出 RL 对齐在提升开放式生成能力的同时，可能牺牲部分标准基准的性能。V2 和 V3 都努力在数据处理和训练策略上缓解此问题，以达到可接受的平衡。 结论、局限性与未来方向 结论 DeepSeek-V2 和 DeepSeek-V3 是两款强大、经济且高效的 MoE 语言模型。它们通过 MLA 和 DeepSeekMoE 架构创新，以及 V3 引入的无辅助损失负载均衡、MTP、FP8 训练和 R1 蒸馏等技术，在性能、训练成本和推理效率上取得了突破。DeepSeek-V3 已成为当前最强的开源模型之一，性能可与顶尖闭源模型竞争。\n局限性 通用 LLM 局限: 如知识截止、幻觉、事实性错误等。 语言覆盖: 主要针对中英文，其他语言能力有限 (V2)。V3 扩展了多语言，但仍以中英文为主。 部署门槛 (V3): 高效推理需要较大的部署单元（多节点），对小型团队可能有挑战。 推理效率: 虽然 V3 推理效率相比 V2 有提升，但仍有优化空间。 未来方向 架构创新: 持续优化 MoE 架构，探索支持无限上下文、突破 Transformer 限制的新架构。 数据扩展: 提升训练数据的数量、质量和维度（多模态等）。 深度思考: 增强模型的推理长度和深度，提升智能水平。 评估方法: 发展更全面、多维度的评估方法，避免过拟合特定基准。 对齐与安全: 持续改进对齐技术（如自奖励），确保模型有用、诚实、无害，与人类价值观对齐。 参考文献 [1] Liu, Aixin, et al. \u0026ldquo;Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.\u0026rdquo; arXiv preprint arXiv:2405.04434 (2024).\n[2] Liu, Aixin, et al. \u0026ldquo;Deepseek-v3 technical report.\u0026rdquo; arXiv preprint arXiv:2412.19437 (2024).\n[3] Dai, Damai, et al. \u0026ldquo;Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.\u0026rdquo; arXiv preprint arXiv:2401.06066 (2024).\n[4] Wang, Lean, et al. \u0026ldquo;Auxiliary-loss-free load balancing strategy for mixture-of-experts.\u0026rdquo; arXiv preprint arXiv:2408.15664 (2024).\n[5] Gloeckle, Fabian, et al. \u0026ldquo;Better \u0026amp; faster large language models via multi-token prediction.\u0026rdquo; Proceedings of the 41st International Conference on Machine Learning. PMLR 235:16821-16841 (2024).\n[6] Vaswani, Ashish, et al. \u0026ldquo;Attention is all you need.\u0026rdquo; Advances in neural information processing systems 30 (2017).\n[7] Shazeer, Noam. \u0026ldquo;Fast transformer decoding: One write-head is all you need.\u0026rdquo; arXiv preprint arXiv:1911.02150 (2019).\n[8] Ainslie, Joshua, et al. \u0026ldquo;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.\u0026rdquo; Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4895-4901 (2023).\n[9] Su, Jianlin, et al. \u0026ldquo;Roformer: Enhanced transformer with rotary position embedding.\u0026rdquo; Neurocomputing 568 (2024): 127063.\n[10] Shazeer, Noam, et al. \u0026ldquo;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\u0026rdquo; arXiv preprint arXiv:1701.06538 (2017).\n[11] Lepikhin, Dmitry, et al. \u0026ldquo;Gshard: Scaling giant models with conditional computation and automatic sharding.\u0026rdquo; arXiv preprint arXiv:2006.16668 (2020).\n[12] Fedus, William, Barret Zoph, and Noam Shazeer. \u0026ldquo;Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\u0026rdquo; The Journal of Machine Learning Research 23.1: 5232-5270 (2022).\n[13] Zhou, Zexuan, et al. \u0026ldquo;Mixture-of-experts with expert choice routing.\u0026rdquo; Advances in Neural Information Processing Systems 35: 7103-7114 (2022).\n[14] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. \u0026ldquo;Fast inference from transformers via speculative decoding.\u0026rdquo; Proceedings of the 40th International Conference on Machine Learning. PMLR 202:19274-19286 (2023).\n[15] Xia, Yichao, et al. \u0026ldquo;Accelerating large language model decoding with speculative sampling.\u0026rdquo; arXiv preprint arXiv:2302.01318 (2023).\n[16] Qi, Hai, et al. \u0026ldquo;ZeroBubble: A High-Performance Framework for Training Mixture-of-Experts Models.\u0026rdquo; arXiv preprint arXiv:2401.10241 (2024).\n[17] Rajbhandari, Samyam, et al. \u0026ldquo;Zero: Memory optimizations toward training trillion parameter models.\u0026rdquo; SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE (2020).\n[18] Harlap, Aaron, et al. \u0026ldquo;Pipedream: Fast and efficient pipeline parallel dnn training.\u0026rdquo; arXiv preprint arXiv:1806.03377 (2018).\n[19] Li, Shigang, and Torsten Hoefler. \u0026ldquo;Chimera: Efficiently training large-scale neural networks with bidirectional pipelines.\u0026rdquo; Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.\n[20] Bauer, Michael, Sean Treichler, and Alex Aiken. \u0026ldquo;Singe: Leveraging warp specialization for high performance on gpus.\u0026rdquo; Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming. 2014.\n[21] Dettmers, Tim, et al. \u0026ldquo;Llm. int8 (): 8-bit matrix multiplication for transformers at scale.\u0026rdquo; Advances in Neural Information Processing Systems 35: 34138-34151 (2022).\n[22] Noune, Badreddine, et al. \u0026ldquo;8-bit numerical formats for deep neural networks.\u0026rdquo; arXiv preprint arXiv:2206.02915 (2022).\n[23] Peng, Houwen, et al. \u0026ldquo;FP8-LM: Training FP8 Large Language Models.\u0026rdquo; arXiv preprint arXiv:2310.18313 (2023).\n[24] Fishman, Maxim, et al. \u0026ldquo;Scaling FP8 training to trillion-token LLMs.\u0026rdquo;) arXiv preprint arXiv:2409.12517 (2024).\n[25] He, Bobby, et al. \u0026ldquo;Understanding and minimising outlier features in neural network training.\u0026rdquo; arXiv preprint arXiv:2405.19279 (2024).\n[26] Sun, Xiao, et al. \u0026ldquo;Massive activations in large language models.\u0026rdquo; arXiv preprint arXiv:2402.17762 (2024).\n[27] NVIDIA. \u0026ldquo;Transformer Engine.\u0026rdquo; GitHub Repository (Accessed 2024).\n[28] Sun, Xiao, et al. \u0026ldquo;Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks.\u0026rdquo; Advances in neural information processing systems 32 (2019).\n[29] Loshchilov, Ilya, and Frank Hutter. \u0026ldquo;Decoupled weight decay regularization.\u0026rdquo; arXiv preprint arXiv:1711.05101 (2017).\n[30] NVIDIA. \u0026ldquo;GPUDirect Storage: A Direct Path Between Storage and GPU Memory.\u0026rdquo; NVIDIA Developer Blog (2022).\n[31] Graham, Richard L., et al. \u0026ldquo;Scalable hierarchical aggregation protocol (SHArP): A hardware architecture for efficient data reduction.\u0026rdquo; 2016 First International Workshop on Communication Optimizations in HPC (COMHPC). IEEE, 2016.\n[32] Ding, Yiran, et al. \u0026ldquo;Longrope: Extending llm context window beyond 2 million tokens.\u0026rdquo; arXiv preprint arXiv:2402.13753 (2024).\n[33] Zhu, Qihao, et al. \u0026ldquo;DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence.\u0026rdquo; arXiv preprint arXiv:2406.11931 (2024).\n[34] Lundberg, Scott M. \u0026ldquo;Guidance: Prompt Boundaries and Token Healing.\u0026rdquo; GitHub Notebook (2023).\n[35] Peng, Bowen, et al. \u0026ldquo;YaRN: Efficient Context Window Extension of Large Language Models.\u0026rdquo; arXiv preprint arXiv:2309.00071 (2023).\n[36] Hendrycks, Dan, et al. \u0026ldquo;Measuring massive multitask language understanding.\u0026rdquo; arXiv preprint arXiv:2009.03300 (2020).\n[37] Chen, Mark, et al. \u0026ldquo;Evaluating large language models trained on code.\u0026rdquo; arXiv preprint arXiv:2107.03374 (2021).\n[38] Cobbe, Karl, et al. \u0026ldquo;Training verifiers to solve math word problems.\u0026rdquo; arXiv preprint arXiv:2110.14168 (2021).\n[39] Huang, Yuzhen, et al. \u0026ldquo;C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 62991-63010.\n[40] Wang, Yubo, et al. \u0026ldquo;Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.\u0026rdquo; The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024.\n[41] Jain, Naman, et al. \u0026ldquo;LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code.\u0026rdquo; arXiv preprint arXiv:2403.07974 (2024).\n[42] Hendrycks, Dan, et al. \u0026ldquo;Measuring mathematical problem solving with the math dataset.\u0026rdquo; arXiv preprint arXiv:2103.03874 (2021).\n[43] OpenAI. \u0026ldquo;MMMLU Dataset.\u0026rdquo; Hugging Face Datasets (Accessed 2024).\n[44] Guo, Daya, et al. \u0026ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\u0026rdquo; arXiv preprint arXiv:2501.12948 (2025).\n[45] Shao, Zhihong, et al. \u0026ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\u0026rdquo; arXiv preprint arXiv:2402.03300 (2024).\n[46] Bai, Yuntao, et al. \u0026ldquo;Constitutional ai: Harmlessness from ai feedback.\u0026rdquo; arXiv preprint arXiv:2212.08073 (2022).\n[47] Kwon, Woosuk, et al. \u0026ldquo;Efficient memory management for large language model serving with pagedattention.\u0026rdquo; Proceedings of the 29th Symposium on Operating Systems Principles. 2023.\n[48] Zheng, Lianmin, et al. \u0026ldquo;Judging llm-as-a-judge with mt-bench and chatbot arena.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 46595-46623.\n[49] Dubois, Yann, et al. \u0026ldquo;Length-controlled alpacaeval: A simple way to debias automatic evaluators.\u0026rdquo; arXiv preprint arXiv:2404.04475 (2024).\n[50] Liu, Xiao, et al. \u0026ldquo;Alignbench: Benchmarking chinese alignment of large language models.\u0026rdquo; arXiv preprint arXiv:2311.18743 (2023).\n[51] Rein, David, et al. \u0026ldquo;GPQA: A Graduate-Level Google-Proof Q\u0026amp;A Benchmark.\u0026rdquo; First Conference on Language Modeling. 2024.\n[52] OpenAI. \u0026ldquo;Introducing SimpleQA\u0026rdquo; OpenAI Blog (2024).\n[53] OpenAI. \u0026ldquo;Introducing SWE-bench Verified \u0026ldquo; OpenAI Blog (2024).\n[54] Mathematical Association of America (MAA). \u0026ldquo;2024 AIME I Problems.\u0026rdquo; Art of Problem Solving Wiki (2024).\n[55] Li, Tianle, et al. \u0026ldquo;From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.\u0026rdquo; arXiv preprint arXiv:2406.11939 (2024).\n[56] Lambert, Nathan, et al. \u0026ldquo;RewardBench: Evaluating Reward Models for Language Modeling.\u0026rdquo; arXiv preprint arXiv:2403.13787 (2024).\n[57] Zhou, Chunting, et al. \u0026ldquo;Lima: Less is more for alignment.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 55006-55021.\n[58] Ouyang, Long, et al. \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in neural information processing systems 35 (2022): 27730-27744.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Apr 2025). DeepSeek-V2 vs V3. https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3\nOr\n@article{syhya2025deepseekv2v3, title = \u0026#34;DeepSeek-V2 vs V3\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3/","summary":"\u003cp\u003eDeepSeek AI 先后发布了 \u003cstrong\u003eDeepSeek-V2\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/2405.04434\"\u003eDeepSeek-AI, 2024\u003c/a\u003e) 和 \u003cstrong\u003eDeepSeek-V3\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/2412.19437\"\u003eDeepSeek-AI, 2024\u003c/a\u003e)，这两款强大的混合专家（Mixture-of-Experts, MoE）语言模型在保持顶尖性能的同时，显著优化了训练成本和推理效率。DeepSeek-V2 拥有 236B 总参数，每次激活 21B；而 DeepSeek-V3 则进一步扩展至 671B 总参数，每次激活 37B。两者均支持 128K 上下文长度。\u003c/p\u003e","title":"DeepSeek-V2 vs V3"},{"content":"LLaMA Meta AI 推出的 LLaMA 系列开源模型已成为大语言模型社区的基石之一，对推动开放研究和应用产生了深远影响。从 2023 年初发布的开创性 LLaMA，到同年性能显著提升的 LLaMA 2，再到针对特定领域（如代码、安全）的衍生模型，以及 2024 年和 2025 年相继推出的新一代 LLaMA 3 和 LLaMA 4，Meta 持续致力于提升开源模型的性能，使其逐步逼近最先进的闭源模型。下面，我们将依次介绍每个主要模型的关键技术细节。\nLLaMA 1 LLaMA 1 (Touvron et al., 2023a) 于 2023 年 2 月发布，是 Meta 开源的首个基础语言模型系列。LLaMA 提供了 7B、13B、33B、65B 四种参数规模，其核心特点在于完全使用公开可用的数据集进行训练，未依赖任何专有数据。尽管参数量远小于当时的 GPT-3 (175B)，LLaMA 13B 模型在多数基准测试上超越了 GPT-3，而 65B 模型则达到了与 Chinchilla-70B 和 PaLM-540B 等顶尖模型相媲美的性能水平。\nFig. 1. Zero-shot performance of LLaMA models on Common Sense Reasoning tasks compared to other foundation models. (Source: Touvron et al., 2023a)\n训练数据： LLaMA 1 的训练基于大规模公开语料。其中，65B 和 33B 模型使用了约 1.4 万亿 tokens，而 7B 和 13B 模型则使用了约 1 万亿 tokens。语料来源广泛，主要包括 Common Crawl、C4、GitHub、Wikipedia、Books、ArXiv 和 StackExchange，覆盖了多种领域和约 20 种语言（以英语为主）。\n架构设计： LLaMA 1 采用了标准的 Transformer 解码器架构，并引入了以下关键改进以提升性能和训练效率：\nPre-normalization \u0026amp; RMSNorm： 采用 Pre-normalization 结构（在每个子层输入前进行归一化），并使用 RMSNorm (Root Mean Square Normalization) 替代标准的 LayerNorm。RMSNorm 通过省略均值中心化步骤，仅依据向量元素的均方根进行缩放，从而降低了计算复杂度，同时有效维持了训练过程的稳定性。 SwiGLU 激活函数： 将前馈网络（FFN）中的激活函数从 ReLU 替换为 SwiGLU (Swish-Gated Linear Unit)。SwiGLU 结合了 Swish 激活函数的平滑非线性和门控机制，增强了模型的表达能力。同时，LLaMA 调整了 FFN 的隐藏层维度（使用 $ \\frac{2}{3} \\times 4d $ 而非标准的 $4d$，其中 $d$ 是模型维度），以在引入门控参数的同时，大致保持 FFN 层的总参数量和计算量不变。 RoPE 旋转位置编码： 采用 Rotary Position Embeddings (RoPE) 进行位置编码。RoPE 通过对 Query 和 Key 向量施加与位置相关的旋转操作，将相对位置信息有效融入自注意力计算中，增强了模型处理长序列和捕捉长距离依赖关系的能力。LLaMA 1 的最大上下文长度为 2048 tokens。 高效注意力实现： 利用 Meta 开源的 xformers 库实现了内存高效且计算优化的因果多头注意力机制 (causal multi-head attention)。 微调对话模型： LLaMA 1 发布时主要提供预训练模型权重（限制商业用途），并未包含官方的对话微调版本。然而，开源社区迅速在其基础上进行了探索，例如 Stanford Alpaca (Taori et al., 2023) 项目证明了仅需少量指令数据进行监督微调（SFT），即可赋予 LLaMA 基础模型强大的对话能力，极大地促进了开源 LLM 的研究与应用生态。\nFig. 2. The pipeline for generating instruction-following demonstrations and training Alpaca 7B based on LLaMA 7B. (Source: Taori et al., 2023)\n训练稳定性与 Loss 突刺 (Loss Spike)\nFig. 3. Training loss curves over processed tokens for the LLaMA 7B, 13B, 33B, and 65B models. (Source: Touvron et al., 2023a)\n从图 3 可以观察到，LLaMA 模型在训练过程中的 Loss 总体呈现下降趋势，表明训练过程相对稳定。然而，在训练 13B、33B 和 65B 模型时，均出现了 Loss 突刺 (Loss Spike) 现象，即训练损失在某个时间点突然异常飙升。模型规模越大，突刺现象似乎越显著，且可能在训练过程中多次出现。\n现象描述： Loss Spike 指的是在模型训练期间，损失函数值出现短暂、急剧且异常的增高。 潜在原因： 通常与多种因素相关，包括训练数据中存在的异常样本或分布突变、学习率设置不当（过高或衰减策略问题）、优化器（如 Adam）内部状态与梯度剧烈变化的相互作用，以及混合精度训练中的数值不稳定性（例如梯度溢出或下溢）。 常见应对策略： 解决或缓解 Loss Spike 的方法包括：加强数据清洗和预处理；应用梯度裁剪 (Gradient Clipping) 限制梯度范数；精细调整学习率调度策略（如 Warmup、Decay）；优化混合精度训练配置；以及在发生突刺后，从最近的检查点恢复训练，并可能跳过导致问题的特定数据批次。 LLaMA 2 LLaMA 2 (Touvron et al., 2023b) 于 2023 年 7 月推出，是 LLaMA 1 的重要升级版本。相较于第一代，LLaMA 2 在模型规模、训练数据量、上下文长度以及模型对齐方面均有显著改进，并首次发布了官方的对话优化版本 LLaMA 2-Chat，且开放了商业使用许可。\n架构与优化： LLaMA 2 的基础架构很大程度上继承了 LLaMA 1 的成功设计（如 RMSNorm, SwiGLU, RoPE）。主要的技术更新包括：\n分组查询注意力 (GQA)： 针对较大的 34B 和 70B 模型，采用了Grouped Query Attention (GQA)。GQA 是一种介于多头注意力 (MHA) 和多查询注意力 (MQA) 之间的折中方案，它允许多个查询头（Query heads）共享同一组键（Key）和值（Value）头。这显著减少了推理过程中 KV 缓存的内存占用和计算开销，从而提高了大模型的推理速度和部署效率，同时对模型性能的影响很小。 上下文长度提升： 将模型的最大上下文长度从 LLaMA 1 的 2048 tokens 扩展到了 4096 tokens。这使得模型能够处理更长的文本输入，增强了其在长文档问答、摘要、长对话等任务上的能力。 训练数据与规模： LLaMA 2 使用了更大规模的预训练数据，总量达到约 2 万亿 tokens，相比 LLaMA 1 增加了约 40%。数据来源更加多样化，并进行了更严格的筛选和清洗。\nPost-Training (LLaMA 2-Chat)： LLaMA 2-Chat 是经过精心对齐的对话模型。其训练流程始于 LLaMA 2 预训练基础模型，主要包含以下阶段：\n监督微调 (Supervised Fine-tuning, SFT)： 使用高质量的指令和对话样本对预训练模型进行微调，使其初步具备遵循指令和进行对话的能力。 人类反馈强化学习 (Reinforcement Learning from Human Feedback, RLHF)： 这是提升模型有用性（Helpfulness）和安全性（Safety）的关键步骤。 奖励模型训练 (Reward Modeling)： 收集大量人类偏好数据（即对模型生成的多个回答进行排序），训练一个或多个奖励模型来学习评估哪个回答更符合人类偏好（在有用性和安全性维度上）。 RL 优化： 利用训练好的奖励模型作为回报信号，通过 PPO 和 拒绝采样 来进一步优化 SFT 后的模型。PPO 旨在最大化奖励信号，而拒绝采样则通过从模型生成的 K 个样本中选出奖励最高的一个进行梯度更新，进一步提升模型质量。这个过程通常是迭代进行的，不断收集新的偏好数据来改进奖励模型和对话模型本身。参考了 Anthropic 提出的 Constitutional AI 和 HH-RLHF (Bai et al., 2022) 的思想。可在 Hugging Face 上获取相关的 HH-RLHF 数据集。 Fig. 4. Illustration of the Llama 2-Chat fine-tuning process, including SFT and RLHF stages with rejection sampling and PPO. (Source: Touvron et al., 2023b)\nCode Llama Code Llama (Rozière et al., 2023) 是 Meta 在 LLaMA 2 基础上于 2023 年 8 月发布的专注于代码能力的大型语言模型系列。通过在海量的编程代码数据上进行额外的持续预训练和特定任务微调，Code Llama 在代码生成、代码补全、代码理解和调试等任务上展现出卓越的能力。\nFig. 5. The Code Llama specialization pipeline, starting from Llama 2 and involving code-specific training stages. (Source: Rozière et al., 2023)\n训练与数据： Code Llama 以 LLaMA 2 的权重为起点，使用了 5000 亿 tokens 的代码及代码相关自然语言语料进行继续预训练（针对 7B/13B/34B 版本）或 1 万亿 tokens（针对 70B 版本）。训练数据主要来源于公开的代码仓库和数据集。关键的技术改进包括：\n长上下文微调 (Long Context Fine-tuning, LCFT)： Code Llama 在训练中特别关注长序列处理能力，将序列长度扩展到 16k tokens 进行训练。为了更好地处理长距离依赖，通过调整 RoPE 位置编码的基数 $\\theta$（从 LLaMA 2 的 10,000 增加到 1,000,000），减缓了随着 token 距离增大注意力分数的衰减。这使得模型在推理时能够稳定地处理长达 100k tokens 的超长上下文。 Fig. 6. Effect of RoPE base period scaling on perplexity for long sequences, showing improved performance with a larger base. (Source: Rozière et al., 2023)\n填充中间 (Fill-in-the-Middle, FIM)： 训练中引入了填充中间任务。模型需要根据给定的代码前缀和后缀，在中间插入合适的代码片段。这种能力对于集成开发环境（IDE）中的代码补全功能至关重要。 模型变体： Code Llama 提供了多个版本以满足不同场景的需求：\nCode Llama (Base): 基础代码模型，擅长代码补全和从自然语言生成代码。 Code Llama - Python: 在基础模型之上，针对 Python 语言额外进行了 1000 亿 tokens 的专门微调，显著增强了 Python 相关任务的性能。 Code Llama - Instruct: 在代码相关的指令和人类反馈数据上进行了微调（约 50 亿 tokens），使其能更好地理解自然语言指令来生成、解释或修改代码，更适合作为代码助手使用。 每个版本均提供 7B、13B、34B、70B 四种参数规模。\nLlama Guard Llama Guard (Inan et al., 2023) 是 Meta 于 2023 年 12 月推出的一款专门用于保障人机对话内容安全的模型。它旨在对用户输入（prompt）和模型输出（response）进行内容审查和风险分类。\nFig. 7. Example task instructions for the Llama Guard prompt and response classification tasks, demonstrating its safety assessment capability. (Source: Inan et al., 2023)\n模型概况： Llama Guard 基于 LLaMA 2-7B 模型，通过指令微调的方式专门训练用于安全风险分类任务。它并非生成式模型，而是接收一段文本输入，判断其内容是否安全，并能根据预定义的安全风险分类体系（taxonomy）输出具体的风险类别标签。\n训练与分类体系： Meta 构建了一个包含多种不安全内容类别（如暴力、仇恨言论、性内容、非法行为推广等）的分类体系，并收集了高质量的标注数据进行训练。Llama Guard 能够进行多标签分类，识别文本中可能同时存在的多种风险。由于采用了指令微调范式，用户可以通过设计不同的提示词（prompt）来灵活调整安全策略或定制分类标准。Llama Guard 可以作为过滤器部署在对话系统的输入端（检测用户输入风险）和输出端（检测模型生成内容的风险）。\nLlama Guard 3 Vision Llama Guard 3 Vision (Chi et al., 2024) 是 Llama Guard 的多模态升级版本，基于 Llama-3.2-Vision 模型构建。它能够同时评估图像和文本内容的安全风险，将安全防护能力扩展到了多模态场景。该模型使用特殊的 \u0026lt;|image|\u0026gt; token 来整合图像信息，进行统一的多模态安全审查。\nFig. 8. Llama Guard 3 Vision classifying harmful content in a multimodal response classification task involving both image and text. (Source: Chi et al., 2024)\nLlama Guard 3 Vision 采用了 ML Commons 定义的安全风险分类标准 (Vidgen et al., 2024)，并在此基础上扩展，增加了对代码解释器滥用风险的检测（S14 类别）。\nFig. 9. The 14 hazard categories used by Llama Guard 3 Vision, based on the MLCommons taxonomy with an added category for code interpreter abuse. (Source: Meta Llama, 2024)\n基准测试结果显示，Llama Guard 3 Vision 在 MLCommons 安全基准上，无论是在检测用户输入风险还是模型输出风险方面，其多项指标均优于 GPT-4o 和 GPT-4o mini 等先进模型。\nFig. 10. Performance comparison of various models on the MLCommons hazard taxonomy internal test set, showing Llama Guard 3 Vision\u0026rsquo;s strong results. (Source: Chi et al., 2024)\nLLaMA 3 LLaMA 3 (Grattafiori et al., 2024) 是 Meta 自 2024 年 4 月起陆续发布的新一代开源大模型系列，其在性能、规模、多语言能力、多模态支持和训练效率进行了优化。\n模型规模与版本演进： LLaMA 3 系列覆盖了从小型到超大规模的广泛参数范围：\nLLaMA 3 (初版, 2024/04): 首先发布了 8B 和 70B 两种规模的预训练和指令微调模型。 LLaMA 3.1 (2024/07): (Meta AI, 2024) 推出了 405B 参数的旗舰模型，其性能在多项基准上接近 GPT-4 水平，同时更新了 8B 和 70B 版本。 LLaMA 3.2 (2024/10): 引入了轻量化模型（如 1B, 3B, 11B, 13B），专为边缘设备（如手机、手表、智能家居）优化，并发布了多模态视觉模型（如 Llama-3.2-11B-Vision 和 Llama-3.2-90B-Vision）。 Fig. 11. Overview of the key hyperparameters for Llama 3 models of different scales. (Source: Grattafiori et al., 2024)\n从上图可以看出，训练更大规模的 LLM 通常需要采用更小的峰值学习率。这主要是因为：\n优化景观复杂性与梯度稳定性： 参数量越大，模型的损失函数景观越复杂、非凸性越强，对参数更新的敏感度也越高。较小的学习率有助于限制每次更新的步长，避免在陡峭区域产生过大的梯度导致训练震荡或发散，从而确保优化过程更稳定地收敛。 避免过拟合与提升泛化能力： 大模型容量更大，更容易在训练数据上过拟合。较小的学习率使得模型以更慢、更稳健的方式学习数据中的模式，减少了对训练数据中噪声或局部特征的过度拟合风险，有助于提升模型在未见数据上的泛化性能。 精细搜索与参数调整： 在高维参数空间中，最优解可能位于狭窄区域。小学习率允许优化算法进行更精细的搜索，逐步逼近最优解，避免因步长过大而“跳过”最优区域，从而可能达到更高的最终模型精度。 Fig. 12. Comparison of the high-level architecture between Llama 2 and Llama 3. (Source: Umar Jamil\u0026rsquo;s PyTorch Llama Slides)\n架构与技术创新： LLaMA 3 在 LLaMA 2 的基础上进行了多方面的显著增强：\n超大规模预训练数据： 预训练数据量达到了惊人的 15 万亿 tokens，是 LLaMA 2 的 7.5 倍。数据来源更加广泛，质量更高，多样性更强，并显著增加了非英语语言（如德语、法语、西班牙语、印地语等，占总数据 5% 以上）和代码数据的比例。 优化的 Tokenizer： 采用了基于 tiktoken 库实现的新分词器，词汇表大小从 LLaMA 2 的 32k 大幅扩展到 128k。更大的词汇表提高了对多种语言（尤其是非拉丁语系）和代码的编码效率，平均能将输入序列长度减少约 15%，从而间接提升了模型的处理效率和性能。 扩展的上下文长度： LLaMA 3 初版（8B, 70B）支持 8k tokens 的上下文窗口。而 LLaMA 3.1 (405B) 将最大上下文窗口进一步提升至 128k tokens，极大地增强了模型处理长文档、长对话历史和复杂上下文推理的能力。这通常通过结合 RoPE 频率调整、注意力机制优化（如 FlashAttention）等技术实现。 全面应用 GQA： 与 LLaMA 2 仅在较大模型使用 GQA 不同，LLaMA 3 的所有规模模型（包括 8B）都采用了 Grouped Query Attention (GQA)，以优化推理时的内存占用和计算速度。 先进的对齐技术： 在指令微调（Post-training）阶段，LLaMA 3 结合了监督微调 (SFT)、拒绝采样 (Rejection Sampling) 和直接偏好优化 (Direct Preference Optimization, DPO) 等多种先进技术，旨在全面提升模型的指令遵循能力、有用性（Helpfulness）和安全性（Safety）。 多模态整合 (LLaMA 3.2)： 通过引入视觉编码器（Vision Encoder）并进行联合训练，实现了图像和文本的融合处理，推出了 Llama-3.2-Vision 系列视觉语言模型。 轻量化模型 (LLaMA 3.2)： 针对资源受限的边缘计算场景，通过模型压缩技术（如剪枝、蒸馏）推出了 1B、3B 等小型化模型，在性能和资源消耗之间取得了良好平衡。 Fig. 13. Illustration of the overall post-training approach for Llama 3, involving multiple stages and iterative refinement. (Source: Grattafiori et al., 2024)\n从上图可以看出，LLaMA 3 的后训练（指令微调）流程是一个精心设计的多阶段迭代过程：\n数据准备 (Data Preparation): 收集大量人类偏好数据。这些数据通常包含一个提示（prompt）以及模型生成的多个回答，标注员会对这些回答进行排序（例如，选出最好的 \u0026ldquo;chosen\u0026rdquo; 回答和较差的 \u0026ldquo;rejected\u0026rdquo; 回答）。同时也会收集高质量的 SFT 数据（prompt-response 对）。\n奖励模型训练 (Reward Modeling, RM): 利用收集到的人类偏好数据 (prompt, chosen, rejected) 三元组训练一个或多个奖励模型。奖励模型的目标是学习预测人类对模型生成回答的偏好程度，为后续的优化提供量化信号。LLaMA 3 训练了两个独立的奖励模型，分别侧重于有用性（Helpfulness）和安全性（Safety）。\n拒绝采样 (Rejection Sampling): 使用训练好的奖励模型对模型生成的候选回答进行打分。选择得分最高的回答作为高质量样本，用于后续的微调阶段。这有助于筛选出比 SFT 数据质量更高的样本。\n监督微调 (Supervised Finetuning, SFT): 结合初始的人工标注 SFT 数据和通过拒绝采样筛选出的高质量数据，对预训练的基础模型进行微调。此阶段旨在让模型学习遵循指令的格式、风格，并初步掌握所需的知识和能力。LLaMA 3 在此阶段混合使用了多种来源的数据。\n偏好优化: 在 SFT 模型的基础上，使用偏好数据 (prompt, chosen, rejected) 通过直接偏好优化 (Direct Preference Optimization, DPO) 算法进一步对齐模型。DPO 直接优化模型以提高其对 \u0026ldquo;chosen\u0026rdquo; 回答的似然，同时降低对 \u0026ldquo;rejected\u0026rdquo; 回答的似然，相比基于 RL 的 PPO 方法，实现更简单且训练更稳定。LLaMA 3 对 DPO 进行了改进，例如在训练 DPO 时屏蔽了回答中的特殊格式化 token，并引入了归一化的负对数似然（NLL）损失作为正则项，以提升训练稳定性和生成质量。其损失函数形式大致可以参考 RPO(Pang et al., 2024) 中的损失，LLaMA3 的具体实现可能略有不同：\n$$ \\begin{aligned} \\mathcal{L}_{\\mathrm{DPO}+\\mathrm{NLL}} \u0026 =\\mathcal{L}_{\\mathrm{DPO}}\\left(y^w, y^l \\mid x\\right)+\\alpha \\mathcal{L}_{\\mathrm{NLL}}\\left(y^w \\mid x\\right) \\\\ \u0026 =-\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y^w \\mid x)}{\\pi_{\\mathrm{ref}}(y^w \\mid x)}-\\beta \\log \\frac{\\pi_\\theta(y^l \\mid x)}{\\pi_{\\mathrm{ref}}(y^l \\mid x)}\\right)-\\alpha \\frac{\\log \\pi_\\theta(y^w \\mid x)}{|y^w|} \\end{aligned} $$ 其中：\n$x$ 是输入 prompt。 $y^w$ 是偏好的 (winning/chosen) 回答，$y^l$ 是不被偏好的 (losing/rejected) 回答。 $\\pi_\\theta$ 是当前正在优化的模型策略（参数为 $\\theta$）。 $\\pi_{\\mathrm{ref}}$ 是参考模型策略（通常是 SFT 后的模型或上一轮迭代的模型）。 $\\beta$ 是控制偏好强度差异的超参数。 $\\sigma$ 是 Sigmoid 函数。 $\\alpha$ 是平衡 DPO 损失和 NLL 正则化损失的权重。 $|y^w|$ 是 winning 回答的长度，用于归一化 NLL 损失。 该损失函数鼓励模型 $\\pi_\\theta$ 相对于参考模型 $\\pi_{\\mathrm{ref}}$ 更倾向于生成 $y^w$ 而非 $y^l$，同时通过 NLL 正则项保持生成文本的流畅性和语言质量。 迭代循环 (Iterative Loop): 上述的 SFT 和 DPO（或 RLHF 变体）过程会重复进行多轮（LLaMA 3 进行了五轮）。每一轮都会使用上一轮优化后的模型来生成新的数据，收集新的人类反馈，训练新的奖励模型，并进行下一轮的 SFT 和 DPO 优化。这种迭代的方式使得模型能够持续学习和改进。\n模型权重平均 (Model Weight Averaging): 在某些阶段，可能会对使用不同数据子集或超参数训练得到的多个模型检查点进行权重平均，以获得更鲁棒、性能更均衡的最终模型。\nLLaMA 4 LLaMA 4 (Meta AI, 2025)系列模型由 Meta AI 于 2025 年 4 月 5 日发布，标志着 LLaMA 生态系统迈入了原生多模态 AI 创新的新阶段。这一代模型首次引入了混合专家（Mixture-of-Experts, MoE）架构，并具备了前所未有的超长上下文处理能力，旨在提供更强大、更高效的开源基础模型。\n模型概览：性能、规模与部署\nLLaMA 4 首批发布了三个不同定位的模型，其中两个开放权重：\n模型名称 活跃参数 专家数量 总参数 关键性能/定位 硬件参考 上下文窗口 Llama 4 Scout 17B 16 109B 优于 Gemma 3 等同级模型；10M Token 超长上下文；图像理解能力强；高性价比 单张 H100 GPU (INT4 量化) 10M Llama 4 Maverick 17B 128 400B 性能比肩甚至超越 GPT-4o/Gemini 2.0 Flash (推理/编码/多语言)；活跃参数少，计算效率高；图像推理/理解领先；LMArena ELO 1417 单台 H100 主机 (多卡) 或分布式部署 1M Llama 4 Behemoth 288B 16 ~2T 教师模型 (未发布)；STEM 基准 (MATH, GPQA) 超越 GPT-4.5/Claude 3.7/Gemini 2.0 Pro；通过共蒸馏提升 Scout/Maverick 仍在训练中，未公开发布 (未明确) 性能亮点： Maverick (17B 活跃参数) 在多个主流基准上展现出与 GPT-4o 等顶尖闭源模型竞争的实力，尤其在推理、编码和多语言任务上，而其活跃参数量显著更少，体现了出色的计算效率。Scout 则凭借其惊人的 10M Token 上下文窗口在同级别模型中脱颖而出。 部署门槛： Scout 的 INT4 量化版本可在单块 H100 上运行，降低了高性能模型的部署门槛。Maverick 虽然需要更强的算力（如单台 H100 多卡主机），但相较于其性能，仍提供了有吸引力的性价比。(注：消费级显卡运行这些模型仍有挑战) 核心架构与训练创新\nLLaMA 4 相比上一代模型有以下优化：\n混合专家 (MoE) 架构：\nLLaMA 4 是首个采用 MoE 的 Llama 系列。MoE 允许模型在推理时仅激活总参数的一小部分（即“活跃参数”），以更低的计算量换取更大的模型容量和更强的性能。这对于计算成本敏感（尤其是吞吐量敏感）的推理场景非常有利。 Maverick 模型采用了交替的密集层和 MoE 层。其 MoE 层包含 128 个路由专家和一个所有 Token 都会访问的共享专家，每个 Token 会被路由到共享专家及其中一个路由专家进行处理。 原生多模态与早期融合 (Early Fusion)：\n告别“拼接式”： 不同于以往将视觉模块“外挂”到 LLM 上的后期融合方法，LLaMA 4 从设计之初就采用早期融合策略。 统一骨干： 文本 Token 和视觉 Token（来自图像和视频帧）在模型主干网络（Backbone）的早期阶段就被无缝集成和共同处理。 深度理解： 这使得模型能在海量的图文、视频数据上进行联合预训练，学习到更深层次、更细粒度的跨模态关联，实现更自然的交互和更强的视觉定位 (Grounding) 能力（将文本提示与图像区域精确对应），而不仅仅是“看图说话”。 视觉编码器： 基于 MetaCLIP (Xu et al., 2023) 进行了改进，并与 Llama 模型协同训练，以更好地适应 LLM 的需求。 超长上下文 (Ultra-Long Context)：\n10M Token 上限： Llama 4 Scout 实现了行业领先的 1000 万 Token 上下文窗口。 技术支撑： iRoPE 架构： 结合了 RoPE (旋转位置编码) 和 NoPE (无位置编码) 的思想。通过交错注意力层 (interleaved attention layers) 实现，部分特定层采用 NoPE (Kazemnejad et al., 2023)，依赖注意力机制隐式学习位置关系，而 RoPE 仍用于大多数其他层。(\u0026ldquo;i\u0026rdquo; 同时代表 interleaved 和 infinite 的目标)。 Scalable-Softmax: 结合推理时温度缩放 (Nakanishi et al., 2025)，增强模型在未见过长度上的泛化能力。 专门训练： 通过专门构建的长上下文数据集进行中期训练 (mid-training) 和后训练 (post-training)，Scout 在 256k 上下文长度上进行了训练，并通过 iRoPE 和 Scalable Softmax 泛化到 10M。 实用性观察： 尽管 10M Token 非常吸引人，但在实际应用中处理如此长的上下文可能会遇到推理效率、注意力分散和带宽瓶颈等问题，其真实场景下的效果和效率仍有待用户验证。 Fig. 14. Cumulative average NLL loss per sequence position for code generation, demonstrating Llama 4 Scout\u0026rsquo;s strong performance over long contexts. (Source: Meta AI, 2025)\n大规模高质量预训练： 数据量级： 训练数据超过 30 万亿 tokens（LLaMA 3 的两倍以上），包含文本、图像和视频。 多语言覆盖： 覆盖 200 种语言，其中超 100 种语言数据量过 10 亿 token，多语言 token 总量是 LLaMA 3 的 10 倍。 训练效率： 采用 FP8 精度进行训练，Behemoth 在 32K GPU 上实现了 390 TFLOPs/GPU 的高利用率。利用 MetaP 技术可靠设置超参数。 革新的后训练 (Post-training) 流程\nLLaMA 4 采用了新的三阶段后训练流程，旨在平衡指令遵循、智能涌现和对话质量：\n轻量级 SFT (Supervised Fine-Tuning)： 专注于使用少量、更困难的数据集进行监督微调，教会模型基本的指令遵循和对话格式，避免过度拟合简单模式，为后续 RL 探索保留空间。相比之前版本，大幅削减了简单 SFT 数据（Maverick \u0026gt;50%, Behemoth \u0026gt;95%）。 在线强化学习 (Online RL)： 这是提升模型核心智能和复杂任务能力的关键阶段。采用持续在线 RL 策略，模型在与环境交互中学习，通过精心选择较难的提示进行探索，并交替进行模型训练和数据过滤（保留中等到困难的交互数据），在计算和效果间取得平衡。 轻量级 DPO (Direct Preference Optimization)： 在 RL 之后进行，用于微调模型响应的风格、安全性和修正边缘案例 (corner cases)，进行最终的“精修和打磨”，确保智能与流畅对话体验的统一。 教师模型与共蒸馏 (Co-Distillation)\n强大的 Behemoth (2T) 虽然未发布，但其通过新颖的共蒸馏技术，在预训练阶段就将其知识传递给了 Scout 和 Maverick。 这种共蒸馏发生在预训练过程中，使用了动态加权软目标（教师模型的 logits）和硬目标（真实标签）的新型蒸馏损失函数，显著提升了学生模型的质量（尤其在数学、编码等领域），同时摊销了教师模型的训练成本。 大规模 RL 基础设施\n为了训练 Behemoth 这样的超大 MoE 模型，Meta 彻底改造了 RL 基础设施，采用了完全异步的在线 RL 训练框架，优化了 MoE 并行策略，实现了灵活的 GPU 资源分配和近 10 倍的训练效率提升。\n对比 特性 LLaMA 1 LLaMA 2 Code Llama Llama Guard LLaMA 3 LLaMA 4 发布时间 2023/02 2023/07 2023/08 2023/12+ 2024/04+ 2025/04+ 基础模型 - - LLaMA 2 LLaMA 2 / LLaMA 3 - - 模型规模 7B - 65B 7B, 13B, 70B 7B - 70B 7B / 8B (+Vision) 1B - 405B (+Vision) 109B, 400B, ~2T (MoE) 训练数据量 1T - 1.4T tokens 2T+ tokens + 0.5T/1T Code ~40k 安全分类 15T+ tokens 30T+ tokens (多模态) 上下文长度 2k tokens 4k tokens 100k 4k / 8k+ 8k / 128k tokens 10M Tokenizer SentencePiece (32k) SentencePiece (32k) SentencePiece (32k) 基于 LLaMA 2/3 tiktoken (128k) tiktoken (256k) 位置编码 RoPE RoPE RoPE (基数调整) RoPE RoPE iRoPE 注意力 MHA MHA / GQA (34B, 70B) MHA / GQA (\u0026gt;13B) 基于 LLaMA 2/3 GQA GQA 归一化 RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) 激活函数 SwiGLU SwiGLU SwiGLU SwiGLU SwiGLU SwiGLU 模型类别 文本模型 文本模型 代码生成 安全分类型 多模态模型 多模态模型 关键技术解析 以下是对 LLaMA 系列中广泛采用的关键技术的解析。\nRMS Normalization (RMSNorm) 在深度学习模型训练中，归一化层对于加速收敛、提高泛化能力和稳定训练过程至关重要。RMSNorm (Root Mean Square Normalization) (Zhang \u0026amp; Sennrich, 2019) 是 Layer Normalization 的一种简化变体，它仅使用输入的均方根（Root Mean Square）进行归一化，省去了均值中心化步骤，从而减少了计算量。\n其数学表达式为： $$ \\text{RMSNorm}(x) = \\frac{x}{\\text{RMS}(x)} \\cdot \\gamma = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$ 其中：\n$ x \\in \\mathbb{R}^d $ 是输入向量。 $ d $ 是向量维度。 $ \\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon} $ 计算输入的均方根。 $ \\epsilon $ 是一个很小的正数（如 $10^{-6}$），用于防止分母为零，增加数值稳定性。 $ \\gamma \\in \\mathbb{R}^d $ 是一个可学习的缩放参数向量（gain）。RMSNorm 通常省略了 LayerNorm 中的可学习偏移参数（bias）$ \\beta $。 LLaMA 系列选择 RMSNorm 的主要原因：\n计算效率高： 相比 LayerNorm，RMSNorm 计算量更小，因为它不需要计算均值。这对于计算密集型的大语言模型训练和推理尤为重要。 性能相当： 实践证明，RMSNorm 在 Transformer 等架构中通常能达到与 LayerNorm 相当甚至更好的性能，同时保持训练稳定性。 实现简单： 其计算逻辑相对简单，易于在各种硬件上高效实现。 关于各种 Norm 的对比和代码实现，可参考博客：Normalization in Deep Learning。\nFFN_SwiGLU Swish-Gated Linear Unit (Shazeer, 2020) 是 LLaMA 中用于增强前馈网络（Feed-Forward Network, FFN）非线性表达能力的关键技术。SwiGLU 结合了 Swish 激活函数和门控机制，显著提升了模型的表现力和性能。此外，与 PaLM (Chowdhery, 2022) 中使用的$4 d$隐藏维度不同，LLaMA 采用了 $\\frac{2}{3}d$ 的隐藏维度，从而在保持参数量和计算量不变的情况下，实现了更高的参数效率。\n数学表达式： $$ \\operatorname{FFN}_{\\mathrm{SwiGLU}}\\left(x, W_1, W_3, W_2\\right)=\\left(\\operatorname{Swish}\\left(x W_1\\right) \\otimes x W_3\\right) W_2 $$ 其中：\n$ \\text{Swish}(x) = x \\cdot \\sigma(x) $（Swish 激活函数）。 $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $（Sigmoid 函数）。 $ \\otimes $ 表示逐元素相乘。 $ W_1, W_2, W_3 $ 为线性变换矩阵。 优势：\n增强非线性表达：SwiGLU 通过结合 Swish 激活函数与门控机制，能够更有效地捕捉复杂的模式和关系，提升 FFN 层的表达能力。 参数效率：采用 $\\frac{2}{3}d$ 的隐藏维度，在引入额外的线性变换矩阵的同时，保持了总参数量不变，实现了参数的高效利用。 性能提升：在多项基准测试中，FFN_SwiGLU 显著提升了模型的性能，尤其在处理复杂任务和长文本时表现尤为出色。例如，在文本生成和理解任务中，SwiGLU 帮助模型更好地理解上下文和长距离依赖关系。 实现细节：\n权重矩阵调整：为了保持与传统 FFN 层相同的参数量和计算量，SwiGLU 通过减少隐藏层的维度（例如，将隐藏层大小从 4d 调整为 $\\frac{2}{3}d$），在引入额外的线性变换矩阵的同时，确保整体模型的效率不受影响。 兼容性：SwiGLU 作为 GLU 家族的一员，能够无缝集成到现有的 Transformer 架构中，替代传统的 ReLU 或 GELU 激活函数，提升模型的整体性能。 实现代码可以参考这个文件：swiglu.py。\nGrouped Query Attention (GQA) Grouped Query Attention (GQA) (Ainslie et al., 2023) 是一种对标准多头注意力（Multi-Head Attention, MHA）机制的关键优化技术，特别是在像 LLaMA 这样的大型语言模型中应用。其主要目标是在推理过程中，减少加载和存储 KV Cache 所需的内存带宽和容量，从而在模型性能和计算效率之间取得更好的平衡。\nGQA 是 MHA 和多查询注意力（Multi-Query Attention, MQA）之间的一种折中：\nMHA： 有 $H$ 个查询头（Query heads），每个头都有自己独立的 $H$ 组 K 和 V 投影。计算量和 KV Cache 大小与头数 $H$ 成正比。 MQA： 仍然有 $H$ 个查询头，但所有头共享同一组 K 和 V 投影。这极大地减少了 KV Cache 大小（减少为 MHA 的 $1/H$），但可能导致模型质量下降。 GQA： 将 $H$ 个查询头分成 $G$ 组（$1 \u0026lt; G \u0026lt; H$，且 $H$ 是 $G$ 的倍数），每组内的 $H/G$ 个查询头共享同一组 K 和 V 投影。总共有 $G$ 组 K 和 V 投影。 Fig. 15. Overview of Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). GQA groups query heads to share key/value heads. (Source: Ainslie et al., 2023)\n计算步骤如下：\n投影： 输入 $X$ 仍然投影得到 $Q, K, V$。$Q$ 被划分为 $H$ 个头 $Q_1, \\dots, Q_H$。$K$ 和 $V$ 被划分为 $G$ 组 $K^1, \\dots, K^G$ 和 $V^1, \\dots, V^G$。 分组注意力： 对于第 $g$ 组（$g=1, \\dots, G$），该组对应的查询头（例如 $Q_i$ 其中 $i$ 属于第 $g$ 组）与共享的 $K^g$ 和 $V^g$ 计算注意力： $$ \\text{Attention}_i(Q_i, K^g, V^g) = \\text{softmax}\\left( \\frac{Q_i (K^g)^\\top}{\\sqrt{d_k}} \\right) V^g $$ 其中 $d_k$ 是每个 K 头（也是 Q 头）的维度。 拼接与输出： 所有头的输出 $ \\text{Attention}_1, \\dots, \\text{Attention}_H $ 拼接起来，再通过一个输出投影矩阵 $W_O$ 得到最终输出。 优势：\n平衡性能与效率： GQA 在大幅减少 KV Cache（是 MHA 的 $G/H$）的同时，通常能保持比 MQA 更接近 MHA 的模型质量。 加速推理： 减少内存带宽需求可以显著加速大模型的推理速度，尤其是在长序列生成场景下。 更多关于注意力机制在 MHA、MQA 和 GQA 之间的详细对比及代码示例，可参考博客：Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA。\nRotary Positional Embeddings (RoPE) Rotary Positional Embeddings (RoPE) (Su et al., 2021) 是一种用于将位置信息注入 Transformer 注意力机制的有效方法，特别擅长编码相对位置信息。与传统的绝对位置编码（如正弦编码或可学习编码）不同，RoPE 通过对 Query 和 Key 向量应用与位置相关的旋转操作来实现。\nFig. 16. Implementation of Rotary Position Embedding(RoPE). (Source: Su et al., 2021)\n假设 $q_m$ 和 $k_n$ 分别是位置 $m$ 的 Query 向量和位置 $n$ 的 Key 向量。RoPE 将 $d$ 维的向量 $x$ （$q$ 或 $k$）视为 $d/2$ 个二维向量块 $[x^{(1)}, x^{(2)}, \\dots, x^{(d/2)}]$，其中 $x^{(i)} = [x_{2i-1}, x_{2i}]$。对于位置 $m$，RoPE 定义了一个旋转矩阵 $R_m$，它由 $d/2$ 个二维旋转矩阵组成： $$ R_m = \\text{diag}(R_{m,1}, R_{m,2}, \\dots, R_{m,d/2}) $$其中每个二维旋转矩阵为： $$ R_{m,i} = \\begin{pmatrix} \\cos(m\\theta_i) \u0026 -\\sin(m\\theta_i) \\\\ \\sin(m\\theta_i) \u0026 \\cos(m\\theta_i) \\end{pmatrix} $$旋转频率 $ \\theta_i = b^{-2(i-1)/d} $，其中 $b$ 是一个预设的基数（LLaMA 中通常为 10000）。\n应用 RoPE 后，新的 Query 和 Key 向量为 $q\u0026rsquo;_m = R_m q_m$ 和 $k\u0026rsquo;_n = R_n k_n$。关键在于，它们之间的内积（点积，决定注意力分数）只依赖于相对位置 $m-n$：\n$$ (q'_m)^\\top k'_n = (R_m q_m)^\\top (R_n k_n) = q_m^\\top R_m^\\top R_n k_n = q_m^\\top R_{n-m} k_n $$这里利用了旋转矩阵的性质 $R_m^\\top R_n = R_{n-m}$。\n优势：\n显式相对位置编码： 内积结果直接依赖于相对距离 $m-n$，这对于捕捉序列中元素间的相对关系非常自然。 长距离衰减特性： 随着相对距离 $|m-n|$ 的增大，旋转导致的向量间夹角变化通常会使得内积值衰减，符合直觉（距离越远，关联越弱）。 良好的外推性： 理论上，RoPE 可以较好地泛化到比训练时更长的序列长度，因为它不依赖于绝对位置的最大值。通过调整基数 $b$（如 Code Llama 和 LLaMA 4 的 iRoPE），可以进一步优化其在超长上下文下的表现。 无额外参数： RoPE 是一种固定的、基于位置的变换，不引入额外的可学习参数。 兼容线性注意力： 可以与各种线性注意力变体结合使用。 Mixture-of-Experts (MoE) Mixture-of-Experts (MoE) 是一种旨在提升模型容量（总参数量）同时控制计算成本（活跃参数量）的神经网络架构范式。它将网络中的某些层（通常是 FFN 层）替换为多个并行的专家子网络，并通过一个轻量级的门控网络（gating network）为每个输入 token 动态地选择性地激活其中少数几个（通常是 Top-K，K=1 或 2）专家进行计算。\nFig. 17. The Illustration of a mixture-of-experts(MoE) in llama4. (Source: Meta AI, 2025)\n假设一个 MoE 层有 $N$ 个专家 $E_1, E_2, \\dots, E_N$（例如，每个专家是一个独立的 FFN 网络）和一个门控网络 $G$。对于输入 token $x$，MoE 层的计算过程如下：\n门控计算： 门控网络 $G$（通常是一个简单的线性层加 Softmax）计算每个专家被选中的概率或权重：$p = G(x) = \\text{Softmax}(\\text{Linear}(x))$，其中 $p \\in \\mathbb{R}^N$。 专家选择 (Top-K Routing)： 根据门控输出 $p$，选择得分最高的 K 个专家。设选中的专家索引集合为 $\\mathcal{T} = \\text{TopKIndices}(p)$。 专家计算： 只有被选中的 K 个专家对输入 $x$ 进行计算，得到输出 $E_i(x)$ for $i \\in \\mathcal{T}$。 输出组合： 最终的输出 $y$ 是被选中专家的输出根据其门控权重（通常是重新归一化后的权重）的加权和： $$ y = \\sum_{i \\in \\mathcal{T}} \\frac{p_i}{\\sum_{j \\in \\mathcal{T}} p_j} \\cdot E_i(x) $$ 或者在某些实现中，权重可能直接使用 $p_i$。 优势：\n参数规模与计算解耦： MoE 允许模型拥有巨大的总参数量（通过增加专家数量 $N$），但每次前向传播的计算量仅取决于激活的 K 个专家的计算量，远低于同等总参数量的密集（Dense）模型。这使得在有限的计算预算下可以训练出容量更大、可能性能更强的模型。 专家特化： 理论上，不同的专家可以学习处理不同类型的数据、模式或任务的特定方面，实现知识的模块化存储和处理，从而提升模型的整体能力和泛化性。 挑战：\n负载均衡 (Load Balancing)： 需要确保所有专家被大致均匀地利用，避免某些专家过载而其他专家空闲。通常需要引入辅助损失函数（如 Load Balancing Loss）来鼓励均匀路由。 通信开销： 在分布式训练和推理中，需要在不同设备（GPU）之间进行高效的通信（如 All-to-All）来将 token 路由到存储相应专家的设备上，并收集结果。这增加了实现的复杂性和通信成本。 训练稳定性： MoE 模型的训练可能比密集模型更不稳定，需要仔细调整超参数和训练策略。 内存占用： 虽然计算量是稀疏的，但模型总参数量巨大，需要大量内存来存储所有专家权重。 关于 MoE 更详细的说明，可参考博客：Parallelism and Memory Optimization Techniques for Training Large Models的混合专家模型部分。\n参考文献 [1] Touvron, Hugo, et al. \u0026ldquo;LLaMA: Open and Efficient Foundation Language Models.\u0026rdquo; arXiv preprint arXiv:2302.13971 (2023).\n[2] Facebook Research. \u0026ldquo;xformers.\u0026rdquo; GitHub repository (Accessed 2024).\n[3] Taori, Rohan, et al. \u0026ldquo;Alpaca: A Strong, Replicable Instruction-Following Model.\u0026rdquo; Stanford CRFM Blog (2023).\n[4] Touvron, Hugo, et al. \u0026ldquo;Llama 2: Open Foundation and Fine-Tuned Chat Models.\u0026rdquo; arXiv preprint arXiv:2307.09288 (2023).\n[5] Bai, Yuntao, et al. \u0026ldquo;Constitutional AI: Harmlessness from AI Feedback.\u0026rdquo; arXiv preprint arXiv:2212.08073 (2022).\n[6] Roziere, Baptiste, et al. \u0026ldquo;Code Llama: Open Foundation Models for Code.\u0026rdquo; arXiv preprint arXiv:2308.12950 (2023).\n[7] Inan, Hakan, et al. \u0026ldquo;Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations.\u0026rdquo; arXiv preprint arXiv:2312.06674 (2023).\n[8] Chi, Jianfeng, et al. \u0026ldquo;Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations.\u0026rdquo; arXiv preprint arXiv:2411.10414 (2024).\n[9] Vidgen, Bertie, et al. \u0026ldquo;Introducing v0.5 of the AI Safety Benchmark from MLCommons.\u0026rdquo; arXiv preprint arXiv:2404.12241 (2024).\n[10] Meta Llama. \u0026ldquo;Llama-Guard-3-8B.\u0026rdquo; Hugging Face Model (Accessed 2024).\n[11] Grattafiori, Aaron, et al. \u0026ldquo;The Llama 3 Herd of Models.\u0026rdquo; arXiv preprint arXiv:2407.21783 (2024).\n[12] Meta AI. \u0026ldquo;Introducing Llama 3.1: Our most capable models to date.\u0026rdquo; Meta AI Blog (2024).\n[13] Umar Jamil. \u0026ldquo;pytorch-llama Slides.\u0026rdquo; GitHub file (Accessed 2024).\n[14] Pang, Richard Yuanzhe, et al. \u0026ldquo;Iterative reasoning preference optimization.\u0026rdquo; Advances in Neural Information Processing Systems 37 (2024): 116617-116637.\n[15] Meta AI. \u0026ldquo;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation\u0026rdquo; Meta AI Blog (2025).\n[16] Xu, Hu, et al. \u0026ldquo;Demystifying clip data.\u0026rdquo; arXiv preprint arXiv:2309.16671 (2023).\n[17] Kazemnejad, Amirhossein, et al. \u0026ldquo;The impact of positional encoding on length generalization in transformers.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 24892-24928.\n[18] Nakanishi, Ken M. \u0026ldquo;Scalable-Softmax Is Superior for Attention.\u0026rdquo; arXiv preprint arXiv:2501.19399 (2025).\n[19] Zhang, Biao, and Rico Sennrich. \u0026ldquo;Root mean square layer normalization.\u0026rdquo; Advances in Neural Information Processing Systems 32 (2019).\n[20] Shazeer, Noam. \u0026ldquo;Glu variants improve transformer.\u0026rdquo; arXiv preprint arXiv:2002.05202 (2020).\n[21] Ainslie, Joshua, et al. \u0026ldquo;Gqa: Training generalized multi-query transformer models from multi-head checkpoints.\u0026rdquo; arXiv preprint arXiv:2305.13245 (2023).\n[22] Su, Jianlin, et al. \u0026ldquo;Roformer: Enhanced transformer with rotary position embedding.\u0026rdquo; Neurocomputing 568 (2024): 127063.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Apr 2025). LLaMA 系列模型. https://syhya.github.io/zh/posts/2025-04-06-llama\nOr\n@article{syhya2025llama, title = \u0026#34;LLaMA 系列模型\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-04-06-llama\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-04-06-llama/","summary":"\u003ch2 id=\"llama\"\u003eLLaMA\u003c/h2\u003e\n\u003cp\u003eMeta AI 推出的 LLaMA 系列开源模型已成为大语言模型社区的基石之一，对推动开放研究和应用产生了深远影响。从 2023 年初发布的开创性 LLaMA，到同年性能显著提升的 LLaMA 2，再到针对特定领域（如代码、安全）的衍生模型，以及 2024 年和 2025 年相继推出的新一代 LLaMA 3 和 LLaMA 4，Meta 持续致力于提升开源模型的性能，使其逐步逼近最先进的闭源模型。下面，我们将依次介绍每个主要模型的关键技术细节。\u003c/p\u003e","title":"LLaMA 系列模型"},{"content":"智能体 自 2022 年 10 月 OpenAI 发布 ChatGPT 以来，随着后续 AutoGPT 和 AgentGPT 等项目的涌现，LLM 相关的智能体（Agent）逐渐成为近年来 AI 的研究热点和实际应用方向。本文将介绍智能体的基本概念、核心技术及其最新应用进展。\nLLM Agent 大语言模型智能体(Large Language Model Agent, LLM agent) 利用 LLM 作为系统大脑，并结合规划、记忆与外部工具等模块，实现了对复杂任务的自动化执行。\n用户请求: 用户通过提示词输入任务，与智能体互动。 智能体: 系统大脑，由一个或多个 LLMs 构成，负责整体协调和执行任务。 规划: 将复杂任务拆解为更小的子任务，并制定执行计划，同时通过反思不断优化结果。 记忆: 包含短期记忆(利用上下文学习即时捕捉任务信息)和长期记忆(采用外部向量存储保存和检索关键信息，确保长时任务的信息连续性)。 工具: 集成计算器、网页搜索、代码解释器等外部工具，用于调用外部数据、执行代码和获取最新信息。 Fig. 1. The illustration of LLM Agent Framework. (Image source: DAIR.AI, 2024)\nRL Agent 强化学习(Reinforcement Learning，RL) 的目标是训练一个智能体(agent)在给定的环境 (environment) 中采取一系列动作(actions, $a_t$)。在交互过程中，智能体从一个状态(state, $s_t$)转移到下一个状态，并在每次执行动作后获得环境反馈的奖励(reward, $r_t$)。这种交互生成了完整的轨迹(trajectory, $\\tau$)，通常表示为:\n$$ \\tau = \\{(s_0, a_0, r_0), (s_1, a_1, r_1), \\dots, (s_T, a_T, r_T)\\}. $$智能体的目标是学习一个策略(policy, $\\pi$)，即在每个状态下选择动作的规则，以最大化期望累积奖励，通常表达为：\n$$ \\max_{\\pi} \\, \\mathbb{E}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right], $$ 其中 $\\gamma \\in [0,1]$ 为折扣因子，用于平衡短期与长期奖励。\nFig. 2. The agent-environment interaction. (Image source: Sutton \u0026amp; Barto, 2018)\n在 LLM 场景中，可以将模型视为一个智能体，而“环境”可理解为用户输入及其对应的期望回答方式：\n状态($s_t$)：可以是当前对话上下文或用户的问题。 动作($a_t$)：模型输出的文本(回答、生成内容等)。 奖励($r_t$)：来自用户或系统的反馈(如用户满意度、奖励模型自动评分等)。 轨迹($\\tau$)：从初始对话到结束的所有文本交互序列，可用于评估模型的整体表现。 策略($\\pi$)：LLM 在每个状态下(对话上下文)如何生成文本的规则，一般由模型的参数所决定。 对于 LLM，传统上先通过海量离线数据进行预训练，而在后训练强化学习环节中，则通过人类或者模型的反馈对模型进行训练，使其输出更符合人类偏好或任务需求的高质量文本。\n对比 下表展示了两者之间的差异：\n对比维度 LLM Agent RL Agent 核心原理 规划、记忆和工具实现复杂任务自动化。 通过与环境互动的试错反馈循环，不断优化策略以最大化长期奖励。 优化方式 不直接更新模型参数，主要依靠上下文扩展、外部记忆和工具提高性能。 持续频繁更新策略模型参数，依赖环境反馈的奖励信号不断优化。 互动方式 使用自然语言与用户或外部系统交互，灵活调用多种工具获得外部信息。 与真实或模拟环境交互，环境提供奖励或惩罚，形成闭环反馈。 实现目标 分解复杂任务、借助外部资源完成任务，关注任务结果质量与准确性。 最大化长期奖励，追求短期与长期回报之间的最优平衡。 随着研究的深入，LLM 与 RL 智能体的结合呈现出更多可能性，例如：\n利用强化学习方法训练 Reasoning LLM(如 o1/o3)，使其更适合作为 LLM 智能体的基础模型。 同时，记录 LLM 智能体执行任务的数据与反馈，为 Reasoning LLM 提供丰富的训练数据，从而提升模型性能。 规划: 任务分解 LLM Agent 的核心组件包括规划、记忆和工具使用，这些组件共同协作，使智能体能够自主执行复杂任务。\nFig. 3. Overview of a LLM-powered autonomous agent system. (Image source: Weng, 2017)\n规划对于成功执行复杂任务至关重要。它可以根据复杂性和迭代改进的需求以不同的方式进行。在简单的场景中，规划模块可以使用 LLM 预先概述详细的计划，包括所有必要的子任务。此步骤确保智能体系统地进行 任务分解 并从一开始就遵循清晰的逻辑流程。\n思维链 思维链(Chain of Thought, CoT) (Wei et al. 2022) 通过逐步生成一系列简短句子来描述推理过程，这些句子称为推理步骤。其目的是显式地展示模型的推理路径，帮助模型更好地处理复杂推理任务。下图展示了少样本提示（左侧）和思维链提示（右侧）的区别。少样本提示得到错误答案，而思维链方法则引导模型逐步陈述推理过程，更清晰地体现模型的逻辑过程，从而提升答案准确性和可解释性。\nFig. 4. The comparison example of few-shot prompting and CoT prompting. (Image source: Wei et al. 2022)\n零样本思维链提示(Zero-Shot CoT) (Kojima et al. 2022)是 CoT 的后续研究，提出了一种极为简单的零样本提示方式。他们发现，仅需在问题末尾添加一句 Let's think step by step，LLM 便能够产生思维链，可以获得更为准确的答案。\nFig. 5. The comparison example of few-shot prompting and CoT prompting. (Image source: Kojima et al. 2022)\n多模态思维链 (Multimodal CoT, MCoT)(Zhang et al. 2023) 将 文本与视觉信息 融合到推理过程中，突破了传统 CoT 仅依赖语言模态的局限。其框架分为两阶段：\n推理链生成：基于多模态信息（文本 + 图像）生成解释性推理链。 答案推断：利用生成的推理链作为辅助，完成最终答案推断。 Fig. 6. Overview of Multimodal-CoT framework. (Image source: Zhang et al. 2023)\n自洽采性 自洽采样(Self-consistency sampling)(Wang et al. 2022a) 通过对同一提示词在 temperature \u0026gt; 0 的情况下多次采样，生成多个多样化答案，并从中选出最佳答案的方法。其核心思想是通过采样多个推理路径，再进行多数投票以提高最终答案的准确性和稳健性。不同任务中选择最佳答案的标准可以有所不同，一般情况下采用多数投票作为通用方案。而对于如编程题这类易于验证的任务，则可以通过解释器运行并结合单元测试对答案进行验证。这是一种对 CoT 的优化，与其结合使用时，能够显著提升模型在复杂推理任务中的表现。\nFig. 7. Overview of the Self-Consistency Method for Chain-of-Thought Reasoning. (Image source: Wang et al. 2022a)\n下面是一些后续优化的工作：\n(Wang et al. 2022b)后续采用另一种集成学习方法进行优化, 通过改变示例顺序或以模型生成的推理代替人为书写，增加随机性后再多数投票。 Fig. 8. An overview of different ways of composing rationale-augmented ensembles, depending on how the randomness of rationales is introduced. (Image source: Wang et al. 2022b)\n如果训练样本仅提供正确答案而无推理依据，可采用STaR(Self-Taught Reasoner)(Zelikman et al. 2022)方法：(1) 让LLM生成推理链，仅保留正确答案的推理。 (2) 用生成的推理微调模型，反复迭代直至收敛。注意 temperature 高时易生成带正确答案但错误推理的结果。如无标准答案，可考虑将多数投票视作“正确答案”。 Fig. 9. An overview of STaR and a STaR-generated rationale on CommonsenseQA. (Image source: Zelikman et al. 2022)\n(Fu et al. 2023)发现复杂程度更高的示例（推理步骤更多）可提高模型性能。分隔推理步骤时，换行符 \\n 效果优于step i、. 或 ;。此外，通过复杂度一致性策略(Complexity-based consistency)，即仅对生成复杂度 top $k$ 的推理链进行多数投票，也能进一步优化模型输出。同时，将提示中的 Q: 替换为 Question: 也被证明对性能有额外提升。 Fig. 10. Sensitivity analysis on step formatting. Complex prompts consistently lead to better performance with regard to different step formatting. (Image source: Fu et al. 2023)\n思维树 思维树 (Tree of Thoughts, ToT)(Yao et al. 2023) 在 CoT 基础上拓展，每一步都探索多个推理可能性。它首先将问题分解为多个思考步骤，并在每一步产生多个不同想法，从而形成树形结构。搜索过程可采用广度优先搜索（Breadth-first search, BFS）或深度优先搜索（Depth-first search, DFS），并通过分类器（也可以让 LLM 进行打分）或多数票方式评估每个状态。它包含三个主要步骤：\n扩展(Expand)：生成一个或多个候选解决方案。 评分(Score)：衡量候选方案的质量。 剪枝(Prune)：保留排名 top $k$ 的最佳候选方案。 如果没有找到解决方案（或者候选方案的质量不够高），则回撤到扩展步骤。\nFig. 11. Schematic illustrating various approaches to problem solving with LLMs. (Image source: Yao et al. 2023)\n提示词工程 提示工程（Prompt Engineering），又称为上下文提示（In-Context Prompting），通过优化输入 prompt 来引导 LLM 产生期望输出的技巧。其核心目标是在不更新模型权重的前提下，通过与模型的高效沟通来控制其行为。\n零样本提示 零样本提示 (Zero-Shot Prompting) 直接向模型提供任务指令，而不提供任何示例。这种方法完全依赖模型在预训练阶段学到的知识和指令遵循能力。例如，进行情感分析：\nFig. 12. Zero-Shot Prompting.\n对于已经过指令微调的模型，如 GPT-5 或 Claude 4，它们能够很好地理解并执行这类直接指令。\n少样本提示 少样本提示 (Few-Shot Prompting) 是在提示中提供一组高质量的示例，每个示例都包含输入和期望的输出。通过这些示例，模型能够更好地理解用户的意图和任务的具体要求，从而获得比零样本提示更优的性能。然而，这种方法的缺点是会消耗更多的上下文窗口长度。例如，提供几个情感分析的示例：\nFig. 13. Few-Shot Prompting.\n自动提示构建 自动提示工程师（Automatic Prompt Engineer, APE）(Zhou et al. 2022) 是一种在模型生成的候选指令池中进行搜索的方法。它通过筛选候选集合，并依据选定的评分函数最终选择得分最高的候选指令。\nFig. 14. Automatic Prompt Engineer (APE) workflow. (Image source: Zhou et al. 2022)\n自动思维链 (Automatic Chain-of-Thought, Auto-CoT) (Zhang et al. 2022) 提出了一种自动化构建思维链示例的方法，旨在解决人工设计提示耗时且可能并非最优的问题。其核心思想是，通过聚类技术对问题进行采样，然后利用大语言模型 (LLM) 自身的零样本推理能力来自动生成推理链，从而构建多样的、高质量的示例。\nFig. 15. Overview of the Auto-CoT method. (Image source: Zhang et al. 2022)\nAuto-CoT 包含两个主要阶段：\n问题聚类：将数据集中的问题进行嵌入并运行 $k$-means 等算法进行聚类。此步骤旨在将相似的问题划分到同一个簇，以保证后续采样问题的多样性。 示例选择与推理生成：从每个簇中选择一个或多个有代表性的问题（例如，选择离簇中心最近的问题）。随后，利用 Zero-Shot CoT 提示来让 LLM 为这些选定的问题生成推理链。这些自动生成的“问题-推理链”对便组成了最终用于执行任务的少样本提示。 知识增强 在应对知识密集型或常识推理任务时，单纯依赖 LLM的参数化知识往往不足，可能导致错误或过时答案。为解决这一问题，研究者提出了两类思路：\n生成知识提示 (Generated Knowledge Prompting)(Liu et al. 2022) 是一种在预测之前让模型先生成相关知识的方法。其核心思想是：当任务需要常识或外部信息时，模型可能因缺乏背景而犯错；若先引导模型生成与输入相关的知识，再基于这些知识作答，则能提升推理的准确性。\nFig. 16. Overview of the Generated Knowledge Prompting. (Image source: Liu et al. 2022)\n知识生成：根据输入，模型先生成相关事实性知识。 知识整合：将生成的知识与原问题合并，形成新的提示输入。 答案推理：基于增强后的输入进行回答。 检索增强生成 (Retrieval Augmented Generation, RAG)(Lewis et al. 2021) 是一种结合 信息检索与文本生成 的方法，旨在解决知识密集型任务。其核心思想是：单纯依赖 LLM 的参数化知识（静态）容易导致事实错误，而通过引入外部知识库检索，可以提升生成结果的 事实一致性与时效性。\nFig. 17. Overview of the Retrieval Augmented Generation. (Image source: Lewis et al. 2021)\n检索：从外部知识源（如 Wikipedia 或者私有知识库）检索相关文档。 整合：将检索到的文档与原始输入拼接，作为提示上下文。 生成：由生成模型（原始论文使用的预训练 seq2seq 模型, 如今主流使用 LLM）基于扩展后的提示输出答案。 主动提示 主动提示（Active Prompt）(Diao et al. 2023)针对传统 CoT 方法依赖固定人工标注示例的局限提出改进。问题在于：固定示例并不一定最适合所有任务，可能导致泛化不足。Active Prompt 通过引入 active learning 策略，自适应地选择和更新任务相关的最佳示例，从而提升模型的推理效果。\nFig. 18. Illustrations of active prompting framework. (Image source: Diao et al. 2023)\n不确定性估计：在有或没有少量人工 CoT 示例的情况下，让 LLM 针对训练问题生成 k 个答案（文中 k=5），并基于这些答案的差异性计算不确定性指标。 选择：根据不确定性水平，筛选最不确定的问题。 人工标注：对筛选出的问题进行人工标注，补充新的高质量 CoT 示例。 推理：使用新标注的示例进行推理，从而提升模型在目标任务上的表现。 规划: 自我反思 自我反思(Self-Reflexion) 是使 Agent 通过改进过去的行动决策和纠正以往错误而实现迭代提升的关键因素。在试错不可避免的现实任务中，它起着至关重要的作用。\nReAct ReAct(Reason + Act) (Yao et al. 2023) 框架通过将任务特定的离散动作和语言空间相结合，实现了 LLM 中推理与行动的无缝整合。这种设计不仅使模型能够通过调用例如 Wikipedia 搜索 API 等外部接口与环境进行交互，同时还能以自然语言生成详细的推理轨迹，从而解决复杂问题。\nReAct 提示模板包含明确的思考步骤，其基本格式如下：\nThought：... Action：... Observation：... ...(Repeated many times) Fig. 19. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023)\n从下图可以看出在知识密集型任务和决策任务中，ReAct 的表现均明显优于仅依赖Action的基础方法，从而展示了其在提升推理效果和交互性能方面的优势。\nFig. 20. PaLM-540B prompting results on HotpotQA and Fever. (Image source: Yao et al. 2023)\nReflexion Reflexion(Shinn et al. 2023)让 LLM 能够通过自我反馈与动态记忆不断迭代、优化决策。\n这种方法本质上借鉴了强化学习的思想，在传统的 Actor-Critic 模型中，Actor 根据当前状态 $s_t$ 选择动作 $a_t$，而 Critic 则会给出估值（例如价值函数 $V(s_t)$ 或动作价值函数 $Q(s_t,a_t)$），并反馈给 Actor 进行策略优化。对应地，在 Reflexion 的三大组件中：\nActor：由 LLM 扮演，基于环境状态（包括上下文和历史信息）输出文本及相应动作。可记为：\n$$ a_t = \\pi_\\theta(s_t), $$其中 $\\pi_\\theta$ 表示基于参数 $\\theta$（即 LLM 的权重或提示）得到的策略。Actor 与环境交互并产生轨迹 $\\tau = {(s_1,a_1,r_1), \\dots, (s_T,a_T,r_T)}$。\nEvaluator：类似于 Critic，Evaluator 接收由 Actor 生成的轨迹并输出奖励信号 $r_t$。在 Reflexion 框架里，Evaluator 可以通过预先设计的启发式规则或额外的 LLM 来对轨迹进行分析，进而产生奖励。例如：\n$$ r_t = R(\\tau_t), $$其中 $R(\\cdot)$ 为基于当前轨迹 $\\tau_t$ 的奖励函数。\nSelf-Reflection：该模块相当于在 Actor-Critic 之外额外增加了自我调节反馈机制。它整合当前轨迹 $\\tau$、奖励信号 ${r_t}$ 以及长期记忆中的历史经验，利用语言生成能力产生针对下一次决策的自我改进建议。这些反馈信息随后被写入外部记忆，为后续 Actor 的决策提供更丰富的上下文，从而在不更新 LLM 内部参数的情况下，通过提示词的动态调整实现类似于策略参数 $\\theta$ 的迭代优化。\nFig. 21. (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm. (Image source: Shinn et al. 2023)\nReflexion 的核心循环与算法描述如下：\n初始化\n同时实例化 Actor、Evaluator、Self-Reflection 三个模型（均可由 LLM 实现），分别记为 $M_a, M_e, M_{sr}$。 初始化策略 $\\pi_\\theta$（包含 Actor 的模型参数或提示，以及初始记忆等信息）。 先让 Actor 按当前策略 $\\pi_\\theta$ 生成一个初始轨迹 $\\tau_0$，$M_e$ 进行评估后，再由 $M_{sr}$ 生成首条自我反思文本并存入长期记忆。 生成轨迹\n在每一次迭代中，$M_a$ 读取当前的长期记忆及环境观测，依次输出动作 ${a_1, a_2, \\ldots}$，与环境交互并获得相应反馈，形成新的轨迹 $\\tau_t$。 $\\tau_t$ 可以视作该任务的短期记忆，仅在本轮迭代中使用。 评估\n$M_e$ 根据轨迹 $\\tau_t$（即 Actor 的行为与环境反馈序列），输出奖励或评分 ${r_1, r_2, \\ldots}$。这一步对应 $M_e$ 的内部反馈，或由外部环境直接给出结果。 自我反思\n$M_{sr}$ 模块综合轨迹 $\\tau_t$ 与奖励信号 ${r_t}$ 在语言层面生成自我修正或改进建议 $\\mathrm{sr}_t$。 反思文本既可视为对错误的剖析，也可提供新的启发思路，并通过存储到长期记忆中。实践中，我们可以将反馈信息向量化后存入向量数据库。 更新并重复\n将最新的自我反思文本 $\\mathrm{sr}_t$ 追加到长期记忆后，Actor 在下一轮迭代时即可从中采用 RAG 检索历史相关信息来调整策略。 反复执行上述步骤，直至 $M_e$ 判定任务达成或到达最大轮次。在此循环中，Reflexion 依靠 自我反思 + 长期记忆 的持续累加来改进决策，而非直接修改模型参数。 下面分别展示了 Reflexion 在决策制定、编程和推理任务中的应用示例：\nFig. 22. Reflexion works on decision-making, programming, and reasoning tasks. (Image source: Shinn et al. 2023)\n在 100 个 HotPotQA 问题的实验中，通过对比 CoT 方法和加入 episodic memory 的方式，结果显示采用 Reflexion 方法在最后增加自我反思步骤后，其搜索、信息检索和推理能力提升明显。\nFig. 23. Comparative Analysis of Chain-of-Thought (CoT) and ReAct on the HotPotQA Benchmark. (Image source: Shinn et al. 2023)\nDeepSeek R1 DeepSeek-R1 (DeepSeek-AI, 2025) 代表了开源社区在复现 OpenAI o1 (OpenAI, 2024) 方面的重大突破，通过强化学习技术成功训练出具备深度反思能力的高级推理模型。\n关于 DeepSeek R1 的详细训练流程与技术实现，请参考我之前的博客：OpenAI o1复现进展：DeepSeek-R1。\nDeepSeek-R1-Zero 在训练过程中的关键转变 — 随着训练的深入，模型逐渐涌现出卓越的自我进化能力。这种能力体现在三个核心方面：\n自我反思：模型能够回溯并批判性地评估先前的推理步骤。 主动探索：当发现当前解题路径不理想时，能够自主寻找并尝试替代方案。 动态思考调整：根据问题复杂度自适应地调整生成 token 数量，实现更深入的思考过程。 这种动态且自发的推理行为显著提升了模型解决复杂问题的能力，使其能够更加高效、准确地应对具有挑战性的任务。\nFig. 24. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)\nDeepSeek-R1-Zero 训练过程中也涌现出一个典型\u0026quot;顿悟时刻\u0026quot;(aha moment)。在这一关键阶段，模型在推理过程中突然意识到先前的思考路径存在错误，随即迅速调整思考方向，最终成功导向正确答案。这一现象有力证明了模型在推理过程中已发展出强大的自我纠错和反思能力，类似于人类思考过程中的顿悟体验。\nFig. 25. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: DeepSeek-AI, 2025)\n记忆 人类记忆 记忆 指的是获取、储存、保持和提取信息的过程。人类的记忆主要分为以下三大类：\nFig. 26. Categorization of human memory. (Image source: Weng, 2017)\n感觉记忆: 用于在原始刺激（视觉、听觉、触觉等）消失后短暂保留感官信息，通常持续时间以毫秒或秒计。感觉记忆又分为：\n视觉记忆：对视觉通道所保留的瞬时图像或视觉印象，一般维持 0.25～0.5 秒，用于在视频或动画场景中形成视觉连续性。 听觉记忆：对听觉信息的短暂存储，可持续数秒，使人能回放刚刚听到的语句或声音片段。 触觉记忆：用于保留短暂的触觉或力觉信息，一般持续毫秒到秒级，例如敲击键盘或盲文阅读时短暂的手指感知。 短期记忆: 存储我们当前意识到的信息。\n持续时间约 20～30 秒，容量通常为 7±2 个项目。 承担学习、推理等复杂认知任务时对信息的临时处理和维持。 长期记忆: 能将信息保存数天到数十年，容量几乎无限。长期记忆分为：\n外显记忆: 可有意识回忆，包含情景记忆（个人经历、事件细节）和语义记忆（事实与概念）。 内隐记忆:无意识记忆，主要与技能和习惯相关，如骑车或盲打。 人类记忆的这三种类型相互交织，共同构成了我们对世界的认知和理解。在构建 LLM Agent 中，我们也可以借鉴人类记忆的这种分类方式：\n感觉记忆 对应 LLM 对输入原始数据（如文本、图片和视频）等的嵌入表征。 短期记忆 对应 LLM 的上下文内学习，受限于模型上下文窗口 max_tokens，当对话长度超出窗口后，早期信息将被截断。 长期记忆 对应 外部向量存储或数据库，Agent 可以基于 RAG 技术在需要时检索历史信息。 LLM Agent 记忆 Agent 与用户多轮互动、执行多步任务时，可以利用不同形式的记忆以及环境信息来完成工作流。\nFig. 27. An overview of the sources, forms, and operations of the memory in LLM-based agents. (Image source: Zhang et al. 2024)\n文本记忆\n完整交互：记录了所有对话与操作轨迹，帮助 Agent 回溯上下文。 近期交互：只保留与当前任务高度相关的对话内容，减少不必要的上下文占用。 检索到的交互：Agent 可从外部知识库中检索到与当前任务相关的历史对话或记录，融入当前上下文中。 外部知识：当 Agent 遇到知识空白时，可通过 API 或外部存储进行检索和获取额外信息。 参数化记忆\n微调：通过给 LLM 注入新的信息或知识，从而扩充模型的内部知识。 知识编辑：在模型层面对已有知识进行修改或更新，实现对模型内部参数记忆的动态调整。 环境\n代表着 Agent 与用户及外部系统交互时涉及的实体和上下文，比如用户 Alice、可能访问的工具或界面（例如订票系统、流媒体平台等）。 Agent\nLLM Agent 负责读与写操作，即读取外部环境或知识库的信息，写入新的动作或内容。 同时包含一系列管理功能，如合并、反思、遗忘等，用以动态维护短期和长期记忆。 另外一个例子是 Agent 在完成在两个不同但相关的任务中，需要同时使用短期记忆和长期记忆：\n任务 A 播放视频：Agent 将当前的计划、操作和环境状态（例如搜索、点击、播放视频等）记录在短期记忆中，这部分信息会保存在内存和 LLM 的上下文窗口中。 任务 B 下载游戏：Agent 利用长期记忆中与 Arcane 和 League of Legend 相关的知识，快速找到游戏下载方案。图中显示，Agent 在 Google 上进行搜索，我们可以将 Google 的知识库视作一个外部知识源，同时所有新的搜索、点击和下载操作也会被更新到短期记忆中。 Fig. 28: Illustration of short-term memory and long-term memory in an LLM-brained GUI agent. (Image source: Zhang et al. 2024)\n常见记忆元素及其对应的存储方式可以总结成以下表格:\n记忆元素 记忆类型 描述 存储介质 / 方式 动作 短期记忆 历史动作轨迹（例如点击按钮、输入文本等） 内存、LLM 上下文窗口 计划 短期记忆 上一步或当前生成的下一步操作计划 内存、LLM 上下文窗口 执行结果 短期记忆 动作执行后返回的结果、报错信息及环境反馈 内存、LLM 上下文窗口 环境状态 短期记忆 当前 UI 环境中可用的按钮、页面标题、系统状态等 内存、LLM 上下文窗口 自身经验 长期记忆 历史任务轨迹与执行步骤 数据库、磁盘 自我引导 长期记忆 从历史成功轨迹中总结出的指导规则与最佳实践 数据库、磁盘 外部知识 长期记忆 辅助任务完成的外部知识库、文档或其他数据源 外部数据库、向量检索 任务成功指标 长期记忆 记录任务成功率、失败率等指标，便于改进和分析 数据库、磁盘 此外，研究者还提出一些新的训练和存储方法，以增强 LLM 的记忆能力：\nLongMem(Language Models Augmented with Long-Term Memory)(Wang, et al. 2023)使 LLM 能够记忆长历史信息。其采用一种解耦的网络结构，将原始 LLM 参数冻结为记忆编码器(memory encoder)固定下来，同时使用自适应残差网络(Adaptive Residual Side-Network，SideNet)作为记忆检索器进行记忆检查和读取。\nFig. 29. Overview of the memory caching and retrieval flow of LongMem. (Image source: Wang, et al. 2023)\n其主要由三部分构成：Frozen LLM、Residual SideNet 和 Cached Memory Bank。其工作流程如下：\n先将长文本序列拆分成固定长度的片段，每个片段在 Frozen LLM 中逐层编码后，在第 $m$ 层提取注意力的 $K, V \\in \\mathbb{R}^{H \\times M \\times d}$ 向量对并缓存到 Cached Memory Bank。 面对新的输入序列时，模型根据当前输入的 query-key 检索长期记忆库，从中获取与输入最相关的前 $k$ 个 key-value（即 top-$k$ 检索结果），并将其融合到后续的语言生成过程中；与此同时记忆库会移除最旧的内容以保证最新上下文信息的可用性。 Residual SideNet 则在推理阶段对冻结 LLM 的隐藏层输出与检索得到的历史 key-value 进行融合，完成对超长文本的有效建模和上下文利用。 通过这种解耦设计，LongMem 无需扩大自身的原生上下文窗口就能灵活调度海量历史信息，兼顾了速度与长期记忆能力。\n工具使用 工具使用 是 LLM Agent 重要组成部分, 通过赋予 LLM 调用外部工具的能力，其功能得到了显著扩展：不仅能够生成自然语言，还能获取实时信息、执行复杂计算以及与各类系统（如数据库、API 等）交互，从而有效突破预训练知识的局限，避免重复造轮子的低效过程。\n传统 LLM 主要依赖预训练数据进行文本生成，但这也使得它们在数学运算、数据检索和实时信息更新等方面存在不足。通过工具调用，模型可以：\n提升运算能力： 例如通过调用专门的计算器工具 Wolfram，模型能够进行更精准的数学计算，弥补自身算术能力的不足。\n实时获取信息： 利用搜索引擎 Gooole、Bing 或数据库 API，模型可以访问最新信息，确保生成内容的时效性和准确性。\n增强信息可信度： 借助外部工具的支持，模型能够引用真实数据来源，降低信息虚构的风险，提高整体可信性。\n提高系统透明度： 跟踪 API 调用记录可以帮助用户理解模型决策过程，提供一定程度的可解释性。\n当前，业界涌现出多种基于工具调用的 LLM 应用，它们利用不同策略和架构，实现了从简单任务到复杂多步推理的全面覆盖。\nToolformer Toolformer (Schick, et al. 2023)是一种能够通过简单 API 使用外部工具的 LLM。它的训练方式基于 GPT-J 模型进行微调，仅需为每个 API 提供少量示例。Toolformer 学到调用的工具包括问答系统、 Wikipedia 搜索、计算器、日历和翻译系统：\nFig. 30. Examples of inputs and outputs for all APIs used. (Image source: Schick, et al. 2023)\nHuggingGPT HuggingGPT (Shen, et al. 2023)是一种利用 ChatGPT 作为任务规划器的框架，通过读取模型描述从 HuggingFace 筛选可用的模型来完成用户任务，并根据执行结果进行总结。\nFig. 31. Illustration of how HuggingGPT works. (Image source: Shen, et al. 2023)\n该系统由以下四个阶段组成：\n任务规划: 将用户请求解析为多个子任务。每个任务包含四个属性：任务类型、ID、依赖关系以及参数。论文使用少样本提示来指导模型进行任务拆分和规划。 模型选择: 将各子任务分配给不同的专家模型，采用多选题的方式来确定最合适的模型。由于上下文长度有限，需要根据任务类型对模型进行初步过滤。 任务执行: 专家模型执行分配的具体任务并记录结果, 结果会被传递给 LLM 进行后续处理。 结果生成: 接收各专家模型的执行结果，最后给用户输出总结性答案。 LLM 智能体应用 Generative Agent Generative Agent (Park, et al. 2023) 实验通过 25 个由大型语言模型驱动的虚拟角色，在沙盒环境中模拟真实的人类行为。其核心设计融合了记忆、检索、反思以及规划与反应机制，允许 Agent 记录并回顾自身经验，并从中提炼出关键信息，以指导后续行动和互动。\nFig. 32. The screenshot of generative agent sandbox. (Image source: Park, et al. 2023)\n整个系统利用长期记忆模块记录所有观察事件，结合检索模型根据时效性、重要性与相关性提取信息，再通过反思机制生成高层次推断，最终将这些成果转化为具体行动。该仿真实验展示了信息扩散、关系记忆和社会事件协调等涌现行为，为交互式应用提供了逼真的人类行为模拟。\nFig. 25. The generative agent architecture. (Park, et al. 2023)\nWebVoyager WebVoyager(He et al. 2024) 是一种基于多模态大模型的自主网页交互智能体，能够控制鼠标和键盘进行网页浏览。WebVoyager 采用经典的 ReAct 循环。在每个交互步骤中，它查看带有类似 SoM(Set-of-Marks)(Yang, et al. 2023) 方法标注的浏览器截图即通过在网页元素上放置数字标签提供交互提示，然后决定下一步行动。这种视觉标注与 ReAct 循环相结合，使得用户可以通过自然语言与网页进行交互。具体可以参考使用 LangGraph 框架的WebVoyager 代码。\nFig. 33. The overall workflow of WebVoyager. (Image source: He et al. 2024)\nOpenAI Operator Operator (OpenAI, 2025) 是一个 OpenAI 近期发布的 AI 智能体，旨在自主执行网络任务。Operator 能够像人类用户一样与网页互动，通过打字、点击和滚动等操作完成指定任务。Operator 的核心技术是计算机使用智能体(Computer-Using Agent, CUA)(OpenAI, 2025)。CUA 结合了 GPT-4o 的视觉能力和通过强化学习获得更强的推理能力，经过专门训练后能够与图形用户界面（GUI）进行交互，包括用户在屏幕上看到的按钮、菜单和文本框。\nFig. 34. Overview of OpenAI CUA. (Image source: OpenAI, 2025)\nCUA 的运作方式遵循一个迭代循环，包含三个阶段：\n感知: CUA 通过捕获浏览器截图来“观察”网页内容。这种基于视觉的输入方式使其能够理解页面的布局和元素。\n推理: 借助链式思考的推理过程，CUA 会评估下一步行动，其依据是当前和之前的截图以及已执行的操作。这种推理能力使其能够跟踪任务进度、回顾中间步骤，并根据需要进行调整。\n行动: CUA 通过模拟鼠标和键盘操作（如点击、输入和滚动）与浏览器进行交互。这使其能够在无需特定 API 集成的情况下执行各种网络任务。\nCUA 和之前现有的 WebVoyager 不同之处在于这是一个专门经过强化学习训练的 Agent，而不是直接调用 GPT-4o 搭建的固定流程的 Workflow。虽然 CUA 目前仍处于早期阶段且存在一定局限，但它以下基准测试中取得了 SOTA 结果。\nFig. 35. OpenAI CUA Benchmark Results. (Image source: OpenAI, 2025)\nDeep Research Deep Research 本质上是一个报告生成系统：给定用户的查询，系统以 LLM 作为核心 Agent，经过多轮迭代式的信息检索与分析，最终生成一份结构化、翔实的报告。目前，各类 Deep Research 系统的实现逻辑主要可分为Workflow Agent和RL Agent两种方式。\nWorkflow Agent vs RL Agent Workflow Agent 这种方式依赖开发人员预先使用设计的工作流和手工构建的 Prompt 来组织整个报告生成过程。主要特点包括：\n任务分解与流程编排：系统将用户查询拆分为若干子任务，比如生成大纲、信息检索、内容汇总等，再按照预定的流程顺序执行。 固定流程：各个阶段之间的调用和交互是事先设定好的，类似于构建一个静态的流程图或有向无环图（DAG），确保每一步都有明确的职责。 人工设计依赖：该方法主要依赖工程师的经验，通过反复调试 Prompt 来提升输出质量，适用性较强但灵活性有限。 可以利用 LangGraph框架以图的形式来构建和编排工作流。\nFig. 36. A workflow of the LangGraph. (Image source: LangGraph, 2025)\n以下表格对比了 5 种常见的工作流和 agent 的模式:\n模式 核心机制 优势 局限 应用场景 Prompt Chaining 顺序调用 LLM，逐步传递结果 适合分阶段推理，结果更准确 流程固定，延迟高 文档生成（提纲→内容）、翻译润色 Parallelization 拆分子任务并行处理，或多模型投票 提升速度，结果更稳健 子任务需独立，资源消耗大 内容审核并行处理、多模型代码检测 Routing 先分类，再分配到不同模型/流程 针对性强，提高效率 分类准确性决定效果 客服问答分流、模型大小动态选择 Evaluator-Optimizer 生成 → 评估 → 优化迭代 提高结果质量，适合有标准的任务 成本高，多轮迭代增加延迟 翻译优化、多轮检索 refine Orchestrator-Worker 中央编排，动态拆解并调度子任务 灵活，可处理复杂任务 架构复杂，调度成本高 多文件代码修改、实时研究整合 Agent LLM 自主决策，基于环境反馈调用工具 高度灵活，适应动态环境 难以预测，成本和安全需控制 自主研究代理、交互式问题求解 目前 Github 上已经有多个开源项目实现了基于工作流的 Deep Research Agent，如 GPT Researcher 和 open deep research 等。\nFig. 37. An overview of the open deep research. (Image source: LangChain, 2025)\nRL Agent 则是另外一种实现方法，通过 RL 训练推理模型来优化 Agent 的多轮搜索、分析与报告撰写流程进行优化。主要特点包括：\n自主决策能力：系统通过强化学习训练，让 Agent 在面对复杂的搜索与内容整合任务时能够自主判断、决策和调整策略，从而更高效地生成报告。 持续优化：利用奖励机制对生成过程进行打分和反馈，Agent 能够不断迭代优化自身策略，实现从任务拆解到最终报告生成的整体质量提升。 降低人工干预：相较于依赖手工 Prompt 的固定流程，强化学习训练方式减少了对人工设计的依赖，更适合应对多变和复杂的实际应用场景。 下面的表格总结了这两种方式的主要区别：\n特性 Workflow Agent RL Agent 流程设计 预先设计固定工作流，任务分解和流程编排明确 端到端学习，Agent 自主决策和动态调整流程 自主决策能力 依赖人工设计的 Prompt，决策过程固定且不可变 通过强化学习，Agent 能够自主判断、决策并优化策略 人工干预 需要大量手工设计与调试 Prompt，人工干预较多 降低人工干预，通过奖励机制实现自动反馈和持续优化 灵活性与适应性 对复杂或变化场景的适应性较弱，扩展性有限 更适应多变和复杂的实际场景，具备较高的灵活性 优化机制 优化主要依赖工程师的经验调整，缺乏端到端反馈机制 利用强化学习的奖励反馈，实现持续自动化的性能提升 实现难度 实现相对直观，但需要繁琐的流程设计和维护 需要训练数据和计算资源，初期开发投入较大，但长期效果更佳 是否需要训练 无需额外训练，仅依赖手工构建的流程和 Prompt 需要通过强化学习对 Agent 进行训练，以实现自主决策 OpenAI Deep Research OpenAI Deep Research（OpenAI, 2025）是 OpenAI 于 2025 年 2 月正式发布的一款智能 Agent，专为复杂场景设计，能自动搜索、筛选、分析并整合多源信息，最终生成高质量的综合报告。该系统以 o3 为核心基座，并结合强化学习方法，显著提升了多轮迭代搜索和推理过程中的准确性与稳健性。\n相比传统的 ChatGPT 插件式搜索或常规 RAG 技术，OpenAI Deep Research 具有以下突出优势：\n强化学习驱动的迭代推理\n借助 o3 推理模型 与强化学习训练策略，Agent 能在多轮搜索与总结过程中持续优化自身推理路径，有效降低错误累积导致的失真风险。\n多源信息的整合与交叉验证\n突破单一搜索引擎的局限，能够同时调用特定数据库、专业知识库等多种权威数据源，通过交叉验证形成更可靠的研究结论。\n高质量的报告生成\n训练阶段引入 LLM-as-a-judge 评分机制和严格的评价标准，使系统在输出报告时能进行自我评价，从而生成结构更清晰、论证更严密的专业文本。\n训练过程 OpenAI Deep Research 训练过程采用了专为研究场景定制的浏览器交互数据集。通过这些数据集，模型掌握了核心浏览功能——包括搜索、点击、滚动和文件解析等操作；同时习得了在沙盒环境中使用 Python 工具进行计算、数据分析和可视化的能力。此外，借助在这些浏览任务上的强化学习训练，模型能够在海量网站中高效执行信息检索、整合与推理，快速定位关键信息或生成全面的研究报告。\n这些训练数据集既包含带有标准答案、可自动评分的客观任务，也包括配备详细评分量表的开放式任务。在训练过程中，模型的响应会与标准答案或评分标准进行严格对比，并利用模型产生 CoT 思考过程让评估模型提供反馈。\n同时，训练过程中复用了 o1 模型训练阶段积累的安全数据集，并专门增补了针对 Deep Research 场景的安全训练数据，确保模型在自动化搜索与浏览过程中严格遵守相关合规与安全要求。\n性能表现 在评估 AI 在各专业领域回答专家级问题能力的基准测试 Humanity\u0026rsquo;s Last Exam（Phan, et al. 2025）中，该模型取得了 SOTA 成绩。\nFig. 38. Humanity\u0026rsquo;s Last Exam Benchmark Results. (Image source: OpenAI, 2025)\n未来发展方向 智能体展现出广阔前景，但要实现可靠广泛应用，仍需解决以下关键挑战：\n上下文窗口限制：LLM 的上下文窗口限制了信息处理量，影响长期规划和记忆能力，降低任务连贯性。当前研究探索外部记忆机制和上下文压缩技术，以增强长期记忆及复杂信息处理能力。目前 OpenAI 最新的模型 GPT-4.5(OpenAI, 2025)最大上下文窗口为 128k tokens。\n接口标准化与互操作性：当前基于自然语言的工具交互存在格式不统一问题。模型上下文协议(Model Context Protocol, MCP)(Anthropic, 2024) 通过开放标准统一 LLM 与应用程序的交互方式，降低开发复杂性，提高系统稳定性和跨平台兼容性。\n任务规划与分解能力：智能体在复杂任务中难以制定连贯计划、有效分解子任务，并缺乏意外情况下的动态调整能力。需要更强大的规划算法、自我反思机制和动态策略调整方法，以灵活应对不确定环境。\n计算资源与经济效益：部署大模型智能体因多次 API 调用和密集计算而成本高昂，限制了一些实际应用场景。优化方向包括模型结构高效化、量化技术、推理优化、缓存策略及智能调度机制。随着专用 GPU 硬件比如 NVIDIA DGX B200 及分布式技术发展，计算效率有望显著提升。\n安全防护与隐私保障：智能体面临提示注入等安全风险，需建立健全的身份验证、权限控制、输入验证及沙箱环境。针对多模态输入与外部工具，需强化数据匿名化、最小权限原则和审计日志，以满足安全与隐私合规要求。\n决策透明与可解释性：智能体决策难以解释，限制了其在高风险领域的应用。增强可解释性需开发可视化工具、思维链追踪和决策理由生成机制，以提高决策透明度，增强用户信任，满足监管要求。\n参考文献 [1] DAIR.AI. \u0026ldquo;LLM Agents.\u0026rdquo; Prompt Engineering Guide, 2024.\n[2] Sutton, Richard S., and Andrew G. Barto. \u0026ldquo;Reinforcement Learning: An Introduction.\u0026rdquo; MIT Press, 2018.\n[3] Weng, Lilian. \u0026ldquo;LLM-powered Autonomous Agents.\u0026rdquo; Lil’Log, 2023.\n[4] Wei, Jason, et al. \u0026ldquo;Chain-of-thought prompting elicits reasoning in large language models.\u0026rdquo; Advances in neural information processing systems 35 (2022): 24824-24837.\n[5] Kojima, Takeshi, et al. \u0026ldquo;Large language models are zero-shot reasoners.\u0026rdquo; Advances in neural information processing systems 35 (2022): 22199-22213.\n[6] Zhang, Zhuosheng, et al. \u0026ldquo;Multimodal chain-of-thought reasoning in language models.\u0026rdquo; arXiv preprint arXiv:2302.00923 (2023).\n[7] Wang, Xuezhi, et al. \u0026ldquo;Self-consistency improves chain of thought reasoning in language models.\u0026rdquo; arXiv preprint arXiv:2203.11171 (2022).\n[8] Wang, Xuezhi, et al. \u0026ldquo;Rationale-augmented ensembles in language models.\u0026rdquo; arXiv preprint arXiv:2207.00747 (2022).\n[9] Zelikman, Eric, et al. \u0026ldquo;Star: Bootstrapping reasoning with reasoning.\u0026rdquo; Advances in Neural Information Processing Systems 35 (2022): 15476-15488.\n[10] Fu, Yao, et al. \u0026ldquo;Complexity-based prompting for multi-step reasoning.\u0026rdquo; arXiv preprint arXiv:2210.00720 (2022).\n[11] Yao, Shunyu, et al. \u0026ldquo;Tree of thoughts: Deliberate problem solving with large language models.\u0026rdquo; Advances in neural information processing systems 36 (2023): 11809-11822.\n[12] Zhou, Yongchao, et al. \u0026ldquo;Large language models are human-level prompt engineers.\u0026rdquo; The eleventh international conference on learning representations. 2022.\n[13] Zhang, Zhuosheng, et al. \u0026ldquo;Automatic chain of thought prompting in large language models.\u0026rdquo; arXiv preprint arXiv:2210.03493 (2022).\n[14] Liu, Jiacheng, et al. \u0026ldquo;Generated knowledge prompting for commonsense reasoning.\u0026rdquo; arXiv preprint arXiv:2110.08387 (2021).\n[15] Lewis, Patrick, et al. \u0026ldquo;Retrieval-augmented generation for knowledge-intensive nlp tasks.\u0026rdquo; Advances in neural information processing systems 33 (2020): 9459-9474.\n[16] Diao, Shizhe, et al. \u0026ldquo;Active prompting with chain-of-thought for large language models.\u0026rdquo; arXiv preprint arXiv:2302.12246 (2023).\n[17] Yao, Shunyu, et al. \u0026ldquo;React: Synergizing reasoning and acting in language models.\u0026rdquo; International Conference on Learning Representations (ICLR). 2023.\n[18] Shinn, Noah, et al. \u0026ldquo;Reflexion: Language agents with verbal reinforcement learning.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 8634-8652.\n[19] Guo, Daya, et al. \u0026ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\u0026rdquo; arXiv preprint arXiv:2501.12948 (2025).\n[20] OpenAI. \u0026ldquo;Introducing OpenAI o1\u0026rdquo; OpenAI, 2024.\n[21] Zhang, Zeyu, et al. \u0026ldquo;A survey on the memory mechanism of large language model based agents.\u0026rdquo; arXiv preprint arXiv:2404.13501 (2024).\n[22] Zhang, Chaoyun, et al. \u0026ldquo;Large language model-brained gui agents: A survey.\u0026rdquo; arXiv preprint arXiv:2411.18279 (2024).\n[23] Wang, Weizhi, et al. \u0026ldquo;Augmenting language models with long-term memory.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 74530-74543.\n[24] Schick, Timo, et al. \u0026ldquo;Toolformer: Language models can teach themselves to use tools.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 68539-68551.\n[25] Shen, Yongliang, et al. \u0026ldquo;Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 38154-38180.\n[26] Park, Joon Sung, et al. \u0026ldquo;Generative agents: Interactive simulacra of human behavior.\u0026rdquo; Proceedings of the 36th annual acm symposium on user interface software and technology. 2023.\n[27] He, Hongliang, et al. \u0026ldquo;WebVoyager: Building an end-to-end web agent with large multimodal models.\u0026rdquo; arXiv preprint arXiv:2401.13919 (2024).\n[28] Yang, Jianwei, et al. \u0026ldquo;Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v.\u0026rdquo; arXiv preprint arXiv:2310.11441 (2023).\n[29] OpenAI. \u0026ldquo;Introducing Operator.\u0026rdquo; OpenAI, 2025.\n[30] OpenAI. \u0026ldquo;Computer-Using Agent.\u0026rdquo; OpenAI, 2025.\n[31] OpenAI. \u0026ldquo;Introducing Deep Research.\u0026rdquo; OpenAI, 2025.\n[32] Phan, Long, et al. \u0026ldquo;Humanity\u0026rsquo;s Last Exam.\u0026rdquo; arXiv preprint arXiv:2501.14249 (2025).\n[33] OpenAI. \u0026ldquo;Introducing GPT-4.5.\u0026rdquo; OpenAI, 2025.\n[34] Anthropic. \u0026ldquo;Introducing the Model Context Protocol.\u0026rdquo; Anthropic, 2024.\n[35] LangGraph. \u0026ldquo;A workflow of the LangGraph.\u0026rdquo; LangGraph Tutorials, 2025.\n[36] Assaf Elovic. \u0026ldquo;GPT Researcher\u0026rdquo; GitHub Repository, 2025.\n[37] LangChain. \u0026ldquo;Open Deep Research\u0026rdquo; GitHub Repository, 2025.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui.(Mar 2025). 大语言模型智能体. https://syhya.github.io/zh/posts/2025-03-27-llm-agent\nOr\n@article{syhya2025llm-agent, title = \u0026#34;大语言模型智能体\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Mar\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-03-27-llm-agent\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-03-27-llm-agent/","summary":"\u003ch2 id=\"智能体\"\u003e智能体\u003c/h2\u003e\n\u003cp\u003e自 2022 年 10 月 OpenAI 发布 ChatGPT 以来，随着后续 \u003ca href=\"https://github.com/Significant-Gravitas/AutoGPT\"\u003eAutoGPT\u003c/a\u003e 和 \u003ca href=\"https://github.com/reworkd/AgentGPT\"\u003eAgentGPT\u003c/a\u003e 等项目的涌现，LLM 相关的智能体（Agent）逐渐成为近年来 AI 的研究热点和实际应用方向。本文将介绍智能体的基本概念、核心技术及其最新应用进展。\u003c/p\u003e","title":"大语言模型智能体"},{"content":"背景 最近大模型的参数数量不断攀升，从最初的数十亿扩展到如今数千亿乃至数万亿级别。大模模型虽然带来了前所未有的应用效果，但与此同时，也引发了计算资源、内存管理和训练稳定性等一系列严峻挑战。因此本博客总结了一些常用分布式并行训练和内存管理技术，希望能够帮助大家更好地训练和优化大模型。\n大模型的训练挑战 参数规模爆炸式增长\n随着对模型容量和性能的不断追求，神经网络的参数数量呈现出指数级增长。现今从百万级到数十亿、数千亿甚至数万亿参数的模型层出不穷。例如，Llama 3.1 405B 拥有约 4,050 亿参数，而据传 GPT-4 的参数量可能达到 1.7 万亿级别。这种庞大的参数规模使得计算和内存需求急剧上升，给训练过程带来了前所未有的压力。\n计算复杂度剧增\n参数数量的急速增加直接导致整体计算复杂度大幅上升。训练一次大型模型可能需要耗费数周甚至数月的时间，即便采用大规模高性能 GPU 集群，训练周期仍难以令人满意，从而严重制约了模型迭代速度和研究效率。\n内存瓶颈日益凸显\n除了需要存储庞大的模型参数之外，大模型在训练过程中还必须保存中间激活值、梯度信息以及优化器状态，这些数据对 GPU 显存构成了巨大挑战。即使是配备 A100、H100(80GB 显存)、H200(141GB 显存)或 GB200(384GB 显存)等高端 GPU，单卡内存往往也难以满足数千亿甚至数万亿级模型的需求，“Out of Memory(OOM)”错误频发。\n通信开销成为瓶颈\n在多 GPU 分布式训练环境中，节点间需要频繁进行数据同步(如梯度汇总)。随着模型规模和 GPU 数量的增加，这种通信量急剧上升，即使在高带宽网络中，All-Reduce 操作传输海量数据也会消耗大量时间，成为整体并行效率的主要瓶颈之一。\n训练稳定性挑战\n超大规模模型在训练过程中更容易遭遇梯度消失或梯度爆炸问题，导致训练过程不稳定、难以收敛。虽然混合精度训练可以在一定程度上加速训练并降低显存占用，但同时也可能引入新的数值稳定性问题，要求研究人员投入更多精力进行细致调优。\n分布式训练的必要性 面对上述挑战，分布式训练技术成为支撑大模型训练的关键方案。通过将训练任务拆分并分配到多台 GPU 或计算节点上，分布式训练能够充分利用并行计算和集群内存资源，从而突破单 GPU 的局限，主要优势体现在以下几个方面：\n突破单 GPU 算力限制\n单个 GPU 的计算能力终究有限，无法应对万亿级别参数模型的庞大计算需求。借助数据并行与模型并行技术，训练任务可以均匀分布至多个 GPU，从而大幅缩短整体训练时间。\n克服单 GPU 内存瓶颈\n通过将模型参数、中间激活值和优化器状态分散存储在多个 GPU 显存中，分布式训练有效扩展了可用内存容量。典型技术如 ZeRO，通过对模型参数、梯度以及优化器状态进行分片，大幅降低单卡的显存负担，使得超大规模模型的训练成为可能。\n加速模型迭代与研发周期\n分布式训练的高并行度使得原本需要数周甚至数月完成的训练任务有望在数天内完成，从而大幅提升模型迭代速度，使得新架构和新策略能够更快得到验证与应用。\n支持更大规模模型探索\n分布式训练为探索更大规模、更复杂的神经网络架构提供了坚实基础。正是在这种技术支持下，万亿参数级别的模型(如 Switch Transformer)才得以成功训练并投入实际应用。\n提高训练系统的鲁棒性与可扩展性\n分布式系统具备出色的容错能力，当某个 GPU 节点出现故障时，其他节点可迅速接管任务，确保训练过程不被中断。同时，集群规模可以根据具体需求灵活扩展或缩减，满足不同规模模型的训练要求。\n并行训练 下图直观展示了多种并行训练策略的不同之处。不同颜色代表不同的模型层(例如三层)，虚线将不同的 GPU 区分开。从左到右分别是数据并行、模型并行(含流水线并行和张量并行)以及专家并行(MoE)。\nFig. 1. An illustration of various parallelism strategies on a three-layer model. Each color refers to one layer and dashed lines separate different GPUs. (Image source: OpenAI Blog, 2022)\n数据并行\n完整模型会被拷贝到每个 GPU 上，数据集则被切分为不同批次分配给各个 GPU 并行计算，最终在参数更新时聚合所有 GPU 的梯度。\n模型并行\n将模型划分到不同的 GPU 上，每个 GPU 只负责模型的一部分计算；可进一步分为以下两类：\n流水线并行：按层(垂直方向)拆分模型，不同 GPU 负责不同的层，通过微批次(micro-batch)在流水线中传递来并行执行前向和反向计算。 张量并行：在层内(水平方向)对大规模张量操作(如大矩阵乘法)进行切分，各 GPU 并行完成这部分运算并在必要时进行聚合。 专家并行\n通过门控策略，让每个输入样本只经过部分专家(子网络)，从而将整个模型按“专家模块”分布到不同 GPU。常见于 Mixture-of-Experts(MOE) 结构，可实现超大参数规模但推理/训练时仅激活部分专家。\n下面我将对多种并行方式进行详细说明。\n数据并行 Fig. 2. Data Parallelism. (Image source: Clolossal-AI Documentation)\n在深度学习训练中，数据并行(Data Parallelism, DP) 是最常用的并行策略，其核心思路是：\n复制模型参数：在每个计算设备(通常是 GPU)上都放置一份完整的模型参数。 划分训练数据：将大规模的数据集按样本维度拆分为多个子集，不同子集分配给不同的 GPU 进行处理。 局部前向与反向传播：每个 GPU 独立计算损失及对应的局部梯度。 梯度/参数同步：将各 GPU 的梯度聚合后更新模型参数，保证在每一次迭代后所有 GPU 的模型副本保持一致。 下面展示了 数据并行 工作流程：\n数据集划分 将训练数据集 $D$ 划分为 $N$ 个互不重叠的子集 ${D_1, D_2, \\dots, D_N}$，其中 $N$ 是 GPU 数量。通常会确保各子集大小相近，以实现负载均衡。\n模型复制 在每个 GPU 上复制一份完整的模型参数 $\\theta$。在训练开始时，这些参数在各 GPU 上都是相同的。\n数据分发 将子集 $D_i$ 分发给第 $i$ 张 GPU，让其在本地存储并供后续计算使用。\n局部前向传播 每个 GPU 基于其本地数据子集 $D_i$ 做前向传播，得到局部损失 $L_i(\\theta, D_i)$。\n局部反向传播 每个 GPU 基于局部损失 $L_i$ 进行反向传播，计算局部梯度\n$$ g_i = \\nabla_{\\theta} L_i(\\theta, D_i). $$ 梯度同步 各 GPU 之间执行梯度同步(常用 All-Reduce)，将所有局部梯度 ${g_1, g_2, \\ldots, g_N}$ 汇总得到全局平均梯度\n$$ \\bar{g} = \\frac{1}{N} \\sum_{i=1}^{N} g_i. $$ 参数更新 每个 GPU 使用全局平均梯度 $\\bar{g}$ 更新本地模型参数：\n$$ \\theta \\leftarrow \\theta - \\eta \\cdot \\bar{g}, $$其中 $\\eta$ 为学习率(learning rate)。\n迭代循环 重复步骤 4 - 7，直至模型达到收敛或达到预设的训练轮数(epochs)。\n批量同步并行与异步并行 在上面的第 6 步“梯度同步”中，如何以及何时进行“同步”是影响数据并行性能和收敛行为的重要因素之一。一般分为以下两大类：\n批量同步并行(Bulk Synchronous Parallel, BSP) 是数据并行中最常见、也是最易理解的同步模式。其特点可概括为「在每一次小批量(mini-batch)迭代结束后，全局同步一次梯度并更新参数」。具体流程：\n局部计算：各 GPU 基于其数据子集 $D_i$ 分别做前向与反向传播，得到局部梯度 $g_i$。 全局通信：所有 GPU 同步(如通过 All-Reduce)计算 $\\bar{g}$。 参数更新：每个节点均使用 $\\bar{g}$ 更新本地参数副本 $\\theta$。 等待与下一步迭代：所有节点完成上述操作后，再进入下一个迭代。 异步并行(Asynchronous Parallel, ASP) 旨在摆脱 BSP 的全局同步点，让各节点独立进行计算和参数更新。其典型实现是「参数服务器」(Parameter Server, PS) 架构下的 异步 push-pull 过程：\n各节点在本地计算得到梯度 $g_i$, 然后 push 到参数服务器； 参数服务器一旦收到梯度，立即更新全局模型参数； 其他节点在需要最新参数时，会 pull 下来继续下一步计算。 BSP vs. ASP 下表总结了在数据并行环境下，同步并行与异步并行的主要差异：\n对比维度 同步并行(BSP) 异步并行(ASP) 参数更新时机 每个小批量或一定迭代后，全局同步一次 各节点独立更新参数，无需与他人保持同一时间步 收敛稳定性 高。使用的梯度均为最新，收敛路径可控，易于分析 较低。存在过时梯度，收敛速率与稳定性可能受影响 通信需求 高度依赖 All-Reduce，同步时所有节点都需要等待和交换数据 每个节点向参数服务器异步推送/拉取，通信更为灵活，但参数服务器可能成为瓶颈 硬件资源利用 若有慢节点或网络延迟，则其他节点需等待，资源利用率可能降低 无需等待慢节点，可高效使用计算资源 实现复杂度 相对较低，主流框架(PyTorch DDP、Horovod 等)有内置支持 相对更高，需要参数服务器等组件，需处理更多的同步逻辑与数据一致性 适用场景 同构硬件、网络带宽良好、追求较高收敛质量 异构硬件、网络不稳定或带宽较低、需要极高的吞吐量且能容忍一定收敛风险 典型实现 PyTorch DDP、TensorFlow MirroredStrategy Parameter Server 架构(MXNet、TensorFlow ParameterServer 模式等) 建议：在实际项目中，先从简单的同步并行(BSP) 入手，利用 PyTorch DDP 或类似工具进行多 GPU 训练。若网络环境异构、节点繁多或任务对吞吐率要求极高，可再尝试异步并行(ASP) 或参数服务器方案，并配合梯度累积(Gradient Accumulation) 来平衡带宽与更新频率。\n梯度累积 当批量大小较大或通信成为主要瓶颈时，可以采用 梯度累积(Gradient Accumulation) 来减少同步频率。其核心思路是：\n连续计算多个小批量(mini-batch)的局部梯度，并将它们累加到本地的累积缓冲区中； 当累积的 mini-batch 数量达到 $K$ 时，再触发一次全局梯度同步与参数更新。 设第 $j$ 个 mini-batch 的梯度为 $g_j$，则在一个「累积周期」内得到\n$$ G = \\sum_{j=1}^{K} g_j. $$再用学习率 $\\eta$ 更新：\n$$ \\theta \\leftarrow \\theta - \\eta \\cdot G. $$由于梯度同步不再是每个 mini-batch 都进行，而是每累计 $K$ 个 mini-batch 执行一次，通信开销可显著降低。但参数更新频率降低也可能导致训练收敛速度放缓，需在吞吐量与收敛性能之间做权衡。\n分布式数据并行 分布式数据并行(Distributed Data Parallel, DDP) 是 PyTorch v1.5(Li et al. 2020)在 BSP 思想下的高度优化实现，为单机多 GPU 乃至多机多 GPU 的数据并行提供便利。其主要优化包括：\n梯度 Bucketing(梯度桶化)：将模型参数分为多个「桶」(bucket)；反向传播时一旦某个桶内所有梯度都已计算完，就立即启动一次针对该桶的 All-Reduce，而不是等到所有梯度都算完后再一次性同步。 通信与计算重叠：DDP 通过异步通信和非阻塞操作，尽可能地将梯度同步(通信)与前向传播、反向传播(计算)重叠，从而减少了通信开销。这种重叠策略提升了整体的并行效率。 梯度累积：DDP 也能方便地与梯度累积相结合，结合使用，通过增加每次同步的梯度更新间隔，从而减少同步频率。这在大规模分布式训练中有助于进一步降低通信开销，提高训练效率。 Fig. 3. Pseudo code for Pytorch DDP. (Image source: Li et al. 2020)\nRing All-Reduce 在多 GPU(尤其是单机多 GPU)环境下，若有高速互联(如 NVLink、PCIe 交换机等)，可使用 Ring All-Reduce 来显著降低通信开销。其思路是：\n将 $k$ 个节点组织成一个环，并把梯度向量等分成 $k$ 份。 在「加和阶段」，每个节点分别向下一个节点发送其本地的一部分梯度，并与收到的梯度相加；该过程循环若干次后，每个节点会持有完整的「聚合后」梯度。 在「广播阶段」，再将最终结果沿环路分发给所有节点。 理想情况下，Ring All-Reduce 的通信代价与节点数量近似无关(可以视为 $\\mathcal{O}(1)$)，非常适合多 GPU 环境下的梯度同步，是 Horovod、NCCL 等库中广泛使用的核心通信模式。\n参数服务器 当集群规模扩展至多机多 GPU 时，若简单地采用单点聚合(例如一台中心服务器)往往难以支撑海量数据的并行训练。参数服务器(Parameter Server, PS)(Li, et al. 2014)是为可扩展分布式训练而设计的一种典型架构：\n参数分片：将模型参数按键值对(key-value) 的形式进行拆分，不同 PS 节点只管理特定分片的参数； push-pull 语义：计算节点在本地得到梯度后，push 到相应的 PS；PS 更新完该分片参数后，计算节点可在需要时 pull 下最新版本进行下一步计算。 灵活容错与扩展：通过增加或移除 PS 节点，可在带宽或计算需求上灵活扩容；在 PS 上也能实现备份与容错策略。 这种 PS + Worker 模式可以同时结合数据并行和模型并行，将超大模型拆分到多个 PS 上存储，并对超大数据进行分布式训练。PS 本身也能根据负载情况做拆分与合并，形成更加复杂的层次化拓扑结构。\n模型并行 模型并行(Model Parallelism, MP) 是一种将模型本身分割到多个计算设备(GPU) 上进行训练的并行方式。当模型参数规模超过单个 GPU 的内存容量时，模型并行成为必要的选择。模型并行主要分为两种类型：流水线并行(Pipeline Parallelism) 和张量并行(Tensor Parallelism)。\n朴素模型并行与气泡问题\nFig. 4. A naive model parallelism setup where the model is vertically split into 4 partitions. Data is processed by one worker at a time due to sequential dependency, leading to large “bubbles” of idle time. (Image source: Huang et al. 2018)\n朴素的模型并行实现，即将模型简单地按层划分，并顺序地在不同 GPU 上执行，会遇到严重的 \u0026ldquo;气泡\u0026rdquo;(bubble) 问题。由于层之间的依赖关系，当一个 GPU 在处理某个数据样本的某个阶段时，其他 GPU 可能处于空闲状态，等待前一个 GPU 的输出或者后一个 GPU 的输入。这种 GPU 空闲时间被称为 \u0026ldquo;气泡\u0026rdquo;，严重降低了流水线并行的效率。\n其中，$F_i$ 表示 Stage $i$ 的前向传播，$B_i$ 表示 Stage $i$ 的反向传播。可以看到，在朴素流水线并行中，大部分时间只有一个 GPU 在工作，其他 GPU 处于空闲状态，效率低下。\n气泡问题产生的原因:\n层间依赖: 神经网络的层之间存在顺序依赖关系，后一层的计算必须依赖于前一层的输出。 顺序执行: 朴素模型并行按照层顺序依次执行，导致 GPU 之间无法充分并行工作。 流水线并行 Fig. 5. Pipeline Parallelism. (Image source: Clolossal-AI Documentation)\n流水线并行(Pipeline Parallelism, PP) 将模型按层划分为多个阶段(stage)，每个阶段分配到一个 GPU 上。数据像流水线一样在不同 GPU 之间传递，前一个 GPU 的输出作为后一个 GPU 的输入。流水线并行旨在提高模型并行训练的效率，减少 GPU 空闲时间。\nGPipe GPipe(Huang et al. 2018) 是 Google 提出的一个高效的流水线并行训练系统，旨在解决朴素流水线并行的气泡问题。GPipe 的核心思想是将 mini-batch 划分为多个 micro-batch，并采用同步梯度聚合的方式来缓解气泡问题，提高流水线效率。\nFig. 6. Illustration of pipeline parallelism in GPipe with 4 microbatches and 4 partitions. GPipe aggregates and updates gradients across devices synchronously at the end of every batch. (Image source: Huang et al. 2018)\n以下是 GPipe 调度策略：\nMicro-batch 划分: 将一个 mini-batch 划分为 $m$ 个 micro-batch。划分后的每个 micro-batch 的大小为原 mini-batch 的 $1/m$。 流水线阶段划分: 将模型按层划分为 $d$ 个阶段，每个阶段分配到一个 GPU 上。 流水线执行: 依次处理每个 micro-batch，在流水线中进行前向和反向传播。具体流程如下： 前向传播(Forward Propagation): 对于每个 micro-batch，依次在 Stage $1$, Stage $2$, \u0026hellip;, Stage $d$ 上进行前向传播。Stage $i$ 的输出作为 Stage $i+1$ 的输入。 反向传播(Backward Propagation): 当所有 micro-batch 的前向传播都完成后，开始反向传播。对于每个 micro-batch，依次在 Stage $d$, Stage $d-1$, \u0026hellip;, Stage $1$ 上进行反向传播。Stage $i$ 的梯度作为 Stage $i-1$ 的输入。 同步梯度聚合(Synchronous Gradient Aggregation): 在所有 micro-batch 的反向传播都完成后，将所有 micro-batch 的梯度进行聚合(例如求平均)，得到全局平均梯度。 参数更新(Parameter Update): 每个 GPU 使用全局平均梯度更新本地模型参数。 GPipe 气泡比例公式 假设每个 micro-batch 的前向和反向传播时间均为 1 单位，流水线深度为 $d$，micro-batch 数量为 $m$，则 GPipe 的气泡比例为：\n$$ \\text{Bubble Ratio} = 1 - \\frac{2md}{(2m + 2(d-1))d} = \\frac{d-1}{m+d-1} $$当 micro-batch 数量 $m$ 远大于流水线深度 $d$ 时($m \\gg d$)，气泡比例趋近于 0，流水线效率接近线性加速。GPipe 论文中指出，当 $m \u0026gt; 4d$ 时，气泡开销几乎可以忽略不计(在激活重计算的情况下)。因此有以下好处：\n减少气泡: GPipe 通过 micro-batch 划分和流水线调度，显著减少了朴素流水线并行的气泡问题，提高了 GPU 利用率和训练效率。 同步梯度聚合: GPipe 采用同步梯度聚合，保证了训练过程的同步性，模型收敛性较好。 线性加速潜力: 在 micro-batch 数量足够大的情况下，GPipe 可以实现接近线性的加速效果。 PipeDream Fig. 7. Illustration of 1F1B microbatch scheduling in PipeDream. (Image source: Harlap et al. 2018)\nPipeDream(Harlap et al. 2018)是另一种高效的流水线并行训练系统，它采用了 1F1B(1-Forward-1-Backward) 调度策略，并引入了权重暂存(Weight Stashing) 技术，进一步减少气泡，提高流水线效率，并解决 1F1B 调度可能导致的权重版本不一致问题。\nPipeDream 的 1F1B 调度策略的核心思想是，每个 GPU(Stage) 交替执行前向传播和反向传播，尽可能地并行工作，减少 GPU 空闲时间。具体流程如下：\nMicro-batch 划分: 将一个 mini-batch 划分为 $m$ 个 micro-batch。 流水线阶段划分: 将模型按层划分为 $d$ 个阶段，每个阶段分配到一个 GPU 上。 1F1B 调度执行: 每个 GPU 轮流执行前向传播和反向传播。 权重暂存 由于 1F1B 调度中，前向传播和反向传播可能使用不同版本的模型权重，会导致权重版本不一致问题，影响训练的正确性和收敛性。PipeDream 引入了 权重暂存(Weight Stashing) 技术来解决这个问题。权重暂存的核心思想是，每个 GPU 维护多个版本的模型权重，并确保前向传播和反向传播使用同一版本的权重。\n权重暂存实现方式:\n版本管理: 每个 GPU 维护一个权重版本队列，存储多个版本的模型权重。 版本选择: 在进行前向传播时，选择当前最新的权重版本。在进行反向传播时，选择与对应前向传播相同的权重版本。 版本更新: 在完成一个 mini-batch 的所有 micro-batch 的反向传播后，更新模型权重，并生成新的权重版本。 为了进一步优化 PipeDream 的内存使用，尤其是在权重暂存方面，PipeDream 衍生出了 PipeDream-flush 和 PipeDream-2BW 两种内存优化变体。\nPipeDream-flush Fig. 8. Illustration of pipeline scheduling in PipeDream-flush. (Image source: Narayanan et al. 2020)\nPipeDream-flush 在 PipeDream 的基础上，周期性地进行全局同步的流水线刷新(flush)，类似于 GPipe 的同步梯度聚合。通过定期刷新，PipeDream-flush 可以大幅减少权重暂存所需的内存空间，只需维护单个版本的模型权重，但会牺牲少量吞吐量。\nPipeDream-2BW *PipeDream-2BW(Double-Buffered Weights) 维护两个版本的模型权重，即 \u0026ldquo;双缓冲权重\u0026rdquo;。它每 $k$ 个 micro-batch 更新一次模型版本，其中 $k$ 大于流水线深度 $d$($k \u0026gt; d$). 新更新的模型版本不会立即完全替换旧版本，因为可能还有一些剩余的反向传播操作仍然依赖于旧版本。通过双缓冲权重，PipeDream-2BW 可以将权重暂存的内存开销降低到只维护两个版本的模型权重，显著减少内存占用。\nFig. 9. Illustration of pipeline scheduling in PipeDream-2BW. (Image source: Narayanan et al. 2020)\nPipeDream-2BW 策略有以下优点：\n更低的气泡开销: 1F1B 调度策略相比 GPipe 可以进一步减少气泡，提高 GPU 利用率和训练效率。 权重暂存解决版本一致性: 权重暂存技术保证了前向传播和反向传播使用同一版本的权重，解决了 1F1B 调度可能导致的权重版本不一致问题。 内存优化变体: PipeDream-flush 和 PipeDream-2BW 进一步优化了内存使用，降低了权重暂存的内存开销，使得流水线并行更适用于内存受限的场景。 张量并行 张量并行(Tensor Parallelism, TP) 是一种将模型中的张量(通常是权重矩阵) 沿着特定维度切分，并将切分后的分片分配到不同的 GPU 上进行计算的并行方式。张量并行有以下几点优势：\n突破单 GPU 显存限制: 张量并行可以将模型参数分散存储在多个 GPU 上，突破单 GPU 显存容量限制，支持训练更大规模的模型。 层内并行: 张量并行可以实现模型层内部的并行化，例如矩阵乘法操作的并行计算，提高计算效率。 与数据并行和流水线并行结合: 张量并行可以与数据并行和流水线并行等其他并行技术结合使用，形成多维并行策略，进一步提高训练效率和可扩展性。 Megatron-LM Megatron-LM(Shoeybi et al. 2019) 是 NVIDIA 提出的一个用于训练超大型语言模型的系统，它采用了张量并行技术，对 Transformer 模型层内部的矩阵乘法操作进行并行化，包括 self-attention 和 MLP 中的矩阵乘法。\nFig. 10. Illustration of tensor parallelism for key transformer components proposed in Megatron-LM. (Image source: Shoeybi et al. 2019)\nTransformer 的 MLP 层通常包含两个线性层，第一个线性层的计算可表示为 $Y = \\text{GeLU}(XA)$，其中 $X$ 是输入矩阵，$A$ 是权重矩阵，GeLU 是激活函数。Megatron-LM 将权重矩阵 $A$ 沿着列维度切分为 $P$ 个分片 $[A_1, A_2, \u0026hellip;, A_P]$，其中 $P$ 是 GPU 的数量。每个 GPU $i$ 负责存储和计算权重分片 $A_i$。\nMLP 层张量并行计算流程:\n$$ \\begin{aligned} \\text { Split } A \u0026 =\\left[A_1, A_2\\right] \\\\ Y \u0026 =\\operatorname{GeLU}(X A) \\\\ {\\left[Y_1, Y_2\\right] } \u0026 =\\left[\\operatorname{GeLU}\\left(X A_1\\right), \\operatorname{GeLU}\\left(X A_2\\right)\\right] \\end{aligned} $$ 权重分片: 将权重矩阵 $A$ 沿着列维度切分为 $P$ 个分片 $[A_1, A_2, \u0026hellip;, A_P]$，并将分片 $A_i$ 分配到 GPU $i$。 局部矩阵乘法: 每个 GPU $i$ 使用输入矩阵 $X$ 和权重分片 $A_i$ 进行矩阵乘法计算，得到局部输出 $Y_i = \\text{GeLU}(XA_i)$。 全局拼接(All-Gather): 所有 GPU 通过 All-Gather 操作，将局部输出 ${Y_1, Y_2, \u0026hellip;, Y_P}$ 拼接成完整的输出矩阵 $Y = [Y_1, Y_2, \u0026hellip;, Y_P]$。 自注意力层张量并行\nMegatron-LM 也对 Transformer 的自注意力层中的 Query($Q$), Key($K$), Value($V$) 权重矩阵进行张量并行切分，并进行相应的局部矩阵乘法和全局拼接操作，实现自注意力层的张量并行化。自注意力层的计算公式为：\n$$ \\text{Attention}(X, Q, K, V) = \\text{softmax}\\left(\\frac{(XQ)(XK)^T}{\\sqrt{d_k}}\\right)XV $$PTD-P PTD-P(Pipeline, Tensor, and Data Parallelism)(Narayanan et al. 2021)是一个结合了流水线并行、张量并行和数据并行的多维并行策略。PTD-P 旨在充分利用各种并行技术的优势，提高超大型模型训练的效率和可扩展性。\nPTD-P 的特点:\n多维并行结合: PTD-P 同时使用了流水线并行、张量并行和数据并行三种并行技术，可以从多个维度对训练过程进行并行化。 Interleaved 1F1B 调度: PTD-P 采用了 interleaved 1F1B 调度策略，与传统的流水线并行不同，它将模型划分为多个不连续的层块(model chunk)，并将多个层块分配给每个 GPU。这种调度策略可以进一步减少气泡，提高流水线效率。 灵活的并行配置: PTD-P 允许用户根据模型结构和硬件资源灵活配置各种并行技术的组合方式，例如可以只使用张量并行和数据并行，也可以同时使用流水线并行、张量并行和数据并行。 传统的流水线并行通常将模型划分为连续的层块，每个 GPU 负责一个连续的层块。PTD-P 的 interleaved 1F1B 调度则将模型划分为多个不连续的层块，例如，GPU 1 负责层 1, 2, 9, 10，GPU 2 负责层 3, 4, 11, 12，依此类推。每个 GPU 负责多个不连续的层块，可以更有效地利用 GPU 资源，减少气泡开销。\nFig. 11.(Top) Default 1F1B pipeline schedule as in PipeDream-flush.(Bottom) Interleaved 1F1B pipeline schedule. First model chunks are in dark colors and second chunks are in light colors. (Image source: Narayanan et al. 2021)\n混合专家模型 混合专家模型(Mixture-of-Experts, MoE)(Shazeer et al. 2017) 是一种稀疏激活模型，它通过结合多个独立的“专家”网络和一个门控网络(Gating Network)，在不显著增加计算成本的前提下，大幅提升了模型的参数量和性能。MoE 的核心思想是稀疏激活(Sparse Activation)，即对于每个输入样本，仅激活部分专家网络，而不是整个模型。这种方法既提高了计算效率，又增强了模型的表达能力，使其在 LLMs 中表现出色。\nMoE 设计灵感来源于集成学习(Ensemble learning), 一种将复杂任务分解为多个子任务并由不同模型协作完成的技术。在 MoE 中，这些“子任务”由多个独立的专家网络处理，而门控网络则负责根据输入样本的特征动态选择最适合的专家。这种分工合作的机制类似于人类社会中的专家团队：不同领域的专家针对特定问题提供专业意见，最终综合得出结果。\nFig. 12. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)\nMoE 核心组件 一个典型的 MoE 包含以下组件：\n专家网络(Experts): 一组独立的神经网络 ${E_1, E_2, \u0026hellip;, E_n}$，每个专家网络 $E_i$ 可以是任意类型的神经网络，例如 FFN, CNN, RNN 等。专家网络的数量 $n$ 可以很大，例如几十个、几百个甚至几千个。 门控网络(Gating Network): 一个可训练的神经网络 $G$，用于根据输入样本 $x$ 学习一个概率分布，决定激活哪些专家。门控网络的输入是输入样本 $x$，输出是一个 $n$ 维的概率向量 $p = G(x) = [p_1, p_2, \u0026hellip;, p_n]$，其中 $p_i$ 表示激活专家 $E_i$ 的概率。 专家输出聚合(Expert Output Aggregation): 根据门控网络的输出概率分布，将激活的专家网络的输出进行加权求和，得到 MoE 层的最终输出 $y$。 Noisy Top-k Gating 为了实现稀疏激活并确保专家使用均衡，MoE 通常采用 Noisy Top-k Gating 作为门控机制。这种方法通过引入噪声和 top-k 选择，既保证了计算效率，又避免了专家负载不均的问题。以下是其详细工作流程：\n门控分数计算: 对于输入样本 $x$，门控网络首先计算每个专家的门控分数 $H^{(i)}(x)$。这一分数包含两部分：线性变换和噪声项，公式如下：\n$$ H^{(i)}(x) =(x W_g)^{(i)} + \\epsilon \\cdot \\text{softplus}\\left((x W_{\\text{noise}})^{(i)} \\right), \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$ 参数说明： $W_g \\in \\mathbb{R}^{d \\times n}$：门控网络的可训练权重矩阵，$d$ 是输入特征维度，$n$ 是专家数量。 $W_{\\text{noise}} \\in \\mathbb{R}^{d \\times n}$：用于生成噪声的权重矩阵。 $\\epsilon \\sim \\mathcal{N}(0, 1)$：标准高斯噪声，增加门控随机性。 $\\text{softplus}(x) = \\log(1 + e^x)$：平滑激活函数，确保噪声非负。 噪声的引入避免了门控网络总是选择固定的专家，增强了模型的鲁棒性和多样性。\nTop-k 选择: 计算出门控分数向量 $H(x) = [H^{(1)}(x), H^{(2)}(x), \\dots, H^{(n)}(x)]$ 后，门控网络选择其中值最大的前 $k$ 个专家(通常 $k \\ll n$)。这一步骤通过 $\\text{topk}(v, k)$ 函数实现：\n$$ \\text{topk}^{(i)}(v, k) = \\begin{cases} v^{(i)} \u0026 \\text{if } v^{(i)} \\text{ is in the top } k \\text{ elements of } v \\\\ -\\infty \u0026 \\text{otherwise} \\end{cases} $$将非 Top-k 专家的分数设为 $-\\infty$，确保后续 softmax 操作中这些专家的概率为 0，实现稀疏性。\nSoftmax 归一化: 对 Top-k 专家的门控分数进行 softmax 归一化，得到稀疏的概率分布 $G(x)$：\n$$ G(x) = \\text{softmax}\\left( \\text{topk}(H(x), k) \\right) $$只有 Top-k 个专家的概率非零，其余为 0。例如，若 $n=100, k=2$，则 98 个专家的概率为 0。\n加权求和: 将 Top-k 个专家的输出按概率加权求和，得到 MoE 层的输出：\n$$ y = \\sum_{i=1}^{n} G^{(i)}(x) E_i(x) $$由于只有 $k$ 个专家被激活，计算量远低于激活所有 $n$ 个专家。\n辅助损失 为了避免门控网络过度偏向少数专家，MoE 引入了辅助损失(Auxiliary Loss)(Shazeer et al. 2017)，鼓励所有专家被均匀使用。一种常用方法是基于专家使用率的变异系数(Coefficient of Variation, CV)的平方：\n$$ \\mathcal{L}_{\\text{aux}} = w_{\\text{aux}} \\cdot \\text{CV}\\left( \\sum_{x \\in X} G(x) \\right)^2 $$ 参数说明：\n$X$：一个 mini-batch 的输入样本。 $\\sum_{x \\in X} G(x)$：统计每个专家在 mini-batch 中的激活次数。 $\\text{CV}$：标准差与均值的比值，衡量专家使用分布的均匀性。 $w_{\\text{aux}}$：辅助损失的权重，需手动调整。 作用：通过最小化 $\\mathcal{L}_{\\text{aux}}$，模型优化专家选择的均衡性，避免某些专家被过度使用而其他专家闲置。\nGShard GShard(Lepikhin et al. 2020)主要对 MoE 层进行分片，将 MoE 层中的专家网络 ${E_1, E_2, \u0026hellip;, E_n}$ 分散到多个 TPU 设备上。例如，如果有 $P$ 个 TPU 设备，可以将专家网络划分为 $P$ 组，每组专家网络分配到一个 TPU 设备上。Transformer 模型的其他层(例如自注意力层、LayerNorm 层) 则在所有 TPU 设备上复制。\nGShard 的改进门控机制:\nGShard 在 Noisy Top-k Gating 的基础上，进行了一些改进，以提高门控机制的性能和稳定性：\n专家容量(Expert Capacity):\n为了避免专家过载，GShard 引入了专家容量限制。每个专家网络都有一个容量上限，表示它最多可以处理的 token 数量。如果一个 token 被路由到一个已经达到容量上限的专家网络，则该 token 会被标记为 \u0026ldquo;overflowed\u0026rdquo;，门控输出会被设置为零向量，表示该 token 不会被路由到任何专家网络。\n局部组分发(Local Group Dispatching):\n为了提高门控效率，GShard 将 token 分组，在组级别强制执行专家容量限制。例如，将 mini-batch 中的 token 划分为多个局部组，每个局部组包含一定数量的 token。门控网络为每个局部组选择 top-k 个专家网络，并确保每个专家网络在一个局部组内处理的 token 数量不超过其容量上限。\n辅助损失(Auxiliary Loss):\nGShard 也使用了辅助损失函数来平衡专家负载。与原始 MoE 模型的辅助损失不同，GShard 的辅助损失旨在最小化每个专家网络路由到的数据比例的均方误差，更加直接地衡量专家负载平衡程度。\n随机路由(Random Routing):\n为了增加路由的随机性，GShard 在选择 top-k 个专家网络时，引入了随机路由机制。除了选择最佳的 top-k 个专家网络外，GShard 还会以一定的概率随机选择次优的专家网络，增加专家网络的多样性，提高模型的泛化能力。\n下面是 GShard 的核心算法流程:\nFig. 13. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)\nSwitch Transformer Switch Transformer(Fedus et al. 2021) 是 Google 提出的一个参数量达到万亿级别的 MoE 模型。其核心创新是将 Transformer 模型中的密集前馈网络(FFN) 层替换为稀疏的 Switch FFN 层。与 GShard 的 Top-2 Gating 不同，Switch Transformer 每个输入 token 只路由到一个专家网络，具有更高的稀疏性，进一步降低了计算成本，使得训练万亿参数模型成为可能。鼓励 token 路由在 $N$ 个专家之间更加均衡。Switch Transformer 的辅助损失基于实际路由比例与预测路由概率的乘积累加，具体公式如下：\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ 参数说明： $N$：专家的总数。\n$f_i$：路由到第 $i$ 个专家的 token 比例，定义为：\n$$ f_i = \\frac{1}{T} \\sum_{x \\in B} 1\\{\\text{argmax } p(x) = i\\} $$ $P_i$：gating 网络预测的第 $i$ 个专家的路由概率，定义为：\n$$ P_i = \\frac{1}{T} \\sum_{x \\in B} p_i(x) $$ $T$：批次 $B$ 中的 token 总数。\n$\\alpha$：辅助损失的权重超参数，通常设为 $10^{-2}$。\n通过最小化 $\\text{loss}$，模型使实际路由比例 $f_i$ 与预测概率 $P_i$ 趋于一致，从而间接促进专家间的负载平衡，避免部分专家闲置。\nFig. 14. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)\nSwitch Router 机制:\n路由预测:\n对于输入 token $x$，Switch Router 预测每个专家网络的路由概率 $p_i = G^{(i)}(x)$，其中 $i = 1, 2, \u0026hellip;, n$，n 是专家网络数量。\n专家选择:\n选择路由概率最高的专家网络作为最佳专家网络。Switch Transformer 采用 Top-1 路由策略，即每个 token 只路由到路由概率最高的专家网络。\ntoken 路由:\n将输入 token $x$ 路由到选择的最佳专家网络进行处理。\nSwitch Transformer 的训练稳定性优化:\n为提升 Switch Transformer 的训练稳定性，论文提出了如下优化策略：\n选择性精度(Selective Precision)\n在路由函数内部采用 FP32 精度既能提高训练稳定性，又能避免因 FP32 张量通信而产生的额外开销。具体来说，Switch Router 的计算过程全程使用 FP32，最终结果再转换为 FP16 以兼顾效率与精度。\n更小初始化(Smaller Initialization)\n建议将 Transformer 的权重初始化尺度参数 $s$ 从 1 调整至 0.1。较小的初始化尺度有助于缓解训练初期的梯度爆炸风险，从而提升整体训练稳定性。具体实现为：从均值为 0、标准差为 $\\sqrt{s/n}$(其中 $n$ 为输入单元数) 的截断正态分布中采样。\n更高专家 Dropout(Higher Expert Dropout)\n在专家 FFN 层中采用较高的 dropout 率(例如 0.4)，而在非专家层则保持较低的 dropout 率(例如 0.1)，这种设置能有效防止过拟合，进而增强模型的泛化能力。下图实验结果显示，在 GLUE、CNNDM、SQuAD 和 SuperGLUE 等任务上，当专家层 dropout 率设为 0.4 时，模型表现最佳。\nFig. 15. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: Fedus et al. 2021)\nSwitch Transformers 论文中使用下图直观的展示了使用不同的并行技术如何分割模型权重和数据:\nFig. 16. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)\n专家选择 专家选择(Expert Choice, EC)(Zhou et al. 2022) 是一种与 token 选择路由(如 GShard 的 top-2 或 Switch Transformer 的 top-1)相反的路由策略。在 token 选择路由中，每个 token 从所有专家中选择 top-k 个进行路由；而在专家选择路由中，每个专家从所有 token 中挑选 top-k 个进行处理。这种方法旨在解决 token 选择路由中的负载不均和 token 浪费问题，同时显著提高训练效率。下面是具体的计算过程：\n计算 token-to-expert 亲和度分数\n对于输入矩阵 $X \\in \\mathbb{R}^{n \\times d}$，计算 token-to-expert 亲和度分数矩阵 $S \\in \\mathbb{R}^{n \\times e}$ 的过程为：\n$$ S = \\text{softmax}(X \\cdot W_g), \\quad \\text{where } W_g \\in \\mathbb{R}^{d \\times e}. $$ 这里，$W_g$ 为门控权重矩阵，$e$ 为专家数量。\n专家选择 token\n每个专家从所有 token 中选择 top-k 个进行处理。通过对 $S^T$ 进行 top-k 选择：\n$$ G, I = \\text{top-}k(S^T, k), $$得到：\n门控矩阵 $G \\in \\mathbb{R}^{e \\times k}$： 记录专家选择的 token 对应的路由权重，其中 $G[i, j]$ 表示专家 $i$ 选择的第 $j$ 个 token 的权重； token 索引矩阵 $I \\in \\mathbb{R}^{e \\times k}$： 表示每个专家选择的 token 在输入中的索引。 One-hot 编码\n将 token 索引矩阵 $I$ 转换为 one-hot 编码矩阵 $P \\in \\mathbb{R}^{e \\times k \\times n}$，用于后续计算：\n$$ P = \\operatorname{one}-\\operatorname{hot}(I) $$ 构造 Gated FFN 层输入\n对于每个专家 $i$，其 gated FFN 层的输入为：\n$$ (P \\cdot X) \\in \\mathbb{R}^{e \\times k \\times d}. $$ EC 通过正则化限制每个 token 被路由到的专家数量，从而控制模型的稀疏性。一个常见的正则化目标如下：\n$$ \\begin{aligned} \u0026 \\max_{A} \\langle S^{\\top}, A \\rangle + \\lambda H(A) \\\\ \u0026 \\text{s.t. } \\forall i: \\sum_{j'} A[i, j'] = k, \\quad \\forall j: \\sum_{i'} A[i', j] \\leq b, \\quad \\forall i,j: 0 \\leq A[i, j] \\leq 1, \\end{aligned} $$考虑的优化问题中定义了一个矩阵 $A$，其第 $i$ 行第 $j$ 列的元素表示第 $i$ 个专家是否选择了第 $j$ 个 token(取值 0 或 1)。由于该优化问题求解较为复杂，论文中采用 Dijkstra 算法(通过多次迭代获得近似解)来解决。\n参数 $b$ 通常由批量中 token 总数 $n$ 与容量因子决定，其中容量因子表示每个 token 平均使用的专家数量。大多数实验采用较高的容量因子，实验结果表明，即使在容量降低的情况下，EC 整体表现仍优于传统的 top-1 token 选择路由，尽管 capped expert choice 略微降低了微调性能。\nEC 的优势主要体现在以下两方面：\n完美负载均衡： 每个专家固定处理 $k$ 个 token，从而避免了部分专家过载而其他专家闲置的问题，实现了理想的负载均衡。 更高训练效率： 实验表明，EC 能将训练收敛速度提升约 2 倍，相较于传统 token 选择路由具有更高的效率。 但 EC 也存在以下局限性：\n批量大小要求： 由于 EC 对 batch size 有较高要求，因此不适用于较小 batch size 的场景。 自回归生成限制： 在自回归文本生成任务中，由于无法预知未来 token，EC 的 top-k 选择无法实现，因此不适用于此类任务。 序列并行 序列并行(Sequence Parallelism, SP) 是针对长序列模型(如 Transformer)提出的一种并行化策略，通过在序列维度上对输入进行划分，大幅降低激活内存占用并提高训练效率。它常与数据并行、张量并行或流水线并行结合使用，尤其适合处理超长文本或其他序列数据。\nColossal-AI 序列并行 Fig. 17. The overall architecture of the proposed sequence parallelism and existing parallel approaches. For sequence parallelism, Device 1 and Device 2 share the same trainable parameters. (Image source: Li, et al. 2021)\n自注意力(self-attention) 的计算复杂度和内存开销与序列长度 $s$ 的平方 $O(s^2)$ 成正比，长序列数据将增加中间 activation 内存使用量，从而限制设备的训练能力。Colossal-AI 序列并行(Li, et al. 2021)从系统角度提出拆分超长序列到多卡，具体的解决步骤如下。\n序列分块\n将输入序列划分为若干块，每个块由不同 GPU 保存和计算；因此每张卡只需存储自己对应的序列块激活，避免单卡内存爆炸。 环状通信 + 自注意力\n提出环自注意力(Ring Self-Attention, RSA) 机制：各 GPU 先本地计算局部注意力，然后依次向相邻 GPU 传递(环状结构)Key/Value 分块，多轮迭代后保证每个 GPU 能获取全局序列信息。 与其他并行方式结合\n不受注意力头数、层数等超参数限制，可配合数据并行、张量并行、流水线并行等技术，共同突破大规模模型的序列长度限制。 Fig. 18. Ring Self-Attention. (Image source: Li, et al. 2021)\nMegatron-LM 序列并行 Megatron-LM(Shoeybi et al. 2019) 原本使用张量并行分担部分激活值，但 Transformer 中的 LayerNorm、Dropout 等操作的激活值仍需完整保存在单卡，显存消耗依旧庞大。因此 NVIDIA 提出 Megatron-LM 序列并行(Korthikanti, et al. 2022)在序列维度对这些激活值进行切分，大幅降低占用。\nFig. 19. Transformer layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)\nFig. 20. MLP layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)\n序列维度切分\n针对 LayerNorm、Dropout 等难以在张量维度切分的激活，将其沿序列维度划分，使每个 GPU 只处理一部分序列的非线性操作。 张量并行仍保留\n注意力(Attention)、MLP 等线性操作继续使用张量并行；序列并行的激活需要在前后进行对应的 All-Gather 或 Reduce-Scatter 以交换数据。 选择性激活重计算(Selective Activation Recomputation)\n针对部分计算量小但激活量大的操作，选择在反向传播时临时重算，以进一步节省显存。 DeepSpeed-Ulysses 序列并行 DeepSpeed-Ulysses(Jacobs et al. 2023) 针对超长序列训练提出了一种高效的序列并行方案，通过在序列维度对输入进行划分，并结合两阶段的全对全通信，有效降低通信量和激活内存，从而支持训练百万 token 级别的长序列 Transformer 模型。\nFig. 21. DeepSpeed sequence parallelism(DeepSpeed-Ulysses) design. (Image source: Jacobs et al. 2023)\n序列划分 + 全对全通信\n将输入序列沿序列维度划分到 $P$ 张 GPU 上，每个 GPU 只处理局部 $N/P$ 的序列；在注意力计算前，通过 All-to-All 操作交换查询($Q$)、键($K$)和值($V$)，使得每个 GPU 获得完整序列信息，但仅计算分配到的注意力头。\n双阶段通信优化\n第一次 All-to-All： 在注意力计算前对 $Q$/$K$/$V$ 进行全对全交换，分散激活计算并降低每卡内存压力； 第二次 All-to-All： 在注意力计算后收集输出上下文，将其重新映射为局部序列分区，既恢复原始序列结构，又显著减少了通信数据量。 高效通信与通用性\n利用全对全通信，使得通信量降为 $O(N/P)$，相比传统的 All-Gather 方法(通信量 $O(N)$)节省了近 $P$ 倍的带宽；同时，该方案适用于密集和稀疏注意力，并可与 ZeRO-3 内存优化无缝集成，从而支持更大模型和更长序列的高效训练。\nFig. 22. DeepSpeed-Ulysses vs Megatron LM. (Image source: DeepSpeed Blogs)\n在 64 卡 A100 环境下，吞吐量较 Megatron-LM 序列并行最高提升 2.5 倍，并可处理更长序列(百万级 token)； 收敛性能与原模型无差别，可轻松集成到 Megatron-DeepSpeed 框架。 优化器相关的并行：ZeRO ZeRO(Zero Redundancy Optimizer)(Rajbhandari et al. 2019)是一种旨在消除训练大型模型时内存冗余的优化器并行技术。大型模型训练的内存主要消耗在两大部分：\n模型状态(Model States)： 包括优化器状态(如 Adam 的动量和二阶矩)、梯度和模型参数。混合精度训练不仅需要存储 FP16 数据，还需保留 FP32 版本的参数和状态，导致内存占用更高。 激活值、临时缓冲区与内存碎片(Residual States)： 这些数据在前向传播和反向传播中仅被使用一次，但同样会占用大量内存。 为了解决内存冗余问题，ZeRO 采用了两大策略：\nZeRO-DP(Data Parallelism)：\n针对模型状态，通过将优化器状态、梯度和参数分片分布到多个数据并行进程中，消除冗余，同时利用动态通信调度减少通信量。\nZeRO-R(Residuals Optimization)：\n针对激活值和临时缓冲区，采用分片激活值重计算、固定缓冲区大小以及实时内存碎片整理等方法优化内存使用。\nZeRO 分片策略 ZeRO 分为三个阶段，每个阶段在前一阶段基础上进一步减少内存冗余，从而使得训练超大模型成为可能：\nZeRO-1(优化器状态分片) 原理： 将优化器状态(如 Adam 的动量和二阶矩)沿参数维度分为 $P$ 个分片($P$ 为 GPU 数量)，每个 GPU 只存储与其负责模型参数对应的状态。 局部更新：每个 GPU 在参数更新阶段仅更新其本地存储的状态和参数分片，无需额外跨 GPU 通信。 ZeRO-2(梯度分片) 原理： 在优化器状态分片的基础上，将梯度沿参数维度同样进行分片，每个 GPU 只存储对应梯度分片。 每个 GPU 计算局部梯度，利用高效的 Reduce-Scatter 操作进行梯度聚合，再更新本地参数分片。 ZeRO-3(参数分片) 原理： 在 ZeRO-1 和 ZeRO-2 的基础上，将模型参数(通常为 16 位数据)也进行分片，每个 GPU 只存储与其对应的参数分片。 按需参数收集：在前向或反向传播过程中，若某个 GPU 需要完整的模型参数，则从其他 GPU 收集缺失的分片，这一过程仅在必要时进行，以减少通信开销。 下图展示了不同阶段下每个设备上模型状态内存消耗的对比情况：\nFig. 23. Comparing the per-device memory consumption of model states, with three stages of ZeRO-DP optimizations. (Image source: Rajbhandari et al. 2019)\nDeepSpeed ZeRO 分片与 Offload 策略对比 为了更好地理解 DeepSpeed 的 ZeRO 策略，以下对各阶段及 Offload 方案进行对比：\nZeRO Stage 描述 显存占用 训练速度 ZeRO-0 纯数据并行，不进行任何分片，所有状态在每个 GPU 上完全复制。 最高 最快 ZeRO-1 仅分片优化器状态，梯度和参数仍复制。 较高 略慢于 ZeRO-0 ZeRO-2 分片优化器状态和梯度。 中等 慢于 ZeRO-1 ZeRO-3 分片优化器状态、梯度和模型参数。 最低 明显慢于 ZeRO-2，受模型规模和网络带宽影响 Offload 类型 描述 显存占用 训练速度 ZeRO-1 + CPU Offload 在 ZeRO-1 基础上，将优化器状态卸载到 CPU 内存，降低 GPU 显存占用，但依赖 PCIe 带宽且占用 CPU 内存。 中偏低 慢于 ZeRO-1 ZeRO-2 + CPU Offload 在 ZeRO-2 基础上，将优化器状态卸载到 CPU 内存，对大模型进一步降低 GPU 显存，但增加 CPU–GPU 数据传输。 较低 慢于 ZeRO-2 ZeRO-3 + CPU Offload 在 ZeRO-3 基础上，将优化器状态和模型参数卸载到 CPU，GPU 显存占用最低，但 CPU–GPU 通信开销极大。 极低 非常慢 ZeRO-Infinity(NVMe Offload) 基于 ZeRO-3，将状态卸载到 NVMe 设备，突破 CPU 内存限制，适合超大模型；性能高度依赖 NVMe 并行读写速度。 极低需 NVMe 支持 慢于 ZeRO-3，但通常优于 CPU Offload 方案 通信量与性能影响 ZeRO-0/1/2：\n主要依赖 All-Reduce 进行梯度同步，通信量相对较低。\nZeRO-3：\n需要对模型参数进行 All-Gather/All-Reduce 操作，通信量显著增加，网络带宽成为关键瓶颈。\nOffload 策略(CPU/NVMe)：\n数据传输主要在 CPU ↔ GPU 或 NVMe ↔ GPU 之间，传输带宽远低于 GPU 之间的通信，可能显著影响训练速度，尤其在 ZeRO-3 场景下更为明显。\n多维度并行 多维度并行(Multi-dimensional Parallelism) 是指在分布式训练中将数据并行、模型并行和流水线并行等多种并行技术有机结合，以充分利用现代 GPU 集群的计算资源。通过这种“3D 并行”或“4D 并行”策略，不仅能提高内存效率，还能提升计算效率，从而实现超大规模(甚至万亿参数级别)模型的高效训练。\n3D 并行 随着 GPU 集群计算能力的迅速提升，训练万亿参数级别的模型不再遥不可及。DeepSpeed 将数据并行、模型并行与流水线并行三种技术融合，构建了一种“3D 并行”策略。该策略主要解决训练超大模型所面临的两大挑战：\n内存效率：\n模型层被划分到不同的流水线阶段，每个阶段内部又通过模型并行进一步分割，减少了模型、优化器和激活值占用的内存量。但需要注意，模型分割不能无限制进行，否则通信开销会显著增加，进而影响计算效率。\n计算效率：\n为了让计算工作者数量超越单纯模型和流水线并行的限制，同时保证计算效率，DeepSpeed 借助 ZeRO-DP(基于优化器状态分片的数据并行)进行扩展。ZeRO-DP 不仅进一步优化内存使用，还通过拓扑感知映射将数据并行组分配到局部高带宽通信的设备上，极大降低了通信开销。\n下面的图示展示了 3D 并行的整体策略：\nFig. 24. Example 3D parallelism with 32 workers. Layers of the neural network are divided among four pipeline stages. Layers within each pipeline stage are further partitioned among four model parallel workers. Lastly, each pipeline is replicated across two data parallel instances, and ZeRO partitions the optimizer states across the data parallel replicas. (Image source: Majumder et al. 2020)\n每个并行维度(数据、模型、流水线)均经过精心映射，以充分利用节点内和节点间的通信带宽。具体策略包括：\n优化节点内通信： 由于模型并行的通信开销最大，优先将模型并行组安排在同一节点内，以利用较高的节点内带宽(例如采用 NVIDIA Megatron-LM 的张量切分方式)； 数据并行与流水线并行： 当模型并行不覆盖整个节点时，数据并行组尽可能安排在同一节点内；而流水线并行由于通信量较小，可灵活安排跨节点调度。 通过减少每个数据并行组中通信数据量以及提高局部并行通信的并行度，整体通信带宽得到有效放大。\nFig. 25. Mapping of workers in Figure 24 to GPUs on a system with eight nodes, each with four GPUs. Coloring denotes GPUs on the same node. (Image source: Majumder et al. 2020)\n4D 并行 为了进一步扩展模型规模，Llama3(Grattafiori et al. 2024) 训练的时候采用了 4D 并行，它结合了四种并行方法，将模型进行更细粒度的分片，使每个 GPU 上的模型参数、优化器状态、梯度和激活值均能适配高带宽内存(HBM)的容量限制。这四种并行方法分别是：\n张量并行(Tensor Parallelism, TP)： 将单个权重张量划分为多个块，分布在不同设备上； 流水线并行(Pipeline Parallelism, PP)： 将模型垂直划分为多个阶段，各阶段在不同设备上并行处理不同微批次； 上下文并行(Context Parallelism, CP)： 将输入上下文划分成多个段，从而缓解长序列输入时的内存瓶颈； 数据并行(Data Parallelism, DP)，通常采用完全分片的数据并行(FSDP)： 对模型、优化器状态和梯度进行分片，并在每个训练步后同步。 下面的图示展示了 4D 并行在 16 个 GPU 上的实现示例，各 GPU 的位置用向量 [D1, D2, D3, D4] 表示，其中每个维度对应一种并行策略。GPU 按照 [TP, CP, PP, DP] 四个维度分组，每个维度的组内大小均为 2。例如，GPU0 和 GPU1 属于同一张量并行组；GPU0 和 GPU2 属于同一上下文并行组；GPU0 和 GPU4 属于同一流水线并行组；GPU0 和 GPU8 属于同一数据并行组：\nFig. 26. Illustration of 4D parallelism. (Image source: Grattafiori et al. 2024)\n通过 4D 并行策略，Llama3 在训练时能够充分利用多个 GPU 的计算资源，同时有效减少内存占用，支持训练超大规模的模型。\n内存优化技术 除了并行训练技术，还有许多内存优化技术设计可以帮助训练 LLMs，这些设计主要从减少训练过程中各个环节的内存占用入手。\nCPU Offloading CPU Offloading(Rhu et al. 2016) 是指当 GPU 内存不足时，将暂时无需使用的数据或者张量卸载到 CPU 并在需要时再加载回 GPU是一种常见且直观的做法。它的主要目的是利用 CPU 内存更大的容量来扩展可用空间，从而在显存受限的环境下也能训练更大规模的模型。然而，这种方法会带来额外的数据传输开销，通常会降低训练速度，因此近年来应用相对减少。\n识别可卸载张量: 识别训练过程中暂时不使用的张量，例如模型参数、优化器状态、中间激活值等。判断张量是否可以卸载的依据可以是张量的使用频率、生命周期等。 张量卸载: 将可卸载的张量从 GPU 内存移动到 CPU 内存。数据传输通常通过 PCIe 总线进行。 张量预取(Prefetching): 在需要使用卸载到 CPU 内存的张量之前，提前将张量从 CPU 内存加载回 GPU 内存。预取操作可以与 GPU 的计算操作并行进行，以减少数据加载的延迟。 张量使用: GPU 使用加载回 GPU 内存的张量进行计算。 张量再次卸载: 在张量使用完毕后，如果张量在一段时间内不再需要使用，可以再次将其卸载到 CPU 内存，释放 GPU 内存空间。 ZeRO-Offload 和 ZeRO-Infinity 是 DeepSpeed 库中实现的基于 CPU 卸载的内存优化技术。ZeRO-Offload 将优化器状态卸载到 CPU 内存，ZeRO-Infinity 更进一步，将模型参数也卸载到 CPU 内存甚至 NVMe 磁盘，突破 GPU 内存墙，支持训练更大规模的模型。\n下图直观的展示了异构系统(Heterogenous system) 内存优化技术：\nFig. 27. Heterogenous system illustration. (Image source: Clolossal-AI Documentation)\n激活重计算/梯度检查点 激活重计算/梯度检查点(Activation Recomputation/Gradient Checkpointing)(Chen et al. 2016) 是一种以计算换内存的技术。在训练过程中，只保存部分激活值(例如每个 Transformer 层的输入激活值)，在反向传播时，重新计算未保存的激活值。激活重计算可以显著减少训练过程中的激活值内存占用，尤其是在训练深层神经网络时效果明显。\n选择检查点: 选择模型中的一些层作为 checkpoint。通常选择模型中的关键层，例如 Transformer 层的输入层。 前向传播(Forward Pass): 在前向传播过程中，只保存检查点层的激活值，对于非检查点层的激活值，在计算完成后立即释放，不进行保存。 反向传播(Backward Pass): 在反向传播过程中，当需要计算某个非检查点层的梯度时，首先重新进行一次前向传播，计算该层的激活值(重计算)，然后再进行反向传播计算梯度。对于检查点层，由于已经保存了检查点层的激活值，可以直接使用保存的激活值进行反向传播，无需重新计算。 下面进行激活重计算的内存成本分析。为方便分析，假设模型共有 $n$ 层网络结构，并将其均匀地划分为 $k$ 个分段(segment)。这样，每个分段大约包含 $n/k$ 层网络。做激活重计算时，我们只保存各分段边界处(即检查点)的激活值，其余需要时再重算。下面函数表示训练过程中的内存需求：\n$$ \\text{cost-total} \\;=\\; \\max_{i=1,\\ldots,k}\\bigl[\\text{cost-of-segment}(i)\\bigr] \\;+\\; O(k) \\;=\\; O\\Bigl(\\tfrac{n}{k}\\Bigr) \\;+\\; O(k). $$接下来考虑在给定 $n$ 的前提下，如何选择最优 $k$ 以最小化\n$$ f(k) \\;=\\; \\frac{n}{k} \\;+\\; k. $$对 $f(k)$ 关于 $k$ 求导并令其为 0(只考虑 $k\u0026gt;0$)：\n$$ f'(k) \\;=\\; -\\frac{n}{k^2} \\;+\\; 1 \\;=\\; 0 \\quad\\Longrightarrow\\quad k^2 = n \\quad\\Longrightarrow\\quad k = \\sqrt{n}. $$将 $k = \\sqrt{n}$ 代入，可得最小内存开销大约为\n$$ f(\\sqrt{n}) \\;=\\; \\frac{n}{\\sqrt{n}} \\;+\\; \\sqrt{n} \\;=\\; 2\\sqrt{n}. $$因此，该做法的总体峰值内存需求可降到 $O(\\sqrt{n})$ 的量级(对比一般直接保存所有激活的 $O(n)$ 内存)，这就是激活重计算技术能带来“亚线性”内存占用的原因。下图直观的展现了这个 trick 的效果。\nFig. 28. The memory cost of different memory saving algorithms. Sharing: Memory used by intermediate results is recycled when no longer needed. Inplace: Save the output directly into memory of an input value. (Image source: Chen et al. 2016)\n需要注意，激活重计算需要在反向传播阶段额外进行前向重算，每个分段要做一次到 $n/k$ 层的前向计算。若将网络分为 $k$ 个分段，反向传播时重算总计约 $k \\times \\bigl(n/k\\bigr) = n$ 层的前向操作，相当于每个训练迭代中整体多做了大约一次“前向传播”。这在 LLM 训练中通常能被接受，原因是：\n比起快速耗尽 GPU 显存导致无法训练大规模模型，这种在计算上的额外代价通常更容易承受。 当模型十分深($n$ 很大)时，用激活重计算技术可以将内存使用从 $O(n)$ 显著降至 $O(\\sqrt{n})$，使得更多、更深的大模型能够在给定硬件上进行训练。 混合精度训练 混合精度训练(Mixed Precision Training)(Micikevicius al. 2017)是一种在模型训练过程中同时利用低精度浮点数(如 FP16 或 BF16)与高精度浮点数(如 FP32)的技术，其核心目标是在减少显存占用、加速训练的同时，尽可能保持与全精度训练相当的模型精度。\n现代 GPU 在低精度计算上具有更高的吞吐量和更低的显存占用，从而降低访存开销与内存带宽需求，使混合精度训练能显著提升训练速度。下图展示了一个网络层中混合精度训练的基本流程：前向和反向传播主要采用半精度(FP16)运算，而在梯度累积与参数更新时使用全精度(FP32)，以规避低精度计算可能带来的数值精度问题。\nFig. 29. Mixed precision training iteration for a layer. (Image source: Micikevicius al. 2017)\n混合精度训练主要依赖以下三项关键技术：\n权重全精度主副本\n为防止梯度在 FP16 下因幅度过小而被截断为零，训练过程中保持一份 FP32 的权重主副本。具体流程为：\n初始化： 使用 FP32 权重作为模型的主副本； 前向/反向传播： 每次迭代开始前，将 FP32 权重转换为 FP16 用于前向传播和反向传播，计算得到 FP16 梯度； 参数更新： 在更新参数前，将 FP16 梯度转换为 FP32，并用其更新 FP32 主副本。 这种设计既能利用低精度计算的高效性，又确保了参数更新的准确性。\n损失缩放\n为避免因低精度表示范围受限而导致梯度下溢，通常在反向传播前对损失值进行放大处理。具体流程为：\n使用 FP32 计算损失 $L$； 将损失乘以缩放因子 $S$，得到 $L\u0026rsquo; = L \\times S$ 后进行反向传播，计算出 FP16 梯度； 在参数更新前，将梯度除以 $S$ 还原为真实梯度。 缩放因子的选择十分关键：过小可能无法避免梯度下溢，过大则有可能引起梯度上溢。动态损失缩放技术可以根据训练过程中梯度的实际情况自动调整缩放因子。\n如下图所示，通过放大损失使梯度分布更集中于较高数值部分，从而保留那些在低精度表示下可能被截断的细微信息。\nFig. 30. The histogram of gradients in full precision. The left part up to $2^{-24}$ will be zero-ed off once the model switches to FP16. (Image source: Micikevicius al. 2017)\n算术精度控制\n对于对精度要求较高的运算(如向量点积和求和归约)，可采用 FP32 进行累积计算，然后再转换为 FP16 存储；而对于逐元素运算，则可根据具体需求选择使用 FP16 或 FP32。 压缩 在深度学习训练过程中，中间结果(如激活值和梯度信息)虽然仅在一次前向传播和一次反向传播中使用，但往往占用大量内存。考虑到两次使用之间存在明显的时间间隔，可以在第一次使用后对数据进行压缩(Compression)，待后续需要时再解压缩，从而有效降低内存占用。\n压缩技术主要应用于两个场景：\n激活值压缩： 前向传播后对激活值进行压缩，反向传播前解压缩。这对深层神经网络尤为重要，因为激活值通常占用大量内存。 梯度压缩： 在反向传播计算梯度后、梯度同步前对梯度进行压缩，减少跨 GPU 通信的数据量，从而提高分布式训练效率。 压缩技术可以分为两类：\n无损压缩(Lossless Compression):\n采用如 Huffman 编码或 Lempel-Ziv 算法等方法，确保解压缩后的数据与原始数据完全一致。但由于压缩率较低，其内存节省效果有限。\n有损压缩(Lossy Compression):\n使用如 JPEG 或 MPEG 等算法，在允许一定数据损失的前提下获得更高的压缩率。这种方法能显著降低内存占用，但可能对模型精度和收敛性产生一定影响。\nGist(Jain et al. 2018)是一种用于激活值压缩的内存优化技术，其核心在于利用数据编码策略压缩中间结果，主要包含两种编码方案：\n层特定无损编码(Layer-Specific Lossless Encoding):\n针对特定层结构(例如 ReLU-Pool 与 ReLU-Conv)，设计专门的无损编码方案：\n对于 ReLU-Pool 层，可采用二值化编码； 对于 ReLU-Conv 层，则使用稀疏存储与稠密计算编码。 激进有损编码(Aggressive Lossy Encoding):\n采用 延迟精度降低(Delayed Precision Reduction, DPR) 技术。DPR 的核心思想是：激活值在前向传播时需保持高精度，而在反向传播时可容忍较低精度。因此，在前向传播后将激活值压缩到较低精度，反向传播前再解压至高精度。\n内存高效优化器 传统优化器(如 Adam、SGD with Momentum)在训练过程中需要为每个模型参数维护大量状态数据(例如 momentum 和 variance)，其内存占用往往与模型参数量相当甚至更高。例如，以 Adam 优化器(Kingma et al. 2014)为例，每个参数需要存储一阶矩和二阶矩，与参数本身及其梯度加起来，整个训练过程大约需要 4 倍于模型权重的内存，这对大型模型训练构成了严峻挑战。\n为降低内存消耗，内存高效优化器主要通过以下策略进行设计：\n减少状态变量数量： 只保存必要的统计信息，而非完整矩阵； 降低状态变量精度： 采用 FP16 或 bfloat16 存储； 共享状态变量： 在多个参数间共享部分状态信息。 Adafactor Adafactor(Shazeer et al. 2018) 是一种内存高效的自适应学习率优化器。与 Adam 不同，Adafactor 不存储完整的二阶矩估计矩阵，而是只存储两个向量(行、列统计)替代完整的二阶矩矩阵，显著降低了内存占用，特别适用于参数矩阵具有低秩结构的场景。\nSM3 SM3(Sparse Momentum for Massive Models)(Anil et al. 2019) 通过稀疏更新和状态共享，提供了一种同样内存高效的自适应优化方案。\n稀疏 Momentum： 只对梯度非零的参数更新 Momentum，从而减少计算和存储开销； 状态共享： 在一定程度上允许不同参数共享状态变量，进一步降低内存消耗； 自适应学习率： 根据各参数梯度动态调整学习率，提高了模型训练的稳定性和收敛速度。 LoRA LoRA (Low-Rank Adaptation)(Hu et al. 2021) 提出在预训练权重旁引入 低秩适配器 的方法，通过添加少量参数实现高效微调，同时保持预训练模型原有的推理能力。\n下图直观展示了 LoRA 的原理和初始化策略：\nFig. 31. An illustration of LoRA. (Image source: Hu et al. 2021)\n在标准前向传播中，模型输出为\n$$ h = W_0 x, $$而引入 LoRA 后，输出变为\n$$ h = W_0 x + \\Delta W x = W_0 x + B A x. $$其中：\n$A \\in \\mathbb{R}^{r \\times k}$（降维矩阵）：将输入从 $k$ 维映射到更低的 $r$ 维； $B \\in \\mathbb{R}^{d \\times r}$（升维矩阵）：将降维后的特征从 $r$ 维映射回原来的 $d$ 维； 输入 $x$： 维度为 $\\mathbb{R}^{k}$； 原始权重 $W_0$： 维度为 $\\mathbb{R}^{d \\times k}$，因而 $W_0 x \\in \\mathbb{R}^{d}$； 假设预训练权重矩阵为\n$$ \\mathbf{W} \\in \\mathbb{R}^{d \\times k}, $$\nLoRA 在其上添加低秩更新项，从而得到新的权重表示：\n$$ \\mathbf{W}' = \\mathbf{W} + \\alpha\\, \\mathbf{B}\\mathbf{A}, $$其中：\n$A \\in \\mathbb{R}^{r \\times k}$（降维矩阵）：将输入从 $k$ 维映射到更低的 $r$ 维； $B \\in \\mathbb{R}^{d \\times r}$（升维矩阵）：将降维后的特征从 $r$ 维映射回原来的 $d$ 维； $r \\ll \\min(d, k)$（低秩维度）：通常取值为 $4$ 到 $16$，在保证模型表达能力的同时尽量减少新增参数； $\\alpha$（缩放因子）：用于放大低秩更新参数 $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$，补偿低秩分解带来的数值幅度较小的问题（通常设置为 $\\alpha = 2 \\times r$，例如当 $r = 8$ 时，$\\alpha = 16$）。 在微调过程中，原始权重 $\\mathbf{W}$ 被冻结，只更新 $\\mathbf{A}$ 和 $\\mathbf{B}$，因而大大减少了训练和存储的参数量。\n为了确保微调初期引入的更新项 $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$ 对原模型的影响尽量小，通常采用以下初始化策略：\n降维矩阵 $\\mathbf{A}$ 的初始化\n高斯初始化：令 $\\mathbf{A} \\sim \\mathcal{N}(0,\\sigma^2)$（一般 $\\sigma$ 取较小值，如 0.02），保证初始更新量足够小，从而不至于严重干扰模型输出。 Kaiming(He) 初始化：Kaiming 初始化是一种专为深层网络设计的权重初始化方法，其目标是保持前向信号和反向梯度在网络层之间的稳定性。对于 LoRA，只要确保尺度较小(或配合合适的缩放因子)，即可使初始 $\\Delta \\mathbf{W}$ 接近零。 升维矩阵 $\\mathbf{B}$ 的初始化\n通常将 $\\mathbf{B}$ 初始化为全零矩阵，这样初始时即有 $\\mathbf{B}\\mathbf{A} = 0$，进一步降低对原模型影响。 采用 LoRA 进行训练具备以下优势：\n参数高效：仅引入低秩适配器参数，减少了需要训练和存储的总参数量。 显存与计算效率：冻结大部分预训练权重，微调过程中仅更新小规模参数，显著降低了显存占用与算力开销。 无额外推理时延：训练完成后，可将更新项 $\\Delta \\mathbf{W}$ 合并回原始权重，从而在推理阶段不会增加额外计算量。 模块选择灵活性：通过 --lora_target 或 --lora-target 参数，可以指定仅在特定线性模块上应用 LoRA 更新。支持的目标模块包括： q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj, 这种设计允许用户根据具体任务需求，针对性地选择关键模块进行调优，从而进一步提高微调效率和适应性。 QLoRA QLoRA(Dettmers et al. 2023) 是在 LoRA 基础上结合量化思想对大规模模型进行高效微调的一种方法。它通过以下三个关键改进，大幅降低显存占用，同时保持模型精度基本不变：\n4 位标准浮点数(NF4) 量化\n采用基于分块的分位量化策略，将原始模型权重量化为 4 位，从而在细微损失精度的情况下实现显著的存储压缩。\n双重量化(Double Quantization)\n在对普通参数进行一次量化后，再对量化常数进行一次额外的量化，从而进一步减小缓存占用。\n分页优化器(Paged Optimizer)\n当显存使用过高时，自动将部分优化过程转移到 CPU 内存，从而减轻 GPU 显存压力，提升可伸缩性。\n与传统的 LoRA 仅减少需微调参数数量不同，QLoRA 还通过 4 位量化来压缩所有权重，从而在保证接近原有精度的同时，最大限度减少显存占用和数据传输开销。\nFig. 32. Different finetuning methods and their memory requirements. QLoRA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. (Image source: Dettmers et al. 2023)\n这种方法可以看作是对 LoRA 的进一步扩展：LoRA 通过减少需要微调的权重数量来提升效率，而 QLoRA 则在此基础上，将所有权重(包括未微调的部分)量化到 4 位精度，在总体上实现存储与计算的双重压缩，适合对 LLM 进行资源受限环境下的高效微调。\n总结 并行技术和内存优化策略需要根据具体的模型结构、数据集规模、硬件资源和训练目标进行权衡和选择。通常情况下，需要将多种技术结合使用，才能有效地训练大规模的模型，并取得最佳的性能和效率。\n参考文献 [1] Weng, Lilian, and Greg Brockman. \u0026ldquo;Techniques for training large neural networks.\u0026rdquo; OpenAI Blog, 2022.\n[2] Shenggui Li, Siqi Mai. \u0026ldquo;Paradigms of Parallelism.\u0026rdquo; Colossal-AI Documentation, 2024.\n[3] Li, Shen, et al. \u0026ldquo;Pytorch distributed: Experiences on accelerating data parallel training.\u0026rdquo; arXiv preprint, 2020.\n[4] Li, Mu, et al. \u0026ldquo;Communication efficient distributed machine learning with the parameter server.\u0026rdquo; Advances in Neural Information Processing Systems 27, 2014.\n[5] Huang, Yanping, et al. \u0026ldquo;Gpipe: Efficient training of giant neural networks using pipeline parallelism.\u0026rdquo; Advances in Neural Information Processing Systems 32, 2019.\n[6] Harlap, Aaron, et al. \u0026ldquo;Pipedream: Fast and efficient pipeline parallel dnn training.\u0026rdquo; arXiv preprint, 2018.\n[7] Narayanan, Deepak, et al. \u0026ldquo;Memory-efficient pipeline-parallel dnn training.\u0026rdquo; International Conference on Machine Learning, PMLR, 2021.\n[8] Shoeybi, Mohammad, et al. \u0026ldquo;Megatron-lm: Training multi-billion parameter language models using model parallelism.\u0026rdquo; arXiv preprint, 2019.\n[9] Narayanan, Deepak, et al. \u0026ldquo;Efficient large-scale language model training on gpu clusters using megatron-lm.\u0026rdquo; Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021.\n[10] Shazeer, Noam, et al. \u0026ldquo;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\u0026rdquo; arXiv preprint, 2017.\n[11] Lepikhin, Dmitry, et al. \u0026ldquo;Gshard: Scaling giant models with conditional computation and automatic sharding.\u0026rdquo; arXiv preprint, 2020.\n[12] Fedus, William, Barret Zoph, and Noam Shazeer. \u0026ldquo;Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\u0026rdquo; Journal of Machine Learning Research 23.120, 2022: 1–39.\n[13] Zhou, Yanqi, et al. \u0026ldquo;Mixture-of-experts with expert choice routing.\u0026rdquo; Advances in Neural Information Processing Systems 35, 2022: 7103–7114.\n[14] Li, Shenggui, et al. \u0026ldquo;Sequence parallelism: Long sequence training from system perspective.\u0026rdquo; arXiv preprint, 2021.\n[15] Korthikanti, Vijay Anand, et al. \u0026ldquo;Reducing activation recomputation in large transformer models.\u0026rdquo; Proceedings of Machine Learning and Systems 5, 2023: 341–353.\n[16] Jacobs, Sam Ade, et al. \u0026ldquo;Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.\u0026rdquo; arXiv preprint, 2023.\n[17] DeepSpeed. \u0026ldquo;DeepSpeed Ulysses README.\u0026rdquo; GitHub repository.\n[18] Rajbhandari, Samyam, et al. \u0026ldquo;Zero: Memory optimizations toward training trillion parameter models.\u0026rdquo; SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, 2020.\n[19] Microsoft Research. \u0026ldquo;DeepSpeed: Extreme-scale model training for everyone.\u0026rdquo; 2020.\n[20] Dubey, Abhimanyu, et al. \u0026ldquo;The llama 3 herd of models.\u0026rdquo; arXiv preprint, 2024.\n[21] Rhu, Minsoo, et al. \u0026ldquo;vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design.\u0026rdquo; 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture(MICRO), IEEE, 2016.\n[22] Chen, Tianqi, et al. \u0026ldquo;Training deep nets with sublinear memory cost.\u0026rdquo; arXiv preprint, 2016.\n[23] Micikevicius, Paulius, et al. \u0026ldquo;Mixed precision training.\u0026rdquo; arXiv preprint, 2017.\n[24] Jain, Animesh, et al. \u0026ldquo;Gist: Efficient data encoding for deep neural network training.\u0026rdquo; 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture(ISCA), IEEE, 2018.\n[25] Kingma, Diederik P., and Jimmy Ba. \u0026ldquo;Adam: A method for stochastic optimization.\u0026rdquo; arXiv preprint, 2014.\n[26] Shazeer, Noam, and Mitchell Stern. \u0026ldquo;Adafactor: Adaptive learning rates with sublinear memory cost.\u0026rdquo; International Conference on Machine Learning, PMLR, 2018.\n[27] Ginsburg, Boris, et al. \u0026ldquo;Stochastic gradient methods with layer-wise adaptive moments for training of deep networks.\u0026rdquo; arXiv preprint, 2019.\n[28] Hu, Edward J., et al. \u0026ldquo;LoRA: Low-rank adaptation of large language models.\u0026rdquo; ICLR, 2022: 3.\n[29] Dettmers, Tim, et al. \u0026ldquo;Qlora: Efficient finetuning of quantized llms.\u0026rdquo; Advances in Neural Information Processing Systems 36, 2023: 10088–10115.\n[30] Weng, Lilian. \u0026ldquo;How to Train Really Large Models on Many GPUs?\u0026rdquo; Lil\u0026rsquo;blog, 2021.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui.(Mar 2025). 训练大模型并行和内存优化技术. https://syhya.github.io/zh/posts/2025-03-01-train-llm\nOr\n@article{syhya2025train-llm, title = \u0026#34;训练大模型并行和内存优化技术\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Mar\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-03-01-train-llm\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-03-01-train-llm/","summary":"\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\n\u003cp\u003e最近大模型的参数数量不断攀升，从最初的数十亿扩展到如今数千亿乃至数万亿级别。大模模型虽然带来了前所未有的应用效果，但与此同时，也引发了计算资源、内存管理和训练稳定性等一系列严峻挑战。因此本博客总结了一些常用分布式并行训练和内存管理技术，希望能够帮助大家更好地训练和优化大模型。\u003c/p\u003e","title":"训练大模型并行和内存优化技术"},{"content":"OpenAI Deep Research (OpenAI, 2025) 主要是采用以 o3 推理模型为基座的多 agent 协作方式。相关开源研究和产品之前也有不少，比如 GPT Researcher 和 Google Gemini 版本的 Deep Research (DeepMind, 2024)。不过 OpenAI 的产品是博主目前体验下来效果最好。之后咨询分析、论文综述和新闻编辑等任务都可以先让这个工具自动化完成，然后再由人工进行最后的修正和审查，怎么感觉之后我写技术博客的任务也会被替代了呢？😂\n此外，另一个 OpenAI 近期发布的 agent 产品是 Operator (OpenAI, 2025)，这是一个类似 WebVoyager (He, et al, 2024) 的工具，能够让 agent 在浏览器上进行交互操作，并且操作过程中如果有错误，可以人为进行修正。\n注意：下面内容是由 OpenAI Deep Research 生成的研究案例 😊，模型会有一定幻觉和格式上的错误，仅供参考。\n原文链接: 深度研究技术综合研究报告\n感谢您的详细说明！我将进行深入研究，以撰写一份关于“深度研究”技术的综合报告，主要聚焦于 OpenAI 的 Deep Research，同时涵盖 Google Gemini Deep Research 等类似技术。\n该报告将：\n详细探讨 OpenAI o3 模型的架构及其优化技术。 讨论深度研究所使用的 NLP、网络抓取、机器学习、语义嵌入等核心技术。 结合参考案例进行分析，比较 OpenAI Deep Research 与 Google Gemini Deep Research。 提供相关技术挑战、解决方案以及未来发展趋势。 采用论文格式，引用 arXiv、顶会论文、OpenAI 博客等英文来源。 我将尽快完成研究并为您提供详细报告。\n深度研究技术综合研究报告：OpenAI Deep Research 与 Google Gemini Deep Research 比较 1. 深度研究的技术概述 OpenAI Deep Research 及 O3 模型架构：“深度研究”（Deep Research）是 OpenAI 于 2025 年推出的一种新型自动化研究代理。它由即将发布的 OpenAI O3 模型的特制版本提供支持 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。O3 模型基于大型Transformer架构，具备多模态处理能力和超大参数规模，专为网页浏览和数据分析任务优化 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。该模型在设计上继承了 GPT-4 等前代模型的强大自然语言理解和生成能力，同时通过优化推理链和工具使用接口，能够像人类研究员一样自主执行多步骤研究任务。OpenAI 表示，用户只需提供研究课题提示，Deep Research 代理就会自动搜索、解析并综合上百个在线来源，产出一份全面、有引文的研究报告 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。这一过程据称在十分钟内完成，相当于人类需要数小时才能完成的工作 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central) (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。O3 模型还具备处理文本、图像和PDF等多种输入的能力，使其能分析各种格式的信息 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。架构上，O3 通过引入可微分的外部记忆模块和函数调用接口，强化了与工具和知识库交互的能力。这意味着模型可以将互联网视为扩展“记忆”，在生成答案时检索外部信息，从而缓解仅靠参数存储知识的局限 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)。总的来说，OpenAI Deep Research 基于 O3 模型实现了从问题理解、资料检索到信息综合的端到端自动化，为复杂研究任务提供了新的解决方案。\n网页浏览与数据分析优化技术：Deep Research 特别针对网络信息检索和数据分析进行了优化。首先，代理采用内置浏览器环境执行网页搜索和爬取。这类似于早期的 WebGPT 实验，模型在一个文本浏览环境中可以发出搜索查询、点击链接并读取网页内容 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。与普通搜索引擎返回结果摘要不同，Deep Research 逐页提取原始资料并深入分析。为提高检索效率，系统可能使用查询优化和并行爬取技术：例如根据用户课题生成一系列相关查询词，批量检索多条结果；同时利用多线程抓取多个网页，以减少等待时间。抓取过程中，代理运用了HTML解析和内容过滤策略，自动跳过无关的导航、广告等元素，只保留正文信息进行分析。这种内容提取过程可能结合启发式规则和模型判断，以提炼出每篇文章中与课题相关的段落。\n在数据分析方面，Deep Research 内建了轻量级的数据处理和统计功能。例如，当课题涉及数值数据（如市场份额、增长率等）时，代理可以调用嵌入的 Python 运行环境对收集的数据进行计算、制表和绘图 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。Techopedia 报道指出，OpenAI Deep Research 支持 Python 集成，可以即时生成图表并执行数值计算，以便对趋势进行分析 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。这种能力类似于 ChatGPT 的“代码解释器”工具，允许模型在沙盒中运行代码，对数据做进一步处理，然后将结果纳入报告。因此，在浏览器抓取到包含表格或统计数据的内容后，代理能够编写小段代码计算同比变化、绘制趋势图等，从而更深入地挖掘数据背后的模式。这些优化技术使 Deep Research 在处理涉及大量数据的课题时，既能保证对源信息的全面覆盖，又能进行适当的加工和验证。\n推理机制与自主研究流程：OpenAI Deep Research 的核心是一套逐步推理 (step-by-step reasoning) 机制，指导代理完成多阶段的研究流程 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。这一机制借鉴了学术界提出的 ReAct (Reason+Act) 框架 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。具体来说，模型在每个迭代中都会交替地产生“推理轨迹”和“动作指令” ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。推理轨迹是模型内在的思考过程，例如分析当前有哪些信息缺失、下一步该搜集什么；动作指令则是可执行的操作，如访问搜索引擎查询、打开特定链接、调用工具函数等。通过在推理和操作之间交替，模型能够一边思考一边获取新信息，从而逐步逼近完整答案 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。这一过程类似人类研究员在调研时的工作流：先根据问题分解子问题，搜索文献获取答案片段，再整合验证。Deep Research 会根据初始提示自动提出澄清性子问题或假设，然后针对每个子问题检索证据；若发现某路径结果不满足要求，还会动态调整计划（backtracking），尝试其他信息源 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。例如，OpenAI的代理在开始正式搜索前，会先根据用户提示向用户提问以澄清需求，明确研究范围和衡量指标 (OpenAI Deep Research vs Gemini Deep Research)。接着，它按照“数据提取-\u0026gt;数据验证-\u0026gt;注释标记-\u0026gt;趋势分析”的顺序迭代推进 (OpenAI Deep Research vs Gemini Deep Research)。这种多轮推理使代理具有高度的自主性和灵活性，能够处理复杂的课题。研究表明，此类推理-行动交替策略有助于减少大语言模型的幻觉和错误传播，因为模型会及时用检索到的真实信息来校正和支撑其推理 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。\n值得注意的是，OpenAI Deep Research 强调透明的推理过程。在用户界面上，代理的每一步行动和背后的理由都会实时展示给用户 (OpenAI Deep Research vs Gemini Deep Research)。例如，它会显示当前正在搜索什么关键词，找到了哪些来源，以及选择某个来源的原因。这种透明度一方面增加了结果的可信度，另一方面也方便用户介入指导。如果模型误解了需求，用户可以在中途纠正。相比之下，我们将看到 Google 的方案在这点上有所不同。这种以推理为驱动、结合工具行动的机制，使 Deep Research 能够像人类一样“深度”研究：不仅检索大量信息，还能反思信息质量、交叉验证事实，并不断调整策略直至得到可靠、详尽的结论 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。\n2. 使用的技术 OpenAI Deep Research 和 Google Gemini Deep Research 都融合了多种人工智能与数据处理技术，以实现自主的深入研究。下面分几个方面讨论它们使用的关键技术：\n大规模自然语言处理模型：两家的深度研究代理均由最先进的大型语言模型 (LLM) 提供智能支持。OpenAI 的 O3 模型和 Google 的 Gemini 模型都是数千亿到万亿参数级别的Transformer网络，经过海量语料预训练，具备强大的自然语言理解和生成能力 (Gemini (language model) - Wikipedia)。这些模型可以精准地理解复杂研究请求，生成结构良好的长篇回答。此外，它们都支持多模态输入。O3 模型可接收文本、图像、PDF、表格等作为上下文 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)；Gemini 模型则从一开始就以多模态训练，能够统一处理文本、图像、音频、视频等多种数据 (Introducing Gemini: Google’s most capable AI model yet) (Introducing Gemini: Google’s most capable AI model yet)。例如，用户可以上传学术论文PDF或统计图表图片，OpenAI Deep Research 能直接解析其中内容并纳入分析 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。Google Gemini 在设计上也是原生多模态 (natively multimodal)的——它并非将不同模态的子模型简单拼接，而是在预训练时就让模型同时学习图像像素、音频频谱和文本序列，从而能够跨模态进行推理 (Introducing Gemini: Google’s most capable AI model yet) (Introducing Gemini: Google’s most capable AI model yet)。这使 Gemini 在多模态推理任务上表现出色，例如解读复杂图文信息并据此发现隐藏的知识 (Introducing Gemini: Google’s most capable AI model yet)。在语言处理方面，模型采用链式思维 (Chain-of-Thought) 等提示技术增强推理能力 (GPT-4 Technical Report) (GPT-4 Technical Report)。OpenAI 在 GPT-4 技术报告中指出，通过 few-shot 提示示例引导模型执行逐步推理，可以显著提升复杂任务表现 (GPT-4 Technical Report)。总之，深度研究代理借助最尖端的大模型作为大脑，支撑其理解任务、生成详实报告的能力。\n网络爬取与数据提取：为了获取所需的最新资料，这些代理都集成了网页搜索和爬取技术。OpenAI Deep Research 通过 ChatGPT 插件式的浏览器接口或内部工具实现自动网页浏览。模型能够构造搜索引擎查询，从返回结果中筛选有用链接并抓取页面内容 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。爬取过程结合了自然语言解析和HTML处理：代理或许使用预置的正则规则、DOM解析库，或直接利用语言模型本身来识别网页中的主要内容段落。例如，它可能让模型总结网页内容，以快速判断该页是否相关，然后决定是否深入阅读。对于半结构化的数据源（如维基百科、政府统计表格等），代理可以使用定制的爬虫或API获取结构化数据。Google Gemini Deep Research 则直接依托 Google 强大的搜索基础设施进行信息获取 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。它利用 Google 实时索引的海量网页，使得检索极为迅速且覆盖全面 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。代理会自动从搜索结果中爬取必要网页，并将内容输入其语言模型进行处理。在数据提取上，模型可能采取逐段阅读和语义过滤相结合的方法：将长篇文章按段落分割，利用语义嵌入或内容摘要来挑选出与查询相关的段落重点阅读 (Deep Chat JS, semantic search, and OpenAI integration) (Deep Chat JS, semantic search, and OpenAI integration)。这样可以避免将整篇无关内容塞入上下文，节省token空间和计算。深度研究还善于信息抽取，比如从新闻或论文中定位人名、时间、数值等关键实体，并整理到笔记中。对于需要的数据（如统计指标），代理可能搜索官方数据库或报告，并爬取表格数据。然后通过编程或内置函数将HTML表格转换为可用的数值。整个网络爬取与提取过程在幕后高度自动化，确保代理获得高质量、多样化的信息供后续分析使用。\n特定算法与人工智能技术：深度研究代理的研发融合了多项前沿 AI 技术和算法。首先，上文提到的 ReAct 推理+行动框架就是一个关键算法思想 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。通过在模型推理过程中引入决策动作，ReAct使模型可以与外部环境交互，从而获取新信息来支撑后续推理。这有效降低了纯静态推理时的幻觉风险，在OpenAI和学界的评测中被证明能显著提高问答的事实准确性 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。其次，基于工具的强化学习也是相关技术之一。OpenAI 的代理很可能经过专门的微调训练，使其善于调用搜索、浏览、代码执行等函数。例如，Meta AI 提出的 Toolformer 方法表明，可以通过在训练数据中加入工具使用示例，让模型学会何时调用哪个API以及如何利用返回结果 (Toolformer: Language Models Can Teach Themselves to Use Tools)。Deep Research 可能应用了类似思想，模型在遇到知识空缺时会主动发出搜索动作，而非凭空编造答案。\n另外，为了提高生成内容的可靠性，这些模型还经过人类反馈强化学习 (RLHF) 调优。OpenAI 早在 WebGPT 项目中就使用人类偏好反馈来训练模型给出有引用支撑的答案 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。Deep Research 代理的回答需要合乎学术标准，引用真实来源，因此推断其训练过程中也融入了“回答必须有引用依据”的奖惩机制 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。模型如引用了不存在的文献将被扣分，从而学会去引用真实检索到的资料。这与 WebGPT 在ELI5问答上的做法相似：模型需在回答中附上证据来源，由人类评审其答案是否充分且来源可靠 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。Google Gemini Deep Research 则可能借鉴 AlphaGo 式的强化学习来增强规划能力。DeepMind CEO Demis Hassabis 曾表示，Gemini 结合了 AlphaGo 的强项和大型语言模型的能力 (Gemini (language model) - Wikipedia)。AlphaGo 等强化学习算法善于长序决策和规划，Gemini 因此可能在研究任务上加入了类似MCTS（蒙特卡洛树搜索）或策略梯度方法，使其能预先规划研究步骤。事实上，Gemini代理在开始执行前会生成一份研究计划供用户审核 (OpenAI Deep Research vs Gemini Deep Research) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)，这体现了一种规划算法的应用。总结来说，深度研究代理并非单靠一个语言模型完成任务，而是整合了检索、阅读、计算、规划等多个模块，各模块由相应算法驱动：检索模块用搜索引擎算法，阅读理解由NLP模型完成，计算分析借助嵌入的编程环境，规划决策通过强化学习和提示工程来实现。这种多技术融合使代理具备了“全栈”能力，从而胜任复杂的研究流程。\n向量数据库与语义嵌入：在信息处理过程中，深度研究代理大量运用了语义嵌入 (semantic embedding)技术和向量数据库来高效管理知识。语义嵌入是将文本转化为高维向量的方法，使语义上相似的文本在向量空间中距离更近 (Deep Chat JS, semantic search, and OpenAI integration) (Deep Chat JS, semantic search, and OpenAI integration)。无论是 OpenAI 还是 Google 的系统，都可能在后台构建了一个向量索引：当代理抓取了几十份相关文档后，它可以将这些文档片段转成向量存储在内存或临时数据库中。随后，当代理需要某方面的信息时，不必全文逐页翻找，而是通过向量相似度检索快速定位最相关的片段 (Deep Chat JS, semantic search, and OpenAI integration)。例如，代理在分析一个话题的不同侧面时，可以针对每个子课题将已收集资料向量化，然后对用户的新提问或下一个推理点生成查询向量，在向量数据库中检索Top K相近的段落 (Deep Chat JS, semantic search, and OpenAI integration)。这种方式大大加快了信息检索速度，并减少重复爬取。同样，向量数据库还有助于融合多来源信息：模型可将来自不同网站的相似内容聚类，以发现共识或差异。这也有利于可靠性分析——如果多个嵌入相近的段落来自不同权威来源，模型会更信任其中信息。OpenAI 的嵌入模型（如 text-embedding-ada 系列）以及向量索引库（如 FAISS、Qdrant）为此提供了支持 (Deep Chat JS, semantic search, and OpenAI integration)。实际应用中，开发者经常采用检索增强生成 (Retrieval-Augmented Generation, RAG)框架 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)：先用嵌入检索知识，再把相关内容喂给LLM生成回答。Deep Research 正是RAG思想的高级实践——自动完成了检索和生成两个步骤。在这一过程中，Top-K最近邻搜索算法被用来从数以百计的向量中选出最相关的若干段落 (Deep Chat JS, semantic search, and OpenAI integration)。这种最近邻搜索通常基于高效的数据结构（如HNSW图），可以在子毫秒级返回结果，即使向量库很大也足够快。另外，语义嵌入还能用于结果去重和多样性：如果不同搜索查询返回的结果在向量空间非常接近，说明内容高度相似，可以避免重复阅读同质信息，从而将精力用于补充新的视角。综上，语义嵌入和向量数据库是深度研究系统的信息“记忆”和检索引擎，支撑了其对海量资料的高效利用。\n可扩展性与性能优化：深度研究代理为了处理大规模数据和长时任务，采用了一系列性能优化策略来保证可扩展性。首先，并行处理是提高效率的主要手段之一。OpenAI Deep Research 在多步骤流程中可能会并行执行某些独立任务。例如，在初始检索阶段，代理可以同时发出多个不同的搜索查询（针对课题的不同子问题） (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)；对于获得的一批网页，也可以并发抓取和粗略摘要，再由主模型逐一精读分析。这种并行爬取和预处理能够充分利用多核CPU和分布式计算资源，减少等待瓶颈。Google 的系统在这方面更具优势，借助其强大的云计算和搜索架构，可以在短短几分钟内抓取并分析几十个来源 (OpenAI Deep Research vs Gemini Deep Research)。其次，逐层细化策略提高了处理长文档的能力。代理不会尝试一次性将所有文本都放入上下文，而是分层摘要：先用一个较小的上下文窗口获取文档要点，再根据需要细读其中相关段落。这有点类似分页读取，加上向量检索筛选相关页的手段，使其能够扩展处理超出单次上下文长度的内容。此外，模型可能利用分段内存或检查点机制来保持长程信息：例如，将中间结论和已验证的事实存入一个临时内存（可能也是以嵌入形式），在后续步骤通过检索引用，而不必把早期所有内容一直携带在prompt中。这种外部化记忆可以看作扩大了有效上下文。\n在生成长篇报告时，模型也应用了分段生成策略，以确保连贯和效率。它可能先生成报告的大纲（包括章节结构），然后逐段填充具体内容。每一章节完成后，再结合下一章节需要的信息进行衔接。这样做可以将一次生成拆分成多次，小段落生成更快且可控。同时，代理注重源引用缓存：当某段内容已经找到了满意的来源并引用，后面若再次需要相似信息，会直接调用之前缓存的出处以避免重复查询。所有这些措施令深度研究代理能够在5-30分钟内完成一个复杂课题的全面研究 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (OpenAI Deep Research vs Gemini Deep Research)。Google Gemini Deep Research 通常更快，5-15分钟即可完成，因为其流程较为固定且充分利用了Google现有的搜索速度优势 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。最后，从基础架构看，OpenAI 为专业用户提供的 Deep Research 服务限制每月约100次查询、每次最长30分钟 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。这样的限制也是出于可扩展性考虑，避免单个用户长时间占用计算资源，从而服务更多用户。在未来，随着模型和硬件效率提升，这些时间和次数限制有望逐步放宽，使深度研究技术以更大规模应用。\n3. 参考项目和实现 **OpenAI Deep Research 与 Google Gemini Deep Research 比较：**OpenAI 和 Google 都推出了各自的深度研究代理，它们在目标功能上相似，但在实现细节和用户体验上存在显著差异。以下从多个方面对比两者的特点：\n模型与多模态能力：OpenAI Deep Research 基于 OpenAI O3 模型，支持文本、图片、PDF、表格等多模态输入，甚至可以处理用户上传的复杂数据 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。它还能在报告中嵌入生成的图表、可视化等，属于真正的多模态研究助手 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。Google Gemini Deep Research 则以 Google Gemini 2.0 模型为核心 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。Gemini 本身是多模态模型，但当前 Gemini Deep Research 服务主要面对文本查询，输入以文字为主 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。输出上，OpenAI 报告包含引用、摘要和嵌入式可视化，而 Google 输出的是带关键发现和链接的文本报告 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。不过，Google 报告常附有结构良好的数据表格，并提供导出到 Sheets 的选项 (OpenAI Deep Research vs Gemini Deep Research)。总体而言，OpenAI 的多模态处理和输出呈现更丰富；Google 则注重与其办公套件集成，实现结构化结果。\n研究方法与流程：OpenAI Deep Research 强调自主的多步迭代。代理会像研究员一样动态调整搜索和分析策略，过程完全透明地展示给用户 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。例如，它先提出澄清性问题，得到用户确认后再开始正式研究 (OpenAI Deep Research vs Gemini Deep Research)；在研究过程中，实时显示当前执行的步骤（搜索关键词、读取某站点等）及初步发现，并可能绘制中间的分析图表 (OpenAI Deep Research vs Gemini Deep Research)。整个流程高度交互且时间投入较大 (5-30分钟) (OpenAI Deep Research vs Gemini Deep Research)。相比之下，Google Gemini Deep Research 采用预先规划的结构化方法。它一开始会生成一份研究计划，列出要采取的步骤（如“首先搜索X，然后分析Y，最后撰写报告”），让用户可以在执行前修改这个计划 (OpenAI Deep Research vs Gemini Deep Research) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。然后代理按照预定流程执行，各步骤对用户而言是静默进行的，中途不再像OpenAI那样持续展示推理过程 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。这种方法优点是效率高且流程可预测：用户知道大概要花5-15分钟，得到的是规范成文的报告 (OpenAI Deep Research vs Gemini Deep Research)。但缺点在于缺少中途的灵活调整——若检索信息不充分，Gemini 可能不会像OpenAI那样自行回溯增补 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。因此，OpenAI 的方法更适合需要深挖和探索的课题，Google 的方法适合快速获取结构化结论的场景 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。\n透明性与可控性：OpenAI代理在可解释性上更胜一筹。用户可以实时看到模型的“思考”，包括每一步为什么这么做，以及数据从何而来 (OpenAI Deep Research vs Gemini Deep Research)。这种透明度使用户对过程和结果更有信心，也方便在过程中介入指导（例如提供新的线索或纠正方向）。Google代理则过程不透明：除了一开始的计划，执行中只会显示正在阅读的网站列表 (OpenAI Deep Research vs Gemini Deep Research)，用户并不知道模型具体如何判断信息质量。这使得Google代理在使用中更像一个“黑箱”，只能通过结果来评估过程。如果结果有遗漏或偏差，用户需要另行手动研究补充。为提高可控性，Google允许用户在开始前编辑它的研究计划，相当于高层次地约束其行为 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)；而OpenAI的用户则是低层次地观察每步、随时反馈。这体现了两者在产品设计上的不同哲学：OpenAI追求人机协同，Google侧重自动完成任务。\n结果深度与呈现：在报告输出方面，OpenAI Deep Research 倾向于提供深入的分析和洞见，即使引用来源相对较少也无妨 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。它会给出背景概述、不同角度的讨论、历史趋势分析，甚至根据数据得出战略性建议 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。例如，在移动通信普及率的案例研究中，OpenAI代理不仅列出了各国的数据对比，还标注了历史趋势线和区域差异，并结合数据给出了市场拓展建议 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。Google Gemini Deep Research 则注重覆盖更广泛的资料来源并提炼关键信息 (OpenAI Deep Research vs Gemini Deep Research)。它可能从更多的网站获取数据（一次研究中来源数可能与OpenAI相当甚至更多），然后将要点综合到一份井井有条的报告中 (OpenAI Deep Research vs Gemini Deep Research)。Google的报告通常包含明确的**“研究方法”说明**（解释搜集了哪些来源，如何确保质量）和数据汇总表 (OpenAI Deep Research vs Gemini Deep Research)。这些表格按指标列出多实体对比，使读者一目了然 (OpenAI Deep Research vs Gemini Deep Research)。同时每段末尾附有该段所用来源的列表 (OpenAI Deep Research vs Gemini Deep Research)。相比之下，OpenAI的报告是在句中直接插入引用标记（如使用【†】形式），并在旁边列出所有来源，指向精确的证据 (OpenAI Deep Research vs Gemini Deep Research)。这使OpenAI报告的引证粒度更细，读者可以精确定位每句话的出处。而Google报告虽也详列引用但未逐句标注 (OpenAI Deep Research vs Gemini Deep Research)，需要读者自行对照内容和参考文献。总结而言，OpenAI输出偏重深度解析和实时交互（带图表、交互注释等），Google输出偏重正式文档化（带完整章节和表格） (OpenAI Deep Research vs Gemini Deep Research)。前者适合希望理解过程和细节的决策者，后者适合需要成品报告做决策的经理或学生 (OpenAI Deep Research vs Gemini Deep Research)。\n成本与定位：OpenAI Deep Research 当前仅向 ChatGPT Pro 专业版用户开放，订阅费用约为每月 200 美元，限制使用次数 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。定位于金融、政策、学术等专业人士，他们有高价值需求，愿意为深度分析付费 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。Google Gemini Deep Research 则包含在较大众的 Google One AI 增强套餐中，每月只需20美元即可使用（Gemini Advanced计划） (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。因此它吸引的是学生、创业者和普通研究者等更广泛的用户群，他们可能不需要极致深度但追求性价比和速度 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。价格上的巨大差异（10倍）也反映了两家公司产品策略：OpenAI走高端专业路线，Google走普惠大众路线。在资源投入上，OpenAI每次查询耗费大量计算（长时间、多步骤），Google每次则受益于已有搜索基础设施较为轻量。服务形式上，OpenAI集成在ChatGPT界面中，以按钮形式启用 Deep Research 模式 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)；Google则可能通过 Bard 的“Gemini 1.5 Pro 深度研究模式”提供，在 Bard 界面或Docs侧边栏中开启深度研究功能 (ChatGPT\u0026rsquo;s Deep Research vs. Google\u0026rsquo;s Gemini 1.5 Pro with Deep \u0026hellip;)。因此，OpenAI更像一个独立研究助手，而Google更像常规搜索/聊天的扩展。\n综上，OpenAI Deep Research 与 Google Gemini Deep Research 各有优势：前者在推理深度和过程透明上更强，适合需要信任和细节的专业应用；后者在速度、集成和成本上更有优势，更贴近一般信息查询的升级版 (OpenAI Deep Research vs Gemini Deep Research)。选择哪种取决于用户需求：若追求交互式深入洞察，OpenAI是值得的投入；若偏好快速得到结构化结果且预算有限，Google方案是不錯的选择 (OpenAI Deep Research vs Gemini Deep Research)。\n相关 OpenAI 项目：OpenAI Deep Research 的开发并非孤立无援，它建立在此前一系列OpenAI项目的基础上，也和其他同期项目并行发展。一个直接的先驱是 2021年的WebGPT实验 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。WebGPT 将GPT-3模型与一个文本浏览器环境相结合，通过模仿人类上网找答案并辅以人类反馈训练，证明了语言模型可以执行浏览器查询、阅读网页、引用来源来回答问题 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。Deep Research 明显继承了 WebGPT 的理念，将其拓展为通用的多步研究。另一个相关项目是 OpenAI 2023年开始在ChatGPT中提供的插件功能，尤其是浏览插件和代码运行器。这些插件让模型可以调用外部工具，访问网络或执行代码，与Deep Research的能力有相似之处。可以认为Deep Research将这些插件的功能整合为一个统一的代理，使之更自动化和面向特定任务。\n2023年中，OpenAI的GPT-4模型开放了函数调用 (Function Calling) 接口，允许开发者定义工具供模型调用 (Function calling and other API updates - OpenAI)。Deep Research 很可能利用了函数调用机制来执行搜索和抓取等动作：即定义诸如 search(query)、open_url(url)、run_python(code) 等函数，由模型在对话中决定调用，从而实现与外部环境交互。这种架构极大增强了模型处理复杂任务的能力，也是Deep Research技术栈的一部分。\n除了研究任务本身，OpenAI还在推进其他自主代理。例如OpenAI Operator是2025年1月推出的另一款代理，用于控制计算机执行各类任务 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。Operator 可看作是面向操作系统/软件操作的自动化助手，而 Deep Research 面向知识获取领域。两者都属于OpenAI探索通用代理 (Agent) 的一环，被称为一系列“推理模型 (reasoning models)”中的成员 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。员工透露OpenAI已有 O1、O2、O3 等不同代号的推理模型，不断提升推理和自主能力 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。可以预见，这些项目之间会有技术和经验的共享，例如Operator在规划多步骤任务上的经验可反哺Deep Research。\n值得一提的还有一些社区驱动的项目，如2023年流行的Auto-GPT和BabyAGI等开源自主代理。这些项目尝试让GPT-4自动分解目标、连续调用自身来完成复杂任务，被视为通往 AGI 的尝试。虽然不是OpenAI官方产品，但OpenAI显然从中获得了启发。Deep Research 更加稳健和有针对性，相当于一个受控的“AutoGPT”用于研究领域，避免了无目的循环。但理念上都在探索让AI 自主连续地工作。因此，Deep Research 可以看作 OpenAI 在专业受控环境下实现自主 AI的实例。这一项目与上述OpenAI插件、Operator代理一起，构成了OpenAI AGI研究路线的重要布局 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。\n技术案例研究：实际应用示例\n为了更直观地了解深度研究代理如何运作，这里提供一个实际案例，比较OpenAI和Google代理的表现：\n案例: 移动互联网渗透率与语言学习市场分析。假设我们希望研究“过去5年中前10个发达国家和前10个发展中国家的移动电话普及率变化，并比较iOS/Android的使用率，以及各国有多少人口有意愿学习另一种语言”，最终产出一份报告并给出在哪些市场推出语言学习App机会最大 (OpenAI Deep Research vs Gemini Deep Research)。\n**OpenAI Deep Research 执行：代理首先与用户交互，提出澄清问题：“这里的‘渗透率’是指占总人口的百分比吗？语言学习意愿是否有数据来源？”用户确认渗透率按百分比，并希望它自行假设缺失数据。 (OpenAI Deep Research vs Gemini Deep Research)得到明确需求后，代理开始多步研究。它将任务拆解为几部分：1）搜集各国历年移动普及率数据，2）获取各国iOS与Android市场份额，3）调查各国居民学习第二语言的意愿数据，4）综合分析并给出建议。代理首先查询世界银行或ITU统计报告获取移动普及率，并找到过去5年的数据表；随后搜索移动操作系统市场份额的报告，得到前十大国家OS占比的数据；再搜索教育或调查报告，找到各国关于语言学习意向的调查结果。每一步搜索后，代理都会验证数据可靠性，例如对比多个来源的数值是否一致，并标注来源 (OpenAI Deep Research vs Gemini Deep Research)。在分析阶段，OpenAI代理使用Python计算各国渗透率5年增长率，绘制趋势图（这些图在生成报告时会转换成Markdown格式的ASCII图表或描述） (OpenAI Deep Research vs Gemini Deep Research)。它还对发达国家与发展中国家进行了组间对比，发现发展中国家总体增速更快。接着，代理将所有信息组织成报告：开头是对移动渗透和语言学习的背景概述，其次按发达和发展中两组分别讨论数据，插入一张对比表格列出每个国家的当前渗透率、iOS占比、Android占比和语言学习意愿百分比。报告还包括“研究方法”**小节，说明数据来源和可靠性（这一点是Google报告默认包含的，但OpenAI代理为了专业性也会生成说明，以获得用户信任） (OpenAI Deep Research vs Gemini Deep Research)。在结论和建议部分，OpenAI代理综合数据指出：一些发展中大国（例如印度、巴西）移动普及率虽快速提升但仍有提升空间，且语言学习意愿高，而这些国家Android占比远超iOS；因此建议如果推出语言学习App，应优先面向Android平台并针对这些市场 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。整个过程中，OpenAI代理共引用了大约30个来源，每个关键数据后的括号中标注了来源编号，对应参考文献列表，方便读者核查 (OpenAI Deep Research vs Gemini Deep Research)。完成时间约12分钟，得到一份详细、图文并茂且具有战略见解的报告 (OpenAI Deep Research vs Gemini Deep Research)。\nGoogle Gemini Deep Research 执行：对于相同的课题，Google代理在用户提交查询后，首先自动生成了研究计划，例如：“计划：1）使用Google搜索获取20国移动渗透率数据，2）获取iOS/Android占比数据，3）获取语言学习意向数据，4）整理分析并生成报告。”用户可以编辑这个计划（本例中用户接受计划，但为了公平将OpenAI澄清的问题加入提示） (OpenAI Deep Research vs Gemini Deep Research)。然后代理开始执行而无需进一步提问。它利用Google强大的搜索直接获取到了相关数据来源列表（可能包括世界银行数据库、StatCounter网站、EF英语能力报告等）。由于速度较快，约6分钟后，代理完成了信息收集和分析 (OpenAI Deep Research vs Gemini Deep Research)。Google代理将输出发送到Google文档作为最终报告。报告结构清晰，包含引言、方法、分析和结论等部分 (OpenAI Deep Research vs Gemini Deep Research)。其中“研究方法”部分详细说明了数据来自何处，例如引用了ITU报告2024版、StatCounter 2023年统计、EF教育报告等，并陈述已交叉核对主要指标 (OpenAI Deep Research vs Gemini Deep Research)。报告的分析部分以段落叙述各国数据趋势，穿插表格比较。例如，有一个表格列出了“发达 vs 发展中国家的移动渗透率（2018和2023年）、年均增长、iOS%和Android%、第二语言学习意愿%” (OpenAI Deep Research vs Gemini Deep Research)。这些数据都有脚注标明来源编号，每段末尾列出引用链接 (OpenAI Deep Research vs Gemini Deep Research)。不过，报告正文没有将单句与具体来源一一对应（这点不如OpenAI的细粒度引用透明）。Google代理的结论聚焦于关键发现：比如“发展中国家智能机渗透仍落后发达国家约20个百分点，但增长更快 (OpenAI Deep Research vs Gemini Deep Research)，Android主导新兴市场90%以上份额，同时这些国家有平均40%以上的人有学习新语言的意愿”，然后建议“针对印度尼西亚、尼日利亚等Android用户众多且语言学习需求高的国家制定市场策略”。整个报告行文流畅、风格正式，有如咨询公司白皮书。由于过程不可见，我们只能从结果推测，Google代理在内部也是先把数据存入临时结构（或许利用其知识图谱）然后生成。不过，它在结果中很强调来源可靠性，并无明显幻觉错误。执行时间比OpenAI快，大约6分钟，来源数量同样在25-30个左右 (OpenAI Deep Research vs Gemini Deep Research)。\n比较分析: 这个案例显示，OpenAI Deep Research 和 Google Gemini Deep Research都能胜任复杂多维的数据分析任务，但各有侧重。OpenAI代理提供了更深入的探索过程：通过不断提问和交互，确保对需求的理解万无一失，并在分析中加入了更多解释和可视化（如趋势图）。这使报告富有洞察力，但也更费时。Google代理则充分发挥工具和速度优势：借助搜索和表格，它迅速汇总信息并排版成报告，省去了交互时间。其结论虽中肯详实，但在洞察力和创新建议上可能略少一分“惊喜”，更像是中立地呈现事实。这契合它设计用于高效信息汇总的定位。值得注意的是，两者都体现了AI代理超越人力的效率：如OpenAI员工所说，Deep Research 在10分钟完成了传统上需要3小时的市场调研工作 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。这一效率提升对知识工作者具有革命意义。\n4. 关键研究论文和学术参考 深度研究代理结合了近年来多个前沿研究方向的成果。下面总结几篇奠基性和最新的研究工作，它们构成了该技术的学术基础：\nReAct: Synergizing Reasoning and Acting in Language Models (Yao et al., 2022) ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models) ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models) – 这项工作提出了令LLM同时生成推理轨迹和动作指令的ReAct框架。传统上，“链式思维”侧重让模型写出思考步骤，而“工具使用”是分开研究的。ReAct 将二者融合，模型一边内在推理一边外部行动（如查询知识库）。论文通过在问答和复杂决策任务上实验，证明ReAct不仅提高了模型准确性，还使模型行为更可解释、更可信 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。例如在开放域问答中，引入Wikipedia查询动作可以纠正模型原本不可靠的推断。ReAct 框架正是像OpenAI Deep Research这样代理系统的灵魂：让模型自己决定何时需要搜索或工具，从而动态地完成多步任务。这篇论文发表于2023年ICLR会议，引用了丰富的实验数据，奠定了LLM作为自主代理的理论基础。\nWebGPT: Browser-assisted Question-Answering with Human Feedback (Nakano et al., 2021) ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback) ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback) – OpenAI的研究团队在这篇论文中探索了让GPT-3模型学会上网找答案的方法。他们构建了一个文本浏览环境，模型可以输入搜索查询、点击链接阅读网页，并必须在回答中引用来源 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。通过模仿人类示范（行为克隆）和加入人类反馈奖励，WebGPT 模型在长文问答数据集ELI5上表现超越了人类回答的平均水平 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。这项工作最重要的贡献在于引入人类偏好优化使模型学会引用真 实来源，而非臆造答案 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。WebGPT展示了语言模型能用于实际的网络信息查询，并达到高可信度输出，对后续深度研究代理的开发具有里程碑意义。\nRetrieval-Augmented Generation (RAG) for Knowledge-Intensive NLP (Lewis et al., 2020) ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks) ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks) – 来自Facebook AI的研究者提出了RAG模型，将预训练语言模型与非参数知识库结合，用以解决知识密集型任务。论文指出，仅靠参数记忆知识的LM难以更新知识且缺乏溯源机制，而融合了检索模块的模型能动态获取最新信息并提供来源 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)。RAG框架使用一个神经检索器从Wikipedia向量索引中找出相关段落，再让生成模型以此为条件生成答案 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)。实验证明，在开放域问答等任务上，RAG比纯语言模型和传统检索+抽取方法取得更好效果 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)，并生成更具体、更真实的回答。这奠定了后来诸多系统的范式，包括Bing/GPT-4的检索增强模式以及深度研究代理的设计思路：知识检索 + 生成。RAG作为NeurIPS 2020论文，影响深远。\nGemini: A Family of Highly Capable Multimodal Models (Gemini团队, 2023) () () – 这是一份由Google DeepMind发布的技术报告，详细介绍了Gemini模型的架构与性能。报告指出，Gemini 1.0系列包括Ultra、Pro、Nano三个规模，旨在在跨模态领域取得突破 ()。Gemini使用了联合训练的方法，让模型同时学习文本和图像、音频、视频，从而拥有原生的跨模态理解和推理能力 ()。实验结果令人瞩目：Gemini Ultra在测试的32项基准任务中有30项达到新SOTA，在MMLU综合考试基准上首次达到人类专家水平（超过90%） () ()。这证明了大模型在多模态和推理上的新高度。对于深度研究，Gemini报告的意义在于展现了大模型多模态融合的潜力，以及在推理任务上赶超甚至部分超越GPT-4的实力，为Google Gemini Deep Research提供了强大的模型支撑。\nDemis Hassabis 对 Gemini 模型的展望 (Hassabis, 2023) (Gemini (language model) - Wikipedia) – 在一次接受《连线》(Wired)杂志采访时，DeepMind CEO Demis Hassabis 谈到了即将推出的Gemini模型。他强调Gemini将融合AlphaGo的能力与Google/DeepMind过往语言模型的优势，有望在能力上超越OpenAI的ChatGPT/GPT-4 (Gemini (language model) - Wikipedia)。AlphaGo等强化学习系统擅长长序规划和博弈推理，将这些思想融入Gemini意味着模型可能具有更强的决策规划和问题求解能力。这番言论虽非正式技术细节，但代表了业界对下一代通用智能模型的期许，即通过借鉴不同AI领域的顶尖成果来打造更全面的AI。事实也证明，在Gemini 2.0中Google引入了原生工具使用和规划功能，推动其成为“agentic”时代的基础 (Google introduces Gemini 2.0: A new AI model for the agentic era)。Hassabis的观点为理解Google Gemini Deep Research背后的理念提供了背景：那就是将强化学习、规划与LLM结合，实现代理式智能。\nGPT-4 Technical Report \u0026amp; System Card (OpenAI, 2023) (GPT-4 Technical Report) (GPT-4 Technical Report) – 虽然OpenAI对GPT-4模型的细节讳莫如深，但技术报告和系统卡片提供了关于安全和伦理的重要信息。其中提到，GPT-4在输出中仍存在各种偏见，OpenAI为缓解这些问题做出了许多努力，但彻底解决仍需时间 (GPT-4 Technical Report)。报告描述了OpenAI通过增加安全相关的RLHF数据和规则奖励模型，来降低有害或偏颇内容的产生，并希望模型默认行为能反映广大用户的价值观，同时允许在一定范围内定制 (GPT-4 Technical Report)。这些安全措施和价值对齐方法与深度研究息息相关。因为一个研究代理需要避免被网上充斥的偏见或错误信息误导，也要防止输出不当结论。OpenAI在GPT-4上的安全策略，如对抗性测试和模型辅助的安全审查，为Deep Research的开发提供了指导，确保代理在自动检索开放网络信息时不会一味采信不可靠来源，而是倾向权威数据 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。此外，OpenAI还强调了隐私、虚假信息等风险 (GPT-4 Technical Report)。这些考量在深度研究场景下体现为：对爬取到的数据进行可信度打分、过滤阴谋论或极端内容，以及在报告中明确引用来源增加透明度。GPT-4系统卡片和相关研究（如TruthfulQA测评等）为开发深度研究代理提供了伦理与安全准绳。\n其它相关研究：除了上述主要工作，许多研究为深度研究技术提供了支撑。例如，Stanford 大学2022年的 “Self-Ask” 方法探索了让模型自行提出子问题并用Google搜索寻找答案，从而逐步解决复杂问答 (Do Large Language Models with Reasoning and Acting Meet \u0026hellip; - arXiv)。这种思路与Deep Research代理的自我提问类似。Meta AI的 “Toolformer” (Schick et al., 2023) 则展示了模型可以通过少量有监督信号学会使用各种工具API (Toolformer: Language Models Can Teach Themselves to Use Tools)。这直接启发了深度研究代理调用多种工具（搜索、计算）的能力。此外，在长文档处理上，Google Research的模型如 LongT5、Transformer-XL 等关于扩展上下文和记忆的研究，也可能贡献了技术要素。LangChain 等开源框架在工业界提供了构建自主代理的范式，包含计划-执行-记忆模块划分，与Deep Research类似的流水线思路。学术界对知识可信度评估、事实验证的研究（如 Fever 数据集等）帮助代理判断网络信息真伪。总之，深度研究技术不是凭空出现的，而是站在众多NLP、IR（信息检索）、RL（强化学习）工作的肩膀上，集大成地将这些成果应用于自动化研究这一创新场景。\n5. 技术挑战和解决方案 尽管深度研究代理表现出令人惊叹的能力，但在实现过程中也面临诸多技术挑战。下面讨论一些关键挑战以及采用的应对策略：\n数据质量保证与可靠性：互联网信息良莠不齐，如何确保代理引用的数据准确可信是首要挑战。为此，Deep Research 代理实现了多层面的质量控制。首先在检索阶段，它倾向于权威来源。OpenAI代理会优先选择学术论文、官方统计、知名媒体等作为信息来源 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。模型可以根据域名特征（如“.edu”, “.gov”）或站点知名度来判断可信度。此外，代理常常交叉验证关键事实：如果某数据只在一个来源出现，模型会尝试寻找第二个来源印证，避免孤证。第二，在引用过程中，模型只引用它实际看过并解析过的资料，这通过训练和系统约束保证。例如WebGPT的方法，让模型必须提供引用才能得高分 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)，因此Deep Research也会自发去检索能够佐证其断言的页面，没有依据的断言会被模型自身识别为不可靠而避免输出。第三，系统针对常见错误信息有过滤机制。OpenAI可能内置了一个“黑名单”或使用大型语言模型自身来评估内容的真实性。一些明显谣言或虚假医学信息，模型通过大概率识别并不予采用。这类似于OpenAI在GPT-4安全系统中所做的，对训练和生成进行约束以减少传播错误信息 (GPT-4 Technical Report)。最后，用户在环路中仍扮演重要角色：深度研究报告提供详尽引用，使用户能够方便地自行验证。如果某条信息看起来可疑，用户可以一键打开引用链接核查原文。这种可验证性确保了即使AI遗漏了不实信息，用户也有机会识别并纠正。因此，多管齐下的措施提升了代理输出的可靠性。当然，这并非万无一失，仍需警惕模型可能的幻觉引用（引错来源）或来源本身的不准确。为进一步改进，可能引入专门的事实核查模型对代理草稿进行审核，在最终输出前纠错。\n大规模数据处理的可扩展性：面对动辄数十上百份文档的研究任务，如何让代理高效处理且在硬件限制内运行是一大挑战。前面提到的一些并行和分层策略缓解了这个问题。然而，仍有必要考虑极端情况：若用户问题需要全网爬取大量资料（比如系统综述），模型上下文长度或调用次数都可能不堪重负。为此，OpenAI和Google采取了任务范围限制和资源调度策略。系统可能对每次Deep Research请求设定了隐含的资料检索上限（例如最多访问50个网页或检索1小时的内容）。一旦达到上限，代理会开始整理已有信息，即使还有未完全覆盖的角度，也优先产出一个可用报告。这保证了延迟不会无上限增长，也防止单次任务耗尽资源。另外，Google由于有自主搜索索引，许多查询的答案可能已被摘要存储，可以直接获取，而不必真正去访问远端站点（例如知识图谱或FAQ提供快速答案）。OpenAI则可能在自家域（如openai API文档、已有数据库）上构建了部分索引，提高常见知识的访问速度。流式处理也是方案之一：代理可以一边爬取一边输出初步结果，以流水线方式工作，而不是等所有数据拿到后才开始分析。不过当前Deep Research产品界面未体现流式输出，猜测其内部可能还是同步处理。为了扩展性，OpenAI正在探索Team版本（团队共享）和插件API，使多个用户可以并行使用代理而互不干扰。这需要优化后端服务架构，如对模型实例的复用和负载均衡。未来，随着更高性能模型（如O4）和更高效算法（如稀疏注意力）的出现，处理百篇文档甚至整站抓取将更为可行，深度研究代理的可扩展性也会水涨船高。\n数据隐私与安全、偏见和不当信息的处理：深度研究代理在上网搜索时，可能接触到涉及隐私或敏感的内容。同时，模型本身可能带有训练数据中的社会偏见。这些都是必须谨慎应对的伦理挑战。首先，在用户数据隐私方面，当用户提供自己的资料（如私密PDF）让代理分析时，OpenAI承诺不会将这些内容用于训练，并采取严格的访问控制，确保这些敏感数据仅在完成请求所需的短期内存在于内存 (Peer review of GPT-4 technical report and systems card - PMC)。如果代理需要访问需要登录验证的页面（如个人账户数据），当前设计中一般不会去抓取，以免越权。此外，OpenAI和Google都对深度研究代理加入了内容过滤模块。当模型检索到潜在不安全内容（暴力、色情、仇恨言论等），可能会触发过滤规则，中止该方向的研究并提醒用户。这类似于ChatGPT内置的审核系统，确保代理不会因为检索开放网络而变成不受控地输出违规内容。\n关于偏见缓解，正如GPT-4系统卡所述，OpenAI投入大量精力减少模型固有偏见 (GPT-4 Technical Report)。深度研究场景下，一个体现是：模型在汇报多个来源的信息时，会注意多元性和平衡性。例如在政策话题上，代理会尽量呈现不同观点来源，而非单方面引用，从而避免偏颇。对于模型可能产生的不当表述，OpenAI的RLHF和价值观对齐使其默认采用中性、客观的语气叙事 (GPT-4 Technical Report)。Google也有AI原则明确禁止模型输出歧视性内容。Gemini模型经过了Google迄今最全面的安全评估，在出厂时已过滤大量有害数据 (Introducing Gemini: Google’s most capable AI model yet)。因此，代理很少主动生成冒犯或偏激的语言。错误信息的伦理处理也是焦点：代理或许能访问到阴谋论网站、虚假新闻。如果模型不加选择地引用这些，将助长错误信息传播。应对上，除了之前提到的可信度判断，模型在报告措辞中也会保留谨慎。如果某重要数据只有存疑来源支持，模型可能会标注“有些来源声称X，但可靠性未知”，而不会把它当确定事实写出。这种不确定性的表达，实际上也是在承担研究伦理责任——让读者知晓信息可能不可靠。最后，深度研究代理和现有基础设施集成时，需要考虑安全接口：OpenAI在提供Deep Research功能给用户时，会设置调用频率和数据大小限制，以防被用于爬取敏感站点或进行钓鱼攻击。Google也只允许其代理检索公开网页而非深网私人信息。这些限制确保代理行为在既定法律和伦理框架内运作。\n与现有AI和数据基础设施的集成：深度研究代理要发挥最大价值，必须融入用户现有的工作流程和数据生态。这带来了集成方面的挑战和解决方案。对于Google而言，Gemini Deep Research已经与Google Docs、Sheets无缝衔接：用户可以一键将代理生成的报告保存为文档，或将数据表插入Sheets继续加工 (OpenAI Deep Research vs Gemini Deep Research)。这充分利用了用户熟悉的工具，降低采用门槛。OpenAI则在探索通过API接口让Deep Research接入企业内部知识库或应用。例如，一个公司可以将自有数据库（如销售数据、专利库）接入OpenAI插件系统，供Deep Research在调研时一并检索。这涉及将向量数据库技术部署在企业环境，并授权给OpenAI模型访问。由于有函数调用能力，模型可以通过安全网关查询企业API，在报告中结合内部数据和公开数据。这样的集成实现了专有信息 + 公开信息的融合研究，对企业决策非常有价值。当然，这也要求严格的身份验证和权限控制，防止公司敏感数据泄露给OpenAI或外部。OpenAI团队版或插件模式可能通过加密传输和隔离执行来保障这一点。\n此外，深度研究可与现有的数据分析管线集成。例如将Deep Research作为BI工具的上游：先由代理做定性研究和数据收集，然后将数据传给BI系统做深入建模。反过来，如果企业已有数据仓库，代理也可直接查询仓库获取最新数据点。技术上，这要求代理支持SQL查询或通过自然语言接口访问数据库。OpenAI近来的功能扩展（如数据库插件）表明这是可行的。Google的优势则是其搜索和知识图谱本身就是巨大的数据基础设施，Gemini代理天然集成了这些。当用户问到某公司财报数据，Gemini可能直接从其知识图谱提取结构化数据，而无需额外爬取。这种与知识图谱的结合也是一项集成，提升了效率和准确性。\n最后，在MLOps层面，深度研究代理需要与模型监控、日志系统集成。每次代理执行都产生大量过程数据，如何存储有用的日志供将来审计或改进？解决方案可能是将过程日志矢量化存储，日后训练新模型时用作示例（反馈学习）。也可以与现有A/B测试平台集成，通过比较代理结果与人工结果、或OpenAI vs Google结果，持续评估改进代理算法。总体而言，深度研究代理的集成性还在早期，但趋势是开放接口。OpenAI已表示计划让Deep Research面向Plus用户甚至开放API (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。届时，各行业就能把这一强大工具嵌入自己的数据和应用中，形成“AI研究助手无处不在”的新生态。\n6. 未来技术发展 随着深度研究技术的发展，我们可以预见它在未来的增强方向和潜在研究热点：\n即将发布的功能和增强：首先，在模型层面，OpenAI可能会推出更强大的推理模型（代号也许是O4或直接称为GPT-5），具备更长上下文、更高推理深度和更快速度。这将直接提升Deep Research的性能。例如，O3模型据传已支持数十万字节级的上下文，那么未来模型可能上下文扩展到百万字级，使代理一次能处理整本书或全站点数据，而无需分段 (Google Gemini Explained: How It Works \u0026amp; Where It Beats Every \u0026hellip;) ()。Google方面，Gemini 2.0在2024年末已上线，其升级版本（Gemini 3.0？）可能引入即时多模态输出，比如代理在报告中直接生成图表插图甚至配套音频讲解 (Google introduces Gemini 2.0: A new AI model for the agentic era)。Gemini 2.0已经具备原生图像和音频输出能力 (Google introduces Gemini 2.0: A new AI model for the agentic era)，因此深度研究的结果形式会更加丰富。\n其次，交互方面，OpenAI Deep Research 可能加入对话式引导功能，让用户在过程中可以像对待真人研究员一样，不断提问和细化要求。目前Deep Research是一次性生成报告，未来也许允许报告生成后用户继续追问细节、要求补充，这相当于报告模式和Chat模式的融合。Google可能也会让Gemini代理过程更加交互：比如执行过程中弹出选项让用户决定下一步侧重哪个方向，以使结果更符合期望。两者都会朝人类在环的方向改进交互，使AI助手更可控。\n又一值得期待的功能是实时数据接入。目前代理主要检索静态网页，而未来可能连接实时数据库和API，例如财经数据API、新闻流等。这会让研究涵盖最新的实时信息。OpenAI或Google或将提供插件，让代理直接调用诸如Twitter检索、证券行情接口，生成的报告可以有最新截至分钟的数据（当然可靠性也需审查）。Gemini 2.0已经支持实时音视频输入 (Gemini - Google DeepMind)，可以想象将来代理甚至能观看一场直播发布会并进行分析总结，真正达到实时深度研究。\n性能上，针对OpenAI目前Deep Research偏慢的问题，可能引入模型压缩和并行推理优化，让一次研究在几分钟内完成而不牺牲太多深度。OpenAI也许会开发mini版Deep Research（类似轻量模式），处理较简单的问题时快速给出结果，而复杂问题自动切换到深度模式。Google则可能提升深度：让Gemini代理在需要时也能做多轮迭代而不是严格按初始计划进行，从而缩小和OpenAI在灵活性上的差距。\n潜在研究方向和技术创新：未来的深度研究领域有许多开放研究课题。例如，如何让AI代理具备因果推理能力，而不仅是相关信息罗列？目前代理主要综合资料，缺乏主动提出假说、设计实验的科学推理能力。未来研究可能将LLM与因果推理模型结合，代理可根据数据推断因果关系，并建议进一步验证的方案。\n另一方向是增强长程规划。虽然OpenAI代理已能多步迭代，但当任务跨度非常大（例如撰写一本综述性书籍）时，如何保持全局一致性是难题。有学者提出“树状思维 (Tree-of-Thoughts)”算法，让模型搜索多种可能思路路径再择优 (Synergizing Reasoning and Acting in Language Models | by Ronny H)。这种搜索式推理可以应用于代理决策，使其在开始研究前对各种方案“预演”，选最优策略，从而提高质量和效率。结合进化算法或MCTS等AI规划技术，代理在复杂课题上会更加得心应手。\n多代理协作也是值得探索的方向。未来或许不止一个AI在工作，而是一组专家代理协同：一个负责数据爬取清洗，一个负责模型分析推理，一个负责结果审校优化。这样的架构可以平行开发不同专长模型，各司其职又互相检查。初步的迹象比如AutoGPT已经尝试让多个子代理合作完成任务。OpenAI也可能研究让Deep Research与Operator等其他代理互动，比如Operator代理帮助Deep Research自动打开专业应用、下载论文等，让研究范围扩展。\n在人机协作方面，一个创新方向是记忆注入的个性化研究。代理可维护一个长期知识档案，记录用户以往关注的领域和立场，从而定制研究侧重面。例如一位医学研究者用代理多次查资料，代理可积累一个医学知识向量库，下一次在通用检索外先检索自己的记忆，以输出更有针对性的报告。这样每个用户都会培养出自己独特风格和强项的AI研究助理。技术上涉及长期记忆模块和持续学习，需解决遗忘和模型更新兼容性问题。\n另一个前沿方向是让AI代理不仅汇总知识，还能创造新知识。目前代理停留在综合已有信息层次。未来研究AGI的学者设想，AI代理能够通过模拟试验、自动推导证明等，真正提出前所未有的见解，即从“知识的搬运工”升级为“知识的生产者”。这或许需要引入强化学习进行假设检验，或AutoML生成新模型进行数据分析，从而获得新发现。这是离真正科研助理甚至独立科学家更近的一步，现在还处于科幻想象，但随着代理智能增强，可能逐步看到雏形。\n**对人工通用智能 (AGI) 的潜在影响：**深度研究代理被一些OpenAI员工誉为“个人AGI时刻” (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。的确，当一个AI系统能在几分钟内完成综合调研、分析推理并写出详尽报告，这已经覆盖了相当广泛的人类智力活动（阅读、归纳、写作、判断）。Sam Altman 曾大胆预测Deep Research这样的工具可胜任“全球经济中单位数百分比的有价值工作” (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。这是一个惊人的比例，意味它开始取代相当一部分知识工作，这正是AGI预兆之一。深度研究代理体现出AGI的几个要素：自主性（无需细粒度指令能自己推进任务）、多能性（涉猎各领域知识并运用）、长时推理（能进行数十步推理链）、自我改进（通过反馈可以变得更好）。虽然当前系统离“全面胜任任何人类任务”还有距离，但已在专业调研领域达到甚至超过一般人水平 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。\n深度研究技术对AGI的影响主要有两方面：一是能力层面的突破。如果一个代理可以无监督地阅读互联网并整理知识，那让它去学习新技能（比如新的编程语言、设计原理）也是可行的。通过不断读取和实验，它可能自行掌握越来越多任务，逐步逼近通用智能。这和人类通过阅读学习相似。二是对人类角色的改变。AGI并非孤立存在，而是与人协作。深度研究代理已经在扮演人类“认知外包”的角色，让人类专注于决策和创造。随着这些代理能力提高，人类可能将更多基础认知工作交给AI，自己扮演监督者和高层决策者。这种分工的成熟也许是AGI真正落地的标志之一。\n不过，实现AGI还需要克服更多挑战，如常识推理、抽象概念理解等。有分析指出当前代理主要在收集和总结，缺乏自主规划长期目标的能力。不过Google和OpenAI正通过各自的 agent 项目探索这最后一段路。例如Google在Gemini 2.0中就明确提出面向“agentic era”，并开发了Project Astra等原型，模拟通用AI助手完成复杂任务 (Gemini - Google DeepMind) (Gemini - Google DeepMind)。OpenAI的Operator和Deep Research也是在现实环境中测试代理实用性。可以预见，两家会逐步扩展代理技能并融合，最终产出真正的AGI代理。Deep Research作为早期形态，已经展示了AGI部分特征，也提醒我们AGI的发展需要关注可信、安全、伦理，确保在它变得更强大之前，我们已经建立起充分的控制和引导机制。总之，深度研究技术的进步既是AGI征途中的里程碑，也将在今后继续推动AGI的实现并塑造我们与知识、与AI共存的方式。\n7. 结论 深度研究技术将大型语言模型的能力拓展到复杂、多步骤的研究任务中，融合了搜索引擎、数据分析和专家写作的功能，代表了AI从“对话助手”向“自主研究员”的跨越。本报告深入分析了OpenAI Deep Research和Google Gemini Deep Research的技术细节与差异，得到以下关键见解：\n**架构与机制方面：**深度研究代理基于强大的Transformer语言模型（如OpenAI O3、Google Gemini），通过引入推理+行动交替的机制，实现了自动化的资料检索与分析综合 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models) ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。它们能够处理文本、图像等多模态信息，利用语义嵌入和向量数据库高效检索知识，并通过多轮推理构建对课题的全面理解 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Deep Chat JS, semantic search, and OpenAI integration)。OpenAI代理采用高度动态的迭代过程，深度挖掘且透明可控；Google代理采用结构化预规划，快速产出结构良好的报告 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。\n**使用技术方面：**这些代理集成了NLP最新成果与传统数据处理手段。它们借助网络爬虫技术获取海量资料，并用大型模型进行阅读理解和信息抽取。通过工具调用接口，代理可以执行代码计算、调用API获取数据，从而不仅能读懂数据还能加工利用数据 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。向量检索和RAG技术保障了对大规模知识的利用和更新 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)。同时，系统在性能上通过并行、分层处理实现可扩展，在可靠性上通过交叉验证和来源约束减少幻觉。这些工程与算法上的优化使深度研究成为可能。\n**比较与应用方面：**OpenAI Deep Research在推理深度、多模态支持和交互透明度上更突出，而Google Gemini Deep Research在速度、价格和文档集成上更具优势 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。两者各有适用场景：前者适宜需要深入洞察和过程追踪的专业研究，后者适宜快速信息汇总和决策支持。实际案例显示，它们均能在分钟级时间内完成过去需要数小时的人力研究任务，涵盖数据收集、分析到建议，体现了AI在知识工作上的巨大潜力 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。\n**挑战与未来方面：**深度研究代理仍需持续改进数据可信度把控，避免偏见和错误信息，这要求更精细的内容过滤和事实核查机制 (GPT-4 Technical Report) (GPT-4 Technical Report)。在可扩展性上，未来模型和算法进步会缓解长文档和多任务处理的瓶颈。我们可能见证更强的因果推理、更长程的自主规划以及多代理协作，使研究代理更接近人类专家水平甚至超越人类在某些领域的发现能力。深度研究技术作为当今AGI探索的一部分，其演进将直接推动通用人工智能的实现，并对知识工作形态产生深远影响 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。\n综上所述，深度研究代理展现了AI在高层次认知任务上的突破，为科研、商业分析、政策制定等领域提供了强有力的工具。它减轻了人类在信息爆炸时代的负担，让专业知识触手可及并易于综合。随着技术成熟，我们应积极研究如何安全地将其融入日常工作流程，并思考人类如何与这样强大的智能体协同共处。进一步的技术研究可以围绕提升推理可靠性、引入自主创新能力以及完善人机交互界面展开。可以预见，未来的人工智能研究助理将更加智能、高效且可信，其影响将不仅是提高工作效率，更可能改变人类获取和创造知识的范式，为迈向真正的人工通用智能奠定基础。\n8. 参考文献 Yao et al., 2022. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models) ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models) – 提出让大型语言模型在推理过程中执行动作的ReAct框架，验证了将链式思维与工具使用相结合可提升复杂任务性能和可解释性。\nNakano et al., 2021. WebGPT: Browser-assisted Question-Answering with Human Feedback. arXiv:2112.09332 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback) ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback) – 将GPT-3与网页浏览环境结合，通过人类反馈微调，实现了带引用的开放域问答，答案质量超过人类平均水平。\nLewis et al., 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS 2020 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks) ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks) – 提出RAG模型，将预训练语言模型与向量化知识库结合，通过检索支撑生成，有效提高知识密集任务的准确性和可更新性。\nOpenAI, 2025. Introducing deep research. OpenAI 官方博客，Feb 2025 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central) – 宣布推出Deep Research代理：“利用即将发布的O3模型，通过推理来搜索、解读和分析海量在线文本、图像和PDF，自动完成多步研究任务”，专为ChatGPT Pro用户提供服务。\nOkemwa, K., 2025. “Deep Research has been a personal AGI moment for me”: OpenAI’s new AI agentic tool simulates a personal research analyst. Windows Central 新闻报道, Feb 4 2025 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central) (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central) – 报道OpenAI Deep Research发布，引用OpenAI和Sam Altman的话强调其效率：“10分钟完成数小时人类工作”，并称员工体验“个人AGI时刻”。\nPankratyeva, A., 2025. Google Gemini vs. OpenAI Deep Research: Which Is Better?. Techopedia 技术评测文章, Feb 5 2025 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) – 从功能、强项、限制等方面详细比较了OpenAI Deep Research和Google Gemini Deep Research，指出OpenAI更深入细致但昂贵，Google更快捷平价但缺少深度推理。\nAnalytics Vidhya, 2025. OpenAI Deep Research vs Gemini Deep Research: Who does deep research better? Analytics Vidhya 博客文章, Feb 2025 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research) – 作者实测了OpenAI和Google深度研究代理对同一课题的表现，对比两者流程、用时、输出结构，总结OpenAI注重实时迭代和透明，Google注重结构化和速度。\nHassabis, D., 2023. (引自 Wired 采访，见维基百科) – DeepMind CEO Demis Hassabis 谈Google Gemini：“将结合AlphaGo的威力和语言模型的能力”，预期Gemini能在AI能力上超越OpenAI的GPT-4 (Gemini (language model) - Wikipedia)。\nGemini Team (Google), 2023. Gemini: A Family of Highly Capable Multimodal Models. 技术报告, Dec 6 2023 () () – 系统介绍了Google DeepMind的Gemini多模态模型系列，展示Gemini Ultra在众多语言、图像、视频任务上达到SOTA性能，并强调其跨模态推理和agent式能力。\nOpenAI, 2023. GPT-4 Technical Report \u0026amp; System Card. OpenAI (arXiv:2303.08774) (GPT-4 Technical Report) (GPT-4 Technical Report) – 描述了GPT-4模型的能力和局限，特别是安全挑战，包括偏见、虚假信息、隐私等，并列出OpenAI为缓解这些问题所采取的措施（如RLHF、安全测试等），为深度研究代理的安全设计提供了指导原则。\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui \u0026amp; OpenAI Deep Research. (Feb 2025). OpenAI Deep Research 案例分享 - 深度研究技术综合研究报告\u0026quot;. https://syhya.github.io/zh/posts/2025-02-15-deep-research-tech-report/\nOr\n@article{syhya2025deepresearch, title = \u0026#34;OpenAI Deep Research 案例分享 - 深度研究技术综合研究报告\u0026#34;\u0026#34;, author = \u0026#34;Yue Shui \u0026amp; OpenAI Deep Research\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-02-15-deep-research-tech-report\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-02-15-deep-research-tech-report/","summary":"\u003cp\u003eOpenAI Deep Research (\u003ca href=\"https://openai.com/index/introducing-deep-research/\"\u003eOpenAI, 2025\u003c/a\u003e) 主要是采用以 o3 推理模型为基座的多 agent 协作方式。相关开源研究和产品之前也有不少，比如 \u003ca href=\"https://github.com/assafelovic/gpt-researcher\"\u003eGPT Researcher\u003c/a\u003e 和 Google Gemini 版本的 Deep Research (\u003ca href=\"https://openai.com/index/introducing-deep-research/\"\u003eDeepMind, 2024\u003c/a\u003e)。不过 OpenAI 的产品是博主目前体验下来效果最好。之后咨询分析、论文综述和新闻编辑等任务都可以先让这个工具自动化完成，然后再由人工进行最后的修正和审查，怎么感觉之后我写技术博客的任务也会被替代了呢？😂\u003c/p\u003e","title":"OpenAI Deep Research 案例分享 - 深度研究技术综合研究报告"},{"content":"这篇博客主要介绍一种比 RLHF 更精简的替代算法 DPO。与 RLHF 一样，DPO 目的是使模型输出与人类偏好保持一致，但它在实现上更加简单，并且对资源的需求更低。在项目资源受限的情况下，DPO 是一个实用解决方案。\n符号 符号 含义 \\( x \\) 用户输入（Prompt），即模型需要回答的问题 \\( y \\) 模型生成的回答（Response / Completion），即模型输出的文本 \\( \\pi_\\theta(y \\mid x) \\) Actor 模型：待训练策略，用于生成回答 \\(y\\)；参数为 \\(\\theta\\) \\( \\pi_{\\mathrm{ref}}(y \\mid x) \\) 参考模型：冻结的 SFT 模型，作为对齐基准 \\( r_\\phi(x,y) \\) 奖励模型：用于评估回答 \\(y\\) 质量的奖励函数；参数为 \\(\\phi\\) \\( V_\\psi(x) \\) critic 模型：用于估计给定输入 \\(x\\) 下未来累计奖励的值函数；参数为 \\(\\psi\\) \\( \\pi^*(y \\mid x) \\) 最优策略分布，通过参考模型与奖励函数确定 \\( r_\\theta(x,y) \\) 基于 Actor 模型导出的奖励函数，通过 \\(\\pi_\\theta\\) 与 \\(\\pi_{\\mathrm{ref}}\\) 构造 \\(\\beta\\) 超参数，控制 KL 惩罚项或对数比差异项的权重 \\(\\mathbb{D}_{\\mathrm{KL}}[P \\| Q]\\) KL 散度，衡量概率分布 \\(P\\) 与 \\(Q\\) 之间的差异 \\(\\sigma(z)\\) Sigmoid 函数，定义为：\\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\) \\(\\log\\) 对数函数 \\(\\mathbb{E}\\) 期望算子，用于求随机变量的平均值 \\( (y_w, y_l) \\) 一对偏好数据，其中 \\( y_w \\) 表示被偏好（质量更好）的回答，\\( y_l \\) 表示质量较差的回答 \\( P\\left(y_w \\succ y_l \\mid x\\right) \\) 在输入 \\(x\\) 下，回答 \\( y_w \\) 优于 \\( y_l \\) 的概率 \\( Z(x) \\) 配分函数，对所有回答 \\(y\\) 归一化概率分布 \\( \\mathcal{L}_{\\mathrm{DPO}} \\) DPO 的损失函数 从 RLHF 到 DPO RLHF OpenAI 主要利用人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)(Christiano et al., 2017) 来训练 InstructGPT(Ouyang et al., 2022)，而其构成了大语言模型（如 ChatGPT, Llama 等）的基础。整个训练过程通常包括以下三个主要步骤：\nFig. 1. A diagram illustrating the three steps of InstructGPT. (Image source: Ouyang et al., 2022)\n监督微调（SFT）\n利用大量人工示例数据对预训练模型进行微调，得到一个初步能理解指令并生成合理回答的模型，即参考模型 \\( \\pi_{\\mathrm{ref}}(y \\mid x) \\) 。\n奖励模型训练\n这里我们简化只考虑生成两个不同的结果，实际可以生成多个结果进行排序。针对同一输入 \\(x\\) 生成两个回答 \\(y_w\\)（较优）和 \\(y_l\\)（较劣），由人工排序后收集偏好数据。基于这些数据训练奖励模型 \\(r_\\phi(x, y)\\)，使其能预测哪种回答更符合人类偏好。\n基于 PPO 的强化学习\n利用奖励模型 \\(r_\\phi\\) 提供的反馈，通过 PPO 算法优化 Actor 模型 \\(\\pi_\\theta\\) 以提升回答质量。为防止模型偏离 \\(\\pi_{\\mathrm{ref}}\\)，在优化过程中引入 KL 惩罚项。该阶段通常涉及以下 4 个模型：\n\\(\\pi_\\theta\\)：经过 SFT 后待更新的模型。 \\(\\pi_{\\mathrm{ref}}\\)：冻结的 SFT 模型，作为对齐基准。 \\(r_\\phi\\)：用于评估回答质量，参数固定。 \\(V_\\psi\\)：用于估计未来奖励，辅助 Actor 模型更新。 RLHF 的局限性 尽管 RLHF 能充分利用人类偏好信息提升模型对齐效果，但其固有局限性包括：\n多模型训练：除 Actor 模型 \\(\\pi_\\theta\\) 外，还需额外训练奖励模型 \\(r_\\phi\\) 和 Critic 模型 \\(V_\\psi\\)，整体训练过程复杂且资源消耗大。 高采样成本：LLM 生成文本计算量大，强化学习过程中的大量在线采样进一步推高了计算开销；采样不足可能导致错误的优化方向。 训练不稳定与超参数敏感：PPO 涉及众多超参数（如学习率、采样量等），调参复杂且训练过程易受不稳定因素影响。 对齐税效应：在提高模型对齐性的同时，可能会降低模型在其他任务上的表现。 DPO 简介 Fig. 2. DPO optimizes for human preferences while avoiding reinforcement learning.（Image source: Rafailov et al., 2023）\n直接偏好优化(Direct Preference Optimization, DPO) (Rafailov et al., 2023)为了解决 RLHF 的上述问题，其核心思路是将 RLHF 的目标转化为类似于监督微调的对比学习任务，从而实现：\n省略奖励模型训练：直接利用人类偏好数据优化 Actor 模型 \\(\\pi_\\theta\\)，无须单独训练 \\(r_\\phi\\)。 消除强化学习采样：采用对比损失函数替代 PPO，降低采样和计算资源消耗。 提升训练稳定性：基于监督学习的方法对超参数不敏感，训练过程更加平稳。 虽然 DPO 在 LLM 性能提升的上限上可能不及 RLHF，但在资源利用、实现复杂度和训练稳定性方面具有优势。\n方法对比 方法 训练步骤 模型 训练方式 优点 缺点 RLHF 先训练奖励模型，再使用 PPO 优化策略 \\(\\pi_\\theta\\)、\\(\\pi_{\\mathrm{ref}}\\)、\\(r_\\phi\\)、\\(V_\\psi\\) 强化学习和在线采样 充分利用人类偏好，上限潜力较高 资源消耗大、训练不稳定、超参数敏感 DPO 直接利用偏好数据训练 Actor 模型 \\(\\pi_\\theta\\)、\\(\\pi_{\\mathrm{ref}}\\) 类似 SFT 监督学习 流程简化、训练稳定、资源消耗低 性能提升上限可能低于 RLHF DPO 数学推导 RLHF 目标与最优策略分布 在大规模语言模型对齐中，我们希望利用人类反馈强化学习（RLHF）来优化模型输出。设输入 \\( x \\) 来自数据集 \\(\\mathcal{D}\\)，模型生成回答 \\( y \\)；待训练的 模型记为 \\(\\pi_\\theta(y \\mid x)\\)，而参考模型记为 \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\)（通常为SFT模型），同时引入奖励函数 \\( r(x,y) \\) 衡量回答质量。RLHF 的目标可写为\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\Big[ r(x,y) \\Big] \\;-\\; \\beta\\, \\mathbb{D}_{\\mathrm{KL}}\\Big[ \\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x) \\Big], \\tag{1} \\]其中 \\(\\beta\\) 为调节奖励与参考模型偏差的超参数。利用 KL 散度的定义\n\\[ \\mathbb{D}_{\\mathrm{KL}} \\Big[\\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\Big] = \\mathbb{E}_{y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right], \\tag{2} \\]式 (1) 可重写为\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ r(x,y) - \\beta \\, \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right]. \\tag{3} \\]将 (3) 式转换为最小化问题并除以 \\(\\beta\\) 得\n\\[ \\min_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} - \\frac{1}{\\beta} r(x,y) \\right]. \\tag{4} \\]假设存在一个最优策略分布 \\(\\pi^*(y \\mid x)\\) 使 (4) 式全局最优，则令\n\\[ \\pi^*(y \\mid x) \\;=\\; \\frac{1}{Z(x)} \\,\\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big), \\tag{5} \\]其中配分函数 \\( Z(x) \\) 定义为\n\\[ Z(x) = \\sum_{y}\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big). \\tag{6} \\] \\(Z(x)\\) 对所有可能的 \\(y\\) 求和，实现归一化，使得 \\(\\pi^*(y \\mid x)\\) 构成合法概率分布。 \\(Z(x)\\) 是 \\(x\\) 的函数，与待优化的 Actor 模型 \\(\\pi_\\theta\\) 无关。 对 (5) 式取对数得到\n\\[ \\log \\pi^*(y \\mid x) = \\log \\pi_{\\mathrm{ref}}(y \\mid x) + \\frac{1}{\\beta}\\, r(x,y) - \\log Z(x), \\tag{7} \\]从而解得\n\\[ r(x,y) = \\beta \\left[\\log \\frac{\\pi^*(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{8} \\]Bradley–Terry 模型 为了利用成对偏好数据 \\((x, y_w, y_l)\\) 训练模型，我们希望在相同输入 \\( x \\) 下，模型输出更偏好于高质量回答 \\( y_w \\) 而不是低质量回答 \\( y_l \\)。\nBradley–Terry 模型用于预测成对比较的结果。对于任意两个对象 \\( i \\) 和 \\( j \\)，若为每个对象分配正实数得分 \\( p_i \\) 和 \\( p_j \\)，则对象 \\( i \\) 被认为比对象 \\( j \\) 强的概率为\n\\[ \\Pr(i \u003e j) = \\frac{p_i}{p_i + p_j}. \\tag{9} \\]在我们的场景中，将每个回答 \\( y \\) 的强度参数设为 \\( p_{y} = \\exp\\big(r(x,y)\\big) \\)（保证为正实数）。因此，给定输入 \\( x \\) 下，回答 \\( y_w \\) 好于 \\( y_l \\) 的概率为\n\\[ P\\left(y_w \\succ y_l \\mid x\\right)=\\frac{\\exp \\big[r(x,y_w)\\big]}{\\exp \\big[r(x,y_w)\\big]+\\exp \\big[r(x,y_l)\\big]}. \\tag{10} \\]为了使得数据集中每个成对偏好数据 \\((x, y_w, y_l)\\) 中，高质量回答 \\( y_w \\) 的胜出概率尽可能大，我们将奖励模型训练目标设计为最大化 \\( y_w \\) 被偏好的概率，或等价地最小化负对数似然损失：\n\\[ L_{R}\\left(r_{\\phi}, D\\right) = -\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\left[\\log P\\left(y_w \\succ y_l \\mid x\\right)\\right], \\tag{11} \\] 其中数据集定义为 \\[ D=\\{(x^i, y_w^i, y_l^i)\\}_{i=1}^{N}. \\tag{12} \\]利用公式 (10)、(11) 以及下面的恒等式\n\\[ \\log \\frac{e^a}{e^a+e^b} = \\log\\frac{1}{1+e^{b-a}} = \\log \\sigma(a-b), \\tag{13} \\]其中 Sigmoid 函数定义为\n\\[ \\sigma(z)=\\frac{1}{1+e^{-z}}, \\tag{14} \\]可得\n\\[ \\log P\\left(y_w \\succ y_l \\mid x\\right) = \\log \\sigma\\Big(r(x,y_w)-r(x,y_l)\\Big). \\tag{15} \\]直接偏好优化 注意到 (8) 式中，奖励 \\( r(x,y) \\) 与最优策略的对数比有关。为避免显式训练一个单独的奖励模型 \\(r_\\phi\\)，我们采用 DPO的思想，即直接用待训练 Actor 模型 \\(\\pi_\\theta\\) 替换最优策略 \\(\\pi^*\\) 的位置，将 (8) 式中的奖励表示为\n\\[ r_\\theta(x,y) \\;=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{16} \\]在成对比较中，对于相同输入 \\( x \\)，两个回答 \\( y_w \\) 和 \\( y_l \\) 均包含相同的 \\(\\log Z(x)\\) 项，因此在计算奖励差值时，该项会被消去，即\n\\[ \\begin{aligned} r_\\theta(x,y_w)-r_\\theta(x,y_l) \u0026=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} + \\log Z(x)\\right] - \\beta \\left[\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} + \\log Z(x)\\right] \\\\ \u0026=\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)}. \\end{aligned} \\tag{17} \\]将 (17) 式代入 (15) 式，并结合 (11) 式，我们最终得到 DPO 的损失函数\n\\[ \\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}}) = - \\mathbb{E}_{(x,y_w,y_l) \\sim D} \\left[ \\log \\sigma\\Big( \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} -\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big) \\right]. \\tag{18} \\]该损失函数针对待训练 Actor 模型 \\(\\pi_\\theta\\) 而设，通过比较其在高质量回答 \\(y_w\\) 与低质量回答 \\(y_l\\) 上、相对于参考模型 \\(\\pi_{\\mathrm{ref}}\\) 的对数概率比来区分好坏回答：当 \\(\\pi_\\theta\\) 在 \\(y_w\\) 上的相对概率比远大于 \\(y_l\\) 时，Sigmoid 输出更接近 1，损失更小；反之，若 \\(\\pi_\\theta\\) 对 \\(y_l\\) 的相对概率过大，则损失增加。参数 \\(\\beta\\) 用于放大或缩小这些对数比的差值，从而调节模型对好坏答案的区分强度。\n训练流程 数据准备 偏好数据采集\n收集大量偏好数据，每个样本包含输入 \\( x \\) 及一对回答 \\((y_w, y_l)\\)，其中 \\( y_w \\) 为被偏好回答(chosen)，\\( y_l \\) 为质量较差的回答(rejected)。\n数据集例子: dpo_en_demo.json\n{ \u0026#34;conversations\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;human\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;what does abstraction mean on oop\u0026#34; } ], \u0026#34;chosen\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Abstraction in Object-Oriented Programming (OOP) is the process of exposing only the necessary information to the outside world while hiding the implementation details. It\u0026#39;s a way to simplify complex systems by focusing only on the essential aspects and encapsulating the implementation details.\\n\\nIn other words, abstraction is about hiding the internal workings of an object or system and only exposing the necessary interfaces or APIs to interact with it. This helps to reduce complexity, improve modularity, and make the system easier to understand and maintain.\\n\\nFor example, consider a car. A car has many components like engine, transmission, brakes, etc. However, when you drive a car, you don\u0026#39;t need to know how the engine works or how the transmission changes gears. All you need to know is how to operate the car\u0026#39;s controls like accelerator, brakes, and steering wheel. The car\u0026#39;s controls are an abstraction of the underlying complex systems, allowing you to interact with the car without needing to know the details of how it works.\\n\\nIn programming, abstraction is achieved by defining interfaces or abstract classes that define the essential functions or behaviors of an object without specifying how they are implemented. This allows developers to create different implementations of the same interface or abstract class, which can be swapped out without affecting the rest of the system.\\n\\nAbstraction is a fundamental concept in OOP and is used to create modular, maintainable, and flexible software systems. It helps to reduce code duplication, improve code reuse, and make software systems more understandable and easier to evolve over time.\u0026#34; }, \u0026#34;rejected\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Abstraction is a principle in object-oriented programming that refers to the process of focusing on the essential features of an object or concept without emphasizing the details that are not important for its functionality. It enables programmers to create classes that represent the objects they want to work with, and specifies only the behavior of those objects, leaving out unnecessary details. Abstraction helps to make the code more maintainable, modular, and scalable. It also improves the productivity of developers by reducing the amount of code they need to write.\u0026#34; } } 输出预计算\n为降低计算开销，预先计算并缓存参考模型 \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\) 的输出。\n模型训练 训练目标\n通过最小化 DPO 损失 \\(\\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}})\\) 直接优化 Actor 模型 \\(\\pi_\\theta\\)，使其生成的回答更符合人类偏好。\n训练步骤\n从数据集中采样一批 \\((x, y_w, y_l)\\) 数据。\n计算 Actor 模型 \\(\\pi_\\theta(y \\mid x)\\) 的输出概率。\n利用下式计算损失：\n\\[ \\mathcal{L}_{\\mathrm{DPO}} = - \\log \\sigma\\Big( \\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big). \\] 通过反向传播更新 Actor 模型参数 \\(\\theta\\)。\n模型推理 训练完成后，得到的 Actor 模型 \\(\\pi_\\theta\\) 可直接用于推理。给定输入 \\( x \\) 后，模型基于学到的概率分布生成回答。由于训练过程中参考了人类偏好，同时受到参考模型 \\(\\pi_{\\mathrm{ref}}\\) 的约束，生成的回答既符合预期，又能保持生成文本的稳定性。\n总结 DPO 将 RLHF 过程简化为直接的监督学习任务，不仅节省了资源、提高了训练稳定性，同时降低了实现复杂度，是 LLM 对齐训练的一种高效替代方法。在实际应用中，我们可以根据业务场景选择 RLHF 或 DPO 方法，以达到最佳的训练效果。\n参考文献 [1] Christiano, Paul F., et al. \u0026ldquo;Deep reinforcement learning from human preferences.\u0026rdquo; Advances in neural information processing systems 30 (2017).\n[2] Ouyang, Long, et al. \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in neural information processing systems 35 (2022): 27730-27744.\n[3] Rafailov, Rafael, et al. \u0026ldquo;Direct preference optimization: Your language model is secretly a reward model.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Feb 2025). 大语言模型对齐: 直接偏好优化(DPO). https://syhya.github.io/posts/2025-02-08-dpo\nOr\n@article{syhya2025dpo, title = \u0026#34;大语言模型对齐: 直接偏好优化(DPO)\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-02-08-dpo\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-02-08-dpo/","summary":"\u003cp\u003e这篇博客主要介绍一种比 RLHF 更精简的替代算法 DPO。与 RLHF 一样，DPO 目的是使模型输出与人类偏好保持一致，但它在实现上更加简单，并且对资源的需求更低。在项目资源受限的情况下，DPO 是一个实用解决方案。\u003c/p\u003e","title":"大语言模型对齐: 直接偏好优化(DPO)"},{"content":" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有较大变动。请随时关注最新版本。\n大型语言模型（Large Language Models, LLMs）在自然语言处理领域取得了革命性的进展，展现出强大的文本理解和生成能力。然而，LLMs 并非完美无缺，它们仍然面临着一些固有的挑战，例如：\n知识截止性: LLMs 的知识库通常仅限于其预训练数据，对于训练数据截止日期之后的新信息或事件，它们往往无法准确回答。 幻觉: LLMs 有时会“一本正经地胡说八道”，即生成看似合理但实际上是虚假或不准确的信息。 领域知识和长尾知识缺乏: 对于特定垂直领域或私有知识库，通用 LLMs 可能缺乏足够的专业知识。相关研究指出 LLMs 在学习长尾知识方面存在困难(Kandpal et al., 2022)，而 RAG 正是解决这一问题的有效途径。 可解释性与可追溯性差: LLMs 的决策过程通常是一个“黑箱”，难以追溯其生成内容的具体来源和依据。 上下文窗口限制: LLMs 的上下文窗口长度有限，无法处理过长的输入文本，直接将多个过长的文档输入给 LLM 会导致超过模型的上下文窗口限制，并且推理成本较高。 为了克服这些局限性，检索增强生成（Retrieval-Augmented Generation, RAG） 技术应运而生。RAG 的核心思想是将 LLMs 的强大生成能力与外部知识库的实时、准确信息检索能力相结合，从而提升模型输出的质量、时效性和可信度。\nRAG 基础知识 在深入探讨 RAG 的高级技术之前，我们首先需要理解其基本工作流程和核心组件。一个典型的 RAG 系统主要包含数据处理（或称索引）、检索和生成三个阶段。\nFig. 1. A representative instance of the RAG process applied to question answering. (Image source: general RAG concepts)\n图 1 展示了一个标准的 RAG 工作流程，其核心思想是在 LLM 生成答案之前，先从外部知识源中检索相关信息，并将这些信息作为上下文提供给 LLM。\n1. 数据处理与索引 (Data Processing and Indexing) RAG 的第一步是准备和处理外部知识源。这些知识源可以是各种形式的文档，如 PDF、网页、数据库记录、代码库、甚至是多模态数据（如图像、音视频）。\n文档加载 (Document Loading): 从不同的数据源（文件系统、数据库、API、Sharepoint 等）加载原始数据。 文本分块 (Chunking / Segmentation): 将加载的文档分割成较小的文本块（chunks）。这是因为 LLMs 的上下文窗口长度有限（尽管现代 LLM 的上下文窗口越来越大，如 Gemini 1.5 Pro 的百万级 token 上下文，但过长的上下文仍可能导致中间遗忘问题 (Liu et al., 2023)，并且更小的文本块有助于提高检索的精确度。 分块策略 (Chunking Strategies): 常见的策略包括： 固定大小分块 (Fixed-size Chunking): 将文本按固定数量的 token 或字符分割。简单但可能切断语义单元。 基于字符分块 (Character-based Chunking): 根据特定的字符（如换行符 \\n\\n、句号 . ）进行分割。 递归分块 (Recursive Character Text Splitting): 尝试按一系列分隔符递归地分割文本，直到块大小满足要求。这有助于保持语义块的完整性，是 LangChain 等框架中常用的方法。 语义分块 (Semantic Chunking): 利用 NLP 技术（如句子嵌入、主题模型）来识别文本中的语义边界，并据此进行分块。例如，可以基于句子嵌入的差异来判断语义是否发生较大转变，从而确定分块点。 文档特定分块 (Document Specific Chunking): 针对特定文档类型（如 Markdown、HTML、代码文件）的结构进行分块，例如按标题、段落、代码块等。 块大小 (Chunk Size): 需要根据具体任务、LLM 的上下文窗口大小、嵌入模型的最佳输入长度以及数据的特性来选择。通常在几百到一千 tokens 之间。 重叠 (Overlap): 在相邻块之间设置一定的重叠内容（如 50-100 tokens），以保证语义的连续性，避免关键信息在分块边界被切断。 文本嵌入 (Text Embedding): 使用预训练的文本嵌入模型（如 OpenAI\u0026rsquo;s text-embedding-3-small/large）将每个文本块转换为高维向量。这些向量能够捕捉文本块的语义信息，使得语义相似的文本块在向量空间中距离更近。查询 \\( q \\) 也会被同一个嵌入模型转换为向量 \\( \\mathbf{v}_q = E(q) \\)，而每个文本块 \\( c \\) 被转换为 \\( \\mathbf{v}_c = E(c) \\)。 向量存储与索引 (Vector Storage and Indexing): 将所有文本块的向量表示存储在专门的向量数据库（Vector Database）中。向量数据库不仅存储向量，还为这些向量建立高效的索引（如 FAISS (Johnson et al., 2021), HNSW, IVFADC, SCANN 等），以加速后续的相似性搜索。常用的向量数据库有 Pinecone, Weaviate, Milvus, ChromaDB, Qdrant, 以及云服务商提供的向量搜索服务（如 Azure AI Search, Google Vertex AI Vector Search, Amazon OpenSearch）。 索引类型: Flat: 精确搜索，但对于大规模数据效率较低。 IVF (Inverted File Index): 通过聚类将向量分组，搜索时只在相关的簇内进行。 HNSW (Hierarchical Navigable Small World): 基于图的索引，通过构建多层图结构实现高效的近似最近邻搜索。 LSH (Locality Sensitive Hashing): 通过哈希函数将相似的向量映射到相同的桶中。 Fig. 3. Comparison of different ANN algorithms: Flat, LSH, HNSW, IVF. (Image source: Pinecone)\n图 3 展示了几种常见的向量索引类型及其在检索质量、速度和内存消耗方面的权衡。\n2. 检索 (Retrieval) 当用户提出一个查询 \\( q \\) (或 \\( Q \\)) 时，RAG 系统会执行以下检索步骤：\n查询嵌入 (Query Embedding): 使用与文档处理阶段相同的嵌入模型将用户查询 \\( q \\) 转换为查询向量 \\( \\mathbf{v}_q \\)。 相似性搜索 (Similarity Search): 在向量数据库中，计算查询向量 \\( \\mathbf{v}_q \\) 与所有已索引文本块向量 \\( \\mathbf{v}_c \\) 之间的相似度。最常用的相似度度量是余弦相似度 (Cosine Similarity)： \\[ \\text{sim}(\\mathbf{v}_q, \\mathbf{v}_c) = \\frac{\\mathbf{v}_q \\cdot \\mathbf{v}_c}{\\|\\mathbf{v}_q\\| \\|\\mathbf{v}_c\\|} \\] 其中 \\( \\cdot \\) 表示向量点积，\\( \\|\\cdot\\| \\) 表示向量的欧几里得范数（L2 范数）。其他相似度度量还包括： 点积 (Dot Product / Inner Product): \\( S_{\\text{dot}}(\\mathbf{v}_q, \\mathbf{v}_c) = \\mathbf{v}_q \\cdot \\mathbf{v}_c \\)。如果向量已归一化，则等价于余弦相似度。 欧氏距离 (Euclidean Distance / L2 Distance): \\( L_2(\\mathbf{v}_q, \\mathbf{v}_c) = \\|\\mathbf{v}_q - \\mathbf{v}_c\\| = \\sqrt{\\sum_{i=1}^{n} (v_{q_i} - v_{c_i})^2} \\)。距离越小表示越相似。 上下文选择 (Context Selection): 选择与查询向量最相似的 \\( K \\) 个文本块 \\( \\{C_1, C_2, \\ldots, C_K\\} \\) 作为增强 LLM 上下文的背景知识。\\( K \\) 值的选择是一个超参数，需要根据任务和 LLM 的上下文窗口大小进行调整。 3. 生成 (Generation) 检索到相关的文本块后，RAG 系统将这些信息与原始用户查询结合起来，输入给 LLM 以生成最终答案：\n提示构建 (Prompt Engineering): 将用户查询 \\( q \\) 和检索到的 \\( K \\) 个文本块 \\( \\{C_1, \\ldots, C_K\\} \\) 组合成一个结构化的提示 \\( P \\)。一个常见的提示模板如下：\n请根据以下提供的上下文信息来回答用户的问题。如果上下文信息不足以回答问题，请明确指出。 上下文信息： --- [上下文块 1: C_1 的内容] --- [上下文块 2: C_2 的内容] --- ... --- [上下文块 K: C_K 的内容] --- 用户问题：[用户查询 q 的内容] 答案： 提示的设计对 RAG 系统的性能至关重要。\nLLM 推理 (LLM Inference): 将构建好的提示 \\( P \\) 输入给 LLM，LLM 会基于其预训练知识和提供的上下文信息生成答案 \\( A = \\text{LLM}(P) \\)。\n通过这种方式，RAG 系统能够利用外部知识库的最新和特定领域信息来增强 LLM 的回答，从而提高答案的准确性、相关性和时效性。\nRAG 与模型微调的对比 在将领域知识融入 LLM 时，RAG 和模型微调 (Fine-tuning, FT) 是两种主要的技术路径。它们各有优劣，适用于不同的场景。\n特性 RAG (检索增强生成) 模型微调 (Fine-tuning) 知识更新 实时，通过更新外部知识库即可实现 滞后，需要重新训练模型以融入新知识 计算成本 推理时有检索开销；知识库构建成本相对较低 训练成本高昂，尤其对于大型模型；推理成本固定 幻觉问题 显著减少，答案基于检索到的事实 仍可能存在，尤其对于训练数据未覆盖的知识 可解释性 较高，可以追溯答案来源 较低，模型决策过程不透明 领域适应性 强，易于接入不同领域的知识库 强，模型可以深度学习特定领域的语言模式和知识 数据隐私 外部知识库可以保持私有，不直接暴露给 LLM 服务提供商 训练数据可能需要上传给模型服务提供商，存在隐私风险 实现复杂度 相对较高，需要构建和维护检索系统 较高，需要准备高质量的微调数据集和专业的训练技巧 适用场景 动态知识、事实问答、需要溯源的场景、减少幻觉 学习特定风格、语气、格式，或深度掌握特定领域知识的场景 在许多实际应用中，RAG 和微调并非互斥，而是可以结合使用。例如，可以先对 LLM 进行领域相关的微调，使其更好地理解领域术语和概念，然后再结合 RAG 来获取最新的动态信息。\nRAG 的核心技术与范式演进 Fig. 4. The Evaluation of RAG system.\nRAG 技术自提出以来，经历了从简单到复杂，从通用到模块化的演进过程。理解这些演进有助于我们更好地设计和优化 RAG 系统。\n1. 朴素 RAG (Naive RAG) 朴素 RAG (或称简单 RAG) 是 RAG 技术最初也是最基础的形态，其流程如上一节所述，遵循“索引-检索-生成”的线性模式。\n索引 (Indexing): 文档被分割成文本块，通过嵌入模型转换为向量，并存储在向量数据库中。 检索 (Retrieval): 用户查询被嵌入后，在向量数据库中进行相似性搜索，找出最相关的文本块。 生成 (Generation): 将检索到的文本块与用户查询一起构建成提示，输入给 LLM 生成答案。 局限性: 朴素 RAG 虽然直接有效，但也面临一些挑战:\n检索质量: 低精确率 (Low Precision): 检索到的上下文中可能包含大量与问题不直接相关或冗余的信息（噪声），导致 LLM 分心或生成不相关的答案。 低召回率 (Low Recall): 可能未能检索到所有相关的关键信息，导致答案不完整或不准确。 过时信息 (Outdated Information): 如果向量数据库中的知识未及时更新，检索到的信息可能是过时的。 生成质量: 幻觉 (Hallucination): 即使有检索到的上下文，LLM 仍可能生成与上下文不符或完全捏造的内容。 不连贯/重复 (Incoherence/Repetition): LLM 可能难以将多个检索到的文本块融合成一个连贯、流畅的答案，或者在答案中重复某些信息。 上下文整合不佳 (Poor Context Integration): LLM 可能未能充分利用所有相关的上下文信息，或者错误地理解/组合了上下文。 增强过程的挑战: 分块策略与大小选择 (Chunking Strategy and Size Selection): 块太小可能丢失上下文，块太大可能引入噪声并增加处理成本。 上下文窗口限制 (Context Window Limitation): 检索到的上下文总长度可能超出 LLM 的上下文窗口限制。 成本与延迟 (Cost and Latency): 嵌入、索引、检索和 LLM 生成都需要计算资源和时间。 2. 高级 RAG (Advanced RAG) 为了克服朴素 RAG 的局限性，研究者们提出了多种高级 RAG 技术，主要围绕优化检索和生成过程展开。\n2.1 优化检索过程 (Optimizing Retrieval) 提升检索质量是高级 RAG 的核心目标之一。\n预检索策略 (Pre-retrieval Strategies): 这些策略在实际检索发生之前对数据或查询进行优化。\n数据清洗与优化 (Data Cleaning and Optimization): 数据清洗: 去除无关字符、格式化文本、处理噪声数据。 元数据提取与增强 (Metadata Extraction and Enrichment): 从文档中提取结构化信息（如标题、作者、日期、章节、关键词、文件来源、上次修改时间等）作为元数据，并在检索时用于过滤或加权。例如，可以根据文档的创建日期过滤掉过时的信息，或者优先检索来自特定作者的文档。 信息提取与结构化: 将非结构化文本中的关键信息（如实体、关系）提取出来，并可能将其转换为结构化格式（如知识图谱节点/边），以便后续更精确的检索。 索引结构优化 (Index Structure Optimization): 分块策略优化 (Chunking Strategy Optimization): 内容感知分块 (Content-aware Chunking): 例如，基于 NLP 的句子分割、段落分割，或者使用 LLM 来判断最佳分块点。LlamaIndex 中的 SemanticSplitterNodeParser 是一种尝试。 滑动窗口 (Sliding Window): 在文本切分时，采用重叠的滑动窗口，以避免重要信息在块边界被切断。 小块到大块 (Small-to-Big / Parent Document Retriever): 索引较小的、更具针对性的文本块，但在检索到这些小块后，将其父文档或更大的上下文窗口提供给 LLM。这有助于平衡检索精度和上下文完整性。LlamaIndex 中的 ParentDocumentRetriever 就是一个例子。 摘要嵌入 (Summary Embedding / Proposition Indexing): 为每个文档或大块文本生成摘要或原子性的命题（propositions），然后对摘要/命题进行嵌入和索引。检索时先匹配摘要/命题，再定位到原始文档或相关块。DenseX (Chen et al., 2023d) 提出使用命题作为检索单元。 多向量检索器 (Multi-Vector Retriever): 为同一个文档块生成多个向量表示，以从不同角度捕捉其语义。例如： 小块向量: 将文档分割成更小的子块，为每个子块生成向量。 摘要向量: 为文档块生成摘要，并对摘要进行嵌入。 假设性问题向量 (Hypothetical Questions): 为每个文档块生成一些可能由该块回答的问题，然后对这些问题进行嵌入。检索时，将用户查询与这些假设性问题进行匹配。 查询优化 (Query Optimization): 查询扩展 (Query Expansion): 使用同义词、相关术语、LLM 生成的多个查询变体或子问题来扩展原始查询，以召回更全面的相关文档 (Jagerman et al., 2023)。Query2Doc (Wang et al., 2023c) 使用 LLM 生成伪文档来扩展查询。 查询转换/重写 (Query Transformation/Rewriting): 利用 LLM 将用户的原始查询重写为更清晰、更适合检索的表达形式。例如，去除歧义、补充上下文、明确意图。Rewrite-Retrieve-Read (Ma et al., 2023) 框架就是一个例子。 查询路由 (Query Routing): 根据查询的类型或意图，将其路由到不同的索引、数据源或检索策略。例如，事实性问题可能路由到向量索引，而分析性问题可能需要更复杂的检索流程或知识图谱查询。 HyDE (Hypothetical Document Embeddings) (Gao et al., 2022): LLM 根据用户查询生成一个假设性的答案文档 \\(D_{hypo}\\)。然后，使用 \\(D_{hypo}\\) 的嵌入 \\( \\mathbf{v}_{D_{hypo}} = E(D_{hypo}) \\) 来检索真实的文档块。这种方法认为，与查询直接相关的文档，其语义可能更接近于一个理想的答案，而不是查询本身。 步退提示 (Step-Back Prompting) (Zheng et al., 2023): 对于特定、细节性的问题，先让 LLM 生成一个更泛化、更高层次的“步退问题”，然后检索与这个步退问题相关的上下文，再结合原始问题进行回答。这有助于获取更全面的背景知识。 检索中策略 (During-retrieval Strategies): 这些策略在检索过程中动态调整。\n嵌入模型微调 (Fine-tuning Embedding Models): 在特定领域的数据集或与任务相关的查询-文档对上微调嵌入模型。可以使用对比学习损失（如 InfoNCE）来拉近相关对的距离，推远不相关对的距离。 \\[ \\mathcal{L}_{\\text{InfoNCE}} = -\\log \\frac{\\exp(\\text{sim}(\\mathbf{v}_Q, \\mathbf{v}_{C^+}) / \\tau)}{\\sum_{i=0}^{N} \\exp(\\text{sim}(\\mathbf{v}_Q, \\mathbf{v}_{C_i}) / \\tau)} \\] 其中 \\( \\mathbf{v}_{C^+} \\) 是与查询 \\( \\mathbf{v}_Q \\) 相关的正例文本块嵌入，\\( \\mathbf{v}_{C_i} \\) 是负例文本块嵌入（通常包括批内负例和难负例），\\( \\tau \\) 是温度超参数。BGE M3-Embedding (Chen et al., 2024) 通过自知识蒸馏和高效批处理策略训练多语言、多功能、多粒度的文本嵌入。 混合检索 (Hybrid Search): 结合稀疏检索（如 BM25，基于词频统计）和稠密检索（向量检索，基于语义相似性）的结果。通常通过对两种检索器的得分进行加权求和或使用更复杂的融合策略（如 Reciprocal Rank Fusion, RRF）来实现。 后检索策略 (Post-retrieval Strategies): 这些策略在检索到初步结果后进行优化。\n重排序 (Re-ranking): 对初步检索到的 \\( K \\) 个文本块进行二次排序，以提高排序顶部的相关性。 基于模型的重排序: 使用更强大的模型（如 Cross-Encoder，它将查询和每个文档块拼接后输入模型进行打分，计算成本较高但通常更准确）或专门的重排序模型（如 Cohere Rerank, bge-reranker-large）。 基于 LLM 的重排序: 提示 LLM 对检索到的文本块列表进行相关性打分或排序。 上下文压缩与选择 (Context Compression and Selection): LLMLingua (Jiang et al., 2023): 使用小型 LLM 来识别和移除提示中的冗余词元，从而在保持关键信息的同时压缩上下文长度，减少 LLM 推理成本和延迟。 选择性上下文 (Selective Context): 根据某些启发式规则或小型模型来选择最重要的文本块或句子，而不是使用所有检索到的内容。 信息过滤: 过滤掉与查询主题无关或质量较低的检索结果。FILCO (Wang et al., 2023) 学习过滤上下文以增强 RAG。 RECOMP (Retrieve, Compress, Prepend) (Xu et al., 2023): 检索文档后，使用 LLM 将其压缩成摘要，然后将摘要添加到原始查询前，再输入给 LLM 生成答案。 2.2 优化生成过程 (Optimizing Generation) 除了优化检索，提升 LLM 在 RAG 框架下的生成能力也至关重要。\nLLM 微调 (Fine-tuning LLMs for RAG): 特定任务微调: 在包含“上下文-查询-答案”三元组的数据集上微调 LLM，使其更擅长利用检索到的上下文来回答问题。RETRO (Borgeaud et al., 2022) 就是一个在预训练阶段就进行检索增强的例子。RA-DIT (Lin et al., 2023b) 提出了一种检索增强的双指令微调方法。 指令微调 (Instruction Tuning): 使用包含各种指令的数据集进行微调，使 LLM 更好地遵循 RAG 流程中的指令（如“根据以下上下文回答问题”）。 提示工程 (Prompt Engineering for RAG): 结构化提示: 精心设计提示的结构，清晰地分隔查询、上下文和生成指令。 角色扮演: 指示 LLM 扮演特定角色（如“你是一个乐于助人的 AI 助手”）。 思维链提示 (Chain-of-Thought Prompting, CoT): 引导 LLM 在生成答案前先进行一步步的推理。IRCoT (Trivedi et al., 2022) 将 CoT 与检索交错进行。 处理噪声和矛盾: 在提示中加入如何处理上下文中噪声或矛盾信息的指令。 引用来源: 要求 LLM 在答案中明确指出信息的来源（来自哪个检索到的文本块）。 3. 模块化 RAG (Modular RAG) 模块化 RAG 将 RAG 系统视为一个由多个可插拔、可替换、可配置的模块组成的灵活框架。这种范式允许根据具体需求定制和优化 RAG 流程，是当前 RAG 发展的主流趋势。\n核心模块 (Core Modules):\n搜索模块 (Search Module): 负责执行实际的检索操作，可以集成多种检索引擎（向量搜索、关键词搜索、图搜索）和数据库。 记忆模块 (Memory Module): 存储和利用历史交互信息（如对话历史、用户偏好、先前检索结果）来个性化检索和生成过程，或支持多轮对话。Self-Mem (Cheng et al., 2023) 利用迭代的检索增强生成器构建记忆池。 融合模块 (Fusion Module): 当从多个来源或通过多种方法检索到信息时，融合模块负责将这些信息整合起来，例如对不同检索器的结果进行加权融合或选择最佳结果。RAG-Fusion (Raudaschl, 2023) 采用多查询策略并对结果进行重排序融合。 路由模块 (Routing Module): 根据查询的类型、意图或复杂性，将查询导向不同的处理路径、知识源或特定工具。例如，简单的事实查询可能直接由 LLM 回答，而复杂查询则需要经过多步检索和推理。 预测模块 (Predict Module): 在某些场景下，可以预测用户可能需要的下一步信息或操作，主动提供相关内容或建议。GenRead ([Yu et al., 2022]https://arxiv.org/abs/2209.10063)) 提出用 LLM 生成的上下文替代检索。 任务适配器 (Task Adapter Module): 针对特定的下游任务（如问答、摘要、对话、代码生成）调整 RAG 的行为、提示和输出格式。PROMPTAGATOR (Dai et al., 2022) 或 UPRISE (Cheng et al., 2023) 为特定任务自动检索提示。 交互模式 (Interaction Patterns):\n重写-检索-阅读 (Rewrite-Retrieve-Read): 先利用 LLM 重写用户查询（例如，使其更清晰、更具体，或分解为子查询），再进行检索，最后让 LLM 基于检索结果生成答案。 生成-阅读 (Generate-Read / Retrieve-then-Read with Generated Content): 先让 LLM 生成初步内容（如假设性答案、草稿、或与查询相关的关键词/概念），再将这些生成的内容用于检索相关信息，最后进行修正或完善。 迭代检索 (Iterative Retrieval): 多轮检索和生成，每一轮的输出可以作为下一轮检索的输入或引导，逐步逼近最终答案。这对于需要多步推理的复杂问题尤其有效。例如，第一轮检索可能提供初步线索，LLM 基于这些线索生成新的查询或细化方向，进行第二轮检索。ITER-RETGEN (Shao et al., 2023) 就是一个迭代检索和生成的例子。 自适应检索 (Adaptive Retrieval): Self-RAG (Asai et al., 2023): LLM 在生成过程中动态决定是否需要检索以及检索什么内容。它会生成“反思令牌 (reflection tokens)”（如 [Retrieve], [NoRetrieve], [Critique]）来评估当前生成内容的质量和是否需要外部知识。如果判断需要检索，它会生成检索查询，获取信息，然后继续生成。它还可以对检索到的段落进行批判性评估，判断其相关性。 FLARE (Forward-Looking Active REtrieval augmented generation) (Jiang et al., 2023): 一种主动检索方法。LLM 在生成答案时，会临时生成一个短期的未来句子。如果这个未来句子中包含低概率的词元（表明模型对这部分内容不确定），系统就会触发一次检索，查询与这个未来句子相关的信息，然后用检索到的信息重新生成。 SKR (Self-Knowledge Guided Retrieval Augmentation) (Wang et al., 2023): 利用 LLM 自身的知识判断是否需要检索。 AdaptiveRAG (Jeong et al., 2024): 通过分类器判断查询复杂度，动态选择检索策略。 递归/分层检索 (Recursive/Hierarchical Retrieval): 例如 RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) (Sarthi et al., 2024)，它通过递归地聚类文本块并生成摘要来构建一个树状的索引结构。查询时，可以从树的不同层级进行检索，从而在不同粒度上获取信息。 模块化 RAG 的设计理念使得 RAG 系统更加灵活、强大和易于维护，能够适应更广泛和复杂的应用场景。\n核心检索技术详解 检索是 RAG 系统中至关重要的一环，其目的是从海量数据中快速准确地找到与用户查询最相关的信息。现代 RAG 系统，特别是与 Azure AI Search 等企业级搜索服务结合时，会利用多种先进的检索技术。\n1. 全文检索 (Full-Text Search) - BM25 全文检索是一种经典的基于词频的稀疏向量检索方法。BM25 (Best Matching 25) 及其变种是目前最常用和最有效的全文检索算法之一 (Robertson and Zaragoza, 2009)。它通过计算查询词在文档中的词频 (Term Frequency, TF) 和逆文档频率 (Inverse Document Frequency, IDF) 来评估文档与查询的相关性。\nBM25 的评分公式通常如下： $$ \\text{score}(Q, D) = \\sum_{q_i \\in Q} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})} $$ 其中：\n\\( Q = \\{q_1, q_2, \\ldots, q_m\\} \\) 是用户查询，包含 \\( m \\) 个查询词。 \\( D \\) 是一个待评分的文档。 \\( \\text{IDF}(q_i) \\) 是查询词 \\( q_i \\) 的逆文档频率，衡量词语的稀有程度。计算方式通常为： $$ \\text{IDF}(q_i) = \\log\\left(1 + \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}\\right) $$ 其中 \\( N \\) 是文档总数，\\( n(q_i) \\) 是包含查询词 \\( q_i \\) 的文档数。 \\( f(q_i, D) \\) 是查询词 \\( q_i \\) 在文档 \\( D \\) 中的词频。 \\( |D| \\) 是文档 \\( D \\) 的长度（通常指词数）。 \\( \\text{avgdl} \\) 是数据集中所有文档的平均长度。 \\( k_1 \\) 和 \\( b \\) 是可调参数： \\( k_1 \\) (通常取值在 1.2 到 2.0 之间) 控制词频饱和度，即词频对相关性得分的非线性影响。较高的 \\( k_1 \\) 值意味着词频对得分的贡献更大，直到达到一个饱和点。 \\( b \\) (通常取值在 0.75 左右) 控制文档长度归一化的程度。当 \\( b=1 \\) 时，文档长度的影响被完全归一化；当 \\( b=0 \\) 时，不考虑文档长度。 BM25 的优点在于其简单、高效，并且在很多场景下能提供良好的基线性能。然而，它无法理解词语的语义相似性，例如无法识别同义词或近义词。\n2. 向量搜索 (Vector Search) 与近似最近邻 (ANN) 向量搜索通过将文本（查询和文档块）转换为稠密的数值向量（嵌入向量, embeddings），然后在向量空间中查找与查询向量最相似的文档向量。这种方法能够捕捉文本的语义信息。\n生成嵌入向量通常使用预训练的深度学习模型，如 BERT、Sentence-BERT、OpenAI Ada、BGE 等。相似度计算常用余弦相似度（已在“RAG 基础知识”中定义）。\n由于向量数据库可能包含数百万甚至数十亿个向量，进行精确的暴力搜索（称为 Flat 搜索或 K-Nearest Neighbors, KNN）计算成本非常高。因此，实际应用中广泛采用近似最近邻 (Approximate Nearest Neighbor, ANN) 算法，它们牺牲一定的精度来换取检索速度的大幅提升。常见的 ANN 算法（如 LSH, IVF）已在“RAG 基础知识”的索引部分提及，这里重点介绍 HNSW：\n分层可导航小世界图 (Hierarchical Navigable Small World, HNSW): HNSW 是一种基于图的 ANN 算法，因其出色的性能（高召回率和高查询速度）而被广泛应用 (Malkov and Yashunin, 2016)。 构建过程: HNSW 构建一个多层图结构。最底层包含所有数据点。每一层都是下一层的一个子集，层数越高，节点越稀疏，连接的边也越长（跳跃性更大）。在每一层中，节点都与其“近邻”相连，形成一个可导航的小世界网络。新节点插入时，会从顶层开始贪婪地搜索其最近邻，并在每一层找到的最近邻基础上，在下一层继续搜索，直到到达底层。然后，新节点会与找到的最近邻建立连接。 搜索过程: 查询时，从顶层的入口点（entry point）开始，贪婪地向着查询向量方向在当前层图上移动，直到找到局部最优（即当前节点的所有邻居中没有比它更接近查询向量的）。然后，将这个局部最优点作为下一层的入口点，重复此过程，直到到达最底层。在最底层找到的若干最近邻即为最终的近似结果。 HNSW 的关键参数包括 M（每个节点的最大出度）、efConstruction（构建图时动态候选列表的大小）和 efSearch（搜索时动态候选列表的大小）。 3. 混合搜索 (Hybrid Search) 与 RRF 混合搜索结合了全文检索（如 BM25）和向量搜索的优点。全文检索擅长匹配关键词，而向量搜索擅长理解语义。通过融合两者的结果，可以获得更全面和相关的搜索结果。Azure AI Search 等现代搜索引擎提供了强大的混合搜索功能。\n倒数排序融合 (Reciprocal Rank Fusion, RRF) 是一种常用的无参数结果融合算法，它根据文档在不同检索系统结果列表中的排名来计算最终得分 (Cormack et al., 2009)。 RRF 的评分公式为： $$ \\text{RRFScore}(D) = \\sum_{r \\in R_{systems}} \\frac{1}{k_{rrf} + \\text{rank}_r(D)} $$ 其中：\n\\( D \\) 是一个待评分的文档。 \\( R_{systems} \\) 是参与融合的检索系统（例如，一个 BM25 检索器和一个向量检索器）的集合。 \\( \\text{rank}_r(D) \\) 是文档 \\( D \\) 在检索系统 \\( r \\) 返回结果列表中的排名（从1开始）。如果文档 \\( D \\) 未被系统 \\( r \\) 检索到，则其排名可以视为无穷大，或者该项不参与求和。 \\( k_{rrf} \\) 是一个小常数（例如 60），用于平滑排名影响，减少排名靠前但仅被一个系统检索到的文档的权重过高问题。 RRF 的优点在于其简单性、无需训练参数，并且对不同检索系统的得分范围不敏感，只依赖于排名。\n4. 重排序 (Reranking) 重排序是在初步检索（召回阶段）之后，对候选文档列表进行更精细的排序，以提高最终结果的质量。召回阶段通常使用计算效率较高的方法（如 BM25、ANN）快速筛选出大量潜在相关的文档，而重排序阶段则可以使用计算成本较高但更精确的模型来处理这个较小的候选集。\n常见的重排序方法包括：\n基于交叉编码器的重排序 (Cross-encoder Reranking): 交叉编码器（如基于 BERT 的模型）将查询和每个候选文档拼接起来作为输入，共同编码后输出一个相关性得分。这种方式能够捕捉查询和文档之间更细致的交互信息，因此排序效果通常优于双编码器（用于向量检索）。但由于需要对每个查询-文档对进行独立编码，计算成本较高，不适合用于大规模召回。 基于 LLM 的重排序: 可以利用大型语言模型强大的理解和推理能力来进行重排序。例如，可以设计提示让 LLM 评估每个候选文档与查询的相关性，或者比较多个候选文档的优劣。 重排序是提升 RAG 系统性能的关键步骤，尤其是在处理复杂查询或需要高度相关上下文的场景中。\n图 RAG (GraphRAG) 传统的 RAG 方法主要依赖于文本块的独立检索，难以捕捉和利用文档集合中实体之间以及概念之间的复杂关系和全局上下文。GraphRAG (Edge et al., 2024; Peng et al., 2024) 通过构建和利用知识图谱（Knowledge Graphs, KGs）来解决这一问题，从而支持更深层次的理解和全局性问题的回答。\nFig. 8. The GraphRAG pipeline using LLM-derived graph indexing. (Image source: Edge et al., 2024)\n微软提出的 GraphRAG 核心流程包括：\n知识图谱构建 (Knowledge Graph Construction): 文本分块 (Text Chunking): 与标准 RAG 类似，首先将源文档分割成文本块。 实体与关系提取 (Entities \u0026amp; Relationships Extraction): 使用 LLM 从每个文本块中提取关键实体（如人物、组织、地点、概念等）及其属性，以及实体之间的关系。LLM 还会为这些实体和关系生成简短描述。 声明提取 (Claim Extraction): LLM 还可以被提示提取关于实体的关键事实性声明。 图谱生成 (Graph Generation): 将提取的实体作为图的节点，关系作为边，声明可以作为节点或边的属性。重复检测到的实体和关系可以进行合并，关系出现的频率可以作为边的权重。 图社区检测与摘要 (Graph Communities \u0026amp; Summaries): 社区检测 (Community Detection): 应用图社区检测算法（如 Leiden 算法 (Traag et al., 2019)将知识图谱划分为若干个紧密连接的实体社区。这个过程可以是分层的，即在每个社区内部递归地检测子社区。 社区摘要生成 (Community Summaries Generation): 使用 LLM 为每个社区（在不同层级上）生成摘要。叶子节点的社区摘要基于其内部实体、关系和声明的描述。更高层级社区的摘要则递归地整合其子社区的摘要和该层级新增的关联信息。这些社区摘要提供了对数据不同粒度下的全局性描述和洞察。 查询与答案生成 (Querying \u0026amp; Answering): 社区答案映射 (Map Community Answers): 当用户提出查询时，针对选定层级的每个社区摘要，LLM 会并行地生成一个与查询相关的局部答案，并给出一个该局部答案对回答原始问题的“帮助性”评分。 全局答案归约 (Reduce to Global Answer): 将所有局部答案根据其帮助性评分进行排序，然后迭代地将评分最高的局部答案添加到新的上下文中（直到达到 LLM 的上下文窗口限制），最后 LLM 基于这个整合的上下文生成最终的全局答案。 GraphRAG 的优势:\n显式关系利用: 能够利用实体之间明确定义的关系进行推理，回答更复杂、需要多步关系推断的问题。 多跳推理 (Multi-hop Reasoning): 支持通过一系列相连的关系找到答案。 可解释性增强: 检索到的图路径本身可以为答案提供一定的可解释性。 全局上下文理解: 能够回答需要理解整个语料库的“全局性”或“探索性”问题，例如“数据集中的主要主题是什么？”或“不同概念之间是如何关联的？”。 GraphRAG 的挑战:\n知识图谱的构建和维护成本较高，且质量难以保证。 如何有效地将图结构信息与 LLM 的输入/输出进行对齐和融合。 图检索算法的效率和可扩展性。 其他 GraphRAG 方法 除了微软提出的 GraphRAG 流程，还有其他利用图结构增强 RAG 的方法：\nG-Retriever (He et al., 2024): 一种用于文本图理解和问答的检索增强生成方法。 Agent-G (Lee et al., 2024): 这是一个代理框架，它动态地分配检索任务给专门的代理，这些代理可以分别从图知识库和非结构化文档中检索信息。Agent-G 包含一个“批评家模块”（Critic Module）来评估检索信息的质量和相关性，并通过反馈循环进行迭代优化。 GeAR (Graph-Enhanced Agent for Retrieval-Augmented Generation) (Shen et al., 2024): GeAR 通过图扩展技术（Graph Expansion）来增强传统检索器（如 BM25）的能力。它将图结构数据整合到检索过程中，使得系统能够捕捉实体间的复杂关系和依赖，从而更好地处理多跳查询。 这些方法都强调了图结构在 RAG 中的重要性，尤其是在需要复杂推理和多源信息整合的场景下。\nAgentic RAG Agentic RAG (Singh et al., 2025) 是一种更高级的 RAG 范式，它将自主代理（AI Agents）集成到 RAG 流程中。这些代理能够进行动态决策、规划、使用工具并进行协作，从而实现更灵活、更智能的检索和生成过程。\nFig. 9. Overview of AI Agent components. (Image source: Singh et al., 2025)\n代理 RAG 的核心在于赋予 RAG 系统更强的自主性和适应性，使其能够处理更复杂、动态和多步骤的任务。其关键的代理模式包括：\n反思 (Reflection): 代理能够评估和迭代改进其自身的输出或中间结果。通过自我批评和反馈机制，代理可以识别错误、不一致之处，并进行修正。Self-Refine (Madaan et al., 2023), Reflexion (Shinn et al., 2023), 和 CRITIC (Gou et al., 2024) 是相关研究。 规划 (Planning): 代理能够将复杂任务分解为更小、可管理的子任务，并规划执行顺序。这对于需要多跳推理或动态调整策略的场景至关重要。 工具使用 (Tool Use): 代理可以调用外部工具、API 或计算资源来扩展其能力，例如执行代码、访问数据库、进行网络搜索等。Toolformer (Schick et al., 2024) 和 GraphToolformer (Zhang, 2023b) 是让 LLM 学会使用工具的例子。 多代理协作 (Multi-Agent Collaboration): 多个具有不同专长或角色的代理可以协同工作，共同完成一个复杂任务。它们可以通信、共享中间结果，实现任务的并行处理和专业化分工。AutoGen (Wu et al., 2023) 和 CrewAI 是支持多代理协作的框架。 基于这些模式，Agentic RAG 可以有多种架构:\n单代理 RAG (Single-Agent Agentic RAG: Router) 由一个中心化的代理负责管理整个 RAG 流程，包括接收用户查询、分析查询意图、选择合适的知识源（如结构化数据库、向量数据库、网络搜索）、执行检索、整合信息并驱动 LLM 生成答案。这种架构相对简单，适用于任务明确或工具集有限的场景。\n多代理 RAG (Multi-Agent Agentic RAG) 将 RAG 流程中的不同职责分配给多个专门的代理。例如，可以有专门负责查询理解的代理、负责从不同类型数据源（SQL 数据库、文档库、API）检索信息的检索代理、负责信息融合的代理以及负责最终答案生成的代理。这些代理并行工作并通过协调器进行协作。这种架构具有更好的模块化和可扩展性。\n分层代理 RAG (Hierarchical Agentic RAG) 代理被组织成一个层次结构。高层代理负责战略规划和任务分解，并将子任务分配给低层代理执行。低层代理专注于特定的检索或处理任务，并将结果汇报给高层代理进行整合与综合。这种架构适合处理非常复杂、需要多层次决策的查询。\n纠正性 RAG (Corrective RAG / CRAG) CRAG (Yan et al., 2024a) 引入了对检索结果进行自我纠正的机制。它包含一个评估器来判断检索到的文档的相关性。如果文档相关性高，则直接用于生成；如果相关性低或不确定，系统会触发额外的操作，如从其他来源（如网络）进行补充检索，或者对检索到的文档进行分解和筛选，以提高最终生成答案的鲁棒性和准确性。\n自适应 RAG (Adaptive RAG) 自适应 RAG (Jeong et al., 2024) 根据输入查询的复杂性动态调整其检索和生成策略。例如，对于简单的事实性问题，可能直接由 LLM 回答（无需检索）；对于需要少量上下文的问题，执行单次检索；对于复杂问题，则可能启动多轮迭代检索或更复杂的推理链。通常会有一个轻量级的分类器或 LLM 本身来判断查询的类型并选择合适的处理路径。\n多模态 RAG (Multimodal RAG) 随着信息的多样化，仅仅处理文本数据已不能满足许多应用场景的需求。多模态 RAG (Multimodal RAG) (Abootorabi et al., 2025; Zhao et al., 2023a) 将 RAG 的能力扩展到多种数据模态，如文本、图像、音频、视频等。它旨在通过从包含多种类型数据的外部知识库中检索和融合信息，来增强多模态大型语言模型 (MLLMs) 的生成能力。\nFig. 10. Overview of the Multimodal Retrieval-Augmented Generation (RAG) pipeline. (Image source: Abootorabi et al., 2025)\n任务定义 给定一个多模态查询 \\( q \\) 和一个包含多种模态文档 \\( D = \\{d_1, d_2, \\ldots, d_n\\} \\) 的语料库，多模态 RAG 的目标是生成一个相关的多模态响应 \\( r \\)。 每个文档 \\( d_i \\) 具有其自身的模态 \\( M_{d_i} \\)。首先，使用特定于模态的编码器 \\( \\text{Enc}_{M_{d_i}}(\\cdot) \\) 将每个文档 \\( d_i \\) 编码为一个向量表示 \\( z_i \\)： $$ z_i = \\text{Enc}_{M_{d_i}}(d_i) $$ 这些编码器通常旨在将不同模态的信息映射到一个共享的语义空间，以便进行跨模态对齐和比较。 然后，检索模型 \\( R \\) 根据查询 \\( q \\)（其编码表示为 \\( e_q \\)）与每个编码后的文档 \\( z_i \\) 之间的相关性评分 \\( s(e_q, z_i) \\) 来选择最相关的文档子集 \\( X \\)： $$ X = \\{d_i \\mid s(e_q, z_i) \\geq \\tau_{M_{d_i}}\\} $$ 其中 \\( \\tau_{M_{d_i}} \\) 是针对模态 \\( M_{d_i} \\) 的相关性阈值。 最后，生成模型 \\( G_{model} \\) 利用原始查询 \\( q \\) 和检索到的多模态上下文 \\( X \\) 来生成最终的多模态响应 \\( r \\)： $$ r = G_{model}(q, X) $$关键技术与创新 多模态 RAG 的实现涉及以下几个关键环节的技术创新 (Abootorabi et al., 2025)\n多模态数据处理与索引 (Multimodal Data Processing and Indexing)\n对不同模态的数据进行预处理和特征提取。例如，图像可用视觉编码器（如 ViT、CLIP Image Encoder (Radford et al., 2021)）提取特征向量；音频可先用 ASR 转文本，或利用音频编码器（如 CLAP (Wu et al., 2023a) 提取声学向量。 将不同模态的特征向量存储在多模态向量数据库中，或为每种模态维护独立索引。MegaPairs (Zhou et al., 2024a) 是一个大规模多模态配对数据集，可用于训练通用检索器。 多模态检索策略 (Multimodal Retrieval Strategy)\n高效搜索与相似性检索: 依赖于把不同模态输入映射到统一嵌入空间的模型，如基于 CLIP (Radford et al., 2021) 或 BLIP (Li et al., 2022a)。利用 MIPS（如 ScaNN、FAISS）进行高效检索。\n模态中心检索 (Modality‑Centric Retrieval):\n文本中心: 传统基础，使用 BM25、DPR、Contriever (Izacard et al., 2022) 等。 视觉中心: 直接基于图像向量检索，如 EchoSight、ImgRet；CIR 模型支持多图组合查询。 视频中心: 融合时序动态信息，例如 iRAG (Arefeen et al., 2024) 的增量检索、Video‑RAG (Luo et al., 2024b) 借助 OCR/ASR 辅助文本。 文档检索与布局理解: 处理含文本、图像、表格、布局信息的整页文档，如 ColPali (Faysse et al., 2024)、DocLLM (Wang et al., 2024a)。 重排序与选择策略:\n优化样本选择: 采用多步检索，结合有监督 / 无监督的候选筛选。 相关性评分评估: 使用 SSIM、NCC、BERTScore 等多模态相似度度量进行重排序。 过滤机制: 通过硬负例挖掘、一致性过滤等方法去除不相关结果。 多模态融合机制 (Multimodal Fusion Mechanisms)\n分数融合与对齐 (Score Fusion and Alignment): 将不同模态的检索得分或特征进行融合。例如，将文本、表格、图像转换为统一的文本格式后进行评分，或将图像和查询嵌入到共享的 CLIP 空间中。REVEAL (Hu et al., 2023a) 将检索分数注入注意力层。 基于注意力的机制 (Attention‑Based Mechanisms): 使用交叉注意力等机制动态加权不同模态间的交互，以支持特定任务的推理。例如，RAMM (Yuan et al., 2023) 使用双流共同注意力 Transformer。 统一框架与投影 (Unified Frameworks and Projections): 将多模态输入整合为一致的表示。例如，M3DocRAG (Cho et al., 2024) 将多页文档展平为单个嵌入张量；SAM‑RAG (Zhai, 2024) 通过为图像生成标题，将多模态输入转换为单模态文本。 多模态增强技术 (Multimodal Augmentation Techniques)\n上下文丰富 (Context Enrichment): 通过添加额外的上下文元素（文本块、图像标记、结构化数据）来增强检索知识的相关性。\n自适应与迭代检索 (Adaptive and Iterative Retrieval):\n自适应检索： 根据查询复杂度动态调整检索策略。SKURG (Yang et al., 2023) 根据查询复杂度决定检索跳数；OmniSearch (Li et al., 2024b) 使用自适应检索代理分解复杂问题。 迭代检索： 通过多轮检索并结合先前迭代的反馈来优化结果。IRAMIG (Liu et al., 2024b) 根据检索到的内容动态更新查询。 多模态生成技术 (Multimodal Generation Techniques)\n上下文学习 (In‑Context Learning, ICL)： 利用检索到的多模态内容作为少样本示例，无需重新训练即可增强 MLLM (如 GPT‑4V (OpenAI, 2023), LLaVA (Liu et al., 2023a), Gemini (暂无 arXiv 预印本)) 的推理能力。Raven (Rao et al., 2024) 提出 Fusion‑in‑Context Learning。 推理 (Reasoning)： 使用结构化推理技术（如 Chain‑of‑Thought）将复杂推理分解为顺序步骤。RAGAR (Khaliq et al., 2024) 引入 Chain of RAG 和 Tree of RAG。 指令调优 (Instruction Tuning)： 针对特定应用对生成组件进行微调或指令调优。RA‑BLIP (Ding et al., 2024b) 利用 InstructBLIP 的 QFormer 架构。 来源归属与证据透明性 (Source Attribution and Evidence Transparency)： 确保生成的内容能追溯到其原始来源。VISA (Ma et al., 2024b) 生成带有视觉来源归属的答案。 训练策略与损失函数 (Training Strategies and Loss Functions):\n对齐 (Alignment): 主要使用对比学习（Contrastive Learning）来提高表示质量，常用损失函数为 InfoNCE loss： $$ \\mathcal{L}_{\\text{InfoNCE}} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j) / \\tau)}{\\sum_{k=1}^{K} \\exp(\\text{sim}(z_i, z_k) / \\tau)} $$ 其中 \\( z_i, z_j \\) 是正样本对的嵌入，\\( z_k \\) 是负样本嵌入，\\( \\tau \\) 是温度参数。 生成 (Generation): 文本生成：通常使用交叉熵损失 (Cross-Entropy Loss)。 图像生成：常用生成对抗网络 (GANs) 或扩散模型 (Diffusion Models)。GAN 的损失函数包括二元交叉熵、Wasserstein 损失等。 $$ \\mathcal{L}_D = -\\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] - \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G_{model}(z)))] $$ $$ \\mathcal{L}_G = \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G_{model}(z)))] $$ 扩散模型通常使用均方误差 (MSE) 损失进行噪声预测。 鲁棒性与噪声管理 (Robustness and Noise Management): 通过在训练中注入不相关结果、使用渐进式知识蒸馏、噪声注入训练（如添加硬负例、高斯噪声）等方法来提高模型对噪声输入的鲁棒性。 多模态 RAG 通过整合来自不同来源和类型的知识，极大地丰富了 LLM 的上下文理解和生成能力，为构建更智能、更全面的 AI 系统奠定了基础。例如，在 SharePoint 文档库中，可能包含 PPT、Word 文档、图片等，多模态 RAG 可以帮助用户基于这些混合内容进行问答。\nRAG 系统的评估 评估 RAG 系统的性能是一个多维度的问题，需要同时考虑检索质量和生成质量。\n1. Ragas 框架核心指标 Ragas (Retrieval Augmented Generation Assessment) (Es et al., 2023) 是一个专注于 RAG 系统自动化、主要依赖 LLM 进行无参考或有参考评估的框架。它提出了一系列指标，用于从不同维度评估 RAG 流水线。\nRagas 关注以下核心质量维度：\n忠实度 (Faithfulness):\n定义: 衡量生成的答案 \\( A \\) 是否完全基于给定的上下文 \\( C \\) 得出，即答案中的声明是否都能从上下文中推断出来。这个指标对于避免幻觉至关重要。 计算方法 (Ragas): 使用 LLM 从生成的答案 \\( A \\) 中提取一组声明（statements）\\( S = \\{s_1, s_2, \\ldots, s_m\\} \\)。 对于每个声明 \\( s_i \\in S \\)，再次使用 LLM 验证该声明是否可以从上下文 \\( C \\) 中推断出来。得到一个验证为真的声明集合 \\( V \\)。 忠实度得分 \\( F_{faithfulness} \\) 计算为： $$ F_{faithfulness} = \\frac{|V|}{|S|} $$ 其中 \\( |V| \\) 是被验证为真实的声明数量，\\( |S| \\) 是从答案中提取的声明总数。得分范围为 \\( [0, 1] \\)，越高越好。 答案相关性 (Answer Relevance):\n定义: 衡量生成的答案 \\( A \\) 与原始问题 \\( Q \\) 的相关程度。它惩罚不完整或包含冗余信息的答案。 计算方法 (Ragas): 给定生成的答案 \\( A \\)，使用 LLM 生成 \\( n_{q'} \\) 个可能的反向问题 \\( Q' = \\{q'_1, q'_2, \\ldots, q'_{n_{q'}}\\} \\)，这些问题应该可以由答案 \\( A \\) 来回答。 计算每个生成的反向问题 \\( q'_j \\) 与原始问题 \\( Q \\) 之间的嵌入相似度 \\( \\text{sim}(Q, q'_j) \\)（例如使用 OpenAI text-embedding-ada-002 计算余弦相似度）。 答案相关性得分 \\( \\text{AR} \\) 计算为这些相似度的平均值： $$ \\text{AR} = \\frac{1}{n_{q'}} \\sum_{j=1}^{n_{q'}} \\text{sim}(Q, q'_j) $$ 得分范围为 \\( [0, 1] \\)，越高越好。 上下文精确率 (Context Precision) / 上下文相关性 (Context Relevance):\n定义: 衡量检索到的上下文 \\( C \\) 中包含了多少回答问题 \\( Q \\) 所必需的信息，同时惩罚包含过多无关或冗余信息的情况。Ragas 中更侧重于上下文的“精确性”，即上下文中与问题相关的句子比例。 计算方法 (Ragas 中的 Context Precision): 给定问题 \\( Q \\) 和检索到的上下文 \\( C \\)，使用 LLM 从上下文 \\( C \\) 中提取出对回答问题 \\( Q \\) 至关重要的句子集合 \\( S_{ext} \\)。 上下文精度 (Context Precision) 得分 \\( \\text{CR}_{precision} \\) 计算为： $$ \\text{CR}_{precision} = \\frac{|S_{ext}|}{N_{sent}(C)} $$ 其中 \\( |S_{ext}| \\) 是从上下文 \\( C \\) 中提取的关键句数量，\\( N_{sent}(C) \\) 是上下文 \\( C \\) 中的总句数。得分范围为 \\( [0, 1] \\)，越高越好。 上下文召回率 (Context Recall): (需要真实答案 Ground Truth)\n定义: 衡量检索到的上下文 \\( C \\) 是否包含了回答问题 \\( Q \\) 所需的所有真实答案 \\( GT \\) 中的信息。 计算方法 (Ragas): 对于真实答案 \\( GT \\) 中的每个句子，判断其是否可以从检索到的上下文 \\( C \\) 中归因得到。上下文召回率 \\( \\text{CR}_{recall} \\) 是可归因的真实答案句子数 \\( |S_{GT\\_attributable}| \\) 与真实答案总句数 \\( |S_{GT}| \\) 的比率。 $$ \\text{CR}_{recall} = \\frac{|S_{GT\\_attributable}|}{|S_{GT}|} $$ 答案语义相似度 (Answer Semantic Similarity): (需要真实答案 Ground Truth)\n定义: 衡量生成的答案与标准答案之间的语义相似度。 计算方法 (Ragas): 通常使用嵌入模型计算两个答案向量的余弦相似度。 \\[ \\text{Answer Semantic Similarity} = \\text{sim}(E(A_{\\text{generated}}), E(A_{\\text{GT}})) \\] 答案正确性 (Answer Correctness): (需要真实答案 Ground Truth)\n定义: 衡量生成的答案在事实层面上的正确性，与标准答案的一致性。 计算方法 (Ragas): 结合事实相似性和语义相似性进行评估，判断生成答案与标准答案是否表达相同的事实。 2. 其他检索质量评估指标 命中率 (Hit Rate): 衡量检索到的文档中是否至少包含一个正确答案或相关信息。 平均倒数排名 (Mean Reciprocal Rank, MRR): 评估检索结果中第一个相关文档的排名。对于一组查询 \\( Q_{set} \\)，MRR 计算如下： \\[ \\text{MRR} = \\frac{1}{|Q_{set}|} \\sum_{i=1}^{|Q_{set}|} \\frac{1}{\\text{rank}_i} \\] 其中 \\( \\text{rank}_i \\) 是第 \\( i \\) 个查询的第一个相关文档的排名。 归一化折扣累积增益 (Normalized Discounted Cumulative Gain, NDCG@k): 考虑检索结果的排序和相关性等级。 \\[ \\text{DCG}@k = \\sum_{i=1}^{k} \\frac{2^{\\text{rel}_i} - 1}{\\log_2(i+1)} \\quad ; \\quad \\text{NDCG}@k = \\frac{\\text{DCG}@k}{\\text{IDCG}@k} \\] 其中 \\( \\text{rel}_i \\) 是排名第 \\( i \\) 位的文档的相关性得分，\\( \\text{IDCG}@k \\) 是理想排序下的 DCG@k 值。 3. 其他生成质量评估指标 BLEU (Bilingual Evaluation Understudy) (Papineni et al., 2002): 基于 N-gram 精确率。 \\[ \\text{BLEU} = \\text{BP} \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right) \\] ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin, 2004): 基于 N-gram 召回率。 \\[ \\text{ROUGE-N} = \\frac{\\sum_{S \\in \\{\\text{RefSummaries}\\}} \\sum_{\\text{gram}_n \\in S} \\text{Count}_{\\text{match}}(\\text{gram}_n)}{\\sum_{S \\in \\{\\text{RefSummaries}\\}} \\sum_{\\text{gram}_n \\in S} \\text{Count}(\\text{gram}_n)} \\] METEOR (Metric for Evaluation of Translation with Explicit ORdering) (Banerjee and Lavie, 2005): 基于对齐的词级别匹配。 BERTScore (Zhang et al., 2019): 计算嵌入之间的余弦相似度。 困惑度 (Perplexity, PPL): 衡量语言模型对其生成文本的自信程度。 \\[ \\text{PPL}(W) = P(w_1 w_2 \\dots w_N)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P(w_1 w_2 \\dots w_N)}} \\] 4. 端到端评估与基准 RAGAS (Retrieval Augmented Generation Assessment) (Es et al., 2023)：如上所述，提供了一套自动化的评估指标。 ARES (Automated RAG Evaluation System) (Saad‑Falcon et al., 2023)：使用 LLM 生成的合成数据进行评估。 TruLens (TruLens Docs, 2024)：关注上下文相关性、无害性/忠实度和答案相关性。 RGB (RAG Benchmark) (Chen et al., 2023c)：评估噪声鲁棒性、负例拒绝、信息整合和反事实鲁棒性。 RECALL (Robustness Evaluation of Counterfactual Knowledge in RAG for LLMs) (Liu et al., 2023c)：专注于反事实知识的鲁棒性。 CRUD‑RAG (Lyu et al., 2024)：全面的中文 RAG 基准。 BEIR (Benchmarking IR) (Thakur et al., 2021)：通用信息检索基准。 KILT (Knowledge Intensive Language Tasks) (Petroni et al., 2020)：知识密集型任务基准。 MTEB (Massive Text Embedding Benchmark) (Muennighoff et al., 2022)：文本嵌入模型基准。 Ragas 等工具通过这些指标，使得开发者能够在没有大量人工标注数据的情况下，快速迭代和优化 RAG 系统的各个组件。\nRAG 的应用场景 RAG 技术因其能够结合外部知识源的优势，在多种 NLP 应用中展现出巨大潜力 :\n开放域问答 (Open‑Domain Question Answering): 如 DrQA (Chen et al., 2017)。 领域特定问答 (Domain‑Specific QA): 医疗 (Zhu et al., 2024c ‑ REALM for EHRs)、法律 (Cui et al., 2023 ‑ ChatLaw)、金融 (Zhang et al., 2023d ‑ AlphaFin)。 对话系统/聊天机器人 (Conversational AI/Chatbots): 如 BlenderBot 3 (Shuster et al., 2022)。 内容生成 (Content Generation): 生成报告、摘要等。 代码生成与辅助 (Code Generation and Assistance): 如 DocPrompting (Zhou et al., 2022b)。 推荐系统 (Recommender Systems): 如 CoRAL (Wu et al., 2024a)。 AI 智能体 (AI Agents): 作为 Agent 获取外部知识的核心能力。 多模态应用 (Multimodal Applications): 视觉问答、图像/视频字幕生成、文本到图像/视频生成。 科学研究 (AI for Science): 分子发现 (Li et al., 2023a ‑ MolReGPT)、蛋白质研究。 结论 检索增强生成（RAG）作为一项关键技术，有效地结合了大型语言模型的强大生成能力与外部知识库的广度和实时性。从朴素 RAG 到高级 RAG、模块化 RAG，再到针对特定需求的图 RAG、代理 RAG 和多模态 RAG，RAG 的范式不断演进，以应对日益复杂的应用场景。核心检索技术如 BM25、基于 HNSW 的向量搜索、RRF 混合搜索以及重排序机制，为 RAG 提供了坚实的基础。同时，像 Ragas 这样的评估工具也为 RAG 系统的迭代优化提供了重要支持。\n尽管 RAG 已经取得了显著的成就，但在检索质量、上下文处理、生成可控性、复杂推理、多模态融合、评估方法以及系统效率等方面仍面临挑战。未来的研究将继续围绕这些挑战展开，致力于构建更智能、更鲁棒、更高效、更可信的 RAG 系统，从而在更广泛的领域释放 AI 的潜力。\n参考文献 xxx\n","permalink":"https://syhya.github.io/zh/posts/2025-02-03-rag/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e注意\u003c/strong\u003e: 本文正在更新中，内容只是\u003cstrong\u003e草稿版本，并不完善，后续会有较大变动\u003c/strong\u003e。请随时关注最新版本。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e大型语言模型（Large Language Models, LLMs）在自然语言处理领域取得了革命性的进展，展现出强大的文本理解和生成能力。然而，LLMs 并非完美无缺，它们仍然面临着一些固有的挑战，例如：\u003c/p\u003e","title":"检索增强生成 (RAG) 技术综述（长期更新中）"},{"content":"引言 在深度学习中，网络架构的设计对模型的性能和训练效率有着至关重要的影响。随着模型深度的增加，训练深层神经网络面临诸多挑战，如梯度消失和梯度爆炸问题。为了应对这些挑战，残差连接和各种归一化方法被引入并广泛应用于现代深度学习模型中。本文将首先介绍残差连接和两种架构，分别是 pre-norm 和 post-norm。随后介绍四种常见的方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析为何当前主流的大模型倾向于采用 RMSNorm 与 Pre-Norm 结合的架构。\n残差连接 残差连接（Residual Connection） 是深度神经网络中的一项关键创新，它构成了残差网络（ResNet）(He, et al., 2015) 的核心。残差连接是一种重要的架构设计，目的是缓解深层网络训练中的梯度消失问题，并促进信息在网络中的流动。它通过引入快捷路径（Shortcut/Skip Connection），允许信息直接从浅层传递到深层，从而增强模型的表达能力和训练稳定性。\nFig. 1. Residual learning: a building block. (Image source: He, et al., 2015)\n在标准的残差连接中，输入 $x_l$ 经过一系列变换函数 $\\text{F}(\\cdot)$ 后，与原始输入 $x_l$ 相加，形成输出 $x_{l+1}$：\n$$ x_{l+1} = x_l + \\text{F}(x_l) $$其中：\n$x_l$ 是第 $l$ 层的输入。 $\\text{F}(x_l)$ 表示由一系列非线性变换（例如卷积层、全连接层、激活函数等）组成的残差函数。 $x_{l+1}$ 是第 $l+1$ 层的输出。 使用残差连接的结构有以下几个优势：\n缓解梯度消失： 通过快捷路径直接传递梯度，有效减少梯度在深层网络中的衰减，从而更容易训练更深的模型。 促进信息流动： 快捷路径允许信息更自由地在网络层之间流动，有助于网络学习更复杂的特征表示。 优化学习过程： 残差连接使得损失函数曲面更加平滑，优化模型的学习过程，使其更容易收敛到较好的解。 提升模型性能： 在图像识别、自然语言处理等多种深度学习任务中，使用残差连接的模型通常表现出更优越的性能。 Pre-Norm 与 Post-Norm 在讨论归一化方法时，Pre-Norm 和 Post-Norm 是两个关键的架构设计选择，尤其在 Transformer 模型中表现突出。以下将详细探讨两者的定义、区别及其对模型训练的影响。\n定义 Fig. 2. (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer. (Image source: Xiong, et al., 2020)\n从上图我可以直观看到，Post-Norm 和 Pre-Norm 的主要区别在于归一化层的位置：\nPost-Norm：传统的 Transformer 架构中，归一化层（如 LayerNorm）通常位于残差连接之后。\n$$ \\text{Post-Norm}: \\quad x_{l+1} = \\text{Norm}(x_l + \\text{F}(x_l)) $$ Pre-Norm：将归一化层放在残差连接之前。\n$$ \\text{Pre-Norm}: \\quad x_{l+1} = x_l + \\text{F}(\\text{Norm}(x_l)) $$ 对比分析 特性 Post-Norm Pre-Norm 归一化位置 残差连接之后 残差连接之前 梯度流动 可能导致梯度消失或爆炸，尤其在深层模型中 梯度更稳定，有助于训练深层模型 训练稳定性 难以训练深层模型，需要复杂的优化技巧 更容易训练深层模型，减少对学习率调度的依赖 信息传递 保留了原始输入的特性，有助于信息传递 可能导致输入特征的信息被压缩或丢失 模型性能 在浅层模型或需要强正则化效果时表现更优 在深层模型中表现更好，提升训练稳定性和收敛速度 实现复杂度 实现较为直接，但训练过程可能需要更多调优 实现简单，训练过程更稳定 Pre-Norm 和 Post-Norm 在模型训练中的差异可以从梯度反向传播的角度理解：\nPre-Norm：归一化操作在前，梯度在反向传播时能够更直接地传递到前面的层，减少了梯度消失的风险。但这也可能导致每一层的实际贡献被弱化，降低模型的实际有效深度。\nPost-Norm：归一化操作在后，有助于保持每一层的输出稳定，但在深层模型中，梯度可能会逐层衰减，导致训练困难。\nDeepNet (Wang, et al., 2022) 论文表明 Pre-Norm 在极深的 Transformer 模型中能够有效训练，而 Post-Norm 难以扩展到如此深度。\n归一化方法 在深度学习中，归一化方法种类繁多，不同的方法在不同的应用场景下表现各异。下面将详细介绍四种常见的归一化方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析它们的优劣势及适用场景。\nBatch Normalization Batch Normalization (Ioffe, et al., 2015) 旨在通过标准化每一批次的数据，使其均值为0，方差为1，从而缓解内部协变量偏移（Internal Covariate Shift）的问题。其数学表达式如下：\n$$ \\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta $$其中：\n$x_i$ 为输入向量中的第 $i$ 个样本。 $\\mu_{\\text{B}}$ 为当前批次的均值： $$ \\mu_{\\text{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i $$ 其中 $m$ 为批次大小。 $\\sigma_{\\text{B}}^2$ 为当前批次的方差： $$ \\sigma_{\\text{B}}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\text{B}})^2 $$ $\\epsilon$ 为一个极小的常数，用于防止分母为零。 $\\gamma$ 和 $\\beta$ 为可学习的缩放和平移参数。 优势：\n加速训练：通过标准化加速模型的收敛速度。 正则化效果：在一定程度上减少过拟合，降低了对 Dropout 等正则化技术的依赖。 减轻梯度消失问题：有助于缓解梯度消失，提高深层网络的训练效果。 缺点：\n对小批次不友好：在批次大小较小时，均值和方差的估计可能不稳定，影响归一化效果。 依赖批次大小：需要较大的批次才能获得良好的统计量估计，限制了在某些应用场景中的使用。 在某些网络结构中应用复杂：如循环神经网络（RNN），需要特殊处理以适应时间步的依赖性。 Layer Normalization Layer Normalization (Ba, et al., 2016) 通过在特征维度上进行归一化，使得每个样本的特征具有相同的均值和方差。其数学表达式如下：\n$$ \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta $$其中：\n$x$ 为输入向量。 $\\mu_{\\text{L}}$ 为特征维度的均值： $$ \\mu_{\\text{L}} = \\frac{1}{d} \\sum_{i=1}^{d} x_i $$ 其中 $d$ 为特征维度的大小。 $\\sigma_{\\text{L}}^2$ 为特征维度的方差： $$ \\sigma_{\\text{L}}^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu_{\\text{L}})^2 $$ $\\epsilon$ 为一个极小的常数，用于防止分母为零。 $\\gamma$ 和 $\\beta$ 为可学习的缩放和平移参数。 优势：\n对批次大小不敏感：适用于小批次或动态批次大小的场景，尤其在序列模型中表现优异。 适用于多种网络结构：在循环神经网络（RNN）和 Transformer 等模型中表现良好。 简化实现：无需依赖批次统计量，简化了在分布式训练中的实现。 缺点：\n计算量较大：相比 BatchNorm，计算均值和方差的开销稍高。 可能不如 BatchNorm 提升训练速度：在某些情况下，LayerNorm 的效果可能不如 BatchNorm 显著。 Weight Normalization Weight Normalization (Salimans, et al., 2016) 通过重新参数化神经网络中的权重向量来解耦其模长（norm）和方向（direction），从而简化优化过程并在一定程度上加速训练。其数学表达式如下：\n$$ w = \\frac{g}{\\lVert v \\rVert} \\cdot v $$$$ \\text{WeightNorm}(x) = w^T x + b $$其中：\n$w$ 是重新参数化后的权重向量。 $g$ 为可学习的标量缩放参数。 $v$ 为可学习的方向向量（与原始 $w$ 维度相同）。 $\\lVert v \\rVert$ 表示 $v$ 的欧几里得范数。 $x$ 为输入向量。 $b$ 为偏置项。 优势：\n简化优化目标：单独控制权重的模长与方向，有助于加速收敛。 稳定训练过程：在某些情况下，可减少梯度爆炸或消失问题。 实现不依赖批次大小：与输入数据的批次无关，适用性更广。 缺点：\n实现复杂度：需要对网络层进行重新参数化，可能带来额外的实现成本。 与其他归一化方法结合时需谨慎：如与 BatchNorm、LayerNorm 等同用时，需要调试和实验来确定最佳组合。 RMS Normalization RMS Normalization (Zhang, et al., 2019) 是一种简化的归一化方法，通过仅计算输入向量的均方根（RMS）进行归一化，从而减少计算开销。其数学表达式如下：\n$$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$其中：\n$x$ 为输入向量。 $d$ 为特征维度的大小。 $\\epsilon$ 为一个极小的常数，用于防止分母为零。 $\\gamma$ 为可学习的缩放参数。 优势：\n计算效率高：相比 LayerNorm 需要计算均值和方差，RMSNorm 仅需计算均方根，减少了计算开销。 训练稳定性：通过归一化输入，提升了模型的训练稳定性，使其在更大的学习率下仍能稳定训练。 资源优化：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。 简化实现：RMSNorm 的实现相对简单，便于在复杂模型中集成和优化，减少了工程实现的复杂性。 缺点：\n信息损失：仅使用均方根进行归一化，可能丢失部分信息，如均值信息。 适用性有限：在某些任务中，可能不如 BatchNorm 或 LayerNorm 表现优异。 代码示例 可以参考normalization.py\n归一化方法对比 以下两个表格对比了 BatchNorm、LayerNorm、WeightNorm 和 RMSNorm 四种归一化方法的主要特性：\nBatchNorm vs. LayerNorm 特性 BatchNorm (BN) LayerNorm (LN) 计算的统计量 批量的均值和方差 每个样本的均值和方差 操作维度 对批量数据的所有样本进行归一化 对每个样本的所有特征进行归一化 适用场景 适用于大批量数据，卷积神经网络 (CNN) 适用于小批量或序列数据，RNN 或 Transformer 是否依赖批量大小 强依赖批量大小 不依赖批量大小，适用于小批量或单样本任务 可学习的参数 缩放参数 $\\gamma$ 和平移参数 $\\beta$ 缩放参数 $\\gamma$ 和平移参数 $\\beta$ 公式 $\\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta$ $\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta$ 计算复杂度 需要计算批量的均值和方差 需要计算每个样本的均值和方差 使用示例 CNN, Vision Transformers RNN, Transformer, NLP WeightNorm vs. RMSNorm 特性 WeightNorm (WN) RMSNorm (RMS) 计算的统计量 分解权重向量的模长和方向 每个样本的均方根 (RMS) 操作维度 针对权重向量的维度进行重新参数化 对每个样本的所有特征进行归一化 适用场景 适用于需要更灵活的权重控制或加速收敛的场景 适用于需要高效计算的任务，如 RNN 或 Transformer 是否依赖批量大小 不依赖批量大小，与输入数据的维度无关 不依赖批量大小，适用于小批量或单样本任务 可学习的参数 标量缩放 $g$ 和方向向量 $v$ 缩放参数 $\\gamma$ 公式 $\\text{WeightNorm}(x) = w^T x + b$ $\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma$ 计算复杂度 参数重新分解和更新，额外开销较小，但需修改网络层实现 只需计算每个样本的均方根，计算较为高效 使用示例 深度网络的全连接层、卷积层等，提升训练稳定性和收敛速度 Transformer, NLP, 高效序列任务 通过上述对比，可以看出四种归一化方法各有优劣：\nBatchNorm 在大批量数据和卷积神经网络中表现优异，但对小批量敏感。 LayerNorm 适用于各种批量大小，尤其是在 RNN 和 Transformer 中效果显著。 WeightNorm 通过重新参数化权重向量，在一定程度上简化了优化过程并加速收敛。 RMSNorm 则在需要高效计算的场景下提供了一种轻量级的替代方案。 为什么当前主流 LLM 都使用 Pre-Norm 和 RMSNorm？ 近年来，随着大规模语言模型（LLM）如 GPT、LLaMA 和 Qwen 系列等的兴起，RMSNorm 和 Pre-Norm 已成为这些模型的标准选择。\nRMSNorm 的优势 Fig. 3. RMSNorm vs. LayerNorm: A Comparison of Time Consumption (Image source: Zhang, et al., 2019)\n计算效率更高\n减少运算量：只需计算输入向量的均方根（RMS），无需计算均值和方差。 加快训练速度：实际测试中，RMSNorm 显著缩短了训练时间（如上图由 665s 降至 501s），在大规模模型训练中尤其明显。 训练更稳定\n适应更大学习率：在保持稳定性的同时，能够使用更大学习率，加速模型收敛。 保持表达能力：通过适当的缩放参数 $\\gamma$ 简化归一化过程的同时，仍能维持模型的表现。 节省资源\n降低硬件需求：更少的计算开销既能提升速度，也能减少对硬件资源的占用，适合在资源受限环境中部署。 Pre-Norm 的优势 更易训练深层模型\n稳定梯度传播：在残差连接之前进行归一化，可有效缓解梯度消失或爆炸。 减少对复杂优化技巧的依赖：即使模型很深，训练过程依然稳定。 加速模型收敛\n高效的梯度流动：Pre-Norm 使梯度更容易传递到前面的层，整体收敛速度更快。 结论 残差连接和归一化方法在深度学习模型中扮演着至关重要的角色，不同的归一化方法和网络架构设计各有其适用场景和优缺点。通过引入残差连接，ResNet 成功地训练了极深的网络，显著提升了模型的表达能力和训练效率。同时，归一化方法如 BatchNorm、LayerNorm、WeightNorm 和 RMSNorm 各自提供了不同的优势，适应了不同的应用需求。\n随着模型规模的不断扩大，选择合适的归一化方法和网络架构设计变得尤为重要。RMSNorm 由于其高效的计算和良好的训练稳定性，结合 Pre-Norm 的架构设计，成为当前主流 LLM 的首选。这种组合不仅提升了模型的训练效率，还确保了在大规模参数下的训练稳定性和性能表现。\n参考文献 [1] He, Kaiming, et al. \u0026ldquo;Deep residual learning for image recognition.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[2] Xiong, Ruibin, et al. \u0026ldquo;On layer normalization in the transformer architecture.\u0026rdquo; International Conference on Machine Learning. PMLR, 2020.\n[3] Wang, Hongyu, et al. \u0026ldquo;Deepnet: Scaling transformers to 1,000 layers.\u0026rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).\n[4] Ioffe, Sergey. \u0026ldquo;Batch normalization: Accelerating deep network training by reducing internal covariate shift.\u0026rdquo; arXiv preprint arXiv:1502.03167 (2015).\n[5] Ba, Jimmy Lei. \u0026ldquo;Layer normalization.\u0026rdquo; arXiv preprint arXiv:1607.06450 (2016).\n[6] Salimans, Tim, and Durk P. Kingma. \u0026ldquo;Weight normalization: A simple reparameterization to accelerate training of deep neural networks.\u0026rdquo; Advances in neural information processing systems 29 (2016).\n[7] Zhang, Biao, and Rico Sennrich. \u0026ldquo;Root mean square layer normalization.\u0026rdquo; Advances in Neural Information Processing Systems 32 (2019).\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Feb 2025). 深度学习中的归一化.\nhttps://syhya.github.io/posts/2025-02-01-normalization\nOr\n@article{syhya2025normalization, title = \u0026#34;深度学习中的归一化\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-02-01-normalization\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-02-01-normalization/","summary":"\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e在深度学习中，网络架构的设计对模型的性能和训练效率有着至关重要的影响。随着模型深度的增加，训练深层神经网络面临诸多挑战，如梯度消失和梯度爆炸问题。为了应对这些挑战，残差连接和各种归一化方法被引入并广泛应用于现代深度学习模型中。本文将首先介绍残差连接和两种架构，分别是 pre-norm 和 post-norm。随后介绍四种常见的方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析为何当前主流的大模型倾向于采用 \u003cstrong\u003eRMSNorm\u003c/strong\u003e 与 \u003cstrong\u003ePre-Norm\u003c/strong\u003e 结合的架构。\u003c/p\u003e","title":"深度学习中的归一化"},{"content":"DeepSeek AI 近期发布 DeepSeek-R1 (DeepSeek-AI, 2025)，其推理性能在多个 benchmark 上已接近 OpenAI o1 (OpenAI, 2024)的水平，是开源社区成功复现 o1 的重要一步。R1 相关代码可以参考huggingface 尝试开源复现 open-r1 项目。以往的研究多依赖于海量的监督数据来提升大语言模型（Large Language Model, LLM）性能，但 DeepSeek-R1 及其早期实验 DeepSeek-R1-Zero 的成功，有力证明了纯粹大规模强化学习在提升 LLM 推理能力方面的潜力。其印证了 Richard Sutton 在 “The Bitter Lesson” 中提出的深刻见解:\nOne thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning. (Richard Sutton, 2019)\n符号 下面列举了文章所使用的数学公式，可以帮你更轻松阅读。\n符号 含义 \\( q \\) 或 \\( Q \\) 问题，用户提出的输入或指令 \\( o \\) 或 \\( O \\) 输出，模型生成的文本回复或答案 \\( t \\) token 索引，表示输出文本中的第 \\( t \\) 个 token 的位置 \\( o_t \\) 输出文本 \\( o \\) 中的第 \\( t \\) 个 token \\( o_{\u0026lt;t} \\) 输出文本 \\( o \\) 中前 \\( t-1 \\) 个 tokens \\( \u0026#124;o\u0026#124; \\) 输出文本 \\( o \\) 的长度，通常指 token 的数量 \\( G \\) 输出组的大小，在 GRPO 算法中，为每个问题采样的输出数量 \\( \\pi_\\theta, \\pi_{\\theta_{\\text{old}}}, \\pi_{\\text{ref}}, \\pi_{\\text{sft}} \\) 策略模型及其变体，用于生成文本输出或作为参考模型 \\( A_t, A_i \\) 优势函数与相对优势值 \\( \\varepsilon \\) 剪辑超参数, 用于限制重要性采样率的范围，保证策略更新的稳定性 \\( \\beta \\) 正则化系数，用于控制 KL 散度惩罚项在目标函数中的权重 \\( \\mathbb{D}_{KL} \\) KL 散度，衡量两个概率分布之间差异的度量，用于约束新策略与参考策略的距离 \\( \\mathcal{J}, \\mathcal{L} \\) 目标函数与损失函数 \\( \\mathbb{E} \\) 期望，表示对随机变量的平均值，在目标函数中表示对样本数据的平均 \\( P_{\\text{sft}}(Q, O) \\) SFT 数据集的分布，表示 \\( SFT \\) 数据集中问题 \\( Q \\) 和输出 \\( O \\) 的联合概率分布 \\( P_{\\text{sft}}(Q) \\) SFT 数据集中问题的分布，表示 \\( SFT \\) 数据集中问题 \\( Q \\) 的边缘概率分布 \\( \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\) 策略模型在给定问题 \\( q \\) 和之前生成的 tokens: \\( o_{\u0026lt;t} \\) 的条件下，生成第 \\( t \\) 个 token: \\( o_t \\) 条件概率 \\( \\mathbb{I}(o) \\) 判断输出 \\( o \\) 的答案是否为高质量的函数，高质量时为 1，否则为 0 \\( r(o) \\) 奖励函数，评估模型输出 \\( o \\) 质量的函数 \\( r_i \\) 第 \\( i \\) 个输出的奖励值 \\( \\nabla_{\\theta} \\) 梯度算子，表示对函数关于模型参数 \\( \\theta \\) 求梯度 \\( \\mathcal{N}(\\mu, 1) \\) 正态分布，均值为 \\( \\mu \\)，标准差为 1 \\( \\binom{a}{b} \\) 二项式系数，表示从 \\( a \\) 个元素中选择 \\( b \\) 个元素的组合数 \\( r(o) = \\frac{\\pi_{\\text{ref}}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\) 概率比值，参考模型与当前策略模型生成输出 \\( o \\) 的概率之比 训练流程概要 DeepSeek-R1 系列模型的训练是一个多阶段的过程，旨在构建具备卓越推理能力和通用语言能力的大型语言模型。整个训练流程从 DeepSeek-V3 (DeepSeek-AI, 2024) 模型出发，逐步迭代优化，最终得到不同版本的 DeepSeek-R1 模型。\nFig. 1. DeepSeek R1 Training Pipeline. (Image source: Harris Chan\u0026rsquo;s Tweet, 2025)\n如图1清晰的显示了 DeepSeek-R1 整个训练流程，主要分为以下几个关键阶段：\n基座模型与初步微调: 流程的起点是 DeepSeek-V3 Base 模型。首先，使用监督式微调 (SFT) 技术，在 冷启动长文本 CoT 数据 上对基础模型进行初步训练，赋予模型初步的推理能力。\n强化推理能力: 在 SFT 基础上，采用面向推理的强化学习方法，具体为组相对策略优化 (GRPO) 算法，并结合基于规则的奖励和CoT 语言一致性奖励，进一步提升模型的推理能力。\n推理数据生成与拒绝采样: 利用推理提示和拒绝采样技术，并以规则和让 DeepSeek-V3 模型进行评判数据质量，生成高质量的推理数据。\n非推理数据生成: 使用 CoT 提示方法，让 DeepSeek-V3 模型进行数据增强，生成非推理数据并且结合原始 SFT 数据，以提升模型的通用语言能力。\n蒸馏: 将推理数据和非推理数据结合，用于蒸馏训练。通过 SFT，将 DeepSeek-V3 的能力迁移到一系列小型模型 (Qwen 和 Llama 系列)，得到 DeepSeek-R1-Distill 系列模型。\n最终模型微调: 对 DeepSeek-V3 模型再次进行 SFT 和强化学习微调。强化学习阶段采用 推理和偏好奖励，并使用多样化的训练提示，最终得到 DeepSeek-R1 模型。\nDeepSeek-R1-Zero: 通过 GRPO 算法直接在 DeepSeek-V3 Base 上进行训练得到，作为其他模型的对比基准。\n接下来博主将深入分析 DeepSeek-R1 训练流程中的关键技术和方法。\nDeepSeek-R1-Zero PPO 近端策略优化 (Proximal Policy Optimization, PPO) (Schulman et al., 2017) 算法是一种广泛应用于强化学习的经典算法，在 InstructGPT(Ouyang et al., 2022) 论文中被证明是训练 LLM 强化学习微调阶段的有效且稳定的方法。\n强化学习核心思想是让智能体 (Agent) 在与环境的交互中学习，通过试错来最大化累积奖励。在LLM场景下，模型本身就是智能体，“环境” 可以理解为用户提出的问题和期望的回答方式。策略 (Policy) \\(\\pi_\\theta\\) 代表了智能体的行为准则，即给定一个输入 (例如问题 \\(q\\))，策略会输出一个动作 (例如生成文本 \\(o\\))。策略 \\(\\pi_\\theta\\) 通常由一个神经网络模型参数化，训练的目标是找到最优的参数 \\(\\theta\\)，使得策略能够生成高质量的输出。\nActor-Critic 框架是强化学习中常用的一种架构，PPO 也属于 Actor-Critic 算法。Actor-Critic 框架包含两个核心组件：\nActor (策略模型)：负责学习策略 \\(\\pi_\\theta\\)，即如何根据当前状态 (例如用户问题) 选择动作 (生成文本)。 Critic (价值模型)：负责评估 Actor 策略的好坏，通常通过学习一个价值函数 \\(V(s)\\) 或 \\(Q(s, a)\\) 来实现。价值函数预测在给定状态 \\(s\\) (或状态-动作对 \\((s, a)\\)) 下，未来能够获得的累积奖励的期望值。 PPO 的目标是改进策略模型 (Actor)，使其能够生成更高质量的输出，同时借助价值模型 (Critic) 来稳定训练过程。PPO 通过最大化以下目标函数来更新策略模型 \\(\\pi_{\\theta}\\)：\n\\[ \\mathcal{J}_{PPO}(\\theta) = \\mathbb{E}\\!\\Biggl[ \\min\\Bigl( \\underbrace{\\frac{\\pi_\\theta(a\\!\\mid\\!s)}{\\pi_{\\theta_{\\text{old}}}(a\\!\\mid\\!s)}}_{\\text{重要性采样率}}\\underbrace{A_t}_{\\text{优势函数}},\\, \\operatorname{clip}\\Bigl( \\underbrace{\\frac{\\pi_\\theta(a\\!\\mid\\!s)}{\\pi_{\\theta_{\\text{old}}}(a\\!\\mid\\!s)}}_{\\text{重要性采样率}}, 1-\\varepsilon,\\, 1+\\varepsilon \\Bigr)\\underbrace{A_t}_{\\text{优势函数}} \\Bigr) \\Biggr] \\]参数说明：\n期望 \\(\\mathbb{E}[\\cdot]\\)：表示对样本的平均。在实际训练中，我们会采样一批数据 (例如用户问题和模型生成的回答)，然后计算这批数据的平均目标函数值。\n重要性采样率：衡量当前策略 \\(\\pi_\\theta\\) 与旧策略 \\(\\pi_{\\theta_{\\text{old}}}\\) 在动作 \\(a\\) 上的概率比值。PPO 采用 近端策略更新 的思想，限制每次策略更新的幅度，避免策略变化过大导致训练不稳定。\n优势函数 \\(A_t\\)：评估在状态 \\(s\\) 下采取动作 \\(a\\) 相对于平均水平的优势。优势函数通常由 Critic 模型 (价值网络) 估计得到，其采用 广义优势估计 (Generalized Advantage Estimation, GAE) (Schulman et al., 2015)方法。优势函数 \\(A_t\\) 越大，表示当前动作 \\(a\\) 越好，策略模型应该增加采取该动作的概率。\nclip：PPO 的核心机制之一，本质上可以看作是一个惩罚函数，用于限制重要性采样率的范围在 \\([1-\\varepsilon, 1+\\varepsilon]\\) 之间，其中 \\(\\varepsilon\\) 是一个超参数 (通常设置为 0.2)。剪辑操作防止策略更新步幅过大，提高训练的稳定性。\nclip 函数通过限制重要性采样率来惩罚过大或过小的策略更新幅度。\n当重要性采样率超出 \\([1-\\varepsilon, 1+\\varepsilon]\\) 范围时，clip 函数会将其限制在该范围内，从而降低目标函数的增益 (或减少损失)。 对于正向更新 (\\(A_t \u003e 0\\))： 如果重要性采样率过大 (超过 \\(1+\\varepsilon\\))，clip 会将其限制为 \\(1+\\varepsilon\\)，降低了实际的更新幅度，惩罚了过于激进的策略改进。 对于负向更新 (\\(A_t \u003c 0\\))： 如果重要性采样率过小 (小于 \\(1-\\varepsilon\\))，clip 会将其限制为 \\(1-\\varepsilon\\)，同样限制了更新幅度，避免策略发生剧烈变化。 目标函数取 clip 之前和 clip 之后的最小值，确保在重要性采样率超出范围时，PPO 会对策略更新进行惩罚，保证策略更新的“保守性”。\n在实际优化过程中，我们通常将 PPO 损失函数 \\(\\mathcal{L}_{PPO}(\\theta)\\) 定义为目标函数的负值，通过最小化损失来最大化目标函数：\n\\[ \\mathcal{L}_{PPO}(\\theta) = -\\,\\mathcal{J}_{PPO}(\\theta). \\]PPO 算法因其 简单有效、相对稳定 的特点，成为强化学习领域的基准算法之一，并在各种任务中取得了成功，包括大型语言模型的强化学习微调。PPO 通常被认为比早期的 TRPO 等方法更稳定，但在大模型上的具体应用仍需要细致的超参数调优。在大语言模型场景下，如果价值网络与策略网络完全分离且规模相当，势必会带来更多的计算与内存开销。为解决这些问题，DeepSeek 团队提出了组相对策略优化 (GRPO)算法。\nGRPO 组相对策略优化 (Group Relative Policy Optimization, GRPO) (Shao et al., 2024) 是 DeepSeek 团队为训练 DeepSeek-R1-Zero 这样的大语言模型而专门设计的一种高效稳定的强化学习算法。GRPO 的核心创新在于摒弃了传统Actor-Critic 框架中对独立价值网络 (critic model) 的依赖，降低了计算成本，并提高了训练的稳定性。 从广义上讲，GRPO 可以被视为一种 Actor-Only 的强化学习方法。\nGRPO 的灵感来源于 相对评估 的思想。在许多实际场景中，我们往往更容易判断一组事物之间的相对好坏，而不是给出绝对的价值评估。例如，在评价一组学生的作业时，老师可能更容易比较不同作业之间的优劣，而不是给每份作业打一个绝对分数。GRPO 将这种相对评估的思想引入强化学习，通过 组内相对评分来构建基准 (baseline)，完全替代了对价值网络的依赖。\n具体而言，对于每个问题 \\( q \\)，GRPO 会从旧策略 \\( \\pi_{\\theta_{\\text{old}}} \\) 中采样一组输出 \\( \\{o_1, o_2, \\ldots, o_G\\} \\)，形成一个 输出组。然后，通过最大化下面的目标函数来更新策略模型 \\( \\pi_{\\theta} \\)：\n\\[ \\begin{aligned} \\mathcal{J}_{GRPO}(\\theta) \u0026 = \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O \\mid q)\\right] \\\\ \u0026 \\quad \\frac{1}{G} \\sum_{i=1}^G \\Biggl( \\min\\biggl( \\underbrace{\\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)}}_{\\text{重要性采样率}} \\,\\underbrace{A_i}_{\\text{相对优势值}},\\, \\operatorname{clip}\\Bigl( \\underbrace{\\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)}}_{\\text{重要性采样率}}, 1-\\varepsilon,\\, 1+\\varepsilon \\Bigr)\\,\\underbrace{A_i}_{\\text{相对优势值}} \\biggr) \\;-\\;\\beta\\,\\underbrace{\\mathbb{D}_{KL}\\bigl(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\bigr)}_{\\text{KL 散度惩罚项}} \\Biggr), \\end{aligned} \\]与 PPO 的目标函数类似，GRPO 的目标函数也包含重要性采样率和 clip，用于保证策略更新的稳定性。不同之处在于：\n相对优势值 \\(A_i\\)：GRPO 使用 相对优势值 \\(A_i\\) 代替 PPO 中的优势函数 \\(A_t\\)。相对优势值 \\(A_i\\) 是根据 组内奖励 计算得到的，无需价值网络估计。 KL 散度惩罚项 \\(\\mathbb{D}_{KL}\\bigl(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\bigr)\\)：为了进一步约束策略更新，GRPO 引入了 KL 散度惩罚项，限制新策略 \\(\\pi_\\theta\\) 与参考策略 \\(\\pi_{\\text{ref}}\\) 之间的差异过大。 Fig. 2. The comparison of PPO and GRPO. (Image source: DeepSeek-AI, 2024)\n从上图2我们可以看出GRPO 的核心创新在于 相对优势值 \\(A_i\\) 的计算方式。与 PPO 不同，GRPO 不依赖于独立的价值网络，而是直接利用 组内奖励 进行相对评估。对于每个输出组 \\( \\{o_1, o_2, \\ldots, o_G\\} \\)，GRPO 首先获取每个输出对应的奖励值 \\( \\{r_1, r_2, \\ldots, r_G\\} \\)。然后，根据以下公式计算相对优势值 \\( A_i \\)：\n\\[ A_i = \\frac{\\,r_i \\;-\\; \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})\\,}{ \\text{std}\\bigl(\\{r_1, r_2, \\ldots, r_G\\}\\bigr)}. \\]相对优势值 \\( A_i \\) 通过 标准化 组内奖励 \\( \\{r_1, r_2, \\ldots, r_G\\} \\) 得到，具有 零均值和单位方差，更好地反映了每个输出在组内的相对优劣程度。\nGRPO 采用 相对评估 的方式，具有以下优点：\n无需训练价值网络：避免了训练大规模价值网络带来的计算开销和不稳定性。 降低价值估计方差：相对评估关注组内输出的相对优劣，而不是绝对价值，降低了估计方差，提高了训练稳定性。 更符合奖励模型的比较特性：奖励模型通常基于比较数据训练，GRPO 的相对评估方式与之更契合。 更适用于序列生成任务的信用分配：即使奖励是稀疏的，GRPO 也能有效学习，因为它关注同组输出之间的相对好坏。 Schulman 无偏估计器 KL 散度 \\(\\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right)\\) 衡量了策略 \\(\\pi_\\theta\\) 相对于参考策略 \\(\\pi_{\\text{ref}}\\) 的信息损失，其标准定义为：\n\\[ \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) = \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\log \\frac{\\pi_\\theta(o \\mid q)}{\\pi_{\\text{ref}}(o \\mid q)} \\right]. \\]如前所述，直接计算上述期望在实际中面临挑战。为了解决这个问题，GRPO 采用了 Schulman 无偏估计器 (Schulman, 2020)。与公式中可能使用的 KL 散度惩罚项不同，我们使用以下无偏估计器来估计 \\(\\pi_\\theta\\) 和 \\(\\pi_{ref}\\) 之间的 KL 散度：\n$$ \\mathbb{D}_{K L}\\left[\\pi_{\\theta}| | \\pi_{r e f}\\right]=\\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}{\\pi_{\\theta}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}{\\pi_{\\theta}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}-1. $$为了理解这个估计器的优点，我们首先从数学上推导其无偏性。\n无偏性证明 为了简化符号，我们令 \\(r(o) = \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)}\\)。则 Schulman 估计器可以写成：\n\\[ \\hat{D}_{KL}(o) = r(o) - \\log r(o) - 1. \\]我们需要证明，当 \\(o\\) 从 \\(\\pi_\\theta(\\cdot|q)\\) 中采样时，\\(\\hat{D}_{KL}(o)\\) 的期望等于真实的 KL 散度 \\(\\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right)\\)。\n\\[ \\begin{aligned} \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} [\\hat{D}_{KL}(o)] \u0026= \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} [r(o) - \\log r(o) - 1] \\\\ \u0026= \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - 1 \\right] \\\\ \u0026= \\sum_{o} \\pi_\\theta(o \\mid q) \\left[ \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - 1 \\right] \\quad (\\text{离散情况，连续情况为积分}) \\\\ \u0026= \\sum_{o} \\left[ \\pi_{ref}(o \\mid q) - \\pi_\\theta(o \\mid q) \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\pi_\\theta(o \\mid q) \\right] \\\\ \u0026= \\underbrace{\\sum_{o} \\pi_{ref}(o \\mid q)}_{=1} - \\underbrace{\\sum_{o} \\pi_\\theta(o \\mid q) \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)}}_{=-\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})} - \\underbrace{\\sum_{o} \\pi_\\theta(o \\mid q)}_{=1} \\\\ \u0026= 1 - (-\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})) - 1 \\\\ \u0026= \\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref}). \\end{aligned} \\]因此，我们证明了 \\(\\hat{D}_{KL}(o)\\) 是 \\(\\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right)\\) 的无偏估计。\n三种 KL 散度估计器对比 为了直观理解三种估计器的差异，以下表格列出了它们的数学表达式，其中 \\( r(o) = \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\)：\n估计器 数学表达式 主要特点 k1 (朴素估计器) \\(\\hat{D}_{KL}^{(k1)}(o) = \\log \\frac{\\pi_\\theta(o \\mid q)}{\\pi_{ref}(o \\mid q)} = \\log \\frac{1}{r(o)}\\) 简单直接，对应 KL 散度定义；方差较高，估计结果波动较大。 k2 (平方对数比估计器) \\(\\hat{D}_{KL}^{(k2)}(o) = \\frac{1}{2} (\\log r(o))^2\\) 使用对数比的平方，始终为正，降低方差；引入偏差，尤其在分布差异大时。 k3 (Schulman 无偏估计器) \\(\\hat{D}_{KL}^{(k3)}(o) = r(o) - \\log r(o) - 1\\) 结合了比值 \\( r(o) \\) 和对数比 \\(\\log r(o)\\)；无偏，较低方差，估计稳定。 k1 (朴素估计器): 无偏简单直接，但方差较高，导致估计结果不稳定。 k2 (平方对数比估计器): 降低了方差，但引入了偏差，特别是在分布差异较大时偏差显著。 k3 (Schulman 无偏估计器): 兼具无偏性和较低的方差，提供了稳定的估计结果。 实验结果 为了评估三种 KL 散度估计器的性能，我们进行了数值实验，结果如下表所示。实验中，固定分布 \\( q = \\mathcal{N}(0, 1) \\)，通过改变分布 \\( p = \\mathcal{N}(\\mu, 1) \\) 的均值 \\(\\mu\\) 来控制真实的 KL 散度 \\(\\mathbb{D}_{KL}(p \\| q)\\)。使用5亿个样本进行 Monte Carlo 估计，并重复实验以获得稳定结果。\n实验代码可以参考 unbiased_kl_divergence.py\n真实 KL 散度 估计器 平均估计值 标准差 相对偏差 (%) 0.005 k1 0.005 0.1 0.0387 0.005 k2 0.005 0.0071 0.2415 0.005 k3 0.005 0.0071 -0.0082 0.125 k1 0.125 0.5 -0.0389 0.125 k2 0.1328 0.1875 6.2500 0.125 k3 0.125 0.1845 0.0072 0.5 k1 0.5 1 -0.0018 0.5 k2 0.625 0.866 25.0004 0.5 k3 0.5 0.8478 0.0052 朴素估计器 (k1):\n无偏性: 平均估计值与真实 KL 散度高度吻合，相对偏差接近 0%。 方差: 标准差高于 k3，且随着真实 KL 散度的增加而增大，导致估计结果不稳定。 平方对数比估计器 (k2):\n无偏性: 存在一定偏差，且偏差随真实 KL 散度的增加而显著增大（例如，真实 KL 为 0.5 时相对偏差达到 25%）。 方差: 在较低的真实 KL 散度下方差较低，但整体表现不稳定。 Schulman 无偏估计器 (k3):\n无偏性: 实验结果显示相对偏差极小，几乎为 0%，验证了其无偏性。 方差: 标准差明显低于 k1，且与 k1 相比在所有 KL 散度下均表现出更低的方差，尤其在较低 KL 散度时优势显著。 优点总结 无偏性: 理论和实验结果均表明，k3 是无偏估计器，能够准确反映真实的 KL 散度。 正定性: 估计值始终为非负，符合 KL 散度的性质。 较低的方差: 相较于 k1，k3 显著降低了估计方差，提供了更稳定的估计结果，尤其在 KL 散度较小时表现突出。 Schulman 无偏估计器 \\( \\hat{D}_{KL}^{(k3)}(o) = r(o) - \\log r(o) - 1 \\) 为 KL 散度提供了一种兼具无偏性和低方差的估计方法。其无偏性确保了估计的准确性，而较低的方差提升了估计的稳定性，特别适用于需要稳定梯度信号的强化学习场景，如策略优化。基于这些优势，GRPO 算法选择使用 k3 作为惩罚策略偏离的估计器，从而保证训练过程的稳定性和最终策略的性能。\n在实际优化中，GRPO 损失函数 \\(\\mathcal{L}_{GRPO}(\\theta)\\) 被定义为目标函数 \\(\\mathcal{J}_{GRPO}(\\theta)\\) 的负值，通过最小化损失函数 \\(\\mathcal{L}_{GRPO}(\\theta)\\) 来实现目标函数 \\(\\mathcal{J}_{GRPO}(\\theta)\\) 的最大化：\n\\[ \\mathcal{L}_{GRPO}(\\theta) = -\\,\\mathcal{J}_{GRPO}(\\theta) \\]PPO 与 GRPO 对比 为更清晰理解 PPO 和 GRPO 的异同，以下表格对两种算法进行对比：\n特性 PPO GRPO 是否 Actor-Critic 是 是 (广义上可以认为是 Actor-Only) 是否价值网络 需要独立的价值网络 (Critic) 无需独立的价值网络 优势函数估计 通过价值网络估计绝对优势值 通过组内奖励相对评估相对优势值 计算开销 较高，需要训练价值网络 较低，无需训练价值网络 训练稳定性 相对较好，但价值网络训练可能引入不稳定性 更好，避免了价值网络训练带来的不稳定性 算法复杂度 相对复杂，需要维护和更新策略网络和价值网络 相对简单，只需维护和更新策略网络 适用场景 广泛适用于各种强化学习任务，包括中小规模语言模型微调 特别适用于大语言模型的强化学习微调，注重效率和稳定性 信用分配 依赖价值网络进行时间差分学习，处理信用分配问题 依赖最终奖励和组内相对评估，也可辅助中间奖励 方差问题 价值网络估计可能引入方差 组内相对优势估计在小组规模下可能存在方差，可通过增大组规模等缓解 从表中可以看出，PPO 是一种通用且强大的强化学习算法，但其训练价值网络的机制在大语言模型场景下带来了额外的计算负担和潜在的不稳定性。GRPO 通过引入组相对评分，巧妙地规避了对价值网络的需求，在保证性能的同时，显著降低了计算成本，并提升了训练稳定性。这使得 GRPO 成为在训练资源不多的情况下训练 DeepSeek-R1-Zero 这样 LLM 的理想选择。\n代码生成评估指标 代码生成会采用更严谨的测试方法。通过编译器执行模型生成的代码，并使用预定义的测试用例进行多次单元测试，以判断代码的正确性。常用的评估指标包括 pass@k(Chen et al., 2021) 和 cons@N(OpenAI, 2024)。\npass@k: 衡量模型在生成 k 个代码样本时，至少有一个样本能够通过所有预定义测试用例的概率。\npass@k 有偏估计公式 \\[ \\text{Simplified pass@k} = \\frac{1}{P} \\sum_{i=1}^{P} C_i \\] 其中，对于每个问题 \\(i\\), \\(C_i\\) 定义为： \\[ C_i = \\begin{cases} 1 \u0026 \\text{如果生成的 k 个样本中至少有一个是正确的} \\\\ 0 \u0026 \\text{如果生成的 k 个样本全部都不正确} \\end{cases} \\]参数说明:\n\\( P \\): 评估的问题总数。 \\( C_i \\): 对于第 \\(i\\) 个问题，如果生成的 \\(k\\) 个样本中至少有一个是正确的，则 \\(C_i = 1\\)，否则 \\(C_i = 0\\)。 \\( \\sum_{i=1}^{P} C_i \\): 表示在所有 \\(P\\) 个问题中，被 “解决” 的问题总数。 \\( \\frac{1}{P} \\sum_{i=1}^{P} C_i \\): 表示 “解决” 问题的比例，即准确率。 公式含义: 这种简化方法直接计算 生成 k 个样本后，至少有一个样本正确的比例。 虽然这种方法提供的是 pass@k 的 有偏估计，可能会略微高估真实值，但它在实践中非常常用，因为它 直观、易于计算，并且在样本量足够大时，能够提供对模型性能的合理近似。尤其在工业界和快速评估场景中，这种简化方法非常实用。\n然而，LLM 在推理解码时会受到 temperature、top_p（核采样概率）、top_k（候选词数量）和repetition_penalty 等参数的影响。这些参数会使代码生成结果有随机性和多样性，并且当样本 K 比较少的如果设置随机过高的参数，会影响 pass@k 的评估结果。因此，采用无偏估计方法能够更准确地反映模型的真实性能。\npass@k 的无偏估计公式 \\[ \\text { pass @ } k:=\\underset{\\text { Problems }}{\\mathbb{E}}\\left[1-\\frac{\\binom{n-c}{k}}{\\binom{n}{k}}\\right] \\]参数说明:\n\\( n \\): 为每个问题生成的代码样本总数。 \\( c \\): 在 \\( n \\) 个样本中，能够通过所有单元测试的正确样本数。 \\( k \\): pass@\\(k\\) 指标中的参数 \\(k\\)，表示我们考虑的生成样本数量。 \\( \\binom{a}{b} \\): 表示二项式系数，计算从 \\(a\\) 个元素中选择 \\(b\\) 个元素的组合数。 \\( \\underset{\\text { Problems }}{\\mathbb{E}} \\): 表示对所有评估问题的期望值（平均值）。 公式含义:\n公式实际上计算的是至少有一个正确样本的概率。公式 \\( \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\) 计算的是在生成的 \\(n\\) 个样本中，随机抽取 \\(k\\) 个样本，且这 \\(k\\) 个样本都不正确的概率。我们用 1 减去这个概率，就得到了 在 \\(n\\) 个样本中，随机抽取 \\(k\\) 个样本，且这 \\(k\\) 个样本中至少有一个正确的概率， 这就是 pass@\\(k\\) 指标的含义。 这个公式提供的是 pass@k 的 无偏估计，更适用于学术研究等需要精确评估的场景。 在实际计算中，通常会生成远大于 \\(k\\) 的样本数 \\(n\\) (例如论文中使用 \\(n=200\\), \\(k \\leq 100\\))，以更稳定地估计 pass@\\(k\\)。 pass@k 简化乘积形式 为了更方便数值计算，原始公式还可以转化为以下乘积形式，它仍然是无偏估计，并能避免数值溢出问题：\n\\[ \\text { pass @ } k = \\underset{\\text { Problems }}{\\mathbb{E}}\\left[1 - \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i}\\right] \\]推导过程:\n至少有一个正确样本的反面是所有 k 个样本都不正确。 因此，pass@k 等于 1 减去 所有 k 个样本都不正确的概率。\n考虑不放回抽样的场景。假设我们从 \\(n\\) 个样本中抽取 \\(k\\) 个样本，要计算这 \\(k\\) 个样本都不正确的概率。总共有 \\(n\\) 个样本，其中 \\(n-c\\) 个是不正确的。\n第一次抽取时，抽到不正确样本的概率为 \\( \\frac{n-c}{n} \\)。\n在第一次抽取到不正确样本的条件下，第二次抽取时，剩余 \\(n-1\\) 个样本中，有 \\(n-c-1\\) 个不正确样本。因此，第二次仍然抽到不正确样本的条件概率为 \\( \\frac{n-c-1}{n-1} \\)。\n以此类推，第 \\(i\\) 次抽取时 ( \\(i\\) 从 1 到 \\(k\\) )，在之前 \\(i-1\\) 次都抽到不正确样本的条件下，第 \\(i\\) 次仍然抽到不正确样本的条件概率为 \\( \\frac{n-c-(i-1)}{n-(i-1)} = \\frac{n-c-i+1}{n-i+1} \\)。 为了与公式中的索引 \\(i=0\\) 对齐，我们将索引改为从 \\(i=0\\) 到 \\(k-1\\)，则第 \\(i+1\\) 次抽取时 ( \\(i\\) 从 0 到 \\(k-1\\) )，条件概率为 \\( \\frac{n-c-i}{n-i} \\)。\n将这 \\(k\\) 次抽取的条件概率连乘，即可得到所有 \\(k\\) 个样本都不正确的概率：\n\\[ P(\\text{所有 k 个样本都不正确}) = \\frac{n-c}{n} \\times \\frac{n-c-1}{n-1} \\times \\cdots \\times \\frac{n-c-k+1}{n-k+1} = \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i} \\] 最终，pass@k 的简化公式为：\n\\[ \\text { pass @ } k = \\underset{\\text { Problems }}{\\mathbb{E}}\\left[1 - \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i}\\right] \\] 这个乘积形式的公式，避免了直接计算可能数值很大的二项式系数，更易于理解和数值计算，尤其是在编程实现时，可以逐项累乘，有效防止数值溢出。\ncons@N cons@N: 通过生成 N 个样本，并从中选择出现频率最高的答案作为最终答案，评估该答案的准确率。在 DeepSeek-R1-Zero 的评估中，使用了 cons@64，即生成 64 个样本，并取其中出现次数最多的答案作为最终答案进行评估。\n\\[ \\text{cons@N} = \\frac{1}{P} \\sum_{i=1}^{P} \\mathbb{I}(\\text{ConsensusAnswer}_i \\text{ is correct}) \\]参数说明:\n\\( P \\)：评估的问题总数。 \\( \\text{ConsensusAnswer}_i \\)：通过多数投票得到的 共识答案。 \\( \\mathbb{I}(\\text{ConsensusAnswer}_i \\text{ is correct}) \\)：指示函数，若共识答案正确，则为 1，否则为 0。 公式含义： 计算在所有评估问题中，共识答案正确的比例。通过增加生成样本数 \\(N\\)，并采用多数投票策略，cons@N 指标能够更稳定和可靠地评估模型的平均性能。在模型生成结果存在一定随机性的情况下，该指标可以验证模型输出的一致性和准确性。\n奖励模型 奖励模型在 LLM 的研发中至关重要，主要应用于以下关键环节：\n基于人类反馈的强化学习: 在基于人类反馈的强化学习（RLHF）流程中，奖励模型用于评估模型生成结果的质量，并为后续的强化学习提供奖励信号。\n拒绝采样的关键工具: 在拒绝采样过程中，奖励模型对大量候选结果进行评分，筛选出高质量样本用于监督微调（SFT）。拒绝采样是自动化样本工程的重要方法，而奖励模型是其核心组成部分。\n业务场景中的判别器: 在实际应用中，奖励模型作为 LLM 输出结果的判别器或校验器，评估生成结果的质量。只有得分超过预设阈值的结果才会输出，否者进行再生成或降级处理，提高输出的可靠性和安全性。\nORM 与 PRM Fig. 3. Outcome reward vs Process reward. (Image source: Zeng et al., 2024)\n当前奖励模型主要分为两种范式：结果奖励模型(Outcome Reward Model, ORM) 和 过程奖励模型(Process Reward Model, PRM)。上图3直观的展示了这两种奖励模型的区别。以下表格也对比了这两种模型的主要特性：\n特性 ORM PRM 定义 对模型生成的完整结果进行整体评分 在内容生成过程中，对每一步或每个阶段进行细粒度评分 主要优势 简单直接，易于实现 对整体结果进行全面评估 提供更精细的奖励信号 有助于指导模型生成过程的每个步骤 主要劣势 方差较高，估计结果波动较大 缺乏过程中的反馈 训练和应用更为复杂 可能引入偏差，尤其在分布差异大时 适用场景 需要整体评估生成结果的任务 需要细粒度控制生成过程的任务，如分步推理或复杂生成任务 避免奖励欺骗的能力 中等，依赖于整体评分的准确性 较低，可通过优化每一步的奖励而非整体表现来作弊 训练复杂度 较低，无需对生成过程进行额外的监督 较高，需要在生成的每一步进行评分，增加了计算和数据需求 可解释性 高，评分基于最终结果 较低，评分涉及生成过程的多个步骤，难以全面理解每一步的评分依据 为了训练 DeepSeek-R1-Zero，DeepSeek 团队选择了ORM，而非PRM。此选择基于以下考虑：\n避免奖励欺骗\nPRM在大规模 RL 训练中，容易被智能体利用，导致奖励欺骗（Reward Hacking）（Gao et al., 2022）。模型可能采取“旁门左道”的策略以最大化奖励，而非提升推理能力。基于规则的奖励系统通过明确且可解释的规则，有效避免了奖励欺骗问题。\n基于规则的奖励系统在问题场景复杂或需要创造性回答时，可能难以覆盖所有类型的问题，规则设计可能存在漏洞被模型利用。\n降低训练复杂度\n训练 PRM 需要大量计算资源和数据，增加了训练流程的复杂性。而基于规则的奖励系统无需额外训练，规则一旦确定即可直接应用，简化了训练流程。基于规则的奖励系统特别适合自动判分或目标明确的任务，如数学题、LeetCode 编程题及对输出格式有明确要求的任务。对于开放式对话或创意类任务，则可能需要结合人类反馈或训练好的奖励模型。\n奖励机制 DeepSeek-R1-Zero 的奖励系统采用双重奖励机制，通过预定义的规则进行自动化评估，确保评估过程的高效性和实时性。这套系统主要包含以下两种类型的奖励：\n1. 准确性奖励\n定义： 衡量模型输出结果的正确性，是奖励系统中最关键的部分。 实现方式： 根据不同任务类型采用不同的验证方法： 数学问题： 验证最终答案是否与标准答案一致。 代码生成： 通过编译器执行模型生成的代码，并使用预设的单元测试用例进行多次测试，判断代码的正确性。 目的： 引导模型生成准确、可靠的输出结果。 2. 格式奖励\n定义： 为了提升模型输出的可读性和结构性，方便后续分析和评估而引入的奖励机制。 评估方式： 在强化学习训练过程中，通过预定义的规则系统进行自动化评估。 目的： 鼓励模型生成结构化的输出，例如包含思考过程和最终答案，使其更易于理解和分析。 DeepSeek-R1-Zero 的奖励函数 \\(r(o)\\) 由准确性奖励和格式奖励加权求和构成：\n$$ r(o) = r_{\\text{accuracy}}(o) + \\lambda \\cdot r_{\\text{format_effective}}(o) $$其中，有效格式奖励 \\(r_{\\text{format_effective}}(o)\\) 的计算方式如下：\n$$ r_{\\text{format_effective}}(o) = \\begin{cases} r_{\\text{format}}(o) \u0026 \\text{如果 } o \\text{ 的基本格式符合要求} \\\\ 0 \u0026 \\text{如果 } o \\text{ 的基本格式不符合要求} \\end{cases} $$基础格式奖励 \\(r_{\\text{format}}(o)\\) 则根据格式规范的符合程度进行分级：\n$$ r_{\\text{format}}(o) = \\begin{cases} R_{\\text{format_full}} \u0026 \\text{如果 } o \\text{ 的格式完全符合规范} \\\\ R_{\\text{format_partial}} \u0026 \\text{如果 } o \\text{ 的格式部分符合规范} \\\\ 0 \u0026 \\text{如果 } o \\text{ 的格式不符合规范} \\end{cases} $$训练模板 为了引导基模型遵循指定的指令，DeepSeek 团队设计了一个简洁而有效的训练模板。该模板要求模型首先生成推理过程（放在 \u0026lt;think\u0026gt; 和 \u0026lt;/think\u0026gt; 标签之间），然后再提供最终答案（放在 \u0026lt;answer\u0026gt; 和 \u0026lt;/answer\u0026gt; 标签之间）。这种结构化的格式，不仅确保了输出的可读性，更使研究人员能够清晰地观察模型在 RL 训练过程中的推理过程，从而更准确地评估模型的学习进展。\n角色 提示内容 助手回复 用户 prompt (用户提出的问题) 助手： \u0026lt;think\u0026gt; 推理过程 \u0026lt;/think\u0026gt; \u0026lt;answer\u0026gt; 答案 \u0026lt;/answer\u0026gt; \u0026lt;think\u0026gt; 和 \u0026lt;/think\u0026gt; (思维过程标签): 用于包裹模型的中间推理步骤，清晰展示模型的思考过程，便于理解模型的推理逻辑和进行错误分析。 \u0026lt;answer\u0026gt; 和 \u0026lt;/answer\u0026gt; (最终答案标签): 用于包裹模型的最终答案，方便程序自动化提取答案部分，进行高效的评估和后续处理。 评估流程 准确性评估： 评估模型输出 \\(o\\) 的答案是否正确，计算准确性奖励 \\(r_{\\text{accuracy}}(o)\\)。 基本格式检查： 检查输出 \\(o\\) 的基本格式是否符合预定义要求，例如是否包含必要的标签 \u0026lt;think\u0026gt; 和 \u0026lt;answer\u0026gt;，以及标签是否正确闭合和嵌套。 有效格式奖励判断： 基本格式不符合： 有效格式奖励 \\(r_{\\text{format_effective}}(o) = 0\\)。 基本格式符合： 进一步评估格式规范程度，计算基础格式奖励 \\(r_{\\text{format}}(o)\\)。 最终奖励计算： 将准确性奖励 \\(r_{\\text{accuracy}}(o)\\) 和有效格式奖励 \\(r_{\\text{format_effective}}(o)\\) 进行线性加权求和，得到最终奖励 \\(r(o)\\)。 通过结合准确性奖励和格式奖励，DeepSeek-R1-Zero 的奖励系统不仅关注模型输出的正确性，更重视输出结果的结构化和可读性。这使得模型不仅能够给出正确的答案，还能展现其思考过程，使其更像一个具备推理能力的智能体，而不仅仅是一个简单的答案输出机器。\n基准测试 Fig. 4. Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. (Image source: DeepSeek-AI, 2025)\n图4展示了不同模型在多项基准测试上的表现。在 AIME 2024 基准测试中，DeepSeek-R1-Zero 模型的 pass@1 分数达到了71.0%，此外 cons@64 分数为86.7%，与 OpenAI o1-0912 模型相当。\n自我反思 Fig. 5. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)\n图5展示了随着训练过程的深入，DeepSeek-R1-Zero 逐渐涌现出自我进化能力，模型不仅能够通过自我反思重新评估此前的推理步骤，还能主动探索替代的求解路径。此外模型能够根据问题的复杂程度，自适应地增加推理过程中生成的 token 数量，以实现更长、更深入的思考时间。总的来说这种动态且自发的行为显著增强了模型的推理能力，使其能够更加高效且准确地解决更复杂、更具挑战性的任务。\nFig. 6. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: DeepSeek-AI, 2025)\n图6展示了模型 DeepSeek-R1-Zero 出现了顿悟时刻（aha moment）在此阶段，本质上是模型在推理过程中，突然意识到之前的思考路径是错误的，并迅速调整了思考方向，最终得出了正确的答案。此现象同样表明模型在推理过程中具备了一定的自我反思能力。\nDeepSeek-R1 训练流程 为了在 DeepSeek-R1-Zero 的基础上进一步提升模型性能，DeepSeek 团队采用了 多阶段训练 策略，并将 冷启动数据 融入到训练流程中。DeepSeek-R1 的训练流程主要包括以下四个阶段，体现了从初步策略初始化到全面能力提升的进阶之路：\n冷启动: 利用高质量的思维链 (Chain-of-Thought, CoT) 数据，对 DeepSeek-V3-Base 基模型进行初步的监督微调，为后续强化学习奠定基础。\n面向推理的强化学习: 在冷启动模型基础上，应用强化学习算法，专注于增强模型在推理密集型任务中的能力。\n拒绝采样与监督微调: 通过拒绝采样技术筛选高质量推理数据，并结合非推理数据进行监督微调，进一步提升模型推理能力和通用能力。\n面向所有场景的强化学习: 综合考虑推理和非推理任务，进行第二阶段强化学习，使模型与人类偏好对齐，提升在更广泛场景下的表现。\n冷启动 在 DeepSeek-R1 的训练流程中，冷启动阶段至关重要，它如同引擎的点火器，为后续复杂的强化学习过程奠定坚实基础。监督微调是冷启动阶段的核心技术。\n训练目标 冷启动阶段的目标明确而关键：利用高质量的 CoT 数据，对 DeepSeek-V3-Base 基模型进行初步微调。这次微调旨在快速赋予模型以下核心能力：\n初步推理能力： 引导模型学习模仿人类的推理过程，为更复杂的推理打下基础。 良好文本生成质量： 确保模型输出文本的流畅性和自然度，提升用户体验。 这些 CoT 数据如同模型的“启动燃料”，帮助模型快速掌握人类的推理模式，并为后续强化学习提供良好的策略初始化，有效避免 RL 训练初期从零开始探索的低效和不稳定性。\n数据构建 为了构建高质量的冷启动数据，DeepSeek 团队进行了多方探索，最终整合了以下高效方法：\nFew-shot Prompting： 利用少量高质量的示例，引导模型生成更长、更具深度和逻辑性的 CoT 数据。 模型生成 + 反思验证： 直接 Prompt 模型生成答案，并加入反思和验证环节，确保答案的质量和推理的正确性。 优化 R1-Zero 输出： 收集 DeepSeek-R1-Zero 模型的输出，通过人工标注和优化，提升数据的可读性和整体质量。 通过上述策略，DeepSeek 团队积累了数千条高质量的冷启动数据，并以此为基础对 DeepSeek-V3-Base 进行了微调，作为强化学习的坚实起点。\n冷启动优点 相比于直接以 DeepSeek-R1-Zero 作为起点，冷启动数据带来了多项显著优势，为后续训练奠定了更优的基础：\n显著提升可读性：\nDeepSeek-R1-Zero 的输出存在可读性挑战，例如语言混合、缺乏结构化格式等。 冷启动数据特别设计了更易读的输出模式，包括： 添加摘要： 在回复末尾添加精炼的摘要，快速提炼核心结论。 过滤不良回复： 去除不友好或低质量的回复，确保数据纯净度。 结构化输出格式： 采用 | special_token | \u0026lt;reasoning_process\u0026gt; | special_token | \u0026lt;summary\u0026gt; 格式，清晰呈现推理过程和总结。 性能显著提升：\n通过精心设计融入人类先验知识的数据模式，DeepSeek 团队观察到模型性能相较 R1-Zero 有了显著提升。 这进一步验证了迭代训练是提升推理模型性能的有效路径。 更优的策略初始化：\n冷启动阶段的 SFT 核心在于策略初始化。 策略初始化是构建 Reasoing LLM，例如 OpenAI o1 系列的关键步骤。通过学习高质量 CoT 数据，模型初步掌握人类推理模式，并具备生成结构化推理过程的能力，为后续强化学习训练奠定坚实基础，避免了从零开始探索的困境。 监督微调 监督微调 (Supervised Fine-tuning, SFT) 的核心目标是通过在有监督标注的数据上微调模型，使其预测结果尽可能接近真实标签。 这旨在提升模型在特定任务和指令执行方面的能力。\n损失函数 SFT 的训练目标是最小化模型预测与真实标签之间的差异。损失函数通常采用交叉熵损失 (Cross-Entropy Loss)，也称为负对数似然 (Negative Log-Likelihood)，用于衡量模型预测 token 分布与真实 token 分布之间的差异。 为了平衡不同长度输出序列的贡献，我们通常会将损失函数归一化到每个 token 的平均损失。\n损失函数公式如下：\n\\[ \\mathcal{L}_{SFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\log \\pi_\\theta\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]参数说明:\n\\(\\mathcal{L}_{SFT}(\\theta)\\)：SFT 损失函数，通过调整模型参数 \\(\\theta\\) 最小化该函数。 \\(\\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}[\\cdot]\\)：在 SFT 数据集分布 \\(P_{sft}(Q, O)\\) 上的期望。 \\(P_{sft}(Q, O)\\)：SFT 数据集分布，\\(q\\) 代表问题 (Query)，\\(o\\) 代表对应的标准答案输出 (Output)。 \\((q, o)\\)：从 SFT 数据集中采样的 问题-答案对。 \\(|o|\\)：标准答案输出的 token 长度。 \\(o_t\\)：标准答案输出的第 \\(t\\) 个 token。 \\(o_{\u0026lt;t} \\)：标准答案输出的前 \\(t-1\\) 个 tokens。 \\(\\pi_\\theta\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\)：给定问题 \\(q\\) 和前文 \\(o_{\u0026lt;t} \\)，模型预测 token \\(o_t\\) 的概率。 \\(\\frac{1}{|o|}\\): 长度归一化因子，将总损失除以输出序列长度，得到每个 token 的平均损失。 SFT 损失函数旨在惩罚模型预测与标准答案之间的偏差。 对于给定的问题 \\(q\\) 和标准答案 \\(o\\)，损失函数计算模型预测答案 \\(o\\) 中每个 token \\(o_t\\) 的概率 \\(\\pi_\\theta(o_t | q, o_{\u0026lt;t} )\\)。通过除以输出长度 \\(|o|\\)，损失函数被归一化为每个 token 的平均负对数似然。\n模型准确预测标准答案 token 时，\\(\\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\approx 1\\)，\\(\\log \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\approx 0\\)，损失值接近最小值。 模型预测偏离标准答案时，\\(\\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\) 较小，\\(\\log \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\) 为负数且绝对值较大，损失值增大。 最小化 SFT 损失函数的过程，就是让模型学习生成与训练数据集中标准答案尽可能相似文本的过程。从负对数似然角度看，目标是找到最优模型参数 \\(\\theta\\)，最大化模型生成训练数据答案 \\(o\\) 的概率，等价于最小化生成答案 \\(o\\) 的负对数似然。高质量的 CoT 数据蕴含人类对推理和结果的偏好，因此 SFT 也可视为让模型学习并拟合人类推理偏好的过程。\n梯度 SFT 损失函数的梯度用于指导模型参数更新，以降低损失值。 损失函数关于模型参数 \\(\\theta\\) 的梯度为：\n\\[ \\nabla_{\\theta} \\mathcal{L}_{SFT} = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]参数说明:\n\\(\\nabla_{\\theta} \\mathcal{L}_{SFT}\\)：SFT 损失函数关于参数 \\(\\theta\\) 的梯度，指示损失函数值下降最快的方向。 \\(\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\): token 概率对数 \\(\\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\) 关于参数 \\(\\theta\\) 的梯度。 \\(\\frac{1}{|o|}\\): 长度归一化因子，与损失函数保持一致，梯度也是每个 token 平均损失的梯度。 实际计算梯度时，通常使用随机梯度下降算法，沿着梯度下降方向更新模型参数，逐步最小化损失函数，提升模型生成标准答案的准确性。\n梯度系数\n在 SFT 阶段，梯度系数通常设置为 1，这意味着所有训练样本对模型参数的更新贡献相同，模型平等地学习每个示例，力求最小化在整个数据集上的平均损失。\n数据来源与人类偏好 数据来源：SFT 数据集主要由高质量的长链思维 (CoT) 示例构成，代表了期望模型学习的“标准答案”，用于指导损失函数最小化。 数据可能来自人工标注或更强大的模型生成。可参考 Open-o1 项目的 SFT 数据集 OpenO1-SFT，包含长 CoT 回复。 人类偏好：在 SFT 阶段，人类选择可以被视为隐式的奖励函数。 高质量 CoT 数据体现了人类对模型推理和输出的期望，模型通过学习这些数据，最小化与人类期望输出的偏差，从而拟合人类偏好。 面向推理的强化学习 在冷启动微调后，DeepSeek 团队通过强化学习 (RL) 进一步提升模型在推理密集型任务（如编码、数学、科学和逻辑推理）中的能力。 此阶段的核心在于最大化奖励函数，引导模型学习更有效的推理策略。\n奖励函数 为了解决推理过程中 CoT 语言混合问题，DeepSeek 团队引入了语言一致性奖励，并将其与任务奖励结合，构成总奖励函数：\n\\[ r(o) = r_{\\text{task}}(o) + \\alpha \\cdot r_{\\text{lang_consistency}}(o) \\]参数说明:\n\\(r(o)\\)：总奖励函数，RL 训练的目标是最大化该函数。 \\(r_{\\text{task}}(o)\\)：基于任务完成情况的任务奖励，衡量模型推理的准确性。 \\(r_{\\text{lang_consistency}}(o)\\)：语言一致性奖励，衡量 CoT 输出的语言纯度。 \\(\\alpha\\)：超参数，平衡任务奖励和语言一致性奖励的权重。 总奖励函数是任务奖励和语言一致性奖励的加权和。最大化 \\(r(o)\\) 驱动模型在提升推理准确性的同时，保持 CoT 输出的语言一致性。 \\(\\alpha\\) 的作用是调整模型对语言一致性的重视程度。\n训练目标 通过最大化上述奖励函数，DeepSeek 团队在冷启动微调后的模型上进行 RL 训练，优化模型参数，使其在推理任务上获得更高的奖励值，最终提升推理能力。\nRFT 拒绝采样微调 (Rejection Sampling Fine-tuning, RFT) 旨在通过精炼训练数据提升模型通用能力。其核心思想是最小化选择性损失函数，引导模型学习高质量输出的生成模式。\n损失函数 RFT 采用拒绝采样策略，区分推理数据和非推理数据的生成与选择过程，构建高质量 SFT 数据集。训练目标是最小化以下损失函数：\n\\[ \\mathcal{L}_{RFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q) \\times \\pi_{sft}(O \\mid q)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\mathbb{I}(o) \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]其中，指示函数 \\(\\mathbb{I}(o)\\) 定义为：\n\\[ \\mathbb{I}(o) = \\begin{cases} 1, \u0026 \\text{如果输出 } o \\text{ 被判定为高质量} \\\\ 0, \u0026 \\text{否则} \\end{cases} \\]参数说明:\n\\(\\mathcal{L}_{RFT}(\\theta)\\): RFT 损失函数。 \\(P_{sft}(Q)\\): 问题 \\(q\\) 的分布。 \\(\\pi_{sft}(O \\mid q)\\): 给定问题 \\(q\\)，SFT 模型生成输出 \\(O\\) 的条件概率分布。 \\(\\mathbb{I}(o)\\): 指示函数，用于选择高质量答案。当输出 \\(o\\) 被判定为高质量时为 1，否则为 0。 RFT 损失函数基于交叉熵损失，通过指示函数 \\(\\mathbb{I}(o)\\) 选择性地学习高质量输出：\n高质量输出 (\\(\\mathbb{I}(o) = 1\\)): 损失函数退化为标准交叉熵损失，模型根据高质量答案的负对数似然更新参数，最小化模型预测与高质量答案的差异。 低质量输出 (\\(\\mathbb{I}(o) = 0\\)): 损失函数为零，低质量答案不参与参数更新。 RFT 通过最小化损失函数，引导模型专注于学习高质量答案的生成模式，实现选择性学习。\n数据生成 高质量数据 (推理数据): 通过 RL 模型生成候选答案，使用奖励模型（或 DeepSeek-V3 模型）评分，拒绝采样保留高分答案。 SFT 数据 (非推理数据): 复用 DeepSeek-V3 的 SFT 数据集及其生成流程。 训练过程 使用拒绝采样得到的高质量数据集，对 DeepSeek-V3-Base 模型进行监督微调，最小化 RFT 损失函数，提升模型推理和通用能力。\nRFT 迭代精炼数据和重训练模型，期望模型在每轮迭代学习更高质量数据模式，最终收敛到高质量输出模型。 迭代过程中，训练数据分布 \\(P_{sft}(Q, O)\\) 逐渐聚焦于高质量数据，使模型在损失最小化过程中不断提升生成高质量输出的能力。\nOnRFT 在线拒绝采样微调 (Online Rejection Sampling Fine-tuning, OnRFT) 目标与 RFT 类似，都是通过最小化选择性损失函数学习高质量输出模式。OnRFT 与 RFT 的主要区别在于数据采样方式，损失函数形式与 RFT 保持一致。OnRFT 的损失函数梯度为：\n\\[ \\nabla_{\\theta} \\mathcal{L}_{OnRFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q) \\times \\pi_{\\theta}(O \\mid q)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\mathbb{I}(o) \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]参数说明:\n\\(\\nabla_{\\theta} \\mathcal{L}_{OnRFT}\\): OnRFT 损失函数关于模型参数 \\(\\theta\\) 的梯度，指示损失函数下降方向。 \\(\\pi_{\\theta}(O \\mid q)\\): 给定问题 \\(q\\)，当前训练模型 生成输出 \\(O\\) 的条件概率分布。 RFT 与 OnRFT 对比 下面表格简单对比了 RFT 和OnRFT 的主要区别。\n特性 RFT OnRFT 数据生成方式 离线 (Offline) 在线 (Online) 数据生成模型 SFT 模型 \\(\\pi_{sft}\\) 当前训练模型 \\(\\pi_{\\theta}\\) 拒绝采样数据来源 预生成 SFT 数据集 训练时实时生成数据 数据循环 分离 在线循环 损失函数机制 选择性交叉熵损失，选择高质量输出学习 选择性交叉熵损失，选择高质量输出学习 训练数据分布变化 逐渐聚焦于高质量数据 动态变化，贴合当前模型能力 面向所有场景的强化学习 为了进一步对齐人类偏好，DeepSeek 团队进行了第二阶段 RL，目标是在最大化奖励函数的同时，提升模型的有用性 (Helpfulness) 和 无害性 (Harmlessness)，并兼顾推理能力。此阶段仍然是通过最大化奖励函数来指导模型训练，但奖励函数的设计更加复杂，以反映多维度的优化目标。\n此阶段的 RL 训练结合了：\n多样化的 Prompt 分布: 覆盖更广泛的场景，包括推理和通用任务。 多目标奖励信号: 推理数据: 沿用基于规则的任务奖励，侧重推理准确性。最大化任务奖励，引导模型最小化推理错误。 通用数据: 使用奖励模型捕捉人类对有用性和无害性的偏好。奖励模型的目标是学习人类偏好，并输出与人类偏好一致的奖励信号，RL 训练的目标是最大化奖励模型给出的奖励值，从而间接最小化模型输出与人类偏好之间的偏差。 蒸馏 为了将 DeepSeek-R1 的强大推理能力迁移到更高效的小型模型上，DeepSeek 团队采用了蒸馏（Distillation）（Hinton et al., 2015）技术。蒸馏过程主要包括以下步骤：\n数据生成: 利用训练好的 DeepSeek-R1 模型，生成约 80 万条高质量的推理数据。这些数据不仅包括推理密集型任务 (如数学题、编程题)，也涵盖了通用任务 (如问答、对话)，以保证蒸馏数据的多样性和覆盖面。\n模型微调: 将生成的 80 万条高质量推理数据，用于微调小型密集模型。蒸馏实验选择了 Qwen 和 Llama 系列模型作为 Student 模型，涵盖了从 1.5B 到 70B 参数的多种模型规模，以探索蒸馏技术在不同模型规模下的效果。选取的 Student 模型包括 Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, 和 Llama-3.3-70B-Instruct。\n性能评估: 在多个推理相关的 benchmark 中，对蒸馏后的模型进行全面的性能评估。评估结果旨在验证蒸馏技术是否能够有效地将大型模型的推理能力迁移到小型模型，并考察蒸馏后的小型模型在推理能力上是否能够达到甚至超越大型模型的水平。\nKL 散度蒸馏 除了直接使用 Teacher 模型生成的文本输出作为伪标签进行 SFT 蒸馏外，更严谨的方法是将 Teacher 模型生成的 token 概率分布 \\(\\pi_{\\text{teacher}}\\) 也纳入考虑。KL 散度蒸馏 是一种常用的方法，它不仅让 Student 模型学习 Teacher 模型的文本输出，也学习 Teacher 模型的 token 概率分布。通过最小化 Student 模型和 Teacher 模型输出概率分布之间的 KL 散度，可以更充分地将 Teacher 模型的知识迁移到 Student 模型中。但在实际工程中，直接使用 Teacher 模型的文本输出作为伪标签进行 SFT 蒸馏，通常也能取得足够好的效果，并且实现更简单。\n实验结果 实验结果如图7所示：\nFig. 7. Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. (Image source: DeepSeek-AI, 2025)\n实验结果表明，这种直接的 SFT 蒸馏方法能够显著提升小型模型的推理能力。值得注意的是，在除 CodeForces 之外的多个基准测试中，蒸馏后的 Llama-3.3-70B-Instruct 模型表现已超越 OpenAI-o1-mini。仅依靠 SFT 蒸馏更大的基座模型就能获得如此显著的提升，充分展现了这一方法在后续研究与应用中的潜力。\n讨论 DeepSeek-R1 在多阶段训练框架基础上，探索了 Reasoning Model 训练技术的简化路径，主要包括以下几点:\n线性化思维过程：CoT 替代 MCTS\n传统强化学习 AI，如围棋和象棋，曾依赖蒙特卡洛树搜索 (Monte Carlo Tree Search, MCTS)。DeepSeek-R1 等模型则探索使用自回归的链式思维方法简化推理过程，逐步摒弃了计算复杂度高的 MCTS。 CoT 将复杂推理分解为线性步骤，模型像解题一样逐步推理，而非 MCTS 的穷举式搜索。这种线性化思维降低了计算复杂度，更符合人类思维习惯，使模型更易学习复杂推理策略。 消除独立价值网络：简化 RL 架构\n传统强化学习 (如 PPO) 通常需要独立的策略网络和价值网络。DeepSeek-R1 等研究发现，强化的策略网络或简化的价值评估方法 (如 GRPO 的组相对评分) 可替代独立价值网络。 这简化了 RL 训练架构，降低了资源需求，提高了效率。表明大语言模型的策略网络已具备强大的价值评估能力，无需额外价值网络。 聚焦最终结果奖励：最小化奖励信号\nDeepSeek-R1 采用更加简单的 ORM 奖励策略，主要关注最终结果的准确性奖励，弱化中间推理步骤奖励。这种策略受 AlphaZero (Silver et al., 2017) 启发，后者仅关注胜负。 对于 Reasoning Model，最终结果奖励可能比 PRM 更有效，能帮助模型更自然地学习“思维方式”，减少繁琐的逐步监督。 增加思考时间：模型自发涌现深度思考\nDeepSeek-R1-Zero 训练中展现出自发 增加思考时间 的能力。模型随训练深入，根据问题复杂度自适应分配更多“思考时间”，生成更长推理序列。这种“思考时间”增加是模型 RL 训练中自发涌现的行为。 思考时间增加反映模型更深入探索和优化思维过程。复杂问题需要更多推理步骤才能找到答案。DeepSeek-R1-Zero 的自我进化能力印证了强化学习在提升模型推理能力方面的潜力。 总结 DeepSeek-R1 的成功展示了 RL 提升 LLM 推理能力的巨大潜力。DeepSeek-R1 采用的 GRPO 算法在计算效率、优化稳定性、奖励鲁棒性等方面优于 PPO 和 DPO，并通过简化模型架构降低了训练资源消耗。DeepSeek-R1 为开源 Reasoning Model 复现 o1 提供了一条值得参考的路径。\n参考文献 [1] Guo, Daya, et al. \u0026ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\u0026rdquo; arXiv preprint arXiv:2501.12948 (2025).\n[2] OpenAI. \u0026ldquo;Introducing OpenAI o1\u0026rdquo; OpenAI, 2024.\n[3] Jaech, Aaron, et al. \u0026ldquo;OpenAI o1 system card.\u0026rdquo; arXiv preprint arXiv:2412.16720 (2024).\n[4] Open-r1. HuggingFace, 2024.\n[5] Sutton, Richard. \u0026ldquo;The bitter lesson.\u0026rdquo; Incomplete Ideas (blog) 13.1 (2019): 38.\n[6] Liu A, et al. \u0026ldquo;Deepseek-v3 technical report.\u0026rdquo; arXiv preprint arXiv:2412.19437 (2024).\n[7] Schulman, John, et al. \u0026ldquo;Proximal policy optimization algorithms.\u0026rdquo; arXiv preprint arXiv:1707.06347 (2017).\n[8] Ouyang, Long, et al. \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in neural information processing systems 35 (2022): 27730-27744.\n[9] Schulman, John, et al. \u0026ldquo;High-dimensional continuous control using generalized advantage estimation.\u0026rdquo; arXiv preprint arXiv:1506.02438 (2015).\n[10] Shao, Zhihong, et al. \u0026ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\u0026rdquo; arXiv preprint arXiv:2402.03300 (2024).\n[11] J. Schulman. Approximating kl divergence, 2020.\n[12] Gao, Leo, John Schulman, and Jacob Hilton. \u0026ldquo;Scaling laws for reward model overoptimization.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n[13] Chen, Mark, et al. \u0026ldquo;Evaluating large language models trained on code.\u0026rdquo; arXiv preprint arXiv:2107.03374 (2021).\n[14] Learning to Reason with LLMs. OpenAI, 2024.\n[15] AMC. Mathematical Association of America (MAA), 2024.\n[16] Open-O1. Open-Source O1, 2024.\n[17] Zeng, Zhiyuan, et al. \u0026ldquo;Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective.\u0026rdquo; arXiv preprint arXiv:2412.14135 (2024).\n[18] Hinton, Geoffrey. \u0026ldquo;Distilling the Knowledge in a Neural Network.\u0026rdquo; arXiv preprint arXiv:1503.02531 (2015).\n[19] Silver, David, et al. \u0026ldquo;Mastering chess and shogi by self-play with a general reinforcement learning algorithm.\u0026rdquo; arXiv preprint arXiv:1712.01815 (2017).\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Jan 2025). OpenAI o1复现进展：DeepSeek-R1. https://syhya.github.io/posts/2025-01-27-deepseek-r1\nOr\n@article{syhya2025deepseekr1, title = \u0026#34;OpenAI o1复现进展：DeepSeek-R1\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-27-deepseek-r1\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-01-27-deepseek-r1/","summary":"\u003cp\u003eDeepSeek AI 近期发布 \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/2501.12948\"\u003eDeepSeek-AI, 2025\u003c/a\u003e)，其推理性能在多个 benchmark 上已接近 OpenAI o1 (\u003ca href=\"https://openai.com/o1/\"\u003eOpenAI, 2024\u003c/a\u003e)的水平，是开源社区成功复现 o1 的重要一步。R1 相关代码可以参考huggingface 尝试开源复现 \u003ca href=\"https://github.com/huggingface/open-r1\"\u003eopen-r1\u003c/a\u003e 项目。以往的研究多依赖于海量的监督数据来提升大语言模型（Large Language Model, LLM）性能，但 DeepSeek-R1 及其早期实验 DeepSeek-R1-Zero 的成功，有力证明了纯粹大规模强化学习在提升 LLM 推理能力方面的潜力。其印证了 Richard Sutton 在 “The Bitter Lesson” 中提出的深刻见解:\u003c/p\u003e","title":"OpenAI o1复现进展：DeepSeek-R1"},{"content":"背景 Transformer (Vaswani et al., 2017）是一种基于编码器-解码器架构的模型。此模型在自然处理领域中展示了卓越的性能，随后一系列模型在此基础上进行了优化，例如仅使用编码器的 BERT (Devlin et al., 2018）或仅使用解码器的 GPT (Radford et al., 2018）系列，以及后续的大型语言模型如 LLaMA (Touvron et al., 2023）和 GPT-4 (OpenAI al., 2024）系列，这些模型大多采用了仅解码器的结构。\n符号 符号 含义 \\(B\\) 批量大小（Batch Size） \\(S\\) 序列长度（Sequence Length） \\(d\\) 隐藏维度 / 模型维度（Model Size / Hidden Dimension） \\(H\\) 注意力头数量（Number of Heads in Multi-Head Attention） \\(G\\) 分组数量（Group Number），用于分组查询注意力（GQA） \\(d_{\\text{head}} = \\frac{d}{H}\\) 每个注意力头的维度 \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) 输入序列，批量为 \\(B\\)，序列长度为 \\(S\\)，隐藏维度为 \\(d\\) \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{B \\times S \\times d}\\) 经过线性变换后的 Query、Key、Value 矩阵 \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) 分别对应生成 \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) 的可训练线性映射矩阵 \\(W_O \\in \\mathbb{R}^{d \\times d}\\) 多头 / 分组注意力输出后，用于映射回原始维度 \\(d\\) 的可训练线性映射矩阵 \\(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h \\in \\mathbb{R}^{B \\times S \\times d_{\\text{head}}}\\) 第 \\(h\\) 个注意力头对应的 Query、Key、Value 子矩阵 \\(\\mathbf{K}^*, \\mathbf{V}^*\\) 在多查询注意力（MQA）中，将所有头的 \\(\\mathbf{K}_h, \\mathbf{V}_h\\) 平均或合并后得到的共享 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\) \\(\\mathbf{q}, \\mathbf{k}\\in \\mathbb{R}^{d_{\\text{head}}}\\) 在缩放点积注意力的随机向量示例中，用于数学推导（中心极限定理）的单个查询向量和单个键向量 Transformer中的注意力机制 Transformer模型的核心在于自注意力机制（Self-Attention），它允许模型在处理序列数据时，动态地关注序列中的不同部分。具体来说，给定一个输入序列 \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\)（批大小 \\(B\\)，序列长度 \\(S\\)，隐藏维度 \\(d\\)），Transformer会通过三个线性层分别投影为查询（Query, \\(\\mathbf{Q}\\)）、键（Key, \\(\\mathbf{K}\\)）和值（Value, \\(\\mathbf{V}\\)）：\n\\[ \\mathbf{Q} = \\mathbf{X} W_Q, \\quad \\mathbf{K} = \\mathbf{X} W_K, \\quad \\mathbf{V} = \\mathbf{X} W_V \\]其中，\\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) 是可训练的权重矩阵。多头注意力通过将这些投影分成多个头，每个头负责不同的子空间表示，从而增强模型的表示能力。\n注意力机制有多种形式，Transformer 依赖于缩放点积注意力（Scaled Dot-Product Attention）：给定查询矩阵 \\(\\mathbf{Q}\\)、键矩阵 \\(\\mathbf{K}\\) 和值矩阵 \\(\\mathbf{V}\\)，输出是值向量的加权和，其中每个值的权重由查询与对应键的点积决定：\n\\[ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V} \\] Fig. 1. Scaled Dot-Product Attention. (Image source: Vaswani et al., 2017)\n多头注意力（MHA） 多头注意力（MHA）将 \\(\\mathbf{Q}\\)、\\(\\mathbf{K}\\)、\\(\\mathbf{V}\\) 分成多个头，每个头有独立的 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)，从而增加了模型的容量和灵活性：\n\\[ \\text{MHA}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]其中，每个头的计算为：\n\\[ \\text{head}_h = \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q}_h \\mathbf{K}_h^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V}_h \\] Fig. 2. Multi-Head Attention. (Image source: Vaswani et al., 2017)\n使用多头注意力机制的好处 捕捉不同的特征：单头注意力机制只能关注输入序列中的一种特征或模式，而多头注意力机制可以通过多个注意力头同时关注不同的特征或模式，使模型能够更全面地理解输入数据。 增强模型的表达能力：每个注意力头可以学习不同的表示方式，增强模型的表达能力。不同的注意力头可以关注输入序列的不同部分或不同关系，帮助模型更好地捕捉复杂的依赖关系。 提高稳定性和性能：多头注意力机制通过多个注意力头的平均或组合，减少单个注意力头的噪声和不稳定性，提高模型的稳定性和性能。 并行计算：多头注意力机制可以在计算上并行化，因为每个注意力头的计算是独立的。这有助于提高计算效率，特别是在使用GPU或TPU等硬件加速器时。 缩放点积注意力中的Softmax Softmax函数将一个向量 \\(\\mathbf{z} = [z_1, z_2, \\dots, z_n]\\) 转换为一个概率分布 \\(\\mathbf{y} = [y_1, y_2, \\dots, y_n]\\)，其定义如下：\n\\[ y_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)} \\quad \\text{对于} \\quad i = 1, 2, \\dots, n \\]在注意力机制中，softmax函数用于将缩放后的点积 \\(\\tfrac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\) 转换为注意力权重：\n\\[ \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr) = \\Bigl[ \\frac{\\exp\\Bigl(\\frac{Q_1 \\cdot K_1}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_1 \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)}, \\dots, \\frac{\\exp\\Bigl(\\frac{Q_S \\cdot K_S}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_S \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)} \\Bigr] \\]在 Transformer 的注意力机制中，缩放点积注意力公式中的缩放因子 \\(\\sqrt{d_{\\text{head}}}\\) 是为了确保在进行 softmax 之前，点积的结果不会因为向量维度的增加而变得过大。这主要有以下几个原因：\n防止梯度消失：通过缩放注意力得分，可以避免输入 softmax 函数的值过大，从而防止梯度在反向传播过程中出现消失的情况。\n数值不稳定性：缩放注意力得分可以使得 softmax 函数的输入值范围更加合理，避免数值过于极端，从而提升模型的数值稳定性和训练效果。特别是当向量维度较大时，未经缩放的点积结果可能导致 softmax 的指数函数值过大，进而引发溢出问题。\n数学解释：假设向量 \\(\\mathbf{q}\\) 和 \\(\\mathbf{k}\\) 的各分量独立同分布，均值为 0，方差为 1。它们的点积 \\(\\mathbf{q} \\cdot \\mathbf{k}\\) 的均值为 0，方差为 \\(d_{\\text{head}}\\)。为了防止点积的方差随维度 \\(d_{\\text{head}}\\) 增加而变大，需要对其进行缩放处理。通过将点积除以 \\(\\sqrt{d_{\\text{head}}}\\)，可以使缩放后的点积的方差为 1，与 \\(d_{\\text{head}}\\) 无关。\n根据统计学原理，当将随机变量除以一个常数时，其方差会按该常数的平方倒数缩放。因此，缩放因子 \\(\\tfrac{1}{\\sqrt{d_{\\text{head}}}}\\) 可以有效控制注意力得分的规模，从而提高数值稳定性。以下是详细推导过程：\n假设 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^{d_{\\text{head}}}\\)，各分量独立同分布，均值为 0，方差为 1，则它们的点积为：\n\\[ \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d_{\\text{head}}} q_i k_i \\]根据中心极限定理，当 \\(d_{\\text{head}}\\) 较大时，点积 \\(\\mathbf{q} \\cdot \\mathbf{k}\\) 近似服从均值为 0、方差为 \\(d_{\\text{head}}\\) 的正态分布：\n\\[ \\mathbf{q} \\cdot \\mathbf{k} \\sim \\mathcal{N}(0, d_{\\text{head}}) \\]为了使缩放后的点积具有单位方差，我们将点积除以 \\(\\sqrt{d_{\\text{head}}}\\)：\n\\[ \\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}} \\;\\sim\\; \\mathcal{N}\\!\\Bigl(0, \\frac{d_{\\text{head}}}{d_{\\text{head}}}\\Bigr) = \\mathcal{N}(0, 1) \\]因此，经过缩放后，点积 \\(\\tfrac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}}\\) 的方差恒为 1，与维度 \\(d_{\\text{head}}\\) 无关。这种缩放操作能够保持点积在一个稳定的范围内，避免 softmax 函数在计算中因输入值过大或过小而产生数值不稳定性。\n多查询注意力（MQA） 多查询注意力（MQA）(Shazeer, 2019) 通过让所有查询头（Query Heads）共享同一组键（Key）\\(\\mathbf{K}\\) 和值（Value）\\(\\mathbf{V}\\)，从而显著减少了显存带宽的需求。具体地，如果我们将传统多头注意力（MHA）中的所有 \\(\\mathbf{K}_h\\) 和 \\(\\mathbf{V}_h\\) 做如下平均：\n\\[ \\mathbf{K}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{K}_h, \\quad \\mathbf{V}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{V}_h, \\]其中 \\(H\\) 表示查询头的数量，\\(\\mathbf{K}_h\\) 和 \\(\\mathbf{V}_h\\) 分别表示第 \\(h\\) 个头对应的键和值。那么在推理过程中，每个头只需要使用同一个 \\(\\mathbf{K}^*\\) 和 \\(\\mathbf{V}^*\\)，从而大幅降低对显存带宽的占用。最后再将所有头输出拼接并映射回输出空间：\n\\[ \\text{MQA}(\\mathbf{Q}, \\mathbf{K}^*, \\mathbf{V}^*) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]由于键和值只保留了一组，MQA 推理速度更快，但在某些场景下，模型的表达能力和性能会受到一定限制。\n分组查询注意力（GQA） 分组查询注意力（GQA） (Ainslie, 2023) 是介于 MHA 和 MQA 之间的一种折中方案。它通过将查询头分为多个组，让每组共享一组 $\\mathbf{K}$ 和 $\\mathbf{V}$ 头，以在推理速度和模型性能之间取得平衡。每组包含 $\\frac{H}{G}$ 个查询头，每组共享一组 $\\mathbf{K}$ 和 $\\mathbf{V}$ 头。其具体流程如下：\n投影：将输入 $\\mathbf{X}$ 通过线性变换分别投影为 $\\mathbf{Q}$、$\\mathbf{K}$、$\\mathbf{V}$。 分组 Query：将 $\\mathbf{Q}$ 划分为 $H$ 个头后，再将这些头进一步划分为 $G$ 组。 分组 Key/Value：将 $\\mathbf{K}$ 和 $\\mathbf{V}$ 划分为 $G$ 组，每组共享一组 $\\mathbf{K}$ 和 $\\mathbf{V}$。 组内注意力：对每组的 $\\mathbf{Q}$ 与各自组共享的 $\\mathbf{K}$ 和 $\\mathbf{V}$ 进行注意力计算。 拼接输出：将各组的注意力结果在通道维度上拼接，最后通过线性层得到最终输出。 三种 Attention 方法之间的联系 Fig. 3. Overview of grouped-query method. (Image source: Ainslie et al., 2023)\n图3直观展示了这三种注意力机制的关系：多头注意力（MHA）为每个查询头都保留独立的 $\\mathbf{K}$ 和 $\\mathbf{V}$；多查询注意力（MQA）则所有查询头共享同一组 $\\mathbf{K}$ 和 $\\mathbf{V}$；分组查询注意力（GQA）则在两者之间，通过分组共享的方式兼顾速度与性能。\n当 $G=1$ 时：所有查询头共享同一组 $\\mathbf{K}$ 和 $\\mathbf{V}$。此时 GQA 退化为多查询注意力（MQA）。\n$\\mathbf{K}/\\mathbf{V}$ 头数量：$1$ 模型行为：所有头使用相同的 $\\mathbf{K}$ 和 $\\mathbf{V}$ 进行注意力计算，显著降低显存带宽需求。 当 $G=H$ 时：每个查询头都拥有独立的 $\\mathbf{K}$ 和 $\\mathbf{V}$。此时 GQA 退化为多头注意力（MHA）。\n$\\mathbf{K}/\\mathbf{V}$ 头数量：$H$ 模型行为：每个头使用完全独立的 $\\mathbf{K}$ 和 $\\mathbf{V}$，保留 MHA 的高模型容量和性能。 通过调整分组数量 $G$，GQA 在 MHA 和 MQA 之间实现了灵活切换，能够在保持较高模型性能的同时，兼顾推理速度的提升。\n实现代码片段 下面是使用 PyTorch 简单实现的 MHA 、MQA和 GQA 的代码, 其中 GQA采用了广播（Broadcast）和复制（Repeat）两种方法。此外 需要注意的是，在实际的 LLaMA3 源代码中，GQA 的实现还引入了 KV Cache。为简化示例，以下代码并未包含该部分。如果感兴趣，可以参考源代码 model.py 获取更完整的代码细节。\nMHA 代码片段 multi_head_attention.py\nimport math import torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # (nums_head * head_dim = hidden_dim) assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(dropout_rate) # Define linear projection layers self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): # x has shape: (batch_size, seq_len, hidden_dim) batch_size, seq_len, _ = x.size() # Q, K, V each has shape: (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) K = self.k_proj(x) V = self.v_proj(x) # Reshaping from (batch_size, seq_len, hidden_dim) to (batch_size, seq_len, nums_head, head_dim) # Then transpose to (batch_size, nums_head, seq_len, head_dim) # q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3) # [Another approach to do it] q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) k = K.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Matrix multiplication: (batch_size, nums_head, seq_len, head_dim) * (batch_size, nums_head, head_dim, seq_len) # Resulting shape: (batch_size, nums_head, seq_len, seq_len) # Note that the scaling factor uses head_dim, not hidden_dim. attention_val = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) print(f\u0026#34;attention_mask shape is {attention_mask.size()}\u0026#34;) if attention_mask is not None: # If attention_mask is provided, it should have shape (batch_size, nums_head, seq_len, seq_len). assert attention_val.size() == attention_mask.size() attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # Apply softmax along the last dimension to get attention weights. attention_weight = torch.softmax(attention_val, dim=-1) # Dropout on attention weights attention_weight = self.dropout(attention_weight) # Multiply attention weights with V: # (batch_size, nums_head, seq_len, seq_len) * (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v # Transpose back: (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) # # Note: The transpose operation changes the dimension ordering but does not change the memory layout, # resulting in a non-contiguous tensor. The contiguous() method makes the tensor contiguous in memory, # allowing subsequent view or reshape operations without error. output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) # output = output_mid.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.hidden_dim) # # [Another approach to do it] output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 # attention_mask has shape: (batch_size, nums_head, seq_len, seq_len). # Here we use a lower-triangular mask to simulate causal masking. attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_head_attention = MultiHeadAttention(hidden_dim=hidden_dim, nums_head=nums_head) x_forward = multi_head_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) MQA 代码片段 multi_query_attention.py\nimport torch import torch.nn as nn import math class MultiQueryAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(p=dropout) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # For kv, project: hidden_dim -\u0026gt; head_dim self.k_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.v_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() Q, K, V = self.q_proj(x), self.k_proj(x), self.v_proj(x) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Broadcast k and v to match q\u0026#39;s dimensions for attention computation # k -\u0026gt; (batch_size, 1, seq_len, head_dim) # v -\u0026gt; (batch_size, 1, seq_len, head_dim) k = K.unsqueeze(1) v = V.unsqueeze(1) # (batch_size, head_num, seq_len, head_dim) * (batch_size, 1, head_dim, seq_len) # -\u0026gt; (batch_size, head_num, seq_len, seq_len) attention_val = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) attention_weight = torch.softmax(attention_val, dim=-1) print(f\u0026#34;attention_weight is {attention_weight}\u0026#34;) attention_weight = self.dropout(attention_weight) # (batch_size, head_num, seq_len, seq_len) * (batch_size, 1, seq_len, head_dim) # -\u0026gt; (batch_size, head_num, seq_len, head_dim) output_tmp = attention_weight @ v # -\u0026gt; (batch_size, seq_len, head_num, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_query_attention = MultiQueryAttention(hidden_dim=hidden_dim, nums_head=nums_head, dropout=0.2) x_forward = multi_query_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) GQA 代码片段 group_query_attention.py\nimport math import torch import torch.nn as nn class GQABroadcast(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # Total number of Q heads (H) self.nums_kv_head = nums_kv_head # Number of K, V heads (G, groups) assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head # Number of Q heads per group self.q_heads_per_group = nums_head // nums_kv_head self.dropout = nn.Dropout(dropout_rate) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # Projection output dimensions for K, V = nums_kv_head * head_dim self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask= None): batch_size, seq_len, _ = x.size() Q = self.q_proj(x) # (batch_size, seq_len, hidden_dim) K = self.k_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) V = self.v_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) # Q: (batch_size, seq_len, hidden_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2).contiguous() q = q.view(batch_size, self.nums_kv_head, self.q_heads_per_group, seq_len, self.head_dim) # K, V: (batch_size, seq_len, nums_kv_head * head_dim) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim # -\u0026gt; (batch_size, nums_kv_head, 1, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) # q: (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) * (batch_size, nums_kv_head, 1, head_dim, seq_len) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_val = q @ k.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) * (batch_size, nums_kv_head, 1, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) output_tmp = attention_weight @ v # (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = output_tmp.view(batch_size, self.nums_head, seq_len, self.head_dim) # (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) -\u0026gt; (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output class GQARepeat(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head self.nums_kv_head = nums_kv_head assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head self.q_head_per_group = nums_head // nums_kv_head self.q_proj = nn.Linear(hidden_dim, nums_head * self.head_dim) self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) self.dropout = nn.Dropout(dropout_rate) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() # (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) # (batch_size, seq_len, nums_kv_head * self.head_dim) K = self.k_proj(x) V = self.v_proj(x) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) # (batch_size, nums_head, seq_len, head_dim) k_repeat = k.repeat_interleave(self.q_head_per_group, dim=1) v_repeat = v.repeat_interleave(self.q_head_per_group, dim=1) # (batch_size, nums_head, seq_len, seq_len) attention_val = q @ k_repeat.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#39;-inf\u0026#39;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v_repeat # (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 16) batch_size, seq_len, hidden_dim = x.size() nums_head = 8 head_dim = hidden_dim // nums_head nums_kv_head = 4 q_heads_per_group = nums_head // nums_kv_head # v1: Broadcast # attention_mask_v1 has shape: (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_mask_v1 = torch.tril(torch.ones(batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len)) gqa_broadcast = GQABroadcast(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v1 = gqa_broadcast.forward(x, attention_mask=attention_mask_v1) # print(x_forward_v1) print(x_forward_v1.size()) # v2: Repeat # attention_mask_v2 has shape: (batch_size, nums_head, seq_len, seq_len) attention_mask_v2 = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) gqa_repeat = GQARepeat(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v2 = gqa_repeat.forward(x, attention_mask=attention_mask_v2) # print(x_forward_v2) print(x_forward_v2.size()) 时间与空间复杂度分析 说明：下文针对的是一次前向传播（forward propagation）的复杂度；在训练时，还需要额外考虑反向传播（backward propagation）与参数更新。反向传播不仅依赖前向传播保存的中间激活值，还需额外计算梯度和存储中间导数，通常使得总计算量和内存占用比前向传播高，导致训练耗时为前向传播的数倍。\n在分析不同注意力机制（MHA、MQA、GQA）时，我们主要关注它们在 自注意力（self-attention） 或 交叉注意力（cross-attention） 过程中，进行前向传播时的时间复杂度和空间复杂度。即使它们在实现细节上（例如是否共享 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)）有所不同，但从计算量和主要的缓存/显存使用角度来看，其量级大致保持一致。\n假设每个位置都会生成查询 \\(\\mathbf{Q}\\)、键 \\(\\mathbf{K}\\) 和值 \\(\\mathbf{V}\\) 的表征，且各矩阵按批量和头数拆分之后的形状如同下式所示：\n\\[ \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\;\\in\\; \\mathbb{R}^{B \\times H \\times S \\times d_{\\text{head}}} \\]时间复杂度分析 矩阵乘法的通用时间复杂度 对于形状为 $m \\times n$ 的矩阵 $\\mathbf{A}$ 与形状为 $n \\times p$ 的矩阵 $\\mathbf{B}$ 进行乘法 $\\mathbf{A}\\mathbf{B}$，其时间复杂度一般表示为：\n$$ \\mathcal{O}(m \\times n \\times p) $$在注意力机制的计算中，这一基本结论常用于分析 $\\mathbf{Q}\\mathbf{K}^\\top$ 以及注意力分数与 $\\mathbf{V}$ 的乘法等。\n自注意力计算的主要步骤及复杂度 点积计算 ($\\mathbf{Q}\\mathbf{K}^\\top$)\n$\\mathbf{Q}$ 形状：$B \\times H \\times S \\times d_{\\text{head}}$\n$\\mathbf{K}$ 形状：$B \\times H \\times S \\times d_{\\text{head}}$\n因此 $\\mathbf{Q}\\mathbf{K}^\\top$ 的结果形状为 $B \\times H \\times S \\times S$。\n具体的计算量可以视为：对每个批次、每个头，以及序列内所有位置对 $(S \\times S)$ 的点积，其中每个点积涉及 $d_{\\text{head}}$ 维度的乘加运算。\n故其时间复杂度为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S \\times S \\times d_{\\text{head}}\\bigr) \\;=\\; \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) $$ softmax 操作\n在得到的注意力分数矩阵 $B \\times H \\times S \\times S$ 上进行逐元素的 softmax 运算。\nsoftmax 对矩阵的每个元素执行指数与归一化操作，其复杂度一般为：\n$$ \\mathcal{O}(\\text{元素数}) = \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) $$ 相对于上一步的矩阵乘法，其依赖维度 $d_{\\text{head}}$ 的项可以忽略。因此常将其视为比矩阵乘法更小的开销。\n加权平均（注意力分数与 $\\mathbf{V}$ 的乘法）\n$\\mathbf{V}$ 形状：$B \\times H \\times S \\times d_{\\text{head}}$\n注意力分数矩阵形状：$B \\times H \\times S \\times S$\n将每个位置的注意力分数与对应的 $\\mathbf{V}$ 向量乘加之后，输出仍是 $B \\times H \\times S \\times d_{\\text{head}}$。\n其时间复杂度与 $\\mathbf{Q}\\mathbf{K}^\\top$ 的分析类似：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) $$ 将上述三步综合，最主要的开销来自两次矩阵乘法，各为 $\\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}})$。因此在一次完整前向计算时，量级可写为：\n$$ \\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}}) = \\mathcal{O}(B \\times S^2 \\times d). $$（这里用到了 $d_{\\text{head}} = \\frac{d}{H}$）\n增量解码/推理场景（KV Cache）下的时间复杂度 Fig. 4. KV cache example. (Image source: Efficient NLP YouTube Channel)\n参考图4在推理场景（尤其自回归生成）中，通常会使用 KV Cache 来缓存先前时刻的 $\\mathbf{K}$, $\\mathbf{V}$，从而避免重复计算。此时，每生成一个新 token（即处理一个新的时间步）只需：\n对新 token 计算 $\\mathbf{Q}$（及对应的 $\\mathbf{K}$, $\\mathbf{V}$）\n若只保留了投影权重，则新产生的 $\\mathbf{Q}$ 和当前时刻的 $\\mathbf{K}$, $\\mathbf{V}$ 仅涉及 $\\mathcal{O}(d^2)$ 参数乘法，但这是对单个 token而言，相对开销不大。 与已有 KV Cache 做注意力\nKV Cache 中存储了所有先前时刻的 $\\mathbf{K}$, $\\mathbf{V}$，形状约为：\n$$ B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} $$此时 $S_{\\text{past}}$ 表示已经生成的序列长度。\n新的 $\\mathbf{Q}$ 形状是 $B \\times H \\times 1 \\times d_{\\text{head}}$，故新 token 的注意力分数计算为：\n$$ \\mathbf{Q}\\mathbf{K}^\\top : \\; \\mathcal{O}\\bigl(B \\times H \\times 1 \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$ 同理，对 $\\mathbf{V}$ 的加权得到新 token 的输出，也有相同量级：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$ 更新 KV Cache\n将新产生的 $\\mathbf{K}$, $\\mathbf{V}$ 追加到 KV Cache 中，以备下一个时间步使用。此操作在时间复杂度上只是简单的 concat/append，主要在空间上会不断增长。 因此，在增量解码时，每个新 token 的计算量约为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$而不是一次性地进行 $S \\times S$ 规模的注意力计算。若要生成长度为 $S$ 的序列，总体时间在理想情况下也可归纳为\n$$ \\sum_{k=1}^{S} \\mathcal{O}\\bigl(B \\times H \\times k \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) $$与一次性计算的复杂度同阶，只是一次性计算与逐步计算的差异。每步只处理 1 个 token 的注意力时，峰值的临时计算量更小，也无需存储完整的 $S \\times S$ 注意力分数矩阵。\n时间复杂度总结 MHA（多头注意力）：头数多，但每个头分别计算 $\\mathbf{K}$, $\\mathbf{V}$。 MQA（多查询注意力）：多个头共享 $\\mathbf{K}$, $\\mathbf{V}$。 GQA（分组注意力）：将 $H$ 个头分成 $G$ 个组，每组共享一组 $\\mathbf{K}$, $\\mathbf{V}$。 不论 MHA / MQA / GQA，在 完整前向 下，它们的主要矩阵乘法复杂度均为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) $$而在增量推理场景（KV Cache）下，单步计算复杂度降低为\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$但需要在多步解码过程中维护并更新 KV Cache。\n空间复杂度分析 空间复杂度既包括模型参数（权重参数）的规模，也包括前向计算时需要的中间激活值（尤其是注意力得分矩阵、加权结果，以及可能的 KV Cache）的规模。\n模型参数规模 线性投影层的参数\n对输入向量（维度 $d$）投影到 $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ 的维度：\n$$ \\underbrace{d \\times d}_{\\mathbf{Q}\\text{的投影}} \\;+\\; \\underbrace{d \\times d}_{\\mathbf{K}\\text{的投影}} \\;+\\; \\underbrace{d \\times d}_{\\mathbf{V}\\text{的投影}} = 3d^2 $$ 一般而言，这些参数会再根据头数 $H$ 切分成多头的形式，但总和并不因为头数增加而改变。故其量级为 $\\mathcal{O}(d^2)$。\n输出合并层的参数\n将多头输出拼接后再投影回维度 $d$ 时，通常还会有一个 $d \\times d$ 的线性层。这也同样是 $\\mathcal{O}(d^2)$。\n因此，若单独把二者相加，有\n$$ 3d^2 + d^2 = 4d^2 $$ 仍然可记作 $\\mathcal{O}(d^2)$。\n前向计算的中间激活值 在进行训练或完整前向时，需要缓存如下主要张量：\n注意力分数矩阵\n形状为 $B \\times H \\times S \\times S$。无论使用 MHA、MQA 还是 GQA，每个头（或组）都需要计算与 $\\mathbf{Q}\\mathbf{K}^\\top$ 相关的注意力分数，其规模量级为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) $$ 加权后的输出\n形状为 $B \\times H \\times S \\times d_{\\text{head}}$，对应每个位置在前向计算中得到的注意力上下文向量。其量级为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S \\times d\\bigr) $$ 不同注意力机制下的 $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ 存储\n一般在反向传播时，需要缓存 $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ 的前向输出（或中间梯度）。若要显式存储，其形状及规模通常如下：\nMHA（多头注意力） $\\mathbf{Q}$: $B \\times H \\times S \\times d_{\\text{head}}$ $\\mathbf{K}$, $\\mathbf{V}$: $B \\times H \\times S \\times d_{\\text{head}}$ MQA（多查询注意力） $\\mathbf{Q}$: $B \\times H \\times S \\times d_{\\text{head}}$ $\\mathbf{K}$, $\\mathbf{V}$（共享）: $B \\times S \\times d$ GQA（分组注意力） $\\mathbf{Q}$: $B \\times H \\times S \\times d_{\\text{head}}$ $\\mathbf{K}$, $\\mathbf{V}$（分组共享）: $B \\times G \\times S \\times d_{\\text{head}}$, 其中 $G \\times d_{\\text{head}} = d$ 增量解码（KV Cache）下的空间消耗 在推理（增量解码）场景，往往会使用 KV Cache 来保存先前时刻的所有 Key、Value，以免反复计算。此时的存储结构通常是：\nKV Cache 维度（以 MHA 为例）：\n$$ \\mathbf{K}, \\mathbf{V} : B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} $$随着生成序列长度 $S_{\\text{past}}$ 的增长，KV Cache 会线性增大。\n单步注意力分数矩阵：\n由于每次只对新 token 进行注意力计算，分数矩阵的形状约为\n$$ B \\times H \\times 1 \\times S_{\\text{past}} $$显著小于训练时的 $B \\times H \\times S \\times S$。\n因此，增量解码时，大部分临时激活开销（如完整的 $S \\times S$ 矩阵）不再需要，但需要为 KV Cache 额外分配一份 $\\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}})$ 的显存，并随着序列长度增长而累积。\n综合空间复杂度 训练/完整前向\n主要激活值（注意力分数矩阵 + 输出 + Q, K, V 显式缓存）可合并表示为\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) $$当 $S$ 较大时，$B \\times H \\times S^2$ 常是主要瓶颈。\n推理/增量解码（KV Cache）\n无需完整的 $S^2$ 注意力分数矩阵，但需要一份\n$$ \\mathbf{K},\\mathbf{V}\\text{ Cache}: \\;\\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}) $$会随着解码步数 $S_{\\text{past}}$ 增长而线性增加。\n单次注意力分数仅为 $B \\times H \\times 1 \\times S_{\\text{past}}$ 的临时存储，量级显著小于训练场景。\n结论与对比 时间复杂度\n对于自注意力机制，无论是 MHA、MQA 还是 GQA，在完整前向场景下（训练时亦会包含该前向过程），主要的矩阵运算都保持相同量级：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) $$ 在 增量推理（KV Cache） 场景下，每个新 token 只需\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$的计算，但需要维护并更新 KV Cache。\n空间复杂度\n模型参数：三者都在 $\\mathcal{O}(d^2)$ 量级。\n中间激活值（训练/完整前向）：主要由注意力分数矩阵和输出决定，量级为\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) $$ 增量解码（KV Cache）：节省了 $S^2$ 大小的临时分数矩阵，但需要一份随着 $S_{\\text{past}}$ 增长的 K, V 缓存\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$ MQA / GQA 的优势\n虽然从大 $S$ 场景的理论时间复杂度看，MQA、GQA 与 MHA 并无数量级的差别，但它们在键、值共享（或分组共享）带来的实际带宽、缓存访存效率方面，往往能在工程实现中取得更好的显存和速度性能。 下表总结了 MHA、MQA 和 GQA 三种注意力机制的主要差异：\n特性 多头注意力 (MHA) 多查询注意力 (MQA) 分组查询注意力 (GQA) K/V头数量 与头数量相同（$H$） 单一K/V头 分组数（$G$）组，每组1个K/V头 推理时间 较慢 最快 较快，但略高于MQA 显存带宽需求 最高，$H$倍的K/V加载 最低，仅1个K/V头 介于MHA和MQA之间，$G$倍的K/V加载 模型容量 最高 最低 中等，取决于分组数$G$ 性能表现 最佳 略低于MHA 接近MHA，显著优于MQA 向上训练需求 无需 高，需要更多的稳定性和调整 较低，GQA模型在少量数据进行向上训练后即可稳定运行 适用场景 高性能需求但推理速度不敏感的应用 推理速度要求极高，且对模型性能要求较低的场景 需要在推理速度和模型性能之间取得平衡的应用 实验结果 性能测试 本实验在一台配备双 NVIDIA RTX 4090 GPU 的环境下进行，采用数据并行（Data Parallel, DP）方式，将批量大小（batch size）均匀拆分到两张 GPU 上。实验仅测试了前向传播的性能表现，包括平均延迟时间（Time_mean，单位：ms）和峰值显存占用（Peak_Mem_mean，单位：MB），以评估不同注意力机制（MHA、MQA 和 GQA）在推理阶段的资源需求和效率。\n实验代码请参考benchmark_attention.py。 测试基于 Llama3 8B 参数超参数设置 Fig. 5. Overview of the key hyperparameters of Llama 3. (Image source: Grattafiori et al., 2024)\n主要设置参数如下:\n总层数：32 层。 隐藏层维度：4096。 多头注意力总头数：32。 不同组数（nums_kv_head）配置：32（MHA）、1（MQA）、8（GQA-8）。 实验结果 本节主要介绍在不同序列长度（512、1024 和 1536）下，多头注意力（MHA）、多查询注意力（MQA）以及组查询注意力（GQA-8）的实验表现，包含时间延迟和显存占用两个方面的数据。为了方便对比，下表给出了三种注意力机制的具体测试结果。\nModel Size Method nums_kv_head Seq Length Time_mean (ms) Peak_Mem_mean (MB) Llama3 8B GQA-8 8 512 40.8777 2322.328 Llama3 8B MHA 32 512 53.0167 2706.375 Llama3 8B MQA 1 512 37.3592 2210.314 Llama3 8B GQA-8 8 1024 85.5850 6738.328 Llama3 8B MQA 1 1024 80.8002 6570.314 Llama3 8B MHA 32 1024 102.0514 7314.375 Llama3 8B GQA-8 8 1536 147.5949 13586.328 Llama3 8B MHA 32 1536 168.8142 14354.375 Llama3 8B MQA 1 1536 141.5059 13362.314 Fig. 6. Average Time Benchmark.\nFig. 7. Average Peak Memory Benchmark.\n在显存和时间开销敏感的场景下，MQA 和 GQA-8 是更高效的选择，其中 MQA 表现最优，但可能在模型性能能力上有所不足；GQA-8 则在效率和性能之间达到了良好的平衡。\nGQA 论文实验结果 推理性能 Fig. 8. Inference time and performance comparison. (Image source: Ainslie et al., 2023)\nFig. 9. Additional Experimental Results. (Image source: Ainslie et al., 2023)\n从实验结果可以看出：\n推理速度：\nMHA-XXL 的推理时间显著高于 MHA-Large，主要由于其更大的头数量和模型规模。 MQA-XXL 和 GQA-8-XXL 相比 MHA-XXL，推理时间分别减少至约1/6和1/5。 性能表现：\nMHA-XXL 在所有任务上表现最佳，但推理时间较长。 MQA-XXL 在推理速度上具有优势，平均分仅略低于 MHA-XXL。 GQA-8-XXL 在推理速度上接近 MQA-XXL，但在性能上几乎与 MHA-XXL 持平，显示出 GQA 的高效性和优越性。 CheckPoint 转化 Fig. 10. Ablation Study on Checkpoint Conversion Methods. (Image source: Ainslie et al., 2023)\n图10证明了均值池化方法在保留模型信息方面表现最佳，选择第一个头次之，随机初始化效果最差。均值池化有效地融合了多个 $\\mathbf{K}$ 和 $\\mathbf{V}$ 头的信息，保持了模型性能。\n向上训练比例 Fig. 11. Ablation Study on Uptraining Ratios. (Image source: Ainslie et al., 2023)\n图11展示了以MHA模型为基准，T5 XXL模型在MQA和GQA上的性能随着向上训练的数据量增加变化情况。\nGQA：即使在仅进行转换（无向上训练）的情况下，GQA已具备一定性能，随着向上训练比例增加，性能持续提升。 MQA：需要至少5%比例的预训练数据进行向上训练才能达到实用的性能，且随着比例增加，性能提升趋于平缓。 分组数量对推理速度的影响 Fig. 12. Effect of the Number of GQA Groups on Inference Speed. (Image source: Ainslie et al., 2023)\n从图12可以发现随着分组数的增加，推理时间略有上升，但相较于MHA，仍然保持显著的速度优势。选择适中的分组数比如8可以在速度和性能之间取得良好平衡。图3也显示了 llama3 从 7B 到 405B 参数的模型都是才采用8作为分组数（key/value heads = 8）。\n其他优化方法 除了注意力机制的优化，研究者们还提出了多种方法以提升Transformer模型的推理和训练效率：\nLoRA (HU et al., 2021): 通过在预训练模型的权重矩阵上添加低秩矩阵来实现高效的参数微调。 Flash Attention（Dao et al., 2022）：通过优化注意力计算，减少内存和计算开销。 量化技术 LLM.int8（Dettmers et al., 2022)和 GPTQ (Frantar et al., 2022)：通过降低模型权重和激活的精度，减少显存占用和计算成本。 模型蒸馏（Hinton et al., 2015）：通过训练小模型模仿大模型的行为，减小模型规模。 投机采样 Speculative Sampling（Chen et al., 2023）：通过并行生成和筛选，提升生成效率。 关键总结 向上训练方法能够有效利用已有的MHA模型的Checkpoint，通过少量的额外训练，将其转化为更高效的MQA或GQA模型，显著降低了训练成本。 分组查询注意力(GQA) 在推理效率和模型性能之间取得了良好的平衡，尤其适用于需要高效推理和高性能的应用场景。 实验结果表明，GQA 能够在保持与 MHA 模型相近的性能的同时，显著提升推理速度，适合大规模模型部署和实时应用。 参考文献 [1] Vaswani A. Attention is all you need [J]. Advances in Neural Information Processing Systems, 2017.\n[2] Devlin J. Bert: Pre-training of deep bidirectional transformers for language understanding [J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] Radford A. Improving language understanding by generative pre-training [J]. 2018.\n[4] Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models [J]. arXiv preprint arXiv:2302.13971, 2023.\n[5] Achiam J, Adler S, Agarwal S, et al. Gpt-4 technical report [J]. arXiv preprint arXiv:2303.08774, 2023.\n[6] Shazeer N. Fast transformer decoding: One write-head is all you need [J]. arXiv preprint arXiv:1911.02150, 2019.\n[7] Ainslie J, Lee-Thorp J, de Jong M, et al. Gqa: Training generalized multi-query transformer models from multi-head checkpoints [J]. arXiv preprint arXiv:2305.13245, 2023.\n[8] Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models [J]. arXiv preprint arXiv:2106.09685, 2021.\n[9] Dettmers T, Lewis M, Belkada Y, et al. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale [J]. Advances in Neural Information Processing Systems, 2022, 35: 30318-30332.\n[10] Frantar E, Ashkboos S, Hoefler T, et al. Gptq: Accurate post-training quantization for generative pre-trained transformers [J]. arXiv preprint arXiv:2210.17323, 2022.\n[11] Hinton G. Distilling the Knowledge in a Neural Network [J]. arXiv preprint arXiv:1503.02531, 2015.\n[12] Chen C, Borgeaud S, Irving G, et al. Accelerating large language model decoding with speculative sampling [J]. arXiv preprint arXiv:2302.01318, 2023.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Jan 2025). Transformer注意力机制：MHA、MQA与GQA的对比.\nhttps://syhya.github.io/posts/2025-01-16-group-query-attention/\nOr\n@article{syhya2025gqa, title = \u0026#34;Transformer注意力机制：MHA、MQA与GQA的对比\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-16-group-query-attention/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-01-16-group-query-attention/","summary":"\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\n\u003cp\u003eTransformer (\u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eVaswani et al., 2017\u003c/a\u003e）是一种基于编码器-解码器架构的模型。此模型在自然处理领域中展示了卓越的性能，随后一系列模型在此基础上进行了优化，例如仅使用编码器的 BERT (\u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eDevlin et al., 2018\u003c/a\u003e）或仅使用解码器的 GPT (\u003ca href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\"\u003eRadford et al., 2018\u003c/a\u003e）系列，以及后续的大型语言模型如 LLaMA (\u003ca href=\"https://arxiv.org/abs/2302.13971\"\u003eTouvron et al., 2023\u003c/a\u003e）和 GPT-4 (\u003ca href=\"https://arxiv.org/abs/2303.08774\"\u003eOpenAI al., 2024\u003c/a\u003e）系列，这些模型大多采用了仅解码器的结构。\u003c/p\u003e","title":"Transformer注意力机制：MHA、MQA与GQA的对比"},{"content":"背景 随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。\n本文基于作者的工作经验，总结了如何在现有通用模型的基础上，通过数据准备、模型训练、部署、评估及持续迭代，构建具备特定领域知识的大语言模型。\n为什么要向基座模型注入领域知识 挑战一：有限的领域知识 现有的预训练模型（如 GPT-4、Llama 3）主要基于通用语料库进行训练，缺乏对小众语言或专有领域的深入理解，导致模型在处理编程代码时表现不佳。\n挑战二：数据安全与合规 企业在处理敏感数据时，必须遵循严格的数据主权和合规性要求。将私有业务数据上传至第三方云服务存在安全隐患，因此需要在本地环境中完成数据处理与模型训练。\n挑战三：OpenAI微调的局限 主流商用 API 的微调方案通常较为基础，难以实现深层次的对齐和优化。对于需要高度定制化的领域模型，这种方法难以满足需求。\n注入知识两种方法 在实际项目中，常见的将领域知识注入基座模型的方法主要包括 微调 (Fine-Tuning) 和 检索增强生成 (RAG)。下文将详细对比这些方法，以帮助选择最适合的策略。\n方法对比 微调 (Fine-Tuning) 核心思路\n通过持续预训练、监督微调和偏好对齐，直接更新模型参数，使其掌握特定领域知识和任务模式。\n技术细节\n继续预训练 (CPT)：在大量领域特定的无监督数据上继续预训练基座模型。 监督微调 (SFT)：使用高质量的标注数据进行有监督微调。 偏好对齐 (DPO)：通过用户反馈优化模型输出。 参数微调方法：使用全参数微调或者结合 LoRA 等 PEFT 方法冻结部分参数并添加 adapter。 优势\n深度定制：模型内部权重更新，能够深入理解领域知识。 无需依赖外部检索：推理时不需额外的知识库支持，减少延迟和总的 token 消耗。 提升整体性能：在特定领域任务上表现显著优于通用模型。 劣势\n高计算成本：需要大量计算资源进行训练，尤其是 CPT 阶段。 训练周期长：从数据准备到模型训练再到优化，需要较长时间。 灾难性遗忘：模型可能在学习新知识的同时，遗忘原有的通用能力。 检索增强生成 (RAG) 核心思路\n构建领域知识库，在推理阶段检索相关文档，辅助模型生成更准确的回答，无需直接改变模型参数。\n技术细节\n数据处理：对领域文档进行预处理，按块大小和重叠量切分。 向量化：使用文本嵌入模型将文本块转换为向量，存储在向量数据库中。 召回：推理时通过相似度搜索召回相关文档，作为上下文信息或 few-shot 示例提供给基座模型。 优势\n保持通用能力：模型参数不变，仍保留通用语言能力。 快速更新：知识库可动态更新，无需重新训练模型。 计算效率：避免大规模训练，节省计算资源。 劣势\n依赖知识库质量：检索到的文档质量直接影响回答质量。 推理速度：检索过程可能增加推理延迟，并且需要更多的 token。 知识覆盖有限：模型内部知识仍受限于基座模型的预训练数据。 模型与训练资源 基座模型 以 Llama 3 系列 为例，其具有以下特点：\n参数规模\nLlama 3 系列涵盖从 1B 到 405B 参数的模型，广泛支持多语言处理、代码生成、推理，以及视觉和文本任务。小型模型（1B 和 3B）经过专门优化，适合边缘和移动设备，支持最大 128K 的上下文窗口，可高效处理本地任务，例如摘要生成、指令执行和文本重写。\n多模态能力\nLlama 3 的视觉模型（11B 和 90B 参数）在图像理解任务上的表现优于许多封闭模型，同时支持图像、视频和语音的多模态处理。所有模型均支持微调，便于针对特定领域进行定制化开发。\n开源与社区支持\nLlama 3 系列模型及其权重以开源形式发布，可通过 llama.com 和 Hugging Face 平台 获取，为开发者提供便捷的访问和应用支持。\n数据集限制\n虽然 Llama 3 模型本身以开源形式发布，但其训练所使用的数据集并未开源。因此，严格来说，Llama 3 并非完全开源的模型。这一限制可能会在解决灾难性遗忘问题时带来挑战，因为难以获得与原始训练完全一致的数据集。\n训练资源 训练大型语言模型需要强大的计算资源和高效的分布式训练框架。\n硬件资源\nGPU 集群：建议使用 NVIDIA A100 或 H100 GPU，4卡或8卡配置，通过 NVLink 或 InfiniBand 提升通信带宽。 存储资源：高性能 SSD（如 NVMe）以支持快速的数据读写。 软件框架\n并行框架：DeepSpeed、Megatron-LM 等分布式训练框架，支持大规模模型训练。 推理框架：vLLM、ollama 等，优化推理速度和资源利用。 并行策略\n数据并行：适用于单卡可容纳模型的情况，通过 DeepSpeed 的 ZeRO Stage 0 实现。 模型并行、流水线并行和张量并行：单卡无法容纳时，采用 ZeRO Stage 1、2、3 进行优化，或使用 ZeRO-Infinity 将参数和优化器状态部分卸载到 CPU 或 NVMe。 DeepSpeed ZeRO 分片策略对比 为了更好地理解 DeepSpeed 的 ZeRO 分片策略，以下将分为不同的部分进行详细说明。\nZeRO Stage 分片策略 ZeRO Stage 描述 显存占用 训练速度 ZeRO-0 纯数据并行，不进行任何分片。所有优化器状态、梯度和参数在每张 GPU 上完全复制。 最高 最快 ZeRO-1 分片优化器状态（例如动量和二阶矩），减少显存占用，但梯度和参数仍为数据并行。 高 略慢于 ZeRO-0 ZeRO-2 分片优化器状态和梯度，在 ZeRO-1 的基础上进一步减少显存占用。 中 慢于 ZeRO-1 ZeRO-3 分片优化器状态、梯度和模型参数，显存占用最低，适合大规模模型。但需要在前向和后向时进行参数广播（All-Gather/All-Reduce），通信量显著增加。 低 明显慢于 ZeRO-2，取决于模型大小和网络带宽 Offload 策略 Offload 类型 描述 显存占用 训练速度 ZeRO-1 + CPU Offload 在 ZeRO-1 的基础上，将优化器状态卸载到 CPU 内存；可进一步降低 GPU 显存占用，但需要 CPU-GPU 数据传输，依赖 PCIe 带宽，且占用 CPU 内存。 中偏低 慢于 ZeRO-1，受 CPU 性能和 PCIe 带宽影响 ZeRO-2 + CPU Offload 在 ZeRO-2 的基础上，将优化器状态卸载到 CPU 内存；对较大模型进一步降低 GPU 显存占用，但会增加 CPU-GPU 数据传输开销。 较低 慢于 ZeRO-2，受 CPU 性能和 PCIe 带宽影响 ZeRO-3 + CPU Offload 在 ZeRO-3 的基础上，将优化器状态和模型参数卸载到 CPU；GPU 显存占用最小，但 CPU-GPU 通信量极大，且 CPU 带宽远小于 GPU-GPU 通信。 极低 非常慢 ZeRO-Infinity (NVMe Offload) 基于 ZeRO-3，将优化器状态、梯度和参数卸载到 NVMe，突破 CPU 内存限制，适合超大规模模型；性能高度依赖 NVMe 并行读写速度。 极低需 NVMe 支持 慢于 ZeRO-3，但通常优于 ZeRO-3 + CPU Offload 通信量与性能影响 ZeRO-0/1/2\n通信以 梯度同步 为主，使用 All-Reduce 操作，通信量相对较低。 ZeRO-3\n需要对模型参数进行 All-Gather/All-Reduce 操作，通信量显著增大，网络带宽成为关键瓶颈，前后传播时的参数广播进一步加剧通信负担。 CPU Offload（ZeRO-1/2/3 + CPU）\n卸载优化器状态或参数到 CPU，减少 GPU 显存占用。 通信量主要来自 CPU \u0026lt;-\u0026gt; GPU 数据传输，带宽远低于 GPU-GPU 通信，极易造成性能瓶颈，尤其在 ZeRO-3 场景下。 NVMe Offload（ZeRO-Infinity）\n在 ZeRO-3 的基础上进一步卸载至 NVMe，突破 CPU 内存限制以支持超大规模模型。 性能强烈依赖 NVMe I/O 带宽 和并行度，若 NVMe 速度足够高，通常优于 CPU Offload；但在 I/O 性能较弱或高延迟场景下，效果可能不佳。 硬件与配置影响 硬件限制\nPCIe 带宽、网络带宽、NVMe I/O 等对 Offload 性能有显著影响，需根据硬件环境选择最佳策略。 补充说明\nCPU Offload 利用 CPU 内存并通过 PCIe 传输数据；NVMe Offload 则将状态保存于 NVMe 设备。 NVMe Offload 在 NVMe I/O 性能充足 时通常优于 CPU Offload，但需避免因 I/O 性能不足导致的性能瓶颈。 与官方文档对照\n建议结合 DeepSpeed 官方文档 获取最新、最准确的配置参数和性能调优建议。 数据准备：决定训练成败的核心 数据质量直接决定了模型的性能。数据准备包括数据收集、清洗、去重、分类与配比、脱敏等步骤。\n预训练数据 数据来源 公开数据集：如：the-stack-v2、Common Crawl 等。 企业自有数据：内部文档、代码库、业务日志等。 网络爬虫：通过爬虫技术采集领域相关的网页内容。 数据规模 建议使用至少数亿到数十亿个 token，以确保模型能够充分学习领域知识。 当数据量不足时，模型效果可能受限，建议采用数据增强的方法来补充数据。 数据处理 数据预处理\n格式统一：对来自多个数据源的无标注大量语料进行处理，确保其格式一致。推荐使用高效的存储格式，如 Parquet，以提高数据读取和处理的效率。 数据去重\n检测方法：使用 MinHash、SimHash 或余弦相似度等算法进行近似重复检测。 处理粒度：可选择按句子、段落或文档级别去重，根据任务需求灵活调整。 相似度阈值：设定合理的相似度阈值（如 0.9），删除重复度高于阈值的文本，确保数据多样性。 数据清洗\n文本过滤：结合规则和模型评分器（如 BERT/RoBERTa）去除乱码、拼写错误和低质量文本。 格式化处理：优先使用 JSON 格式处理数据，确保代码、Markdown 和 LaTeX 等特殊格式的准确性。 数据脱敏\n隐私保护：匿名化或去除人名、电话号码、邮箱、密码等敏感信息，确保数据合规。 不合规内容过滤：剔除含有违法、色情或种族歧视等内容的数据块。 数据混合与配比\n比例控制：例如，将 70% 的领域特定数据与 30% 的通用数据相结合，避免模型遗忘通用能力。 任务类型：确保数据包含代码生成、问答对话、文档摘要、多轮对话和数学推理等多种任务类型。 数据顺序\n逐步引导：采用课程学习（Curriculum Learning）方法，从简单、干净的数据开始训练，逐步引入更复杂或噪声较高的数据，优化模型的学习效率和收敛路径。 语义连贯性：利用上下文预训练（In-context Pretraining）技术，将语义相似的文档拼接在一起，增强上下文一致性，提升模型的语义理解深度与泛化能力。 监督微调数据 数据格式 可采用 Alpaca 或 Vicuna 风格，比如结构化为 [instruction, input, output] 的单轮和多轮对话。\n规模：几千条到几十万条，具体根据项目需求和计算资源决定。 质量：确保数据的高质量和多样性，避免模型学习到错误或偏见。 数据构建 在数据构建过程中，我们首先收集日常业务数据，并与业务专家共同构建基础问题。随后，利用大语言模型进行数据增强，以提升数据的多样性和鲁棒性。以下是具体的数据增强策略：\n数据增强策略 表达多样化\n通过大语言模型对现有数据进行改写，采用同义词替换和语法变换等方法，增加数据的多样性。\n鲁棒性增强\n构建包含拼写错误、混合语言等输入的提示（Prompt），以模拟真实场景，同时确保生成答案的高质量。\n知识蒸馏\n利用 GPT-4、Claude 等大型语言模型进行知识蒸馏，生成符合需求的问答数据对。\n复杂任务设计\n针对复杂场景（如多轮对话、逻辑推理等），手动设计高质量数据，以覆盖模型的能力边界。\n数据生成管道\n构建自动化数据生成流水线，将数据生成、筛选、格式化和校验等环节集成，提高整体效率。\n关键要点 任务类型标注：每条数据标注明确的任务类型，便于后续精细化分析和调优。 多轮对话与话题切换：构建多轮对话中上下文关联与话题转换的数据，确保模型能够学习话题切换与上下文关联的能力。 思维链（Chain of Thought）策略：分类、推理等任务可先用 COT 生成过程性答案，提高准确率。 数据飞轮：上线后持续收集用户真实问题，结合真实需求迭代数据；定期清洗，确保质量与多样性。 偏好数据 数据格式 三元组结构：[prompt, chosen answer, rejected answer] 标注细节： 多模型采样：使用多个不同训练阶段或不同数据配比的模型生成回答，增加数据多样性。 编辑与优化：标注人员可对选择的回答进行小幅修改，确保回答质量。 采样策略 多模型采样：部署多个不同版本的模型，对同一 prompt 生成不同回答。 对比标注：由人工或自动化系统对生成的回答进行对比，选择更优的回答对。 关键要点 数据多样性与覆盖：确保偏好数据涵盖各种场景和任务，避免模型在特定情境下表现不佳。 高质量标注：偏好数据的质量直接影响模型的对齐效果，需确保标注准确且一致。 训练流程 一个完整的特定领域大语言模型训练流程通常包括 继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO) 三个主要步骤，最终实现模型的部署与持续优化。\n三种方法对比 训练方法概览 训练方法 主要目标 数据需求 典型应用场景 继续预训练 (CPT) 继续在大规模无监督语料上进行预训练，注入新领域知识 大量无标签文本（至少数亿到数十亿 Token） 补足领域知识，如法律、医疗、金融等专业文本 监督微调 (SFT) 在有监督标注的数据上进行微调，强化特定任务和指令执行能力 定制化标注数据（指令/对话对等），从几千到几十万条 各类特定任务，如代码生成、问答、文本改写、复杂指令执行等 直接偏好对齐 (DPO) 利用偏好数据（正例 chosen vs. 负例 rejected）直接对齐人类偏好 偏好数据：[prompt, chosen, rejected](规模相对较小) 对齐人类反馈，如回答风格、合规性、安全性等 优势与挑战 继续预训练 (CPT) 优势:\n更好领域覆盖，全面提升模型在特定领域的理解和生成能力。 无需额外手动标注。 挑战/局限:\n需要大量高质量领域数据。 训练成本高，需大规模算力与时间。 可能引入领域偏见，需谨慎处理数据质量与分布。 监督微调 (SFT) 优势:\n快速获取可用的任务执行能力。 显著提升模型对特定场景的准确性。 挑战/局限:\n数据标注成本较高。 需谨慎选择标注数据以避免过拟合。 微调后可能削弱模型的通用性。 直接偏好对齐 (DPO) 优势:\n无需单独训练 Reward Model。 数据量需求较小，调优效率高。 挑战/局限:\n需要可靠的偏好标注。 对复杂、多样化场景仍需持续迭代收集更多偏好数据。 易受偏好数据分布的限制。 通用训练要点与技术细节 在进行 CPT、SFT、DPO 三种训练方法时，存在许多通用的训练要点和技术细节。以下部分将这些通用内容进行统一描述，以便于更好地理解和应用。\n数据处理与准备 数据质量：无论是 CPT、SFT 还是 DPO，数据的质量都是至关重要的。需要确保数据的准确性、无歧义性和多样性。 数据格式：统一的数据格式有助于简化训练流程。例如，使用 JSON 或其他结构化格式来存储训练数据。 数据增强：通过 LLM 重写和优化等方式增加数据多样性，提升模型的泛化能力。 学习率与优化 学习率设置：通常采用比预训练阶段更小的学习率，如从 3e-4 降低到 3e-5，具体数值视任务和数据量而定。 学习率调度：使用 warm-up 策略（如前 10% 步骤线性递增），随后采用线性衰减或余弦退火策略，确保训练过程平稳。 优化器选择：根据模型规模和硬件资源选择合适的优化器，比如 AdamW。 训练策略 全参数微调：在资源允许的情况下，优先进行全参数微调，以确保模型能够全面掌握新知识。 参数高效微调（PEFT）：如 LoRA，适用于计算资源有限的场景，通过冻结部分参数并添加 adapter 实现高效微调。 混合精度训练：在支持的 GPU 上使用 bf16 或 fp16，降低显存占用，提高训练速度。 训练稳定性：采用梯度裁剪、正则化、dropout、权重衰减等技术，防止梯度爆炸和模型过拟合。 Flash Attention：利用 Flash Attention 技术优化注意力机制的计算效率，提升训练速度和降低显存占用。 监控与调优 收敛监控：实时监控训练集和验证集的 loss 曲线，确保模型逐步收敛，必要时调整学习率和其他超参数。 Checkpoint：定期保留 Checkpoint，防止意外中断导致全部训练进度丢失。 早停机制：防止模型过拟合，适时停止训练，保存最佳模型状态。 模型评估：在训练过程中定期进行评估，确保模型性能符合预期。 继续预训练 (CPT) 目标 通过在大量领域特定的无监督数据上继续预训练基座模型，注入新的领域知识，使模型更好地理解和生成特定领域的内容。\n训练要点 流式加载\n实施流式数据加载，以便在训练过程中动态读取数据，防止超过最大内存，训练中断。\n全参数微调\n在进行模型训练时，通常需要更新模型的全部参数，以确保模型能够全面掌握新知识。\n全量微调相较于参数高效微调（如 LoRA）在领域知识注入方面效果更佳，尤其在运算资源充足的情况下，建议优先选择全参数微调。\n监督微调 (SFT) 目标 通过高质量的标注数据，训练模型执行特定任务，如代码生成、代码修复、复杂指令执行等，提升模型的实用性和准确性。\n训练要点 Epoch 数量\n在数据量充足的情况下通常 1 ~ 4 个 epoch 即可见到显著效果。 如果数据量不够，可以考虑加大 epoch 数量，但要注意过拟合的风险，建议进行数据增强。 数据增强与多样性\n确保训练数据涵盖多种任务类型和指令表达方式，提升模型的泛化能力。 包含多轮对话和鲁棒性数据，增强模型应对真实用户场景的能力。 直接偏好对齐 (DPO) 目标 通过用户反馈和偏好数据，优化模型输出，使其更符合人类的期望和需求，包括回答风格、安全性和可读性等方面。\nDPO 的特点 直接优化\n不需要单独训练 Reward Model，直接通过 (chosen, rejected) 数据对模型进行对比学习。\n高效性\n相较于 PPO，DPO 需要更少的数据和计算资源即可达到相似甚至更好的效果。\n动态适应\n每次有新数据时，模型能立即适应，无需重新训练 Reward Model。\n训练要点 偏好数据的收集\n部署多个不同训练阶段或不同数据配比的模型，生成多样化的回答。 通过人工或自动化方式标注 chosen 和 rejected 回答对，确保数据的多样性和质量。 对比学习\n通过最大化 chosen 回答的概率，最小化 rejected 回答的概率，优化模型参数。\n迭代优化\n持续收集用户反馈，生成新的偏好数据，进行循环迭代，逐步提升模型性能。 结合数据飞轮机制，实现模型的持续进化与优化。 常见问题与解决方案 重复输出 (Repetitive Outputs)\n问题：模型生成内容重复，连续打印停不下来。\n解决方案：\n数据去重与清洗：确保训练数据不含大量重复内容。 检查 EOT（End-of-Token）设置：防止模型连接打印无法停止。 通过 SFT/DPO 进行对齐：优化模型输出质量。 调整解码策略：如增加 top_k、repetition penalty 和 temperature 参数。 灾难性遗忘 (Catastrophic Forgetting)\n问题：模型在微调过程中遗忘原有的通用能力，可以看作是在新的数据集上过拟合，原本模型参数空间变化过大。\n解决方案：\n混合一部分通用数据：保持模型的通用能力。 调低学习率：减少对原有知识的冲击。 增加 Dropout Rate 和 Weight Decay：避免过拟合。 采用 LoRA 等参数高效微调方法：避免大规模参数更新。 使用 RAG 辅助：结合外部知识库提升模型表现。 Chat Vector: 通过模型权重的简单算术操作，快速为模型注入对话和通用能力。 实体关系与推理路径理解不足\n问题：模型难以正确理解复杂的实体关系和推理路径。\n解决方案：\n引入 Chain-of-Thought (CoT) 数据与强化推理训练：\n通过分步推理训练提升模型的能力，结合 强化微调 和 o1/o3 的训练方法。 扩展训练数据覆盖面：\n引入更多包含复杂实体关系和推理路径的多样化场景数据。 结合知识图谱建模：\n利用 GraphRAG 强化模型对实体关系的理解与推理能力。 模型部署与评估 部署 推理框架\nollama：基于 llama.cpp 的本地推理部署，可快速启动 vLLM：主打高并发、多用户场景下的推理吞吐量优化 量化：将模型量化为 8-bit 或 4-bit，进一步降低推理成本，提高部署效率。 集成 RAG \u0026amp; 智能体\nRAG：结合向量知识库，实时检索相关文档或代码片段，辅助模型生成更精准的回答。 智能体：利用 Function Call 或多轮对话机制，让模型调用外部工具或进行复杂推理，提升互动性和实用性。 Langgraph：封装 RAG 和 多智能体工作流，构建定制化的对话系统或代码自动生成平台。 评估 评估指标\nCPT 阶段：使用领域内测试集，评估困惑度（Perplexity，PPL）或者交叉熵（Cross Entropy），衡量模型对新知识的掌握程度。 SFT / DPO 阶段： Human 或模型评测：通过人工评分或自动化工具，评估回答的准确性、连贯性、可读性和安全性。 代码生成：构建大规模单元测试集，评估 pass@k 指标，衡量代码生成的正确率。 通用能力：在常见 benchmark（如 MMLU、CMMLU）对模型进行测试，确保模型在通用任务上的表现下降不大。 解码超参数\n一致性：在评估过程中保持 top_k、top_p、temperature、max_new_tokens 等解码参数一致，确保评估结果的可比性。 网格搜索：在算力允许的情况下，对不同解码参数组合进行评估，选择最优的参数配置。 数据飞轮与持续迭代 数据飞轮机制\n实时收集用户日志\n收集线上用户的真实 prompt 和生成的回答，覆盖多样化的使用场景和任务类型。\n自动或人工标注\n对收集到的用户 prompt 和回答进行偏好标注，生成新的 (chosen, rejected) 数据对。\n迭代训练\n将新生成的偏好数据加入到下一轮的 SFT/DPO 训练中，不断优化模型的回答质量和用户体验。\n鲁棒性数据\n包含拼写错误、混合语言、模糊指令等数据，提升模型在真实场景下的鲁棒性和应对能力。\n持续优化\n反馈循环：利用用户反馈，持续改进训练数据和模型表现，实现模型的自我优化和进化。 多模型协同：部署多个版本的模型，生成多样化的回答，通过对比学习提升模型的综合能力。 结合意图识别和多智能体推理 使用意图分类模型让大模型判断用户输入意图类别。基于意图类别与上下文类型的映射，监督推理路径，然后根据推理路径进行多路召回。将这些信息提供给训练好的模型，生成最终结果。\n总结 通过 继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO) 的组合方法，能够有效地在基座大模型上注入特定领域的知识，构建出具备高效解决业务问题能力的闭源大语言模型。关键步骤如下：\n数据准备\n高质量的数据收集、清洗、去重和分类，确保训练数据的多样性与准确性。 结合数据脱敏策略，保护隐私与合规。 模型训练\n通过 CPT 注入领域知识，SFT 学习特定任务模式，DPO 优化模型输出符合人类偏好和安全。 利用高效的并行训练框架和调参技巧，提升训练效率和资源利用率。 部署与评估\n采用高效的推理框架，结合 RAG 和 Agent 实现知识增强和功能扩展。 通过多维度评估，确保模型在各个阶段的表现符合预期。 持续迭代\n构建数据飞轮，实时收集用户反馈，不断优化训练数据和模型表现。 集成 RAG 和 Agent，实现模型能力的持续提升与扩展。 最终，通过系统化的流程和技术手段，能够构建一个不仅具备深厚领域知识，还能灵活应对复杂业务需求的长生命周期 AI 系统。\n参考资料 DeepSpeed Megatron-LM ollama vLLM GraphRAG The Llama 3 Herd of Models ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages Evaluating Large Language Models Trained on Code Direct Preference Optimization: Your Language Model is Secretly a Reward Model 引用 引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Jan 2025). 构建特定领域的大语言模型.\nhttps://syhya.github.io/posts/2025-01-05-domain-llm-training\nOr\n@article{syhya2024domainllm, title = \u0026#34;构建特定领域的大语言模型\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-05-domain-llm-training/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/","summary":"\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\n\u003cp\u003e随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。\u003c/p\u003e","title":"构建特定领域的大语言模型"},{"content":"租用 GPU 还是购买 GPU？ 在构建深度学习工作环境之前，首先需要综合考虑 使用周期、预算、数据隐私 以及 维护成本。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。\n租用 GPU 的优点：\n无需一次性投入高额硬件成本 可根据项目需求弹性扩容 云厂商通常提供数据合规与安全保障，省去硬件运维烦恼 购买 GPU 的优点：\n长期大规模使用时，整体成本更低 对内部数据和模型有更高的隐私与可控性 硬件可随时调整、升级，部署更灵活 个人建议\n如果预算有限或只是初学阶段，可先使用 Colab、Kaggle 或云 GPU； 当算力需求和隐私需求上升时，再考虑自建多卡服务器或租用多机多卡集群。 背景 在 2023 年 9 月，为了在工作之余继续对大模型（LLM）进行探索和研究，我组装了一台 双 RTX 4090 的个人 AI 实验服务器。该服务器已运行近一年，整体体验如下：\n噪音：服务器放在脚边，满负荷训练时风扇噪音较大，但在日常推理或中等负载下可接受 推理性能：双卡共计 48GB 显存，采用 4bit 量化方案时可运行到 70B 级别的模型（如 Llama 70B、Qwen 72B） 训练性能：在使用 DeepSpeed 的分布式和 offload 技术（ZeRO-3 + CPU offload）后，可对 34B 左右的模型（如 CodeLlama 34B）进行微调 性价比：对于个人或小团队的日常实验和中小规模模型训练而言，该配置较为实用；但若进行超大规模模型的全参数训练，仍需更多专业卡（如多卡 A100 或 H100 集群） 下图展示了不同大小模型、不同训练方法对显存的需求： Fig. 1. Hardware Requirement. (Image source: LLaMA-Factory)\n搭建思路与配置详情 整机预算在 4 万元人民币（约 6000 美元） 左右，以下是我最终选用的配置清单，仅供参考：\n配件 型号 价格 (元) 显卡 RTX 4090 * 2 25098 主板 + CPU AMD R9 7900X + 微星 MPG X670E CARBON 5157.55 内存 美商海盗船(USCORSAIR) 48GB*2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + 三星 990PRO 4TB 4587 电源 美商海盗船 AX1600i 2699 风扇 追风者 T30 12cm P * 6 1066.76 散热 利民 Thermalright FC140 BLACK 419 机箱 PHANTEKS 追风者 620PC 全塔 897.99 显卡延长线 追风者 FL60 PCI-E4.0 *16 399 总计：约 42723.3 元\nGPU 选择 对于大模型研究，浮点运算性能（TFLOPS） 和 显存容量 是最核心的指标。专业卡（A100、H100 等）虽有更高显存以及 NVLink，但价格动辄数十万人民币，对个人用户并不友好。根据 Tim Dettmers 的调研，RTX 4090 在单位美元算力方面表现非常亮眼，且支持 BF16、Flash Attention 等新特性，因此成为高性价比的选择。\n散热方式：涡轮 vs 风冷 vs 水冷 散热方式 优点 缺点 适用场景 涡轮风扇 体积紧凑；适合并行多卡部署 噪音大、整体散热效率一般 企业服务器机柜、多卡密集部署 风冷散热 性能与噪音平衡佳、维护简单 显卡体型通常较大 家用或个人研究（主机摆放空间足够） 水冷散热 散热能力突出、满载噪音更低 可能会出现漏液、价格更高 对静音要求极高或极限超频场景 家用推荐：风冷卡 兼顾散热效率、噪音和维护成本；相对于涡轮卡和水冷卡更友好。\nCPU \u0026amp; 主板 在深度学习场景中，CPU 主要负责数据预处理、管道调度以及多进程/多线程并行管理，确保数据能够以高吞吐量、低延迟的方式传递到 GPU。因此，CPU 的核心需求主要包括 充足的 PCIe 通道 和 卓越的多线程性能。\nIntel：13/14 代 i9（如 13900K）拥有 20 条 PCIe 主通道，能够满足双卡 x8 + x8 的需求 AMD：Ryzen 7000/9000 系列（如 7950X）提供 28 条（可用 24 条）PCIe 通道，支持双卡 x8 + x8，并为 M.2 SSD 提供足够带宽 微星 MPG X670E CARBON 主板 扩展性：支持 PCIe 5.0 和 DDR5 内存，具备充足的未来升级空间 稳定性：高规格供电设计，保障 CPU 与多显卡的稳定运行 接口丰富：支持多块 M.2 SSD 和 USB4，满足多样化使用需求 AMD Ryzen 9 7900X 特点 核心与线程：12 核心、24 线程，在深度学习场景中的数据预处理和多任务处理方面表现强劲 PCIe 带宽：提供 28 条（可用 24 条）PCIe 5.0 通道，可轻松支持双卡 x8 + x8，并为 M.2 SSD 提供高速带宽 能效比：基于 Zen 4 架构，性能与能耗平衡优秀，适合高性能计算需求 主板选购要点 空间布局 RTX 4090 尺寸庞大且卡槽较厚，需确认主板是否能同时容纳两张显卡；若存在空间或散热冲突，可使用显卡延长线竖放第二张卡。 PCIe 通道拆分 主板需至少支持双 PCIe 4.0 x8 + x8 配置，以避免出现 x16 + x2 的情况。x16 + x2 的带宽分配会显著限制第二块 GPU 的数据传输能力，进而影响 GPU 与 CPU 之间的数据交换效率。在大模型训练中，这种带宽瓶颈可能导致性能显著下降，严重影响整体训练效率。 扩展性 在双显卡插满的情况下，仍需确保主板具有足够的 M.2 SSD 插槽和外设接口 综合扩展性、性能与性价比等因素，我最终选择了 AMD Ryzen 9 7900X 搭配 微星 MPG X670E CARBON 主板 的组合。通过显卡延长线解决了 4090 双卡过厚导致的插槽冲突问题。\nBIOS 设置建议 内存优化 开启 XMP/EXPO（对应 Intel/AMD）以提升内存频率，增强带宽性能。 超频调整 如果需要进一步提升性能，可在 BIOS 中启用 PBO（Precision Boost Overdrive） 或 Intel Performance Tuning，并结合系统监控工具观察稳定性。 温度与稳定性 避免过度超频，注意控制温度，避免因崩溃或过热导致系统不稳定。 内存 深度学习训练中，内存会被大量占用用于数据加载、模型优化状态储存（尤其在多 GPU Zero-stage 并行场景下）。内存容量最好 ≥ 显存总容量的两倍。本配置中，使用了 48GB * 2（共 96GB），满足日常多任务和分布式训练的需求，减少内存不足导致的频繁 swap。\n硬盘 优先选用 M.2 NVMe SSD：其读写性能更优，对加载超大模型权重、缓存中间文件、训练日志等都有显著速度提升 容量建议 ≥ 2TB：随着大模型文件越来越庞大，2TB 往往很快就会被占满，建议根据自身需求选 4TB 或更多 SSD 品牌：三星、海力士或西部数据等主流大厂都拥有稳定的高端产品线 电源 双 4090 满载时整机功耗可达 900W~1000W 左右，CPU、主板和硬盘等还需额外功率余量。通常建议选择 1500W 以上 的铂金或钛金电源，以确保在高负载下电流供给稳定、降低电压波动带来的系统不稳定。\n我在此使用美商海盗船 AX1600i（数字电源），可以通过软件监控实时功耗，并提供充足冗余。\n散热与风扇 我采用 风冷 方案，包括：\nCPU 散热器：利民 FC140（双塔式气冷方案，兼顾了较高的散热效率和相对低噪音） 机箱风扇：追风者 T30 12cm * 6，保持机箱内部正压或者稍微正压的风道布局，保证显卡和供电模块的进风顺畅 在 GPU 长时间高负载训练（如分布式训练大型模型）时，机箱内的风道管理和风扇配置非常重要。建议使用监控软件及时查看 CPU、GPU、主板供电模块温度，适度调节风扇转速。\n散热进阶\n若对静音有更高要求，可考虑 Hybrid 散热（半水冷方案）或更精细的风扇调速曲线。 适度清理机箱灰尘、使用防尘网并定期更换导热硅脂也能提升散热和稳定性。 机箱 RTX 4090 体型巨大，且双卡堆叠时需要充足的内部空间和散热风道。全塔机箱能提供更好的走线空间和气流组织。我选用了 PHANTEKS 追风者 620PC，除了体型大、空间充裕外，也自带良好的线缆管理通道。\n装机完成后的图片如下：\nFig. 2. Computer\n系统与软件环境 操作系统方面强烈推荐 Linux，例如 Ubuntu 22.04 LTS，因其对 CUDA、NVIDIA 驱动以及常见深度学习框架有更好的支持和兼容性。大致流程如下：\n安装 OS：使用 Ubuntu 或其他 Linux 系统即可。 安装 NVIDIA 驱动：确保 nvidia-smi 能正确识别两张 4090:\nFig. 3. nvidia-smi Output\n安装 CUDA 工具链：通过 nvcc -V 确认版本信息:\nFig. 4. nvcc -V Output\n安装 cuDNN：确保深度学习框架可以调用 GPU 加速卷积和 RNN 等操作 测试框架：使用 PyTorch、TensorFlow 或 JAX 简单测试模型推理/训练是否正常 Docker 容器化： 利用 nvidia-container-toolkit 让容器直接访问 GPU 资源，避免主机环境污染。 在多机多卡环境下，还能结合 Kubernetes、Ray 或 Slurm 等进行集群调度与资源管理。 常用工具与框架推荐 训练框架\nLLaMA-Factory：对大语言模型训练/推理流程有较好封装，新手友好 DeepSpeed：支持大模型分布式训练、多种并行策略和优化功能 Megatron-LM：NVIDIA 官方的大规模语言模型训练框架，适合多机多卡场景 监控 \u0026amp; 可视化\nWeights \u0026amp; Biases 或 TensorBoard：实时监控训练过程中的损失函数、学习率等指标，支持远程可视化 推理工具\nollama：基于 llama.cpp 的本地推理部署，可快速启动 vLLM：主打高并发、多用户场景下的推理吞吐量优化 Framework Ollama vLLM 作用 简易本地部署 LLM 高并发 / 高吞吐的 LLM 推理 多请求处理 并发数增加时，推理速度下降明显 并发数增大也能保持较高吞吐 16 路并发 ~17 秒/请求 ~9 秒/请求 吞吐对比 Token 生成速度较慢 Token 生成速度可提升约 2 倍 极限并发 32 路以上并发时，性能衰减较严重 仍能平稳处理高并发 适用场景 个人项目或低并发应用 企业级或多用户并发访问 WebUI\nOpen-WebUI：基于 Web 界面的多合一 AI 界面，支持多种后端推理（ollama、OpenAI API 等），便于快速原型和可视化 进阶建议 开发与调试效率\n使用 SSH 工具提升远程开发效率，制作自定义容器镜像减少环境配置时间。\n量化与剪枝\n通过 4bit、8bit 量化和剪枝技术，减少模型参数和显存需求，优化推理性能。\n混合精度训练\n使用 BF16 或 FP16 提升训练速度，结合 GradScaler 提高数值稳定性。\nCPU 协同优化\n使用多线程、多进程或 RAM Disk 缓存优化数据加载，支持流式加载大规模预训练数据集。\n多机集群部署\n通过 InfiniBand 或高速以太网搭建集群，使用 Kubernetes 实现高效资源调度。\n总结 通过以上配置与思路，我成功搭建了一台 双卡 RTX 4090 深度学习主机。它在 推理 和 中小规模微调 场景中表现良好，对于想要在个人或小团队环境下进行大模型（LLM）科研或应用探索的人来说，这种方案兼具 性价比 与 灵活性。当然，如果要大规模全参数训练上百亿乃至上千亿参数的大模型，依然需要更多 GPU（如多卡 A100/H100 集群）。\n就个人体验而言，双 4090 在预算范围内提供了较好的训练与推理性能，可以满足绝大部分中小规模研究与实验需求，值得有条件的个人或小团队参考。\n参考资料 Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe 通道规格 AMD R5 7600X PCIe 通道规格 MSI MPG X670E CARBON 规格 nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI 版权声明与引用 声明：本文所涉及的配置清单、价格与建议仅供技术交流与研究参考。实际购买与部署请结合个人预算和需求进行综合评估。若因参考或采纳文中信息导致任何直接或间接后果，本文作者恕不承担责任。\n引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Dec 2024). 基于双卡 RTX 4090 搭建家用深度学习主机. https://syhya.github.io/posts/2024-12-21-build-gpu-server\nOr\n@article{syhya2024build, title = \u0026#34;基于双卡 RTX 4090 搭建家用深度学习主机\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Dec\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2024-12-21-build-gpu-server/\u0026#34; ","permalink":"https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/","summary":"\u003ch2 id=\"租用-gpu-还是购买-gpu\"\u003e租用 GPU 还是购买 GPU？\u003c/h2\u003e\n\u003cp\u003e在构建深度学习工作环境之前，首先需要综合考虑 \u003cstrong\u003e使用周期\u003c/strong\u003e、\u003cstrong\u003e预算\u003c/strong\u003e、\u003cstrong\u003e数据隐私\u003c/strong\u003e 以及 \u003cstrong\u003e维护成本\u003c/strong\u003e。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。\u003c/p\u003e","title":"基于双卡 RTX 4090 搭建家用深度学习主机"},{"content":"摘要 股票市场是金融市场的重要组成部分，近些年来，股票市场蓬勃发展，股票价格预测和量化投资策略研究吸引了许多领域的研究学者。其中最近几年随着人工智能和机器学习的发展，学者们从传统的统计学模型迁移到了人工智能算法，尤其是在深度学习热潮掀起后，神经网络在股票价格预测和量化投资策略研究中取得了不错的效果。深度学习的目标是学习多层次的特征，通过组合低级特征构建抽象的高级特征，从而挖掘数据的分布式特征表示，基于此进行复杂的非线性建模，从而实现预测任务。其中 RNN 被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价，交易信息都是序列数据，因此之前有很多研究者，基于 RNN 来预测股票价格。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题，之后出现了诸如 GRU，Peephole LSTM，BiLSTM 等 LSTM 的变体。但传统的股票预测模型有些并未考虑时间因素，有些仅考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。\n本文实验第一部分通过利用国内浦发银行和国外 IBM 的股票数据，分别建立了 LSTM，GRU，BiLSTM 的股票预测模型，通过比较这三种深度学习模型最后预测的结果，发现对于两个数据集都是 BiLSTM 模型优于其他模型，有更好的预测准确率。第二部分通过使用 A 股全市场的股票数据，并先使用 LightGBM 模型进行对 50 个因子的篮选，选出重要程度最高的 10 个因子。之后再用 BiLSTM 模型选取进行因子组合，建立量化投资策略，最后对该策略进行实证与回测，发现该策略优于市场基准指数，说明了 BiLSTM 模型在股票价格预测和量化投资的实际应用价值。\n关键词：量化投资；深度学习；神经网络模型；多因子选股；BiLSTM\n第一章 绪论 1.1 研究背景与意义 1.1.1 研究背景 从 1970 年逐渐兴起，量化投资进入各投资者的视野，一场新的革命就此拉开序幕，改变了从前被动管理和有效市场假说主导的投资组合管理局面。有效市场假说认为，在市场有效的前提下，股票价格能反映市场的所有信息，不存在超额收益。被动投资管理的投资理念是市场是有效的，更加注重资产类别，最常见的方法是购买指数基金并关注已发布指数的表现。而主动投资管理主要依赖于投资者对于市场和个股的主观判定，根据能够获取的公开数据，通过将数学模型应用于金融领域，对股票进行评判，从而构建投资组合获取收益。量化投资通过对大量历史数据进行统计处理，挖掘投资机会，规避主观因素，受到越来越多投资者的追捧。量化投资兴起以后，人们也逐渐利用各种技术来进行股票价格的预测，从而更好地建立量化投资策略。早期国内外学者采用统计学方法建模，来预测股票价格，如指数平均法，多元回归法，自回归平均移动模型等，但是由于股票市场受多种因素影响，同时数据量很大，导致股票预测难度很大，各种统计学模型预测效果并不令人满意。\n近几年来，机器学习，深度学习和神经网络等相关技术不断发展，为股票价格预测和量化策略的构建提供了技术支持，不少学者通过随机森林法，神经网络，支持向量机和卷积神经网络等方法完成新的突破。股票市场足够的历史数据加之以多元的技术支撑，为股票价格预测和量化策略的构建提供了有利条件。\n1.1.2 研究意义 从国家经济体系和金融市场的长远发展来看，对于股票价格预测模型和量化投资策略的研究必不可少。我国起步较晚，金融市场不够成熟，金融工具不够丰富，市场效率较低，但是近几年来国家逐渐放宽政策，大力建设金融市场，为量化投资的发展提供“温床”，发展量化投资及新兴金融技术可以提供我们国家金融市场弯道超车的机会。并且股票价格指数作为一项重要的经济指标，对于我国经济发展起着晴雨表的作用。\n从个人投资者和机构投资者的角度来看，构建股票价格预测模型和量化投资策略模型提高了市场的有效性。个人投资者的专业知识不够完善，投资行为具有一定的盲目性，构建相关模型，为其提供参考，能够减少判断失误的概率，改变个人投资者在资本市场处于相对弱势的地位。并且对于机构投资者而言，理性客观的模型加之以个人经验的判断，提高了决策的正确性，为投资行为提供了新的方向。\n综上，结合我国目前的发展现状，本文选取个股进行股票价格预测模型和 A 股全市场的股票进行量化策略研究有重要的现实研究意义。\n1.2 研究综述 White（1988）$^{[1]}$ 使用 BP 神经网络来预测 IBM 股票的日收益率。然而，由于 BP 神经网络模型易受梯度爆炸的影响，导致模型无法收敛到全局最小值，从而无法实现准确的预测。\nKimoto（1990）$^{[2]}$ 使用模块化神经网络技术开发了一个用于东证股价指数（Tokyo Stock Exchange Prices Indexes，TOPIX）预测的系统。该系统不仅成功预测了东京证券交易所的 TOPIX，还通过基于预测结果的股票交易模拟，实现了一定程度的盈利。\nG．Peter Zhang（2003）$^{[3]}$ 对差分整合移动平均自回归（Autoregressive Integrated Moving Average，ARIMA）模型和人工神经网络（Artificial Neural Network，ANN）模型在时间序列预测中的性能进行了对比研究。结果显示，ANN 模型在时间序列预测的精度上显著优于 ARIMA 模型。\nRyo Akita（2016）$^{[4]}$ 选取消费者物价指数、市盈率以及报纸上的各种事件作为特征，利用段落向量和 LSTM 网络构建了一个金融时间序列预测模型。通过东京证券交易所五十家上市公司的实际数据，验证了该模型在股票开盘价预测方面的有效性。\nKunihiro Miyazaki（2017）$^{[5]}$ 通过提取股票日线图像及每 30 分钟的股票价格数据，构建了一个针对东证核心 30 指数（Topix Core 30）及其成分股涨跌预测的模型。研究对比了多种模型，包括逻辑回归（Logistic Regression, LR）、随机森林（Random Forest, RF）、多层感知器（Multilayer Perceptron, MLP）、LSTM、CNN、PCA-CNN 和 CNN-LSTM。结果表明，LSTM 在预测性能上最优，CNN 表现一般，但结合 CNN 和 LSTM 的混合模型可以提升预测精度。\nTaewook Kim（2019）$^{[6]}$ 提出了一个 LSTM-CNN 混合模型，用于结合股票价格时间序列与股票价格图像两种数据表示形式的特征，以预测 S\u0026amp;P 500 指数的股价。研究表明，LSTM-CNN 模型在股价预测方面优于单一模型，并且这种预测对于构建量化投资策略具有一定的实际意义。\n1.3 论文的创新点 本文股票预测方面具有以下创新点：\n分别选用国内 A 股上海浦东发展银行和国外美股 IBM 的数据进行研究，避免单一市场研究的局限性。并且传统的 BP 模型从未考虑时间因素，要么像 LSTM 模型考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。实证过程中并与 LSTM 模型，GRU 模型进行对比，说明其能够提升预测准确率。 股票价格预测模型采用股票多特征进行训练，包括开盘价，闭盘价，最高价和交易量等特征，相对于单特征预测股票收盘价理论上更加精确，能更好地比较 LSTM，GRU 和 BiLSTM 对于股票的预测效果。 本文量化策略研究方面具有以下创新点：\n未使用市面上已有的常见因子，使用自己通过遗传规划算法（Genetic Programming，GP）和人工数据挖掘得到的多个价量因子，并通过 LightGBM 模型进行对 50 个因子的筛选，选出重要程度最高的 10 个因子。 传统的量化投资模型一般用 LSTM 模型和 CNN 模型构建量化投资策略，本文使用 A 股全市场的数据，利用 BiLSTM 模型选取进行因子组合，建立量化投资策略，最后对该策略进行实证与回测，发现该策略优于市场基准指数（中证全指），说明了 BiLSTM 模型在股票价格预测和量化投资的实际应用价值。 1.4 论文研究框架 本文基于深度学习算法分别进行了股票价格预测和量化策略研究，本文的具体研究框架如 Fig. 1 所示：\nFig. 1. Research Framework.\n本文具体研究框架内容如下：\n第一章为绪论。本章首先对股票价格预测和量化策略研究的研究意义和研究背景进行了介绍。随后对研究现状进行了综述，然后说明了本文相比现有的研究的创新点，最后简要阐述了本文的研究框架。\n第二章为相关理论基础。本章对本文研究中涉及到的深度学习模型和量化选股的基本理论进行了介绍。深度学习模型小节依次介绍了 RNN，LSTM，GRU 和 BiLSTM 这四个深度学习模型，其中重点介绍了 LSTM 模型的内部结构。量化选股理论小节依次介绍了均值－方差模型，资本资产定价模型，套利定价理论，多因子模型，Fama－French 三因子模型和五因子模型。本小节从各种金融理论和模型发展脉络中介绍了多因子量化选股的历程。\n第三章为 LSTM，GRU 和 BiLSTM 在股票价格预测中比较研究。本章首先介绍了实验所用国内及国外股票的数据集，然后对于数据进行归一化和数据划分的预处理步骤。紧接着说明了本章所使用 LSTM，GRU 和 BiLSTM 这三个模型的网络结构，模型的编译和超参数设置，并进行了实验得到实验结果。最后对实验结果进行分析和本章小结。\n第四章为基于 LightGBM-BiLSTM 的量化投资模型研究。本章首先大致介绍了实验步骤，然后分别介绍了实验所用的股票数据和因子数据。之后再对因子依次进行缺失值处理，去极值，因子标准化和因子中性化处理得到清洗后的因子。随后再利用 LightGBM 和 BiLSTM 分别进行因子选择和因子组合，最后根据得到的模型进行量化策略构建，并对量化策略进行回测。\n第五章为总结与展望。本章对于本文关于股票价格预测与量化投资策略的主要研究内容进行了总结，之后针对目前研究所存在的不足，对未来研究的方向进行了展望。\n第二章 相关理论基础 2.1 深度学习模型 2.1.1 RNN 循环神经网络（Recurrent Neural Network，RNN）被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价和交易信息都是序列数据，因此之前有很多工作，基于 RNN 来预测股票价格。RNN 采用十分简单的重复模块的链式结构，例如单个 tanh 层。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题。Fig. 2 是 RNN 结构图。\nFig. 2. RNN Structure Diagram. (Image source: Understanding LSTM Networks)\n2.1.2 LSTM 长短时记忆网络（Long Short-Term Memory，LSTM）是一种特殊的 RNN，能够学习长期依赖关系。它们是由 Hochreiter \u0026amp; Schmidhuber（1997）$^{[7]}$ 提出的，并在随后的工作中被许多人改进和推广。由于独特的设计结构，LSTM 有着间隙长度相对不敏感的特点，并且解决了传统 RNN 的梯度消失和梯度爆炸的问题。相对于传统 RNN 和隐马尔可夫模型（Hidden Markov Model，HMM）等其他时间序列模型，LSTM 能更好地处理和预测时间序列中间隔和延迟非常长的重要事件。因此，LSTM 广泛应用在机器翻译和语音识别的领域。\nLSTM 被明确设计为避免长期依赖问题。所有的递归神经网络都具有神经网络的重复模块链的形式，而 LSTM 对 RNN 的结构进行了改进。LSTM 并没有采用单一神经网络层，而是采用一种特殊的方式进行交互的四层结构。\nFig. 3. LSTM Structure Diagram 1. (Image source: Understanding LSTM Networks)\nFig. 4. LSTM Structure Diagram 2. (Image source: Understanding LSTM Networks)\n如 Fig.3 所示，黑线用来表示传输一个节点的输出向量到另一个节点的输入向量。神经网络层（Neural network layer）是带有 $\\sigma$ 激活函数或者 tanh 激活函数的处理模块；逐点运算（Pointwise operation）是代表向量与向量之间进行点乘运算；向量传输（Vector transfer）是表示信息传递方向；汇合（Concatenate）和复制（Copy）分别用两个黑线合在一起和两个黑线分开来表示信息的汇合和信息的复制。\n下面我们以 LSTM 为例，进行其结构详细的说明。\n遗忘门（forget gate） Fig. 5. Forget Gate Calculation (Image source: Understanding LSTM Networks)\n$$ f_{t} = \\sigma\\left(W_{f} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{f}\\right) $$参数说明：\n$h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $\\sigma$ ：sigmoid 激活函数 $W_{f}$ ：遗忘门的权重矩阵 $b_{f}$ ：遗忘门的偏差向量参数 第一步如 Fig.5 所示，是一个决定从细胞状态中丢弃的信息的过程。该过程由 sigmoid 函数计算得到 $f_{t}$ 的值（$f_{t}$ 的范围在 0 到 1 之间，其中 0 代表完全不通过，1 代表完全通过）来决定细胞状态 $C_{t-1}$ 通过或者部分通过。\n输入门（input gate） Fig. 6. Input Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} i_{t} \u0026= \\sigma\\left(W_{i} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{i}\\right) \\\\ \\tilde{C}_{t} \u0026= \\tanh\\left(W_{C} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{C}\\right) \\end{aligned} $$参数说明：\n$h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $\\sigma$ ：sigmoid 激活函数 $W_{i}$ ：输入门的权重矩阵 $b_{i}$ ：输入门的偏差向量参数 $W_{C}$ ：细胞状态的权重矩阵 $b_{C}$ ：细胞状态的偏差向量参数 $\\tanh$ ：tanh 激活函数 第二步如 Fig.6 所示，通过 sigmoid 函数计算我们要在细胞状态中存储什么信息，接下来通过一个 $\\tanh$ 层创建候选向量 $\\tilde{C}_{t}$，该向量将会被加到细胞的状态中。\nFig. 7. Current Cell State Calculation (Image source: Understanding LSTM Networks)\n$$ C_{t} = f_{t} * C_{t-1} + i_{t} * \\tilde{C}_{t} $$参数说明：\n$C_{t-1}$ ：上一时刻的细胞状态 $\\tilde{C}_{t}$ ：临时细胞状态 $i_{t}$ ：输入门的值 $f_{t}$ ：遗忘门的值 第三步如 Fig.7 所示，当前时刻的细胞状态 $C_t$ 通过结合遗忘门和输入门的作用计算得到。\n遗忘门 $f_t$ 对上一时刻的细胞状态 $C_{t-1}$ 进行加权，以丢弃不需要的信息。 输入门 $i_t$ 对候选细胞状态 $\\tilde{C}_t$ 进行加权，决定引入多少新信息。\n最终，两个部分相加，更新得出当前时刻的细胞状态 $C_t$。 输出门（output gate） Fig. 8. Output Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} o_{t} \u0026= \\sigma\\left(W_{o} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{o}\\right) \\\\ h_{t} \u0026= o_{t} * \\tanh\\left(C_{t}\\right) \\end{aligned} $$参数说明：\n$o_{t}$ ：输出门的值 $\\sigma$ ：sigmoid 激活函数 $W_{o}$ ：输出门的权重矩阵 $h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $b_{o}$ ：输出门的偏差向量参数 $h_{t}$ ：当前时刻的输出 $\\tanh$ ：tanh 激活函数 $C_{t}$ ：当前时刻的细胞状态 第四步如 Fig.8 所示，使用 sigmoid 函数计算输出门的值，最后通过 tanh 激活函数将这一时刻的细胞状态 $C_{t}$ 进行处理并与输出门的值 $o_{t}$ 相乘得到当前时刻的输出 $h_{t}$。\n2.1.3 GRU K. Cho（2014）$^{[8]}$ 提出了门控循环单元（Gated Recurrent Unit，GRU）。GRU 主要是在 LSTM 的基础上进行了简化和调整，将 LSTM 原有的遗忘门、输入门和输出门合并为更新门（update gate）和重置门（reset gate）。此外，GRU 还将细胞状态与隐藏状态合并，从而减少了模型的复杂性，同时在某些任务中仍能够达到与 LSTM 相当的性能。\n该模型在训练数据集比较大的情况下可以节省很多时间，在某些较小和较不频繁的数据集上表现出更好的性能$^{[9][10]}$。\nFig. 9. GRU Structure Diagram (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} z_{t} \u0026= \\sigma\\left(W_{z} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ r_{t} \u0026= \\sigma\\left(W_{r} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ \\tilde{h}_{t} \u0026= \\tanh\\left(W \\cdot \\left[r_{t} * h_{t-1}, x_{t}\\right]\\right) \\\\ h_{t} \u0026= \\left(1 - z_{t}\\right) * h_{t-1} + z_{t} * \\tilde{h}_{t} \\end{aligned} $$参数说明：\n$z_{t}$ ：更新门的值 $r_{t}$ ：重置门的值 $W_{z}$ ：更新门的权重矩阵 $W_{r}$ ：重置门的权重矩阵 $\\tilde{h}_{t}$ ：临时的输出 2.1.4 BiLSTM 双向长短时记忆网络（Bidirectional Long Short-Term Memory，BiLSTM）是由前向的 LSTM 与后向的 LSTM 结合成的。BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，能够学习到对时间有长期依赖性的信息，与单向 LSTM 相比可以更好地考虑未来数据的反向影响。Fig. 10 是 BiLSTM 结构图。\nFig. 10. BiLSTM Structure Diagram. (Image source: Baeldung)\n2.2 量化选股理论 2.2.1 均值－方差模型 量化选股策略起源于 20 世纪 50 年代，Markowitz（1952）$^{[11]}$ 提出了均值－方差模型（Mean-Variance Model）。该模型不仅奠定了现代投资组合理论的基础，将投资风险量化，还建立了一个描述风险和预期收益率的具体模型。它打破了以往仅对投资组合进行定性分析而缺乏定量分析的局面，将数学模型成功引入金融投资领域。\n$$ \\begin{aligned} \\mathrm{E}\\left(R_{p}\\right) \u0026= \\sum_{i=1}^{n} w_{i} \\mathrm{E}\\left(R_{i}\\right) \\\\ \\sigma_{p}^{2} \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\operatorname{Cov}\\left(R_{i}, R_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\sigma_{i} \\sigma_{j} \\rho_{ij} \\\\ \\sigma_{i} \u0026= \\sqrt{\\operatorname{Var}\\left(R_{i}\\right)}, \\quad \\rho_{ij} = \\operatorname{Corr}\\left(R_{i}, R_{j}\\right) \\end{aligned} $$$$ \\min \\sigma_{p}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} \\mathrm{E}\\left(R_{i}\\right) w_{i} = \\mu_{p}, \\quad \\sum_{i=1}^{n} w_{i} = 1 $$$$ \\begin{aligned} \\Omega \u0026= \\begin{pmatrix} \\sigma_{11} \u0026 \\cdots \u0026 \\sigma_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\sigma_{n1} \u0026 \\cdots \u0026 \\sigma_{nn} \\end{pmatrix} = \\begin{pmatrix} \\operatorname{Var}\\left(R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Cov}\\left(R_{1}, R_{n}\\right) \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\operatorname{Cov}\\left(R_{n}, R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Var}\\left(R_{n}\\right) \\end{pmatrix} \\\\ \\Omega^{-1} \u0026= \\begin{pmatrix} v_{11} \u0026 \\cdots \u0026 v_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ v_{n1} \u0026 \\cdots \u0026 v_{nn} \\end{pmatrix} \\\\ w_{i} \u0026= \\frac{1}{D}\\left(\\mu_{p} \\sum_{j=1}^{n} v_{ij}\\left(C \\mathrm{E}\\left(R_{j}\\right) - A\\right) + \\sum_{j=1}^{n} v_{ij}\\left(B - A \\mathrm{E}\\left(R_{j}\\right)\\right)\\right), \\quad i = 1, \\ldots, n \\end{aligned} $$$$ \\begin{aligned} A \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{j}\\right), \\quad B = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{i}\\right) \\mathrm{E}\\left(R_{j}\\right), \\quad C = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij}, \\quad D = BC - A^{2} \u003e 0 \\\\ \\sigma_{p}^{2} \u0026= \\frac{C \\mu_{p}^{2} - 2A \\mu_{p} + B}{D} \\end{aligned} $$其中：\n$\\mathrm{E}\\left(R_{p}\\right)$ 和 $\\mu_{p}$ 是投资组合 $p$ 的预期收益率 $\\mathrm{E}\\left(R_{i}\\right)$ 是资产 $i$ 的预期收益率 $\\sigma_{i}$ 是资产 $i$ 的标准差 $\\sigma_{j}$ 是资产 $j$ 的标准差 $w_{i}$ 是资产 $i$ 在投资组合中的比例 $\\sigma_{p}^{2}$ 是投资组合 $p$ 的方差 $\\rho_{ij}$ 是资产 $i$ 和资产 $j$ 之间的相关系数 通过以上公式$^{[12]}$，我们可以构建投资组合，来让我们的投资组合在一定的期望收益率的条件下，非系统风险降低到最小。\n2.2.2 资本资产定价模型 William Sharpe（1964）$^{[13]}$、John Lintner（1965）$^{[14]}$ 和 Jan Mossin（1966）$^{[15]}$ 提出了资本资产定价模型（Capital Asset Pricing Model，CAPM）。该模型认为，一项资产的预期收益与该资产的风险度量 $\\beta$ 值相关。这一模型通过简单的线性关系，将资产的预期收益率与市场风险联系起来，使得 Markowitz（1952）$^{[11]}$ 的投资组合选择理论更贴近现实世界，同时为多因子选股模型的建立奠定了理论基础。\n根据资本资产定价模型，对于一个给定的资产 $i$，它的预期收益率和市场投资组合的预期收益率之间的关系可以表示为：\n$$ E\\left(r_{i}\\right) = r_{f} + \\beta_{im}\\left[E\\left(r_{m}\\right) - r_{f}\\right] $$其中：\n$E\\left(r_{i}\\right)$ 是资产 $i$ 的预期收益率 $r_{f}$ 是无风险利率 $\\beta_{im}$（Beta）是资产 $i$ 的系统性风险系数，$\\beta_{im} = \\frac{\\operatorname{Cov}\\left(r_{i}, r_{m}\\right)}{\\operatorname{Var}\\left(r_{m}\\right)}$ $E\\left(r_{m}\\right)$ 是市场投资组合 $m$ 的预期收益率 $E\\left(r_{m}\\right) - r_{f}$ 是市场风险溢价 2.2.3 套利定价理论和多因子模型 Ross（1976）$^{[16]}$ 提出了套利定价理论（Arbitrage Pricing Theory，APT）。该理论认为，套利行为是形成市场均衡价格的决定性因素，通过在收益率形成过程中引入一系列因子构建线性相关关系，克服了资本资产定价模型（CAPM）的局限性，为后续研究者提供了重要的理论指导。\n套利定价理论被认为是多因子模型（Multiple-Factor Model，MFM）的理论基础，是资产价格模型的重要组成部分，也是资产价格理论的基石之一。多因子模型的一般表达形式为：\n$$ r_{j} = a_{j} + \\lambda_{j1} f_{1} + \\lambda_{j2} f_{2} + \\cdots + \\lambda_{jn} f_{n} + \\delta_{j} $$其中：\n$r_{j}$ 是资产 $j$ 的收益率 $a_{j}$ 是资产 $j$ 的常数 $f_{n}$ 是系统性因素 $\\lambda_{jn}$ 是因子载荷 $\\delta_{j}$ 是随机误差 2.2.4 Fama－French 三因子模型和五因子模型 Fama（1992）和 French（1992）$^{[17]}$ 使用横截面回归与时间序列结合的方法发现，股票市场的 $\\beta$ 值无法解释不同股票回报率的差异，而上市公司的市值、账面市值比和市盈率等可以显著解释股票回报率的差异。他们认为超额收益是对 CAPM 中 $\\beta$ 未能反映的风险因素的补偿，由此提出了 Fama－French 三因子模型。这三个因子分别为：\n市场风险溢价因子（Market Risk Premium）\n表示市场整体的系统性风险，即市场投资组合的预期收益减去无风险利率的差值。 衡量投资者承担系统性风险（即无法通过分散投资消除的风险）所期望的超额回报。 计算公式为：\n$$ \\text{Market Risk Premium} = E(R_m) - R_f $$ 其中 $E(R_m)$ 是市场的预期收益率，$R_f$ 是无风险利率。 市值因子（Size, SMB: Small Minus Big）\n表示小市值股票与大市值股票之间的收益差异。 小市值股票通常风险更高，但历史数据显示，其预期收益也往往高于大市值股票。 计算公式为：\n$$ SMB = R_{\\text{Small}} - R_{\\text{Big}} $$ 反映了市场对小市值股票的额外风险溢价的补偿。 账面市值比因子（Value, HML: High Minus Low）\n反映高账面市值比（即“价值型股票”）与低账面市值比（即“成长型股票”）之间的收益差异。 高账面市值比的股票通常定价较低（被市场低估），但长期来看可能获得较高回报。 计算公式为：\n$$ HML = R_{\\text{High}} - R_{\\text{Low}} $$ 低账面市值比的股票可能因市场对其过于乐观的预期而被高估。 该模型将 APT 模型中的因子具体化，并得出结论：投资小市值、高成长的股票具有高风险高收益的特性。Fama－French 三因子模型被广泛应用于现代投资行为的分析和实践中。\n随后，Fama（2015）和 French（2015）$^{[18]}$ 对三因子模型进行了扩展，新增了以下两个因子：\n盈利水平因子（Profitability, RMW: Robust Minus Weak）\n反映高盈利公司与低盈利公司之间的收益差异。 盈利能力强的公司（高 ROE、净利润率）更可能提供稳定且较高的回报。 计算公式为：\n$$ RMW = R_{\\text{Robust}} - R_{\\text{Weak}} $$ 投资水平因子（Investment, CMA: Conservative Minus Aggressive）\n反映保守型投资公司与激进型投资公司之间的收益差异。 激进型公司（扩张迅速，资本开支较高）通常伴随着更大的经营风险，而保守型公司（资本支出相对稳健）表现出更高的稳定性和收益。 计算公式为：\n$$ CMA = R_{\\text{Conservative}} - R_{\\text{Aggressive}} $$ Fama-French 三因子模型公式为：\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\epsilon_i $$Fama-French 五因子模型公式为：\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\beta_{i,RMW} \\cdot RMW + \\beta_{i,CMA} \\cdot CMA + \\epsilon_i $$其中：\n$R_i$: 股票 $i$ 的预期收益率 $R_f$: 无风险收益率 $R_m$: 市场组合的预期收益率 $R_m - R_f$: 市场风险溢价因子 $SMB$: 小市值减去大市值股票收益 $HML$: 高账面市值比减去低账面市值比股票收益 $RMW$: 高盈利能力减去低盈利能力股票收益 $CMA$: 低投资活动减去高投资活动股票收益 $\\beta_{i,*}$: 股票 $i$ 对应因子的敏感度 $\\epsilon_i$: 回归残差 2.2.5 模型对比表格 以下表格总结了 均值-方差模型、资本资产定价模型 (CAPM)、套利定价理论 (APT) 和 Fama-French 模型 的核心内容及因子来源：\n模型 核心内容 因子来源 均值-方差模型 投资组合理论的基础，通过期望收益和协方差矩阵优化投资组合。 投资组合中资产的期望收益和协方差矩阵 资本资产定价模型 (CAPM) 通过市场风险因子（$\\beta$）解释资产收益，奠定多因子模型理论基础。 市场因子 $\\beta$ 套利定价理论 (APT) 多因子框架，允许多个经济变量解释资产收益，例如通胀率、利率等。 多因子（宏观经济变量，如通胀率、利率等） Fama-French 三因子模型 增加市值因子和账面市值比因子，改进对资产收益的解释能力。 市场因子、SMB（市值因子）、HML（账面市值比因子） Fama-French 五因子模型 在三因子模型基础上增加盈利因子和投资因子，进一步完善资产定价模型。 市场因子、SMB、HML、RMW（盈利因子）、CMA（投资因子） 以下表格总结了这些模型的优点及不足：\n模型 优点 不足 均值-方差模型 提供了系统化投资组合优化方法，奠定现代投资理论基础。 仅针对收益和方差进行优化，未明确风险补偿的来源。 资本资产定价模型 (CAPM) 简单易用，通过市场风险解释收益差异，为多因子模型提供理论基础。 假设单因子（市场风险）决定收益，忽略其他系统性风险因子。 套利定价理论 (APT) 允许多个因子解释资产收益，减少对单因子假设的依赖，更灵活。 未明确具体因子，实操性较低，仅提供框架。 Fama-French 三因子模型 通过增加市值因子和账面市值比因子，显著提高了对资产收益的解释能力。 忽略了盈利能力和投资行为等其他因子。 Fama-French 五因子模型 在三因子模型基础上增加盈利因子和投资因子，更全面地捕捉影响资产收益的关键变量。 模型复杂度较高，对数据要求高，仍可能遗漏某些潜在因子。 第三章 LSTM，GRU 和 BiLSTM 在股票价格预测中比较研究 3.1 实验数据介绍 国内外很多学者的研究以本国的股票指数为主，对于不同市场的单个股票研究相对较少。并且很少有研究将 LSTM、GRU、BiLSTM 这三个模型进行对比研究。因此本文分别选择国内 A 股上海浦东发展银行（简称浦发银行，代码600000）和美股 International Business Machines Corporation（简称 IBM）进行研究，这样更能准确地对我们使用的三个模型进行对比。其中浦发银行采用 2008 年 1 月 1 日到 2020 年 12 月 31 日的股票数据，共有 3114 条有效数据，数据来源于 Tushare 金融大数据平台。我们选取该数据集的日期（date）、开盘价（open）、收盘价（close）、最高价（high）、最低价（low）和成交量（volume）这 6 个特征进行实验。浦发银行的数据集除日期作为时间序列的索引以外，其他 5 个特征均作为自变量。IBM 采用 1990 年 1 月 2 日到 2018 年 11 月 15 日的股票数据，共有 7278 条有效数据，数据来源于雅虎财经。我们选取该数据集的日期（date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、调整后的收盘价（Adj Close）和成交量（volume）这 7 个特征进行实验。IBM 的数据集除日期作为时间序列的索引以外，其他 6 个特征均作为自变量。本次实验选择收盘价（close）作为被预测的变量。表3.1.1和表3.1.2分别是两个数据集的部分数据。\n3.1.1 浦发银行数据集部分展示 date open close high low volume code 2008-01-02 9.007 9.101 9.356 8.805 131583.90 600000 2008-01-03 9.007 8.645 9.101 8.426 211346.56 600000 2008-01-04 8659 9.009 9.111 8.501 139249.67 600000 2008-01-07 8.970 9.515 9.593 8.953 228043.01 600000 2008-01-08 9.551 9.583 9.719 9.517 161255.31 600000 2008-01-09 9.583 9.663 9.772 9.432 102510.92 600000 2008-01-10 9.701 9.680 9.836 9.602 217966.25 600000 2008-01-11 9.670 10.467 10.532 9.670 231544.21 600000 2008-01-14 10.367 10.059 10.433 10.027 142918.39 600000 2008-01-15 10.142 10.051 10.389 10.006 161221.52 600000 数据来源：Tushare\n3.1.2 IBM数据集部分展示 Date Open High Low Close Adj Close Volume 1990-01-02 23.6875 24.5313 23.6250 24.5000 6.590755 7041600 1990-01-03 24.6875 24.8750 24.5938 24.7188 6.649599 9464000 1990-01-04 24.7500 25.0938 24.7188 25.0000 6.725261 9674800 1990-01-05 24.9688 25.4063 24.8750 24.9375 6.708448 7570000 1990-01-08 24.8125 25.2188 24.8125 25.0938 6.750481 4625200 1990-01-09 25.1250 25.3125 24.8438 24.8438 6.683229 7048000 1990-01-10 24.8750 25.0000 24.6563 24.7500 6.658009 5945600 1990-01-11 24.8750 25.0938 24.8438 24.9688 6.716855 5905600 1990-01-12 24.6563 24.8125 24.4063 24.4688 6.582347 5390800 1990-01-15 24.4063 24.5938 24.3125 24.5313 6.599163 4035600 数据来源：雅虎财经\n3.2 实验数据预处理 3.2.1 数据的归一化 实验中各个特征在单位和量级上存在差异，比如股票价格和成交量之间量级差异巨大，会对我们实验最终预测的结果产生影响。因此我们采用 sklearn.preprocessing 库中的 MinMaxScaler 方法将数据的特征缩放至 0 到 1 之间。这样既能提升模型精度，也能提升模型收敛速度。归一化公式：\n$$ x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)} $$其中 $x^{\\prime}$ 是归一化后的数据，$x$ 是原始数据， $\\min (x)$ 是原始数据集的最小值， $\\max (x)$ 是原始数据集的最大值。之后在我们的实验过程中获得预测结果之后，还要将数据进行反归一化处理，之后才能进行对于股票价格的预测和模型评估。\n3.2.2 数据的划分 此处分别将浦发银行和 IBM 的整个实验数据集送入，设置循环核时间步（timestep）都为 60，每个时间步输入特征个数分别为 5 和 6。这样可以输入前 60 个交易日的数据，预测出第 61 天的收盘价。使我们的数据集符合之后要比较的三种神经网络模型输入的要求，依次是送入样本数，循环核时间展开步数和每个时间步输入特征个数。之后我们再将浦发银行的数据集按照 2488：311：255 的比例将归一化的数据集划分为训练集，验证集，测试集三个部分。将 IBM 的数据集按照 6550：364：304 的比例将归一化的数据集划分为训练集，验证集，测试集三个部分。我们这里划分出验证集的目的是为了方便进行调整模型的超参数以便优化各个模型之后再进行比较。\n3.3 模型网络结构 本文通过大量反复试验最终各个模型设置的网络结构如下表所示，其中层与层之间使用循环神经网络默认的 tanh 和 linear 作为激活函数，并且为了防止过拟合加入 Dropout，Dropout 的丢弃比例（rate）取值为 0.2。LSTM 和 GRU 每个循环层的神经元个数为 50，BiLSTM 循环层的神经元的个数为 100。LSTM、GRU、BiLSTM 每个模型分别采用四层 LSTM、GRU、BiLSTM 和一层全连接层，其中每层网络之间都设置了一个 Dropout。\n3.3.1 IBM的LSTM网络结构 Layer(type) Output Shape Param# lstm_1 (LSTM) (None, 60, 50) 11400 dropout_1 (Dropout) (None, 60, 50) 0 lstm_2 (LSTM) (None, 60, 50) 20200 dropout_2 (Dropout) (None, 60, 50) 0 lstm_3 (LSTM) (None, 60, 50) 20200 dropout_3 (Dropout) (None, 60, 50) 0 lstm_4 (LSTM) (None, 50) 20200 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params：72,051\nTrainable params：72,051\nNon-trainable params：0\n3.3.2 IBM的GRU网络结构 Layer(type) Output Shape Param# gru_1 (GRU) (None, 60, 50) 8550 dropout_1 (Dropout) (None, 60, 50) 0 gru_2 (GRU) (None, 60, 50) 15150 dropout_2 (Dropout) (None, 60, 50) 0 gru_3 (GRU) (None, 60, 50) 15150 dropout_3 (Dropout) (None, 60, 50) 0 gru_4 (GRU) (None, 50) 15150 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params：54,051\nTrainable params：54,051\nNon-trainable params：0\n3.3.3 IBM的BiLSTM网络结构 Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 60, 100) 22800 dropout_1 (Dropout) (None, 60, 100) 0 bidirectional_2 (Bidirection) (None, 60, 100) 60400 dropout_2 (Dropout) (None, 60, 100) 0 bidirectional_3 (Bidirection) (None, 60, 100) 60400 dropout_3 (Dropout) (None, 60, 100) 0 bidirectional_4 (Bidirection) (None, 100) 60400 dropout_4 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 1) 101 Total params：204,101\nTrainable params：204,101\nNon-trainable params：0\n3.4 模型和编译及超参数设置 本文模型在以验证集的损失函数最小为目标，进行不断的超参数调试之后，对于浦发银行的三个模型都选用 epochs=100，batch_size=32；对于 IBM 的三个模型都选用 epochs=50，batch_size=32。其中优化器都采用自适应矩估计（Adaptive moment estimation，Adam）$^{[19]}$。使用其 keras 包中的默认值，即 lr=0.001、beta_1=0.9、beta_2=0.999、epsilon=1e-08 和 decay=0.0。损失函数采用均方误差（Mean Square Error，MSE）。\n参数解释：\nlr：学习率 beta_1：一阶矩估计的指数衰减率 beta_2：二阶矩估计的指数衰减率 epsilon：模糊因子 decay：每次更新后的学习率衰减值 3.5 实验结果与分析 首先简单介绍一下模型使用评价的这几个指标。计算公式如下：\n均方误差（Mean Square Error，MSE）： $$ M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2} $$ 均方根误差（Root Mean Squared Error，RMSE)： $$ R M S E=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}} $$ 平均绝对误差（Mean Absolute Error，MAE）： $$ M A E=\\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-\\hat{Y}_{i}\\right| $$ \\( R^2 \\)（R Squared）： $$ \\begin{gathered} \\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} \\\\ R^{2}=1-\\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\end{gathered} $$其中：$n$ 是样本的数量，$Y$ 是股票实际的收盘价，$\\hat{Y}_{i}$ 是股票预测的收盘价， $\\bar{Y}$ 是股票平均的收盘价。MSE，RMSE 和 MAE 越小该模型越精确。 $R^{2}$ 评价模型系数拟合优度越大越好。\n3.5.1 浦发银行实验结果 LSTM GRU BiLSTM MSE 0.059781 0.069323 0.056454 RMSE 0.244501 0.263292 0.237601 MAE 0.186541 0.202665 0.154289 R-squared 0.91788 0.896214 0.929643 比较三个模型的评价指标，我们可以发现在浦发银行测试集上 BiLSTM 模型的 MSE、RMSE 和 MAE 都小于 LSTM 模型和 GRU 模型，而 R-Squared 都大于 LSTM 模型和 GRU 模型。我们通过对比 RMSE 发现，BiLSTM 相较于 LSTM 在验证集上有 2.90%的性能提升，BiLSTM 相较于 GRU 在验证集上有 10.81%的性能提升。\n3.5.2 IBM实验结果 LSTM GRU BiLSTM MSE 18.01311 12.938584 11.057501 RMSE 4.244186 3.597024 3.325282 MAE 3.793223 3.069033 2.732075 R-squared 0.789453 0.851939 0.883334 比较三个模型的评价指标，我们可以发现在 IBM 测试集上 BiLSTM 模型的 MSE、RMSE 和 MAE 都小于 LSTM 模型和 GRU 模型，而 R-Squared 都大于 LSTM 模型和 GRU 模型。我们通过对比 RMSE 发现，BiLSTM 相较于 LSTM 在验证集上有 27.63%的性能提升，BiLSTM 相较于 GRU 在验证集上有 8.17%的性能提升。\n3.6 本章小结 本章先是介绍了实验所选用的浦发银行和 IBM 两个数据集以及选用的特征，之后对数据集进行了归一化、数据划分的预处理步骤。同时详细的说明了实验所使用 LSTM、GRU 和 BiLSTM 模型的网络结构和超参数。最后得到了每个模型的损失函数图像和一系列的拟合图形。比较了模型的多个评价指标和拟合图像最终得到 BiLSTM 模型能够更好地对股票价格进行预测，为我们下一章研究 LightGBM-BiLSTM 的量化投资策略奠定了基础。\n第四章 基于 LightGBM-BiLSTM 的量化投资模型研究 4.1 实验步骤 Fig. 11. LightGBM-BiLSTM Diagram.\n如 Fig.11 所示，本实验先从因子库中选取 50 个因子。之后对因子依次进行去极值、标准化和缺损值填充的因子清洗步骤。再利用 LightGBM 模型进行因子选择，根据因子重要性进行排序得到前十的因子作为本横截面挑选出来的因子。紧接着使用 BiLSTM 建立多因子模型，最后再进行回测分析。\n4.2 实验数据 本文采用的行情数据来源于 Tushare。具体数据集的特征如下表所示。\n4.2.1 股票数据集包含的特征 名称 类型 描述 ts_code str 股票代码 trade_date str 交易日期 open float 开盘价 high float 最高价 low float 最低价 close float 收盘价 pre_close float 昨收价 change float 涨跌额 pct_chg float 涨跌幅（未复权） vol float 成交量（手） amount float 成交额（千元） A股全市场日线数据集包含5,872,309行数据，即包含5,872,309个样本。如表4.2.1所示，A股全市场日线数据集数据集有以下11个特征，分别依次为股票代码（ts_code）、交易日期（trade_date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、昨收价（pre_close）、涨跌额（change）、换手率（turnover_rate）、交易金额（amount）、总市值（total_mv）和复权因子（adj_factor）。\n4.2.2 A股全市场日线数据集部分展示 ts_code trade_date open high low close pre_close change vol amount 600613.SH 20120104 8.20 8.20 7.84 7.86 8.16 -0.30 4762.98 3854.1000 600690.SH 20120104 9.00 9.17 8.78 8.78 8.93 -0.15 142288.41 127992.6050 300277.SZ 20120104 22.90 22.98 20.81 20.88 22.68 -1.80 12212.39 26797.1370 002403.SZ 20120104 8.87 8.90 8.40 8.40 8.84 -0.441 10331.97 9013.4317 300179.SZ 20120104 19.99 20.32 19.20 19.50 19.96 -0.46 1532.31 3008.0594 600000.SH 20120104 8.54 8.56 8.39 8.41 8.49 -0.08 342013.79 290229.5510 300282.SZ 20120104 22.90 23.33 21.02 21.02 23.35 -2.33 38408.60 86216.2356 002319.SZ 20120104 9.74 9.95 9.38 9.41 9.73 -0.32 4809.74 4671.4803 601991.SH 20120104 5.17 5.39 5.12 5.25 5.16 0.09 145268.38 76547.7490 000780.SZ 20120104 10.42 10.49 10.00 10.00 10.30 -0.30 20362.30 20830.1761 [5872309 rows x 11 columns]\n中证全指日线数据集包含5,057行数据，即包含5,057个样本。如表4.2.2所示，中证全指日线数据集有以下7个特征，分别依次为交易日期（trade_date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、交易量（volume）和昨收价（pre_close）。\n4.2.3 中证全指日线数据集部分展示 trade_date open high low close volume pre_close 2006-11-24 1564.3560 1579.3470 1549.9790 1576.1530 7.521819e+09 1567.0910 2006-11-27 1574.1130 1598.7440 1574.1130 1598.7440 7.212786e+09 1581.1530 2006-11-28 1597.7200 1604.7190 1585.3620 1596.8400 7.025637e+09 1598.7440 2006-11-29 1575.3030 1620.2870 1575.3030 1617.9880 7.250354e+09 1596.8400 2006-11-30 1621.4280 1657.3230 1621.4280 1657.3230 9.656888e+09 1617.9880 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 2020-11-11 5477.8870 5493.5867 5422.9110 5425.8017 5.604086e+10 5494.1042 2020-11-12 5439.2296 5454.3452 5413.9659 5435.1379 4.594251e+10 5425.8017 2020-11-13 5418.2953 5418.3523 5364.2031 5402.7702 4.688916e+10 5435.1379 2020-11-16 5422.3565 5456.7264 5391.9232 5456.7264 5.593672e+10 5402.7702 2020-11-17 5454.0696 5454.0696 5395.6052 5428.0765 5.857009e+10 5456.7264 [5057 rows x 7 columns]\n下表4.2.4是原始的因子部分数据。依次经过上述因子缺失值填充、因子去极值、因子标准化和因子中性化这 4 个因子清洗的步骤后，得到如表4.2.5展示的经过因子清洗后的因子部分数据。\n4.2.4 原始的因子数据 trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 4.2.5 清洗后的因子数据 sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.2.6 因子数据 价量因子构建 本文使用如下方式构建价量因子，构建价量因子的基础要素有两点：首先是基础字段，其次是算子。如表4.2.1 所示，基础字段包括日频的最高价（high），最低价（low），开盘价（open），收盘价（close），上一日收盘价（pre_close），成交量（vol），涨跌（pct_chg），换手率（turnover_rate），交易金额（amount），总市值（total_mv）和复权因子（adj_factor）。\n4.2.7 基础字段表 编号 字段名 意义 high 最高价 当日成交订单中最高的价格 low 最低价 当日成交订单中最低的价格 open 开盘价 当日集合竞价成交的价格 close 收盘价 当日最后一笔成交订单的价格 pre_close 上一日收盘价 上一日最后一笔成交订单的价格 vol 成交 全天成交的股票数 pct_chg 涨跌 本日证券涨跌 turnover_rate 换手率 本日证券的换手率 amount 成交金额 全天成交的金额 total_mv 总市值 总股本数乘以当时股价得出的股票总价值 adj_factor 复权因子 权息修复比例 本文通过 gplearn提供的基础算子集和自己定义的一些特殊算子，得到如表所示的算子列表。\n4.2.8 算子列表 算子 名称 定义 add(x, y) 和 \\( x + y\\)；点运算 \\(\\operatorname{div}(x, y)\\) 除 \\( x / y\\)；点运算 \\(\\operatorname{mul}(x, y)\\) 乘 \\( x \\cdot y\\)；点运算 \\(\\operatorname{sub}(x, y)\\) 减 \\( x - y\\)；点运算 neg(x) 负 \\(-x\\)；点运算 \\(\\log(x)\\) 对数 \\(\\log(x)\\)；点运算 max(x, y) 最大值 \\(x, y\\) 中数值较大的数；点运算 \\(\\min(x, y)\\) 最小值 \\(x, y\\) 中数值较小的数；点运算 delta_d(x) d 日差值 当日的 \\(x\\) 值减去 d 日前的 \\(x\\) 值；时序运算 delay_d(x) d 日延时 d 日前的 \\(x\\) 值；时序运算 Corr_d(x, y) d 日相关性 d 日 \\(x\\) 值和 d 日 \\(y\\) 值的相关性；时序运算 Max_d(x) d 日最大值 d 日 \\(x\\) 值的最大值；时序运算 Min_d(x) d 日最小值 d 日 \\(x\\) 值的最小值；时序运算 sort_d(x) d 日排序位置 d 日 \\(x\\) 值的排序值；时序运算 Argsortmin_d(x) d 日最小值位置 d 日 \\(x\\) 值的最小值的位置；时序运算 Argsortmax_d(x) d 日最大值位置 d 日 \\(x\\) 值的最大值的位置；时序运算 \\(\\operatorname{inv}(x)\\) 倒数 \\( 1 / x\\)；点运算 Std_d(x) d 日方差 d 日 \\(x\\) 值的方差；时序运算 abs(x) 绝对值 \\(\\lvert x\\rvert\\)；点运算 4.2.9 遗传规划 遗传规划（Genetic Programming, GP）的核心思想是使用进化算法在算子（operators）与基础字段（terminals）组合而成的巨大搜索空间中，自动“进化”出具有较强预测能力的因子表达式。对于本文中的因子挖掘来说，GP 的主要目标是从表4.2.7中的基础字段和表4.2.8中的算子所能组合成的所有可能表达式中，搜索并找到那些能对下一期股票收益有较好预测效果的因子。GP 的核心流程可分为以下几个步骤：\n初始化（Initialization） 定义算子集与基础字段\n算子集（operators）如表4.2.8所示，包括加、减、乘、除、对数、绝对值、延时、移动最大/最小值、移动相关系数等运算。 基础字段（terminals）如表4.2.7所示，包括开盘价、收盘价、最高价、最低价、成交量、复权因子等。\n这些算子和基础字段可以视作因子表达式树中的“节点”，其中基础字段为叶子节点（终端节点），算子为内部节点。 随机生成初始种群\n在初始化阶段，根据给定的算子集与字段集，随机“拼接”生成一系列因子表达式（可表示为若干语法树或表达式树），形成初始种群。 例如，可能随机产生\n\\[ \\text{因子1}: \\mathrm{Max\\_5}\\bigl(\\mathrm{add}(\\mathrm{vol}, \\mathrm{close})\\bigr), \\quad \\text{因子2}: \\mathrm{sub}\\bigl(\\mathrm{adj\\_factor}, \\mathrm{neg}(\\mathrm{turnover\\_rate})\\bigr), \\dots \\] 每个因子表达式都将对应一个个体（individual）。 适应度函数（Fitness Function） 度量因子的预测能力\n针对每个表达式（个体），我们需要评估它对未来收益或其他目标的预测能力。具体来说，可以在下一期股票收益 \\( r^{T+1} \\) 与当前期因子暴露度 \\( x_k^T \\) 之间，计算其相关系数（IC）或更综合的指标 IR（Information Ratio）来衡量。 设定目标\n若我们希望因子具有更高的相关性（IC），则可令适应度函数为 \\(\\lvert \\rho(x_k^T, r^{T+1})\\rvert\\)； 若我们希望因子的 IR 更高，则可设定适应度函数为 IR 值。 因子 IC 或 IR 越高，该表达式的“适应度”就越高。 因此，我们通常令： \\[ \\text{Fitness} \\bigl(F(x)\\bigr) \\;=\\; \\begin{cases} \\lvert \\rho(x_k^T, r^{T+1})\\rvert \\quad \u0026\\text{(IC最大化)},\\\\[6pt] \\mathrm{IR}(x_k^T) \\quad \u0026\\text{(IR最大化)}. \\end{cases} \\] 其中 \\(\\rho(\\cdot)\\) 表示相关系数，\\(\\mathrm{IR}(\\cdot)\\) 为 IR 指标。\n选择（Selection）、交叉（Crossover）与变异（Mutation） 选择（Selection）\n根据适应度函数的结果，将因子适应度高的表达式“保留”或“繁衍”，适应度较低的表达式则被淘汰。 这类似于生物进化中的“优胜劣汰”。 交叉（Crossover）\n将若干适应度较高的表达式（父本）随机选取一部分“节点”进行交换，从而得到新的表达式（子本）。 在表达式树结构中，可以将子树 A 与子树 B 互换，从而产生新的后代表达式。 例如，若表达式树 \\(\\mathrm{FactorA}\\) 的某个子树与表达式树 \\(\\mathrm{FactorB}\\) 的对应子树相交换，就生成了两个新的表达式。 变异（Mutation）\n以一定概率对表达式的某些节点进行随机变更，比如： 更换节点的算子（例如将 \\(\\mathrm{add}\\) 换为 \\(\\mathrm{sub}\\)）， 替换终端节点的基础字段（例如将 \\(\\mathrm{vol}\\) 换为 \\(\\mathrm{close}\\)）， 或随机改变运算参数（如移动窗口长度、平滑因子等）。 变异可以增加群体的多样性，避免过早收敛或陷入局部最优。 迭代进化（Iteration） 循环执行\n将选择、交叉、变异的操作反复执行多代（generations）。 每一代都产生一个新的因子表达式种群，并对其进行适应度评估。 收敛与终止\n当进化达到预先设定的停止条件（如迭代次数、适应度阈值等）时，算法终止。 通常我们会选出若干个最终适应度较高的因子表达式，将它们视为进化结果。 数学表征：搜索最优因子表达式 将上述过程抽象成下式，可以简单表示因子的搜索目标：\n\\[ F(x) \\;=\\; \\mathrm{GP}\\bigl(\\{\\text{operators}\\}, \\{\\text{terminals}\\}\\bigr), \\] 表示通过 GP 算法在给定算子集（operators）和基础字段集（terminals）上搜索出一个函数 \\(F(x)\\)。从最优化的角度看，我们希望找到：\n\\[ \\max_{F} \\bigl\\lvert \\rho(F^T, r^{T+1}) \\bigr\\rvert \\quad \\text{或者} \\quad \\max_{F} \\; \\mathrm{IR}\\bigl(F\\bigr), \\] 其中\n\\(\\rho(\\cdot)\\) 表示因子与下一期收益的相关系数（IC）， \\(\\mathrm{IR}(\\cdot)\\) 表示该因子的 IR 指标。 在实际应用中，我们会给定一段回测期，对每一代的候选因子进行打分（IC/IR 评估），通过选择、交叉和变异的迭代过程不断“进化”出更优质的因子。\n通过以上步骤，我们最终能够在庞大的算子组合与基础字段组合的搜索空间中，自动挖掘到一批对未来收益有较强预测能力、且具有较好稳健性（如 IR 较高）的因子表达式。\n4.2.10 挖掘出的部分因子 因子名 定义 0 Max＿25(add(turnover_rate, vol)) 1 Max＿30(vol) 2 Max＿25(turnover_rate) 3 Max＿35(add(vol, close)) 4 Max＿30(turnover_rate) 5 sub(Min＿20(neg(pre_close)), div(vol, adj_factor)) 6 Max＿60(max(vol, adj_factor)) 7 Max＿50(amount) 8 div(vol, neg(close)) 9 min(ArgSortMin＿25(pre_close), neg(vol)) 10 neg(max(vol, turnover_rate)) 11 mul(amount, neg(turnover_rate)) 12 inv(add(ArgSortMax＿40(change), inv(pct_chg))) 13 Std＿40(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))) 14 div(log(total_mv),amount) 15 div(neg(Max＿5(amount)), Min＿20(ArgSort＿60(high))) 16 Corr＿30(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))), add(log(Max＿10(pre_close)), high)) 17 ArgSort＿60(neg(turnover_rate)) \u0026hellip; \u0026hellip; 这些因子均是通过遗传规划从算子列表（表4.2.8）与基础字段列表（表4.2.7）中组合而得，具有不同的数学表达形式。\n因子有效性检验 当我们得到挖掘的因子之后，需要对因子进行有效性检验，常见的检验指标有信息系数（Information Coefficient，IC）和信息比率（Information Ratio，IR）。\n信息系数（IC）描述的是所选股票下期收益率和本期因子暴露度的线性相关程度，可以反应该因子进行收益率预测的稳健性 **信息比率（IR）**是超额收益的均值与超额收益的标准差之比，信息比率与夏普比率类似，主要区别在于夏普比率使用无风险收益作为基准，而信息比率使用风险指数作为基准。夏普比率有助于确定投资组合的绝对收益，信息比率有助于确定投资组合的相对收益。当我们计算了 IC 之后，可以根据 IC 的值再对 IR 进行计算。当 IR 大于 0.5 时，因子稳定获取超额收益能力较强。 实际计算中，因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 值一般是指所选股票第 \\(T\\) 期的因子 \\(k\\) 上的暴露度 \\( x_k^T \\) 与所选股票第 \\(T+1\\) 期的收益率 \\( r^{T+1} \\) 的相关系数；因子 \\(k\\) 的 \\( \\mathrm{IR} \\) 值为因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 的均值除以因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 的标准差，计算公式如下：\n$$ \\begin{gathered} I C=\\rho_{x_{k}^{T}, r^{T+1}}=\\frac{\\operatorname{cov}\\left(x_{k}^{T}, r^{T+1}\\right)}{\\sigma_{x_{k}^{T}} \\sigma_{r^{T+1}}}=\\frac{\\mathrm{E}\\left(x_{k}^{T} * r^{T+1}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right) \\mathrm{E}\\left(r^{T+1}\\right)}{\\sqrt{\\mathrm{E}\\left(\\left(x_{k}^{T}\\right)^{2}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right)^{2}} \\cdot \\sqrt{\\mathrm{E}\\left(\\left(r^{T+1}\\right)^{2}\\right)-\\mathrm{E}\\left(r^{T+1}\\right)^{2}}} \\\\ I R=\\frac{\\overline{I C}}{\\sigma_{I C}} \\end{gathered} $$其中：\n$x_{k}^{T}$ ：所选股票第 $T$ 期的因子 $k$ 上的暴露度 $r^{T+1}$ ：所选股票第 $T+1$ 期的收益率 $\\overline{I C}: I C$ 的均值 本文采用 IR 判断因子好坏，通过对大量不同的算子和基础数据的组合以及 IC 和 IR 的“篮选”，文章得到了本文所选用的 50 个价量因子。经过 IR 检测，按 IR 由高到低排序得到如下图所示的表格。从下表中我们可以看出来所选的 50 个价量因子的 IR 都大于 0.5 ，说明这些因子稳定获取超额收益能力较强。\n4.2.11 因子 IR 检验表 因子名 IR 因子名 IR 0 3.11 25 2.73 1 2.95 26 2.71 2 2.95 27 2.70 3 2.95 28 2.69 4 2.95 29 2.69 5 2.94 30 2.69 6 2.94 31 2.68 7 2.94 32 2.68 8 2.93 33 2.68 9 2.93 34 2.68 10 2.93 35 2.67 11 2.92 36 2.67 12 2.91 37 2.66 13 2.89 38 2.65 14 2.86 39 2.65 15 2.83 40 2.65 16 2.83 41 2.65 17 2.83 42 2.64 18 2.79 43 2.63 19 2.78 44 2.63 20 2.78 45 2.62 21 2.76 46 2.62 22 2.75 47 2.62 从该表可见，在所筛选的因子中，所有因子的 IR 均大于 0.5，具有较强且稳定的获取超额收益的能力。\n4.3 因子清洗 4.3.1 因子缺失值处理和去极值 对于因子的缺失值处理的方法有个案剔除法，均值替换法，回归替换法等方法。本文采用较为简单的均值替换法对缺损值进行处理，即利用因子的平均值来替代缺失的数据。对于因子去极值有中位数去极值，百分比去极值和 $3 \\sigma$ 去极值等方法。本文采用的是 $3 \\sigma$ 去极值法，该方法是利用统计学上的 $3 \\sigma$ 原则，将把离该因子均值三个标准差以上的极值因子转化到刚好离均值三个标准差的位置，具体计算公式如下：\n$$ X_i^{\\prime}= \\begin{cases} \\bar{X}+3 \\sigma \u0026 \\text{if } X_i \u003e \\bar{X} + 3 \\sigma \\\\ \\bar{X}-3 \\sigma \u0026 \\text{if } X_i \u003c \\bar{X} - 3 \\sigma \\\\ X_i \u0026 \\text{if } \\bar{X} - 3 \\sigma \u003c X_i \u003c \\bar{X} + 3 \\sigma \\end{cases} $$其中：\n$X_{i}$ ：因子处理之前的值 $\\bar{X}$ ：因子序列的均值 $\\sigma$ ：因子序列的标准差 $X_{i}^{\\prime}$ ：去极值后的因子的值 4.3.2 因子的标准化 本文实验选取了多个因子，并且各个因子量纲并不完全相同，为了我们方便进行比较和回归，我们还需对因子进行标准化处理。目前常用的具体的标准化方法有 Min－Max标准化，Z－score 标准化和 Decimal scaling 小数定标标准化等。本文选择 Z－score 标准化的方法。通过原始数据的均值和标准差进行数据的标准化，经过处理的数据符合标准正态分布，即均值为 0 ，标准差为 1 ，其标准化后的数值大小有正有负，得到标准正态分布曲线。\n本文采用 Z－score 标准化公式如下：\n$$ \\tilde{x}=\\frac{x_{i}-u}{\\sigma} $$其中：\n$x_{i}$ ：因子的原值 $u$ ：因子序列的均值 $\\sigma$ ：因子序列的标准差 $\\tilde{x}$ ：标准化后的因子值 4.3.3 因子的中性化 因子中性化是为了剔除其他因素对我们所选因子的影响，使我们构建量化投资策略组合所选择的股票更加分散，而不是集中在特定的行业或者市值的股票上，可以更好地分担投资组合的风险和解决因子多重共线性的问题。市值和行业是影响股票收益最主要的两种自变量，所以在进行因子清洗的过程中，还必须考虑市值和行业的影响。本文实证中我们采用仅纳入行业因子，而将市场因子包含在行业因子中的方法。针对因子的单因子回归模型见公式(31)，我们将以下回归模型的残差项作为因子中性化后的新的因子值。\n$$ \\tilde{r}_{j}^{t}=\\sum_{s=1}^{s} X_{j s}^{t} \\tilde{f}_{s}^{t}+X_{j k}^{t} \\tilde{f}_{k}^{t}+\\tilde{u}_{j}^{t} $$其中：\n$\\tilde{r}_{j}^{t}$ ：股票 $j$ 在第 $t$ 期的收益率 $X_{j s}^{t}$ ：股票 $j$ 在第 $t$ 期在行业 $s$ 上的暴露度 $\\tilde{f}_{s}^{t}$ ：行业在第 $t$ 期的收益率 $X_{j k}^{t}$ ：股票 $j$ 在第 $t$ 期在因子 $k$ 上的暴露度 $\\tilde{f}_{k}^{t}$ ：因子 $k$ 在第 $t$ 期的收益率 $\\tilde{u}_j^t$ ：一个 $0-1$ 哑变量，即如果股票 $j$ 属于行业 $s$ ，则暴露度为 1 ，否则为 0 在本文中，并不会对公司所属行业进行按照比例拆分，即股票 $j$ 只能属于一个特定的行业 $s$ ，在行业 $s$ 上的暴露度为 1 ，在其他所有行业的暴露度为 0 。本文使用申万宏源行业分类标准，具体分类情况依次为农林牧渔，采掘，化工，钢铁，有色金属，电子元器件，家用电器，食品饮料，纺织服装，轻工制造，医药生物，公用事业，交通运输，房地产，商业贸易，餐饮旅游，综合，建筑材料，建筑装饰，电器设备，国防军工，计算机，传媒，通信，银行，非银金融，汽车和机械设备这 28 类。下表为 2021 年 2 月 5 日的申万指数一级行业历史行情图。\n4.3.3.1 2021年2月5日的申万指数一级行业历史行情图 指数代码 指数名称 发布日期 开盘指数 最高指数 最低指数 收盘指数 成交量(亿殴) 成交额(亿元) 涨跌幅(%) 801010 农林牧渔 2021/2/5 0:00 4111.43 4271.09 4072.53 4081.81 15.81 307.82 -0.3 801020 采掘 2021/2/5 0:00 2344.62 2357.33 2288.97 2289.41 18.06 115.6 -2.25 801030 化工 2021/2/5 0:00 4087.77 4097.59 3910.67 3910.67 55.78 778.85 -3.95 801040 钢铁 2021/2/5 0:00 2253.78 2268.17 2243.48 2250.81 11.61 48.39 -1.02 801050 有色金属 2021/2/5 0:00 4212.1 4250.59 4035.99 4036.74 45.41 593.92 -4.43 801080 电子元器件 2021/2/5 0:00 4694.8 4694.8 4561.95 4561.95 52.67 850.79 -2.78 801110 家用电器 2021/2/5 0:00 10033.82 10171.26 9968.93 10096.83 8.55 149.18 0.83 801120 食品饮料 2021/2/5 0:00 30876.33 31545.02 30649.57 30931.69 11.32 657.11 0.47 801130 纺织服装 2021/2/5 0:00 1614.48 1633.89 1604.68 1607.63 6.28 57.47 -0.39 801140 轻工制造 2021/2/5 0:00 2782.07 2791.88 2735.48 2737.24 15.28 176.16 -1.35 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 数据来源：申银万国\n下表是原始的因子部分数据。依次经过上述因子缺失值填充、因子去极值、因子标准化和因子中性化这 4 个因子清洗的步骤后，得到如表展示的经过因子清洗后的因子部分数据。\n4.3.3.2 原始的因子数据 trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 4.3.3.3 清洗后的因子数据 sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.4 基于 LightGBM 的因子选择 4.4.1 GBDT Friedman（2001）$^{[20]}$ 提出的梯度提升决策树（Gradient Boosting Decision Tree，GBDT）是一种基于迭代的回归型决策树。其主要思想是通过逐步添加弱分类器（通常是决策树）来优化模型，使得整体模型能够最小化损失函数。GBDT 的模型可以表示为：\n$$ \\hat{y} = \\sum_{m=1}^{M} \\gamma_m h_m(\\mathbf{x}) $$其中：\n\\( M \\) 是迭代次数， \\( \\gamma_m \\) 是第 \\( m \\) 个弱分类器的权重， \\( h_m(\\mathbf{x}) \\) 是第 \\( m \\) 个决策树模型。 GBDT 的训练过程通过逐步拟合负梯度方向来最小化损失函数，具体更新公式为：\n$$ \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{N} L\\left(y_i, \\hat{y}_{i}^{(m-1)} + \\gamma h_m(\\mathbf{x}_i)\\right) $$其中，\\( L \\) 是损失函数，\\( y_i \\) 是真实值，\\( \\hat{y}_{i}^{(m-1)} \\) 是第 \\( m-1 \\) 次迭代后的预测值。\n4.4.2 LightGBM 轻量级梯度提升机（Light Gradient Boosting Machine，LightGBM)$^{[21]}$ 是一个高效实现 GBDT 算法的框架，最初由 Microsoft 开发，作为一个免费开源的分布式梯度提升框架。LightGBM 基于决策树算法，广泛应用于排名、分类及其他机器学习任务，开发重点在于性能和可伸缩性。其主要优势包括高效率的并行训练、更快的训练速度、更低的内存消耗、更好的准确率，以及支持分布式计算和快速处理海量数据$^{[22]}$。\nLightGBM 的核心算法基于以下优化目标：\n$$ L = \\sum_{i=1}^{N} l(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(h_m) $$其中，\\( l \\) 是损失函数，\\( \\Omega \\) 是正则化项，用于控制模型复杂度，通常表示为：\n$$ \\Omega(h_m) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 $$这里，\\( T \\) 是树的叶子数，\\( w_j \\) 是第 \\( j \\) 个叶子的权重，\\( \\gamma \\) 和 \\( \\lambda \\) 是正则化参数。\nLightGBM 采用基于梯度的单边采样（Gradient-based One-Side Sampling，GOSS）和互斥特征捆绑（Exclusive Feature Bundling，EFB）等技术，显著提升了训练效率和模型性能。\n在本研究中，训练过程中使用的损失函数为均方误差（Mean Squared Error，MSE），其定义为：\n$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$其中，\\( y \\) 为真实收益率，\\( \\hat{y} \\) 为模型预测的收益率，\\( N \\) 为样本数量。\n4.4.3 算法流程 本小节算法的具体流程如下：\n数据准备：使用一年的每只股票的 50 个因子数据（A 股全市场数据）和历史未来一个月的收益率作为特征。\n模型训练：利用网格搜索法（Grid Search）优化 LightGBM 模型的超参数，训练模型以预测未来一个月的收益率。模型训练流程如图4.12所示。\n$$ \\text{参数优化：} \\quad \\theta^* = \\arg\\min_\\theta \\sum_{i=1}^{N} L(y_i, \\hat{y}_i(\\theta)) $$其中，\\( \\theta \\) 表示模型的超参数集合，\\( \\theta^* \\) 为最优参数。\n因子重要性计算：使用 LightGBM 的 feature_importances_ 方法计算各因子的特征重要性。特征重要性主要通过两个指标衡量：\nSplit：该特征在所有树中被用于分裂的次数。 Gain：该特征在所有分裂中带来的总增益（即损失函数的减少量）。 因子的特征重要性可以表示为：\n$$ \\text{Importance}_{\\text{split}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\mathbb{I}(f \\text{ 被用于第 } j \\text{ 个叶节点的分裂}) $$$$ \\text{Importance}_{\\text{gain}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\Delta L_{m,j} \\cdot \\mathbb{I}(f \\text{ 被用于第 } j \\text{ 个叶节点的分裂}) $$其中，\\( \\mathbb{I} \\) 是指示函数，\\( \\Delta L_{m,j} \\) 是因子 \\( f \\) 在第 \\( m \\) 棵树的第 \\( j \\) 个分裂中带来的损失减少量。\n因子筛选：根据模型计算的因子重要性进行排序，选择前十个重要性最高的因子作为本横截面分析中使用的因子。所选因子的重要性如表4.4.4所示。\n4.4.4 部分所选因子重要性排序 importance feature_name trade_date 35 factor_35 2010-08-11 27 factor_27 2010-08-11 33 factor_33 2010-08-11 20 factor_20 2010-08-11 24 factor_24 2010-08-11 45 factor_45 2010-08-11 37 factor_37 2010-08-11 49 factor_49 2010-08-11 19 factor_19 2010-08-11 47 factor_47 2010-08-11 22 factor_22 2010-09-09 20 factor_20 2010-09-09 30 factor_30 2010-09-09 24 factor_24 2010-09-09 4.4.5 代码实现片段 以下是训练过程所使用的部分代码，用于因子选择。\nfeature_choice def feature_choice( self, days=21, is_local=False ): if is_local: feature_info = pd.read_hdf(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;)) else: factors = self.get_env().query_data(Factors_Data) factors = factors[ factors[COM_DATE] \u0026gt;= \u0026#39;2010-01-01\u0026#39; ] trade_list = list(set(factors[COM_DATE])) trade_list.sort() if len(trade_list) % days == 0: n = int(len(trade_list) / days) - 7 else: n = int(len(trade_list) / days) - 6 feature_info = pd.DataFrame() begin_index = 147 feature = list(factors.columns) feature.remove(COM_SEC) feature.remove(COM_DATE) feature.remove(Ret) for i in range(n): end_date = days * i + begin_index - 21 begin_date = days * i trade_date = days * i + begin_index print(trade_list[trade_date]) train_data = factors[ (factors[COM_DATE] \u0026lt;= trade_list[end_date]) \u0026amp; (factors[COM_DATE] \u0026gt;= trade_list[begin_date]) ] model = lgb.LGBMRegressor() model.fit(train_data[feature], train_data[Ret]) feature_info_cell = pd.DataFrame(columns=Info_Fields) feature_info_cell[Importance] = model.feature_importances_ feature_info_cell[Feature_Name] = model.feature_name_ feature_info_cell = feature_info_cell.sort_values(by=Importance).tail(10) feature_info_cell[COM_DATE] = trade_list[trade_date] feature_info = pd.concat( [feature_info, feature_info_cell], axis=0 ) h = pd.HDFStore(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;), \u0026#39;w\u0026#39;) h[\u0026#39;data\u0026#39;] = feature_info h.close() self.get_env().add_data(feature_info, Feature_Info) pass 通过上述流程，利用 LightGBM 高效地筛选出对预测未来收益率最具影响力的因子，从而提升模型的预测能力和解释性。\n4.5 基于 BiLSTM 的因子组合 本小节使用 BiLSTM 进行因子组合。BiLSTM 的具体原理在第二章已经介绍了，这里不再赘述。下面先介绍一下使用模型的具体网络结构，本文通过大量反复试验最终 BiLSTM 设置的网络结构如表4.5.1所示。其中层与层之间使用循环神经网络默认的 tanh 和 linear 作为激活函数。并且为了防止过拟合加入 Dropout，但是如果 Dropout 使用过大的丢弃比例会出现欠拟合的现象，因此 Dropout 的丢弃比例取值为 0.01。最终模型的 BiLSTM 循环层的神经元个数为 100，采用一层 BiLSTM 层和三层全连接层，其中 BiLSTM 层和第一个全连接层之间设置了一个 Dropout。\n4.5.1 BiLSTM的网络结构 Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 100) 24400 dropout_1 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 256) 25856 dropout_2 (Dropout) (None, 256) 0 dense_2 (Dense) (None, 64) 16448 dense_3 (Dense) (None, 1) 0 Total params：66,769\nTrainable params：66,769\nNon-trainable params：0\n因为本次实验使用数据的数据量较大，因此选用 epochs=400，batch_size=1024。模型的损失函数采用均方误差（Mean Square Error，MSE）。其中优化器采用随机梯度下降（Stochastic Gradient Descent，SGD）。随机梯度下降相对于梯度下降（Gradient Descent，GD）有在信息冗余的情况下更能有效地利用信息，前期迭代效果卓越，适合处理大样本的数据这三个优势 $^{[23]}$。由于本实验训练数据量较大，使用 SGD 的话每次仅用一个样本来迭代，训练的速度很快，可以大大减少我们训练所花费的时间。使用其 keras 包中的默认值，即 lr=0.01、momentum=0.0、decay=0.0 和 nesterov=False。\n参数解释：\nlr：学习率 momentum：动量参数 decay：每次更新后的学习率衰减值 nesterov：确定是否使用 Nesterov 动量 4.5.2 算法流程 本小节算法的具体流程如下：\n使用一年的每只股票的 10 个因子（LightGBM 选出来的因子）和历史未来一个月的收益率的 A 股全市场数据作为特征。 以一年每支股票未来一个月的收益率为预测目标，利用 BiLSTM 进行训练，如 Fig. 12 所示。 Fig. 12. Rolling Window\n一个月的样本外数据的实时因子数据通过训练好的 BiLSTM 模型，得到实时的未来一月的每只股票预期收益率。收益率如表4.11所示。 4.5.3 部分股票预测收益率表 sec_code trade_date y_hat 000001.SZ 2011/5/26 0.0424621 000002.SZ 2011/5/26 -0.1632174 000004.SZ 2011/5/26 -0.0642319 000005.SZ 2011/5/26 0.08154649 000006.SZ 2011/5/26 0.00093213 000007.SZ 2011/5/26 -0.073218 000008.SZ 2011/5/26 -0.0464256 000009.SZ 2011/5/26 -0.091549 000010.SZ 2011/5/26 0.08154649 000011.SZ 2011/5/26 -0.1219943 000012.SZ 2011/5/26 -0.1448984 000014.SZ 2011/5/26 0.09038845 000016.SZ 2011/5/26 -0.11225 4.5.4 代码实现片段 以下是训练过程所使用的部分代码，用于构建BiLSTM训练网络。\nbuild_net_blstm def build_net_blstm(self): model = ks.Sequential() model.add( ks.layers.Bidirectional(ks.layers.LSTM( 50 ),input_shape=(11,10)) ) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(256)) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(64)) model.add(ks.layers.Dense(1)) model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;mse\u0026#39;) model.summary() self.set_model(model) 4.6 量化策略和策略回测 4.6.1 回测指标 下面先对策略的一些常见回测指标进行介绍。评价指标包括累计收益率（Total Rate of Return）、年化收益率（Annualized Rate of Return）、年化波动率（Annualized volatility）、夏普比率（Sharpe ratio）、最大回撤率 (Maximum Drawdown，MDD)、年化换手率（Annualized turnover rate）和年化交易成本率（Annualized transaction cost rate），其中假定一年股市开盘252天，无风险利率默认为0.035，手续费默认0.002。\n累计收益率（Total Rate of Return）：在其他指标相同的情况下，累计收益率越大说明该策略越好，越能带来更大的收益。公式如下： $$ \\text{Total Rate of Return} = r_{p} = \\frac{P_{1} - P_{0}}{P_{0}} $$$P_{1}$ ：最终股票和现金的总价值\n$P_{0}$ ：初始股票和现金的总价值\n年化收益率（Annualized Rate of Return）：是将累计总收益率换算成以年为单位的几何平均收益率。在其他指标相同的情况下，年化收益率越大，代表该策略越好。公式如下： $$ \\text{Annualized Rate of Return} = R_{p} = \\left(1 + r_{p}\\right)^{\\frac{252}{t}} - 1 $$$r_{p}$ ：累计收益率\n$t$ ：投资策略执行的天数\n年化波动率（Annualized volatility）：定义为对象资产的年回报率的对数值的标准差。年化波动率用来衡量策略的风险性，波动率越大代表策略的风险越高。公式如下： $$ \\begin{aligned} \\text{Annualized volatility} = \\sigma_{p} \u0026= \\sqrt{\\frac{252}{t-1} \\sum_{i=1}^{t}\\left(r_{d} - \\bar{r}_{d}\\right)^{2}} \\\\ \\bar{r}_{d} \u0026= \\frac{1}{t} \\sum_{i=1}^{t} r_{d_{i}} \\end{aligned} $$$r_{d_{i}}$ ：第 $i$ 天日收益率\n$\\bar{r}_{d}$ ：日平均收益率\n$t$ ：投资策略执行的天数\n夏普比率（Sharpe ratio）：是由 Sharpe（1966）$^{[24]}$ 提出的。它代表投资者额外承受一单位风险，所获得的超额收益$^{[25]}$。这里给出年化的夏普比率计算公式： $$ S = \\frac{R_{p} - R_{f}}{\\sigma_{p}} $$$R_{p}$ ：年化收益率\n$R_{f}$ ：无风险收益率\n$\\sigma_{p}$ ：年化波动率\n最大回撤率 (Maximum Drawdown，MDD)：表示表示我们策略运行期间股票和现金的总价值走到最低点时的收益率回撤幅度的最大值。最大回测率用来策略最极端可能的亏损情况。 $$ MDD = \\frac{\\max \\left(V_{x} - V_{y}\\right)}{V_{x}} $$$V_{x}$ 和 $V_{y}$ 分别为策略组合在第 $x$ 天和第 $y$ 天的股票和现金的总价值，且 $x \u0026lt; y$。\n年化换手率（Annualized turnover rate）：用来衡量对投资组合里面的股票买卖的频繁程度。越大说明改投资组合换仓越频繁，交易成本也会越大。 $$ \\text{change} = \\frac{N \\times 252}{t} $$$t$ ：投资策略执行的天数\n$N$ ：总共买进和卖出的次数\n年化交易成本率（Annualized transaction cost rate）：用来衡量投资组合策略的交易成本，越大说明交易成本越高。 $$ c = \\left(1 + \\text{commison}\\right)^{\\text{change}} - 1 $$change：年化换手率\ncommison：手续费\n4.6.2 策略及回测结果 本文量化交易策略采用每隔一个月进行换仓（即调仓周期为28个交易日），每次换仓采取等额持股的方式买入 BiLSTM 预测出的预期收益率最高的25支股票，卖出原本所持有的股票。本文的回测时间和规则如下：\n回测时间：从 2012 年 1 月到 2020 年 10 月。 回测股票池：全 A 股，剔除特别处理（Special treatment，ST）股票。 交易手续费：买入时支付给券商交易佣金千分之二，卖出时支付给券商交易佣金千分之二，其中单笔交易佣金不满5元券商按5元收取交易佣金。 买卖规则：当天开盘涨停股票不能买入，跌停股票不能卖出。 4.6.2.1 策略回测结果 累计收益率 年化收益率 年化波动率 夏普比率 最大回撤 年化换手率 年化交易成本率 策略 701.00% 29.18% 33.44% 0.77 51.10% 51.10% 11.35% 基准 110.40% 9.70% 26.01% 0.24 58.49% 58.49% 0.00% Fig. 22. Net Profit Curve\n回测结果如上表和 Fig.22 所示。我的策略采用的是本章所介绍的 LightGBM-BiLSTM 量化策略。基准采用的中证全指（000985）。由上面的结果可以看到，本策略累计收益率为701.00%，远高于基准110.40%；年化收益率为29.18%，远高于基准9.70%；夏普率为0.77，高于基准0.24。这三项回测指标说明 LightGBM-BiLSTM 量化策略确实能够给投资者带来更大的收益。本策略年化波动率为33.44%大于基准26.01%，最大回撤为51.10%小于基准58.49%，这两项回测指标说明 LightGBM-BiLSTM 量化策略存在一定的风险，特别是很难抵御系统性风险的冲击。年化换手率为11.35%，年化交易成本率为2.29%，说明我们策略不是高频交易策略，交易成本较小。从收益曲线图可以看出 LightGBM-BiLSTM 量化策略在前两年的收益率和基准相差不大，并没有特别的优势。但从2015年4月左右开始 LightGBM-BiLSTM 量化策略的收益率明显好于基准的收益率。总体而言，该 LightGBM-BiLSTM 量化策略的收益率十分可观，但仍然存在一定的风险。\n第五章 总结与展望 5.1 总结 本文首先介绍了基于深度学习的股票价格预测和量化策略研究的研究背景和研究意义，然后分别介绍了股票价格预测和量化投资策略国内外的研究现状，之后说明了本文的创新点和研究框架。接着本论文在相关理论基础章节大致介绍了本文用到的深度学习模型和量化投资的发展历程。重点介绍了 LSTM，GRU，BiLSTM 这三个模型的基本结构，基本原理和特点。\n随后，本文利用浦发银行和 IBM 的日频数据，通过一系列的数据处理过程和特征提取来对数据进行预处理。然后介绍了 LSTM，GRU，BiLSTM 这三个模型的具体网络结构以及超参数的设定。紧接着我们使用 LSTM，GRU，BiLSTM 分别进行两只股票收盘价的预测和模型评估比较。实验结果表明对于两只股票而言都是 BiLSTM 预测效果更加准确。\n最后，本论文为了进一步说明 BiLSTM 在金融上的运用价值，构建了基于 LightGBM－BiLSTM 的量化投资模型。选取 A 股全市场的股票和多个因子依次进行因子清洗，基于 LightGBM 的因子选择和基于 LSTM 的因子组合等过程。接着，我们构建一定的投资策略并通过累计收益率，年化收益率，年化波动率和夏普比率等评估指标与基准的持有中证全指进行对比。通过对比发现 LightGBM－BiLSTM 量化投资模型能带来更好的收益，说明了利用深度学习构建量化投资策略的有效性。\n5.2 展望 本文虽然分别对比 LSTM，GRU，BiLSTM 这三个模型预测股票收盘价的效果和基于 LightGBM－BiLSTM 量化投资策略取得了一定的成果，但本文研究仍有一些不足之处。结合本文的研究成果，可以进一步进行以下研究和改进：\n预测目标多样化：本文在预测股票价格方面，选取的股票收盘价作为预测目标，虽然这一结果最直观，但 Bachelier（1900）$^{[26]}$ 提出的随机游走假说（Random Walk Hypothesis，RWH）认为股票的价格服从随机漫步，是不可预测的。虽然之后有许多行为经济学家证明这一观点不完全正确，但这也同时说明单纯预测股票的收盘价难度和可解释性不那么强 $^{[27][28]}$。因此可以选择股票波动率预测，股票涨跌判断和股票收益率预测等作为未来的研究的方向。 模型多样化对比：本文在预测股票价格方面，对比了 LSTM，GRU 和 BiLSTM 这三种循环神经网络模型并且说明了 BiLSTM 预测效果比较好，但仍然缺少和其他更多不同模型的对比研究。因此未来可以深入研究与 Autoregressive Integrated Moving Average (ARIMA)，卷积神经网络（Convolutional Neural Networks，CNN），深度神经网络（Deep Neural Networks，DNN）, CNN－LSTM, Transformer 和 TimeGPT 等单一或复合模型之间的对比。 因子多样化：本文在构建量化投资策略方面使用的因子都是技术面的价量因子，因子的种类单一。未来可以选择财务因子，情绪因子，成长因子等不同种类的因子，从而提高策略的性能。同时未来研究还可以适当的加入择时策略，在预测大盘上涨时增加仓位，在预测大盘下跌时减少仓位，赚取贝塔（beta，$\\beta$）的钱。 投资组合优化：本文的因子组合过程仍然不完善，未来可以利用二次规划的方法对投资组合进行优化。 高频交易策略研究：本文的量化投资策略方法采取的是低频交易的策略，未来可以利用股票的 tick 数据来研究高频策略和超高频策略。 参考文献 [1] White, H. “Economic prediction using neural networks: The case of IBM daily stock returns.” Proc. of ICNN. 1988, 2: 451-458.\n[2] Kimoto, T., Asakawa, K., Yoda, M., et al. “Stock market prediction system with modular neural networks.” Proc. of 1990 IJCNN International Joint Conference on Neural Networks. IEEE, 1990: 1-6.\n[3] Zhang, G. P. “Time series forecasting using a hybrid ARIMA and neural network model.” Neurocomputing. 2003, 50: 159-175.\n[4] Akita, R., Yoshihara, A., Matsubara, T., et al. “Deep learning for stock prediction using numerical and textual information.” Proc. of 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS). IEEE, 2016: 1-6.\n[5] 宮崎邦洋, 松尾豊. “Deep Learning を用いた株価予測の分析.” 人工知能学会全国大会論文集 第31回全国大会. 一般社団法人 人工知能学会, 2017: 2D3OS19a3-2D3OS19a3.\n[6] Kim, T., Kim, H. Y. “Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data.” PLoS ONE. 2019, 14(2): e0212320.\n[7] Hochreiter, S., Schmidhuber, J. “Long short-term memory.” Neural Computation. 1997, 9(8): 1735-1780.\n[8] Cho, K., Van Merriënboer, B., Gulcehre, C., et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078. 2014.\n[9] Chung, J., Gulcehre, C., Cho, K. H., et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” arXiv preprint arXiv:1412.3555. 2014.\n[10] Gruber, N., Jockisch, A. “Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?” Frontiers in Artificial Intelligence. 2020, 3(40): 1-6.\n[11] Markowitz, H. “Portfolio Selection.” The Journal of Finance. 1952, 7(1): 77-91. doi:10.2307/2975974.\n[12] Merton, R. C. “An analytic derivation of the efficient portfolio frontier.” Journal of Financial and Quantitative Analysis. 1972: 1851-1872.\n[13] Sharpe, W. F. “Capital asset prices: A theory of market equilibrium under conditions of risk.” The Journal of Finance. 1964, 19(3): 425-442.\n[14] Lintner, J. “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.” Review of Economics and Statistics. 1965, 47(1): 13-37.\n[15] Mossin, J. “Equilibrium in a capital asset market.” Econometrica: Journal of the Econometric Society. 1966: 768-783.\n[16] Ross, S. A. “The arbitrage theory of capital asset pricing.” Journal of Economic Theory. 1976, 13(3): 341-60.\n[17] Fama, E. F., French, K. R. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics. 1993, 33(1): 3-56.\n[18] Fama, E. F., French, K. R. “A five-factor asset pricing model.” Journal of Financial Economics. 2015, 116(1): 1-22.\n[19] Kingma, D. P., Ba, J. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980. 2014.\n[20] Friedman, J. H. “Greedy function approximation: A gradient boosting machine.” Annals of Statistics. 2001: 1189-1232.\n[21] Kopitar, L., Kocbek, P., Cilar, L., et al. “Early detection of type 2 diabetes mellitus using machine learning-based prediction models.” Scientific Reports. 2020, 10(1): 1-12.\n[22] Ke, G., Meng, Q., Finley, T., et al. “Lightgbm: A highly efficient gradient boosting decision tree.” Advances in Neural Information Processing Systems. 2017, 30: 3146-3154.\n[23] Bottou, L., Curtis, F. E., Nocedal, J. “Optimization methods for large-scale machine learning.” SIAM Review. 2018, 60(2): 223-311.\n[24] Sharpe, W. F. “Mutual fund performance.” The Journal of Business. 1966, 39(1): 119-138.\n[25] Sharpe, W. F. “The sharpe ratio.” Journal of Portfolio Management. 1994, 21(1): 49-58.\n[26] Bachelier, L. “Théorie de la spéculation.” Annales Scientifiques de l\u0026rsquo;École Normale Supérieure. 1900, 17: 21-86.\n[27] Fromlet, H. “Behavioral finance-theory and practical application: Systematic analysis of departures from the homo oeconomicus paradigm are essential for realistic financial research and analysis.” Business Economics. 2001: 63-69.\n[28] Lo, A. W. “The adaptive markets hypothesis.” The Journal of Portfolio Management. 2004, 30(5): 15-29.\n参考博客 Colah\u0026rsquo;s Blog. (2015, August 27). Understanding LSTM Networks. 引用 引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Apr 2021). 基于深度学习的股票价格预测和量化策略.\nhttps://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\nOr\n@article{syhya2021stockprediction, title = \u0026#34;基于深度学习的股票价格预测和量化策略\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2021\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2021-04-21-deep-learning-stock-prediction/","summary":"\u003ch2 id=\"摘要\"\u003e摘要\u003c/h2\u003e\n\u003cp\u003e股票市场是金融市场的重要组成部分，近些年来，股票市场蓬勃发展，股票价格预测和量化投资策略研究吸引了许多领域的研究学者。其中最近几年随着人工智能和机器学习的发展，学者们从传统的统计学模型迁移到了人工智能算法，尤其是在深度学习热潮掀起后，神经网络在股票价格预测和量化投资策略研究中取得了不错的效果。深度学习的目标是学习多层次的特征，通过组合低级特征构建抽象的高级特征，从而挖掘数据的分布式特征表示，基于此进行复杂的非线性建模，从而实现预测任务。其中 RNN 被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价，交易信息都是序列数据，因此之前有很多研究者，基于 RNN 来预测股票价格。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题，之后出现了诸如 GRU，Peephole LSTM，BiLSTM 等 LSTM 的变体。但传统的股票预测模型有些并未考虑时间因素，有些仅考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。\u003c/p\u003e","title":"基于深度学习的股票价格预测和量化策略"}]