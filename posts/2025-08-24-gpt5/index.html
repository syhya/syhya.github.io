<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>gpt-oss & GPT-5 | Yue Shui Blog</title><meta name=keywords content="gpt-oss,GPT-5,MoE,Reasoning,Tool Use,OpenAI,LLM Architecture"><meta name=description content="In August 2025, the AI field witnessed a period of intensive releases from OpenAI. Following GPT-2 (OpenAI, 2019) in 2019, OpenAI has once again contributed to the open-source community with its first open-weight large language model series, gpt-oss (OpenAI, 2025), available in 120B and 20B sizes. Shortly after, the highly anticipated next-generation flagship model, GPT-5 (OpenAI, 2025), was also officially launched. This series of releases not only marks a new high for open-source models in reasoning and agent capabilities but also reveals OpenAI&rsquo;s latest advancements in model architecture, training methodologies, and safety alignment."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-08-24-gpt5/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://syhya.github.io/apple-touch-icon.png><link rel=mask-icon href=https://syhya.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-08-24-gpt5/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-08-24-gpt5/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-08-24-gpt5/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="gpt-oss & GPT-5"><meta property="og:description" content="In August 2025, the AI field witnessed a period of intensive releases from OpenAI. Following GPT-2 (OpenAI, 2019) in 2019, OpenAI has once again contributed to the open-source community with its first open-weight large language model series, gpt-oss (OpenAI, 2025), available in 120B and 20B sizes. Shortly after, the highly anticipated next-generation flagship model, GPT-5 (OpenAI, 2025), was also officially launched. This series of releases not only marks a new high for open-source models in reasoning and agent capabilities but also reveals OpenAI‚Äôs latest advancements in model architecture, training methodologies, and safety alignment."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-24T12:00:00+08:00"><meta property="article:modified_time" content="2025-08-24T12:00:00+08:00"><meta property="article:tag" content="Gpt-Oss"><meta property="article:tag" content="GPT-5"><meta property="article:tag" content="MoE"><meta property="article:tag" content="Reasoning"><meta property="article:tag" content="Tool Use"><meta property="article:tag" content="OpenAI"><meta name=twitter:card content="summary"><meta name=twitter:title content="gpt-oss & GPT-5"><meta name=twitter:description content="In August 2025, the AI field witnessed a period of intensive releases from OpenAI. Following GPT-2 (OpenAI, 2019) in 2019, OpenAI has once again contributed to the open-source community with its first open-weight large language model series, gpt-oss (OpenAI, 2025), available in 120B and 20B sizes. Shortly after, the highly anticipated next-generation flagship model, GPT-5 (OpenAI, 2025), was also officially launched. This series of releases not only marks a new high for open-source models in reasoning and agent capabilities but also reveals OpenAI&rsquo;s latest advancements in model architecture, training methodologies, and safety alignment."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"gpt-oss \u0026 GPT-5","item":"https://syhya.github.io/posts/2025-08-24-gpt5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"gpt-oss \u0026 GPT-5","name":"gpt-oss \u0026 GPT-5","description":"In August 2025, the AI field witnessed a period of intensive releases from OpenAI. Following GPT-2 (OpenAI, 2019) in 2019, OpenAI has once again contributed to the open-source community with its first open-weight large language model series, gpt-oss (OpenAI, 2025), available in 120B and 20B sizes. Shortly after, the highly anticipated next-generation flagship model, GPT-5 (OpenAI, 2025), was also officially launched. This series of releases not only marks a new high for open-source models in reasoning and agent capabilities but also reveals OpenAI\u0026rsquo;s latest advancements in model architecture, training methodologies, and safety alignment.\n","keywords":["gpt-oss","GPT-5","MoE","Reasoning","Tool Use","OpenAI","LLM Architecture"],"articleBody":"In August 2025, the AI field witnessed a period of intensive releases from OpenAI. Following GPT-2 (OpenAI, 2019) in 2019, OpenAI has once again contributed to the open-source community with its first open-weight large language model series, gpt-oss (OpenAI, 2025), available in 120B and 20B sizes. Shortly after, the highly anticipated next-generation flagship model, GPT-5 (OpenAI, 2025), was also officially launched. This series of releases not only marks a new high for open-source models in reasoning and agent capabilities but also reveals OpenAI‚Äôs latest advancements in model architecture, training methodologies, and safety alignment.\ngpt-oss gpt-oss (OpenAI, 2025) is OpenAI‚Äôs first open-weight language model series released since GPT-2, designed to provide the open-source community with powerful reasoning and tool-use capabilities. The series includes two versions, gpt-oss-120b and gpt-oss-20b, both released under the Apache 2.0 license.\nArchitecture Overview Fig. 1. A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B. (Image source: Raschka, 2025)\ngpt-oss is built upon the GPT series architecture and incorporates several mainstream technologies from recent years, including RMSNorm, SwiGLU, GQA, RoPE, YaRN, and MoE.\nThe table below provides a direct comparison of the differences between the GPT-OSS 20B and GPT-2 XL 1.5B models.\nFeature GPT-OSS 20B (2025) GPT-2 XL 1.5B (2019) Release Date 2025 2019 Model Size 20B parameters 1.5B parameters Active Parameters 3.5B (per inference) 1.5B (all activated) Vocabulary Size 200k tokens 50k tokens Embedding Dimension 2,880 1,600 Number of Transformer Layers 24 layers 48 layers Number of Attention Heads 64 25 Context Length 131k tokens 1,024 tokens Positional Encoding RoPE (Rotary Positional Embeddings) Absolute Positional Embeddings Attention Mechanism Grouped-Query Attention Multi-head Attention Feed-Forward Network SwiGLU activation + MoE GELU activation MoE Architecture 32 experts, 4 activated None Normalization Method RMSNorm (2 locations) LayerNorm (2 locations) Dropout None Yes Sliding Window Attention Used in every other layer(window of 128 tokens) None Training Features Includes Supervised Fine-Tuning + Reinforcement Learning Pre-training only Quantization Support MXFP4 (Can run on a single GPU) No special quantization License Apache 2.0 MIT Efficient Attention Mechanisms To maintain high efficiency while supporting a 128k long context, gpt-oss employs several advanced attention mechanisms.\nGrouped-Query Attention (GQA): gpt-oss has 64 query heads and 8 key/value heads, meaning every 8 query heads share a single K/V pair. This significantly reduces the size of the KV cache and memory bandwidth requirements, thereby substantially increasing inference throughput with almost no loss in model performance.\nSliding Window Attention: To further reduce computational complexity, gpt-oss draws inspiration from Longformer (Jiang et al., 2020) and Mistral (Jiang et al., 2023) by adopting a sliding window attention. Its Transformer layers alternate between Dense Attention and Locally Banded Sparse Attention. The latter is Sliding Window Attention, which limits the attention scope of each token to a fixed-size local window.\nFig. 2. Comparison between regular attention (left) and sliding window attention (right). (Image source: Jiang et al., 2023)\nIn gpt-oss, this window size is set to 128 tokens. This means that in a local attention layer, a token can only attend to the 128 tokens preceding it, not the entire context. This design reduces the computational complexity of attention from \\( O(L^2) \\) to \\( O(L \\cdot W) \\), where \\( L \\) is the sequence length and \\( W \\) is the window size. By alternating with full attention layers, the model can efficiently process local information while integrating global information through the full attention layers.\nAttention Sinks: The model introduces Attention Sinks (Xiao et al., 2023), which learns a bias \\( \\mathbf{s}_h \\) added to the attention scores. This allows initial tokens to be consistently attended to, helping to stabilize the attention distribution and prevent information loss in long-context scenarios. \\[ \\text{Attention}(Q, K, V)_h = \\text{softmax}\\left(\\frac{Q_h K_h^T}{\\sqrt{d_k}} + \\mathbf{s}_h\\right)V_h \\] Fig. 3. Illustration of StreamingLLM vs existing methods. (Image source: Xiao et al., 2023)\nThe figure above compares the performance and efficiency of StreamingLLM with three common long-text processing methods. Assume a language model is pre-trained on texts of length $L$ and needs to predict the $T$-th token during inference (where $T \\gg L$):\nDense Attention: Retains the key-values (KV) of all historical tokens and computes full attention. The time complexity is $O(T^2)$, and the cache size grows continuously. Performance drops significantly when the input length exceeds the pre-training length $L$. Window Attention: Caches only the KV of the most recent $L$ tokens. It is efficient for inference, but performance plummets once the information from early tokens is replaced. Sliding Window with Re-computation: Reconstructs the KV state from the most recent $L$ tokens each time a new token is generated. Although it performs well on long texts, the re-computation involves quadratic attention, leading to a high time complexity of $O(TL^2)$ and slow inference speed. This method combines attention sinks with recent tokens during computation, not only maintaining inference efficiency but also sustaining a stable attention distribution and low perplexity in long-text scenarios.\nMXFP4 Quantization Fig. 4. Faster MXFP4 Backpropagation via Stochastic Rounding and Hadamard Transform. (Image source: Tseng et al., 2025)\nTo enable the large model to run on consumer-grade hardware, gpt-oss uses the MXFP4 (Tseng et al., 2025) format to quantize the MoE weights. MXFP4 is a micro-scaling floating-point format that can effectively quantize weights to about 4.25 bits. Since MoE weights account for over 90% of the model‚Äôs total parameters, this method drastically compresses the model size, allowing the 120B model to fit into a single 80GB GPU and the 20B model to run on devices with 16GB of VRAM.\nTraining Pre-training: The model is pre-trained on a dataset of several trillion tokens of text, with a focus on STEM, coding, and general knowledge. To enhance safety, the pre-training data reuses the CBRN content filters from GPT-4o.\nPost-training (Reasoning \u0026 Tool Use): After pre-training, the model undergoes post-training using CoT RL techniques similar to those for OpenAI‚Äôs o3. The goal of this stage is to teach the model to:\nReason: Generate detailed Chain-of-Thought (CoT) to solve complex problems. Use Tools: Learn to call external tools (like web search, code execution) to enhance its capabilities. To achieve these advanced agent functionalities, OpenAI designed the Harmony Chat Format. This format introduces the concept of ‚Äúchannels‚Äù (e.g., analysis for CoT, commentary for tool calls, final for the final answer) and establishes a strict instruction hierarchy (System \u003e Developer \u003e User \u003e Assistant \u003e Tool) to ensure the model‚Äôs behavior is controllable.\nAdditionally, the model supports Variable Effort Reasoning. Users can set Reasoning: low/medium/high in the system prompt to trade off between latency and performance. A higher effort level generates a longer CoT, which typically leads to higher accuracy but also increased latency.\nFig. 5. Accuracy vs. average CoT length for different reasoning levels on AIME and GPQA benchmarks. (Image source: OpenAI, 2025)\nEvaluation Fig. 6. Main capabilities evaluations for gpt-oss series. (Image source: OpenAI, 2025)\nBenchmark results show that gpt-oss-120b‚Äôs accuracy surpasses that of OpenAI‚Äôs o3-mini and approaches o4-mini. Meanwhile, gpt-oss-20b, at only one-sixth the size, also demonstrates competitive performance.\nGPT-5 GPT-5 (OpenAI, 2025) is not a single model but a unified intelligent system. It is not a monolithic, massive model but a complex system of multiple specialized models and an intelligent routing mechanism working in concert to balance performance, speed, and cost.\nSystem Architecture Fig. 7. GPT-5 Unified System Architecture. (Image source: Latent Space, 2025)\nThe GPT-5 system consists of three core components:\ngpt-5-main: As the system‚Äôs default model, it is fast and efficient, handling the vast majority of user queries. It can be considered the successor to GPT-4o. gpt-5-thinking: Used for more complex problems that require deep thought. This model is activated when the user explicitly requests it (e.g., ‚Äúthink hard about this‚Äù) or when the system determines the task requires it. It can be seen as the successor to OpenAI‚Äôs o3. Real-time Router: This is a continuously trained decision-making model that quickly determines which model to assign a user request to based on various signals. Its decisions are based on: Conversation Type: Whether it‚Äôs a casual chat, Q\u0026A, or a task-oriented conversation. Complexity: The difficulty of the question and the depth of reasoning required. Tool Needs: Whether external tools like web search or a code interpreter need to be called. Explicit Intent: Users can guide the router to select the deep reasoning model with explicit instructions (e.g., ‚Äúthink hard about this‚Äù). The router continuously learns from real user signals (such as user model-switching behavior, response preference rates, and measured answer correctness) to constantly optimize its decision-making capabilities.\nSafe Completions The traditional safety training paradigm is Hard Refusals, where the model decides whether to answer fully or refuse directly based on a binary classification of user intent (safe or unsafe). This approach is effective for clearly malicious prompts but is very brittle when dealing with ambiguous intent or topics involving dual-use (e.g., biology, cybersecurity), often leading to over-refusals.\nSafe Completions (Baker et al., 2025) moves away from binary classification of user intent and instead focuses on maximizing the model‚Äôs helpfulness while adhering to safety policies.\nFor clearly harmful requests: The model will still refuse. For dual-use requests (e.g., in biology, chemistry, cybersecurity): The model provides safe, high-level answers without directly executable details, rather than refusing entirely. For requests with ambiguous intent: The model attempts to complete the task in a safe manner or offers safe alternatives. This approach significantly improves the model‚Äôs safety and utility in dual-use domains and reduces unnecessary over-refusals.\nFig. 8. Left: Overall structure of the safe-completion training stack. Right: Details of the safecompletion reward design. (Image source: OpenAI, 2025)\nChain-of-Thought Monitoring OpenAI employs Chain-of-Thought Monitoring (CoT Monitoring) (Baker et al., 2025) to ensure the reliability and safety of its reasoning models and to prevent issues like reward hacking. Unlike some approaches that try to optimize CoT through SFT, GPT-5‚Äôs CoT training does not impose direct alignment. This allows the CoT to more genuinely reflect the model‚Äôs ‚Äúthinking‚Äù process, serving as an effective window for detecting erroneous behavior, deceptive intent, or potential risks.\nFig. 9. Monitoring Frontier Reasoning Models for Reward Hacking. (Image source: Baker et al., 2025)\nThrough CoT monitoring, OpenAI found that the incidence of deceptive behavior in gpt-5-thinking was reduced to 2.1% from 4.8% in o3. This technology is crucial for understanding and mitigating the risks of advanced AI systems.\nEvaluation GPT-5 excels across multiple benchmarks, setting new standards, especially in reasoning, coding, and multimodal capabilities. Compared to its predecessors, it not only achieves a leap in accuracy but also makes significant strides in efficiency, often achieving or surpassing the performance of o3 with 50-80% less output.\nFig. 10. GPT-5 performance in SWE-bench Verified Software Engineering. (Image source: OpenAI, 2025)\nüß† Intelligence Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano AIME ‚Äô25 94.6% 91.1% 85.2% 88.9% 92.7% 46.4% 40.2% - FrontierMath (with python tool only) 26.3% 22.1% 9.6% 15.8% 15.4% - - - GPQA diamond 85.7% 82.3% 71.2% 83.3% 81.4% 66.3% 65.0% 50.3% HLE 24.8% 16.7% 8.7% 20.2% 14.7% 5.4% 3.7% - HMMT 2025 93.3% 87.8% 75.6% 81.7% 85.0% 28.9% 35.0% - üñºÔ∏è Multimodal Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano MMMU 84.2% 81.6% 75.6% 82.9% 81.6% 74.8% 72.7% 55.4% MMMU-Pro (avg across standard and vision sets) 78.4% 74.1% 62.6% 76.4% 73.4% 60.3% 58.9% 33.0% CharXiv reasoning (python enabled) 81.1% 75.5% 62.7% 78.6% 72.0% 56.7% 56.8% 40.5% VideoMMMU (max frame 256) 84.6% 82.5% 66.8% 83.3% 79.4% 60.9% 55.1% 30.2% ERQA 65.7% 62.9% 50.1% 64.0% 56.5% 44.3% 42.3% 26.5% üíª Coding Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano SWE-Lancer: IC SWE Diamond Freelance Coding Tasks $112K $75K $49K $86K $66K $34K $31K $9K SWE-bench Verified 74.9% 71.0% 54.7% 69.1% 68.1% 54.6% 23.6% - Aider polyglot (diff) 88.0% 71.6% 48.4% 79.6% 58.2% 52.9% 31.6% 6.2% üìã Instruction Following Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano Scale multichallenge (o3-mini grader) 69.6% 62.3% 54.9% 60.4% 57.5% 46.2% 42.2% 31.1% Internal API instruction following eval (hard) 64.0% 65.8% 56.1% 47.4% 44.7% 49.1% 45.1% 31.6% COLLIE 99.0% 98.5% 96.9% 98.4% 96.1% 65.8% 54.6% 42.5% üîß Function Calling Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano Tau¬≤-bench airline 62.6% 60.0% 41.0% 64.8% 60.2% 56.0% 51.0% 14.0% Tau¬≤-bench retail 81.1% 78.3% 62.3% 80.2% 70.5% 74.0% 66.0% 21.5% Tau¬≤-bench telecom 96.7% 74.1% 35.5% 58.2% 40.5% 34.0% 44.0% 12.1% üìö Long Context Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano OpenAI-MRCR: 2 needle 128k 95.2% 84.3% 43.2% 55.0% 56.4% 57.2% 47.2% 36.6% OpenAI-MRCR: 2 needle 256k 86.8% 58.8% 34.9% - - 56.2% 45.5% 22.6% Graphwalks bfs \u003c128k 78.3% 73.4% 64.0% 77.3% 62.3% 61.7% 61.7% 25.0% Graphwalks parents \u003c128k 73.3% 64.3% 43.8% 72.9% 51.1% 58.0% 60.5% 9.4% BrowseComp Long Context 128k 90.0% 89.4% 80.4% 88.3% 80.0% 85.9% 89.0% 89.4% BrowseComp Long Context 256k 88.8% 86.0% 68.4% - - 75.5% 81.6% 19.1% VideoMME (long, with subtitle category) 86.7% 78.5% 65.7% 84.9% 79.5% 78.7% 68.4% 55.2% üö® Hallucinations Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini LongFact-Concepts hallucination rate (no tools) 1.0% 0.7% 1.0% 5.2% 3.0% 0.7% 1.1% LongFact-Objects hallucination rate (no tools) 1.2% 1.3% 2.8% 6.8% 8.9% 1.1% 1.8% FActScore hallucination rate (no tools) 2.8% 3.5% 7.3% 23.5% 38.7% 6.7% 10.9% These results indicate that GPT-5 has made significant improvements in complex tasks requiring deep reasoning (like GPQA, AIME) and agentic tasks that require interaction with external environments (like SWE-bench, œÑ¬≤-bench). At the same time, the substantial improvement in factual accuracy (hallucination rate reduced by nearly 8x) makes it more reliable for practical applications.\nReferences [1] Raschka, S. (2025). ‚ÄúFrom GPT-2 to gpt-oss: Analyzing the Architectural Advances.‚Äù Ahead of AI.\n[2] Radford, Alec, et al. ‚ÄúLanguage models are unsupervised multitask learners.‚Äù OpenAI blog 1.8 (2019): 9.\n[3] OpenAI. (2025). ‚ÄúIntroducing gpt-oss.‚Äù OpenAI Blog.\n[4] OpenAI. (2025). ‚ÄúIntroducing GPT-5.‚Äù OpenAI Blog.\n[5] OpenAI. (2025). ‚Äúgpt-oss-120b \u0026 gpt-oss-20b Model Card.‚Äù\n[6] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. ‚ÄúLongformer: The long-document transformer.‚Äù arXiv preprint arXiv:2004.05150 (2020).\n[7] Jiang, Dongsheng, et al. ‚ÄúMistral 7B.‚Äù arXiv preprint arXiv:2310.08825 (2023).\n[8] Xiao, G., et al. (2023). ‚ÄúEfficient Streaming Language Models with Attention Sinks.‚Äù arXiv preprint arXiv:2309.17453.\n[9] Tseng, Albert, Tao Yu, and Youngsuk Park. ‚ÄúTraining llms with mxfp4.‚Äù arXiv preprint arXiv:2502.20586 (2025).\n[10] Yuan, Yuan, et al. ‚ÄúFrom Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training.‚Äù arXiv preprint arXiv:2508.09224 (2025).\n[11] B. Baker, J. Huizinga, L. Gao, Z. Dou, M. Y. Guan, A. Madry, W. Zaremba, J. Pachocki, and D. Farhi, ‚ÄúMonitoring reasoning models for misbehavior and the risks of promoting obfuscation.‚Äù arXiv preprint arXiv:2503.11926, 2025. Submitted on 14 March 2025.\n[12] OpenAI. (2025). ‚ÄúGPT-5 System Card.‚Äù\n[13] OpenAI. (2025). ‚ÄúIntroducing GPT-5 for developers.‚Äù OpenAI Blog.\nCitation Citation: When reproducing or citing the content of this article, please credit the original author and source.\nCited as:\nYue Shui. (Aug 2025). gpt-oss \u0026 GPT-5. https://syhya.github.io/posts/2025-08-24-gpt5\nOr\n@article{yue_shui_gpt5_2025 title = \"gpt-oss \u0026 GPT-5\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Aug\", url = \"https://syhya.github.io/posts/2025-08-24-gpt5\" } ","wordCount":"2541","inLanguage":"en","datePublished":"2025-08-24T12:00:00+08:00","dateModified":"2025-08-24T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-08-24-gpt5/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=ÁÆÄ‰Ωì‰∏≠Êñá aria-label=ÁÆÄ‰Ωì‰∏≠Êñá>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">gpt-oss & GPT-5</h1><div class=post-meta><span title='2025-08-24 12:00:00 +0800 +0800'>Created:&nbsp;2025-08-24</span>&nbsp;¬∑&nbsp;Updated:&nbsp;2025-08-24&nbsp;¬∑&nbsp;12 min&nbsp;¬∑&nbsp;2541 words&nbsp;¬∑&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-08-24-gpt5/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#gpt-oss>gpt-oss</a><ul><li><a href=#architecture-overview>Architecture Overview</a></li><li><a href=#efficient-attention-mechanisms>Efficient Attention Mechanisms</a></li><li><a href=#mxfp4-quantization>MXFP4 Quantization</a></li><li><a href=#training>Training</a></li><li><a href=#evaluation>Evaluation</a></li></ul></li><li><a href=#gpt-5>GPT-5</a><ul><li><a href=#system-architecture>System Architecture</a></li><li><a href=#safe-completions>Safe Completions</a></li><li><a href=#chain-of-thought-monitoring>Chain-of-Thought Monitoring</a></li><li><a href=#evaluation-1>Evaluation</a><ul><li><a href=#-intelligence>üß† Intelligence</a></li><li><a href=#-multimodal>üñºÔ∏è Multimodal</a></li><li><a href=#-coding>üíª Coding</a></li><li><a href=#-instruction-following>üìã Instruction Following</a></li><li><a href=#-function-calling>üîß Function Calling</a></li><li><a href=#-long-context>üìö Long Context</a></li><li><a href=#-hallucinations>üö® Hallucinations</a></li></ul></li></ul></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><p>In August 2025, the AI field witnessed a period of intensive releases from OpenAI. Following <strong>GPT-2</strong> (<a href=https://openai.com/index/better-language-models/>OpenAI, 2019</a>) in 2019, OpenAI has once again contributed to the open-source community with its first open-weight large language model series, <strong>gpt-oss</strong> (<a href=https://openai.com/index/introducing-gpt-oss/>OpenAI, 2025</a>), available in 120B and 20B sizes. Shortly after, the highly anticipated next-generation flagship model, <strong>GPT-5</strong> (<a href=https://openai.com/index/introducing-gpt-5/>OpenAI, 2025</a>), was also officially launched. This series of releases not only marks a new high for open-source models in reasoning and agent capabilities but also reveals OpenAI&rsquo;s latest advancements in model architecture, training methodologies, and safety alignment.</p><h2 id=gpt-oss>gpt-oss<a hidden class=anchor aria-hidden=true href=#gpt-oss>#</a></h2><p><strong>gpt-oss</strong> (<a href=https://openai.com/index/introducing-gpt-oss/>OpenAI, 2025</a>) is OpenAI&rsquo;s first open-weight language model series released since GPT-2, designed to provide the open-source community with powerful reasoning and tool-use capabilities. The series includes two versions, <code>gpt-oss-120b</code> and <code>gpt-oss-20b</code>, both released under the Apache 2.0 license.</p><h3 id=architecture-overview>Architecture Overview<a hidden class=anchor aria-hidden=true href=#architecture-overview>#</a></h3><figure class=align-center><a href=gpt_oss_vs_gpt2.png data-fancybox=gallery><img loading=lazy src=gpt_oss_vs_gpt2.png#center alt="Fig. 1. A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B. (Image source: Raschka, 2025)" width=100%></a><figcaption><p>Fig. 1. A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B. (Image source: <a href=https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the>Raschka, 2025</a>)</p></figcaption></figure><p>gpt-oss is built upon the GPT series architecture and incorporates several mainstream technologies from recent years, including <a href=https://syhya.github.io/posts/2025-02-01-normalization/#rms-normalization>RMSNorm</a>, <a href=https://syhya.github.io/posts/2025-04-06-llama/#ffn_swiglu>SwiGLU</a>, <a href=https://syhya.github.io/posts/2025-01-16-group-query-attention/#grouped-query-attention-gqa>GQA</a>, <a href=https://syhya.github.io/posts/2025-04-06-llama/#rotary-positional-embeddings-rope>RoPE</a>, <a href=https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/>YaRN</a>, and <a href=https://syhya.github.io/posts/2025-03-01-train-llm/#mixture-of-experts-model>MoE</a>.</p><p>The table below provides a direct comparison of the differences between the GPT-OSS 20B and GPT-2 XL 1.5B models.</p><table><thead><tr><th><strong>Feature</strong></th><th><strong>GPT-OSS 20B (2025)</strong></th><th><strong>GPT-2 XL 1.5B (2019)</strong></th></tr></thead><tbody><tr><td><strong>Release Date</strong></td><td>2025</td><td>2019</td></tr><tr><td><strong>Model Size</strong></td><td><strong>20B parameters</strong></td><td>1.5B parameters</td></tr><tr><td><strong>Active Parameters</strong></td><td><strong>3.5B</strong> (per inference)</td><td>1.5B (all activated)</td></tr><tr><td><strong>Vocabulary Size</strong></td><td><strong>200k tokens</strong></td><td>50k tokens</td></tr><tr><td><strong>Embedding Dimension</strong></td><td><strong>2,880</strong></td><td>1,600</td></tr><tr><td><strong>Number of Transformer Layers</strong></td><td>24 layers</td><td><strong>48 layers</strong></td></tr><tr><td><strong>Number of Attention Heads</strong></td><td><strong>64</strong></td><td>25</td></tr><tr><td><strong>Context Length</strong></td><td><strong>131k tokens</strong></td><td>1,024 tokens</td></tr><tr><td><strong>Positional Encoding</strong></td><td><strong>RoPE</strong> (Rotary Positional Embeddings)</td><td>Absolute Positional Embeddings</td></tr><tr><td><strong>Attention Mechanism</strong></td><td><strong>Grouped-Query Attention</strong></td><td>Multi-head Attention</td></tr><tr><td><strong>Feed-Forward Network</strong></td><td><strong>SwiGLU activation + MoE</strong></td><td>GELU activation</td></tr><tr><td><strong>MoE Architecture</strong></td><td><strong>32 experts, 4 activated</strong></td><td>None</td></tr><tr><td><strong>Normalization Method</strong></td><td><strong>RMSNorm</strong> (2 locations)</td><td>LayerNorm (2 locations)</td></tr><tr><td><strong>Dropout</strong></td><td><strong>None</strong></td><td>Yes</td></tr><tr><td><strong>Sliding Window Attention</strong></td><td><strong>Used in every other layer</strong>(window of 128 tokens)</td><td>None</td></tr><tr><td><strong>Training Features</strong></td><td><strong>Includes Supervised Fine-Tuning + Reinforcement Learning</strong></td><td>Pre-training only</td></tr><tr><td><strong>Quantization Support</strong></td><td><strong>MXFP4</strong> (Can run on a single GPU)</td><td>No special quantization</td></tr><tr><td><strong>License</strong></td><td>Apache 2.0</td><td>MIT</td></tr></tbody></table><h3 id=efficient-attention-mechanisms>Efficient Attention Mechanisms<a hidden class=anchor aria-hidden=true href=#efficient-attention-mechanisms>#</a></h3><p>To maintain high efficiency while supporting a 128k long context, gpt-oss employs several advanced attention mechanisms.</p><ul><li><p><strong>Grouped-Query Attention (GQA)</strong>: gpt-oss has 64 query heads and 8 key/value heads, meaning every 8 query heads share a single K/V pair. This significantly reduces the size of the KV cache and memory bandwidth requirements, thereby substantially increasing inference throughput with almost no loss in model performance.</p></li><li><p><strong>Sliding Window Attention</strong>: To further reduce computational complexity, gpt-oss draws inspiration from <strong>Longformer</strong> (<a href=https://arxiv.org/abs/2004.05150>Jiang et al., 2020</a>) and <strong>Mistral</strong> (<a href=https://arxiv.org/abs/2310.06825>Jiang et al., 2023</a>) by adopting a <strong>sliding window attention</strong>. Its Transformer layers alternate between Dense Attention and Locally Banded Sparse Attention. The latter is <strong>Sliding Window Attention</strong>, which limits the attention scope of each token to a fixed-size local window.</p></li></ul><figure class=align-center><a href=sliding_window_attention.png data-fancybox=gallery><img loading=lazy src=sliding_window_attention.png#center alt="Fig. 2. Comparison between regular attention (left) and sliding window attention (right). (Image source: Jiang et al., 2023)" width=80%></a><figcaption><p>Fig. 2. Comparison between regular attention (left) and sliding window attention (right). (Image source: <a href=https://arxiv.org/abs/2310.06825>Jiang et al., 2023</a>)</p></figcaption></figure><p>In gpt-oss, this window size is set to 128 tokens. This means that in a local attention layer, a token can only attend to the 128 tokens preceding it, not the entire context. This design reduces the computational complexity of attention from \( O(L^2) \) to \( O(L \cdot W) \), where \( L \) is the sequence length and \( W \) is the window size. By alternating with full attention layers, the model can efficiently process local information while integrating global information through the full attention layers.</p><ul><li><strong>Attention Sinks</strong>: The model introduces <strong>Attention Sinks</strong> (<a href=https://arxiv.org/abs/2309.17453>Xiao et al., 2023</a>), which learns a bias \( \mathbf{s}_h \) added to the attention scores. This allows initial tokens to be consistently attended to, helping to stabilize the attention distribution and prevent information loss in long-context scenarios.</li></ul>\[ \text{Attention}(Q, K, V)_h = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_k}} + \mathbf{s}_h\right)V_h \]<figure class=align-center><a href=StreamingLLM.png data-fancybox=gallery><img loading=lazy src=StreamingLLM.png#center alt="Fig. 3. Illustration of StreamingLLM vs existing methods. (Image source: Xiao et al., 2023)" width=100%></a><figcaption><p>Fig. 3. Illustration of StreamingLLM vs existing methods. (Image source: <a href=https://arxiv.org/abs/2309.17453>Xiao et al., 2023</a>)</p></figcaption></figure><p>The figure above compares the performance and efficiency of <strong>StreamingLLM</strong> with three common long-text processing methods. Assume a language model is pre-trained on texts of length $L$ and needs to predict the $T$-th token during inference (where $T \gg L$):</p><ol><li><strong>Dense Attention</strong>: Retains the key-values (KV) of all historical tokens and computes full attention. The time complexity is $O(T^2)$, and the cache size grows continuously. Performance drops significantly when the input length exceeds the pre-training length $L$.</li><li><strong>Window Attention</strong>: Caches only the KV of the most recent $L$ tokens. It is efficient for inference, but performance plummets once the information from early tokens is replaced.</li><li><strong>Sliding Window with Re-computation</strong>: Reconstructs the KV state from the most recent $L$ tokens each time a new token is generated. Although it performs well on long texts, the re-computation involves quadratic attention, leading to a high time complexity of $O(TL^2)$ and slow inference speed.</li></ol><p>This method combines attention sinks with recent tokens during computation, not only maintaining inference efficiency but also sustaining a stable attention distribution and low perplexity in long-text scenarios.</p><h3 id=mxfp4-quantization>MXFP4 Quantization<a hidden class=anchor aria-hidden=true href=#mxfp4-quantization>#</a></h3><figure class=align-center><a href=mxfp4.png data-fancybox=gallery><img loading=lazy src=mxfp4.png#center alt="Fig. 4. Faster MXFP4 Backpropagation via Stochastic Rounding and Hadamard Transform. (Image source: Tseng et al., 2025)" width=60%></a><figcaption><p>Fig. 4. Faster MXFP4 Backpropagation via Stochastic Rounding and Hadamard Transform. (Image source: <a href=https://arxiv.org/abs/2502.20586>Tseng et al., 2025</a>)</p></figcaption></figure><p>To enable the large model to run on consumer-grade hardware, gpt-oss uses the <strong>MXFP4</strong> (<a href=https://arxiv.org/abs/2502.20586>Tseng et al., 2025</a>) format to quantize the MoE weights. MXFP4 is a micro-scaling floating-point format that can effectively quantize weights to about 4.25 bits. Since MoE weights account for over 90% of the model&rsquo;s total parameters, this method drastically compresses the model size, allowing the 120B model to fit into a single 80GB GPU and the 20B model to run on devices with 16GB of VRAM.</p><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><ul><li><p><strong>Pre-training</strong>: The model is pre-trained on a dataset of several trillion tokens of text, with a focus on STEM, coding, and general knowledge. To enhance safety, the pre-training data reuses the CBRN content filters from GPT-4o.</p></li><li><p><strong>Post-training (Reasoning & Tool Use)</strong>: After pre-training, the model undergoes post-training using <strong>CoT RL</strong> techniques similar to those for OpenAI&rsquo;s o3. The goal of this stage is to teach the model to:</p><ol><li><strong>Reason</strong>: Generate detailed Chain-of-Thought (CoT) to solve complex problems.</li><li><strong>Use Tools</strong>: Learn to call external tools (like web search, code execution) to enhance its capabilities.</li></ol></li></ul><p>To achieve these advanced agent functionalities, OpenAI designed the <a href=https://github.com/openai/harmony>Harmony Chat Format</a>. This format introduces the concept of &ldquo;channels&rdquo; (e.g., <code>analysis</code> for CoT, <code>commentary</code> for tool calls, <code>final</code> for the final answer) and establishes a strict instruction hierarchy (System > Developer > User > Assistant > Tool) to ensure the model&rsquo;s behavior is controllable.</p><p>Additionally, the model supports <strong>Variable Effort Reasoning</strong>. Users can set <code>Reasoning: low/medium/high</code> in the system prompt to trade off between latency and performance. A higher effort level generates a longer CoT, which typically leads to higher accuracy but also increased latency.</p><figure class=align-center><a href=reasoning_efforts.png data-fancybox=gallery><img loading=lazy src=reasoning_efforts.png#center alt="Fig. 5. Accuracy vs. average CoT length for different reasoning levels on AIME and GPQA benchmarks. (Image source: OpenAI, 2025)" width=100%></a><figcaption><p>Fig. 5. Accuracy vs. average CoT length for different reasoning levels on AIME and GPQA benchmarks. (Image source: <a href=https://openai.com/index/gpt-oss-model-card/>OpenAI, 2025</a>)</p></figcaption></figure><h3 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h3><figure class=align-center><a href=gpt_oss_eval.png data-fancybox=gallery><img loading=lazy src=gpt_oss_eval.png#center alt="Fig. 6. Main capabilities evaluations for gpt-oss series. (Image source: OpenAI, 2025)" width=100%></a><figcaption><p>Fig. 6. Main capabilities evaluations for gpt-oss series. (Image source: <a href=https://openai.com/index/introducing-gpt-oss/>OpenAI, 2025</a>)</p></figcaption></figure><p>Benchmark results show that gpt-oss-120b&rsquo;s accuracy surpasses that of OpenAI&rsquo;s o3-mini and approaches o4-mini. Meanwhile, gpt-oss-20b, at only one-sixth the size, also demonstrates competitive performance.</p><h2 id=gpt-5>GPT-5<a hidden class=anchor aria-hidden=true href=#gpt-5>#</a></h2><p><strong>GPT-5</strong> (<a href=https://openai.com/index/introducing-gpt-5/>OpenAI, 2025</a>) is not a single model but a <strong>unified intelligent system</strong>. It is not a monolithic, massive model but a complex system of multiple specialized models and an intelligent routing mechanism working in concert to balance performance, speed, and cost.</p><h3 id=system-architecture>System Architecture<a hidden class=anchor aria-hidden=true href=#system-architecture>#</a></h3><figure class=align-center><a href=gpt5_system_arch.png data-fancybox=gallery><img loading=lazy src=gpt5_system_arch.png#center alt="Fig. 7. GPT-5 Unified System Architecture. (Image source: Latent Space, 2025)" width=100%></a><figcaption><p>Fig. 7. GPT-5 Unified System Architecture. (Image source: <a href=https://www.latent.space/p/gpt5-router>Latent Space, 2025</a>)</p></figcaption></figure><p>The GPT-5 system consists of three core components:</p><ol><li><strong>gpt-5-main</strong>: As the system&rsquo;s default model, it is fast and efficient, handling the vast majority of user queries. It can be considered the successor to GPT-4o.</li><li><strong>gpt-5-thinking</strong>: Used for more complex problems that require deep thought. This model is activated when the user explicitly requests it (e.g., &ldquo;think hard about this&rdquo;) or when the system determines the task requires it. It can be seen as the successor to OpenAI&rsquo;s o3.</li><li><strong>Real-time Router</strong>: This is a continuously trained decision-making model that quickly determines which model to assign a user request to based on various signals. Its decisions are based on:<ul><li><strong>Conversation Type:</strong> Whether it&rsquo;s a casual chat, Q&amp;A, or a task-oriented conversation.</li><li><strong>Complexity:</strong> The difficulty of the question and the depth of reasoning required.</li><li><strong>Tool Needs:</strong> Whether external tools like web search or a code interpreter need to be called.</li><li><strong>Explicit Intent:</strong> Users can guide the router to select the deep reasoning model with explicit instructions (e.g., &ldquo;think hard about this&rdquo;).</li></ul></li></ol><p>The router continuously learns from real user signals (such as user model-switching behavior, response preference rates, and measured answer correctness) to constantly optimize its decision-making capabilities.</p><h3 id=safe-completions>Safe Completions<a hidden class=anchor aria-hidden=true href=#safe-completions>#</a></h3><p>The traditional safety training paradigm is <strong>Hard Refusals</strong>, where the model decides whether to answer fully or refuse directly based on a binary classification of user intent (safe or unsafe). This approach is effective for clearly malicious prompts but is very brittle when dealing with ambiguous intent or topics involving <strong>dual-use</strong> (e.g., biology, cybersecurity), often leading to over-refusals.</p><p><strong>Safe Completions</strong> (<a href=https://openai.com/index/gpt-5-safe-completions/>Baker et al., 2025</a>) moves away from binary classification of user intent and instead focuses on maximizing the model&rsquo;s helpfulness while adhering to safety policies.</p><ul><li><strong>For clearly harmful requests</strong>: The model will still refuse.</li><li><strong>For dual-use requests</strong> (e.g., in biology, chemistry, cybersecurity): The model provides safe, high-level answers without directly executable details, rather than refusing entirely.</li><li><strong>For requests with ambiguous intent</strong>: The model attempts to complete the task in a safe manner or offers safe alternatives.</li></ul><p>This approach significantly improves the model&rsquo;s safety and utility in dual-use domains and reduces unnecessary over-refusals.</p><figure class=align-center><a href=safe_completions.png data-fancybox=gallery><img loading=lazy src=safe_completions.png#center alt="Fig. 8. Left: Overall structure of the safe-completion training stack. Right: Details of the safecompletion reward design. (Image source: OpenAI, 2025)" width=100%></a><figcaption><p>Fig. 8. Left: Overall structure of the safe-completion training stack. Right: Details of the safecompletion reward design. (Image source: <a href=https://openai.com/index/gpt-5-safe-completions/>OpenAI, 2025</a>)</p></figcaption></figure><h3 id=chain-of-thought-monitoring>Chain-of-Thought Monitoring<a hidden class=anchor aria-hidden=true href=#chain-of-thought-monitoring>#</a></h3><p>OpenAI employs <strong>Chain-of-Thought Monitoring (CoT Monitoring)</strong> (<a href=https://arxiv.org/abs/2503.11926>Baker et al., 2025</a>) to ensure the reliability and safety of its reasoning models and to prevent issues like <a href=https://lilianweng.github.io/posts/2024-11-28-reward-hacking/>reward hacking</a>. Unlike some approaches that try to optimize CoT through SFT, GPT-5&rsquo;s CoT training does not impose direct alignment. This allows the CoT to more genuinely reflect the model&rsquo;s &ldquo;thinking&rdquo; process, serving as an effective window for detecting erroneous behavior, deceptive intent, or potential risks.</p><figure class=align-center><a href=monitor_frontier_reasoning.png data-fancybox=gallery><img loading=lazy src=monitor_frontier_reasoning.png#center alt="Fig. 9. Monitoring Frontier Reasoning Models for Reward Hacking. (Image source: Baker et al., 2025)" width=100%></a><figcaption><p>Fig. 9. Monitoring Frontier Reasoning Models for Reward Hacking. (Image source: <a href=https://arxiv.org/abs/2503.11926>Baker et al., 2025</a>)</p></figcaption></figure><p>Through CoT monitoring, OpenAI found that the incidence of deceptive behavior in <code>gpt-5-thinking</code> was reduced to 2.1% from 4.8% in o3. This technology is crucial for understanding and mitigating the risks of advanced AI systems.</p><h3 id=evaluation-1>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation-1>#</a></h3><p>GPT-5 excels across multiple benchmarks, setting new standards, especially in reasoning, coding, and multimodal capabilities. Compared to its predecessors, it not only achieves a leap in accuracy but also makes significant strides in efficiency, often achieving or surpassing the performance of o3 with 50-80% less output.</p><figure class=align-center><a href=gpt5_swe_bench.png data-fancybox=gallery><img loading=lazy src=gpt5_swe_bench.png#center alt="Fig. 10. GPT-5 performance in SWE-bench Verified Software Engineering. (Image source: OpenAI, 2025)" width=100%></a><figcaption><p>Fig. 10. GPT-5 performance in SWE-bench Verified Software Engineering. (Image source: <a href=https://openai.com/index/introducing-gpt-5/>OpenAI, 2025</a>)</p></figcaption></figure><h4 id=-intelligence>üß† Intelligence<a hidden class=anchor aria-hidden=true href=#-intelligence>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>AIME ‚Äô25</td><td><strong>94.6%</strong></td><td>91.1%</td><td>85.2%</td><td>88.9%</td><td>92.7%</td><td>46.4%</td><td>40.2%</td><td>-</td></tr><tr><td>FrontierMath (with python tool only)</td><td><strong>26.3%</strong></td><td>22.1%</td><td>9.6%</td><td>15.8%</td><td>15.4%</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GPQA diamond</td><td><strong>85.7%</strong></td><td>82.3%</td><td>71.2%</td><td>83.3%</td><td>81.4%</td><td>66.3%</td><td>65.0%</td><td>50.3%</td></tr><tr><td>HLE</td><td><strong>24.8%</strong></td><td>16.7%</td><td>8.7%</td><td>20.2%</td><td>14.7%</td><td>5.4%</td><td>3.7%</td><td>-</td></tr><tr><td>HMMT 2025</td><td><strong>93.3%</strong></td><td>87.8%</td><td>75.6%</td><td>81.7%</td><td>85.0%</td><td>28.9%</td><td>35.0%</td><td>-</td></tr></tbody></table><h4 id=-multimodal>üñºÔ∏è Multimodal<a hidden class=anchor aria-hidden=true href=#-multimodal>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>MMMU</td><td><strong>84.2%</strong></td><td>81.6%</td><td>75.6%</td><td>82.9%</td><td>81.6%</td><td>74.8%</td><td>72.7%</td><td>55.4%</td></tr><tr><td>MMMU-Pro (avg across standard and vision sets)</td><td><strong>78.4%</strong></td><td>74.1%</td><td>62.6%</td><td>76.4%</td><td>73.4%</td><td>60.3%</td><td>58.9%</td><td>33.0%</td></tr><tr><td>CharXiv reasoning (python enabled)</td><td><strong>81.1%</strong></td><td>75.5%</td><td>62.7%</td><td>78.6%</td><td>72.0%</td><td>56.7%</td><td>56.8%</td><td>40.5%</td></tr><tr><td>VideoMMMU (max frame 256)</td><td><strong>84.6%</strong></td><td>82.5%</td><td>66.8%</td><td>83.3%</td><td>79.4%</td><td>60.9%</td><td>55.1%</td><td>30.2%</td></tr><tr><td>ERQA</td><td><strong>65.7%</strong></td><td>62.9%</td><td>50.1%</td><td>64.0%</td><td>56.5%</td><td>44.3%</td><td>42.3%</td><td>26.5%</td></tr></tbody></table><h4 id=-coding>üíª Coding<a hidden class=anchor aria-hidden=true href=#-coding>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>SWE-Lancer: IC SWE Diamond Freelance Coding Tasks</td><td><strong>$112K</strong></td><td>$75K</td><td>$49K</td><td>$86K</td><td>$66K</td><td>$34K</td><td>$31K</td><td>$9K</td></tr><tr><td>SWE-bench Verified</td><td><strong>74.9%</strong></td><td>71.0%</td><td>54.7%</td><td>69.1%</td><td>68.1%</td><td>54.6%</td><td>23.6%</td><td>-</td></tr><tr><td>Aider polyglot (diff)</td><td><strong>88.0%</strong></td><td>71.6%</td><td>48.4%</td><td>79.6%</td><td>58.2%</td><td>52.9%</td><td>31.6%</td><td>6.2%</td></tr></tbody></table><h4 id=-instruction-following>üìã Instruction Following<a hidden class=anchor aria-hidden=true href=#-instruction-following>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>Scale multichallenge (o3-mini grader)</td><td><strong>69.6%</strong></td><td>62.3%</td><td>54.9%</td><td>60.4%</td><td>57.5%</td><td>46.2%</td><td>42.2%</td><td>31.1%</td></tr><tr><td>Internal API instruction following eval (hard)</td><td>64.0%</td><td><strong>65.8%</strong></td><td>56.1%</td><td>47.4%</td><td>44.7%</td><td>49.1%</td><td>45.1%</td><td>31.6%</td></tr><tr><td>COLLIE</td><td><strong>99.0%</strong></td><td>98.5%</td><td>96.9%</td><td>98.4%</td><td>96.1%</td><td>65.8%</td><td>54.6%</td><td>42.5%</td></tr></tbody></table><h4 id=-function-calling>üîß Function Calling<a hidden class=anchor aria-hidden=true href=#-function-calling>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>Tau¬≤-bench airline</td><td>62.6%</td><td>60.0%</td><td>41.0%</td><td><strong>64.8%</strong></td><td>60.2%</td><td>56.0%</td><td>51.0%</td><td>14.0%</td></tr><tr><td>Tau¬≤-bench retail</td><td><strong>81.1%</strong></td><td>78.3%</td><td>62.3%</td><td>80.2%</td><td>70.5%</td><td>74.0%</td><td>66.0%</td><td>21.5%</td></tr><tr><td>Tau¬≤-bench telecom</td><td><strong>96.7%</strong></td><td>74.1%</td><td>35.5%</td><td>58.2%</td><td>40.5%</td><td>34.0%</td><td>44.0%</td><td>12.1%</td></tr></tbody></table><h4 id=-long-context>üìö Long Context<a hidden class=anchor aria-hidden=true href=#-long-context>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>OpenAI-MRCR: 2 needle 128k</td><td><strong>95.2%</strong></td><td>84.3%</td><td>43.2%</td><td>55.0%</td><td>56.4%</td><td>57.2%</td><td>47.2%</td><td>36.6%</td></tr><tr><td>OpenAI-MRCR: 2 needle 256k</td><td><strong>86.8%</strong></td><td>58.8%</td><td>34.9%</td><td>-</td><td>-</td><td>56.2%</td><td>45.5%</td><td>22.6%</td></tr><tr><td>Graphwalks bfs &lt;128k</td><td><strong>78.3%</strong></td><td>73.4%</td><td>64.0%</td><td>77.3%</td><td>62.3%</td><td>61.7%</td><td>61.7%</td><td>25.0%</td></tr><tr><td>Graphwalks parents &lt;128k</td><td><strong>73.3%</strong></td><td>64.3%</td><td>43.8%</td><td>72.9%</td><td>51.1%</td><td>58.0%</td><td>60.5%</td><td>9.4%</td></tr><tr><td>BrowseComp Long Context 128k</td><td><strong>90.0%</strong></td><td>89.4%</td><td>80.4%</td><td>88.3%</td><td>80.0%</td><td>85.9%</td><td>89.0%</td><td>89.4%</td></tr><tr><td>BrowseComp Long Context 256k</td><td><strong>88.8%</strong></td><td>86.0%</td><td>68.4%</td><td>-</td><td>-</td><td>75.5%</td><td>81.6%</td><td>19.1%</td></tr><tr><td>VideoMME (long, with subtitle category)</td><td><strong>86.7%</strong></td><td>78.5%</td><td>65.7%</td><td>84.9%</td><td>79.5%</td><td>78.7%</td><td>68.4%</td><td>55.2%</td></tr></tbody></table><h4 id=-hallucinations>üö® Hallucinations<a hidden class=anchor aria-hidden=true href=#-hallucinations>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th></tr></thead><tbody><tr><td>LongFact-Concepts hallucination rate (no tools)</td><td>1.0%</td><td><strong>0.7%</strong></td><td>1.0%</td><td>5.2%</td><td>3.0%</td><td><strong>0.7%</strong></td><td>1.1%</td></tr><tr><td>LongFact-Objects hallucination rate (no tools)</td><td>1.2%</td><td>1.3%</td><td>2.8%</td><td>6.8%</td><td>8.9%</td><td><strong>1.1%</strong></td><td>1.8%</td></tr><tr><td>FActScore hallucination rate (no tools)</td><td><strong>2.8%</strong></td><td>3.5%</td><td>7.3%</td><td>23.5%</td><td>38.7%</td><td>6.7%</td><td>10.9%</td></tr></tbody></table><p>These results indicate that GPT-5 has made significant improvements in complex tasks requiring deep reasoning (like GPQA, AIME) and agentic tasks that require interaction with external environments (like SWE-bench, œÑ¬≤-bench). At the same time, the substantial improvement in factual accuracy (hallucination rate reduced by nearly 8x) makes it more reliable for practical applications.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Raschka, S. (2025). <a href=https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the>&ldquo;From GPT-2 to gpt-oss: Analyzing the Architectural Advances.&rdquo;</a> Ahead of AI.</p><p>[2] Radford, Alec, et al. <a href=https://openai.com/index/better-language-models/>&ldquo;Language models are unsupervised multitask learners.&rdquo;</a> OpenAI blog 1.8 (2019): 9.</p><p>[3] OpenAI. (2025). <a href=https://openai.com/index/introducing-gpt-oss/>&ldquo;Introducing gpt-oss.&rdquo;</a> OpenAI Blog.</p><p>[4] OpenAI. (2025). <a href=https://openai.com/index/introducing-gpt-5/>&ldquo;Introducing GPT-5.&rdquo;</a> OpenAI Blog.</p><p>[5] OpenAI. (2025). <a href=https://openai.com/index/gpt-oss-model-card/>&ldquo;gpt-oss-120b & gpt-oss-20b Model Card.&rdquo;</a></p><p>[6] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. <a href=https://arxiv.org/abs/2004.05150>&ldquo;Longformer: The long-document transformer.&rdquo;</a> arXiv preprint arXiv:2004.05150 (2020).</p><p>[7] Jiang, Dongsheng, et al. <a href=https://arxiv.org/abs/2310.06825>&ldquo;Mistral 7B.&rdquo;</a> arXiv preprint arXiv:2310.08825 (2023).</p><p>[8] Xiao, G., et al. (2023). <a href=https://arxiv.org/abs/2309.17453>&ldquo;Efficient Streaming Language Models with Attention Sinks.&rdquo;</a> arXiv preprint arXiv:2309.17453.</p><p>[9] Tseng, Albert, Tao Yu, and Youngsuk Park. <a href=https://arxiv.org/abs/2502.20586>&ldquo;Training llms with mxfp4.&rdquo;</a> arXiv preprint arXiv:2502.20586 (2025).</p><p>[10] Yuan, Yuan, et al. <a href=https://www.arxiv.org/abs/2508.09224>&ldquo;From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training.&rdquo;</a> arXiv preprint arXiv:2508.09224 (2025).</p><p>[11] B. Baker, J. Huizinga, L. Gao, Z. Dou, M. Y. Guan, A. Madry, W. Zaremba, J. Pachocki, and D. Farhi, <a href=https://arxiv.org/abs/2503.11926>&ldquo;Monitoring reasoning models for misbehavior and the risks of promoting obfuscation.&rdquo;</a> arXiv preprint arXiv:2503.11926, 2025. Submitted on 14 March 2025.</p><p>[12] OpenAI. (2025). <a href=https://openai.com/index/gpt-5-system-card/>&ldquo;GPT-5 System Card.&rdquo;</a></p><p>[13] OpenAI. (2025). <a href=https://openai.com/index/introducing-gpt-5-for-developers/>&ldquo;Introducing GPT-5 for developers.&rdquo;</a> OpenAI Blog.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reproducing or citing the content of this article, please credit the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Aug 2025). gpt-oss & GPT-5.
<a href=https://syhya.github.io/posts/2025-08-24-gpt5>https://syhya.github.io/posts/2025-08-24-gpt5</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>yue_shui_gpt5_2025</span>
</span></span><span class=line><span class=cl>  <span class=err>title</span>   <span class=err>=</span> <span class=err>&#34;gpt-oss</span> <span class=err>&amp;</span> <span class=err>GPT-5&#34;,</span>
</span></span><span class=line><span class=cl>  <span class=err>author</span>  <span class=err>=</span> <span class=err>&#34;Yue</span> <span class=err>Shui&#34;,</span>
</span></span><span class=line><span class=cl>  <span class=err>journal</span> <span class=err>=</span> <span class=err>&#34;syhya.github.io&#34;,</span>
</span></span><span class=line><span class=cl>  <span class=err>year</span>    <span class=err>=</span> <span class=err>&#34;2025&#34;,</span>
</span></span><span class=line><span class=cl>  <span class=err>month</span>   <span class=err>=</span> <span class=err>&#34;Aug&#34;,</span>
</span></span><span class=line><span class=cl>  <span class=err>url</span>     <span class=err>=</span> <span class=err>&#34;https://syhya.github.io/posts/2025-08-24-gpt5&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/gpt-oss/>Gpt-Oss</a></li><li><a href=https://syhya.github.io/tags/gpt-5/>GPT-5</a></li><li><a href=https://syhya.github.io/tags/moe/>MoE</a></li><li><a href=https://syhya.github.io/tags/reasoning/>Reasoning</a></li><li><a href=https://syhya.github.io/tags/tool-use/>Tool Use</a></li><li><a href=https://syhya.github.io/tags/openai/>OpenAI</a></li><li><a href=https://syhya.github.io/tags/llm-architecture/>LLM Architecture</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-09-30-agentic-rl/><span class=title>¬´ Prev</span><br><span>Agentic RL</span>
</a><a class=next href=https://syhya.github.io/posts/2025-06-29-llm-inference/><span class=title>Next ¬ª</span><br><span>Large Language Model Inference</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on x" href="https://x.com/intent/tweet/?text=gpt-oss%20%26%20GPT-5&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-08-24-gpt5%2f&amp;hashtags=gpt-oss%2cGPT-5%2cMoE%2cReasoning%2cToolUse%2cOpenAI%2cLLMArchitecture"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-08-24-gpt5%2f&amp;title=gpt-oss%20%26%20GPT-5&amp;summary=gpt-oss%20%26%20GPT-5&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-08-24-gpt5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-08-24-gpt5%2f&title=gpt-oss%20%26%20GPT-5"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-08-24-gpt5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on whatsapp" href="https://api.whatsapp.com/send?text=gpt-oss%20%26%20GPT-5%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-08-24-gpt5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on telegram" href="https://telegram.me/share/url?text=gpt-oss%20%26%20GPT-5&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-08-24-gpt5%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on ycombinator" href="https://news.ycombinator.com/submitlink?t=gpt-oss%20%26%20GPT-5&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-08-24-gpt5%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>