[{"content":" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。\n智能体 大语言模型智能体 大语言模型智能体(Large Language Model Agent, LLM agent) 利用 LLM 作为系统大脑，并结合规划、记忆与外部工具等模块，实现了对复杂任务的自动化执行。\n用户请求 (User Request): 用户通过 prompt 输入任务，与智能体互动。 智能体 (Agent): 系统大脑，由一个或多个 LLMs 构成，负责整体协调和执行任务。 规划 (Planning): 将复杂任务拆解为更小的子任务，并制定执行计划，同时通过反思不断优化结果。 记忆 (Memory): 包含短期记忆(利用上下文学习即时捕捉任务信息)和长期记忆(采用外部向量存储保存和检索关键信息，确保长时任务的信息连续性)。 工具 (Tools): 集成计算器、网页搜索、代码解释器等外部工具，用于调用外部数据、执行代码和获取最新信息。 Fig. 1. The illustration of LLM Agent Framework. (Image source: DAIR.AI, 2024)\n强化学习智能体 强化学习(Reinforcement Learning，RL) 的目标是训练一个智能体(agent)在给定的环境 (environment) 中采取一系列动作(actions, \\(a_t\\))。在交互过程中，智能体从一个状态(state, \\(s_t\\))转移到下一个状态，并在每次执行动作后获得环境反馈的奖励(reward, \\(r_t\\))。这种交互生成了完整的轨迹(trajectory, \\(\\tau\\))，通常表示为：\n$$ \\tau = \\{(s_0, a_0, r_0), (s_1, a_1, r_1), \\dots, (s_T, a_T, r_T)\\}. $$智能体的目标是学习一个策略(policy, \\(\\pi\\))，即在每个状态下选择动作的规则，以最大化期望累积奖励，通常表达为：\n$$ \\max_{\\pi} \\, \\mathbb{E}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right], $$ 其中 \\(\\gamma \\in [0,1]\\) 为折扣因子，用于平衡短期与长期奖励。\nFig. 2. The agent-environment interaction. (Image source: Sutton \u0026amp; Barto, 2018)\n在 LLM 场景中，可以将模型视为一个智能体，而“环境”可理解为用户输入及其对应的期望回答方式：\n状态(\\(s_t\\))：可以是当前对话上下文或用户的问题。 动作(\\(a_t\\))：模型输出的文本(回答、生成内容等)。 奖励(\\(r_t\\))：来自用户或系统的反馈(如用户满意度、奖励模型自动评分等)。 轨迹(\\(\\tau\\))：从初始对话到结束的所有文本交互序列，可用于评估模型的整体表现。 策略(\\(\\pi\\))：LLM 在每个状态下(对话上下文)如何生成文本的规则，一般由模型的参数所决定。 对于 LLM，传统上先通过海量离线数据进行预训练，而在后训练强化学习环节中，则通过人类或者模型的反馈对模型进行训练，使其输出更符合人类偏好或任务需求的高质量文本。\n对比 下表展示了两者之间的差异：\n对比维度 LLM Agent RL Agent 核心原理 规划、记忆和工具实现复杂任务自动化。 通过与环境互动的试错反馈循环，不断优化策略以最大化长期奖励。 优化方式 不直接更新模型参数，主要依靠上下文扩展、外部记忆和工具提高性能。 持续频繁更新策略模型参数，依赖环境反馈的奖励信号不断优化。 互动方式 使用自然语言与用户或外部系统交互，灵活调用多种工具获得外部信息。 与真实或模拟环境交互，环境提供奖励或惩罚，形成闭环反馈。 实现目标 分解复杂任务、借助外部资源完成任务，关注任务结果质量与准确性。 最大化长期奖励，追求短期与长期回报之间的最优平衡。 随着研究的深入，LLM 与 RL 智能体的结合呈现出更多可能性，例如：\n利用强化学习方法训练 Reasoning LLM(如 o1/o3)，使其更适合作为 LLM 智能体的基础模型。 同时，记录 LLM 智能体执行任务的数据与反馈，为 Reasoning LLM 提供丰富的训练数据，从而提升模型性能。 规划：任务分解 LLM Agent 的核心组件包括规划、记忆和工具使用，这些组件共同协作，使智能体能够自主执行复杂任务。\nFig. 3. Overview of a LLM-powered autonomous agent system. (Image source: Weng, 2017)\n规划对于成功执行复杂任务至关重要。它可以根据复杂性和迭代改进的需求以不同的方式进行。在简单的场景中，规划模块可以使用 LLM 预先概述详细的计划，包括所有必要的子任务。此步骤确保智能体系统地进行 任务分解 并从一开始就遵循清晰的逻辑流程。\n思维链 思维链(Chain of Thought, CoT) (Wei et al. 2022) 通过逐步生成一系列简短句子来描述推理过程，这些句子称为推理步骤。其目的是显式地展示模型的推理路径，帮助模型更好地处理复杂推理任务。下图展示了少样本提示（左侧）和思维链提示（右侧）的区别。少样本提示得到错误答案，而思维链方法则引导模型逐步陈述推理过程，更清晰地体现模型的逻辑过程，从而提升答案准确性和可解释性。\nFig. 4. The comparison example of few-shot prompting and CoT prompting. (Image source: Weng, 2023)\n零样本思维链提示(Zero-Shot CoT) (Kojima et al. 2022)是 CoT 的后续研究，提出了一种极为简单的零样本提示方式。他们发现，仅需在问题末尾添加一句 Let's think step by step，LLM 便能够产生思维链，可以获得更为准确的答案。\nFig. 5. The comparison example of few-shot prompting and CoT prompting. (Image source: Kojima et al. 2022)\n自洽采性 自洽采样(Self-consistency sampling)(Wang et al. 2022a) 通过对同一提示词在 temperature \u0026gt; 0 的情况下多次采样，生成多个多样化答案，并从中选出最佳答案的方法。其核心思想是通过采样多个推理路径，再进行多数投票以提高最终答案的准确性和稳健性。不同任务中选择最佳答案的标准可以有所不同，一般情况下采用多数投票作为通用方案。而对于如编程题这类易于验证的任务，则可以通过解释器运行并结合单元测试对答案进行验证。这是一种对 CoT 的优化，与其结合使用时，能够显著提升模型在复杂推理任务中的表现。\nFig. 6. Overview of the Self-Consistency Method for Chain-of-Thought Reasoning. (Image source: Wang et al. 2022a)\n下面是一些后续优化的工作：\n(Wang et al. 2022b)后续采用另一种集成学习方法进行优化, 通过改变示例顺序或以模型生成的推理代替人为书写，增加随机性后再多数投票。 Fig. 7. An overview of different ways of composing rationale-augmented ensembles, depending on how the randomness of rationales is introduced. (Image source: Wang et al. 2022b)\n如果训练样本仅提供正确答案而无推理依据，可采用STaR（Self-Taught Reasoner）(Zelikman et al. 2022)方法： (1) 让LLM生成推理链，仅保留正确答案的推理。 (2) 用生成的推理微调模型，反复迭代直至收敛。注意 temperature 高时易生成带正确答案但错误推理的结果。如无标准答案，可考虑将多数投票视作“正确答案”。 Fig. 8. An overview of STaR and a STaR-generated rationale on CommonsenseQA (Image source: Zelikman et al. 2022)\n(Fu et al. 2023)发现复杂程度更高的示例（推理步骤更多）可提高模型性能。分隔推理步骤时，换行符 \\n 效果优于step i、. 或 ;。此外，通过复杂度一致性策略(Complexity-based consistency)，即仅对生成复杂度 top $k$ 的推理链进行多数投票，也能进一步优化模型输出。同时，将提示中的 Q: 替换为 Question: 也被证明对性能有额外提升。 Fig. 9. Sensitivity analysis on step formatting. Complex prompts consistently lead to better performance with regard to different step formatting. (Image source: Fu et al. 2023))\n思维树 思维树 (Tree of Thoughts, ToT)(Yao et al. 2023) 在 CoT 基础上拓展，每一步都探索多个推理可能性。它首先将问题分解为多个思考步骤，并在每一步产生多个不同想法，从而形成树形结构。搜索过程可采用广度优先搜索（Breadth-first search, BFS）或深度优先搜索（Depth-first search, DFS），并通过分类器（也可以让 LLM 进行打分）或多数票方式评估每个状态。它包含三个主要步骤：\n扩展(Expand)：生成一个或多个候选解决方案。 评分(Score)：衡量候选方案的质量。 剪枝(Prune)：保留排名 top $k$ 的最佳候选方案。 如果没有找到解决方案（或者候选方案的质量不够高），则回撤到扩展步骤。\nFig. 10. Schematic illustrating various approaches to problem solving with LLMs (Image source: Yao et al. 2023)\n规划：自我反思 自我反思(Self-Reflexion) 是使 Agent 通过改进过去的行动决策和纠正以往错误而实现迭代提升的关键因素。在试错不可避免的现实任务中，它起着至关重要的作用。\nReAct ReAct(Reason + Act) (Yao et al. 2023) 框架通过将任务特定的离散动作和语言空间相结合，实现了 LLM 中推理与行动的无缝整合。这种设计不仅使模型能够通过调用例如 Wikipedia 搜索 API 等外部接口与环境进行交互，同时还能以自然语言生成详细的推理轨迹，从而解决复杂问题。\nReAct 提示模板包含明确的思考步骤，其基本格式如下：\nThought：... Action：... Observation：... ...(Repeated many times) Fig. 11. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023)\n从下图可以看出在知识密集型任务和决策任务中，ReAct 的表现均明显优于仅依赖Actor的基础方法，从而展示了其在提升推理效果和交互性能方面的优势。\nFig. 12. PaLM-540B prompting results on HotpotQA and Fever. (Image source: Yao et al. 2023)\nReflexion Reflexion(Shinn et al. 2023)让 LLM 能够通过自我反馈与动态记忆不断迭代、优化决策。\n这种方法本质上借鉴了强化学习的思想，在传统的 Actor-Critic 模型中，Actor 根据当前状态 $s_t$ 选择动作 $a_t$，而 Critic 则会给出估值（例如价值函数 $V(s_t)$ 或动作价值函数 $Q(s_t,a_t)$），并反馈给 Actor 进行策略优化。对应地，在 Reflexion 的三大组件中：\nActor：由 LLM 扮演，基于环境状态（包括上下文和历史信息）输出文本及相应动作。可记为：\n$$ a_t = \\pi_\\theta(s_t), $$其中 $\\pi_\\theta$ 表示基于参数 $\\theta$（即 LLM 的权重或提示）得到的策略。Actor 与环境交互并产生轨迹 $\\tau = {(s_1,a_1,r_1), \\dots, (s_T,a_T,r_T)}$。\nEvaluator：类似于 Critic，Evaluator 接收由 Actor 生成的轨迹并输出奖励信号 $r_t$。在 Reflexion 框架里，Evaluator 可以通过预先设计的启发式规则或额外的 LLM 来对轨迹进行分析，进而产生奖励。例如：\n$$ r_t = R(\\tau_t), $$其中 $R(\\cdot)$ 为基于当前轨迹 $\\tau_t$ 的奖励函数。\nSelf-Reflection：该模块相当于在 Actor-Critic 之外额外增加了自我调节反馈机制。它整合当前轨迹 $\\tau$、奖励信号 ${r_t}$ 以及长期记忆中的历史经验，利用语言生成能力产生针对下一次决策的自我改进建议。这些反馈信息随后被写入外部记忆，为后续 Actor 的决策提供更丰富的上下文，从而在不更新 LLM 内部参数的情况下，通过提示词的动态调整实现类似于策略参数 $\\theta$ 的迭代优化。\nFig. 13. (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm (Image source: Shinn et al. 2023)\nReflexion 的核心循环与算法描述如下：\n初始化\n同时实例化 Actor、Evaluator、Self-Reflection 三个模型（均可由 LLM 实现），分别记为 $M_a, M_e, M_{sr}$。 初始化策略 $\\pi_\\theta$（包含 Actor 的模型参数或提示，以及初始记忆等信息）。 先让 Actor 按当前策略 $\\pi_\\theta$ 生成一个初始轨迹 $\\tau_0$，$M_e$ 进行评估后，再由 $M_{sr}$ 生成首条自我反思文本并存入长期记忆。 生成轨迹\n在每一次迭代中，$M_a$ 读取当前的长期记忆及环境观测，依次输出动作 ${a_1, a_2, \\ldots}$，与环境交互并获得相应反馈，形成新的轨迹 $\\tau_t$。 $\\tau_t$ 可以视作该任务的短期记忆，仅在本轮迭代中使用。 评估\n$M_e$ 根据轨迹 $\\tau_t$（即 Actor 的行为与环境反馈序列），输出奖励或评分 ${r_1, r_2, \\ldots}$。这一步对应 $M_e$ 的内部反馈，或由外部环境直接给出结果。 自我反思\n$M_{sr}$ 模块综合轨迹 $\\tau_t$ 与奖励信号 ${r_t}$ 在语言层面生成自我修正或改进建议 $\\mathrm{sr}_t$。 反思文本 (Reflective text) 既可视为对错误的剖析，也可提供新的启发思路，并通过存储到长期记忆中。实践中，我们可以将反馈信息向量化后存入向量数据库。 更新并重复\n将最新的自我反思文本 $\\mathrm{sr}_t$ 追加到长期记忆后，Actor 在下一轮迭代时即可从中采用 RAG 检索历史相关信息来调整策略。 反复执行上述步骤，直至 $M_e$ 判定任务达成或到达最大轮次。在此循环中，Reflexion 依靠 自我反思 + 长期记忆 的持续累加来改进决策，而非直接修改模型参数。 下面分别展示了 Reflexion 在决策制定、编程和推理任务中的应用示例：\nFig. 14. Reflexion works on decision-making 4.1, programming 4.3, and reasoning 4.2 tasks (Image source: Shinn et al. 2023)\n在 100 个 HotPotQA 问题的实验中，通过对比 CoT 方法和加入 episodic memory 的方式，结果显示采用 Reflexion 方法在最后增加自我反思步骤后，其搜索、信息检索和推理能力提升明显。\nFig. 15. Comparative Analysis of Chain-of-Thought (CoT) and ReAct on the HotPotQA Benchmark (Image source: Shinn et al. 2023)\nDeepSeek R1 DeepSeek-R1 (DeepSeek-AI, 2025) 代表了开源社区在复现 OpenAI o1 (OpenAI, 2024) 方面的重大突破，通过强化学习技术成功训练出具备深度反思能力的高级推理模型。\n关于 DeepSeek R1 的详细训练流程与技术实现，请参考我的专题博客：OpenAI o1复现进展：DeepSeek-R1。\nDeepSeek-R1-Zero 在训练过程中的关键转变 — 随着训练的深入，模型逐渐涌现出卓越的自我进化能力。这种能力体现在三个核心方面：\n自我反思：模型能够回溯并批判性地评估先前的推理步骤。 主动探索：当发现当前解题路径不理想时，能够自主寻找并尝试替代方案。 动态思考调整：根据问题复杂度自适应地调整生成 token 数量，实现更深入的思考过程。 这种动态且自发的推理行为显著提升了模型解决复杂问题的能力，使其能够更加高效、准确地应对具有挑战性的任务。\nFig. 16. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)\nDeepSeek-R1-Zero 训练过程中也涌现出一个典型\u0026quot;顿悟时刻\u0026quot;(aha moment)。在这一关键阶段，模型在推理过程中突然意识到先前的思考路径存在错误，随即迅速调整思考方向，最终成功导向正确答案。这一现象有力证明了模型在推理过程中已发展出强大的自我纠错和反思能力，类似于人类思考过程中的顿悟体验。\nFig. 17. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: DeepSeek-AI, 2025)\n记忆 人类记忆 记忆 指的是获取、储存、保持和提取信息的过程。人类的记忆主要分为以下三大类：\nFig. 18. Categorization of human memory. (Image source: Weng, 2017)\n感觉记忆: 用于在原始刺激（视觉、听觉、触觉等）消失后短暂保留感官信息，通常持续时间以毫秒或秒计。感觉记忆又分为：\n视觉记忆：对视觉通道所保留的瞬时图像或视觉印象，一般维持 0.25～0.5 秒，用于在视频或动画场景中形成视觉连续性。 听觉记忆：对听觉信息的短暂存储，可持续数秒，使人能回放刚刚听到的语句或声音片段。 触觉记忆：用于保留短暂的触觉或力觉信息，一般持续毫秒到秒级，例如敲击键盘或盲文阅读时短暂的手指感知。 短期记忆: 存储我们当前意识到的信息。\n持续时间约 20～30 秒，容量通常为 7±2 个项目。 承担学习、推理等复杂认知任务时对信息的临时处理和维持。 长期记忆: 能将信息保存数天到数十年，容量几乎无限。长期记忆分为：\n外显记忆: 可有意识回忆，包含情景记忆（个人经历、事件细节）和语义记忆（事实与概念）。 内隐记忆:无意识记忆，主要与技能和习惯相关，如骑车或盲打。 人类记忆的这三种类型相互交织，共同构成了我们对世界的认知和理解。在构建 LLM Agent 中，我们也可以借鉴人类记忆的这种分类方式：\n感觉记忆 对应 LLM 对输入原始数据（如文本、图片和视频）等的嵌入表征。 短期记忆 对应 LLM 的上下文内学习，受限于模型上下文窗口 max_tokens，当对话长度超出窗口后，早期信息将被截断。 长期记忆 对应 外部向量存储或数据库，Agent 可以基于 RAG 技术在需要时检索历史信息。 LLM Agent 记忆 Agent 与用户多轮互动、执行多步任务时，可以利用不同形式的记忆以及环境信息来完成工作流。\nFig. 19. An overview of the sources, forms, and operations of the memory in LLM-based agents. (Image source: Zhang et al. 2024)\n文本记忆\n完整交互：记录了所有对话与操作轨迹，帮助 Agent 回溯上下文。 近期交互：只保留与当前任务高度相关的对话内容，减少不必要的上下文占用。 检索到的交互：Agent 可从外部知识库中检索到与当前任务相关的历史对话或记录，融入当前上下文中。 外部知识：当 Agent 遇到知识空白时，可通过 API 或外部存储进行检索和获取额外信息。 参数化记忆\n微调：通过给 LLM 注入新的信息或知识，从而扩充模型的内部知识。 知识编辑：在模型层面对已有知识进行修改或更新，实现对模型内部参数记忆的动态调整。 环境\n代表着 Agent 与用户及外部系统交互时涉及的实体和上下文，比如用户 Alice、可能访问的工具或界面（例如订票系统、流媒体平台等）。 Agent\nLLM Agent 负责读与写操作，即读取外部环境或知识库的信息，写入新的动作或内容。 同时包含一系列管理功能，如合并、反思、遗忘等，用以动态维护短期和长期记忆。 另外一个例子是 Agent 在完成在两个不同但相关的任务中，需要同时使用短期记忆和长期记忆：\n任务 A 播放视频：Agent 将当前的计划、操作和环境状态（例如搜索、点击、播放视频等）记录在短期记忆中，这部分信息会保存在内存和 LLM 的上下文窗口中。 任务 B 下载游戏：Agent 利用长期记忆中与 Arcane 和 League of Legend 相关的知识，快速找到游戏下载方案。图中显示，Agent 在 Google 上进行搜索，我们可以将 Google 的知识库视作一个外部知识源，同时所有新的搜索、点击和下载操作也会被更新到短期记忆中。 Fig. 20: Illustration of short-term memory and long-term memory in an LLM-brained GUI agent. (Image source: Zhang et al. 2024)\n常见记忆元素及其对应的存储方式可以总结成以下表格:\n记忆元素 记忆类型 描述 存储介质 / 方式 动作 短期记忆 历史动作轨迹（例如点击按钮、输入文本等） 内存、LLM 上下文窗口 计划 短期记忆 上一步或当前生成的下一步操作计划 内存、LLM 上下文窗口 执行结果 短期记忆 动作执行后返回的结果、报错信息及环境反馈 内存、LLM 上下文窗口 环境状态 短期记忆 当前 UI 环境中可用的按钮、页面标题、系统状态等 内存、LLM 上下文窗口 自身经验 长期记忆 历史任务轨迹与执行步骤 数据库、磁盘 自我引导 长期记忆 从历史成功轨迹中总结出的指导规则与最佳实践 数据库、磁盘 外部知识 长期记忆 辅助任务完成的外部知识库、文档或其他数据源 外部数据库、向量检索 任务成功指标 长期记忆 记录任务成功率、失败率等指标，便于改进和分析 数据库、磁盘 此外，研究者还提出一些新的训练和存储方法，以增强 LLM 的记忆能力：\nLongMem(Language Models Augmented with Long-Term Memory)(Wang, et al. 2023)使 LLM 能够记忆长历史信息。其采用一种解耦的网络结构，将原始 LLM 参数冻结为记忆编码器(memory encoder)固定下来，同时使用自适应残差网络(Adaptive Residual Side-Network，SideNet)作为记忆检索器进行记忆检查和读取。\nFig. 21. Overview of the memory caching and retrieval flow of LongMem. (Image source: Wang, et al. 2023)\n其主要由三部分构成：Frozen LLM、Residual SideNet 和 Cached Memory Bank。其工作流程如下：\n先将长文本序列拆分成固定长度的片段，每个片段在 Frozen LLM 中逐层编码后，在第 $m$ 层提取注意力的 $K, V \\in \\mathbb{R}^{H \\times M \\times d}$ 向量对并缓存到 Cached Memory Bank。 面对新的输入序列时，模型根据当前输入的 query-key 检索长期记忆库，从中获取与输入最相关的前 $k$ 个 key-value（即 top-$k$ 检索结果），并将其融合到后续的语言生成过程中；与此同时记忆库会移除最旧的内容以保证最新上下文信息的可用性。 Residual SideNet 则在推理阶段对冻结 LLM 的隐藏层输出与检索得到的历史 key-value 进行融合，完成对超长文本的有效建模和上下文利用。 通过这种解耦设计，LongMem 无需扩大自身的原生上下文窗口就能灵活调度海量历史信息，兼顾了速度与长期记忆能力。\n工具使用 工具使用是 LLM Agent 重要组成部分, 通过赋予 LLM 调用外部工具的能力，其功能得到了显著扩展：不仅能够生成自然语言，还能获取实时信息、执行复杂计算以及与各类系统（如数据库、API 等）交互，从而有效突破预训练知识的局限，避免重复造轮子的低效过程。\n传统 LLM 主要依赖预训练数据进行文本生成，但这也使得它们在数学运算、数据检索和实时信息更新等方面存在不足。通过工具调用，模型可以：\n提升运算能力： 例如通过调用专门的计算器工具 Wolfram，模型能够进行更精准的数学计算，弥补自身算术能力的不足。\n实时获取信息： 利用搜索引擎 Gooole、Bing 或数据库 API，模型可以访问最新信息，确保生成内容的时效性和准确性。\n增强信息可信度： 借助外部工具的支持，模型能够引用真实数据来源，降低信息虚构的风险，提高整体可信性。\n提高系统透明度： 跟踪 API 调用记录可以帮助用户理解模型决策过程，提供一定程度的可解释性。\n当前，业界涌现出多种基于工具调用的 LLM 应用，它们利用不同策略和架构，实现了从简单任务到复杂多步推理的全面覆盖。\nToolformer Toolformer (Schick, et al. 2023)是一种能够通过简单 API 使用外部工具的 LLM。它的训练方式基于 GPT-J 模型进行微调，仅需为每个 API 提供少量示例。Toolformer 学到调用的工具包括问答系统、 Wikipedia 搜索、计算器、日历和翻译系统：\nFig. 22. Examples of inputs and outputs for all APIs used. (Image source: Schick, et al. 2023)\nHuggingGPT HuggingGPT (Shen, et al. 2023)是一种利用 ChatGPT 作为任务规划器的框架，通过读取模型描述从 HuggingFace 筛选可用的模型来完成用户任务，并根据执行结果进行总结。\nFig. 23. Examples of inputs and outputs for all APIs used. (Image source: Shen, et al. 2023)\n该系统由以下四个阶段组成：\n任务规划: 将用户请求解析为多个子任务。每个任务包含四个属性：任务类型、ID、依赖关系以及参数。论文使用少样本提示来指导模型进行任务拆分和规划。 模型选择: 将各子任务分配给不同的专家模型，采用多选题的方式来确定最合适的模型。由于上下文长度有限，需要根据任务类型对模型进行初步过滤。 任务执行: 专家模型执行分配的具体任务并记录结果, 结果会被传递给 LLM 进行后续处理。 结果生成: 接收各专家模型的执行结果，最后给用户输出总结性答案。 LLM 智能体应用 Generative Agent Generative Agent (Park, et al. 2023) 实验通过 25 个由大型语言模型驱动的虚拟角色，在沙盒环境中模拟真实的人类行为。其核心设计融合了记忆、检索、反思以及规划与反应机制，允许 Agent 记录并回顾自身经验，并从中提炼出关键信息，以指导后续行动和互动。\nFig. 24. The screenshot of generative agent sandbox. (Image source: Park, et al. 2023)\n整个系统利用长期记忆模块记录所有观察事件，结合检索模型根据时效性、重要性与相关性提取信息，再通过反思机制生成高层次推断，最终将这些成果转化为具体行动。该仿真实验展示了信息扩散、关系记忆和社会事件协调等涌现行为，为交互式应用提供了逼真的人类行为模拟。\nFig. 25. The generative agent architecture. (Park, et al. 2023)\nWebVoyager WebVoyager(He et al. 2024) 是一种基于多模态大模型的自主网页交互智能体，能够控制鼠标和键盘进行网页浏览。WebVoyager 采用经典的 ReAct 循环。在每个交互步骤中，它查看带有类似 SoM(Set-of-Marks)(Yang, et al. 2023) 方法标注的浏览器截图即通过在网页元素上放置数字标签提供交互提示，然后决定下一步行动。这种视觉标注与 ReAct 循环相结合，使得用户可以通过自然语言与网页进行交互。具体可以参考使用 LangGraph 框架的WebVoyager 代码。\nFig. 26. The overall workflow of WebVoyager. (Image source: He et al. 2024)\nOpenAI Operator Operator (OpenAI, 2025) 是一个 OpenAI 近期发布的 AI 智能体，旨在自主执行网络任务。Operator 能够像人类用户一样与网页互动，通过打字、点击和滚动等操作完成指定任务。Operator 的核心技术是计算机使用智能体（Computer-Using Agent, CUA）(OpenAI, 2025)。CUA 结合了 GPT-4o 的视觉能力和通过强化学习获得更强的推理能力，经过专门训练后能够与图形用户界面（GUI）进行交互，包括用户在屏幕上看到的按钮、菜单和文本框。\nFig. 27. Overview of OpenAI CUA. (Image source: OpenAI, 2025)\nCUA 的运作方式遵循一个迭代循环，包含三个阶段：\n感知: CUA 通过捕获浏览器截图来“观察”网页内容。这种基于视觉的输入方式使其能够理解页面的布局和元素。\n推理: 借助链式思考的推理过程，CUA 会评估下一步行动，其依据是当前和之前的截图以及已执行的操作。这种推理能力使其能够跟踪任务进度、回顾中间步骤，并根据需要进行调整。\n行动: CUA 通过模拟鼠标和键盘操作（如点击、输入和滚动）与浏览器进行交互。这使其能够在无需特定 API 集成的情况下执行各种网络任务。\nCUA 和之前现有的 WebVoyager 不同之处在于这是一个专门经过强化学习训练的 Agent，而不是直接调用 GPT-4o 搭建的固定流程的 Workflow。虽然 CUA 目前仍处于早期阶段且存在一定局限，但它以下基准测试中取得了 SOTA 结果。\nFig. 28. OpenAI CUA Benchmark Results. (Image source: OpenAI, 2025)\nDeep Research Deep Research 本质上是一个报告生成系统：给定用户的查询，系统以 LLM 作为核心 Agent，经过多轮迭代式的信息检索与分析，最终生成一份结构化、翔实的报告。目前，各类 Deep Research 系统的实现逻辑主要可分为Workflow Agent和RL Agent两种方式。\nWorkflow Agent vs RL Agent Workflow Agent 这种方式依赖开发人员预先使用设计的工作流和手工构建的 Prompt 来组织整个报告生成过程。主要特点包括：\n任务分解与流程编排：系统将用户查询拆分为若干子任务，比如生成大纲、信息检索、内容汇总等，再按照预定的流程顺序执行。 固定流程：各个阶段之间的调用和交互是事先设定好的，类似于构建一个静态的流程图或有向无环图（DAG），确保每一步都有明确的职责。 人工设计依赖：该方法主要依赖工程师的经验，通过反复调试 Prompt 来提升输出质量，适用性较强但灵活性有限。 可以利用 LangGraph框架以图的形式来构建和编排工作流。\nFig. 29. A workflow of the LangGraph. (Image source: LangGraph, 2025)\n目前 Github 上已经有多个开源项目实现了基于工作流的 Deep Research Agent，如 GPT Researcher 和 open deep research 等。\nFig. 30. An overview of the open deep research. (Image source: LangChain, 2025)\nRL Agent 则是另外一种实现方法，通过 RL 训练推理模型来优化 Agent 的多轮搜索、分析与报告撰写流程进行优化。主要特点包括：\n自主决策能力：系统通过强化学习训练，让 Agent 在面对复杂的搜索与内容整合任务时能够自主判断、决策和调整策略，从而更高效地生成报告。 持续优化：利用奖励机制对生成过程进行打分和反馈，Agent 能够不断迭代优化自身策略，实现从任务拆解到最终报告生成的整体质量提升。 降低人工干预：相较于依赖手工 Prompt 的固定流程，强化学习训练方式减少了对人工设计的依赖，更适合应对多变和复杂的实际应用场景。 下面的表格总结了这两种方式的主要区别：\n特性 Workflow Agent RL Agent 流程设计 预先设计固定工作流，任务分解和流程编排明确 端到端学习，Agent 自主决策和动态调整流程 自主决策能力 依赖人工设计的 Prompt，决策过程固定且不可变 通过强化学习，Agent 能够自主判断、决策并优化策略 人工干预 需要大量手工设计与调试 Prompt，人工干预较多 降低人工干预，通过奖励机制实现自动反馈和持续优化 灵活性与适应性 对复杂或变化场景的适应性较弱，扩展性有限 更适应多变和复杂的实际场景，具备较高的灵活性 优化机制 优化主要依赖工程师的经验调整，缺乏端到端反馈机制 利用强化学习的奖励反馈，实现持续自动化的性能提升 实现难度 实现相对直观，但需要繁琐的流程设计和维护 需要训练数据和计算资源，初期开发投入较大，但长期效果更佳 是否需要训练 无需额外训练，仅依赖手工构建的流程和 Prompt 需要通过强化学习对 Agent 进行训练，以实现自主决策 OpenAI Deep Research OpenAI Deep Research（OpenAI, 2025）是 OpenAI 于 2025 年 2 月正式发布的一款智能 Agent，专为复杂场景设计，能自动搜索、筛选、分析并整合多源信息，最终生成高质量的综合报告。该系统以 o3 为核心基座，并结合强化学习方法，显著提升了多轮迭代搜索和推理过程中的准确性与稳健性。\n相比传统的 ChatGPT 插件式搜索或常规 RAG 技术，OpenAI Deep Research 具有以下突出优势：\n强化学习驱动的迭代推理\n借助 o3 推理模型 与强化学习训练策略，Agent 能在多轮搜索与总结过程中持续优化自身推理路径，有效降低错误累积导致的失真风险。\n多源信息的整合与交叉验证\n突破单一搜索引擎的局限，能够同时调用特定数据库、专业知识库等多种权威数据源，通过交叉验证形成更可靠的研究结论。\n高质量的报告生成\n训练阶段引入 LLM-as-a-judge 评分机制和严格的评价标准，使系统在输出报告时能进行自我评价，从而生成结构更清晰、论证更严密的专业文本。\n训练过程 OpenAI Deep Research 训练过程采用了专为研究场景定制的浏览器交互数据集。通过这些数据集，模型掌握了核心浏览功能——包括搜索、点击、滚动和文件解析等操作；同时习得了在沙盒环境中使用 Python 工具进行计算、数据分析和可视化的能力。此外，借助在这些浏览任务上的强化学习训练，模型能够在海量网站中高效执行信息检索、整合与推理，快速定位关键信息或生成全面的研究报告。\n这些训练数据集既包含带有标准答案、可自动评分的客观任务，也包括配备详细评分量表的开放式任务。在训练过程中，模型的响应会与标准答案或评分标准进行严格对比，并利用模型产生 CoT 思考过程让评估模型提供反馈。\n同时，训练过程中复用了 o1 模型训练阶段积累的安全数据集，并专门增补了针对 Deep Research 场景的安全训练数据，确保模型在自动化搜索与浏览过程中严格遵守相关合规与安全要求。\n性能表现 在评估 AI 在各专业领域回答专家级问题能力的基准测试 Humanity\u0026rsquo;s Last Exam（Phan, et al. 2025）中，该模型取得了 SOTA 成绩。\nFig. 31. Humanity\u0026rsquo;s Last Exam Benchmark Results。(Image source: OpenAI, 2025)\n未来发展方向 智能体展现出广阔前景，但要实现可靠广泛应用，仍需解决以下关键挑战：\n上下文窗口限制：LLM 的上下文窗口限制了信息处理量，影响长期规划和记忆能力，降低任务连贯性。当前研究探索外部记忆机制和上下文压缩技术，以增强长期记忆及复杂信息处理能力。目前 OpenAI 最新的模型 GPT-4.5(OpenAI, 2025)最大上下文窗口为 128k tokens。\n接口标准化与互操作性：当前基于自然语言的工具交互存在格式不统一问题。模型上下文协议(Model Context Protocol, MCP)(Anthropic, 2024) 通过开放标准统一 LLM 与应用程序的交互方式，降低开发复杂性，提高系统稳定性和跨平台兼容性。\n任务规划与分解能力：智能体在复杂任务中难以制定连贯计划、有效分解子任务，并缺乏意外情况下的动态调整能力。需要更强大的规划算法、自我反思机制和动态策略调整方法，以灵活应对不确定环境。\n计算资源与经济效益：部署大模型智能体因多次 API 调用和密集计算而成本高昂，限制了一些实际应用场景。优化方向包括模型结构高效化、量化技术、推理优化、缓存策略及智能调度机制。随着专用 GPU 硬件比如 NVIDIA DGX B200 及分布式技术发展，计算效率有望显著提升。\n安全防护与隐私保障：智能体面临提示注入等安全风险，需建立健全的身份验证、权限控制、输入验证及沙箱环境。针对多模态输入与外部工具，需强化数据匿名化、最小权限原则和审计日志，以满足安全与隐私合规要求。\n决策透明与可解释性：智能体决策难以解释，限制了其在高风险领域的应用。增强可解释性需开发可视化工具、思维链追踪和决策理由生成机制，以提高决策透明度，增强用户信任，满足监管要求。\n参考文献 [1] DAIR.AI. \u0026ldquo;LLM Agents.\u0026rdquo; Prompt Engineering Guide, 2024.\n[2] Sutton, Richard S., and Andrew G. Barto. \u0026ldquo;Reinforcement Learning: An Introduction.\u0026rdquo; MIT Press, 2018.\n[3] Weng, Lilian. \u0026ldquo;LLM-powered Autonomous Agents.\u0026rdquo; Lil’Log, 2023.\n[4] Wei, Jason, et al. \u0026ldquo;Chain-of-thought prompting elicits reasoning in large language models.\u0026rdquo; Advances in neural information processing systems 35 (2022): 24824-24837.\n[5] Kojima, Takeshi, et al. \u0026ldquo;Large language models are zero-shot reasoners.\u0026rdquo; Advances in neural information processing systems 35 (2022): 22199-22213.\n[6] Wang, Xuezhi, et al. \u0026ldquo;Self-consistency improves chain of thought reasoning in language models.\u0026rdquo; arXiv preprint arXiv:2203.11171 (2022).\n[7] Wang, Xuezhi, et al. \u0026ldquo;Rationale-augmented ensembles in language models.\u0026rdquo; arXiv preprint arXiv:2207.00747 (2022).\n[8] Zelikman, Eric, et al. \u0026ldquo;Star: Bootstrapping reasoning with reasoning.\u0026rdquo; Advances in Neural Information Processing Systems 35 (2022): 15476-15488.\n[9] Fu, Yao, et al. \u0026ldquo;Complexity-based prompting for multi-step reasoning.\u0026rdquo; arXiv preprint arXiv:2210.00720 (2022).\n[10] Yao, Shunyu, et al. \u0026ldquo;Tree of thoughts: Deliberate problem solving with large language models.\u0026rdquo; Advances in neural information processing systems 36 (2023): 11809-11822.\n[11] Yao, Shunyu, et al. \u0026ldquo;React: Synergizing reasoning and acting in language models.\u0026rdquo; International Conference on Learning Representations (ICLR). 2023.\n[12] Shinn, Noah, et al. \u0026ldquo;Reflexion: Language agents with verbal reinforcement learning.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 8634-8652.\n[13] Guo, Daya, et al. \u0026ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\u0026rdquo; arXiv preprint arXiv:2501.12948 (2025).\n[14] OpenAI. \u0026ldquo;Introducing OpenAI o1\u0026rdquo; OpenAI, 2024.\n[15] Zhang, Zeyu, et al. \u0026ldquo;A survey on the memory mechanism of large language model based agents.\u0026rdquo; arXiv preprint arXiv:2404.13501 (2024).\n[16] Zhang, Chaoyun, et al. \u0026ldquo;Large language model-brained gui agents: A survey.\u0026rdquo; arXiv preprint arXiv:2411.18279 (2024).\n[17] Wang, Weizhi, et al. \u0026ldquo;Augmenting language models with long-term memory.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 74530-74543.\n[18] Schick, Timo, et al. \u0026ldquo;Toolformer: Language models can teach themselves to use tools.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 68539-68551.\n[19] Shen, Yongliang, et al. \u0026ldquo;Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 38154-38180.\n[20] Park, Joon Sung, et al. \u0026ldquo;Generative agents: Interactive simulacra of human behavior.\u0026rdquo; Proceedings of the 36th annual acm symposium on user interface software and technology. 2023.\n[21] He, Hongliang, et al. \u0026ldquo;WebVoyager: Building an end-to-end web agent with large multimodal models.\u0026rdquo; arXiv preprint arXiv:2401.13919 (2024).\n[22] Yang, Jianwei, et al. \u0026ldquo;Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v.\u0026rdquo; arXiv preprint arXiv:2310.11441 (2023).\n[23] OpenAI. \u0026ldquo;Introducing Operator.\u0026rdquo; OpenAI, 2025.\n[24] OpenAI. \u0026ldquo;Computer-Using Agent.\u0026rdquo; OpenAI, 2025.\n[25] OpenAI. \u0026ldquo;Introducing Deep Research.\u0026rdquo; OpenAI, 2025.\n[26] Phan, Long, et al. \u0026ldquo;Humanity\u0026rsquo;s Last Exam.\u0026rdquo; arXiv preprint arXiv:2501.14249 (2025).\n[27] OpenAI. \u0026ldquo;Introducing GPT-4.5.\u0026rdquo; OpenAI, 2025.\n[28] Anthropic. \u0026ldquo;Introducing the Model Context Protocol.\u0026rdquo; Anthropic, 2024.\n[29] LangGraph. \u0026ldquo;A workflow of the LangGraph.\u0026rdquo; LangGraph Tutorials, 2025.\n[30] Assaf Elovic. \u0026ldquo;GPT Researcher\u0026rdquo; GitHub Repository, 2025.\n[31] LangChain. \u0026ldquo;Open Deep Research\u0026rdquo; GitHub Repository, 2025.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui.(Mar 2025). 大语言模型智能体. https://syhya.github.io/zh/posts/2025-03-21-llm-agent\nOr\n@article{syhya2025llm-agent, title = \u0026#34;大语言模型智能体\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Mar\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-03-21-llm-agent\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-03-21-llm-agent/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e注意\u003c/strong\u003e: 本文\u003cstrong\u003e正在更新中\u003c/strong\u003e，内容只是\u003cstrong\u003e草稿版本\u003c/strong\u003e，并不完善，后续会有变动。请随时关注最新版本。\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"智能体\"\u003e智能体\u003c/h2\u003e\n\u003ch3 id=\"大语言模型智能体\"\u003e大语言模型智能体\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e大语言模型智能体(Large Language Model Agent, LLM agent)\u003c/strong\u003e 利用 LLM 作为系统大脑，并结合规划、记忆与外部工具等模块，实现了对复杂任务的自动化执行。\u003c/p\u003e","title":"大语言模型智能体（长期更新中）"},{"content":"背景 最近大模型的参数数量不断攀升，从最初的数十亿扩展到如今数千亿乃至数万亿级别。大模模型虽然带来了前所未有的应用效果，但与此同时，也引发了计算资源、内存管理和训练稳定性等一系列严峻挑战。因此本博客总结了一些常用分布式并行训练和内存管理技术，希望能够帮助大家更好地训练和优化大模型。\n大模型的训练挑战 参数规模爆炸式增长\n随着对模型容量和性能的不断追求，神经网络的参数数量呈现出指数级增长。现今从百万级到数十亿、数千亿甚至数万亿参数的模型层出不穷。例如，Llama 3.1 405B 拥有约 4,050 亿参数，而据传 GPT-4 的参数量可能达到 1.7 万亿级别。这种庞大的参数规模使得计算和内存需求急剧上升，给训练过程带来了前所未有的压力。\n计算复杂度剧增\n参数数量的急速增加直接导致整体计算复杂度大幅上升。训练一次大型模型可能需要耗费数周甚至数月的时间，即便采用大规模高性能 GPU 集群，训练周期仍难以令人满意，从而严重制约了模型迭代速度和研究效率。\n内存瓶颈日益凸显\n除了需要存储庞大的模型参数之外，大模型在训练过程中还必须保存中间激活值、梯度信息以及优化器状态，这些数据对 GPU 显存构成了巨大挑战。即使是配备 A100、H100(80GB 显存)、H200(141GB 显存)或 GB200(384GB 显存)等高端 GPU，单卡内存往往也难以满足数千亿甚至数万亿级模型的需求，“Out of Memory(OOM)”错误频发。\n通信开销成为瓶颈\n在多 GPU 分布式训练环境中，节点间需要频繁进行数据同步(如梯度汇总)。随着模型规模和 GPU 数量的增加，这种通信量急剧上升，即使在高带宽网络中，All-Reduce 操作传输海量数据也会消耗大量时间，成为整体并行效率的主要瓶颈之一。\n训练稳定性挑战\n超大规模模型在训练过程中更容易遭遇梯度消失或梯度爆炸问题，导致训练过程不稳定、难以收敛。虽然混合精度训练可以在一定程度上加速训练并降低显存占用，但同时也可能引入新的数值稳定性问题，要求研究人员投入更多精力进行细致调优。\n分布式训练的必要性 面对上述挑战，分布式训练技术成为支撑大模型训练的关键方案。通过将训练任务拆分并分配到多台 GPU 或计算节点上，分布式训练能够充分利用并行计算和集群内存资源，从而突破单 GPU 的局限，主要优势体现在以下几个方面：\n突破单 GPU 算力限制\n单个 GPU 的计算能力终究有限，无法应对万亿级别参数模型的庞大计算需求。借助数据并行与模型并行技术，训练任务可以均匀分布至多个 GPU，从而大幅缩短整体训练时间。\n克服单 GPU 内存瓶颈\n通过将模型参数、中间激活值和优化器状态分散存储在多个 GPU 显存中，分布式训练有效扩展了可用内存容量。典型技术如 ZeRO，通过对模型参数、梯度以及优化器状态进行分片，大幅降低单卡的显存负担，使得超大规模模型的训练成为可能。\n加速模型迭代与研发周期\n分布式训练的高并行度使得原本需要数周甚至数月完成的训练任务有望在数天内完成，从而大幅提升模型迭代速度，使得新架构和新策略能够更快得到验证与应用。\n支持更大规模模型探索\n分布式训练为探索更大规模、更复杂的神经网络架构提供了坚实基础。正是在这种技术支持下，万亿参数级别的模型(如 Switch Transformer)才得以成功训练并投入实际应用。\n提高训练系统的鲁棒性与可扩展性\n分布式系统具备出色的容错能力，当某个 GPU 节点出现故障时，其他节点可迅速接管任务，确保训练过程不被中断。同时，集群规模可以根据具体需求灵活扩展或缩减，满足不同规模模型的训练要求。\n并行训练 下图直观展示了多种并行训练策略的不同之处。不同颜色代表不同的模型层(例如三层)，虚线将不同的 GPU 区分开。从左到右分别是数据并行、模型并行(含流水线并行和张量并行)以及专家并行(MoE)。\nFig. 1. An illustration of various parallelism strategies on a three-layer model. Each color refers to one layer and dashed lines separate different GPUs. (Image source: OpenAI Blog, 2022)\n数据并行\n完整模型会被拷贝到每个 GPU 上，数据集则被切分为不同批次分配给各个 GPU 并行计算，最终在参数更新时聚合所有 GPU 的梯度。\n模型并行\n将模型划分到不同的 GPU 上，每个 GPU 只负责模型的一部分计算；可进一步分为以下两类：\n流水线并行：按层(垂直方向)拆分模型，不同 GPU 负责不同的层，通过微批次(micro-batch)在流水线中传递来并行执行前向和反向计算。 张量并行：在层内(水平方向)对大规模张量操作(如大矩阵乘法)进行切分，各 GPU 并行完成这部分运算并在必要时进行聚合。 专家并行\n通过门控策略，让每个输入样本只经过部分专家(子网络)，从而将整个模型按“专家模块”分布到不同 GPU。常见于 Mixture-of-Experts(MOE) 结构，可实现超大参数规模但推理/训练时仅激活部分专家。\n下面我将对多种并行方式进行详细说明。\n数据并行 Fig. 2. Data Parallelism. (Image source: Clolossal-AI Documentation)\n在深度学习训练中，数据并行(Data Parallelism, DP) 是最常用的并行策略，其核心思路是：\n复制模型参数：在每个计算设备(通常是 GPU)上都放置一份完整的模型参数。 划分训练数据：将大规模的数据集按样本维度拆分为多个子集，不同子集分配给不同的 GPU 进行处理。 局部前向与反向传播：每个 GPU 独立计算损失及对应的局部梯度。 梯度/参数同步：将各 GPU 的梯度聚合后更新模型参数，保证在每一次迭代后所有 GPU 的模型副本保持一致。 下面展示了 数据并行 工作流程：\n数据集划分 将训练数据集 $D$ 划分为 $N$ 个互不重叠的子集 ${D_1, D_2, \\dots, D_N}$，其中 $N$ 是 GPU 数量。通常会确保各子集大小相近，以实现负载均衡。\n模型复制 在每个 GPU 上复制一份完整的模型参数 $\\theta$。在训练开始时，这些参数在各 GPU 上都是相同的。\n数据分发 将子集 $D_i$ 分发给第 $i$ 张 GPU，让其在本地存储并供后续计算使用。\n局部前向传播 每个 GPU 基于其本地数据子集 $D_i$ 做前向传播，得到局部损失 $L_i(\\theta, D_i)$。\n局部反向传播 每个 GPU 基于局部损失 $L_i$ 进行反向传播，计算局部梯度\n$$ g_i = \\nabla_{\\theta} L_i(\\theta, D_i). $$ 梯度同步 各 GPU 之间执行梯度同步(常用 All-Reduce)，将所有局部梯度 ${g_1, g_2, \\ldots, g_N}$ 汇总得到全局平均梯度\n$$ \\bar{g} = \\frac{1}{N} \\sum_{i=1}^{N} g_i. $$ 参数更新 每个 GPU 使用全局平均梯度 $\\bar{g}$ 更新本地模型参数：\n$$ \\theta \\leftarrow \\theta - \\eta \\cdot \\bar{g}, $$其中 $\\eta$ 为学习率(learning rate)。\n迭代循环 重复步骤 4 - 7，直至模型达到收敛或达到预设的训练轮数(epochs)。\n批量同步并行与异步并行 在上面的第 6 步“梯度同步”中，如何以及何时进行“同步”是影响数据并行性能和收敛行为的重要因素之一。一般分为以下两大类：\n批量同步并行(Bulk Synchronous Parallel, BSP) 是数据并行中最常见、也是最易理解的同步模式。其特点可概括为「在每一次小批量(mini-batch)迭代结束后，全局同步一次梯度并更新参数」。具体流程：\n局部计算：各 GPU 基于其数据子集 $D_i$ 分别做前向与反向传播，得到局部梯度 $g_i$。 全局通信：所有 GPU 同步(如通过 All-Reduce)计算 $\\bar{g}$。 参数更新：每个节点均使用 $\\bar{g}$ 更新本地参数副本 $\\theta$。 等待与下一步迭代：所有节点完成上述操作后，再进入下一个迭代。 异步并行(Asynchronous Parallel, ASP) 旨在摆脱 BSP 的全局同步点，让各节点独立进行计算和参数更新。其典型实现是「参数服务器」(Parameter Server, PS) 架构下的 异步 push-pull 过程：\n各节点在本地计算得到梯度 $g_i$, 然后 push 到参数服务器； 参数服务器一旦收到梯度，立即更新全局模型参数； 其他节点在需要最新参数时，会 pull 下来继续下一步计算。 BSP vs. ASP 下表总结了在数据并行环境下，同步并行与异步并行的主要差异：\n对比维度 同步并行(BSP) 异步并行(ASP) 参数更新时机 每个小批量或一定迭代后，全局同步一次 各节点独立更新参数，无需与他人保持同一时间步 收敛稳定性 高。使用的梯度均为最新，收敛路径可控，易于分析 较低。存在过时梯度，收敛速率与稳定性可能受影响 通信需求 高度依赖 All-Reduce，同步时所有节点都需要等待和交换数据 每个节点向参数服务器异步推送/拉取，通信更为灵活，但参数服务器可能成为瓶颈 硬件资源利用 若有慢节点或网络延迟，则其他节点需等待，资源利用率可能降低 无需等待慢节点，可高效使用计算资源 实现复杂度 相对较低，主流框架(PyTorch DDP、Horovod 等)有内置支持 相对更高，需要参数服务器等组件，需处理更多的同步逻辑与数据一致性 适用场景 同构硬件、网络带宽良好、追求较高收敛质量 异构硬件、网络不稳定或带宽较低、需要极高的吞吐量且能容忍一定收敛风险 典型实现 PyTorch DDP、TensorFlow MirroredStrategy Parameter Server 架构(MXNet、TensorFlow ParameterServer 模式等) 建议：在实际项目中，先从简单的同步并行(BSP) 入手，利用 PyTorch DDP 或类似工具进行多 GPU 训练。若网络环境异构、节点繁多或任务对吞吐率要求极高，可再尝试异步并行(ASP) 或参数服务器方案，并配合梯度累积(Gradient Accumulation) 来平衡带宽与更新频率。\n梯度累积 当批量大小较大或通信成为主要瓶颈时，可以采用 梯度累积(Gradient Accumulation) 来减少同步频率。其核心思路是：\n连续计算多个小批量(mini-batch)的局部梯度，并将它们累加到本地的累积缓冲区中； 当累积的 mini-batch 数量达到 $K$ 时，再触发一次全局梯度同步与参数更新。 设第 $j$ 个 mini-batch 的梯度为 $g_j$，则在一个「累积周期」内得到\n$$ G = \\sum_{j=1}^{K} g_j. $$再用学习率 $\\eta$ 更新：\n$$ \\theta \\leftarrow \\theta - \\eta \\cdot G. $$由于梯度同步不再是每个 mini-batch 都进行，而是每累计 $K$ 个 mini-batch 执行一次，通信开销可显著降低。但参数更新频率降低也可能导致训练收敛速度放缓，需在吞吐量与收敛性能之间做权衡。\n分布式数据并行 分布式数据并行(Distributed Data Parallel, DDP) 是 PyTorch v1.5(Li et al. 2020)在 BSP 思想下的高度优化实现，为单机多 GPU 乃至多机多 GPU 的数据并行提供便利。其主要优化包括：\n梯度 Bucketing(梯度桶化)：将模型参数分为多个「桶」(bucket)；反向传播时一旦某个桶内所有梯度都已计算完，就立即启动一次针对该桶的 All-Reduce，而不是等到所有梯度都算完后再一次性同步。 通信与计算重叠：DDP 通过异步通信和非阻塞操作，尽可能地将梯度同步(通信)与前向传播、反向传播(计算)重叠，从而减少了通信开销。这种重叠策略提升了整体的并行效率。 梯度累积：DDP 也能方便地与梯度累积相结合，结合使用，通过增加每次同步的梯度更新间隔，从而减少同步频率。这在大规模分布式训练中有助于进一步降低通信开销，提高训练效率。 Fig. 3. Pseudo code for Pytorch DDP. (Image source: Li et al. 2020)\nRing All-Reduce 在多 GPU(尤其是单机多 GPU)环境下，若有高速互联(如 NVLink、PCIe 交换机等)，可使用 Ring All-Reduce 来显著降低通信开销。其思路是：\n将 $k$ 个节点组织成一个环，并把梯度向量等分成 $k$ 份。 在「加和阶段」，每个节点分别向下一个节点发送其本地的一部分梯度，并与收到的梯度相加；该过程循环若干次后，每个节点会持有完整的「聚合后」梯度。 在「广播阶段」，再将最终结果沿环路分发给所有节点。 理想情况下，Ring All-Reduce 的通信代价与节点数量近似无关(可以视为 $\\mathcal{O}(1)$)，非常适合多 GPU 环境下的梯度同步，是 Horovod、NCCL 等库中广泛使用的核心通信模式。\n参数服务器 当集群规模扩展至多机多 GPU 时，若简单地采用单点聚合(例如一台中心服务器)往往难以支撑海量数据的并行训练。参数服务器(Parameter Server, PS)(Li, et al. 2014)是为可扩展分布式训练而设计的一种典型架构：\n参数分片：将模型参数按键值对(key-value) 的形式进行拆分，不同 PS 节点只管理特定分片的参数； push-pull 语义：计算节点在本地得到梯度后，push 到相应的 PS；PS 更新完该分片参数后，计算节点可在需要时 pull 下最新版本进行下一步计算。 灵活容错与扩展：通过增加或移除 PS 节点，可在带宽或计算需求上灵活扩容；在 PS 上也能实现备份与容错策略。 这种 PS + Worker 模式可以同时结合数据并行和模型并行，将超大模型拆分到多个 PS 上存储，并对超大数据进行分布式训练。PS 本身也能根据负载情况做拆分与合并，形成更加复杂的层次化拓扑结构。\n模型并行 模型并行(Model Parallelism, MP) 是一种将模型本身分割到多个计算设备(GPU) 上进行训练的并行方式。当模型参数规模超过单个 GPU 的内存容量时，模型并行成为必要的选择。模型并行主要分为两种类型：流水线并行(Pipeline Parallelism) 和张量并行(Tensor Parallelism)。\n朴素模型并行与气泡问题\nFig. 4. A naive model parallelism setup where the model is vertically split into 4 partitions. Data is processed by one worker at a time due to sequential dependency, leading to large “bubbles” of idle time. (Image source: Huang et al. 2018)\n朴素的模型并行实现，即将模型简单地按层划分，并顺序地在不同 GPU 上执行，会遇到严重的 \u0026ldquo;气泡\u0026rdquo;(bubble) 问题。由于层之间的依赖关系，当一个 GPU 在处理某个数据样本的某个阶段时，其他 GPU 可能处于空闲状态，等待前一个 GPU 的输出或者后一个 GPU 的输入。这种 GPU 空闲时间被称为 \u0026ldquo;气泡\u0026rdquo;，严重降低了流水线并行的效率。\n其中，$F_i$ 表示 Stage $i$ 的前向传播，$B_i$ 表示 Stage $i$ 的反向传播。可以看到，在朴素流水线并行中，大部分时间只有一个 GPU 在工作，其他 GPU 处于空闲状态，效率低下。\n气泡问题产生的原因:\n层间依赖: 神经网络的层之间存在顺序依赖关系，后一层的计算必须依赖于前一层的输出。 顺序执行: 朴素模型并行按照层顺序依次执行，导致 GPU 之间无法充分并行工作。 流水线并行 Fig. 5. Pipeline Parallelism. (Image source: Clolossal-AI Documentation)\n流水线并行(Pipeline Parallelism, PP) 将模型按层划分为多个阶段(stage)，每个阶段分配到一个 GPU 上。数据像流水线一样在不同 GPU 之间传递，前一个 GPU 的输出作为后一个 GPU 的输入。流水线并行旨在提高模型并行训练的效率，减少 GPU 空闲时间。\nGPipe GPipe(Huang et al. 2018) 是 Google 提出的一个高效的流水线并行训练系统，旨在解决朴素流水线并行的气泡问题。GPipe 的核心思想是将 mini-batch 划分为多个 micro-batch，并采用同步梯度聚合的方式来缓解气泡问题，提高流水线效率。\nFig. 6. Illustration of pipeline parallelism in GPipe with 4 microbatches and 4 partitions. GPipe aggregates and updates gradients across devices synchronously at the end of every batch. (Image source: Huang et al. 2018)\n以下是 GPipe 调度策略：\nMicro-batch 划分: 将一个 mini-batch 划分为 $m$ 个 micro-batch。划分后的每个 micro-batch 的大小为原 mini-batch 的 $1/m$。 流水线阶段划分: 将模型按层划分为 $d$ 个阶段，每个阶段分配到一个 GPU 上。 流水线执行: 依次处理每个 micro-batch，在流水线中进行前向和反向传播。具体流程如下： 前向传播(Forward Propagation): 对于每个 micro-batch，依次在 Stage $1$, Stage $2$, \u0026hellip;, Stage $d$ 上进行前向传播。Stage $i$ 的输出作为 Stage $i+1$ 的输入。 反向传播(Backward Propagation): 当所有 micro-batch 的前向传播都完成后，开始反向传播。对于每个 micro-batch，依次在 Stage $d$, Stage $d-1$, \u0026hellip;, Stage $1$ 上进行反向传播。Stage $i$ 的梯度作为 Stage $i-1$ 的输入。 同步梯度聚合(Synchronous Gradient Aggregation): 在所有 micro-batch 的反向传播都完成后，将所有 micro-batch 的梯度进行聚合(例如求平均)，得到全局平均梯度。 参数更新(Parameter Update): 每个 GPU 使用全局平均梯度更新本地模型参数。 GPipe 气泡比例公式 假设每个 micro-batch 的前向和反向传播时间均为 1 单位，流水线深度为 $d$，micro-batch 数量为 $m$，则 GPipe 的气泡比例为：\n$$ \\text{Bubble Ratio} = 1 - \\frac{2md}{(2m + 2(d-1))d} = \\frac{d-1}{m+d-1} $$当 micro-batch 数量 $m$ 远大于流水线深度 $d$ 时($m \\gg d$)，气泡比例趋近于 0，流水线效率接近线性加速。GPipe 论文中指出，当 $m \u0026gt; 4d$ 时，气泡开销几乎可以忽略不计(在激活重计算的情况下)。因此有以下好处：\n减少气泡: GPipe 通过 micro-batch 划分和流水线调度，显著减少了朴素流水线并行的气泡问题，提高了 GPU 利用率和训练效率。 同步梯度聚合: GPipe 采用同步梯度聚合，保证了训练过程的同步性，模型收敛性较好。 线性加速潜力: 在 micro-batch 数量足够大的情况下，GPipe 可以实现接近线性的加速效果。 PipeDream Fig. 7. Illustration of 1F1B microbatch scheduling in PipeDream. (Image source: Harlap et al. 2018)\nPipeDream(Harlap et al. 2018)是另一种高效的流水线并行训练系统，它采用了 1F1B(1-Forward-1-Backward) 调度策略，并引入了权重暂存(Weight Stashing) 技术，进一步减少气泡，提高流水线效率，并解决 1F1B 调度可能导致的权重版本不一致问题。\nPipeDream 的 1F1B 调度策略的核心思想是，每个 GPU(Stage) 交替执行前向传播和反向传播，尽可能地并行工作，减少 GPU 空闲时间。具体流程如下：\nMicro-batch 划分: 将一个 mini-batch 划分为 $m$ 个 micro-batch。 流水线阶段划分: 将模型按层划分为 $d$ 个阶段，每个阶段分配到一个 GPU 上。 1F1B 调度执行: 每个 GPU 轮流执行前向传播和反向传播。 权重暂存 由于 1F1B 调度中，前向传播和反向传播可能使用不同版本的模型权重，会导致权重版本不一致问题，影响训练的正确性和收敛性。PipeDream 引入了权重暂存(Weight Stashing)技术来解决这个问题。权重暂存的核心思想是，每个 GPU 维护多个版本的模型权重，并确保前向传播和反向传播使用同一版本的权重。\n权重暂存实现方式:\n版本管理: 每个 GPU 维护一个权重版本队列，存储多个版本的模型权重。 版本选择: 在进行前向传播时，选择当前最新的权重版本。在进行反向传播时，选择与对应前向传播相同的权重版本。 版本更新: 在完成一个 mini-batch 的所有 micro-batch 的反向传播后，更新模型权重，并生成新的权重版本。 为了进一步优化 PipeDream 的内存使用，尤其是在权重暂存方面，PipeDream 衍生出了 PipeDream-flush 和 PipeDream-2BW 两种内存优化变体。\nPipeDream-flush Fig. 8. Illustration of pipeline scheduling in PipeDream-flush. (Image source: Narayanan et al. 2020)\nPipeDream-flush 在 PipeDream 的基础上，周期性地进行全局同步的流水线刷新(flush)，类似于 GPipe 的同步梯度聚合。通过定期刷新，PipeDream-flush 可以大幅减少权重暂存所需的内存空间，只需维护单个版本的模型权重，但会牺牲少量吞吐量。\nPipeDream-2BW PipeDream-2BW(Double-Buffered Weights) 维护两个版本的模型权重，即 \u0026ldquo;双缓冲权重\u0026rdquo;。它每 $k$ 个 micro-batch 更新一次模型版本，其中 $k$ 大于流水线深度 $d$($k \u0026gt; d$). 新更新的模型版本不会立即完全替换旧版本，因为可能还有一些剩余的反向传播操作仍然依赖于旧版本。通过双缓冲权重，PipeDream-2BW 可以将权重暂存的内存开销降低到只维护两个版本的模型权重，显著减少内存占用。\nFig. 9. Illustration of pipeline scheduling in PipeDream-2BW. (Image source: Narayanan et al. 2020)\nPipeDream-2BW 策略有以下优点：\n更低的气泡开销: 1F1B 调度策略相比 GPipe 可以进一步减少气泡，提高 GPU 利用率和训练效率。 权重暂存解决版本一致性: 权重暂存技术保证了前向传播和反向传播使用同一版本的权重，解决了 1F1B 调度可能导致的权重版本不一致问题。 内存优化变体: PipeDream-flush 和 PipeDream-2BW 进一步优化了内存使用，降低了权重暂存的内存开销，使得流水线并行更适用于内存受限的场景。 张量并行 张量并行(Tensor Parallelism, TP) 是一种将模型中的张量(通常是权重矩阵) 沿着特定维度切分，并将切分后的分片分配到不同的 GPU 上进行计算的并行方式。张量并行有以下几点优势：\n突破单 GPU 显存限制: 张量并行可以将模型参数分散存储在多个 GPU 上，突破单 GPU 显存容量限制，支持训练更大规模的模型。 层内并行: 张量并行可以实现模型层内部的并行化，例如矩阵乘法操作的并行计算，提高计算效率。 与数据并行和流水线并行结合: 张量并行可以与数据并行和流水线并行等其他并行技术结合使用，形成多维并行策略，进一步提高训练效率和可扩展性。 Megatron-LM Megatron-LM(Shoeybi et al. 2019) 是 NVIDIA 提出的一个用于训练超大型语言模型的系统，它采用了张量并行技术，对 Transformer 模型层内部的矩阵乘法操作进行并行化，包括 self-attention 和 MLP 中的矩阵乘法。\nFig. 10. Illustration of tensor parallelism for key transformer components proposed in Megatron-LM. (Image source: Shoeybi et al. 2019)\nTransformer 的 MLP 层通常包含两个线性层，第一个线性层的计算可表示为 $Y = \\text{GeLU}(XA)$，其中 $X$ 是输入矩阵，$A$ 是权重矩阵，GeLU 是激活函数。Megatron-LM 将权重矩阵 $A$ 沿着列维度切分为 $P$ 个分片 $[A_1, A_2, \u0026hellip;, A_P]$，其中 $P$ 是 GPU 的数量。每个 GPU $i$ 负责存储和计算权重分片 $A_i$。\nMLP 层张量并行计算流程:\n$$ \\begin{aligned} \\text { Split } A \u0026 =\\left[A_1, A_2\\right] \\\\ Y \u0026 =\\operatorname{GeLU}(X A) \\\\ {\\left[Y_1, Y_2\\right] } \u0026 =\\left[\\operatorname{GeLU}\\left(X A_1\\right), \\operatorname{GeLU}\\left(X A_2\\right)\\right] \\end{aligned} $$ 权重分片: 将权重矩阵 $A$ 沿着列维度切分为 $P$ 个分片 $[A_1, A_2, \u0026hellip;, A_P]$，并将分片 $A_i$ 分配到 GPU $i$。 局部矩阵乘法: 每个 GPU $i$ 使用输入矩阵 $X$ 和权重分片 $A_i$ 进行矩阵乘法计算，得到局部输出 $Y_i = \\text{GeLU}(XA_i)$。 全局拼接(All-Gather): 所有 GPU 通过 All-Gather 操作，将局部输出 ${Y_1, Y_2, \u0026hellip;, Y_P}$ 拼接成完整的输出矩阵 $Y = [Y_1, Y_2, \u0026hellip;, Y_P]$。 自注意力层张量并行\nMegatron-LM 也对 Transformer 的自注意力层中的 Query($Q$), Key($K$), Value($V$) 权重矩阵进行张量并行切分，并进行相应的局部矩阵乘法和全局拼接操作，实现自注意力层的张量并行化。。自注意力层的计算公式为：\n$$ \\text{Attention}(X, Q, K, V) = \\text{softmax}\\left(\\frac{(XQ)(XK)^T}{\\sqrt{d_k}}\\right)XV $$PTD-P PTD-P(Pipeline, Tensor, and Data Parallelism)(Narayanan et al. 2021)是一个结合了流水线并行、张量并行和数据并行的多维并行策略。PTD-P 旨在充分利用各种并行技术的优势，提高超大型模型训练的效率和可扩展性。\nPTD-P 的特点:\n多维并行结合: PTD-P 同时使用了流水线并行、张量并行和数据并行三种并行技术，可以从多个维度对训练过程进行并行化。 Interleaved 1F1B 调度: PTD-P 采用了 interleaved 1F1B 调度策略，与传统的流水线并行不同，它将模型划分为多个不连续的层块(model chunk)，并将多个层块分配给每个 GPU。这种调度策略可以进一步减少气泡，提高流水线效率。 灵活的并行配置: PTD-P 允许用户根据模型结构和硬件资源灵活配置各种并行技术的组合方式，例如可以只使用张量并行和数据并行，也可以同时使用流水线并行、张量并行和数据并行。 传统的流水线并行通常将模型划分为连续的层块，每个 GPU 负责一个连续的层块。PTD-P 的 interleaved 1F1B 调度则将模型划分为多个不连续的层块，例如，GPU 1 负责层 1, 2, 9, 10，GPU 2 负责层 3, 4, 11, 12，依此类推。每个 GPU 负责多个不连续的层块，可以更有效地利用 GPU 资源，减少气泡开销。\nFig. 11.(Top) Default 1F1B pipeline schedule as in PipeDream-flush.(Bottom) Interleaved 1F1B pipeline schedule. First model chunks are in dark colors and second chunks are in light colors. (Image source: Narayanan et al. 2021)\n混合专家模型 混合专家模型(Mixture-of-Experts, MoE)(Shazeer et al. 2017) 是一种稀疏激活模型，它通过结合多个独立的“专家”网络和一个门控网络(Gating Network)，在不显著增加计算成本的前提下，大幅提升了模型的参数量和性能。MoE 的核心思想是稀疏激活(Sparse Activation)，即对于每个输入样本，仅激活部分专家网络，而不是整个模型。这种方法既提高了计算效率，又增强了模型的表达能力，使其在 LLMs 中表现出色。\nMoE 设计灵感来源于集成学习(Ensemble learning), 一种将复杂任务分解为多个子任务并由不同模型协作完成的技术。在 MoE 中，这些“子任务”由多个独立的专家网络处理，而门控网络则负责根据输入样本的特征动态选择最适合的专家。这种分工合作的机制类似于人类社会中的专家团队：不同领域的专家针对特定问题提供专业意见，最终综合得出结果。\nFig. 12. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)\nMoE 核心组件 一个典型的 MoE 包含以下组件：\n专家网络(Experts): 一组独立的神经网络 ${E_1, E_2, \u0026hellip;, E_n}$，每个专家网络 $E_i$ 可以是任意类型的神经网络，例如 FFN, CNN, RNN 等。专家网络的数量 $n$ 可以很大，例如几十个、几百个甚至几千个。 门控网络(Gating Network): 一个可训练的神经网络 $G$，用于根据输入样本 $x$ 学习一个概率分布，决定激活哪些专家。门控网络的输入是输入样本 $x$，输出是一个 $n$ 维的概率向量 $p = G(x) = [p_1, p_2, \u0026hellip;, p_n]$，其中 $p_i$ 表示激活专家 $E_i$ 的概率。 专家输出聚合(Expert Output Aggregation): 根据门控网络的输出概率分布，将激活的专家网络的输出进行加权求和，得到 MoE 层的最终输出 $y$。 Noisy Top-k Gating 为了实现稀疏激活并确保专家使用均衡，MoE 通常采用 Noisy Top-k Gating 作为门控机制。这种方法通过引入噪声和 top-k 选择，既保证了计算效率，又避免了专家负载不均的问题。以下是其详细工作流程：\n门控分数计算: 对于输入样本 $x$，门控网络首先计算每个专家的门控分数 $H^{(i)}(x)$。这一分数包含两部分：线性变换和噪声项，公式如下：\n$$ H^{(i)}(x) =(x W_g)^{(i)} + \\epsilon \\cdot \\text{softplus}\\left((x W_{\\text{noise}})^{(i)} \\right), \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$ 参数说明： $W_g \\in \\mathbb{R}^{d \\times n}$：门控网络的可训练权重矩阵，$d$ 是输入特征维度，$n$ 是专家数量。 $W_{\\text{noise}} \\in \\mathbb{R}^{d \\times n}$：用于生成噪声的权重矩阵。 $\\epsilon \\sim \\mathcal{N}(0, 1)$：标准高斯噪声，增加门控随机性。 $\\text{softplus}(x) = \\log(1 + e^x)$：平滑激活函数，确保噪声非负。 噪声的引入避免了门控网络总是选择固定的专家，增强了模型的鲁棒性和多样性。\nTop-k 选择: 计算出门控分数向量 $H(x) = [H^{(1)}(x), H^{(2)}(x), \\dots, H^{(n)}(x)]$ 后，门控网络选择其中值最大的前 $k$ 个专家(通常 $k \\ll n$)。这一步骤通过 $\\text{topk}(v, k)$ 函数实现：\n$$ \\text{topk}^{(i)}(v, k) = \\begin{cases} v^{(i)} \u0026 \\text{if } v^{(i)} \\text{ is in the top } k \\text{ elements of } v \\\\ -\\infty \u0026 \\text{otherwise} \\end{cases} $$将非 Top-k 专家的分数设为 $-\\infty$，确保后续 softmax 操作中这些专家的概率为 0，实现稀疏性。\nSoftmax 归一化: 对 Top-k 专家的门控分数进行 softmax 归一化，得到稀疏的概率分布 $G(x)$：\n$$ G(x) = \\text{softmax}\\left( \\text{topk}(H(x), k) \\right) $$只有 Top-k 个专家的概率非零，其余为 0。例如，若 $n=100, k=2$，则 98 个专家的概率为 0。\n加权求和: 将 Top-k 个专家的输出按概率加权求和，得到 MoE 层的输出：\n$$ y = \\sum_{i=1}^{n} G^{(i)}(x) E_i(x) $$由于只有 $k$ 个专家被激活，计算量远低于激活所有 $n$ 个专家。\n辅助损失 为了避免门控网络过度偏向少数专家，MoE 引入了辅助损失(Auxiliary Loss)(Shazeer et al. 2017)，鼓励所有专家被均匀使用。一种常用方法是基于专家使用率的变异系数(Coefficient of Variation, CV)的平方：\n$$ \\mathcal{L}_{\\text{aux}} = w_{\\text{aux}} \\cdot \\text{CV}\\left( \\sum_{x \\in X} G(x) \\right)^2 $$ 参数说明：\n$X$：一个 mini-batch 的输入样本。 $\\sum_{x \\in X} G(x)$：统计每个专家在 mini-batch 中的激活次数。 $\\text{CV}$：标准差与均值的比值，衡量专家使用分布的均匀性。 $w_{\\text{aux}}$：辅助损失的权重，需手动调整。 作用：通过最小化 $\\mathcal{L}_{\\text{aux}}$，模型优化专家选择的均衡性，避免某些专家被过度使用而其他专家闲置。\nGShard GShard(Lepikhin et al. 2020)主要对 MoE 层进行分片，将 MoE 层中的专家网络 ${E_1, E_2, \u0026hellip;, E_n}$ 分散到多个 TPU 设备上。例如，如果有 $P$ 个 TPU 设备，可以将专家网络划分为 $P$ 组，每组专家网络分配到一个 TPU 设备上。Transformer 模型的其他层(例如自注意力层、LayerNorm 层) 则在所有 TPU 设备上复制。\nGShard 的改进门控机制:\nGShard 在 Noisy Top-k Gating 的基础上，进行了一些改进，以提高门控机制的性能和稳定性：\n专家容量(Expert Capacity):\n为了避免专家过载，GShard 引入了专家容量限制。每个专家网络都有一个容量上限，表示它最多可以处理的 token 数量。如果一个 token 被路由到一个已经达到容量上限的专家网络，则该 token 会被标记为 \u0026ldquo;overflowed\u0026rdquo;，门控输出会被设置为零向量，表示该 token 不会被路由到任何专家网络。\n局部组分发(Local Group Dispatching):\n为了提高门控效率，GShard 将 token 分组，在组级别强制执行专家容量限制。例如，将 mini-batch 中的 token 划分为多个局部组，每个局部组包含一定数量的 token。门控网络为每个局部组选择 top-k 个专家网络，并确保每个专家网络在一个局部组内处理的 token 数量不超过其容量上限。\n辅助损失(Auxiliary Loss):\nGShard 也使用了辅助损失函数来平衡专家负载。与原始 MoE 模型的辅助损失不同，GShard 的辅助损失旨在最小化每个专家网络路由到的数据比例的均方误差，更加直接地衡量专家负载平衡程度。\n随机路由(Random Routing):\n为了增加路由的随机性，GShard 在选择 top-k 个专家网络时，引入了随机路由机制。除了选择最佳的 top-k 个专家网络外，GShard 还会以一定的概率随机选择次优的专家网络，增加专家网络的多样性，提高模型的泛化能力。\n下面是 GShard 的核心算法流程:\nFig. 13. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)\nSwitch Transformer Switch Transformer(Fedus et al. 2021) 是 Google 提出的一个参数量达到万亿级别的 MoE 模型。其核心创新是将 Transformer 模型中的密集前馈网络(FFN) 层替换为稀疏的 Switch FFN 层。与 GShard 的 Top-2 Gating 不同，Switch Transformer 每个输入 token 只路由到一个专家网络，具有更高的稀疏性，进一步降低了计算成本，使得训练万亿参数模型成为可能。鼓励 token 路由在 $N$ 个专家之间更加均衡。Switch Transformer 的辅助损失基于实际路由比例与预测路由概率的乘积累加，具体公式如下：\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ 参数说明： $N$：专家的总数。\n$f_i$：路由到第 $i$ 个专家的 token 比例，定义为：\n$$ f_i = \\frac{1}{T} \\sum_{x \\in B} 1\\{\\text{argmax } p(x) = i\\} $$ $P_i$：gating 网络预测的第 $i$ 个专家的路由概率，定义为：\n$$ P_i = \\frac{1}{T} \\sum_{x \\in B} p_i(x) $$ $T$：批次 $B$ 中的 token 总数。\n$\\alpha$：辅助损失的权重超参数，通常设为 $10^{-2}$。\n通过最小化 $\\text{loss}$，模型使实际路由比例 $f_i$ 与预测概率 $P_i$ 趋于一致，从而间接促进专家间的负载平衡，避免部分专家闲置。\nFig. 14. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)\nSwitch Router 机制:\n路由预测:\n对于输入 token $x$，Switch Router 预测每个专家网络的路由概率 $p_i = G^{(i)}(x)$，其中 $i = 1, 2, \u0026hellip;, n$，n 是专家网络数量。\n专家选择:\n选择路由概率最高的专家网络作为最佳专家网络。Switch Transformer 采用 Top-1 路由策略，即每个 token 只路由到路由概率最高的专家网络。\ntoken 路由:\n将输入 token $x$ 路由到选择的最佳专家网络进行处理。\nSwitch Transformer 的训练稳定性优化:\n为提升 Switch Transformer 的训练稳定性，论文提出了如下优化策略：\n选择性精度(Selective Precision)\n在路由函数内部采用 FP32 精度既能提高训练稳定性，又能避免因 FP32 张量通信而产生的额外开销。具体来说，Switch Router 的计算过程全程使用 FP32，最终结果再转换为 FP16 以兼顾效率与精度。\n更小初始化(Smaller Initialization)\n建议将 Transformer 的权重初始化尺度参数 $s$ 从 1 调整至 0.1。较小的初始化尺度有助于缓解训练初期的梯度爆炸风险，从而提升整体训练稳定性。具体实现为：从均值为 0、标准差为 $\\sqrt{s/n}$(其中 $n$ 为输入单元数) 的截断正态分布中采样。\n更高专家 Dropout(Higher Expert Dropout)\n在专家 FFN 层中采用较高的 dropout 率(例如 0.4)，而在非专家层则保持较低的 dropout 率(例如 0.1)，这种设置能有效防止过拟合，进而增强模型的泛化能力。下图实验结果显示，在 GLUE、CNNDM、SQuAD 和 SuperGLUE 等任务上，当专家层 dropout 率设为 0.4 时，模型表现最佳。\nFig. 15. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: Fedus et al. 2021)\nSwitch Transformers 论文中使用下图直观的展示了使用不同的并行技术如何分割模型权重和数据:\nFig. 16. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)\n专家选择 专家选择(Expert Choice, EC)(Zhou et al. 2022) 是一种与 token 选择路由(如 GShard 的 top-2 或 Switch Transformer 的 top-1)相反的路由策略。在 token 选择路由中，每个 token 从所有专家中选择 top-k 个进行路由；而在专家选择路由中，每个专家从所有 token 中挑选 top-k 个进行处理。这种方法旨在解决 token 选择路由中的负载不均和 token 浪费问题，同时显著提高训练效率。下面是具体的计算过程：\n计算 token-to-expert 亲和度分数\n对于输入矩阵 $X \\in \\mathbb{R}^{n \\times d}$，计算 token-to-expert 亲和度分数矩阵 $S \\in \\mathbb{R}^{n \\times e}$ 的过程为：\n$$ S = \\text{softmax}(X \\cdot W_g), \\quad \\text{where } W_g \\in \\mathbb{R}^{d \\times e}. $$ 这里，$W_g$ 为门控权重矩阵，$e$ 为专家数量。\n专家选择 token\n每个专家从所有 token 中选择 top-k 个进行处理。通过对 $S^T$ 进行 top-k 选择：\n$$ G, I = \\text{top-}k(S^T, k), $$得到：\n门控矩阵 $G \\in \\mathbb{R}^{e \\times k}$： 记录专家选择的 token 对应的路由权重，其中 $G[i, j]$ 表示专家 $i$ 选择的第 $j$ 个 token 的权重； token 索引矩阵 $I \\in \\mathbb{R}^{e \\times k}$： 表示每个专家选择的 token 在输入中的索引。 One-hot 编码\n将 token 索引矩阵 $I$ 转换为 one-hot 编码矩阵 $P \\in \\mathbb{R}^{e \\times k \\times n}$，用于后续计算：\n$$ P = \\operatorname{one}-\\operatorname{hot}(I) $$ 构造 Gated FFN 层输入\n对于每个专家 $i$，其 gated FFN 层的输入为：\n$$ (P \\cdot X) \\in \\mathbb{R}^{e \\times k \\times d}. $$ EC 通过正则化限制每个 token 被路由到的专家数量，从而控制模型的稀疏性。一个常见的正则化目标如下：\n$$ \\begin{aligned} \u0026 \\max_{A} \\langle S^{\\top}, A \\rangle + \\lambda H(A) \\\\ \u0026 \\text{s.t. } \\forall i: \\sum_{j'} A[i, j'] = k, \\quad \\forall j: \\sum_{i'} A[i', j] \\leq b, \\quad \\forall i,j: 0 \\leq A[i, j] \\leq 1, \\end{aligned} $$考虑的优化问题中定义了一个矩阵 $A$，其第 $i$ 行第 $j$ 列的元素表示第 $i$ 个专家是否选择了第 $j$ 个 token(取值 0 或 1)。由于该优化问题求解较为复杂，论文中采用 Dykstra 算法(通过多次迭代获得近似解)来解决。\n参数 $b$ 通常由批量中 token 总数 $n$ 与容量因子决定，其中容量因子表示每个 token 平均使用的专家数量。大多数实验采用较高的容量因子，实验结果表明，即使在容量降低的情况下，EC(Expert Choice)整体表现仍优于传统的 top-1 token 选择路由，尽管 capped expert choice 略微降低了微调性能。\nEC 的优势主要体现在以下两方面：\n完美负载均衡： 每个专家固定处理 $k$ 个 token，从而避免了部分专家过载而其他专家闲置的问题，实现了理想的负载均衡。 更高训练效率： 实验表明，EC 能将训练收敛速度提升约 2 倍，相较于传统 token 选择路由具有更高的效率。 但 EC 也存在以下局限性：\n批量大小要求： 由于 EC 对 batch size 有较高要求，因此不适用于较小 batch size 的场景。 自回归生成限制： 在自回归文本生成任务中，由于无法预知未来 token，EC 的 top-k 选择无法实现，因此不适用于此类任务。 序列并行 序列并行(Sequence Parallelism, SP) 是针对长序列模型(如 Transformer)提出的一种并行化策略，通过在序列维度上对输入进行划分，大幅降低激活内存占用并提高训练效率。它常与数据并行、张量并行或流水线并行结合使用，尤其适合处理超长文本或其他序列数据。\nColossal-AI 序列并行 Fig. 17. The overall architecture of the proposed sequence parallelism and existing parallel approaches. For sequence parallelism, Device 1 and Device 2 share the same trainable parameters. (Image source: Li, et al. 2021)\n自注意力(self-attention) 的计算复杂度和内存开销与序列长度 $s$ 的平方 $O(s^2)$ 成正比，长序列数据将增加中间 activation 内存使用量，从而限制设备的训练能力。Colossal-AI 序列并行(Li, et al. 2021)从系统角度提出拆分超长序列到多卡，具体的解决步骤如下。\n序列分块\n将输入序列划分为若干块，每个块由不同 GPU 保存和计算；因此每张卡只需存储自己对应的序列块激活，避免单卡内存爆炸。 环状通信 + 自注意力\n提出环自注意力(Ring Self-Attention, RSA) 机制：各 GPU 先本地计算局部注意力，然后依次向相邻 GPU 传递(环状结构)Key/Value 分块，多轮迭代后保证每个 GPU 能获取全局序列信息。 与其他并行方式结合\n不受注意力头数、层数等超参数限制，可配合数据并行、张量并行、流水线并行等技术，共同突破大规模模型的序列长度限制。 Fig. 18. Ring Self-Attention. (Image source: Li, et al. 2021)\nMegatron-LM 序列并行 Megatron-LM(Shoeybi et al. 2019) 原本使用张量并行分担部分激活值，但 Transformer 中的 LayerNorm、Dropout 等操作的激活值仍需完整保存在单卡，显存消耗依旧庞大。因此 NVIDIA 提出 Megatron-LM 序列并行(Korthikanti, et al. 2022)在序列维度对这些激活值进行切分，大幅降低占用。\nFig. 19. Transformer layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)\nFig. 20. MLP layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)\n序列维度切分\n针对 LayerNorm、Dropout 等难以在张量维度切分的激活，将其沿序列维度划分，使每个 GPU 只处理一部分序列的非线性操作。 张量并行仍保留\n注意力(Attention)、MLP 等线性操作继续使用张量并行；序列并行的激活需要在前后进行对应的 All-Gather 或 Reduce-Scatter 以交换数据。 选择性激活重计算(Selective Activation Recomputation)\n针对部分计算量小但激活量大的操作，选择在反向传播时临时重算，以进一步节省显存。 DeepSpeed-Ulysses 序列并行 DeepSpeed-Ulysses(Jacobs et al. 2023) 针对超长序列训练提出了一种高效的序列并行方案，通过在序列维度对输入进行划分，并结合两阶段的全对全通信，有效降低通信量和激活内存，从而支持训练百万 token 级别的长序列 Transformer 模型。\nFig. 21. DeepSpeed sequence parallelism(DeepSpeed-Ulysses) design. (Image source: Jacobs et al. 2023)\n序列划分 + 全对全通信\n将输入序列沿序列维度划分到 $P$ 张 GPU 上，每个 GPU 只处理局部 $N/P$ 的序列；在注意力计算前，通过 All-to-All 操作交换查询($Q$)、键($K$)和值($V$)，使得每个 GPU 获得完整序列信息，但仅计算分配到的注意力头。\n双阶段通信优化\n第一次 All-to-All： 在注意力计算前对 $Q$/$K$/$V$ 进行全对全交换，分散激活计算并降低每卡内存压力； 第二次 All-to-All： 在注意力计算后收集输出上下文，将其重新映射为局部序列分区，既恢复原始序列结构，又显著减少了通信数据量。 高效通信与通用性\n利用全对全通信，使得通信量降为 $O(N/P)$，相比传统的 All-Gather 方法(通信量 $O(N)$)节省了近 $P$ 倍的带宽；同时，该方案适用于密集和稀疏注意力，并可与 ZeRO-3 内存优化无缝集成，从而支持更大模型和更长序列的高效训练。\nFig. 22. DeepSpeed-Ulysses vs Megatron LM. (Image source: DeepSpeed Blogs)\n在 64 卡 A100 环境下，吞吐量较 Megatron-LM 序列并行最高提升 2.5 倍，并可处理更长序列(百万级 token)； 收敛性能与原模型无差别，可轻松集成到 Megatron-DeepSpeed 框架。 优化器相关的并行：ZeRO ZeRO(Zero Redundancy Optimizer)(Rajbhandari et al. 2019)是一种旨在消除训练大型模型时内存冗余的优化器并行技术。大型模型训练的内存主要消耗在两大部分：\n模型状态(Model States)： 包括优化器状态(如 Adam 的动量和二阶矩)、梯度和模型参数。混合精度训练不仅需要存储 FP16 数据，还需保留 FP32 版本的参数和状态，导致内存占用更高。 激活值、临时缓冲区与内存碎片(Residual States)： 这些数据在前向传播和反向传播中仅被使用一次，但同样会占用大量内存。 为了解决内存冗余问题，ZeRO 采用了两大策略：\nZeRO-DP(Data Parallelism)：\n针对模型状态，通过将优化器状态、梯度和参数分片分布到多个数据并行进程中，消除冗余，同时利用动态通信调度减少通信量。\nZeRO-R(Residuals Optimization)：\n针对激活值和临时缓冲区，采用分片激活值重计算、固定缓冲区大小以及实时内存碎片整理等方法优化内存使用。\nZeRO 分片策略 ZeRO 分为三个阶段，每个阶段在前一阶段基础上进一步减少内存冗余，从而使得训练超大模型成为可能：\nZeRO-1(优化器状态分片) 原理： 将优化器状态(如 Adam 的动量和二阶矩)沿参数维度分为 $P$ 个分片($P$ 为 GPU 数量)，每个 GPU 只存储与其负责模型参数对应的状态。 局部更新：每个 GPU 在参数更新阶段仅更新其本地存储的状态和参数分片，无需额外跨 GPU 通信。 ZeRO-2(梯度分片) 原理： 在优化器状态分片的基础上，将梯度沿参数维度同样进行分片，每个 GPU 只存储对应梯度分片。 每个 GPU 计算局部梯度，利用高效的 Reduce-Scatter 操作进行梯度聚合，再更新本地参数分片。 ZeRO-3(参数分片) 原理： 在 ZeRO-1 和 ZeRO-2 的基础上，将模型参数(通常为 16 位数据)也进行分片，每个 GPU 只存储与其对应的参数分片。 按需参数收集：在前向或反向传播过程中，若某个 GPU 需要完整的模型参数，则从其他 GPU 收集缺失的分片，这一过程仅在必要时进行，以减少通信开销。 下图展示了不同阶段下每个设备上模型状态内存消耗的对比情况：\nFig. 23. Comparing the per-device memory consumption of model states, with three stages of ZeRO-DP optimizations. (Image source: Rajbhandari et al. 2019)\nDeepSpeed ZeRO 分片与 Offload 策略对比 为了更好地理解 DeepSpeed 的 ZeRO 策略，以下对各阶段及 Offload 方案进行对比：\nZeRO Stage 描述 显存占用 训练速度 ZeRO-0 纯数据并行，不进行任何分片，所有状态在每个 GPU 上完全复制。 最高 最快 ZeRO-1 仅分片优化器状态，梯度和参数仍复制。 较高 略慢于 ZeRO-0 ZeRO-2 分片优化器状态和梯度。 中等 慢于 ZeRO-1 ZeRO-3 分片优化器状态、梯度和模型参数。 最低 明显慢于 ZeRO-2，受模型规模和网络带宽影响 Offload 类型 描述 显存占用 训练速度 ZeRO-1 + CPU Offload 在 ZeRO-1 基础上，将优化器状态卸载到 CPU 内存，降低 GPU 显存占用，但依赖 PCIe 带宽且占用 CPU 内存。 中偏低 慢于 ZeRO-1 ZeRO-2 + CPU Offload 在 ZeRO-2 基础上，将优化器状态卸载到 CPU 内存，对大模型进一步降低 GPU 显存，但增加 CPU–GPU 数据传输。 较低 慢于 ZeRO-2 ZeRO-3 + CPU Offload 在 ZeRO-3 基础上，将优化器状态和模型参数卸载到 CPU，GPU 显存占用最低，但 CPU–GPU 通信开销极大。 极低 非常慢 ZeRO-Infinity(NVMe Offload) 基于 ZeRO-3，将状态卸载到 NVMe 设备，突破 CPU 内存限制，适合超大模型；性能高度依赖 NVMe 并行读写速度。 极低需 NVMe 支持 慢于 ZeRO-3，但通常优于 CPU Offload 方案 通信量与性能影响 ZeRO-0/1/2：\n主要依赖 All-Reduce 进行梯度同步，通信量相对较低。\nZeRO-3：\n需要对模型参数进行 All-Gather/All-Reduce 操作，通信量显著增加，网络带宽成为关键瓶颈。\nOffload 策略(CPU/NVMe)：\n数据传输主要在 CPU ↔ GPU 或 NVMe ↔ GPU 之间，传输带宽远低于 GPU 之间的通信，可能显著影响训练速度，尤其在 ZeRO-3 场景下更为明显。\n多维度并行 多维度并行(Multi-dimensional Parallelism) 是指在分布式训练中将数据并行、模型并行和流水线并行等多种并行技术有机结合，以充分利用现代 GPU 集群的计算资源。通过这种“3D 并行”或“4D 并行”策略，不仅能提高内存效率，还能提升计算效率，从而实现超大规模(甚至万亿参数级别)模型的高效训练。\n3D 并行 随着 GPU 集群计算能力的迅速提升，训练万亿参数级别的模型不再遥不可及。DeepSpeed 将数据并行、模型并行与流水线并行三种技术融合，构建了一种“3D 并行”策略。该策略主要解决训练超大模型所面临的两大挑战：\n内存效率：\n模型层被划分到不同的流水线阶段，每个阶段内部又通过模型并行进一步分割，减少了模型、优化器和激活值占用的内存量。但需要注意，模型分割不能无限制进行，否则通信开销会显著增加，进而影响计算效率。\n计算效率：\n为了让计算工作者数量超越单纯模型和流水线并行的限制，同时保证计算效率，DeepSpeed 借助 ZeRO-DP(基于优化器状态分片的数据并行)进行扩展。ZeRO-DP 不仅进一步优化内存使用，还通过拓扑感知映射将数据并行组分配到局部高带宽通信的设备上，极大降低了通信开销。\n下面的图示展示了 3D 并行的整体策略：\nFig. 24. Example 3D parallelism with 32 workers. Layers of the neural network are divided among four pipeline stages. Layers within each pipeline stage are further partitioned among four model parallel workers. Lastly, each pipeline is replicated across two data parallel instances, and ZeRO partitions the optimizer states across the data parallel replicas. (Image source: Majumder et al. 2020)\n每个并行维度(数据、模型、流水线)均经过精心映射，以充分利用节点内和节点间的通信带宽。具体策略包括：\n优化节点内通信： 由于模型并行的通信开销最大，优先将模型并行组安排在同一节点内，以利用较高的节点内带宽(例如采用 NVIDIA Megatron-LM 的张量切分方式)； 数据并行与流水线并行： 当模型并行不覆盖整个节点时，数据并行组尽可能安排在同一节点内；而流水线并行由于通信量较小，可灵活安排跨节点调度。 通过减少每个数据并行组中通信数据量以及提高局部并行通信的并行度，整体通信带宽得到有效放大。\nFig. 25. Mapping of workers in Figure 24 to GPUs on a system with eight nodes, each with four GPUs. Coloring denotes GPUs on the same node. (Image source: Majumder et al. 2020)\n4D 并行 为了进一步扩展模型规模，Llama3(Grattafiori et al. 2024) 训练的时候采用了 4D 并行策略，它结合了四种并行方法，将模型进行更细粒度的分片，使每个 GPU 上的模型参数、优化器状态、梯度和激活值均能适配高带宽内存(HBM)的容量限制。这四种并行方法分别是：\n张量并行(Tensor Parallelism, TP)： 将单个权重张量划分为多个块，分布在不同设备上； 流水线并行(Pipeline Parallelism, PP)： 将模型垂直划分为多个阶段，各阶段在不同设备上并行处理不同微批次； 上下文并行(Context Parallelism, CP)： 将输入上下文划分成多个段，从而缓解长序列输入时的内存瓶颈； 数据并行(Data Parallelism, DP)，通常采用完全分片的数据并行(FSDP)： 对模型、优化器状态和梯度进行分片，并在每个训练步后同步。 下面的图示展示了 4D 并行在 16 个 GPU 上的实现示例，各 GPU 的位置用向量 [D1, D2, D3, D4] 表示，其中每个维度对应一种并行策略。GPU 按照 [TP, CP, PP, DP] 四个维度分组，每个维度的组内大小均为 2。例如，GPU0 和 GPU1 属于同一张量并行组；GPU0 和 GPU2 属于同一上下文并行组；GPU0 和 GPU4 属于同一流水线并行组；GPU0 和 GPU8 属于同一数据并行组：\nFig. 26. Illustration of 4D parallelism. (Image source: Grattafiori et al. 2024)\n通过 4D 并行策略，Llama3 在训练时能够充分利用多个 GPU 的计算资源，同时有效减少内存占用，支持训练超大规模的模型。\n内存优化技术 除了并行训练技术，还有许多内存优化技术设计可以帮助训练 LLMs，这些设计主要从减少训练过程中各个环节的内存占用入手。\nCPU Offloading CPU Offloading(Rhu et al. 2016) 是指当 GPU 内存不足时，将暂时无需使用的数据或者张量卸载到 CPU 并在需要时再加载回 GPU是一种常见且直观的做法。它的主要目的是利用 CPU 内存更大的容量来扩展可用空间，从而在显存受限的环境下也能训练更大规模的模型。然而，这种方法会带来额外的数据传输开销，通常会降低训练速度，因此近年来应用相对减少。\n识别可卸载张量: 识别训练过程中暂时不使用的张量，例如模型参数、优化器状态、中间激活值等。判断张量是否可以卸载的依据可以是张量的使用频率、生命周期等。 张量卸载: 将可卸载的张量从 GPU 内存移动到 CPU 内存。数据传输通常通过 PCIe 总线进行。 张量预取(Prefetching): 在需要使用卸载到 CPU 内存的张量之前，提前将张量从 CPU 内存加载回 GPU 内存。预取操作可以与 GPU 的计算操作并行进行，以减少数据加载的延迟。 张量使用: GPU 使用加载回 GPU 内存的张量进行计算。 张量再次卸载: 在张量使用完毕后，如果张量在一段时间内不再需要使用，可以再次将其卸载到 CPU 内存，释放 GPU 内存空间。 ZeRO-Offload 和 ZeRO-Infinity 是 DeepSpeed 库中实现的基于 CPU 卸载的内存优化技术。ZeRO-Offload 将优化器状态卸载到 CPU 内存，ZeRO-Infinity 更进一步，将模型参数也卸载到 CPU 内存甚至 NVMe 磁盘，突破 GPU 内存墙，支持训练更大规模的模型。\n下图直观的展示了异构系统(Heterogenous system) 内存优化技术：\nFig. 27. Heterogenous system illustration. (Image source: Clolossal-AI Documentation)\n激活重计算/梯度检查点 激活重计算/梯度检查点(Activation Recomputation/Gradient Checkpointing)(Chen et al. 2016) 是一种以计算换内存的技术。在训练过程中，只保存部分激活值(例如每个 Transformer 层的输入激活值)，在反向传播时，重新计算未保存的激活值。激活重计算可以显著减少训练过程中的激活值内存占用，尤其是在训练深层神经网络时效果明显。\n选择检查点: 选择模型中的一些层作为 checkpoint。通常选择模型中的关键层，例如 Transformer 层的输入层。 前向传播(Forward Pass): 在前向传播过程中，只保存检查点层的激活值，对于非检查点层的激活值，在计算完成后立即释放，不进行保存。 反向传播(Backward Pass): 在反向传播过程中，当需要计算某个非检查点层的梯度时，首先重新进行一次前向传播，计算该层的激活值(重计算)，然后再进行反向传播计算梯度。对于检查点层，由于已经保存了检查点层的激活值，可以直接使用保存的激活值进行反向传播，无需重新计算。 下面进行激活重计算的内存成本分析。为方便分析，假设模型共有 $n$ 层网络结构，并将其均匀地划分为 $k$ 个分段(segment)。这样，每个分段大约包含 $n/k$ 层网络。做激活重计算时，我们只保存各分段边界处(即检查点)的激活值，其余需要时再重算。下面函数表示训练过程中的内存需求：\n$$ \\text{cost-total} \\;=\\; \\max_{i=1,\\ldots,k}\\bigl[\\text{cost-of-segment}(i)\\bigr] \\;+\\; O(k) \\;=\\; O\\Bigl(\\tfrac{n}{k}\\Bigr) \\;+\\; O(k). $$接下来考虑在给定 $n$ 的前提下，如何选择最优 $k$ 以最小化\n$$ f(k) \\;=\\; \\frac{n}{k} \\;+\\; k. $$对 $f(k)$ 关于 $k$ 求导并令其为 0(只考虑 $k\u0026gt;0$)：\n$$ f'(k) \\;=\\; -\\frac{n}{k^2} \\;+\\; 1 \\;=\\; 0 \\quad\\Longrightarrow\\quad k^2 = n \\quad\\Longrightarrow\\quad k = \\sqrt{n}. $$将 $k = \\sqrt{n}$ 代入，可得最小内存开销大约为\n$$ f(\\sqrt{n}) \\;=\\; \\frac{n}{\\sqrt{n}} \\;+\\; \\sqrt{n} \\;=\\; 2\\sqrt{n}. $$因此，该做法的总体峰值内存需求可降到 $O(\\sqrt{n})$ 的量级(对比一般直接保存所有激活的 $O(n)$ 内存)，这就是激活重计算技术能带来“亚线性”内存占用的原因。下图直观的展现了这个 trick 的效果。\nFig. 28. The memory cost of different memory saving algorithms. Sharing: Memory used by intermediate results is recycled when no longer needed. Inplace: Save the output directly into memory of an input value. (Image source: Chen et al. 2016)\n需要注意，激活重计算需要在反向传播阶段额外进行前向重算，每个分段要做一次到 $n/k$ 层的前向计算。若将网络分为 $k$ 个分段，反向传播时重算总计约 $k \\times \\bigl(n/k\\bigr) = n$ 层的前向操作，相当于每个训练迭代中整体多做了大约一次“前向传播”。这在 LLM 训练中通常能被接受，原因是：\n比起快速耗尽 GPU 显存导致无法训练大规模模型，这种在计算上的额外代价通常更容易承受。 当模型十分深($n$ 很大)时，用激活重计算技术可以将内存使用从 $O(n)$ 显著降至 $O(\\sqrt{n})$，使得更多、更深的大模型能够在给定硬件上进行训练。 混合精度训练 混合精度训练(Mixed Precision Training)(Micikevicius al. 2017)是一种在模型训练过程中同时利用低精度浮点数(如 FP16 或 BF16)与高精度浮点数(如 FP32)的技术，其核心目标是在减少显存占用、加速训练的同时，尽可能保持与全精度训练相当的模型精度。\n现代 GPU 在低精度计算上具有更高的吞吐量和更低的显存占用，从而降低访存开销与内存带宽需求，使混合精度训练能显著提升训练速度。下图展示了一个网络层中混合精度训练的基本流程：前向和反向传播主要采用半精度(FP16)运算，而在梯度累积与参数更新时使用全精度(FP32)，以规避低精度计算可能带来的数值精度问题。\nFig. 29. Mixed precision training iteration for a layer. (Image source: Micikevicius al. 2017)\n混合精度训练主要依赖以下三项关键技术：\n权重全精度主副本\n为防止梯度在 FP16 下因幅度过小而被截断为零，训练过程中保持一份 FP32 的权重主副本。具体流程为：\n初始化： 使用 FP32 权重作为模型的主副本； 前向/反向传播： 每次迭代开始前，将 FP32 权重转换为 FP16 用于前向传播和反向传播，计算得到 FP16 梯度； 参数更新： 在更新参数前，将 FP16 梯度转换为 FP32，并用其更新 FP32 主副本。 这种设计既能利用低精度计算的高效性，又确保了参数更新的准确性。\n损失缩放\n为避免因低精度表示范围受限而导致梯度下溢，通常在反向传播前对损失值进行放大处理。具体流程为：\n使用 FP32 计算损失 $L$； 将损失乘以缩放因子 $S$，得到 $L\u0026rsquo; = L \\times S$ 后进行反向传播，计算出 FP16 梯度； 在参数更新前，将梯度除以 $S$ 还原为真实梯度。 缩放因子的选择十分关键：过小可能无法避免梯度下溢，过大则有可能引起梯度上溢。动态损失缩放技术可以根据训练过程中梯度的实际情况自动调整缩放因子。\n如下图所示，通过放大损失使梯度分布更集中于较高数值部分，从而保留那些在低精度表示下可能被截断的细微信息。\nFig. 30. The histogram of gradients in full precision. The left part up to $2^{-24}$ will be zero-ed off once the model switches to FP16. (Image source: Micikevicius al. 2017)\n算术精度控制\n对于对精度要求较高的运算(如向量点积和求和归约)，可采用 FP32 进行累积计算，然后再转换为 FP16 存储；而对于逐元素运算，则可根据具体需求选择使用 FP16 或 FP32。 压缩 在深度学习训练过程中，中间结果(如激活值和梯度信息)虽然仅在一次前向传播和一次反向传播中使用，但往往占用大量内存。考虑到两次使用之间存在明显的时间间隔，可以在第一次使用后对数据进行压缩(Compression)，待后续需要时再解压缩，从而有效降低内存占用。\n压缩技术主要应用于两个场景：\n激活值压缩： 前向传播后对激活值进行压缩，反向传播前解压缩。这对深层神经网络尤为重要，因为激活值通常占用大量内存。 梯度压缩： 在反向传播计算梯度后、梯度同步前对梯度进行压缩，减少跨 GPU 通信的数据量，从而提高分布式训练效率。 压缩技术可以分为两类：\n无损压缩(Lossless Compression):\n采用如 Huffman 编码或 Lempel-Ziv 算法等方法，确保解压缩后的数据与原始数据完全一致。但由于压缩率较低，其内存节省效果有限。\n有损压缩(Lossy Compression):\n使用如 JPEG 或 MPEG 等算法，在允许一定数据损失的前提下获得更高的压缩率。这种方法能显著降低内存占用，但可能对模型精度和收敛性产生一定影响。\nGist(Jain et al. 2018)是一种用于激活值压缩的内存优化技术，其核心在于利用数据编码策略压缩中间结果，主要包含两种编码方案：\n层特定无损编码(Layer-Specific Lossless Encoding):\n针对特定层结构(例如 ReLU-Pool 与 ReLU-Conv)，设计专门的无损编码方案：\n对于 ReLU-Pool 层，可采用二值化编码； 对于 ReLU-Conv 层，则使用稀疏存储与稠密计算编码。 激进有损编码(Aggressive Lossy Encoding):\n采用延迟精度降低(Delayed Precision Reduction, DPR)技术。DPR 的核心思想是：激活值在前向传播时需保持高精度，而在反向传播时可容忍较低精度。因此，在前向传播后将激活值压缩到较低精度，反向传播前再解压至高精度。\n内存高效优化器 传统优化器(如 Adam、SGD with Momentum)在训练过程中需要为每个模型参数维护大量状态数据(例如 momentum 和 variance)，其内存占用往往与模型参数量相当甚至更高。例如，以 Adam 优化器(Kingma et al. 2014)为例，每个参数需要存储一阶矩和二阶矩，与参数本身及其梯度加起来，整个训练过程大约需要 4 倍于模型权重的内存，这对大型模型训练构成了严峻挑战。\n为降低内存消耗，内存高效优化器主要通过以下策略进行设计：\n减少状态变量数量： 只保存必要的统计信息，而非完整矩阵； 降低状态变量精度： 采用 FP16 或 bfloat16 存储； 共享状态变量： 在多个参数间共享部分状态信息。 Adafactor Adafactor(Shazeer et al. 2018) 是一种内存高效的自适应学习率优化器。与 Adam 不同，Adafactor 不存储完整的二阶矩估计矩阵，而是只存储两个向量(行、列统计)替代完整的二阶矩矩阵，显著降低了内存占用，特别适用于参数矩阵具有低秩结构的场景。\nSM3 SM3(Sparse Momentum for Massive Models)(Anil et al. 2019) 通过稀疏更新和状态共享，提供了一种同样内存高效的自适应优化方案。\n稀疏 Momentum： 只对梯度非零的参数更新 Momentum，从而减少计算和存储开销； 状态共享： 在一定程度上允许不同参数共享状态变量，进一步降低内存消耗； 自适应学习率： 根据各参数梯度动态调整学习率，提高了模型训练的稳定性和收敛速度。 LoRA LoRA(Low-Rank Adaptation)(Hu et al. 2021) 提出在预训练权重旁引入低秩适配器，通过添加少量参数来实现高效微调，且不干扰预训练模型原有的推理能力。\n下图直观展示了 LoRA 的原理和初始化策略：\nFig. 31. An illustration of LoRA. (Image source: Hu et al. 2021)\n假设预训练权重矩阵为 $ \\mathbf{W} \\in \\mathbb{R}^{d \\times k} $。LoRA 在其中添加一个低秩更新项 $ \\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A} $，从而得到新的权重：\n$$ \\mathbf{W}' = \\mathbf{W} + \\alpha\\, \\mathbf{B}\\mathbf{A}, $$其中：\n$A \\in \\mathbb{R}^{r \\times d}$ （降维矩阵），用来将输入从维度 $d$ 映射到一个更低的维度 $r$； $B \\in \\mathbb{R}^{k \\times r}$ （升维矩阵），再将降维后的特征从 $r$ 维映射回原来的 $k$ 维度； $r \\ll \\min(d, k)$ （低秩维度），通常取 $4$ 到 $16$，以在保证模型表达能力的同时尽量减少额外参数量； $\\alpha$ 为缩放因子，用于放大低秩更新参数 $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$ ，补偿由于低秩分解带来的数值幅度较小的问题，从而确保即使新增参数量极少，也能在前向传播时有效传递新任务信息。通常可设置为 $\\alpha = 2 \\times r$（例如 $r = 8$ 时，取 $\\alpha = 16$）。 在微调过程中，原始权重 $ \\mathbf{W} $ 冻结不变，只更新 $ \\mathbf{A} $ 和 $ \\mathbf{B} $。由于 $ r $ 远小于 $ d $ 或 $ k $，大大减少了需要训练的参数量。\n为了确保微调初期引入的 $ \\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A} $ 对原模型的影响尽量小，需要让 $ \\Delta \\mathbf{W} \\approx 0 $。常见做法如下：\n降维矩阵 $ \\mathbf{A} $ 的初始化\n高斯初始化\n令 $ \\mathbf{A} \\sim \\mathcal{N}(0,\\sigma^2) $(一般 $ \\sigma $ 取较小值，如 0.02)。这样能保证初始更新量较小，不至于严重干扰模型输出。 Kaiming(He)初始化\nKaiming 初始化是一种专为深层网络设计的权重初始化方法，其目标是保持前向信号和反向梯度在网络层之间的稳定性。对于 LoRA，只要确保尺度较小(或配合合适的缩放因子)，即可使初始 $ \\Delta \\mathbf{W} $ 较接近零。 升维矩阵 $ \\mathbf{B} $ 的初始化\n通常将 $ \\mathbf{B} $ 初始化为全零矩阵，这样在初始时就有 $ \\mathbf{B}\\mathbf{A} = 0 $，进一步保证对原模型的影响微乎其微。 采用 LoRA 进行训练有以下优势：\n参数高效：仅引入低秩适配器参数，减少了需要训练和存储的总参数量。 显存与计算效率：冻结大部分预训练权重，微调过程只更新小规模参数，显著降低显存占用与算力开销。 无额外推理时延：训练完成后，可将更新项 $ \\Delta \\mathbf{W} $ 合并回原始权重，不会增加推理阶段的计算量。 QLoRA QLoRA(Dettmers et al. 2023) 是在 LoRA 基础上结合量化思想对大规模模型进行高效微调的一种方法。它通过以下三个关键改进，大幅降低显存占用，同时保持模型精度基本不变：\n4 位标准浮点数(NF4) 量化\n采用基于分块的分位量化策略，将原始模型权重量化为 4 位，从而在细微损失精度的情况下实现显著的存储压缩。\n双重量化(Double Quantization)\n在对普通参数进行一次量化后，再对量化常数进行一次额外的量化，从而进一步减小缓存占用。\n分页优化器(Paged Optimizer)\n当显存使用过高时，自动将部分优化过程转移到 CPU 内存，从而减轻 GPU 显存压力，提升可伸缩性。\n与传统的 LoRA 仅减少需微调参数数量不同，QLoRA 还通过 4 位量化来压缩所有权重，从而在保证接近原有精度的同时，最大限度减少显存占用和数据传输开销。\nFig. 32. Different finetuning methods and their memory requirements. QLoRA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. (Image source: Dettmers et al. 2023)\n这种方法可以看作是对 LoRA 的进一步扩展：LoRA 通过减少需要微调的权重数量来提升效率，而 QLoRA 则在此基础上，将所有权重(包括未微调的部分)量化到 4 位精度，在总体上实现存储与计算的双重压缩，适合对 LLM 进行资源受限环境下的高效微调。\n总结 并行技术和内存优化策略需要根据具体的模型结构、数据集规模、硬件资源和训练目标进行权衡和选择。通常情况下，需要将多种技术结合使用，才能有效地训练大规模的模型，并取得最佳的性能和效率。\n参考文献 [1] Weng, Lilian, and Greg Brockman. \u0026ldquo;Techniques for training large neural networks.\u0026rdquo; OpenAI Blog, 2022.\n[2] Shenggui Li, Siqi Mai. \u0026ldquo;Paradigms of Parallelism.\u0026rdquo; Colossal-AI Documentation, 2024.\n[3] Li, Shen, et al. \u0026ldquo;Pytorch distributed: Experiences on accelerating data parallel training.\u0026rdquo; arXiv preprint, 2020.\n[4] Li, Mu, et al. \u0026ldquo;Communication efficient distributed machine learning with the parameter server.\u0026rdquo; Advances in Neural Information Processing Systems 27, 2014.\n[5] Huang, Yanping, et al. \u0026ldquo;Gpipe: Efficient training of giant neural networks using pipeline parallelism.\u0026rdquo; Advances in Neural Information Processing Systems 32, 2019.\n[6] Harlap, Aaron, et al. \u0026ldquo;Pipedream: Fast and efficient pipeline parallel dnn training.\u0026rdquo; arXiv preprint, 2018.\n[7] Narayanan, Deepak, et al. \u0026ldquo;Memory-efficient pipeline-parallel dnn training.\u0026rdquo; International Conference on Machine Learning, PMLR, 2021.\n[8] Shoeybi, Mohammad, et al. \u0026ldquo;Megatron-lm: Training multi-billion parameter language models using model parallelism.\u0026rdquo; arXiv preprint, 2019.\n[9] Narayanan, Deepak, et al. \u0026ldquo;Efficient large-scale language model training on gpu clusters using megatron-lm.\u0026rdquo; Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021.\n[10] Shazeer, Noam, et al. \u0026ldquo;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\u0026rdquo; arXiv preprint, 2017.\n[11] Lepikhin, Dmitry, et al. \u0026ldquo;Gshard: Scaling giant models with conditional computation and automatic sharding.\u0026rdquo; arXiv preprint, 2020.\n[12] Fedus, William, Barret Zoph, and Noam Shazeer. \u0026ldquo;Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\u0026rdquo; Journal of Machine Learning Research 23.120, 2022: 1–39.\n[13] Zhou, Yanqi, et al. \u0026ldquo;Mixture-of-experts with expert choice routing.\u0026rdquo; Advances in Neural Information Processing Systems 35, 2022: 7103–7114.\n[14] Li, Shenggui, et al. \u0026ldquo;Sequence parallelism: Long sequence training from system perspective.\u0026rdquo; arXiv preprint, 2021.\n[15] Korthikanti, Vijay Anand, et al. \u0026ldquo;Reducing activation recomputation in large transformer models.\u0026rdquo; Proceedings of Machine Learning and Systems 5, 2023: 341–353.\n[16] Jacobs, Sam Ade, et al. \u0026ldquo;Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.\u0026rdquo; arXiv preprint, 2023.\n[17] DeepSpeed. \u0026ldquo;DeepSpeed Ulysses README.\u0026rdquo; GitHub repository.\n[18] Rajbhandari, Samyam, et al. \u0026ldquo;Zero: Memory optimizations toward training trillion parameter models.\u0026rdquo; SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, 2020.\n[19] Microsoft Research. \u0026ldquo;DeepSpeed: Extreme-scale model training for everyone.\u0026rdquo; 2020.\n[20] Dubey, Abhimanyu, et al. \u0026ldquo;The llama 3 herd of models.\u0026rdquo; arXiv preprint, 2024.\n[21] Rhu, Minsoo, et al. \u0026ldquo;vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design.\u0026rdquo; 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture(MICRO), IEEE, 2016.\n[22] Chen, Tianqi, et al. \u0026ldquo;Training deep nets with sublinear memory cost.\u0026rdquo; arXiv preprint, 2016.\n[23] Micikevicius, Paulius, et al. \u0026ldquo;Mixed precision training.\u0026rdquo; arXiv preprint, 2017.\n[24] Jain, Animesh, et al. \u0026ldquo;Gist: Efficient data encoding for deep neural network training.\u0026rdquo; 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture(ISCA), IEEE, 2018.\n[25] Kingma, Diederik P., and Jimmy Ba. \u0026ldquo;Adam: A method for stochastic optimization.\u0026rdquo; arXiv preprint, 2014.\n[26] Shazeer, Noam, and Mitchell Stern. \u0026ldquo;Adafactor: Adaptive learning rates with sublinear memory cost.\u0026rdquo; International Conference on Machine Learning, PMLR, 2018.\n[27] Ginsburg, Boris, et al. \u0026ldquo;Stochastic gradient methods with layer-wise adaptive moments for training of deep networks.\u0026rdquo; arXiv preprint, 2019.\n[28] Hu, Edward J., et al. \u0026ldquo;LoRA: Low-rank adaptation of large language models.\u0026rdquo; ICLR, 2022: 3.\n[29] Dettmers, Tim, et al. \u0026ldquo;Qlora: Efficient finetuning of quantized llms.\u0026rdquo; Advances in Neural Information Processing Systems 36, 2023: 10088–10115.\n[30] Weng, Lilian. \u0026ldquo;How to Train Really Large Models on Many GPUs?\u0026rdquo; Lil\u0026rsquo;blog, 2021.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui.(Mar 2025). 训练大模型并行和内存优化技术. https://syhya.github.io/zh/posts/2025-03-01-train-llm\nOr\n@article{syhya2025train-llm, title = \u0026#34;训练大模型并行和内存优化技术\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Mar\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-03-01-train-llm\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-03-01-train-llm/","summary":"\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\n\u003cp\u003e最近大模型的参数数量不断攀升，从最初的数十亿扩展到如今数千亿乃至数万亿级别。大模模型虽然带来了前所未有的应用效果，但与此同时，也引发了计算资源、内存管理和训练稳定性等一系列严峻挑战。因此本博客总结了一些常用分布式并行训练和内存管理技术，希望能够帮助大家更好地训练和优化大模型。\u003c/p\u003e","title":"训练大模型并行和内存优化技术"},{"content":" 注意：下面内容是由 OpenAI Deep Research 生成的研究案例 😊，模型会有一定幻觉和格式上的错误，仅供参考。\n原文链接: LLaMA 系列技术演进详解\n好的，我将撰写一篇详细的技术博客，全面介绍 LLaMA 系列的发展进展，包括从 LLaMA 1 到最新 LLaMA 3 的对比，以及其在模型架构、训练方法、推理性能等方面的演进。我会加入数学公式来解析关键技术点，如 RMS Normalization、FFN_SwiGLU、RoPE、GQA 和 BPE，并提供代码示例帮助理解。此外，我还会整理 LLaMA 最新模型与其他大模型（如 DeepSeek R1、O1/O3 等）的对比数据。\n我也会探讨 LLaMA 系列未来的发展方向，比如可能的 LLaMA 4 版本是否会朝着更轻量级的推理模型演进。稍后我会给你完整的技术博客。\nLLaMA 系列技术演进详解 Meta AI 在 2023 年至 2024 年开源了 LLaMA 系列大语言模型，从 LLaMA1 到 LLaMA3 持续改进，在开放模型领域树立了标杆。本文将详细介绍 LLaMA 系列的发展进展，包括预训练数据、模型架构、训练基础设施、规模化与效率、训练策略、训练后优化、推理性能、多模态能力，以及核心技术解析，并通过表格比较 LLaMA 系列各版本与其他先进模型（如 DeepSeek R1、OpenAI O1/O3）。文章还将给出部分代码示例解析关键实现，并探讨 LLaMA 系列未来的发展方向。\n1. 预训练数据 (Pre-Training Data) LLaMA1 (2023) 使用了大规模的公开数据集进行训练，总计约 1.4 万亿 (1.4 trillion) tokens 的文本。数据来源包括：Common Crawl爬取的网页文本、C4数据集、GitHub开源代码、维基百科、图书语料（如Project Gutenberg和Books3）、学术论文（ArXiv）、问答社区（Stack Exchange）等。其中英文Common Crawl网页约占 67%，C4 占 15%，GitHub代码占 4.5%，维基百科占 4.5%，图书语料占 4.5%，ArXiv论文约 2.5%，问答占 2%。LLaMA1 针对多语言也做了一定覆盖，训练语料包含20种语言（主要是英语，占比最高）。具体而言，LLaMA1 的7B和13B模型各训练了约 1.0 万亿 tokens，而33B和65B模型训练了 1.4 万亿 tokens。\nLLaMA2 (2023) 进一步扩充了预训练数据规模，总计约 2 万亿 tokens。虽然官方论文未详细列出所有数据来源（可能出于开源协议和筛选策略考虑），但可以推断 LLaMA2 使用了与LLaMA1相似的数据来源，并更新至2023年更多的公共数据。例如，LLaMA2 很可能使用了更新的Common Crawl快照和更多的开源代码仓库数据，使模型对最新语言现象和编程语言有更好的覆盖。LLaMA2 的预训练语料同样来自公开数据，但去除了来自Meta平台的私有数据，以确保模型完全开源可用。相较LLaMA1，LLaMA2的数据增加了约2倍，这也是其性能提升的重要原因之一。\nLLaMA3 (2024) 采取了大规模的数据扩充策略，预训练数据飙升至 15万亿 (15 trillion) tokens 量级——比 LLaMA2 增加了7倍。如此庞大的数据接近涵盖了互联网公开高质量文本的上限规模。数据来源更为多样化且覆盖更广，其中代码数据占比显著提高：LLaMA3 预训练语料中有 超过5% 是代码（比 LLaMA2 增加了4倍）。这意味着 LLaMA3 对编程语言和代码理解/生成的能力有大幅增强。此外，LLaMA3 的语料库更加多语言，支持超过100种语言。大约30%的预训练tokens是非英语文本，使模型具备更强的多语种理解能力。在如此大规模多样的数据支持下，LLaMA3 成为了当时训练数据规模最大的开源基础模型之一。\n数据质量控制方面，LLaMA 系列采用了严格的过滤与清洗策略。例如去重、剔除低质量文本、编程代码去除私密信息等，从而确保训练语料的高品质。总的来说，预训练数据的数量和多样性随着 LLaMA 版本升级而显著提升：从LLaMA1的1.4T增长到LLaMA2的2T，再跃升至LLaMA3的15T以上。这为模型性能的跃升奠定了基础，也反映了Meta对公开数据挖掘和利用能力的提升。\n2. 模型架构 (Model Architecture) 总体架构：LLaMA 系列模型均采用Transformer 解码器（Decoder-only）的架构，是自回归语言模型。它们以 GPT 类似的结构为基础，但在细节上做了多处优化和改进。每一层包含自注意力子层和前馈网络子层，并使用预归一化配置（pre-normalization）。以下是 LLaMA 系列架构上的关键特点：\n$$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2 + \\epsilon}} \\odot \\gamma,$$其中 $\\epsilon$ 为微小常数，$\\gamma$ 为可学习的缩放参数，$\\odot$ 表示按元素乘法。RMSNorm通过均方根范数归一化向量的尺度，而不改变向量方向。\n$$\\text{SwiGLU}(x) = (xW_1) \\otimes \\text{Swish}(xW_2),$$其中 $W_1, W_2$ 是线性变换矩阵，$\\otimes$ 表示逐元素乘积。这种门控线性单元可以增加模型表达能力，实验证明相对于ReLU激活能提升语言模型的表现。SwiGLU最早应用于PaLM等模型，在LLaMA中也发挥了增益作用。\n$$x_{2i}^{(p)} = x_{2i}\\cos\\theta_p - x_{2i+1}\\sin\\theta_p,$$$$x_{2i+1}^{(p)} = x_{2i}\\sin\\theta_p + x_{2i+1}\\cos\\theta_p.$$其中 $\\theta_p$ 通常设定为随维度线性增长的函数（例如 $\\theta_p = p \\cdot \\alpha^{i}$，$\\alpha$为常数）。这样一来，不同位置的表示可通过旋转操作体现相对位移关系。RoPE的优势在于它以相对位置的方式编码序列，使模型可以外推到比训练时更长的上下文长度，并改善长距离依赖的捕获。\n多头注意力与 GQA：LLaMA1 采用标准的多头自注意力 (Multi-Head Attention) 机制，每个注意力层有多组独立的 Query/K/V 投影头。然而在 LLaMA2 及后续模型中，引入了 Grouped Query Attention (GQA) 技术。GQA的思想是减少K/V头的数量：将原本 $H$ 个注意力头的 K、V 向量分成 $G$ 组，同一组内的多个 Q 头共享一组 K、V 表示。例如，如果总注意力头数 $H=32$，使用 GQA 分组数 $G=8$，则仅生成8组 K/V，而32个 Q头两两共享这8组K/V（每组供4个Q头使用）。这样做大幅减少了注意力计算中需要存储和传输的 K/V 向量数量，降低显存占用和内存带宽压力。GQA几乎不影响模型性能，却使长上下文的开销降低，为上下文长度翻倍创造了条件。LLaMA2 使用8组 GQA（即 n_kv_heads = 8，无论总头数多少）。这也是 LLaMA2 能将上下文从2048扩展到4096的重要原因之一。\n分词和词表 (BPE Tokenization)：LLaMA 系列使用子词分词，最初版本采用 SentencePiece 的 BPE (Byte Pair Encoding) 方法 (Tokenizer. Finetune here to talk a bit about… | by Anlatan - NovelAI)。LLaMA1和2使用了约 32k 大小的词表 (vocabulary) (Tokenizer. Finetune here to talk a bit about… | by Anlatan - NovelAI)。SentencePiece BPE 会将文本分割为更细粒度的子词单元，以适应模型词汇。与GPT系列模型类似，词表中包括字母、常见词片段以及控制符等。值得注意的是，LLaMA3 更新了分词器：改为使用更先进的分词算法（基于 OpenAI 的 Tiktoken 实现），将词表扩充到 128k 个 token (I can not extend vocab of LLaMA-3 using sentencepiece anymore vs \u0026hellip;)。更大的词表可以减少文本长度（因为单个token包含更多信息），特别对编程符号、Unicode表情符号、多语种文本等的表示更加高效 (Training — NVIDIA NeMo Framework User Guide 24.07 \u0026hellip;)。例如，LLaMA2 对于代码中的长标识符可能需要拆成多个 token，而 LLaMA3 较大的词表可以用单个token表示，从而提升了编码和生成效率。这种改变使 LLaMA3 在代码理解和多语言任务上性能进一步提升 (I can not extend vocab of LLaMA-3 using sentencepiece anymore vs \u0026hellip;)。\n轻量化设计：LLaMA 模型架构在设计上追求参数高效。例如，在线性层中完全去除了偏置项 (bias)。Transformer中每个全连接层、注意力投影层通常有形如 $Wx + b$ 的偏置，但 LLaMA 选择不使用偏置 $b$，从而减少参数量和轻微的推理开销。实验表明这对模型性能影响可以忽略不计，却简化了模型结构。此外，LLaMA 没有采用一些增加参数的架构变体，如反馈记忆单元或额外的嵌入层等，而是专注于提高基础Transformer每个参数的效率。这种“less is more”的理念使得相同参数量下，LLaMA 系列往往比其他模型表现更佳。\n架构对比：总体而言，LLaMA1确立了基础架构：Transformer Decoder + RMSNorm + RoPE + SwiGLU + 无偏置。LLaMA2 保持这些核心架构不变，引入 GQA 优化注意力，并将最大上下文从 2048 增至 4096 tokens。LLaMA3 延续架构风格，同时扩大上下文至 8192 tokens（8K） (Context Length Limits in Bedrock | AWS re:Post)、升级分词器，并针对多模态输入进行了架构扩展（详见下文多模态能力部分）。这些架构改进共同确保了 LLaMA 系列在各代均保持高效的计算性能和强大的表达能力。\n3. 训练基础设施 (Infrastructure) 训练如此大规模的模型需要强大的计算基础设施支持。Meta 为 LLaMA 系列的训练投入了顶尖的超级计算资源：\nLLaMA1：据报道，65B参数模型的训练使用了 2048 张 NVIDIA A100 80GB GPU 并行运行。在这样的配置下，吞吐量可达每秒处理约 $3.8\\times10^2$ tokens/每GPU，即总计约 $7.8\\times10^5$ tokens/秒。以65B模型1.4万亿tokens的语料计算，总训练时间约为 21天左右（不计中断）才能跑完一个 epoch。LLaMA1 其它尺寸模型（7B、13B、33B）训练时间更短一些，但总体都在数周量级。训练时采用数据并行和张量模型并行的混合并行策略，将模型权重和计算负载分摊到上千GPU上同步进行。\nLLaMA2：模型参数扩大到70B、训练语料提升到2T，使得计算需求进一步增加。Meta 与 Microsoft Azure 合作提供了训练基础设施。据推测，LLaMA2 训练使用了 Meta新建的 AI Research SuperCluster (RSC) 超级计算机的一部分，该集群配备有 16000张 A100 GPU（80GB）互联。虽然具体参与的GPU数量未公开，但很可能上万张GPU协同训练，以在合理时间内完成2万亿token的训练。推测训练时间在数周到1~2月之间。LLaMA2 模型训练过程中也使用了混合精度（bfloat16/FP16）训练、ZeRO/FSDP等技术以充分利用GPU内存和计算。\nLLaMA3：为训练超大规模的 LLaMA3 系列模型，Meta动用了其最新一代 GPU 集群。官方透露 LLaMA3 的训练在 Meta 定制的 两套 24000卡 GPU 集群 上进行。这可能指每个集群有24k张 GPU，总计 48000 张，或者两个集群总共24k张，但更可能是前者。更重要的是，这批GPU很可能是 NVIDIA H100（配备HBM3高速显存），因为Meta提到训练中出现多次 HBM3相关故障中断。H100 GPU相较A100有更高的算力和更大的显存带宽，适合超大模型训练。根据Meta的数据中心报告，在一次 为期54天的 LLaMA3-405B 模型训练中，集群经历了419次意外中断（一半以上与GPU和HBM故障相关）。尽管遇到不少硬件挑战，Meta最终成功完成了405B模型的训练。4050亿参数模型的训练开销极其惊人：如果按照54天×24000 GPU计算，相当于 1296000 GPU天（相当于单GPU连续训练约3547年）！这展示了Meta在基础设施和分布式训练技术上的卓越能力。\n优化策略：在如此庞大的基础设施上训练，需要精心优化的软件栈。LLaMA系列使用了高度定制的分布式训练框架（基于 PyTorch 深度优化版），包括 Fully Sharded Data Parallel (FSDP) 以在GPU之间拆分模型权重、Tensor Parallelism在单机多卡上并行矩阵运算，以及Pipeline Parallelism在层间流水。为了充分利用GPU算力，训练采用混合精度 (Mixed Precision)，将大部分计算用 FP16/BF16 完成，梯度聚合时使用FP32累积以保证数值稳定。这样既降低显存占用又加速计算。此外，还应用了激活检查点 (Activation Checkpointing) 技术，在反向传播时重算部分前向，以节省内存，从而能增大batch大小。LLaMA1报告使用了 4M tokens 的全局batch（例如并行gpu数×每gpu批次size×accumulation steps的乘积），如此大批量能提高硬件利用率和收敛稳定性。LLaMA2/3 也沿用了大批量训练策略，同时根据模型大小调整学习率等超参数以确保收敛 (What is the Llama2 number of steps? [closed] - Cross Validated)。\n训练开销对比：总结来看，LLaMA1 使用 2048×A100 级别的规模训练了几周；LLaMA2 进一步扩展集群且可能用了近万卡，数据翻倍可能训练耗时相近或稍更多；LLaMA3 则调用了数万卡的 H100 超级集群，405B模型训练耗时接近两个月。这种大规模训练对软硬件可靠性要求极高。Meta 通过冗余和检查点容错机制来应对训练中途的中断，将代价巨大的长时间训练顺利跑完。可以说，LLaMA 系列的成功离不开 Meta 在基础设施和工程能力上的投入，其规模之大在开源模型中首屈一指。\n4. 规模化与计算效率 (Scaling and Efficiency) 模型规模增长：LLaMA系列在参数规模上遵循一定的扩展策略。LLaMA1 提供了 7B、13B、33B、65B 四种模型规模，以验证在不同算力条件下的性能。结果显示，65B 模型已经在许多基准上媲美更大参数的闭源模型（如175B的GPT-3或540B的PaLM）。LLaMA2 将顶层规模提高到 70B（相对65B略增），移除了33B中档规模，仅保留7B、13B、70B三个档次。70B模型在2万亿token上训练，参数量提升不大但训练数据翻倍，使其性能显著超过LLaMA1-65B。LLaMA3 进行更激进的扩展，除了常规的 8B、70B 模型外，新增了 405B 的超大模型。4050亿参数的 LLM 在业界尚属首次开源，标志着 Meta 在模型规模探索上迈入超大模型领域。405B 模型的层数、隐藏维度、注意力头等均远超以往（据推测，LLaMA3-405B的层数约×2，隐藏维度×~1.5，相应参数量提升）。尽管参数爆炸，但 Meta 通过优化使其训练成为可能，并成功开源了该模型权重。\nChinchilla 定律：在扩展参数的同时，LLaMA系列也注重遵循Chinchilla最优法则（即模型大小与训练token数成比例，以充分利用算力）。LLaMA1 65B模型用1.4T tokens，约是参数量的20倍，实现了接近Chinchilla最优的训练。LLaMA2 70B用2T tokens，约28倍关系，也基本合理。LLaMA3-70B用15T tokens，超过200倍，这是超出Chinchilla建议的（说明Meta倾向于过训练以榨取中小模型性能）。而405B模型用15T，约37倍，比较接近最优范围。事实证明，这种增加数据的策略对提高中小模型效果很明显：例如 LLaMA3-8B 在充足训练下，其性能已经超过 LLaMA2-13B，甚至接近 LLaMA2-70B 在某些任务上的表现。换言之，通过大幅度增加训练语料，较小的模型可以弥补参数上的不足，获得异常出色的效果。\n计算效率优化：为了让扩展规模后的模型仍能有效训练和推理，Meta 在计算效率上做了多方面改进：\n高效注意力：引入的 Grouped Query Attention (GQA) 显著降低了注意力计算的内存/带宽需求，使得上下文长度扩展的代价降低。LLaMA2/3 中，一个注意力头的 Key/Value 在计算和缓存时只对应1/8个全尺寸头的存储，这意味着注意力模块的显存占用减少接近87.5%。这使 LLaMA3 在8K上下文时仍可接受地运行，而没有显著超出硬件内存限制。\n高效算子实现：LLaMA 系列模型均使用了高度优化的低级算子。Meta基于 PyTorch 开发了定制的内核，例如FlashAttention算法来加速注意力计算，通过在GPU上优化内存访问和并行度，使长序列注意力的速度提升数倍。在矩阵乘法、卷积等核心计算上，也利用了NVIDIA提供的 cuBLASLt、TensorRT 等库的最新优化。对于H100 GPU，利用Tensor Core和BF16的高算力，使405B模型的每秒FLOPs利用率达到峰值。还有一些优化如张量融合、Operator Folding等，减少不必要的kernel启动开销。这些工程上的优化叠加，使得即便参数规模呈数量级上升，实际训练和推理速度并没有线性变慢。\n混合精度与压缩：LLaMA 模型在推理阶段可以采用更低精度来提升效率和减小内存占用。社区常用的有 4-bit / 8-bit 量化 技术（如 GPTQ, AWQ 等）来压缩模型。LLaMA 系列由于架构规整，经过量化后依然保持较好性能。例如LLaMA2-70B 量化到4-bit时仅有~35GB，占用显存大减而质量损失很小。Meta 官方也提供了一些优化版本（如 int8/int4 推理加速库），使LLaMA模型能够更高效地部署。在LLaMA3发布时，亚马逊等合作伙伴报告他们使用定制硬件和量化，在单机上运行了405B模型的推理——虽然仍需要上TB的内存，但证明了通过优化手段，超大模型也能实际投入使用。\n模块化和剪枝：对于LLaMA3及后续，Meta也在探索稀疏化/专家混合 (MoE)等技术以提升参数利用率。例如社区的 DeepSeek R1 采用了MoE架构（671B参数但每次只激活37B）实现了超大模型的可用性。虽然当前开源的LLaMA3-405B仍是密集模型，但未来版本可能借鉴类似思路，使得“规模化”不以线性增加计算开销为代价。这方面在第9节的未来展望中详细讨论。\n总的来说，LLaMA 系列通过架构和工程优化，实现了规模化与效率提升并举。参数数量从几十亿到几千亿的跃升，没有牺牲训练可行性和推理效率。这种成功经验使得开源社区对继续扩大的 LLaMA4 充满期待。\n5. 训练策略 (Training Recipes) 大模型的成功不仅依赖架构和数据，训练策略（即超参数和优化细节）也至关重要。LLaMA 系列在训练过程中采用了精心设计的配方：\n优化器 (Optimizer)：LLaMA1/2 使用了 AdamW 优化器，这是在Adam基础上增加权重衰减 (Weight Decay) 的版本 (What is the Llama2 number of steps? [closed] - Cross Validated)。AdamW在大规模语言模型训练中表现出稳定的收敛。LLaMA3 可能亦采用AdamW或其改进版本。一些研究指出对于超大模型，Adafactor 等优化器在内存上更有优势，但Meta并未明确提到更换优化器。此外，有文献探讨了Adam-mini等简化变种，可提高吞吐量而保持效果 (Adam-mini: Use Fewer Learning Rates To Gain More - arXiv)。不排除Meta在405B模型上尝试了更高效的优化算法。\n学习率调度 (LR Schedule)：采用 预热 + 余弦退火 策略。根据LLaMA2公布的信息，训练使用了2000步的热身 (warmup) 来线性增加学习率到最大，然后随训练过程按照余弦曲线下降至最终约为初始值的10% (What is the Llama2 number of steps? [closed] - Cross Validated)。这种调度在GPT等模型训练中已被验证有利于稳定收敛，防止一开始步长过大造成发散，又能在后期充分减小学习率以精调损失。LLaMA的具体初始学习率和批次大小会随模型大小调整：通常较大模型用略低学习率、较小模型用稍高学习率以平衡收敛速度和稳定性 (LLaMA 2: The New Open Source Language Model - E2E Networks)。例如，有经验表明7B模型可以用2e-4，而70B模型可能用1e-4的学习率。\n批次大小 (Batch Size)：LLaMA1使用了4M tokens/步的超大批次。这是通过 2048 GPU × 每GPU 2048 tokens × 若干累积 得到的总有效批次。如此大的批次可以降低梯度噪声，使训练更接近梯度下降的极限。然而受限于显存，通常需要 gradient accumulation 实现。LLaMA2/3 延续大批次策略，并可能针对405B模型调整批次大小以兼顾内存。由于405B模型非常庞大，受限于单GPU显存，实际总批次可能略小于4M，但通过更多GPU可以部分弥补。此外，Meta或采用分段批次（比如对长序列使用稍小批次）等策略来平衡显存占用。\n正则化手段：LLaMA模型训练中没有使用Dropout等显式正则化。这在大模型中很常见，因为海量数据本身提供了足够的正则效果。为了防止过拟合和梯度爆炸，主要依赖于权重衰减（AdamW中的L2正则）和归一化技术。LLaMA1/2 可能采用了 ~$0.1$ 左右的权重衰减系数，使模型参数在训练中不会无限增大。另一种隐式正则是随机截断序列：每次训练以一定概率使用比最大长度短的序列，迫使模型适应多种长度，防止只拟合长上下文。\n梯度裁剪 (Gradient Clipping)：为控制梯度范数，LLaMA 可能使用了梯度裁剪策略，如将梯度范数限制在一个阈值（例如1.0）。这可避免偶发的梯度爆炸导致训练发散。大模型的梯度在训练初期尤其容易很大，适度裁剪有助于稳定。\n混合精度训练：如前所述，训练中使用 FP16/BF16 的混合精度。BF16（bfloat16）由于有更宽的指数范围，被Meta偏爱用于稳定性。梯度累积在FP32下完成，以保持精度。Loss标量也在FP32下计算。整个训练框架保证数值误差不至于损害最终模型质量。\n数据混合与课例调度：LLaMA1 刚训练时可能对不同数据源进行了分段训练或混合采样。例如，先训练普通文本语料，再在代码上继续训练，以提升代码能力。或者对多语言数据动态调整采样权重，确保高低资源语言都得到适当训练。LLaMA3 中，由于数据极为庞大，可能采用了分阶段训练：先在部分数据上预热模型，再扩大数据范围。这方面细节未公开，但推测合理的课程学习（Curriculum Learning）有助于模型更好地学习。\n监控和调整：在训练过程中，Meta 团队会持续监控 验证集困惑度 (perplexity) 和下游任务性能。如果发现模型损失下降趋势放缓或出现震荡，可能微调学习率或其他超参。LLaMA2 论文提到在2T tokens时模型loss仍未收敛饱和，这表明他们在观察训练曲线时判断模型有潜力进一步训练，但出于算力考虑在2T处停止。LLaMA3 则大概率训练到预设tokens数即止。\n总之，LLaMA系列的训练配方是成熟的大模型训练实践的体现：合适的优化器+大批次+余弦退火学习率+混合精度，再辅以必要的正则和梯度控制。这样的策略确保了模型可以高效且稳定地从海量数据中学习。在实际开源模型训练社区中，这些经验也被广泛借鉴，为众多跟进者提供了范例。\n6. 训练后优化 (Post-Training Processes) 训练基础模型 (pre-trained LLM) 完成后，通常还需要一系列后处理和微调使其更好地应用于下游任务。LLaMA 系列在训练后进行了多种优化：\n指令微调 (Instruction Tuning)：为了让模型更善于遵循人类指令、进行对话，Meta 对 LLaMA 基础模型进行了指令微调。以 LLaMA2-Chat 为例，它是在基础模型上使用超过100万人类标注的对话/指令数据继续微调得到的。这些数据包括用户提出的指令及高质量的参考回答，使模型学会用更自然的方式回复。LLaMA2-Chat 在对话基准上超过了同时期的所有开源Chat模型。类似地，LLaMA3 发布时也提供了 LLaMA3-Instruct 8B/70B 等微调版本。微调过程一般采用较小学习率在指令数据上训练一个Epoch左右，并可能使用反馈机制挑选最优checkpoint。\nRLHF 强化学习对齐：除了有监督的指令微调，LLaMA2-Chat 等还使用了 人类反馈强化学习 (RLHF) 来提高回答的有益性和安全性。这通常包括训练一个奖励模型（比如LLaMA模型微调成判断回复质量的模型），然后用近端策略优化 (PPO) 方式调整聊天模型，使其输出获得更高的奖励评分。Meta 并未详细公开RLHF细节，但提到他们采集了人类偏好数据用于对话模型调优。通过RLHF，模型更倾向于给出详尽、有礼且无害的回答。这使 LLaMA 系列在对话场景下更加可靠。\nLLaMA Guard 安全守卫：针对大模型可能输出有害内容的问题，Meta 提出了 LLaMA Guard 系列模型。例如 LLaMA Guard 3 是一个基于LLaMA3微调的安全过滤模型，专门用于检测并拦截不当输入或输出。LLaMA Guard 会对用户的提问或模型要输出的回答进行审查，识别是否含有违规内容（仇恨、色情、暴力等），从而在人机对话系统中作为一道安全闸。 (Llama Guard: LLM-based Input-Output Safeguard for Human-AI \u0026hellip;)研究表明，LLaMA Guard 3 能有效分类对话内容的安全性，并标记出违反的内容类别。值得注意的是，LLaMA3 开源发布时，Meta 同时开源了一个 LLaMA-2-7b 微调而来的安全模型，用于社区构建安全过滤器 (Llama Guard: LLM-based Input-Output Safeguard for Human-AI \u0026hellip;)。这些举措说明在基础模型开源后，Meta仍非常重视后续的安全对齐，通过附加模型和规则使LLM应用更加可信。\n模型压缩与蒸馏：在LLaMA3时代，模型参数极其庞大（405B），直接部署困难。为此，一些衍生工作对 LLaMA 大模型进行了蒸馏 (Distillation) 和 量化 (Quantization)。例如社区中出现了将 LLaMA3-405B 的知识蒸馏到 70B 甚至 7B 模型的尝试，使小模型在特定任务上逼近大模型性能。Meta 本身也在 LLaMA3.2 版本中发布了 1B 和 3B 参数的轻量级模型 (Llama 3 vs 3.1 vs 3.2 : r/LocalLLaMA - Reddit)。这些小模型可以被视作对大模型的压缩，适合在移动端或边缘设备上运行。蒸馏通常通过让小模型学习大模型生成的海量伪数据（问题-回答对）来实现，从而将405B模型的推理能力迁移给小模型。虽然这些小模型无法完全达到大模型的水平，但在推理速度和内存要求上具有巨大优势，实用性更强。\n插件与工具使用：在后训练阶段，Meta 还探索了让 LLaMA 模型与外部工具集成。例如，让模型学会使用检索工具查询数据库，或调用计算器、执行代码等。虽然这更多属于应用开发，但也涉及一些微调。Meta 的年终博客提到，未来的 LLaMA 可能结合工具使用（Agents），例如结合浏览器、搜索引擎来增强事实性。这需要在训练后通过强化学习或示教数据来教模型如何调用API、解析工具结果等。LLaMA3 并未直接包含这类能力，但为此进行了架构铺垫（如更长上下文，可嵌入工具格式的结果）。\n评估与迭代：训练后，Meta 进行了广泛的基准测试（详见下一节）。根据评估结果，有时需要返工微调模型。例如，发现模型在数学推理上较弱，则可能针对数学数据再微调一轮（类似OpenAI的Gradual Release策略）。此外，Meta通过红队攻击模型，找出其弱点并进行针对性修正——这可能包括继续在有害内容上对抗训练，使模型学会拒绝回答违规请求等。这些都是后训练改进的一部分。\n概括来说，LLaMA 系列在基础模型训练完成后，通过指令/对话微调提升易用性，通过RLHF和安全模型保证输出对齐人类期望，通过模型压缩扩大部署范围，并通过持续评估优化不断迭代。训练后的这些过程极大地提高了模型的实际应用性能和安全可靠性，也是让 LLaMA 模型从研究走向产品的关键步骤。\n7. 推理性能 (Results and Inference) LLaMA 系列模型在各项NLP基准测试中表现优异，多次刷新开源模型的纪录：\nLLaMA1 性能：初代 LLaMA 发布时（Feb 2023）引起轰动，因为 LLaMA-13B 在许多任务上已经超越了175B参数的GPT-3 (Davinci)，而 LLaMA-65B 更是逼近 Chinchilla-70B 和 PaLM-540B 的水平。论文报告显示，65B模型在常用基准如Wiki知识问答、自然语言推理、阅读理解等上，与当时最好的大模型旗鼓相当。尤其惊艳的是 LLaMA-13B：仅凭130亿参数就显著超过了老牌175B模型GPT-3和70B的OPT。这证明了数据和架构的高效，使小模型发挥大模型效果。LLaMA1 的弱项在于对话和编程等方面缺乏专门优化，但总体作为基础模型，性能已确立SOTA开源模型的位置。\nLLaMA2 性能：LLaMA2 (Jul 2023) 进一步提升，LLaMA2-70B 成为开源领域新的顶尖模型。在学术基准例如 MMLU（多任务语言理解），70B模型得分可达 ~68%，已非常接近 GPT-3.5 系列。在常识推理（HellaSwag）、翻译、代码生成 (HumanEval) 等任务上，LLaMA2-70B 也全面超越LLaMA1-65B，并击败同时期的竞争开源模型如 Falcon-40B、MPT-30B 等。值得注意的是，Meta 发布了LLaMA2-Chat版本，在对话基准如 Vicuna-Bench、MT-Bench 上与 ChatGPT (GPT-3.5) 不相上下。一些内部评估甚至显示 LLaMA2-Chat70B 在特定基准上略优于 ChatGPT。这标志着开源模型第一次在对话能力上接近主流闭源模型。LLaMA2-7B/13B 相比1代也有明显进步，但与70B差距仍存在，在复杂推理任务上小模型力不从心。\nLLaMA3 性能：LLaMA3 (2024) 包括8B、70B、405B等，引领开源模型进入GPT-4时代的能力水平。根据Meta公布，LLaMA3-70B 在多数NLP任务上已经赶上甚至超越 Anthropic Claude 和 Google PaLM2 等闭源模型。而社区实测显示，LLaMA3-70B 在MMLU上分数逼近74%左右，开始触及GPT-4的表现区间。在代码生成 HumanEval 测试上，70B也有接近50%的通过率，与GPT-4的52%非常接近。LLaMA3-405B 作为迄今参数最大的开源LM，性能进一步提升。虽然405B由于资源限制未被广泛测试，但Meta称其为“目前最强大的开源基础模型”。推测405B在常识、数学、逻辑等任务上都达到了GPT-4同级，并可能在一些领域超越GPT-4。据一篇第三方报告，405B模型在 MATH 高等数学题库上显著领先70B模型，也超过了OpenAI的专用推理模型O1-mini。这些都表明LLaMA3系列已经把开源模型性能推至与顶级闭源模型分庭抗礼的地步。\n多模态推理：LLaMA3 系列扩展了视觉和语音能力（详见下一节），其Vision 版本 (LLaMA-3.2 90B) 可以对图像内容进行复杂的理解和描述 (Llama 3.2: Open-Source Edge and Multimodal LLMs - Jon Krohn)。例如，在图文QA基准 ScienceQA 图像问题上，该视觉模型可以达到媲美专用多模态模型的准确率。这显示出基础模型结合视觉训练后具备了强大的多模态推理潜力。\n推理开销与优化：模型性能的提升往往以更高的推理成本为代价。以下是推理效率方面的信息：\n上下文长度：LLaMA1 最大上下文2048，LLaMA2 提升到4096（双倍扩展主要通过RoPE和相应数据训练实现），LLaMA3 进一步提升到8192 (Context Length Limits in Bedrock | AWS re:Post)。更长的上下文允许模型处理更长的文档或进行更长的对话，但推理时间和内存使用也线性增加。LLaMA3通过GQA和高效注意力kernel，使8K长度下的推理仍可接受。而有报告通过位置插值 (positional interpolation) 技术，可以让LLaMA3推理时扩展到 32K 或更长的上下文 ([R] Why can Llama-3 work with 32K context if it only had 8K \u0026hellip; - Reddit)，对长文档问答非常有用。不过长上下文推理可能略降低准确率，需要均衡。\n内存占用：推理时，模型需要加载权重和维护KV缓存。LLaMA2-70B fp16需要约140GB显存；通过4-bit量化可降至~35GB，许多消费者级GPU组合即可运行。LLaMA3-405B 则非常庞大，光fp16权重就超过800GB，再加上KV缓存8K上下文则远超1TB。有用户估计405B模型完整精度推理需要约2TB显存。即使使用int4量化，仍需要数百GB的内存才能加载。这基本超出了单机能力，必须借助多机模型并行或磁盘缓存方案。因此，405B更多作为研究模型，其推理往往通过分布式集群完成（例如使用8台A100 80GB的服务器分担权重，各自处理部分注意力头）。\n速度：在单GPU上的推理速度，LLaMA系列随模型增大而下降。7B模型在消费级GPU上每秒可生成几十个token，70B模型可能只有每秒几token。但通过模型并行可以提高生成速度。LLaMA3-405B 若分布在比如16卡A100上，实测单卡每秒1-2 token，那么16卡集齐约每秒16-32 token，勉强达到实时对话的下限。而通过新的硬件（如GPU升级或FP8精度）以及批量生成，可以提升吞吐。NVIDIA 已经在优化 LLaMA3 推理的软件栈 (Deploying Accelerated Llama 3.2 from the Edge to the Cloud)。另外，像 DeepSpeed-Inference 和 TensorRT 等引擎也能对LLaMA推理进行加速优化。\nBenchmark成绩：总结一些标志性的成绩：在 MMLU 上，LLaMA1-65B ~63.4%，LLaMA2-70B ~68.9%，LLaMA3-70B ~74%，405B估计在77%以上（GPT-4约86%）；在 HumanEval 代码生成，LLaMA2-70B ~30%、LLaMA2-Code 34B ~53%, LLaMA3-70B ~48%，405B有望\u0026gt;50%接近GPT-4的67%；在数学 MATH基准，LLaMA2-70B ~25%，LLaMA3-70B ~35%，405B ~45%（GPT-4 ~50%）。这些数字表明LLaMA系列每一代都有大幅跨越，并逐步接近甚至追平当前最强闭源模型的水平。\n推理体验：LLaMA2-Chat 已经能比较流畅地用于对话助手，LLaMA3-Chat 则更进一步，它产生的回答更详细和精确。用户反馈LLaMA3模型在逻辑推理、多步骤算题上比LLaMA2表现更可靠，不容易犯简单错误，这得益于更多的训练数据和可能引入的链式思维微调。综合而言，LLaMA系列在推理阶段展现出一流水平的NLP能力，尤其在开放可用的模型中遥遥领先。同时，它也暴露了超大模型推理的工程挑战，这正推动研究社区在模型压缩和加速方面不断创新。\n8. 多模态能力 (Vision, Speech, Multimodality) 随着模型能力提升，LLaMA 系列也在探索向多模态AI扩展，包括视觉、语音等领域的实验和应用。\n8.1 视觉实验 (Vision Experiments) LLaMA 3.2 Vision：在 2024 年9月，Meta 发布了 LLaMA 3.2 系列，其中包含 视觉大模型 (Llama 3.2: Revolutionizing edge AI and vision with open \u0026hellip; - AI at Meta)。具体来说，LLaMA3.2 引入了 11B 和 90B 参数 两个 Vision-LLM 模型 (Llama 3.2: Open-Source Edge and Multimodal LLMs - Jon Krohn)。这些模型在基础 LLaMA 架构上融合了图像输入。实现方式可能是在 Transformer 的前若干层加入视觉编码模块。例如，使用一个预训练的CNN或ViT将图像转换为一串图像特征向量，再通过投影映射到与文本embedding同维度，作为额外的输入序列前缀喂给LLaMA模型。LLaMA3.2的90B视觉模型应该具备图像描述、视觉问答等能力，可以对输入图像进行复杂推理，如理解场景、识别物体关系并用自然语言回答问题 (Llama 3.2: Open-Source Edge and Multimodal LLMs - Jon Krohn)。在公开演示中，该模型能够根据图片内容回答细节问题，表现接近当时最好的多模态模型（如 OpenAI GPT-4V 等）。\n训练数据：Vision-LLM 的训练需要图文配对数据。Meta 很可能使用了大规模的公开图像-描述数据集（比如 LAION-400M、COYO等）以及内部收集的更高质量数据。模型可能先经过视觉专门预训练，然后在包含图像的多模态指令数据上微调，以学会遵循文本指令描述图像。例如“这张图片里有什么？”这样的问答对。这类似于BLIP-2等图文模型的训练流程。LLaMA3.2 Vision 90B 参数巨大，说明它有足够容量处理视觉信息，其在COCO图像字幕等任务上取得了接近或超过专用模型的结果 (Llama 3.2: Open-Source Edge and Multimodal LLMs - Jon Krohn)。\n架构细节：LLaMA3.2 Vision 大概率采用了LoRA适配或Q-Former等技术：也就是保持原文本模型权重冻结，仅训练一个较小的视觉适配模块。不过由于Meta有强大算力，他们或许直接联合训练了视觉-语言模型，使其在一个模型中同时具备两种模态能力。此外，Meta还推出了LLaMA Guard 3 Vision模型，用于对多模态对话进行安全过滤。这一11B参数的模型可以看作视觉版的安全守卫，能检测生成的图像描述中是否包含不良内容。\n8.2 语音实验 (Speech Experiments) 截至 LLaMA3，官方尚未明确发布语音输入能力的 LLM。然而，Meta 在 2023 年发布了 SeamlessM4T 等多语种语音-文本模型，以及 AudioCraft 等生成模型，显示出对语音AI的重视。未来展望部分Meta提到语音和推理是下一个重点。可以想见，LLaMA4 可能引入语音模块，使模型能够听懂和生成语音。\n潜在方案包括：将语音识别模型 (ASR) 提取的文本交给LLaMA处理，或者更先进的是直接将语音特征输入Transformer，比如利用音频编码器（类似Transformer的音频前端）生成表示，再融合进LLaMA。Meta的 Voicebox、AudioMAE 等研究为此提供了可能的接口。语音输出则可通过TTS引擎实现。虽然目前LLaMA本身不处理音频，但随着多模态统一模型趋势，未来很可能出现听觉-语言统一的大模型。我们可以期待LLaMA系列将语音理解纳入其技能集中，实现真正多模态的对话助手。\n8.3 代理与工具使用 (Agents \u0026amp; Tools) 另一个多模态/多能力方向是工具使用，即让语言模型调用非语言的外部工具完成任务。这包括执行代码、查询数据库、控制浏览器等等。OpenAI的Plugins、Tools API等已经展示了这种可能性。Meta在 LLaMA3 的博客中也谈到Agentic Capabilities，例如一个用LLaMA驱动的聊天机器人可以在 AWS 上调用各种服务以增强回答。\nLLaMA 要实现这一点，需要在训练后经过工具使用示例微调。社区已有项目将 LLM 与浏览器、计算引擎集成，模型学会在需要时输出特定格式，让工具执行后再读取结果继续回答。这实际上赋予模型一种额外的“模态”——API交互模态。LLaMA3 强化了推理能力和上下文长度，为嵌入工具交互打下基础。未来版本也许会直接支持调用一些常用API接口。\n8.4 多模态融合前景 LLaMA 系列正朝着统一多模态模型的方向演进。LLaMA3.2 已经证明视觉和文本的融合模型在开源中可行且性能出色 (Introducing Llama 3.2 models from Meta in Amazon Bedrock - AWS)。我们可以预见 LLaMA4 可能成为一个真正多模态的大模型平台：同时接受文本、图像、音频，甚至视频、表格等多种输入，并生成相应模态的输出（文本回答、语音回复、图像说明等）。\n这种多模态模型的意义在于：AI 将不再局限于文字对话，而是可以理解我们的语言、看到我们看到的、听到我们说的，从而更深入地参与到人类活动中。例如在增强现实中，模型可以通过摄像头看到用户环境并提供语音讲解；在办公助理中，模型可以看懂用户发来的截图或PDF然后给出总结。\n当然，多模态融合也面临挑战：需要巨量带标注的跨模态数据，以及更复杂的架构融合和对齐训练。但Meta的路线图清晰地表明他们在稳步推进这一目标。LLaMA 系列的多模态能力将不断拓展，最终形成一个通用的AI模型，具备类人“看、听、读、写”的综合智能。\n9. 核心技术解析：RMSNorm、SwiGLU、RoPE、GQA 等 在前文提到的 LLaMA 架构和训练中，有多项关键技术发挥了重要作用。下面对其中几个核心技术点进行更深入的解析，并辅以数学公式和代码示例来说明其实现细节。\n9.1 RMS Normalization (RMSNorm) 原理：RMSNorm (Root Mean Square Normalization) 是一种替代 LayerNorm 的归一化方法。不同于 LayerNorm 对每层输入执行 $(x - \\mu)/\\sigma$ 的均值方差归一，RMSNorm 仅使用输入的平方均值（均方根）进行缩放，无需减去均值。公式如下：\n$$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2 + \\epsilon}} \\cdot \\gamma, $$其中 $x \\in \\mathbb{R}^d$ 是待归一化的向量，$\\epsilon$ 是防止除零的极小值，$\\gamma$ 是可学习的尺度参数向量（与$x$维度相同）。可以看到，RMSNorm计算 $x$ 每个维度平方的平均，然后开方得到均方根，再用其倒数乘以$x$，最后乘以$\\gamma$进行缩放。没有偏移项和均值计算。\n作用：由于 RMSNorm 不涉及减均值操作，计算开销略低，并且实现更简洁。更重要的是，在Transformer中采用预归一化时，RMSNorm能提供跟LayerNorm相当的训练稳定性和性能表现。对于深层网络，RMSNorm 有时能避免 LayerNorm 带来的缩放不稳定问题。实际测算表明，用 RMSNorm 可减少层归一部分 7%～64% 的运行时间。LLaMA 是将 RMSNorm 用于每个子层输入处（即Pre-Norm Transformer架构)：在自注意力和FFN之前各有一层RMSNorm。这种设计保证了梯度更容易通过层归一传递。\n实现：在代码中，RMSNorm 一般实现为一个自定义的 nn.Module：\nimport torch import torch.nn as nn class RMSNorm(nn.Module): def __init__(self, dim: int, eps: float = 1e-6): super().__init__() self.dim = dim self.eps = eps # 初始化scale参数gamma为全1向量 self.weight = nn.Parameter(torch.ones(dim)) def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: # 计算每个样本的均方根：先求平方的均值，再开方 # x.shape: (batch, seq, dim) var = x.pow(2).mean(dim=-1, keepdim=True) # 均值 (最后一维) rms = torch.sqrt(var + self.eps) # 均方根 # 用均方根归一化，并乘以gamma x_norm = x / rms return self.weight * x_norm # 应用可学习缩放 上述实现中，我们先计算 var = x.pow(2).mean(dim=-1, keepdim=True) 得到每个向量的各维平方的平均值 (shape: batch × seq × 1)，然后 rms = sqrt(var + eps) 为均方根 (batch × seq × 1)。最后 x_norm = x / rms 完成归一化，再乘以 self.weight（即$\\gamma$参数）。需要注意，没有偏置参数，也不计算均值减去。这样就实现了RMSNorm (llama/model.py · chansung/LLaMA-7B at main - Hugging Face)。\n9.2 SwiGLU 前馈结构 原理：SwiGLU 是 Transformer 前馈层 (Feed-Forward Network, FFN) 中的一种激活和结构改进。传统FFN为两层全连接：$y = W_2(\\text{ReLU}(W_1 x))$。而 GLU（门控线性单元）结构将第一层输出拆成两部分，用一部分作为信号，一部分经过$\\sigma$激活作为门控，然后逐元素相乘。具体：$\\text{GLU}(x) = (xW_1 + b_1) \\otimes \\sigma(xW_2 + b_2)$。\nSwiGLU 则使用 Swish 作为激活函数替代 $\\sigma$。Swish定义为 $\\text{Swish}(z) = z \\cdot \\sigma(z)$，是Google提出的一种平滑激活，在Transformer等模型中表现优良。SwiGLU可以表示为：\n$$ \\text{SwiGLU}(x) = (xW_1 + b_1) \\otimes \\text{Swish}(xW_2 + b_2). $$其中 $W_1, W_2$ 将输入 $x$ 从模型维度映射到FFN隐藏维度（通常是4倍），$\\otimes$ 表逐元素乘。这样，相当于FFN第一层输出了 2×hidden_dim 的值，一半作为内容，一半作为门。Swish激活使门的开关更加平滑连续，相比$\\sigma$更有表达能力。\n优点：SwiGLU 改善了FFN的表达能力和梯度流动。相比简单ReLU，GLU结构让网络可以动态控制哪些特征通过（类似注意力机制作用于FFN），而Swish激活进一步提高了非线性程度。PaLM等大型模型采用SwiGLU据称使收敛更快，并提升了验证集表现。LLaMA 全系列都采用了SwiGLU替代原来的ReLU。\n实现：在PyTorch中，实现SwiGLU FFN比较直接。以隐藏维度 ffn_dim = 4×d_model 为例：\nimport torch.nn.functional as F class TransformerFFN(nn.Module): def __init__(self, d_model: int, ffn_dim: int): super().__init__() self.w_in = nn.Linear(d_model, ffn_dim, bias=False) # 输入到FFN self.w_gate = nn.Linear(d_model, ffn_dim, bias=False) # 门控分支 (与w_in相同输出维度) self.w_out = nn.Linear(ffn_dim, d_model, bias=False) # FFN输出回模型维度 def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: hidden = self.w_in(x) # (batch, seq, ffn_dim) gate = self.w_gate(x) # (batch, seq, ffn_dim) # Swish 激活: gate * sigmoid(gate) gated_hidden = hidden * F.silu(gate) # F.silu 实现 Swish，silu(x) = x * sigmoid(x) out = self.w_out(gated_hidden) return out 这里，我们使用了两个线性层 w_in 和 w_gate 作用于输入 x。hidden = w_in(x)，gate = w_gate(x)，然后通过 F.silu(gate) 计算 Swish(gate)，并与 hidden 按元素相乘，实现门控。最后通过 w_out 将维度映射回 $d_{\\text{model}}$。代码中将 bias=False，也是遵循LLaMA无偏置的设计。这样就构造了SwiGLU的FFN层。\n分析：通过这个结构，w_gate 控制了 w_in 输出哪些成分通过。当某些单元的 gate 输出接近0（Swish在负值时接近0），对应的 hidden 单元就被抑制为几乎0，实现了类似特征选择的功能；当 gate 较大时（正值使Swish接近线性通过），对应 hidden 单元就顺利通过。这种自门控机制，使模型可以根据输入动态调整通过FFN的路径，更灵活地建模复杂关系。这对提升模型在语言上的表现起到了积极作用。\n9.3 Rotary Positional Embeddings (RoPE) 原理：RoPE 是一种相对位置编码方法，最早由 Su 等人在论文 (Su et al. 2021) 中提出。它的核心思想是：将 Transformer 注意力中的 Query 和 Key 向量的某些维度对视为复数的实部和虚部，然后根据位置信息对这些二维坐标进行旋转变换，从而嵌入位置差。\n相比于经典的正余弦绝对位置编码 (APE) 将一个位置映射到一个固定向量，RoPE 实际上为任意长度的位置生成了一组相对相移。公式在前面架构部分已经给出：对于每个注意力头维度的一对分量 $(x_{2i}, x_{2i+1})$：\n$$ x_{2i}^{(p)} = x_{2i}\\cos(\\theta_{p,i}) - x_{2i+1}\\sin(\\theta_{p,i}), $$$$ x_{2i+1}^{(p)} = x_{2i}\\sin(\\theta_{p,i}) + x_{2i+1}\\cos(\\theta_{p,i}), $$其中 $p$ 是位置索引，$\\theta_{p,i}$ 是与位置和维度相关的旋转角度。通常$\\theta_{p,i}$选择为线性函数，如 $\\theta_{p,i} = p \\cdot \\lambda^i$，其中 $\\lambda$ 是一个常数小于1，用于控制不同维度的周期。在实践中，常将 $\\lambda = 10000^{-2/d_{\\text{head}}}$（类似Transformer绝对位置的频率设定）。这样$(x_{2i}, x_{2i+1})$这个二维向量在位置 $p$ 处旋转 $\\theta_{p,i}$。而对于另一向量在位置 $q$ 处，会旋转 $\\theta_{q,i}$。两者做内积时，旋转角的差 $\\theta_{p,i} - \\theta_{q,i}$ 影响了结果，相当于编码了 $p-q$ 的相对位移。\n特点：RoPE 带来几个好处：(1) 等效相对位置：它使注意力分数仅依赖位置差 $p-q$，模型可以以相对位置不变的方式泛化。这样LLaMA在训练时只见过2K或4K长度，但推理时可拓展更长不会遇到全新的位置embedding。(2) 数值稳定：相较传统的Transformer XL中的相对位置bias，RoPE通过代数操作嵌入，不引入额外参数，也不会破坏Attention的线性结构。(3) 简洁：RoPE易于实现，只需要对Q/K做前处理，对模型几乎无侵入式改动。\n实现：RoPE的实现通常在模型每层计算 Q 和 K 向量后进行。在代码中，可以预先计算好 cos 和 sin 表：\nimport math def precompute_rope_cache(seq_len, dim): # 假设 dim 是偶数，成对使用 theta = 10000 ** (-2 * torch.arange(0, dim//2, dtype=torch.float32) / dim) # theta shape: (dim/2,) pos = torch.arange(seq_len, dtype=torch.float32)[:, None] # (seq, 1) angles = pos * theta # (seq, dim/2) return torch.cos(angles), torch.sin(angles) # 返回cosine和sine表 然后在Attention前：\ndef apply_rope(q, k, cos, sin): # q, k shape: (batch, seq, n_heads, head_dim) # 先把最后一维拆成2个分量: q_reshaped = q.view(..., q.shape[-1]//2, 2) # -\u0026gt; (..., dim/2, 2) k_reshaped = k.view(..., k.shape[-1]//2, 2) # 应用旋转: (x, y) -\u0026gt; (x*cos - y*sin, x*sin + y*cos) q_x, q_y = q_reshaped[..., 0], q_reshaped[..., 1] k_x, k_y = k_reshaped[..., 0], k_reshaped[..., 1] # 广播 cos, sin: 假设 cos, sin shape = (seq, dim/2) q_rotated_x = q_x * cos[:q.shape[1]] - q_y * sin[:q.shape[1]] q_rotated_y = q_x * sin[:q.shape[1]] + q_y * cos[:q.shape[1]] k_rotated_x = k_x * cos[:k.shape[1]] - k_y * sin[:k.shape[1]] k_rotated_y = k_x * sin[:k.shape[1]] + k_y * cos[:k.shape[1]] # 合并回2维: q_out = torch.stack([q_rotated_x, q_rotated_y], dim=-1).view_as(q) k_out = torch.stack([k_rotated_x, k_rotated_y], dim=-1).view_as(k) return q_out, k_out 以上是RoPE的大致实现思路。通过预计算 cos/sin，我们在 forward 时对 q 和 k 相应位置进行旋转。这种实现等价于将embedding编码进q,k中。\nRoPE在LLaMA中的效果：实践证明，RoPE使LLaMA2能够从2K拓展到4K上下文无缝过渡。而LLaMA3虽然训练8K，但社区用插值法扩展到32K仍能保持较好性能 ([R] Why can Llama-3 work with 32K context if it only had 8K \u0026hellip; - Reddit)。RoPE相对于绝对位置embedding，不需要重新训练就可以外推，这是其巨大优势。这项技术现已被众多开源模型采用，如GPT-J, Mistral等，也成为长上下文LLM的标配之一。\n9.4 Grouped Query Attention (GQA) 原理：Grouped Query Attention (GQA) 是一种注意力头分组技术，旨在降低多头注意力的内存和计算成本，同时尽可能保持其效果。在标准多头注意力中，每个注意力头都有独立的 Q, K, V 投影矩阵，生成 $H$ 组不同的键和值。如果上下文长度为 $L$，每头需要存储 $O(Ld_h)$ 的K和V（$d_h$为每头维度）。当 $H$ 很大、$L$很长时（比如LLaMA2有32头，L=4096），总KV存储为 $H \\times L \\times d_h$，相当可观。\nGQA 的做法是：减少独立 K/V 的头数。例如，将32个头分成8组，则只需要计算8份 K和V，每组K/V被其对应的那组内的4个Q头共享。这样K/V存储缩小为 $G \\times L \\times d_h$，其中 $G$ 是组数。显然 $G \u0026lt; H$ 会节省内存和计算。极端情况 $G=1$ 则所有头共享一套K/V，这就是 Multi-Query Attention (MQA)。\n实现：假设有总头数 $H$，我们选择 $n_{\\text{kv}}$（即 $G$）个K/V头。例如LLaMA2-70B设置 $n_{\\text{kv}}=8$ 固定，不论 $H$（不同层可能H=32或64）。那么实现时：\nQ 投影：形状 $(B, L, H \\cdot d_h)$，最后reshape为 $(B, H, L, d_h)$。 K 投影：形状 $(B, L, n_{\\text{kv}} \\cdot d_h)$，reshape为 $(B, n_{\\text{kv}}, L, d_h)$。 V 投影：同K类似 $(B, n_{\\text{kv}}, L, d_h)$。 在计算注意力时，需要让每个Q头使用对应组的K/V。常见实现是：如果 $H=32, n_{\\text{kv}}=8$，可以约定第 $0-3$号Q头对应第0号K/V头，第 $4-7$号Q头对应第1号K/V头，以此类推。这样，计算注意力的时候，可以重用K/V。例如，在实现中，可以先将Q reshape为 $(B, n_{\\text{kv}}, \\frac{H}{n_{\\text{kv}}}, L, d_h)$，即先分组再列出组内Q头，然后K自然是 $(B, n_{\\text{kv}}, L, d_h)$可直接广播比较。\n效果：GQA大幅减少了KV缓存大小和内存访问。例如LLaMA2-70B，H=32，每头$d_h=1280/32=40$（假设embedding 1280），L=4096，那么标准MHA KV存储 = 32×4096×40 ≈ 5.24百万float，而GQA (n_kv=8) 则为8×4096×40 ≈ 1.31百万float，仅为原来的1/4。提到，GQA几乎不影响精度（尤其当共享的头数不多时，比如每组4个Q头），但带来了线性级别的效率提升，使得可以训练和使用更长上下文。\n与MQA区别：MQA是GQA的特例 (n_kv=1)。一些模型如GPT-3.5 Turbo reportedly使用MQA以优化服务器推理。MQA会轻微降低模型表达力，因为所有Q头看同样的K/V，会限制注意力多样性。GQA通过组的方式折中，实践表明**$n_{kv}=8$是个好平衡**：性能几乎无损，但显存减半以上。\nLLaMA2采用GQA也是为了能在相同硬件上支持双倍上下文（4K vs 2K）。LLaMA3继续这一设置，使8K成为可能。GQA如今被多个开源模型采用，如 Code Llama, Mistral 等都沿用了LLaMA2的做法，可见其通用价值。\n9.5 Byte Pair Encoding (BPE) 分词 原理：BPE 分词是一种基于数据统计的子词单元提取算法，被广泛用于训练语言模型的分词器。其思想是：以单字符为初始token集合，然后迭代地合并文本中最频繁的相邻符号对，作为新的符号，一直合并直到达到预定的词表大小。 (Tokenizer. Finetune here to talk a bit about… | by Anlatan - NovelAI)\n在实践中，LLaMA1和LLaMA2使用了SentencePiece库来训练BPE分词器 (Llama2 - Hugging Face)。SentencePiece会将训练语料统一处理，如添加空格标记，然后执行BPE算法。LLaMA1/2 最终得到约 32000 个 token 的词汇表 (Tokenizer. Finetune here to talk a bit about… | by Anlatan - NovelAI)。这些token包括常见词（特别是英语高频词可能作为单个token）、词缀、字符甚至空格和控制符。LLaMA的分词表特别针对代码也做了一定优化：例如包含 \u0026ldquo;def\u0026rdquo;, \u0026ldquo;function\u0026rdquo;, \u0026ldquo;{\u0026rdquo;, \u0026ldquo;}\u0026rdquo; 等符号token，以便更好地表示代码结构 (Tokenizer. Finetune here to talk a bit about… | by Anlatan - NovelAI)。\n特点：BPE 分词能在压缩文本长度和泛化之间取得平衡。较大的词表意味着常见词可以单个token表示，长度短，罕见词拆分成多个子词可以处理未登录词问题。32000的词表在英文文本上表现良好。但在多语言场景下，32000可能有点小，因为需覆盖各种语言的字词。LLaMA3因此扩充到128k词表 (I can not extend vocab of LLaMA-3 using sentencepiece anymore vs \u0026hellip;)。\n改进：LLaMA3改用的Tiktoken分词，可能还引入了多字节字符处理优化和空格显式标记等（OpenAI的分词特点） (I can not extend vocab of LLaMA-3 using sentencepiece anymore vs \u0026hellip;)。更大的词表128k显著减少了非英语文本和代码的长度。例如，以前表示一个中文字符需要独立的token（因为32000词表难以涵盖所有汉字），而128k词表或可直接包含高频汉字，实现1字符=1token，提升表示效率和模型性能（因为以前汉字被分成字节序列或者拼音字母序列，会极大拉长输入）。\n对模型的影响：良好的分词能减轻模型负担。token数量减少意味着序列长度短，在同样位置编码和注意力长度下模型能涵盖更多实际内容。代码等内容的处理也更直接，很多标识符不会被切碎。在LLaMA3中，代码数据占比5%，分词提升使模型在理解代码长变量名、稀有符号时更加得心应手。由于embedding层参数 = 词表大小 × 隐藏维度，128k词表也使LLaMA3的嵌入参数约增加4倍。不过相对于数百0亿的模型总参数，这部分增长可以接受，而且embedding层计算开销相对较小，不影响整体效率太多。\n句例：例如，对于英文句子 \u0026ldquo;Internationalization\u0026rdquo;：LLaMA2的BPE可能切分为 \u0026ldquo;▁Intern\u0026rdquo;, \u0026ldquo;ational\u0026rdquo;, \u0026ldquo;ization\u0026rdquo; 3个token（假设\u0026quot;▁\u0026ldquo;表示前面的空格）；而LLaMA3的更大词表也许有 \u0026ldquo;▁International\u0026rdquo;, \u0026ldquo;ization\u0026rdquo; 两个token，长度减半。对于中文 \u0026ldquo;你好世界\u0026rdquo;: 在SentencePiece BPE下可能是 \u0026ldquo;你\u0026rdquo;, \u0026ldquo;好\u0026rdquo;, \u0026ldquo;世\u0026rdquo;, \u0026ldquo;界\u0026rdquo; 4个token，而128k词表或许把 \u0026ldquo;你好\u0026rdquo; 当作一个token，\u0026ldquo;世界\u0026quot;一个token，长度减为2。这种差异对模型的训练和推理效率都有帮助。\n9.6 轻量级模型设计 (参数高效设计) LLaMA 系列在模型设计上强调“参数效率”，即用更少的参数达到同等甚至更好的性能。这在前文架构部分已有提及，这里汇总一些关键点：\n无偏置 (No bias)：移除线性层和LayerNorm中的偏置参数。这减少了一定数量的参数（虽然相对总参数不算太多），但更主要是简化计算图，减少不必要的加法操作。此外，有研究指出，去掉LayerNorm的beta与bias，可避免冗余的偏置作用，使模型更稳定。实践表明，没有bias不影响LLM表现，LLaMA验证了这一点，随后很多模型也跟进这一做法。\n适度的宽度/深度：LLaMA1在设计7B,13B,33B,65B时，遵循一定宽深比（例如65B是80层，hidden 8192），选取了较优的配置使参数利用率高。Meta在LLaMA1论文中比较了Chinchilla等的经验，确保模型没有明显的“宽而浅”或“窄而深”浪费。例如，相比之下，GPT-3 175B只有96层但非常宽（12288隐层，96 heads），属于偏宽的配置，推测LLaMA发现增深比无效宽度更划算，所以65B用了80层，比GPT-3深很多但总参数少得多。这样的结构让每一层都能有效提炼特征，没有闲置参数。\n优化的 FFN 比例：传统Transformer FFN是4×隐藏维度。LLaMA采用了SwiGLU后，实际上GLU结构需要的FFN隐藏维度可以略微减小以匹配参数量。例如PaLM论文中提到，用SwiGLU可以把FFN比例从4降低到3.5左右以保持参数不变。LLaMA模型具体是否调整了FFN维度未知，但很可能做了类似的调优，使得在应用门控后FFN部分参数不过多膨胀。每减少一些FFN维度，乘以层数也会节省大量参数。\n低熵初始化：这一点不是参数量，而是参数初始值的设置。为了训练稳定，LLaMA使用了较小的标准差初始化，使得初始模型接近线性（输出方差不过大），以此减少前期训练发散的风险。这属于小技巧，但能避免为纠正初始不佳而增加模型规模。\n训练阶段的参数高效：通过FSDP零冗余、共享梯度buff等技术，虽然不直接减少模型参数，但减少了有效需要存储的冗余副本。这样有限的GPU显存可以放下更多参数模型进行训练，从而LLaMA系列得以在相对给定硬件约束内把参数推到最大。\n综上，LLaMA系列在每一步都追求精简和优化。没有单纯为了参数大而大，而是结合数据和算力寻找最佳平衡点。正是这种对参数效率的极致追求，使得LLaMA1能用65B对抗GPT-3 175B；LLaMA2用70B追平ChatGPT；LLaMA3-70B直逼GPT-4。这给业界一个重要启示：并非参数越大越好，关键是每个参数是否发挥了作用。LLaMA通过架构和策略让参数“各司其职”，从而以小胜大、以巧胜多。\n10. LLaMA1、LLaMA2、LLaMA3 对比 下表总结了 LLaMA 一代、二代、三代模型的一些关键指标，包括发布时间、参数规模、训练数据、上下文长度和训练基础设施等：\n版本 发布 模型规模 预训练语料 上下文长度 训练计算 架构与特性 LLaMA 1 2023年2月 7B、13B、33B、65B 参数 ~1.0T tokens (7B/13B); ~1.4T tokens (33B/65B)来源：CommonCrawl (67%)、C4 (15%)、GitHub (4.5%)、维基 (4.5%)、书籍 (4.5%)、ArXiv (2.5%) 等 2048 tokens 2048×A100 GPU, 80GB训练约21天 (65B模型) RMSNorm、RoPE、SwiGLU、无偏置Seq2Seq Decoder-only LLaMA 2 2023年7月 7B、13B、70B 参数 ~2.0T tokens来源：未详列（公开数据，涵盖2023新数据） 4096 tokens 上万GPU (部分在Azure超算)训练若干周 (70B模型) 引入 GQA (8组)改进Tokenizer (SentencePiece BPE) (Llama2 - Hugging Face)优化对话微调、RLHF LLaMA 3* 2024年1月起 (3.0/3.1/3.2/3.3 更新) 8B、70B、405B 参数(3.2增补: 1B、3B edge模型；11B、90B Vision模型 (Llama 3 vs 3.1 vs 3.2 : r/LocalLLaMA - Reddit)) ~15T tokens (7×LLaMA2)包含 \u0026gt;5% 代码数据、多语种 (100+语言) 8192 tokens ([Context Length Limits in Bedrock AWS re:Post](https://repost.aws/questions/QUVcRYN1olTZyKwuIkyXh9rg/context-length-limits-in-bedrock#:~:text=Context%20Length%20Limits%20in%20Bedrock,have%20to%20do%20the%20same)) 两套 24k×H100 GPU 超算405B模型训练≈54天 注：LLaMA3 系列多次迭代发布：3.0 (8B/70B基础模型)，3.1 (405B模型和Guard模型), 3.2 (Vision和Edge模型) (Llama 3 vs 3.1 vs 3.2 : r/LocalLLaMA - Reddit), 3.3 (进一步调优和兼容改进)。上表综合为一列描述。\n表中可以看出，LLaMA每一代都有数据规模的指数级增长和上下文长度的翻倍，并通过架构优化来支撑更大模型的训练和推理。特别是LLaMA3时代跨入了超大模型(405B)和多模态，使其成为全面的AI模型平台雏形。与此同时，参数效率的理念贯穿始终，例如LLaMA2仅用70B参数就达到或超过前代65B + RLHF ChatGPT的水平。而LLaMA3-70B接棒成为新的主力开源模型，在性能上直追GPT-4。405B则探索了Chinchilla定律在超大规模下的效应。\n11. LLaMA 最新模型与其他大模型对比 LLaMA 系列的最新代表（如 LLaMA3.1 405B 或 LLaMA3-70B）与其他前沿大模型（包括开源和闭源）相比如何？下面通过一个比较表格，聚焦参数规模、训练方式和擅长领域：\n模型 类型 参数规模 训练方式/数据 主要特长 开源情况 LLaMA 3.1 (Meta) 通用LLM (Dense) 4050亿 (dense 全参数激活) 15T文本预训练 + 指令微调/对齐 综合NLP能力，代码\u0026amp;推理突出 开源 (社区许可证) LLaMA 3 (70B) 通用LLM (Dense) 700亿 15T文本预训练 + 指令微调/对齐 高效开源LLM，性能接近GPT-4 开源 (社区许可证) DeepSeek R1 推理LLM (MoE) 6710亿 参数(37B 激活/推理) 大规模预训练 + 强化学习(推理任务) 数学、逻辑推理、代码解题 开源 (Apache-2.0) OpenAI O1 推理LLM (Dense?) 未知 (推测数千亿)链式思维架构 从GPT-4衍生，强化学习提升推理 多步推理，严谨准确 (类似GPT-4) 非开源 (API 可用) OpenAI O3-mini 推理LLM (Dense) 未知 (~几十亿量级?) 推理数据RL微调，小模型高效 快速响应，中等规模但逻辑强 部分开放 (API+有限公开) GPT-4 (OpenAI) 通用LLM (Dense) 未公开 (估计数千亿-万亿) 多模态大模型，有监督+RLHF 综合能力顶尖，多模态（图像） 非开源 (API 可用) Claude 2 (Anthropic) 通用LLM (Dense) 1000亿+ (估计) 语料预训练 + 人类价值观对齐 长上下文 (100k+) 对话，安全性强 部分开放 (API) 注: O1/O3 是 OpenAI 的推理专用模型系列，确切架构和参数未公布；GPT-4 和 Claude 参数亦未公开，此处基于推测和侧面消息。\n比较分析：\n模型类型：LLaMA3 属于经典的密集Transformer模型，定位通用基础；DeepSeek R1 则采用Mixture-of-Experts (MoE) 架构，拥有极多参数但推理时只激活局部专家（37B），有效平衡了容量与计算。OpenAI O系列强调推理能力，可能在架构上引入了链式思维或规划模块，使其更擅长逻辑推理，但具体实现未知。可以猜想O1是类似GPT-4的大模型+特殊训练，O3-mini则是小模型也经过专门推理优化。\n参数规模：LLaMA3.1 405B 是目前开源可用的最大密集模型。DeepSeek R1名义上671B总参数最多，但由于MoE结构，其每次只用37B专家，相当于有效模型规模37B。OpenAI O1可能与GPT-4规模相当甚至更大（一些猜测认为GPT-4有1.5T参数 MoE架构），但未证实。O3-mini据称是“小型推理模型”，可能几十亿到百亿量级，用于低延迟场景。\n训练方式：LLaMA主要靠海量多样数据的自监督预训练，然后少量指令/对齐调优。DeepSeek R1 在预训练基础上通过大规模强化学习专攻推理任务（比如让模型自己反复解题，优化奖励）。OpenAI O系列显然也是用了RLHF或RLiFine-Tuning，使模型学会逐步思考和校验答案。可以说LLaMA代表海量纯知识学习，而O1/R1代表结合反馈的定向能力训练。\n能力表现：LLaMA3在通用能力上非常强，能胜任各种语言任务和代码生成，在很多benchmark上接近SOTA。DeepSeek R1 则在数学、逻辑推理和代码挑战上突出，被誉为首个开源的GPT-4级推理模型。OpenAI O1 是目前已知最强的逻辑推理模型之一，在复杂数学证明、谜题、编程挑战上表现卓越，据报道比GPT-4更善于严谨推理但有时不如GPT-4流畅对话。O3-mini尽管小，但通过精心训练在典型推理任务（算术、多步骤推理）上甚至超越了大模型，如超过DeepSeek R1的部分能力。不过小模型在知识储备和语言丰富性上不及大模型。\n开源与可用性：LLaMA系列和DeepSeek R1 都已开源，前者需要遵守Meta的社区协议（非商用等），后者采用Apache许可完全开放。这意味着开发者可以自由研究和部署它们。OpenAI的O1/O3和GPT-4、Claude等则未开源，只能通过API使用，并且详细信息保密。这导致学术界对其内部工作了解有限。LLaMA405B虽然提供了权重，但部署难度大；相比之下DeepSeek团队还开源了R1的蒸馏小模型（32B、70B），这些小模型性能可比肩OpenAI O1-mini。因此在开源生态中，LLaMA和DeepSeek形成了互补：一个提供通用大模型基底，一个提供推理特长的模型。\n小结：LLaMA最新模型在综合实力上位居开源前列，特别是405B作为通才模型拥有巨大的知识容量。DeepSeek R1等则体现了新颖训练方法带来的性能跃迁，在特定任务上可超过传统训练的更大模型。OpenAI的O1/O3系列展示了闭源模型通过RL等手段在推理领域取得领先，但开源模型也在迅速追赶。未来趋势可能是：开源模型（如LLaMA4）引入更多推理专长训练，而闭源模型也可能融入开源社区思想优化，如更高参数效率或多模态扩展。总之，LLaMA与同行先进模型的竞争与互鉴，将推动大模型能力持续提升。\n12. 代码示例：关键技术实现 本节通过简要的代码示例来解析 LLaMA 模型实现中的几个关键技术点，包括 RMSNorm、SwiGLU FFN、RoPE 位置编码 和 GQA注意力 的实现要点。这些代码以 PyTorch 风格给出，帮助读者直观理解模型背后的实际操作。\n示例1：RMSNorm 层实现 class RMSNorm(nn.Module): def __init__(self, dim: int, eps: float = 1e-6): super().__init__() self.eps = eps # gamma 初始化为全1向量（dim维） self.weight = nn.Parameter(torch.ones(dim)) def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: # 计算每个样本的均方根（RMS） # x.shape = (batch, seq, dim) var = x.pow(2).mean(dim=-1, keepdim=True) # 均值: (batch, seq, 1) rms = torch.sqrt(var + self.eps) # RMS: (batch, seq, 1) x_norm = x / rms # 归一化 return self.weight * x_norm # 缩放 说明：如前所述，RMSNorm 不做减均值，只除以均方根并乘以可学习参数 weight (相当于$\\gamma$) (llama/model.py · chansung/LLaMA-7B at main - Hugging Face)。上面代码中 var = x.pow(2).mean(...) 计算每个向量元素平方的平均值，再 sqrt 得 RMS。最后 x_norm = x / rms 完成归一化，再乘以 weight。偏置项被省略，只有尺度。这样就实现了 LLaMA 的 RMSNorm 层。\n示例2：SwiGLU 前馈层实现 class SwiGLUFFN(nn.Module): def __init__(self, d_model: int, expansion: int = 4): super().__init__() inner_dim = d_model * expansion # 前馈层扩展维度 # 两个线性层: 一个产生隐藏值, 一个产生门控值 self.w_in = nn.Linear(d_model, inner_dim, bias=False) self.w_gate = nn.Linear(d_model, inner_dim, bias=False) # 输出线性层，将隐藏维度投回 d_model self.w_out = nn.Linear(inner_dim, d_model, bias=False) def forward(self, x: torch.Tensor) -\u0026gt; torch.Tensor: # 计算隐藏和门控 h = self.w_in(x) # hidden g = self.w_gate(x) # gate # Swish激活门控: σ(g) * g = F.silu(g) (PyTorch的SiLU即Swish) gated = h * F.silu(g) # 投影回输出 return self.w_out(gated) 说明：这里我们定义了两个线性层 w_in 和 w_gate，它们输入维度都是 d_model，输出都是 inner_dim（4倍扩展）。forward中得到 h 和 g，然后计算 gated = h * F.silu(g) 实现SwiGLU门控。PyTorch中的 F.silu 函数实现了 $x * \\sigma(x)$ 的Swish激活，相当于计算 $\\text{Swish}(g)$ 并乘以 $h$。最后通过 w_out 将维度规约回来。这样，一个SwiGLU的前馈层就完成了。LLaMA中所有FFN层基本遵循这个结构，只是 expansion 可能略调而非严格4（例如3.5）来平衡参数。\n示例3：RoPE 位置编码的应用 def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor): \u0026#34;\u0026#34;\u0026#34; x: (..., seq_len, head_dim) 假定 head_dim 是偶数 cos, sin: (seq_len, head_dim/2) \u0026#34;\u0026#34;\u0026#34; d_half = x.shape[-1] // 2 # 将最后一维拆成两部分 x1 = x[..., :d_half] # 实部 x2 = x[..., d_half:] # 虚部 # 扩展 cos/sin 维度以匹配x (利用广播机制) cos_expand = cos[:x.shape[-2], :].unsqueeze(-1) # (seq_len, d_half, 1) sin_expand = sin[:x.shape[-2], :].unsqueeze(-1) # (seq_len, d_half, 1) # 按公式旋转 x_rotated_real = x1 * cos_expand - x2 * sin_expand x_rotated_imag = x1 * sin_expand + x2 * cos_expand # 合并旋转后的实部虚部 return torch.cat([x_rotated_real, x_rotated_imag], dim=-1) 说明：假设我们已经预计算好 cos 和 sin 表（形状 [max_seq, head_dim/2]），该函数对输入张量 x 应用RoPE。 ([PDF] arXiv:2404.19553v1 [cs.CL] 30 Apr 2024)实现上，将 x 的每个向量拆成实部 x1 和虚部 x2，然后按RoPE公式组合：x_rotated_real = x1*cos - x2*sin; x_rotated_imag = x1*sin + x2*cos。最后拼接回去。这样处理后的 x 含有位置相关的旋转信息。在LLaMA源码中，通常对 Q 和 K 分别 apply_rope 后再做注意力。\n示例4：多头注意力的 GQA 分组 class MHA(nn.Module): def __init__(self, d_model: int, n_heads: int, n_kv_heads: int): super().__init__() self.n_heads = n_heads self.n_kv = n_kv_heads self.d_h = d_model // n_heads # 每个注意力头的维度 # 投影矩阵 self.q_proj = nn.Linear(d_model, d_model, bias=False) self.k_proj = nn.Linear(d_model, d_model // n_heads * n_kv_heads, bias=False) self.v_proj = nn.Linear(d_model, d_model // n_heads * n_kv_heads, bias=False) # 输出 self.o_proj = nn.Linear(d_model, d_model, bias=False) def forward(self, x: torch.Tensor): B, L, _ = x.shape # Q: (B, n_heads, L, d_h) q = self.q_proj(x).view(B, L, self.n_heads, self.d_h).transpose(1, 2) # K, V: (B, n_kv, L, d_h) k = self.k_proj(x).view(B, L, self.n_kv, self.d_h).transpose(1, 2) v = self.v_proj(x).view(B, L, self.n_kv, self.d_h).transpose(1, 2) # 计算注意力分数 (QK^T / sqrt(d_h)), 注意 Q和K头数不同 # 需要将 Q 和 K 对应分组匹配: # 扩展 Q 或 K 以对齐维度，然后进行 batch matmul # 以下示例为简单起见假设 n_heads 是 n_kv 的整数倍: groups = self.n_heads // self.n_kv # 把Q reshape为(B, n_kv, groups, L, d_h) q_grouped = q.view(B, self.n_kv, groups, L, self.d_h) # 扩展K为(B, n_kv, groups, L, d_h) k_expanded = k.unsqueeze(2).expand(B, self.n_kv, groups, L, self.d_h) v_expanded = v.unsqueeze(2).expand(B, self.n_kv, groups, L, self.d_h) # 计算注意力 (组内) attn_scores = torch.einsum(\u0026#39;bngld,bngmd-\u0026gt;bnglm\u0026#39;, q_grouped, k_expanded) / math.sqrt(self.d_h) attn_weights = F.softmax(attn_scores, dim=-1) out_grouped = torch.einsum(\u0026#39;bnglm,bngmd-\u0026gt;bngld\u0026#39;, attn_weights, v_expanded) # 合并分组: (B, n_heads, L, d_h) out = out_grouped.view(B, self.n_heads, L, self.d_h) # 恢复形状: (B, L, d_model) out = out.transpose(1, 2).reshape(B, L, -1) return self.o_proj(out) 说明：上述代码示例展示了分组注意力的大致实现方法。其中 n_kv_heads 是 K/V 的头数（例如8），n_heads 是 Q 的头数（例如32）。为了简化，示例假设 $n_heads$ 是 $n_kv$ 的整数倍。实现流程：\n用 q_proj 得到全尺寸的 Q，然后 reshape 为 (B, n_heads, L, d_h)。 用 k_proj, v_proj 得到尺寸为 d_model // n_heads * n_kv 的向量，然后 reshape 为 (B, n_kv, L, d_h)。这样K、V头数只有 n_kv个。 将 Q reshape 为 (B, n_kv, groups, L, d_h)，其中 groups = n_heads/n_kv，表示每组包含的Q头数量。 将 K, V 扩展 groups 维度，使其 shape 变为 (B, n_kv, groups, L, d_h)，K的每个头复制groups次，对应每组。 计算注意力得分时，用 einstein sum 或批量矩阵乘处理组内 Q与对应K的乘积，再 softmax 得权重，然后乘以对应V，得到组内输出。 将组和 n_kv 两个维度合并还原为 n_heads，然后拼接输出。 通过这样的逻辑，就实现了Q头多于K/V头的注意力，即 GQA。实际Meta实现可能更简洁，比如通过张量reshape和广播巧妙处理，这里为清晰展开。\n检验：如果 $n_kv = n_heads$，groups=1，则 K/V 未复制，Q每组即自身，不损失信息，相当于回到普通多头注意力。如果 $n_kv=1$，则所有Q共享一个K/V，这就是MQA的情况。\n以上代码侧重于演示思想，忽略了一些实际例如掩码应用等。关键在于 K/V维度减少 带来的实现差异。LLaMA2正是使用了类似手段将每层注意力的 K/V 头固定为8，极大节约了KVCache内存。\n通过这些代码示例，我们可以更直观地理解 LLaMA 模型内部的重要技术点是如何实现的。从 RMSNorm 如何归一化张量，到 SwiGLU 如何门控前馈输出，再到 RoPE 对向量进行旋转编码，以及 GQA 如何共享K/V头，这些底层实现共同造就了 LLaMA 模型的高性能和高效率。\n13. LLaMA 系列未来发展展望 自 2023 年初 LLaMA 横空出世以来，短短一年多时间，LLaMA 系列已经完成三代飞跃，成为开源大模型的旗手。那么展望未来，LLaMA4 乃至后续版本将如何发展？特别地，会否朝着类似 OpenAI O1/O3 和 DeepSeek R1 这样的推理增强方向演进？本节结合当前趋势和Meta官方暗示进行探讨。\n1. 更强的推理和工具使用：OpenAI 的 O1/O3 模型证明了通过链式推理训练，语言模型在复杂推理任务上可以显著超越常规训练的同级模型。LLaMA4 很可能借鉴这一思路，引入Reasoning Transformers的概念。例如，增加让模型在回答前生成隐含推理步骤（Chain-of-Thought）的训练，或采用自我提升 (self-refine) 机制，让模型迭代完善答案。Meta 的年终博客已指出，将专注提升模型的推理能力和工具使用。因此，LLaMA4 有望在数学、逻辑、规划等需要多步推演的任务上大幅提升，缩小与OpenAI O系列的差距，甚至凭借更大模型规模实现超越。我们可能会看到 LLaMA4-Chat 在推理类基准（如MATH、GSM8K、CodeForce题目）上达到前所未有的高度。\n2. 多模态彻底融合：LLaMA3.2 已经迈出视觉模态融合的第一步，LLaMA4 应该会扩展多模态边界。一方面，加入语音功能几乎是确定的方向。Meta拥有强大的语音数据和模型（如MassiveText语音翻译等），将其结合LLaMA，可使模型直接处理语音输入输出。另一方面，视觉模态将进一步完善，也许405B模型也会有视觉版，使其在图像推理上媲美人类水平。甚至，LLaMA4 可能探索视频理解，即在模型中加入对时间序列视觉的建模，使得模型可以看视频、讲解视频内容或生成视频字幕。多模态融合的终极目标是打造“一模多能”的AI——例如，用户给出一段音频和一张图片，模型能听懂音频内容、看懂图像场景，再用文字回答问题或用语音对话。这将极大拓展模型的应用场景。\n3. 参数规模与架构：405B模型的成功表明，Meta有能力训练并开源超大模型。下一步，他们会继续推高参数吗？值得注意的是，DeepSeek R1 采用了MoE实现671B参数，而405B是密集模型。LLaMA4 也许会尝试Mixture-of-Experts架构，使得参数量级上到万亿级成为可能，但推理成本仍可控制在百亿级别。这意味着LLaMA4可能拥有如“LLaMA4-1T (含16专家每次激活1/16)”这样的配置。如果成功，模型的知识容量将空前强大。然而，也有观点认为，仅靠堆参数收益渐少，下阶段重点应转向精细打磨模型，如推理能力、知识升级等，而非纯规模扩张。因此，Meta可能暂时不会急于推出更大密集模型，而是优化405B的使用效率（通过蒸馏、稀疏化等）。轻量模型也是一大方向，LLaMA3.2已经推出1B/3B微型模型，LLaMA4或许致力于让十亿级模型也能有不错表现，这样才能真正实现边缘部署和个性化小模型的落地。\n4. 数据与训练：由于OpenAI等公司开始封闭其模型输出，未来高质量开放语料变得尤为重要。LLaMA4可能利用Meta自有的庞大社交数据（如果能匿名公开的话），或者像近期的Jais模型那样针对垂直领域（例如金融、法律）引入专门数据训练。另一方面，合成数据将扮演更大角色，比如用405B模型自回归产生海量难度更高的数据，再训练70B模型，提高小模型能力（Self-Feeding）。这种思路在DeepMind的AlphaGo 自我博弈、OpenAI的命名实体发明数据中已有体现。Meta或许也会用LLaMA自己来产生挑战性数据，提升下代模型的上限。\n5. 对齐和安全：模型越强大，安全和伦理风险越高。LLaMA4应该会更深入地融合对齐方案。例如，将LLaMA Guard引入一个端到端框架中，训练出自带过滤功能的模型，不需要外接Guard而在生成时就能避开敏感内容。Anthropic提倡的宪法AI(Constitutional AI) 可能也被Meta参考，让模型遵循一套多语言的价值观准则自行调整回答。这样做的目标是让LLaMA4成为“既聪明又善良”的AI。同样，对抗鲁棒性、事实准确性（减少幻觉hallucination）方面，也会有技术加强，例如引入检索增强(RETRO/Realm)或工具调取来查证事实。这会影响模型架构（需要模块接口）和训练（需要检索数据）。\n6. 竞争与定位：LLaMA4 面临来自各方的竞争，如Google可能推出的更强PaLM、开源界的Mistral 2.0等、更专注推理的DeepSeek R2等。Meta大概率会坚持开源策略，以社区共创来抵御竞争压力，正如LLaMA2/3已经积累起庞大生态。LLaMA4若开放出来，又有巨大参数和多模态，本身将成为AI研究的大型实验平台，加速全行业的进步。这也是Meta“开放创新”的理念体现。\n在 Meta 官方博客《The future of AI: Built with Llama》中，有一句总结：“Imagine what we will do in the next 12 months.” 可以想见，接下来的一年，LLaMA4 很可能横空出世，再次引领AI风潮。或许LLaMA4不会以单纯参数取胜，而是以更聪慧、更通用、更安全的形象出现，让开源AI向真正的AGI又迈进一大步。作为AI从业者，我们期待并将持续关注这一系列的演进，它不仅是技术的革新，更是AI民主化进程的重要里程碑。\n参考文献（References）:\n【1】Meta AI Blog: The future of AI: Built with Llama 【3】IBM: What is grouped query attention (GQA)? 【5】Medium: Takeaways From the Llama 3 Release Paper 【6】DeepSeek R1 GitHub Release Notes 【7】OpenAI API Documentation: Reasoning models (o1 \u0026amp; o3) 【8】Llama Official Site (llama.com) 【9】ArXiv: Llama 2: Open Foundation and Fine-Tuned Chat Models 【11】ArXiv: Llama Guard 3 Vision: Safeguarding Human-AI Image 【13】Medium: Meta Llama 3: The most capable openly available LLM 【14】NVIDIA NIM: deepseek-r1 Model 【15】Medium: Review — LLaMA: Open and Efficient Foundation LMs 【16】Meta AI Blog: Introducing LLaMA: A foundational 65B LLM 【19】Medium (dair-ai): LLaMA - Papers Explained 55 【20】llama.com: Meta Llama 2 【21】Medium: LLaMA: Concepts Explained 【22】Medium: Comprehensive Comparison of Llama Series 【23】Meta AI Blog: Introducing Meta Llama 3 【24】GitHub Issue: Llama 3 tokenizer (I can not extend vocab of LLaMA-3 using sentencepiece anymore vs \u0026hellip;) (Llama2 - Hugging Face) 【26】Meta AI Blog: Llama 3.2: Vision and Edge (Llama 3.2: Open-Source Edge and Multimodal LLMs - Jon Krohn) (Llama 3 vs 3.1 vs 3.2 : r/LocalLLaMA - Reddit) 【27】StackExchange: Llama 2 number of steps (training) (What is the Llama2 number of steps? [closed] - Cross Validated) (LLaMA 2: The New Open Source Language Model - E2E Networks) 【28】Adrian Colyer: Reading the LLaMA code (llama/model.py · chansung/LLaMA-7B at main - Hugging Face) 【29】AWS re:Post: Llama-3 context length (Context Length Limits in Bedrock | AWS re:Post) 引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nOpenAI Deep Research. (Feb 2025). OpenAI Deep Research 案例分享 - LLaMA 系列技术演进详解. https://syhya.github.io/zh/posts/2025-02-15-deep-research-llama3\nOr\n@article{syhya2025deepresearch, title = \u0026#34;OpenAI Deep Research 案例分享 - LLaMA 系列技术演进详解\u0026#34;, author = \u0026#34;OpenAI Deep Research\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-02-15-deep-research-llama3\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-02-15-deep-research-llama3/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e注意\u003c/strong\u003e：下面内容是由 OpenAI Deep Research 生成的研究案例 😊，模型会有一定幻觉和格式上的错误，仅供参考。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e原文链接\u003c/strong\u003e: \u003ca href=\"https://chatgpt.com/share/67b15090-a5b8-8011-83c9-01cd6d962617\"\u003eLLaMA 系列技术演进详解\u003c/a\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e好的，我将撰写一篇详细的技术博客，全面介绍 LLaMA 系列的发展进展，包括从 LLaMA 1 到最新 LLaMA 3 的对比，以及其在模型架构、训练方法、推理性能等方面的演进。我会加入数学公式来解析关键技术点，如 RMS Normalization、FFN_SwiGLU、RoPE、GQA 和 BPE，并提供代码示例帮助理解。此外，我还会整理 LLaMA 最新模型与其他大模型（如 DeepSeek R1、O1/O3 等）的对比数据。\u003c/p\u003e","title":"OpenAI Deep Research 案例分享 - LLaMA 系列技术演进详解"},{"content":"OpenAI Deep Research (OpenAI, 2025) 主要是采用以 o3 推理模型为基座的多 agent 协作方式。相关开源研究和产品之前也有不少，比如 GPT Researcher 和 Google Gemini 版本的 Deep Research (DeepMind, 2024)。不过 OpenAI 的产品是博主目前体验下来效果最好。之后咨询分析、论文综述和新闻编辑等任务都可以先让这个工具自动化完成，然后再由人工进行最后的修正和审查，怎么感觉之后我写技术博客的任务也会被替代了呢？😂\n此外，另一个 OpenAI 近期发布的 agent 产品是 Operator (OpenAI, 2025)，这是一个类似 WebVoyager (He, et al, 2024) 的工具，能够让 agent 在浏览器上进行交互操作，并且操作过程中如果有错误，可以人为进行修正。\n注意：下面内容是由 OpenAI Deep Research 生成的研究案例 😊，模型会有一定幻觉和格式上的错误，仅供参考。\n原文链接: 深度研究技术综合研究报告\n感谢您的详细说明！我将进行深入研究，以撰写一份关于“深度研究”技术的综合报告，主要聚焦于 OpenAI 的 Deep Research，同时涵盖 Google Gemini Deep Research 等类似技术。\n该报告将：\n详细探讨 OpenAI o3 模型的架构及其优化技术。 讨论深度研究所使用的 NLP、网络抓取、机器学习、语义嵌入等核心技术。 结合参考案例进行分析，比较 OpenAI Deep Research 与 Google Gemini Deep Research。 提供相关技术挑战、解决方案以及未来发展趋势。 采用论文格式，引用 arXiv、顶会论文、OpenAI 博客等英文来源。 我将尽快完成研究并为您提供详细报告。\n深度研究技术综合研究报告：OpenAI Deep Research 与 Google Gemini Deep Research 比较 1. 深度研究的技术概述 OpenAI Deep Research 及 O3 模型架构：“深度研究”（Deep Research）是 OpenAI 于 2025 年推出的一种新型自动化研究代理。它由即将发布的 OpenAI O3 模型的特制版本提供支持 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。O3 模型基于大型Transformer架构，具备多模态处理能力和超大参数规模，专为网页浏览和数据分析任务优化 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。该模型在设计上继承了 GPT-4 等前代模型的强大自然语言理解和生成能力，同时通过优化推理链和工具使用接口，能够像人类研究员一样自主执行多步骤研究任务。OpenAI 表示，用户只需提供研究课题提示，Deep Research 代理就会自动搜索、解析并综合上百个在线来源，产出一份全面、有引文的研究报告 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。这一过程据称在十分钟内完成，相当于人类需要数小时才能完成的工作 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central) (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。O3 模型还具备处理文本、图像和PDF等多种输入的能力，使其能分析各种格式的信息 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。架构上，O3 通过引入可微分的外部记忆模块和函数调用接口，强化了与工具和知识库交互的能力。这意味着模型可以将互联网视为扩展“记忆”，在生成答案时检索外部信息，从而缓解仅靠参数存储知识的局限 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)。总的来说，OpenAI Deep Research 基于 O3 模型实现了从问题理解、资料检索到信息综合的端到端自动化，为复杂研究任务提供了新的解决方案。\n网页浏览与数据分析优化技术：Deep Research 特别针对网络信息检索和数据分析进行了优化。首先，代理采用内置浏览器环境执行网页搜索和爬取。这类似于早期的 WebGPT 实验，模型在一个文本浏览环境中可以发出搜索查询、点击链接并读取网页内容 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。与普通搜索引擎返回结果摘要不同，Deep Research 逐页提取原始资料并深入分析。为提高检索效率，系统可能使用查询优化和并行爬取技术：例如根据用户课题生成一系列相关查询词，批量检索多条结果；同时利用多线程抓取多个网页，以减少等待时间。抓取过程中，代理运用了HTML解析和内容过滤策略，自动跳过无关的导航、广告等元素，只保留正文信息进行分析。这种内容提取过程可能结合启发式规则和模型判断，以提炼出每篇文章中与课题相关的段落。\n在数据分析方面，Deep Research 内建了轻量级的数据处理和统计功能。例如，当课题涉及数值数据（如市场份额、增长率等）时，代理可以调用嵌入的 Python 运行环境对收集的数据进行计算、制表和绘图 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。Techopedia 报道指出，OpenAI Deep Research 支持 Python 集成，可以即时生成图表并执行数值计算，以便对趋势进行分析 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。这种能力类似于 ChatGPT 的“代码解释器”工具，允许模型在沙盒中运行代码，对数据做进一步处理，然后将结果纳入报告。因此，在浏览器抓取到包含表格或统计数据的内容后，代理能够编写小段代码计算同比变化、绘制趋势图等，从而更深入地挖掘数据背后的模式。这些优化技术使 Deep Research 在处理涉及大量数据的课题时，既能保证对源信息的全面覆盖，又能进行适当的加工和验证。\n推理机制与自主研究流程：OpenAI Deep Research 的核心是一套逐步推理 (step-by-step reasoning) 机制，指导代理完成多阶段的研究流程 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。这一机制借鉴了学术界提出的 ReAct (Reason+Act) 框架 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。具体来说，模型在每个迭代中都会交替地产生“推理轨迹”和“动作指令” ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。推理轨迹是模型内在的思考过程，例如分析当前有哪些信息缺失、下一步该搜集什么；动作指令则是可执行的操作，如访问搜索引擎查询、打开特定链接、调用工具函数等。通过在推理和操作之间交替，模型能够一边思考一边获取新信息，从而逐步逼近完整答案 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。这一过程类似人类研究员在调研时的工作流：先根据问题分解子问题，搜索文献获取答案片段，再整合验证。Deep Research 会根据初始提示自动提出澄清性子问题或假设，然后针对每个子问题检索证据；若发现某路径结果不满足要求，还会动态调整计划（backtracking），尝试其他信息源 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。例如，OpenAI的代理在开始正式搜索前，会先根据用户提示向用户提问以澄清需求，明确研究范围和衡量指标 (OpenAI Deep Research vs Gemini Deep Research)。接着，它按照“数据提取-\u0026gt;数据验证-\u0026gt;注释标记-\u0026gt;趋势分析”的顺序迭代推进 (OpenAI Deep Research vs Gemini Deep Research)。这种多轮推理使代理具有高度的自主性和灵活性，能够处理复杂的课题。研究表明，此类推理-行动交替策略有助于减少大语言模型的幻觉和错误传播，因为模型会及时用检索到的真实信息来校正和支撑其推理 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。\n值得注意的是，OpenAI Deep Research 强调透明的推理过程。在用户界面上，代理的每一步行动和背后的理由都会实时展示给用户 (OpenAI Deep Research vs Gemini Deep Research)。例如，它会显示当前正在搜索什么关键词，找到了哪些来源，以及选择某个来源的原因。这种透明度一方面增加了结果的可信度，另一方面也方便用户介入指导。如果模型误解了需求，用户可以在中途纠正。相比之下，我们将看到 Google 的方案在这点上有所不同。这种以推理为驱动、结合工具行动的机制，使 Deep Research 能够像人类一样“深度”研究：不仅检索大量信息，还能反思信息质量、交叉验证事实，并不断调整策略直至得到可靠、详尽的结论 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。\n2. 使用的技术 OpenAI Deep Research 和 Google Gemini Deep Research 都融合了多种人工智能与数据处理技术，以实现自主的深入研究。下面分几个方面讨论它们使用的关键技术：\n大规模自然语言处理模型：两家的深度研究代理均由最先进的大型语言模型 (LLM) 提供智能支持。OpenAI 的 O3 模型和 Google 的 Gemini 模型都是数千亿到万亿参数级别的Transformer网络，经过海量语料预训练，具备强大的自然语言理解和生成能力 (Gemini (language model) - Wikipedia)。这些模型可以精准地理解复杂研究请求，生成结构良好的长篇回答。此外，它们都支持多模态输入。O3 模型可接收文本、图像、PDF、表格等作为上下文 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)；Gemini 模型则从一开始就以多模态训练，能够统一处理文本、图像、音频、视频等多种数据 (Introducing Gemini: Google’s most capable AI model yet) (Introducing Gemini: Google’s most capable AI model yet)。例如，用户可以上传学术论文PDF或统计图表图片，OpenAI Deep Research 能直接解析其中内容并纳入分析 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。Google Gemini 在设计上也是原生多模态 (natively multimodal)的——它并非将不同模态的子模型简单拼接，而是在预训练时就让模型同时学习图像像素、音频频谱和文本序列，从而能够跨模态进行推理 (Introducing Gemini: Google’s most capable AI model yet) (Introducing Gemini: Google’s most capable AI model yet)。这使 Gemini 在多模态推理任务上表现出色，例如解读复杂图文信息并据此发现隐藏的知识 (Introducing Gemini: Google’s most capable AI model yet)。在语言处理方面，模型采用链式思维 (Chain-of-Thought) 等提示技术增强推理能力 (GPT-4 Technical Report) (GPT-4 Technical Report)。OpenAI 在 GPT-4 技术报告中指出，通过 few-shot 提示示例引导模型执行逐步推理，可以显著提升复杂任务表现 (GPT-4 Technical Report)。总之，深度研究代理借助最尖端的大模型作为大脑，支撑其理解任务、生成详实报告的能力。\n网络爬取与数据提取：为了获取所需的最新资料，这些代理都集成了网页搜索和爬取技术。OpenAI Deep Research 通过 ChatGPT 插件式的浏览器接口或内部工具实现自动网页浏览。模型能够构造搜索引擎查询，从返回结果中筛选有用链接并抓取页面内容 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。爬取过程结合了自然语言解析和HTML处理：代理或许使用预置的正则规则、DOM解析库，或直接利用语言模型本身来识别网页中的主要内容段落。例如，它可能让模型总结网页内容，以快速判断该页是否相关，然后决定是否深入阅读。对于半结构化的数据源（如维基百科、政府统计表格等），代理可以使用定制的爬虫或API获取结构化数据。Google Gemini Deep Research 则直接依托 Google 强大的搜索基础设施进行信息获取 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。它利用 Google 实时索引的海量网页，使得检索极为迅速且覆盖全面 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。代理会自动从搜索结果中爬取必要网页，并将内容输入其语言模型进行处理。在数据提取上，模型可能采取逐段阅读和语义过滤相结合的方法：将长篇文章按段落分割，利用语义嵌入或内容摘要来挑选出与查询相关的段落重点阅读 (Deep Chat JS, semantic search, and OpenAI integration) (Deep Chat JS, semantic search, and OpenAI integration)。这样可以避免将整篇无关内容塞入上下文，节省token空间和计算。深度研究还善于信息抽取，比如从新闻或论文中定位人名、时间、数值等关键实体，并整理到笔记中。对于需要的数据（如统计指标），代理可能搜索官方数据库或报告，并爬取表格数据。然后通过编程或内置函数将HTML表格转换为可用的数值。整个网络爬取与提取过程在幕后高度自动化，确保代理获得高质量、多样化的信息供后续分析使用。\n特定算法与人工智能技术：深度研究代理的研发融合了多项前沿 AI 技术和算法。首先，上文提到的 ReAct 推理+行动框架就是一个关键算法思想 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。通过在模型推理过程中引入决策动作，ReAct使模型可以与外部环境交互，从而获取新信息来支撑后续推理。这有效降低了纯静态推理时的幻觉风险，在OpenAI和学界的评测中被证明能显著提高问答的事实准确性 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。其次，基于工具的强化学习也是相关技术之一。OpenAI 的代理很可能经过专门的微调训练，使其善于调用搜索、浏览、代码执行等函数。例如，Meta AI 提出的 Toolformer 方法表明，可以通过在训练数据中加入工具使用示例，让模型学会何时调用哪个API以及如何利用返回结果 (Toolformer: Language Models Can Teach Themselves to Use Tools)。Deep Research 可能应用了类似思想，模型在遇到知识空缺时会主动发出搜索动作，而非凭空编造答案。\n另外，为了提高生成内容的可靠性，这些模型还经过人类反馈强化学习 (RLHF) 调优。OpenAI 早在 WebGPT 项目中就使用人类偏好反馈来训练模型给出有引用支撑的答案 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。Deep Research 代理的回答需要合乎学术标准，引用真实来源，因此推断其训练过程中也融入了“回答必须有引用依据”的奖惩机制 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。模型如引用了不存在的文献将被扣分，从而学会去引用真实检索到的资料。这与 WebGPT 在ELI5问答上的做法相似：模型需在回答中附上证据来源，由人类评审其答案是否充分且来源可靠 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。Google Gemini Deep Research 则可能借鉴 AlphaGo 式的强化学习来增强规划能力。DeepMind CEO Demis Hassabis 曾表示，Gemini 结合了 AlphaGo 的强项和大型语言模型的能力 (Gemini (language model) - Wikipedia)。AlphaGo 等强化学习算法善于长序决策和规划，Gemini 因此可能在研究任务上加入了类似MCTS（蒙特卡洛树搜索）或策略梯度方法，使其能预先规划研究步骤。事实上，Gemini代理在开始执行前会生成一份研究计划供用户审核 (OpenAI Deep Research vs Gemini Deep Research) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)，这体现了一种规划算法的应用。总结来说，深度研究代理并非单靠一个语言模型完成任务，而是整合了检索、阅读、计算、规划等多个模块，各模块由相应算法驱动：检索模块用搜索引擎算法，阅读理解由NLP模型完成，计算分析借助嵌入的编程环境，规划决策通过强化学习和提示工程来实现。这种多技术融合使代理具备了“全栈”能力，从而胜任复杂的研究流程。\n向量数据库与语义嵌入：在信息处理过程中，深度研究代理大量运用了语义嵌入 (semantic embedding)技术和向量数据库来高效管理知识。语义嵌入是将文本转化为高维向量的方法，使语义上相似的文本在向量空间中距离更近 (Deep Chat JS, semantic search, and OpenAI integration) (Deep Chat JS, semantic search, and OpenAI integration)。无论是 OpenAI 还是 Google 的系统，都可能在后台构建了一个向量索引：当代理抓取了几十份相关文档后，它可以将这些文档片段转成向量存储在内存或临时数据库中。随后，当代理需要某方面的信息时，不必全文逐页翻找，而是通过向量相似度检索快速定位最相关的片段 (Deep Chat JS, semantic search, and OpenAI integration)。例如，代理在分析一个话题的不同侧面时，可以针对每个子课题将已收集资料向量化，然后对用户的新提问或下一个推理点生成查询向量，在向量数据库中检索Top K相近的段落 (Deep Chat JS, semantic search, and OpenAI integration)。这种方式大大加快了信息检索速度，并减少重复爬取。同样，向量数据库还有助于融合多来源信息：模型可将来自不同网站的相似内容聚类，以发现共识或差异。这也有利于可靠性分析——如果多个嵌入相近的段落来自不同权威来源，模型会更信任其中信息。OpenAI 的嵌入模型（如 text-embedding-ada 系列）以及向量索引库（如 FAISS、Qdrant）为此提供了支持 (Deep Chat JS, semantic search, and OpenAI integration)。实际应用中，开发者经常采用检索增强生成 (Retrieval-Augmented Generation, RAG)框架 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)：先用嵌入检索知识，再把相关内容喂给LLM生成回答。Deep Research 正是RAG思想的高级实践——自动完成了检索和生成两个步骤。在这一过程中，Top-K最近邻搜索算法被用来从数以百计的向量中选出最相关的若干段落 (Deep Chat JS, semantic search, and OpenAI integration)。这种最近邻搜索通常基于高效的数据结构（如HNSW图），可以在子毫秒级返回结果，即使向量库很大也足够快。另外，语义嵌入还能用于结果去重和多样性：如果不同搜索查询返回的结果在向量空间非常接近，说明内容高度相似，可以避免重复阅读同质信息，从而将精力用于补充新的视角。综上，语义嵌入和向量数据库是深度研究系统的信息“记忆”和检索引擎，支撑了其对海量资料的高效利用。\n可扩展性与性能优化：深度研究代理为了处理大规模数据和长时任务，采用了一系列性能优化策略来保证可扩展性。首先，并行处理是提高效率的主要手段之一。OpenAI Deep Research 在多步骤流程中可能会并行执行某些独立任务。例如，在初始检索阶段，代理可以同时发出多个不同的搜索查询（针对课题的不同子问题） (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)；对于获得的一批网页，也可以并发抓取和粗略摘要，再由主模型逐一精读分析。这种并行爬取和预处理能够充分利用多核CPU和分布式计算资源，减少等待瓶颈。Google 的系统在这方面更具优势，借助其强大的云计算和搜索架构，可以在短短几分钟内抓取并分析几十个来源 (OpenAI Deep Research vs Gemini Deep Research)。其次，逐层细化策略提高了处理长文档的能力。代理不会尝试一次性将所有文本都放入上下文，而是分层摘要：先用一个较小的上下文窗口获取文档要点，再根据需要细读其中相关段落。这有点类似分页读取，加上向量检索筛选相关页的手段，使其能够扩展处理超出单次上下文长度的内容。此外，模型可能利用分段内存或检查点机制来保持长程信息：例如，将中间结论和已验证的事实存入一个临时内存（可能也是以嵌入形式），在后续步骤通过检索引用，而不必把早期所有内容一直携带在prompt中。这种外部化记忆可以看作扩大了有效上下文。\n在生成长篇报告时，模型也应用了分段生成策略，以确保连贯和效率。它可能先生成报告的大纲（包括章节结构），然后逐段填充具体内容。每一章节完成后，再结合下一章节需要的信息进行衔接。这样做可以将一次生成拆分成多次，小段落生成更快且可控。同时，代理注重源引用缓存：当某段内容已经找到了满意的来源并引用，后面若再次需要相似信息，会直接调用之前缓存的出处以避免重复查询。所有这些措施令深度研究代理能够在5-30分钟内完成一个复杂课题的全面研究 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (OpenAI Deep Research vs Gemini Deep Research)。Google Gemini Deep Research 通常更快，5-15分钟即可完成，因为其流程较为固定且充分利用了Google现有的搜索速度优势 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。最后，从基础架构看，OpenAI 为专业用户提供的 Deep Research 服务限制每月约100次查询、每次最长30分钟 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。这样的限制也是出于可扩展性考虑，避免单个用户长时间占用计算资源，从而服务更多用户。在未来，随着模型和硬件效率提升，这些时间和次数限制有望逐步放宽，使深度研究技术以更大规模应用。\n3. 参考项目和实现 **OpenAI Deep Research 与 Google Gemini Deep Research 比较：**OpenAI 和 Google 都推出了各自的深度研究代理，它们在目标功能上相似，但在实现细节和用户体验上存在显著差异。以下从多个方面对比两者的特点：\n模型与多模态能力：OpenAI Deep Research 基于 OpenAI O3 模型，支持文本、图片、PDF、表格等多模态输入，甚至可以处理用户上传的复杂数据 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。它还能在报告中嵌入生成的图表、可视化等，属于真正的多模态研究助手 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。Google Gemini Deep Research 则以 Google Gemini 2.0 模型为核心 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。Gemini 本身是多模态模型，但当前 Gemini Deep Research 服务主要面对文本查询，输入以文字为主 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。输出上，OpenAI 报告包含引用、摘要和嵌入式可视化，而 Google 输出的是带关键发现和链接的文本报告 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。不过，Google 报告常附有结构良好的数据表格，并提供导出到 Sheets 的选项 (OpenAI Deep Research vs Gemini Deep Research)。总体而言，OpenAI 的多模态处理和输出呈现更丰富；Google 则注重与其办公套件集成，实现结构化结果。\n研究方法与流程：OpenAI Deep Research 强调自主的多步迭代。代理会像研究员一样动态调整搜索和分析策略，过程完全透明地展示给用户 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。例如，它先提出澄清性问题，得到用户确认后再开始正式研究 (OpenAI Deep Research vs Gemini Deep Research)；在研究过程中，实时显示当前执行的步骤（搜索关键词、读取某站点等）及初步发现，并可能绘制中间的分析图表 (OpenAI Deep Research vs Gemini Deep Research)。整个流程高度交互且时间投入较大 (5-30分钟) (OpenAI Deep Research vs Gemini Deep Research)。相比之下，Google Gemini Deep Research 采用预先规划的结构化方法。它一开始会生成一份研究计划，列出要采取的步骤（如“首先搜索X，然后分析Y，最后撰写报告”），让用户可以在执行前修改这个计划 (OpenAI Deep Research vs Gemini Deep Research) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。然后代理按照预定流程执行，各步骤对用户而言是静默进行的，中途不再像OpenAI那样持续展示推理过程 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。这种方法优点是效率高且流程可预测：用户知道大概要花5-15分钟，得到的是规范成文的报告 (OpenAI Deep Research vs Gemini Deep Research)。但缺点在于缺少中途的灵活调整——若检索信息不充分，Gemini 可能不会像OpenAI那样自行回溯增补 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。因此，OpenAI 的方法更适合需要深挖和探索的课题，Google 的方法适合快速获取结构化结论的场景 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。\n透明性与可控性：OpenAI代理在可解释性上更胜一筹。用户可以实时看到模型的“思考”，包括每一步为什么这么做，以及数据从何而来 (OpenAI Deep Research vs Gemini Deep Research)。这种透明度使用户对过程和结果更有信心，也方便在过程中介入指导（例如提供新的线索或纠正方向）。Google代理则过程不透明：除了一开始的计划，执行中只会显示正在阅读的网站列表 (OpenAI Deep Research vs Gemini Deep Research)，用户并不知道模型具体如何判断信息质量。这使得Google代理在使用中更像一个“黑箱”，只能通过结果来评估过程。如果结果有遗漏或偏差，用户需要另行手动研究补充。为提高可控性，Google允许用户在开始前编辑它的研究计划，相当于高层次地约束其行为 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)；而OpenAI的用户则是低层次地观察每步、随时反馈。这体现了两者在产品设计上的不同哲学：OpenAI追求人机协同，Google侧重自动完成任务。\n结果深度与呈现：在报告输出方面，OpenAI Deep Research 倾向于提供深入的分析和洞见，即使引用来源相对较少也无妨 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。它会给出背景概述、不同角度的讨论、历史趋势分析，甚至根据数据得出战略性建议 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。例如，在移动通信普及率的案例研究中，OpenAI代理不仅列出了各国的数据对比，还标注了历史趋势线和区域差异，并结合数据给出了市场拓展建议 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。Google Gemini Deep Research 则注重覆盖更广泛的资料来源并提炼关键信息 (OpenAI Deep Research vs Gemini Deep Research)。它可能从更多的网站获取数据（一次研究中来源数可能与OpenAI相当甚至更多），然后将要点综合到一份井井有条的报告中 (OpenAI Deep Research vs Gemini Deep Research)。Google的报告通常包含明确的**“研究方法”说明**（解释搜集了哪些来源，如何确保质量）和数据汇总表 (OpenAI Deep Research vs Gemini Deep Research)。这些表格按指标列出多实体对比，使读者一目了然 (OpenAI Deep Research vs Gemini Deep Research)。同时每段末尾附有该段所用来源的列表 (OpenAI Deep Research vs Gemini Deep Research)。相比之下，OpenAI的报告是在句中直接插入引用标记（如使用【†】形式），并在旁边列出所有来源，指向精确的证据 (OpenAI Deep Research vs Gemini Deep Research)。这使OpenAI报告的引证粒度更细，读者可以精确定位每句话的出处。而Google报告虽也详列引用但未逐句标注 (OpenAI Deep Research vs Gemini Deep Research)，需要读者自行对照内容和参考文献。总结而言，OpenAI输出偏重深度解析和实时交互（带图表、交互注释等），Google输出偏重正式文档化（带完整章节和表格） (OpenAI Deep Research vs Gemini Deep Research)。前者适合希望理解过程和细节的决策者，后者适合需要成品报告做决策的经理或学生 (OpenAI Deep Research vs Gemini Deep Research)。\n成本与定位：OpenAI Deep Research 当前仅向 ChatGPT Pro 专业版用户开放，订阅费用约为每月 200 美元，限制使用次数 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。定位于金融、政策、学术等专业人士，他们有高价值需求，愿意为深度分析付费 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。Google Gemini Deep Research 则包含在较大众的 Google One AI 增强套餐中，每月只需20美元即可使用（Gemini Advanced计划） (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。因此它吸引的是学生、创业者和普通研究者等更广泛的用户群，他们可能不需要极致深度但追求性价比和速度 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。价格上的巨大差异（10倍）也反映了两家公司产品策略：OpenAI走高端专业路线，Google走普惠大众路线。在资源投入上，OpenAI每次查询耗费大量计算（长时间、多步骤），Google每次则受益于已有搜索基础设施较为轻量。服务形式上，OpenAI集成在ChatGPT界面中，以按钮形式启用 Deep Research 模式 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)；Google则可能通过 Bard 的“Gemini 1.5 Pro 深度研究模式”提供，在 Bard 界面或Docs侧边栏中开启深度研究功能 (ChatGPT\u0026rsquo;s Deep Research vs. Google\u0026rsquo;s Gemini 1.5 Pro with Deep \u0026hellip;)。因此，OpenAI更像一个独立研究助手，而Google更像常规搜索/聊天的扩展。\n综上，OpenAI Deep Research 与 Google Gemini Deep Research 各有优势：前者在推理深度和过程透明上更强，适合需要信任和细节的专业应用；后者在速度、集成和成本上更有优势，更贴近一般信息查询的升级版 (OpenAI Deep Research vs Gemini Deep Research)。选择哪种取决于用户需求：若追求交互式深入洞察，OpenAI是值得的投入；若偏好快速得到结构化结果且预算有限，Google方案是不錯的选择 (OpenAI Deep Research vs Gemini Deep Research)。\n相关 OpenAI 项目：OpenAI Deep Research 的开发并非孤立无援，它建立在此前一系列OpenAI项目的基础上，也和其他同期项目并行发展。一个直接的先驱是 2021年的WebGPT实验 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。WebGPT 将GPT-3模型与一个文本浏览器环境相结合，通过模仿人类上网找答案并辅以人类反馈训练，证明了语言模型可以执行浏览器查询、阅读网页、引用来源来回答问题 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。Deep Research 明显继承了 WebGPT 的理念，将其拓展为通用的多步研究。另一个相关项目是 OpenAI 2023年开始在ChatGPT中提供的插件功能，尤其是浏览插件和代码运行器。这些插件让模型可以调用外部工具，访问网络或执行代码，与Deep Research的能力有相似之处。可以认为Deep Research将这些插件的功能整合为一个统一的代理，使之更自动化和面向特定任务。\n2023年中，OpenAI的GPT-4模型开放了函数调用 (Function Calling) 接口，允许开发者定义工具供模型调用 (Function calling and other API updates - OpenAI)。Deep Research 很可能利用了函数调用机制来执行搜索和抓取等动作：即定义诸如 search(query)、open_url(url)、run_python(code) 等函数，由模型在对话中决定调用，从而实现与外部环境交互。这种架构极大增强了模型处理复杂任务的能力，也是Deep Research技术栈的一部分。\n除了研究任务本身，OpenAI还在推进其他自主代理。例如OpenAI Operator是2025年1月推出的另一款代理，用于控制计算机执行各类任务 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。Operator 可看作是面向操作系统/软件操作的自动化助手，而 Deep Research 面向知识获取领域。两者都属于OpenAI探索通用代理 (Agent) 的一环，被称为一系列“推理模型 (reasoning models)”中的成员 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。员工透露OpenAI已有 O1、O2、O3 等不同代号的推理模型，不断提升推理和自主能力 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。可以预见，这些项目之间会有技术和经验的共享，例如Operator在规划多步骤任务上的经验可反哺Deep Research。\n值得一提的还有一些社区驱动的项目，如2023年流行的Auto-GPT和BabyAGI等开源自主代理。这些项目尝试让GPT-4自动分解目标、连续调用自身来完成复杂任务，被视为通往 AGI 的尝试。虽然不是OpenAI官方产品，但OpenAI显然从中获得了启发。Deep Research 更加稳健和有针对性，相当于一个受控的“AutoGPT”用于研究领域，避免了无目的循环。但理念上都在探索让AI 自主连续地工作。因此，Deep Research 可以看作 OpenAI 在专业受控环境下实现自主 AI的实例。这一项目与上述OpenAI插件、Operator代理一起，构成了OpenAI AGI研究路线的重要布局 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。\n技术案例研究：实际应用示例\n为了更直观地了解深度研究代理如何运作，这里提供一个实际案例，比较OpenAI和Google代理的表现：\n案例: 移动互联网渗透率与语言学习市场分析。假设我们希望研究“过去5年中前10个发达国家和前10个发展中国家的移动电话普及率变化，并比较iOS/Android的使用率，以及各国有多少人口有意愿学习另一种语言”，最终产出一份报告并给出在哪些市场推出语言学习App机会最大 (OpenAI Deep Research vs Gemini Deep Research)。\n**OpenAI Deep Research 执行：代理首先与用户交互，提出澄清问题：“这里的‘渗透率’是指占总人口的百分比吗？语言学习意愿是否有数据来源？”用户确认渗透率按百分比，并希望它自行假设缺失数据。 (OpenAI Deep Research vs Gemini Deep Research)得到明确需求后，代理开始多步研究。它将任务拆解为几部分：1）搜集各国历年移动普及率数据，2）获取各国iOS与Android市场份额，3）调查各国居民学习第二语言的意愿数据，4）综合分析并给出建议。代理首先查询世界银行或ITU统计报告获取移动普及率，并找到过去5年的数据表；随后搜索移动操作系统市场份额的报告，得到前十大国家OS占比的数据；再搜索教育或调查报告，找到各国关于语言学习意向的调查结果。每一步搜索后，代理都会验证数据可靠性，例如对比多个来源的数值是否一致，并标注来源 (OpenAI Deep Research vs Gemini Deep Research)。在分析阶段，OpenAI代理使用Python计算各国渗透率5年增长率，绘制趋势图（这些图在生成报告时会转换成Markdown格式的ASCII图表或描述） (OpenAI Deep Research vs Gemini Deep Research)。它还对发达国家与发展中国家进行了组间对比，发现发展中国家总体增速更快。接着，代理将所有信息组织成报告：开头是对移动渗透和语言学习的背景概述，其次按发达和发展中两组分别讨论数据，插入一张对比表格列出每个国家的当前渗透率、iOS占比、Android占比和语言学习意愿百分比。报告还包括“研究方法”**小节，说明数据来源和可靠性（这一点是Google报告默认包含的，但OpenAI代理为了专业性也会生成说明，以获得用户信任） (OpenAI Deep Research vs Gemini Deep Research)。在结论和建议部分，OpenAI代理综合数据指出：一些发展中大国（例如印度、巴西）移动普及率虽快速提升但仍有提升空间，且语言学习意愿高，而这些国家Android占比远超iOS；因此建议如果推出语言学习App，应优先面向Android平台并针对这些市场 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research)。整个过程中，OpenAI代理共引用了大约30个来源，每个关键数据后的括号中标注了来源编号，对应参考文献列表，方便读者核查 (OpenAI Deep Research vs Gemini Deep Research)。完成时间约12分钟，得到一份详细、图文并茂且具有战略见解的报告 (OpenAI Deep Research vs Gemini Deep Research)。\nGoogle Gemini Deep Research 执行：对于相同的课题，Google代理在用户提交查询后，首先自动生成了研究计划，例如：“计划：1）使用Google搜索获取20国移动渗透率数据，2）获取iOS/Android占比数据，3）获取语言学习意向数据，4）整理分析并生成报告。”用户可以编辑这个计划（本例中用户接受计划，但为了公平将OpenAI澄清的问题加入提示） (OpenAI Deep Research vs Gemini Deep Research)。然后代理开始执行而无需进一步提问。它利用Google强大的搜索直接获取到了相关数据来源列表（可能包括世界银行数据库、StatCounter网站、EF英语能力报告等）。由于速度较快，约6分钟后，代理完成了信息收集和分析 (OpenAI Deep Research vs Gemini Deep Research)。Google代理将输出发送到Google文档作为最终报告。报告结构清晰，包含引言、方法、分析和结论等部分 (OpenAI Deep Research vs Gemini Deep Research)。其中“研究方法”部分详细说明了数据来自何处，例如引用了ITU报告2024版、StatCounter 2023年统计、EF教育报告等，并陈述已交叉核对主要指标 (OpenAI Deep Research vs Gemini Deep Research)。报告的分析部分以段落叙述各国数据趋势，穿插表格比较。例如，有一个表格列出了“发达 vs 发展中国家的移动渗透率（2018和2023年）、年均增长、iOS%和Android%、第二语言学习意愿%” (OpenAI Deep Research vs Gemini Deep Research)。这些数据都有脚注标明来源编号，每段末尾列出引用链接 (OpenAI Deep Research vs Gemini Deep Research)。不过，报告正文没有将单句与具体来源一一对应（这点不如OpenAI的细粒度引用透明）。Google代理的结论聚焦于关键发现：比如“发展中国家智能机渗透仍落后发达国家约20个百分点，但增长更快 (OpenAI Deep Research vs Gemini Deep Research)，Android主导新兴市场90%以上份额，同时这些国家有平均40%以上的人有学习新语言的意愿”，然后建议“针对印度尼西亚、尼日利亚等Android用户众多且语言学习需求高的国家制定市场策略”。整个报告行文流畅、风格正式，有如咨询公司白皮书。由于过程不可见，我们只能从结果推测，Google代理在内部也是先把数据存入临时结构（或许利用其知识图谱）然后生成。不过，它在结果中很强调来源可靠性，并无明显幻觉错误。执行时间比OpenAI快，大约6分钟，来源数量同样在25-30个左右 (OpenAI Deep Research vs Gemini Deep Research)。\n比较分析: 这个案例显示，OpenAI Deep Research 和 Google Gemini Deep Research都能胜任复杂多维的数据分析任务，但各有侧重。OpenAI代理提供了更深入的探索过程：通过不断提问和交互，确保对需求的理解万无一失，并在分析中加入了更多解释和可视化（如趋势图）。这使报告富有洞察力，但也更费时。Google代理则充分发挥工具和速度优势：借助搜索和表格，它迅速汇总信息并排版成报告，省去了交互时间。其结论虽中肯详实，但在洞察力和创新建议上可能略少一分“惊喜”，更像是中立地呈现事实。这契合它设计用于高效信息汇总的定位。值得注意的是，两者都体现了AI代理超越人力的效率：如OpenAI员工所说，Deep Research 在10分钟完成了传统上需要3小时的市场调研工作 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。这一效率提升对知识工作者具有革命意义。\n4. 关键研究论文和学术参考 深度研究代理结合了近年来多个前沿研究方向的成果。下面总结几篇奠基性和最新的研究工作，它们构成了该技术的学术基础：\nReAct: Synergizing Reasoning and Acting in Language Models (Yao et al., 2022) ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models) ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models) – 这项工作提出了令LLM同时生成推理轨迹和动作指令的ReAct框架。传统上，“链式思维”侧重让模型写出思考步骤，而“工具使用”是分开研究的。ReAct 将二者融合，模型一边内在推理一边外部行动（如查询知识库）。论文通过在问答和复杂决策任务上实验，证明ReAct不仅提高了模型准确性，还使模型行为更可解释、更可信 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。例如在开放域问答中，引入Wikipedia查询动作可以纠正模型原本不可靠的推断。ReAct 框架正是像OpenAI Deep Research这样代理系统的灵魂：让模型自己决定何时需要搜索或工具，从而动态地完成多步任务。这篇论文发表于2023年ICLR会议，引用了丰富的实验数据，奠定了LLM作为自主代理的理论基础。\nWebGPT: Browser-assisted Question-Answering with Human Feedback (Nakano et al., 2021) ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback) ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback) – OpenAI的研究团队在这篇论文中探索了让GPT-3模型学会上网找答案的方法。他们构建了一个文本浏览环境，模型可以输入搜索查询、点击链接阅读网页，并必须在回答中引用来源 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。通过模仿人类示范（行为克隆）和加入人类反馈奖励，WebGPT 模型在长文问答数据集ELI5上表现超越了人类回答的平均水平 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。这项工作最重要的贡献在于引入人类偏好优化使模型学会引用真 实来源，而非臆造答案 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)。WebGPT展示了语言模型能用于实际的网络信息查询，并达到高可信度输出，对后续深度研究代理的开发具有里程碑意义。\nRetrieval-Augmented Generation (RAG) for Knowledge-Intensive NLP (Lewis et al., 2020) ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks) ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks) – 来自Facebook AI的研究者提出了RAG模型，将预训练语言模型与非参数知识库结合，用以解决知识密集型任务。论文指出，仅靠参数记忆知识的LM难以更新知识且缺乏溯源机制，而融合了检索模块的模型能动态获取最新信息并提供来源 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)。RAG框架使用一个神经检索器从Wikipedia向量索引中找出相关段落，再让生成模型以此为条件生成答案 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)。实验证明，在开放域问答等任务上，RAG比纯语言模型和传统检索+抽取方法取得更好效果 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)，并生成更具体、更真实的回答。这奠定了后来诸多系统的范式，包括Bing/GPT-4的检索增强模式以及深度研究代理的设计思路：知识检索 + 生成。RAG作为NeurIPS 2020论文，影响深远。\nGemini: A Family of Highly Capable Multimodal Models (Gemini团队, 2023) () () – 这是一份由Google DeepMind发布的技术报告，详细介绍了Gemini模型的架构与性能。报告指出，Gemini 1.0系列包括Ultra、Pro、Nano三个规模，旨在在跨模态领域取得突破 ()。Gemini使用了联合训练的方法，让模型同时学习文本和图像、音频、视频，从而拥有原生的跨模态理解和推理能力 ()。实验结果令人瞩目：Gemini Ultra在测试的32项基准任务中有30项达到新SOTA，在MMLU综合考试基准上首次达到人类专家水平（超过90%） () ()。这证明了大模型在多模态和推理上的新高度。对于深度研究，Gemini报告的意义在于展现了大模型多模态融合的潜力，以及在推理任务上赶超甚至部分超越GPT-4的实力，为Google Gemini Deep Research提供了强大的模型支撑。\nDemis Hassabis 对 Gemini 模型的展望 (Hassabis, 2023) (Gemini (language model) - Wikipedia) – 在一次接受《连线》(Wired)杂志采访时，DeepMind CEO Demis Hassabis 谈到了即将推出的Gemini模型。他强调Gemini将融合AlphaGo的能力与Google/DeepMind过往语言模型的优势，有望在能力上超越OpenAI的ChatGPT/GPT-4 (Gemini (language model) - Wikipedia)。AlphaGo等强化学习系统擅长长序规划和博弈推理，将这些思想融入Gemini意味着模型可能具有更强的决策规划和问题求解能力。这番言论虽非正式技术细节，但代表了业界对下一代通用智能模型的期许，即通过借鉴不同AI领域的顶尖成果来打造更全面的AI。事实也证明，在Gemini 2.0中Google引入了原生工具使用和规划功能，推动其成为“agentic”时代的基础 (Google introduces Gemini 2.0: A new AI model for the agentic era)。Hassabis的观点为理解Google Gemini Deep Research背后的理念提供了背景：那就是将强化学习、规划与LLM结合，实现代理式智能。\nGPT-4 Technical Report \u0026amp; System Card (OpenAI, 2023) (GPT-4 Technical Report) (GPT-4 Technical Report) – 虽然OpenAI对GPT-4模型的细节讳莫如深，但技术报告和系统卡片提供了关于安全和伦理的重要信息。其中提到，GPT-4在输出中仍存在各种偏见，OpenAI为缓解这些问题做出了许多努力，但彻底解决仍需时间 (GPT-4 Technical Report)。报告描述了OpenAI通过增加安全相关的RLHF数据和规则奖励模型，来降低有害或偏颇内容的产生，并希望模型默认行为能反映广大用户的价值观，同时允许在一定范围内定制 (GPT-4 Technical Report)。这些安全措施和价值对齐方法与深度研究息息相关。因为一个研究代理需要避免被网上充斥的偏见或错误信息误导，也要防止输出不当结论。OpenAI在GPT-4上的安全策略，如对抗性测试和模型辅助的安全审查，为Deep Research的开发提供了指导，确保代理在自动检索开放网络信息时不会一味采信不可靠来源，而是倾向权威数据 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。此外，OpenAI还强调了隐私、虚假信息等风险 (GPT-4 Technical Report)。这些考量在深度研究场景下体现为：对爬取到的数据进行可信度打分、过滤阴谋论或极端内容，以及在报告中明确引用来源增加透明度。GPT-4系统卡片和相关研究（如TruthfulQA测评等）为开发深度研究代理提供了伦理与安全准绳。\n其它相关研究：除了上述主要工作，许多研究为深度研究技术提供了支撑。例如，Stanford 大学2022年的 “Self-Ask” 方法探索了让模型自行提出子问题并用Google搜索寻找答案，从而逐步解决复杂问答 (Do Large Language Models with Reasoning and Acting Meet \u0026hellip; - arXiv)。这种思路与Deep Research代理的自我提问类似。Meta AI的 “Toolformer” (Schick et al., 2023) 则展示了模型可以通过少量有监督信号学会使用各种工具API (Toolformer: Language Models Can Teach Themselves to Use Tools)。这直接启发了深度研究代理调用多种工具（搜索、计算）的能力。此外，在长文档处理上，Google Research的模型如 LongT5、Transformer-XL 等关于扩展上下文和记忆的研究，也可能贡献了技术要素。LangChain 等开源框架在工业界提供了构建自主代理的范式，包含计划-执行-记忆模块划分，与Deep Research类似的流水线思路。学术界对知识可信度评估、事实验证的研究（如 Fever 数据集等）帮助代理判断网络信息真伪。总之，深度研究技术不是凭空出现的，而是站在众多NLP、IR（信息检索）、RL（强化学习）工作的肩膀上，集大成地将这些成果应用于自动化研究这一创新场景。\n5. 技术挑战和解决方案 尽管深度研究代理表现出令人惊叹的能力，但在实现过程中也面临诸多技术挑战。下面讨论一些关键挑战以及采用的应对策略：\n数据质量保证与可靠性：互联网信息良莠不齐，如何确保代理引用的数据准确可信是首要挑战。为此，Deep Research 代理实现了多层面的质量控制。首先在检索阶段，它倾向于权威来源。OpenAI代理会优先选择学术论文、官方统计、知名媒体等作为信息来源 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。模型可以根据域名特征（如“.edu”, “.gov”）或站点知名度来判断可信度。此外，代理常常交叉验证关键事实：如果某数据只在一个来源出现，模型会尝试寻找第二个来源印证，避免孤证。第二，在引用过程中，模型只引用它实际看过并解析过的资料，这通过训练和系统约束保证。例如WebGPT的方法，让模型必须提供引用才能得高分 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback)，因此Deep Research也会自发去检索能够佐证其断言的页面，没有依据的断言会被模型自身识别为不可靠而避免输出。第三，系统针对常见错误信息有过滤机制。OpenAI可能内置了一个“黑名单”或使用大型语言模型自身来评估内容的真实性。一些明显谣言或虚假医学信息，模型通过大概率识别并不予采用。这类似于OpenAI在GPT-4安全系统中所做的，对训练和生成进行约束以减少传播错误信息 (GPT-4 Technical Report)。最后，用户在环路中仍扮演重要角色：深度研究报告提供详尽引用，使用户能够方便地自行验证。如果某条信息看起来可疑，用户可以一键打开引用链接核查原文。这种可验证性确保了即使AI遗漏了不实信息，用户也有机会识别并纠正。因此，多管齐下的措施提升了代理输出的可靠性。当然，这并非万无一失，仍需警惕模型可能的幻觉引用（引错来源）或来源本身的不准确。为进一步改进，可能引入专门的事实核查模型对代理草稿进行审核，在最终输出前纠错。\n大规模数据处理的可扩展性：面对动辄数十上百份文档的研究任务，如何让代理高效处理且在硬件限制内运行是一大挑战。前面提到的一些并行和分层策略缓解了这个问题。然而，仍有必要考虑极端情况：若用户问题需要全网爬取大量资料（比如系统综述），模型上下文长度或调用次数都可能不堪重负。为此，OpenAI和Google采取了任务范围限制和资源调度策略。系统可能对每次Deep Research请求设定了隐含的资料检索上限（例如最多访问50个网页或检索1小时的内容）。一旦达到上限，代理会开始整理已有信息，即使还有未完全覆盖的角度，也优先产出一个可用报告。这保证了延迟不会无上限增长，也防止单次任务耗尽资源。另外，Google由于有自主搜索索引，许多查询的答案可能已被摘要存储，可以直接获取，而不必真正去访问远端站点（例如知识图谱或FAQ提供快速答案）。OpenAI则可能在自家域（如openai API文档、已有数据库）上构建了部分索引，提高常见知识的访问速度。流式处理也是方案之一：代理可以一边爬取一边输出初步结果，以流水线方式工作，而不是等所有数据拿到后才开始分析。不过当前Deep Research产品界面未体现流式输出，猜测其内部可能还是同步处理。为了扩展性，OpenAI正在探索Team版本（团队共享）和插件API，使多个用户可以并行使用代理而互不干扰。这需要优化后端服务架构，如对模型实例的复用和负载均衡。未来，随着更高性能模型（如O4）和更高效算法（如稀疏注意力）的出现，处理百篇文档甚至整站抓取将更为可行，深度研究代理的可扩展性也会水涨船高。\n数据隐私与安全、偏见和不当信息的处理：深度研究代理在上网搜索时，可能接触到涉及隐私或敏感的内容。同时，模型本身可能带有训练数据中的社会偏见。这些都是必须谨慎应对的伦理挑战。首先，在用户数据隐私方面，当用户提供自己的资料（如私密PDF）让代理分析时，OpenAI承诺不会将这些内容用于训练，并采取严格的访问控制，确保这些敏感数据仅在完成请求所需的短期内存在于内存 (Peer review of GPT-4 technical report and systems card - PMC)。如果代理需要访问需要登录验证的页面（如个人账户数据），当前设计中一般不会去抓取，以免越权。此外，OpenAI和Google都对深度研究代理加入了内容过滤模块。当模型检索到潜在不安全内容（暴力、色情、仇恨言论等），可能会触发过滤规则，中止该方向的研究并提醒用户。这类似于ChatGPT内置的审核系统，确保代理不会因为检索开放网络而变成不受控地输出违规内容。\n关于偏见缓解，正如GPT-4系统卡所述，OpenAI投入大量精力减少模型固有偏见 (GPT-4 Technical Report)。深度研究场景下，一个体现是：模型在汇报多个来源的信息时，会注意多元性和平衡性。例如在政策话题上，代理会尽量呈现不同观点来源，而非单方面引用，从而避免偏颇。对于模型可能产生的不当表述，OpenAI的RLHF和价值观对齐使其默认采用中性、客观的语气叙事 (GPT-4 Technical Report)。Google也有AI原则明确禁止模型输出歧视性内容。Gemini模型经过了Google迄今最全面的安全评估，在出厂时已过滤大量有害数据 (Introducing Gemini: Google’s most capable AI model yet)。因此，代理很少主动生成冒犯或偏激的语言。错误信息的伦理处理也是焦点：代理或许能访问到阴谋论网站、虚假新闻。如果模型不加选择地引用这些，将助长错误信息传播。应对上，除了之前提到的可信度判断，模型在报告措辞中也会保留谨慎。如果某重要数据只有存疑来源支持，模型可能会标注“有些来源声称X，但可靠性未知”，而不会把它当确定事实写出。这种不确定性的表达，实际上也是在承担研究伦理责任——让读者知晓信息可能不可靠。最后，深度研究代理和现有基础设施集成时，需要考虑安全接口：OpenAI在提供Deep Research功能给用户时，会设置调用频率和数据大小限制，以防被用于爬取敏感站点或进行钓鱼攻击。Google也只允许其代理检索公开网页而非深网私人信息。这些限制确保代理行为在既定法律和伦理框架内运作。\n与现有AI和数据基础设施的集成：深度研究代理要发挥最大价值，必须融入用户现有的工作流程和数据生态。这带来了集成方面的挑战和解决方案。对于Google而言，Gemini Deep Research已经与Google Docs、Sheets无缝衔接：用户可以一键将代理生成的报告保存为文档，或将数据表插入Sheets继续加工 (OpenAI Deep Research vs Gemini Deep Research)。这充分利用了用户熟悉的工具，降低采用门槛。OpenAI则在探索通过API接口让Deep Research接入企业内部知识库或应用。例如，一个公司可以将自有数据库（如销售数据、专利库）接入OpenAI插件系统，供Deep Research在调研时一并检索。这涉及将向量数据库技术部署在企业环境，并授权给OpenAI模型访问。由于有函数调用能力，模型可以通过安全网关查询企业API，在报告中结合内部数据和公开数据。这样的集成实现了专有信息 + 公开信息的融合研究，对企业决策非常有价值。当然，这也要求严格的身份验证和权限控制，防止公司敏感数据泄露给OpenAI或外部。OpenAI团队版或插件模式可能通过加密传输和隔离执行来保障这一点。\n此外，深度研究可与现有的数据分析管线集成。例如将Deep Research作为BI工具的上游：先由代理做定性研究和数据收集，然后将数据传给BI系统做深入建模。反过来，如果企业已有数据仓库，代理也可直接查询仓库获取最新数据点。技术上，这要求代理支持SQL查询或通过自然语言接口访问数据库。OpenAI近来的功能扩展（如数据库插件）表明这是可行的。Google的优势则是其搜索和知识图谱本身就是巨大的数据基础设施，Gemini代理天然集成了这些。当用户问到某公司财报数据，Gemini可能直接从其知识图谱提取结构化数据，而无需额外爬取。这种与知识图谱的结合也是一项集成，提升了效率和准确性。\n最后，在MLOps层面，深度研究代理需要与模型监控、日志系统集成。每次代理执行都产生大量过程数据，如何存储有用的日志供将来审计或改进？解决方案可能是将过程日志矢量化存储，日后训练新模型时用作示例（反馈学习）。也可以与现有A/B测试平台集成，通过比较代理结果与人工结果、或OpenAI vs Google结果，持续评估改进代理算法。总体而言，深度研究代理的集成性还在早期，但趋势是开放接口。OpenAI已表示计划让Deep Research面向Plus用户甚至开放API (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。届时，各行业就能把这一强大工具嵌入自己的数据和应用中，形成“AI研究助手无处不在”的新生态。\n6. 未来技术发展 随着深度研究技术的发展，我们可以预见它在未来的增强方向和潜在研究热点：\n即将发布的功能和增强：首先，在模型层面，OpenAI可能会推出更强大的推理模型（代号也许是O4或直接称为GPT-5），具备更长上下文、更高推理深度和更快速度。这将直接提升Deep Research的性能。例如，O3模型据传已支持数十万字节级的上下文，那么未来模型可能上下文扩展到百万字级，使代理一次能处理整本书或全站点数据，而无需分段 (Google Gemini Explained: How It Works \u0026amp; Where It Beats Every \u0026hellip;) ()。Google方面，Gemini 2.0在2024年末已上线，其升级版本（Gemini 3.0？）可能引入即时多模态输出，比如代理在报告中直接生成图表插图甚至配套音频讲解 (Google introduces Gemini 2.0: A new AI model for the agentic era)。Gemini 2.0已经具备原生图像和音频输出能力 (Google introduces Gemini 2.0: A new AI model for the agentic era)，因此深度研究的结果形式会更加丰富。\n其次，交互方面，OpenAI Deep Research 可能加入对话式引导功能，让用户在过程中可以像对待真人研究员一样，不断提问和细化要求。目前Deep Research是一次性生成报告，未来也许允许报告生成后用户继续追问细节、要求补充，这相当于报告模式和Chat模式的融合。Google可能也会让Gemini代理过程更加交互：比如执行过程中弹出选项让用户决定下一步侧重哪个方向，以使结果更符合期望。两者都会朝人类在环的方向改进交互，使AI助手更可控。\n又一值得期待的功能是实时数据接入。目前代理主要检索静态网页，而未来可能连接实时数据库和API，例如财经数据API、新闻流等。这会让研究涵盖最新的实时信息。OpenAI或Google或将提供插件，让代理直接调用诸如Twitter检索、证券行情接口，生成的报告可以有最新截至分钟的数据（当然可靠性也需审查）。Gemini 2.0已经支持实时音视频输入 (Gemini - Google DeepMind)，可以想象将来代理甚至能观看一场直播发布会并进行分析总结，真正达到实时深度研究。\n性能上，针对OpenAI目前Deep Research偏慢的问题，可能引入模型压缩和并行推理优化，让一次研究在几分钟内完成而不牺牲太多深度。OpenAI也许会开发mini版Deep Research（类似轻量模式），处理较简单的问题时快速给出结果，而复杂问题自动切换到深度模式。Google则可能提升深度：让Gemini代理在需要时也能做多轮迭代而不是严格按初始计划进行，从而缩小和OpenAI在灵活性上的差距。\n潜在研究方向和技术创新：未来的深度研究领域有许多开放研究课题。例如，如何让AI代理具备因果推理能力，而不仅是相关信息罗列？目前代理主要综合资料，缺乏主动提出假说、设计实验的科学推理能力。未来研究可能将LLM与因果推理模型结合，代理可根据数据推断因果关系，并建议进一步验证的方案。\n另一方向是增强长程规划。虽然OpenAI代理已能多步迭代，但当任务跨度非常大（例如撰写一本综述性书籍）时，如何保持全局一致性是难题。有学者提出“树状思维 (Tree-of-Thoughts)”算法，让模型搜索多种可能思路路径再择优 (Synergizing Reasoning and Acting in Language Models | by Ronny H)。这种搜索式推理可以应用于代理决策，使其在开始研究前对各种方案“预演”，选最优策略，从而提高质量和效率。结合进化算法或MCTS等AI规划技术，代理在复杂课题上会更加得心应手。\n多代理协作也是值得探索的方向。未来或许不止一个AI在工作，而是一组专家代理协同：一个负责数据爬取清洗，一个负责模型分析推理，一个负责结果审校优化。这样的架构可以平行开发不同专长模型，各司其职又互相检查。初步的迹象比如AutoGPT已经尝试让多个子代理合作完成任务。OpenAI也可能研究让Deep Research与Operator等其他代理互动，比如Operator代理帮助Deep Research自动打开专业应用、下载论文等，让研究范围扩展。\n在人机协作方面，一个创新方向是记忆注入的个性化研究。代理可维护一个长期知识档案，记录用户以往关注的领域和立场，从而定制研究侧重面。例如一位医学研究者用代理多次查资料，代理可积累一个医学知识向量库，下一次在通用检索外先检索自己的记忆，以输出更有针对性的报告。这样每个用户都会培养出自己独特风格和强项的AI研究助理。技术上涉及长期记忆模块和持续学习，需解决遗忘和模型更新兼容性问题。\n另一个前沿方向是让AI代理不仅汇总知识，还能创造新知识。目前代理停留在综合已有信息层次。未来研究AGI的学者设想，AI代理能够通过模拟试验、自动推导证明等，真正提出前所未有的见解，即从“知识的搬运工”升级为“知识的生产者”。这或许需要引入强化学习进行假设检验，或AutoML生成新模型进行数据分析，从而获得新发现。这是离真正科研助理甚至独立科学家更近的一步，现在还处于科幻想象，但随着代理智能增强，可能逐步看到雏形。\n**对人工通用智能 (AGI) 的潜在影响：**深度研究代理被一些OpenAI员工誉为“个人AGI时刻” (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。的确，当一个AI系统能在几分钟内完成综合调研、分析推理并写出详尽报告，这已经覆盖了相当广泛的人类智力活动（阅读、归纳、写作、判断）。Sam Altman 曾大胆预测Deep Research这样的工具可胜任“全球经济中单位数百分比的有价值工作” (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。这是一个惊人的比例，意味它开始取代相当一部分知识工作，这正是AGI预兆之一。深度研究代理体现出AGI的几个要素：自主性（无需细粒度指令能自己推进任务）、多能性（涉猎各领域知识并运用）、长时推理（能进行数十步推理链）、自我改进（通过反馈可以变得更好）。虽然当前系统离“全面胜任任何人类任务”还有距离，但已在专业调研领域达到甚至超过一般人水平 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。\n深度研究技术对AGI的影响主要有两方面：一是能力层面的突破。如果一个代理可以无监督地阅读互联网并整理知识，那让它去学习新技能（比如新的编程语言、设计原理）也是可行的。通过不断读取和实验，它可能自行掌握越来越多任务，逐步逼近通用智能。这和人类通过阅读学习相似。二是对人类角色的改变。AGI并非孤立存在，而是与人协作。深度研究代理已经在扮演人类“认知外包”的角色，让人类专注于决策和创造。随着这些代理能力提高，人类可能将更多基础认知工作交给AI，自己扮演监督者和高层决策者。这种分工的成熟也许是AGI真正落地的标志之一。\n不过，实现AGI还需要克服更多挑战，如常识推理、抽象概念理解等。有分析指出当前代理主要在收集和总结，缺乏自主规划长期目标的能力。不过Google和OpenAI正通过各自的 agent 项目探索这最后一段路。例如Google在Gemini 2.0中就明确提出面向“agentic era”，并开发了Project Astra等原型，模拟通用AI助手完成复杂任务 (Gemini - Google DeepMind) (Gemini - Google DeepMind)。OpenAI的Operator和Deep Research也是在现实环境中测试代理实用性。可以预见，两家会逐步扩展代理技能并融合，最终产出真正的AGI代理。Deep Research作为早期形态，已经展示了AGI部分特征，也提醒我们AGI的发展需要关注可信、安全、伦理，确保在它变得更强大之前，我们已经建立起充分的控制和引导机制。总之，深度研究技术的进步既是AGI征途中的里程碑，也将在今后继续推动AGI的实现并塑造我们与知识、与AI共存的方式。\n7. 结论 深度研究技术将大型语言模型的能力拓展到复杂、多步骤的研究任务中，融合了搜索引擎、数据分析和专家写作的功能，代表了AI从“对话助手”向“自主研究员”的跨越。本报告深入分析了OpenAI Deep Research和Google Gemini Deep Research的技术细节与差异，得到以下关键见解：\n**架构与机制方面：**深度研究代理基于强大的Transformer语言模型（如OpenAI O3、Google Gemini），通过引入推理+行动交替的机制，实现了自动化的资料检索与分析综合 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models) ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models)。它们能够处理文本、图像等多模态信息，利用语义嵌入和向量数据库高效检索知识，并通过多轮推理构建对课题的全面理解 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Deep Chat JS, semantic search, and OpenAI integration)。OpenAI代理采用高度动态的迭代过程，深度挖掘且透明可控；Google代理采用结构化预规划，快速产出结构良好的报告 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。\n**使用技术方面：**这些代理集成了NLP最新成果与传统数据处理手段。它们借助网络爬虫技术获取海量资料，并用大型模型进行阅读理解和信息抽取。通过工具调用接口，代理可以执行代码计算、调用API获取数据，从而不仅能读懂数据还能加工利用数据 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。向量检索和RAG技术保障了对大规模知识的利用和更新 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks)。同时，系统在性能上通过并行、分层处理实现可扩展，在可靠性上通过交叉验证和来源约束减少幻觉。这些工程与算法上的优化使深度研究成为可能。\n**比较与应用方面：**OpenAI Deep Research在推理深度、多模态支持和交互透明度上更突出，而Google Gemini Deep Research在速度、价格和文档集成上更具优势 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia)。两者各有适用场景：前者适宜需要深入洞察和过程追踪的专业研究，后者适宜快速信息汇总和决策支持。实际案例显示，它们均能在分钟级时间内完成过去需要数小时的人力研究任务，涵盖数据收集、分析到建议，体现了AI在知识工作上的巨大潜力 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。\n**挑战与未来方面：**深度研究代理仍需持续改进数据可信度把控，避免偏见和错误信息，这要求更精细的内容过滤和事实核查机制 (GPT-4 Technical Report) (GPT-4 Technical Report)。在可扩展性上，未来模型和算法进步会缓解长文档和多任务处理的瓶颈。我们可能见证更强的因果推理、更长程的自主规划以及多代理协作，使研究代理更接近人类专家水平甚至超越人类在某些领域的发现能力。深度研究技术作为当今AGI探索的一部分，其演进将直接推动通用人工智能的实现，并对知识工作形态产生深远影响 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central)。\n综上所述，深度研究代理展现了AI在高层次认知任务上的突破，为科研、商业分析、政策制定等领域提供了强有力的工具。它减轻了人类在信息爆炸时代的负担，让专业知识触手可及并易于综合。随着技术成熟，我们应积极研究如何安全地将其融入日常工作流程，并思考人类如何与这样强大的智能体协同共处。进一步的技术研究可以围绕提升推理可靠性、引入自主创新能力以及完善人机交互界面展开。可以预见，未来的人工智能研究助理将更加智能、高效且可信，其影响将不仅是提高工作效率，更可能改变人类获取和创造知识的范式，为迈向真正的人工通用智能奠定基础。\n8. 参考文献 Yao et al., 2022. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models) ([2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models) – 提出让大型语言模型在推理过程中执行动作的ReAct框架，验证了将链式思维与工具使用相结合可提升复杂任务性能和可解释性。\nNakano et al., 2021. WebGPT: Browser-assisted Question-Answering with Human Feedback. arXiv:2112.09332 ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback) ([2112.09332] WebGPT: Browser-assisted question-answering with human feedback) – 将GPT-3与网页浏览环境结合，通过人类反馈微调，实现了带引用的开放域问答，答案质量超过人类平均水平。\nLewis et al., 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS 2020 ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks) ([2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks) – 提出RAG模型，将预训练语言模型与向量化知识库结合，通过检索支撑生成，有效提高知识密集任务的准确性和可更新性。\nOpenAI, 2025. Introducing deep research. OpenAI 官方博客，Feb 2025 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central) – 宣布推出Deep Research代理：“利用即将发布的O3模型，通过推理来搜索、解读和分析海量在线文本、图像和PDF，自动完成多步研究任务”，专为ChatGPT Pro用户提供服务。\nOkemwa, K., 2025. “Deep Research has been a personal AGI moment for me”: OpenAI’s new AI agentic tool simulates a personal research analyst. Windows Central 新闻报道, Feb 4 2025 (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central) (OpenAI\u0026rsquo;s deep research tool emulates a research analyst | Windows Central) – 报道OpenAI Deep Research发布，引用OpenAI和Sam Altman的话强调其效率：“10分钟完成数小时人类工作”，并称员工体验“个人AGI时刻”。\nPankratyeva, A., 2025. Google Gemini vs. OpenAI Deep Research: Which Is Better?. Techopedia 技术评测文章, Feb 5 2025 (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) (Google Gemini vs. OpenAI Deep Research: Which Is Better? - Techopedia) – 从功能、强项、限制等方面详细比较了OpenAI Deep Research和Google Gemini Deep Research，指出OpenAI更深入细致但昂贵，Google更快捷平价但缺少深度推理。\nAnalytics Vidhya, 2025. OpenAI Deep Research vs Gemini Deep Research: Who does deep research better? Analytics Vidhya 博客文章, Feb 2025 (OpenAI Deep Research vs Gemini Deep Research) (OpenAI Deep Research vs Gemini Deep Research) – 作者实测了OpenAI和Google深度研究代理对同一课题的表现，对比两者流程、用时、输出结构，总结OpenAI注重实时迭代和透明，Google注重结构化和速度。\nHassabis, D., 2023. (引自 Wired 采访，见维基百科) – DeepMind CEO Demis Hassabis 谈Google Gemini：“将结合AlphaGo的威力和语言模型的能力”，预期Gemini能在AI能力上超越OpenAI的GPT-4 (Gemini (language model) - Wikipedia)。\nGemini Team (Google), 2023. Gemini: A Family of Highly Capable Multimodal Models. 技术报告, Dec 6 2023 () () – 系统介绍了Google DeepMind的Gemini多模态模型系列，展示Gemini Ultra在众多语言、图像、视频任务上达到SOTA性能，并强调其跨模态推理和agent式能力。\nOpenAI, 2023. GPT-4 Technical Report \u0026amp; System Card. OpenAI (arXiv:2303.08774) (GPT-4 Technical Report) (GPT-4 Technical Report) – 描述了GPT-4模型的能力和局限，特别是安全挑战，包括偏见、虚假信息、隐私等，并列出OpenAI为缓解这些问题所采取的措施（如RLHF、安全测试等），为深度研究代理的安全设计提供了指导原则。\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui \u0026amp; OpenAI Deep Research. (Feb 2025). OpenAI Deep Research 案例分享 - 深度研究技术综合研究报告\u0026quot;. https://syhya.github.io/zh/posts/2025-02-15-deep-research-tech-report/\nOr\n@article{syhya2025deepresearch, title = \u0026#34;OpenAI Deep Research 案例分享 - 深度研究技术综合研究报告\u0026#34;\u0026#34;, author = \u0026#34;Yue Shui \u0026amp; OpenAI Deep Research\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/zh/posts/2025-02-15-deep-research-tech-report\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-02-15-deep-research-tech-report/","summary":"\u003cp\u003eOpenAI Deep Research (\u003ca href=\"https://openai.com/index/introducing-deep-research/\"\u003eOpenAI, 2025\u003c/a\u003e) 主要是采用以 o3 推理模型为基座的多 agent 协作方式。相关开源研究和产品之前也有不少，比如 \u003ca href=\"https://github.com/assafelovic/gpt-researcher\"\u003eGPT Researcher\u003c/a\u003e 和 Google Gemini 版本的 Deep Research (\u003ca href=\"https://openai.com/index/introducing-deep-research/\"\u003eDeepMind, 2024\u003c/a\u003e)。不过 OpenAI 的产品是博主目前体验下来效果最好。之后咨询分析、论文综述和新闻编辑等任务都可以先让这个工具自动化完成，然后再由人工进行最后的修正和审查，怎么感觉之后我写技术博客的任务也会被替代了呢？😂\u003c/p\u003e","title":"OpenAI Deep Research 案例分享 - 深度研究技术综合研究报告"},{"content":"这篇博客主要介绍一种比 RLHF 更精简的替代算法 DPO。与 RLHF 一样，DPO 目的是使模型输出与人类偏好保持一致，但它在实现上更加简单，并且对资源的需求更低。在项目资源受限的情况下，DPO 是一个实用解决方案。\n符号 符号 含义 \\( x \\) 用户输入（Prompt），即模型需要回答的问题 \\( y \\) 模型生成的回答（Response / Completion），即模型输出的文本 \\( \\pi_\\theta(y \\mid x) \\) Actor 模型：待训练策略，用于生成回答 \\(y\\)；参数为 \\(\\theta\\) \\( \\pi_{\\mathrm{ref}}(y \\mid x) \\) 参考模型：冻结的 SFT 模型，作为对齐基准 \\( r_\\phi(x,y) \\) 奖励模型：用于评估回答 \\(y\\) 质量的奖励函数；参数为 \\(\\phi\\) \\( V_\\psi(x) \\) critic 模型：用于估计给定输入 \\(x\\) 下未来累计奖励的值函数；参数为 \\(\\psi\\) \\( \\pi^*(y \\mid x) \\) 最优策略分布，通过参考模型与奖励函数确定 \\( r_\\theta(x,y) \\) 基于 Actor 模型导出的奖励函数，通过 \\(\\pi_\\theta\\) 与 \\(\\pi_{\\mathrm{ref}}\\) 构造 \\(\\beta\\) 超参数，控制 KL 惩罚项或对数比差异项的权重 \\(\\mathbb{D}_{\\mathrm{KL}}[P \\| Q]\\) KL 散度，衡量概率分布 \\(P\\) 与 \\(Q\\) 之间的差异 \\(\\sigma(z)\\) Sigmoid 函数，定义为：\\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\) \\(\\log\\) 对数函数 \\(\\mathbb{E}\\) 期望算子，用于求随机变量的平均值 \\( (y_w, y_l) \\) 一对偏好数据，其中 \\( y_w \\) 表示被偏好（质量更好）的回答，\\( y_l \\) 表示质量较差的回答 \\( P\\left(y_w \\succ y_l \\mid x\\right) \\) 在输入 \\(x\\) 下，回答 \\( y_w \\) 优于 \\( y_l \\) 的概率 \\( Z(x) \\) 配分函数，对所有回答 \\(y\\) 归一化概率分布 \\( \\mathcal{L}_{\\mathrm{DPO}} \\) DPO 的损失函数 从 RLHF 到 DPO RLHF OpenAI 主要利用人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)(Christiano et al., 2017) 来训练 InstructGPT(Ouyang et al., 2022)，而其构成了大语言模型（如 ChatGPT, Llama 等）的基础。整个训练过程通常包括以下三个主要步骤：\nFig. 1. A diagram illustrating the three steps of InstructGPT. (Image source: Ouyang et al., 2022)\n监督微调（SFT）\n利用大量人工示例数据对预训练模型进行微调，得到一个初步能理解指令并生成合理回答的模型，即参考模型 \\( \\pi_{\\mathrm{ref}}(y \\mid x) \\) 。\n奖励模型训练\n这里我们简化只考虑生成两个不同的结果，实际可以生成多个结果进行排序。针对同一输入 \\(x\\) 生成两个回答 \\(y_w\\)（较优）和 \\(y_l\\)（较劣），由人工排序后收集偏好数据。基于这些数据训练奖励模型 \\(r_\\phi(x, y)\\)，使其能预测哪种回答更符合人类偏好。\n基于 PPO 的强化学习\n利用奖励模型 \\(r_\\phi\\) 提供的反馈，通过 PPO 算法优化 Actor 模型 \\(\\pi_\\theta\\) 以提升回答质量。为防止模型偏离 \\(\\pi_{\\mathrm{ref}}\\)，在优化过程中引入 KL 惩罚项。该阶段通常涉及以下 4 个模型：\n\\(\\pi_\\theta\\)：经过 SFT 后待更新的模型。 \\(\\pi_{\\mathrm{ref}}\\)：冻结的 SFT 模型，作为对齐基准。 \\(r_\\phi\\)：用于评估回答质量，参数固定。 \\(V_\\psi\\)：用于估计未来奖励，辅助 Actor 模型更新。 RLHF 的局限性 尽管 RLHF 能充分利用人类偏好信息提升模型对齐效果，但其固有局限性包括：\n多模型训练：除 Actor 模型 \\(\\pi_\\theta\\) 外，还需额外训练奖励模型 \\(r_\\phi\\) 和 Critic 模型 \\(V_\\psi\\)，整体训练过程复杂且资源消耗大。 高采样成本：LLM 生成文本计算量大，强化学习过程中的大量在线采样进一步推高了计算开销；采样不足可能导致错误的优化方向。 训练不稳定与超参数敏感：PPO 涉及众多超参数（如学习率、采样量等），调参复杂且训练过程易受不稳定因素影响。 对齐税效应：在提高模型对齐性的同时，可能会降低模型在其他任务上的表现。 DPO 简介 Fig. 2. DPO optimizes for human preferences while avoiding reinforcement learning.（Image source: Rafailov et al., 2023）\n直接偏好优化(Direct Preference Optimization, DPO) (Rafailov et al., 2023)为了解决 RLHF 的上述问题，其核心思路是将 RLHF 的目标转化为类似于监督微调的对比学习任务，从而实现：\n省略奖励模型训练：直接利用人类偏好数据优化 Actor 模型 \\(\\pi_\\theta\\)，无须单独训练 \\(r_\\phi\\)。 消除强化学习采样：采用对比损失函数替代 PPO，降低采样和计算资源消耗。 提升训练稳定性：基于监督学习的方法对超参数不敏感，训练过程更加平稳。 虽然 DPO 在 LLM 性能提升的上限上可能不及 RLHF，但在资源利用、实现复杂度和训练稳定性方面具有优势。\n方法对比 方法 训练步骤 模型 训练方式 优点 缺点 RLHF 先训练奖励模型，再使用 PPO 优化策略 \\(\\pi_\\theta\\)、\\(\\pi_{\\mathrm{ref}}\\)、\\(r_\\phi\\)、\\(V_\\psi\\) 强化学习和在线采样 充分利用人类偏好，上限潜力较高 资源消耗大、训练不稳定、超参数敏感 DPO 直接利用偏好数据训练 Actor 模型 \\(\\pi_\\theta\\)、\\(\\pi_{\\mathrm{ref}}\\) 类似 SFT 监督学习 流程简化、训练稳定、资源消耗低 性能提升上限可能低于 RLHF DPO 数学推导 RLHF 目标与最优策略分布 在大规模语言模型对齐中，我们希望利用人类反馈强化学习（RLHF）来优化模型输出。设输入 \\( x \\) 来自数据集 \\(\\mathcal{D}\\)，模型生成回答 \\( y \\)；待训练的 模型记为 \\(\\pi_\\theta(y \\mid x)\\)，而参考模型记为 \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\)（通常为SFT模型），同时引入奖励函数 \\( r(x,y) \\) 衡量回答质量。RLHF 的目标可写为\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\Big[ r(x,y) \\Big] \\;-\\; \\beta\\, \\mathbb{D}_{\\mathrm{KL}}\\Big[ \\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x) \\Big], \\tag{1} \\]其中 \\(\\beta\\) 为调节奖励与参考模型偏差的超参数。利用 KL 散度的定义\n\\[ \\mathbb{D}_{\\mathrm{KL}} \\Big[\\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\Big] = \\mathbb{E}_{y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right], \\tag{2} \\]式 (1) 可重写为\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ r(x,y) - \\beta \\, \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right]. \\tag{3} \\]将 (3) 式转换为最小化问题并除以 \\(\\beta\\) 得\n\\[ \\min_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} - \\frac{1}{\\beta} r(x,y) \\right]. \\tag{4} \\]假设存在一个最优策略分布 \\(\\pi^*(y \\mid x)\\) 使 (4) 式全局最优，则令\n\\[ \\pi^*(y \\mid x) \\;=\\; \\frac{1}{Z(x)} \\,\\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big), \\tag{5} \\]其中配分函数 \\( Z(x) \\) 定义为\n\\[ Z(x) = \\sum_{y}\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big). \\tag{6} \\] \\(Z(x)\\) 对所有可能的 \\(y\\) 求和，实现归一化，使得 \\(\\pi^*(y \\mid x)\\) 构成合法概率分布。 \\(Z(x)\\) 是 \\(x\\) 的函数，与待优化的 Actor 模型 \\(\\pi_\\theta\\) 无关。 对 (5) 式取对数得到\n\\[ \\log \\pi^*(y \\mid x) = \\log \\pi_{\\mathrm{ref}}(y \\mid x) + \\frac{1}{\\beta}\\, r(x,y) - \\log Z(x), \\tag{7} \\]从而解得\n\\[ r(x,y) = \\beta \\left[\\log \\frac{\\pi^*(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{8} \\]Bradley–Terry 模型 为了利用成对偏好数据 \\((x, y_w, y_l)\\) 训练模型，我们希望在相同输入 \\( x \\) 下，模型输出更偏好于高质量回答 \\( y_w \\) 而不是低质量回答 \\( y_l \\)。\nBradley–Terry 模型用于预测成对比较的结果。对于任意两个对象 \\( i \\) 和 \\( j \\)，若为每个对象分配正实数得分 \\( p_i \\) 和 \\( p_j \\)，则对象 \\( i \\) 被认为比对象 \\( j \\) 强的概率为\n\\[ \\Pr(i \u003e j) = \\frac{p_i}{p_i + p_j}. \\tag{9} \\]在我们的场景中，将每个回答 \\( y \\) 的强度参数设为 \\( p_{y} = \\exp\\big(r(x,y)\\big) \\)（保证为正实数）。因此，给定输入 \\( x \\) 下，回答 \\( y_w \\) 好于 \\( y_l \\) 的概率为\n\\[ P\\left(y_w \\succ y_l \\mid x\\right)=\\frac{\\exp \\big[r(x,y_w)\\big]}{\\exp \\big[r(x,y_w)\\big]+\\exp \\big[r(x,y_l)\\big]}. \\tag{10} \\]为了使得数据集中每个成对偏好数据 \\((x, y_w, y_l)\\) 中，高质量回答 \\( y_w \\) 的胜出概率尽可能大，我们将奖励模型训练目标设计为最大化 \\( y_w \\) 被偏好的概率，或等价地最小化负对数似然损失：\n\\[ L_{R}\\left(r_{\\phi}, D\\right) = -\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\left[\\log P\\left(y_w \\succ y_l \\mid x\\right)\\right], \\tag{11} \\]\\[ D=\\{(x^i, y_w^i, y_l^i)\\}_{i=1}^{N}. \\tag{12} \\]利用公式 (10)、(11) 以及下面的恒等式\n\\[ \\log \\frac{e^a}{e^a+e^b} = \\log\\frac{1}{1+e^{b-a}} = \\log \\sigma(a-b), \\tag{13} \\]其中 Sigmoid 函数定义为\n\\[ \\sigma(z)=\\frac{1}{1+e^{-z}}, \\tag{14} \\]可得\n\\[ \\log P\\left(y_w \\succ y_l \\mid x\\right) = \\log \\sigma\\Big(r(x,y_w)-r(x,y_l)\\Big). \\tag{15} \\]直接偏好优化 注意到 (8) 式中，奖励 \\( r(x,y) \\) 与最优策略的对数比有关。为避免显式训练一个单独的奖励模型 \\(r_\\phi\\)，我们采用 DPO的思想，即直接用待训练 Actor 模型 \\(\\pi_\\theta\\) 替换最优策略 \\(\\pi^*\\) 的位置，将 (8) 式中的奖励表示为\n\\[ r_\\theta(x,y) \\;=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{16} \\]在成对比较中，对于相同输入 \\( x \\)，两个回答 \\( y_w \\) 和 \\( y_l \\) 均包含相同的 \\(\\log Z(x)\\) 项，因此在计算奖励差值时，该项会被消去，即\n\\[ \\begin{aligned} r_\\theta(x,y_w)-r_\\theta(x,y_l) \u0026=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} + \\log Z(x)\\right] - \\beta \\left[\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} + \\log Z(x)\\right] \\\\ \u0026=\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)}. \\end{aligned} \\tag{17} \\]将 (17) 式代入 (15) 式，并结合 (11) 式，我们最终得到 DPO 的损失函数\n\\[ \\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}}) = - \\mathbb{E}_{(x,y_w,y_l) \\sim D} \\left[ \\log \\sigma\\Big( \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} -\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big) \\right]. \\tag{18} \\]该损失函数针对待训练 Actor 模型 \\(\\pi_\\theta\\) 而设，通过比较其在高质量回答 \\(y_w\\) 与低质量回答 \\(y_l\\) 上、相对于参考模型 \\(\\pi_{\\mathrm{ref}}\\) 的对数概率比来区分好坏回答：当 \\(\\pi_\\theta\\) 在 \\(y_w\\) 上的相对概率比远大于 \\(y_l\\) 时，Sigmoid 输出更接近 1，损失更小；反之，若 \\(\\pi_\\theta\\) 对 \\(y_l\\) 的相对概率过大，则损失增加。参数 \\(\\beta\\) 用于放大或缩小这些对数比的差值，从而调节模型对好坏答案的区分强度。\n训练流程 数据准备 偏好数据采集\n收集大量偏好数据，每个样本包含输入 \\( x \\) 及一对回答 \\((y_w, y_l)\\)，其中 \\( y_w \\) 为被偏好回答(chosen)，\\( y_l \\) 为质量较差的回答(rejected)。\n数据集例子: dpo_en_demo.json\n{ \u0026#34;conversations\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;human\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;what does abstraction mean on oop\u0026#34; } ], \u0026#34;chosen\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Abstraction in Object-Oriented Programming (OOP) is the process of exposing only the necessary information to the outside world while hiding the implementation details. It\u0026#39;s a way to simplify complex systems by focusing only on the essential aspects and encapsulating the implementation details.\\n\\nIn other words, abstraction is about hiding the internal workings of an object or system and only exposing the necessary interfaces or APIs to interact with it. This helps to reduce complexity, improve modularity, and make the system easier to understand and maintain.\\n\\nFor example, consider a car. A car has many components like engine, transmission, brakes, etc. However, when you drive a car, you don\u0026#39;t need to know how the engine works or how the transmission changes gears. All you need to know is how to operate the car\u0026#39;s controls like accelerator, brakes, and steering wheel. The car\u0026#39;s controls are an abstraction of the underlying complex systems, allowing you to interact with the car without needing to know the details of how it works.\\n\\nIn programming, abstraction is achieved by defining interfaces or abstract classes that define the essential functions or behaviors of an object without specifying how they are implemented. This allows developers to create different implementations of the same interface or abstract class, which can be swapped out without affecting the rest of the system.\\n\\nAbstraction is a fundamental concept in OOP and is used to create modular, maintainable, and flexible software systems. It helps to reduce code duplication, improve code reuse, and make software systems more understandable and easier to evolve over time.\u0026#34; }, \u0026#34;rejected\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Abstraction is a principle in object-oriented programming that refers to the process of focusing on the essential features of an object or concept without emphasizing the details that are not important for its functionality. It enables programmers to create classes that represent the objects they want to work with, and specifies only the behavior of those objects, leaving out unnecessary details. Abstraction helps to make the code more maintainable, modular, and scalable. It also improves the productivity of developers by reducing the amount of code they need to write.\u0026#34; } } 输出预计算\n为降低计算开销，预先计算并缓存参考模型 \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\) 的输出。\n模型训练 训练目标\n通过最小化 DPO 损失 \\(\\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}})\\) 直接优化 Actor 模型 \\(\\pi_\\theta\\)，使其生成的回答更符合人类偏好。\n训练步骤\n从数据集中采样一批 \\((x, y_w, y_l)\\) 数据。\n计算 Actor 模型 \\(\\pi_\\theta(y \\mid x)\\) 的输出概率。\n利用下式计算损失：\n\\[ \\mathcal{L}_{\\mathrm{DPO}} = - \\log \\sigma\\Big( \\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big). \\] 通过反向传播更新 Actor 模型参数 \\(\\theta\\)。\n模型推理 训练完成后，得到的 Actor 模型 \\(\\pi_\\theta\\) 可直接用于推理。给定输入 \\( x \\) 后，模型基于学到的概率分布生成回答。由于训练过程中参考了人类偏好，同时受到参考模型 \\(\\pi_{\\mathrm{ref}}\\) 的约束，生成的回答既符合预期，又能保持生成文本的稳定性。\n总结 DPO 将 RLHF 过程简化为直接的监督学习任务，不仅节省了资源、提高了训练稳定性，同时降低了实现复杂度，是 LLM 对齐训练的一种高效替代方法。在实际应用中，我们可以根据业务场景选择 RLHF 或 DPO 方法，以达到最佳的训练效果。\n参考文献 [1] Christiano, Paul F., et al. \u0026ldquo;Deep reinforcement learning from human preferences.\u0026rdquo; Advances in neural information processing systems 30 (2017).\n[2] Ouyang, Long, et al. \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in neural information processing systems 35 (2022): 27730-27744.\n[3] Rafailov, Rafael, et al. \u0026ldquo;Direct preference optimization: Your language model is secretly a reward model.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Feb 2025). 大语言模型对齐: 直接偏好优化(DPO). https://syhya.github.io/posts/2025-02-08-dpo\nOr\n@article{syhya2025dpo, title = \u0026#34;大语言模型对齐: 直接偏好优化(DPO)\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-02-08-dpo\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-02-08-dpo/","summary":"\u003cp\u003e这篇博客主要介绍一种比 RLHF 更精简的替代算法 DPO。与 RLHF 一样，DPO 目的是使模型输出与人类偏好保持一致，但它在实现上更加简单，并且对资源的需求更低。在项目资源受限的情况下，DPO 是一个实用解决方案。\u003c/p\u003e","title":"大语言模型对齐: 直接偏好优化(DPO)"},{"content":" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。\n检索增强生成（Retrieval-Augmented Generation, RAG） 是一种结合大型语言模型（LLM）与外部知识库的先进技术框架。RAG 在回答问题时，不再仅依赖模型自身参数，而是通过实时检索外部知识库，显著降低模型的知识盲区，提升答案的准确性与可信度。\n为什么需要 RAG？ 1. 长尾知识与私有数据 通用大型语言模型在训练时未覆盖企业或个人特定领域的长尾知识与私密数据，通过 RAG 的外部知识召回机制，能够有效弥补这一短板。\n2. 降低上下文开销 尽管现代 LLM 的上下文容量不断提升（如 GPT-4o 的 128K、Gemini 的百万级上下文），但一次性输入海量信息既不经济也不高效。RAG 通过精准检索，仅向模型提供最相关的内容，有效减少 Token 的使用量，加快响应速度。\n3. 提升实时响应能力 RAG 可与实时数据库或搜索引擎对接，快速获取最新信息，特别适用于新闻问答、实时情报分析等对时效性要求较高的场景。\n4. 提高可解释性 通过明确返回检索内容的索引，RAG 增强了模型输出的透明度，使答案的来源易于追溯和验证，这对于金融、法律、医疗等对可信度要求较高的领域尤为关键。\nRAG 与模型微调的对比分析 在实际项目中，常见的将领域知识注入基座模型的方法主要包括 微调（Fine-Tuning） 与 检索增强生成（RAG）。RAG 是一种高效的搜索技术范式，适合实时更新知识和对可信性要求较高的场景；而模型微调则更适合对特定领域知识要求极致精度、且更新频率较低的应用场景。两者也可协同使用，实现精度与时效的平衡。\n下表详细对比了这两种方法的特点：\n比较维度 检索增强生成（RAG） 模型微调（Fine-Tuning） 知识更新速度 即时，随知识库更新即时生效 慢，需要重新训练模型 计算成本 推理阶段需执行检索+LLM生成，训练阶段成本低 训练阶段成本高，推理阶段成本固定 实时响应能力 强，能实时检索最新数据 弱，模型参数训练后固定，无法实时更新 可扩展性 水平扩展容易，知识库扩容简单，性能稳定 扩展规模大时成本高，训练复杂度指数增加 可解释性 高，答案来源明确可追溯 低，答案来源隐藏在模型参数内，难以验证 深度定制性 弱，依赖外部知识库，模型本身未深入掌握特定领域知识 强，模型深度掌握特定领域语义与模式 灾难性遗忘 无，不改变模型参数，避免遗忘原有知识 有，新知识学习可能导致旧知识被遗忘 推理效率 中，检索过程增加推理开销，Token 消耗较多 高，部署后无需额外检索，推理高效 知识覆盖范围 受限于知识库规模与质量，但扩展灵活 受限于训练数据覆盖面，新增知识成本高 RAG Pipeline：技术流程详解 RAG Pipeline 主要包含以下关键步骤：\n数据预处理（Data Preprocessing） 数据预处理是 RAG Pipeline 的基石，其核心任务是将来自各种数据源的内容转化为系统可高效处理和检索的格式。数据源类型繁多，包括结构化数据（如数据库表）、非结构化文本（如 PDF 文档、网页）以及富媒体数据（如带图像的报告）。\n针对非结构化或富媒体文档，预处理流程通常包含以下关键环节：\nOCR（光学字符识别）：对于图像中的文字信息，OCR 技术至关重要，它能将图片中的文字转化为可检索的文本数据。 命名实体识别（NER）：NER 技术能够从文本中提取关键实体（如人名、地名、组织机构名等）和元数据，为后续的索引和检索提供结构化信息。 例如，对于扫描的合同或表格，可以借助 Azure 认知服务等文档智能解析工具，高效提取其中的文本、表格和结构信息。通过精细的数据预处理，原始数据被规范化为适合检索的文本片段和结构化记录，为后续 RAG 流程奠定坚实基础。\n文本分块（Chunking） 文本分块是 RAG Pipeline 中至关重要的一步，它将大型文档分割成更小、更易管理和检索的文本片段（chunks）。分块的目的是在检索时能够快速定位与用户查询最相关的局部内容，从而提高检索效率和准确性。\n分块策略涉及两个关键参数：\nChunk Size（分块大小）：每个文本片段包含的 Token 数量。 Overlap（重叠度）：相邻文本片段之间重叠的 Token 数量。 Chunk Size 与 Overlap 的选择需要根据具体应用场景和数据特点精细调整，需要在检索效率与上下文完整性之间寻求最佳平衡。例如，1024 Token 的 Chunk Size 与 128 Token 的 Overlap 是一种常见设置，适用于许多通用场景。较小的 Chunk Size 可以提高检索精确度，但可能丢失上下文信息；而较大的 Chunk Size 则能保留更多上下文，但可能降低检索效率。设置重叠则有助于确保语义连贯，避免信息割裂。\n文本嵌入（Embedding） 文本嵌入模型（Text Embedding Model） 是 RAG Pipeline 的核心组件之一，其作用是将非结构化文本数据转化为结构化向量，从而将文本语义映射到高维向量空间。目前，基于深度学习的稠密向量嵌入模型是主流选择。\n常用文本嵌入模型：\nOpenAI text-embedding-ada-002：OpenAI 提供的 text-embedding-ada-002 模型是业界标杆之一，以其卓越的嵌入效果和强大性能著称。该模型支持处理高达 8191 Token 的文本，足以应对大多数应用场景。 Instructor 模型：Instructor 模型是一类经过指令微调（Instruction Tuning）的文本嵌入模型，其特点在于能根据不同任务（如分类、检索、聚类、文本评估等）和领域（如科学、金融等）生成定制化的文本嵌入向量。这使得 Instructor 模型在特定任务和领域中能实现更精准的语义表示，而无需额外微调，极大提升了模型的灵活性和应用价值。 检索（Retrieval） 相似向量检索 是 RAG Pipeline 中连接用户查询与知识库的关键环节。其核心问题在于：给定一个查询向量，如何从海量候选向量中快速、准确地检索出与其语义最相似的一个或多个向量。\n相似性度量方法：\n相似性度量定义了向量之间相似程度的计算方式，常见方法包括：\n余弦相似度（Cosine Similarity）：衡量向量方向的相似度，取值范围为 [-1, 1]，值越大表示越相似。 点积（Dot Product）：在向量归一化后与余弦相似度等价，计算效率较高。 欧氏距离（Euclidean Distance）：衡量向量在空间中的距离，值越小表示越相似。 汉明距离（Hamming Distance）：用于衡量布尔或离散向量间的差异。 在实际应用中，由于其良好的语义表征能力和计算效率，余弦相似度 通常成为向量检索的首选方法。\n相似性检索算法与实现：\n面对海量候选向量以及对检索速度和准确率的不同需求，选择合适的相似性检索算法和实现方法至关重要。需要综合考虑以下因素：\n候选向量数量级：向量库的大小直接影响检索算法的选择。 检索速度和准确率要求：不同应用场景对检索性能的要求不同。 内存限制：内存资源限制了可使用的索引结构和算法。 向量数据库（Vector Database） 向量数据库专门用于存储、索引和查询向量嵌入，是构建高效 RAG Pipeline 的关键基础设施。它能够对高维向量进行高效相似性搜索，满足 RAG 系统对快速检索相关知识的需求。\nAzure AI Search：\nAzure AI Search 提供了强大的向量搜索功能，可作为 RAG Pipeline 的向量数据库选择。\n向量数据库数据处理流程：\n下图展示了向量数据库中常见的数据处理流程：\n索引（Indexing）：为加速向量检索，向量数据库通常采用高效索引算法，例如：\n乘积量化（Product Quantization, PQ）：一种有损压缩与索引方法，通过向量量化降低存储空间并加速检索。 局部敏感哈希（Locality Sensitive Hashing, LSH）：一种近似最近邻搜索算法，通过哈希函数将相似向量映射到同一桶中。 分层可导航小世界图（Hierarchical Navigable Small World, HNSW）：一种基于图的索引算法，以其高效检索性能和良好扩展性受到欢迎。 索引过程将原始向量映射到特定的数据结构中，从而实现更快速的相似性搜索。\n查询（Query）：在查询阶段，向量数据库将输入的查询向量与索引中的向量进行比较，通过预先构建的索引结构快速找到最近邻的相似向量。\n后处理（Post-processing）：在某些应用场景下，向量数据库检索出的最近邻向量可能需要经过后处理才能得到最终结果。后处理步骤可能包括结果过滤、排序、去重等操作，以进一步提升检索结果的质量和相关性。\nHyDE（Hypothetical Document Embeddings） HyDE，即 Hypothetical Document Embeddings（假设文档嵌入），是一种创新的检索增强技术，旨在提升检索的准确性和相关性。其核心思想是：先生成假设文档，再利用假设文档进行检索。\n传统检索方法直接使用用户初始查询进行向量检索，而 HyDE 则另辟蹊径。对于给定的初始查询，HyDE 首先利用大型语言模型生成一个假设文档或回复，该文档或回复即为对用户查询潜在答案或相关信息的推测。随后，将这个假设文档作为新的查询进行检索，而不是直接使用原始查询。\nHyDE 的优势：\n弥合语义鸿沟：用户的初始查询可能表述不够清晰或含有歧义，导致直接检索效果不佳。HyDE 通过生成的假设文档，能够更全面、更准确地捕捉用户查询的潜在意图和语义信息，从而弥合查询与知识库文档之间的语义鸿沟，提高检索相关性。 增强检索鲁棒性：生成的假设文档可视为对原始查询的一种语义泛化或扩展，能够更好地应对查询的多样性和复杂性，从而提升检索系统的鲁棒性。 面向大型语言模型的检索增强生成（RAG） 在使用大型语言模型（LLM）时，经常会遇到领域知识缺口、事实准确性问题以及幻觉现象等挑战。检索增强生成（RAG）通过引入外部知识（如数据库）来缓解这些问题，特别适用于知识密集型或需持续更新知识的领域应用。\nRAG 的一大优势在于，无需为特定任务对 LLM 重新训练，而能直接利用最新信息生成可靠答案。正因如此，RAG 已在对话代理等应用中得到广泛应用。\n在此部分，我们重点总结了 Gao 等人（2023）在《Retrieval-Augmented Generation for Large Language Models: A Survey》中的主要发现和实践见解，涵盖现有方法、最前沿的 RAG 技术、评估方法、应用场景及构成 RAG 系统的各个组件（检索、生成和增强技术）。\nRAG 简介与框架 RAG 框架 可以定义为：\n输入：用户提出的问题作为输入。如果不使用 RAG，则直接由 LLM 回答。 索引：使用 RAG 时，首先将一系列相关文档通过分块、生成嵌入后，建立向量存储。在推理时，查询同样会生成相似的嵌入。 检索：通过比较查询向量与索引向量，检索出相关文档（即“相关文档”）。 生成：将检索到的相关文档与原始提示拼接为上下文，传入模型生成最终答案，作为系统的输出。 例如，直接使用模型可能由于缺乏对最新事件的了解而无法回答问题，而使用 RAG 时，系统能提取所需信息，从而使模型生成恰当回答。\n想了解更多关于 RAG 及高级提示方法的信息，请关注我们的新 AI 课程，现在加入还能享受额外 20% 折扣（使用代码 PROMPTING20）。\nRAG 范式 近年来，RAG 系统经历了从简单 RAG 到高级 RAG 以及模块化 RAG 的演进，以解决性能、成本和效率上的种种局限：\n简单 RAG（Naive RAG）\n简单 RAG 遵循传统的索引、检索和生成流程。简而言之，用户输入用于检索相关文档，然后将这些文档与提示拼接，传入模型生成最终答案。如果涉及多轮对话，历史对话记录也可融入提示中。\n然而，简单 RAG 存在一些局限，如低精度（检索的片段未能完全匹配）和低召回（未能检索到所有相关片段），甚至可能传递过时信息给模型，从而引发幻觉和错误回答。此外，多段检索时还可能面临冗余、重复以及风格不统一等问题。\n高级 RAG（Advanced RAG）\n高级 RAG 针对简单 RAG 中存在的问题进行改进，如通过优化预检索、检索及后检索过程来提升检索质量。\n在预检索阶段，通过提升数据粒度、优化索引结构、添加元数据、对齐优化及混合检索等方法，提高被索引数据的质量；\n在检索阶段，则可以通过优化嵌入模型（例如微调嵌入模型或采用动态嵌入）来改善上下文的语义表达；\n后检索阶段主要关注解决上下文窗口限制和降低噪声干扰，常见方法包括重排序和提示压缩。\n模块化 RAG（Modular RAG）\n模块化 RAG 强化了各功能模块，例如集成搜索模块进行相似性检索，以及对检索器进行微调。简单 RAG和高级 RAG均可视为模块化 RAG 的特例，具有固定模块，而扩展的 RAG 模块则包括搜索、记忆、融合、路由、预测和任务适配器，可根据任务需求灵活调整模块和流程。\n此外，还提出了其他优化技术，例如：\n混合搜索探索（Hybrid Search Exploration）：结合关键词搜索与语义搜索以检索更相关、上下文丰富的信息。 递归检索与查询引擎（Recursive Retrieval and Query Engine）：通过从小语义片段开始递归检索，再逐步扩展到更大片段，平衡检索效率与上下文丰富性。 StepBack-prompt：利用提示技术使 LLM 能够进行概念抽象，生成更有依据的回答。 子查询（Sub-Queries）：针对不同场景采用树状查询或顺序查询策略，如 LlamaIndex 提供的子问题查询引擎，可将一个查询拆分为多个子问题，分别从不同数据源检索。 检索 检索是 RAG 系统中负责从检索器中提取高度相关上下文的关键组件，主要涉及以下改进方向：\n提升语义表示：选择合适的分块策略，根据内容和任务需求调整文本块大小，并结合微调嵌入模型以确保查询与文档在语义空间中的对齐。 查询与文档对齐：通过查询重写（如 Query2Doc、ITER-RETGEN、HyDE）和嵌入转换，优化查询嵌入的表示，使其与文档语义更加匹配。 检索器与 LLM 的对齐：利用 LLM 的反馈信号微调检索器，或引入外部适配器（如 PRCA、RECOMP、PKG）以更好地将检索结果与 LLM 偏好对齐。 生成 RAG 系统中的生成器负责将检索到的信息整合并转化为连贯的文本。生成过程可能需要对检索结果进行后处理或对 LLM 进行微调，以确保输出既自然又充分利用检索到的知识。\n增强 增强过程涉及如何有效将检索到的上下文整合到生成任务中。增强可以在预训练、微调和推理各阶段进行，关键包括：\n增强阶段：如 RETRO 系统在大规模预训练中利用外部知识构建额外编码器，或在推理阶段利用迭代检索和重排序来优化上下文。 增强数据来源：可分为非结构化数据、结构化数据以及 LLM 生成的数据。 增强过程：对于多步推理等复杂任务，单次检索可能不足，因此提出了迭代检索、递归检索和自适应检索等方法。 RAG 评估 与对 LLM 的各项能力进行评估类似，对 RAG 系统的评估也至关重要。传统上，RAG 系统通过下游任务指标（如 F1 分数、准确率等）来评估，同时结合信息检索指标（如 NDCG、命中率）评估检索质量，并关注生成答案的相关性、忠实度以及潜在有害性。评估方法既可采用人工也可自动化，主要从以下维度展开：\n上下文相关性：检验检索到的上下文是否精准且具备针对性。 答案忠实度：生成的答案与检索上下文之间的一致性。 答案相关性：答案与用户问题的契合度。 此外，还需评估系统的噪声鲁棒性、负面拒绝能力、信息整合能力及反事实鲁棒性。\nRAG 的挑战与未来 尽管 RAG 系统在提升知识密集型任务表现上展现了巨大潜力，但在未来的发展中仍面临如下挑战：\n上下文长度问题：随着 LLM 上下文窗口的不断扩展，如何确保检索到最相关的信息仍然具有挑战性。 鲁棒性：如何应对反事实和对抗性信息，提升系统鲁棒性。 混合方法：如何更好地结合 RAG 与微调模型以取得最佳效果仍需进一步探索。 扩展 LLM 的角色：增强 LLM 在 RAG 系统中的作用和能力。 扩展定律：如何理解 LLM 扩展定律在 RAG 系统中的应用仍未明晰。 生产级 RAG 系统：构建生产级 RAG 系统需要在性能、效率、数据安全与隐私等方面达到工程上的卓越水平。 多模态 RAG：尽管目前大部分 RAG 研究聚焦于文本任务，但扩展到图像、音频、视频、代码等领域已成为发展趋势。 评估方法：随着 RAG 应用的不断复杂化，需要发展更细致和多维的评估指标与工具，以评估上下文相关性、创造性、内容多样性及事实准确性，同时提升系统的可解释性。 RAG 工具 目前流行的构建 RAG 系统工具包括 LangChain、LlamaIndex 和 DSPy，同时也有诸如 Flowise AI 这样提供低代码解决方案的专用工具。其他相关技术还包括 HayStack、Meltano、Cohere Coral 等。各大软件和云服务供应商也在陆续推出面向 RAG 的服务，例如 Weaviate 的 Verba 和亚马逊的 Kendra。\n结论 总体来看，RAG 系统正处于快速演进阶段，高级范式的不断涌现使得定制化和性能优化成为可能，进一步拓宽了 RAG 在各领域的应用前景。面对不断增长的知识密集型任务需求，围绕 RAG 各组件的优化方法也在持续探索，从混合方法到自检索，现代 RAG 模型的研究正不断丰富。同时，评估工具和指标的改进也将为 RAG 系统的进一步发展提供有力支持。\n","permalink":"https://syhya.github.io/zh/posts/2025-03-07-rag/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e注意\u003c/strong\u003e: 本文\u003cstrong\u003e正在更新中\u003c/strong\u003e，内容只是\u003cstrong\u003e草稿版本\u003c/strong\u003e，并不完善，后续会有变动。请随时关注最新版本。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/zh/posts/2025-03-07-rag/image-3.png\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e检索增强生成（Retrieval-Augmented Generation, RAG）\u003c/strong\u003e 是一种结合大型语言模型（LLM）与外部知识库的先进技术框架。RAG 在回答问题时，不再仅依赖模型自身参数，而是通过实时检索外部知识库，显著降低模型的知识盲区，提升答案的准确性与可信度。\u003c/p\u003e","title":"RAG 技术综述（长期更新中）"},{"content":"引言 在深度学习中，网络架构的设计对模型的性能和训练效率有着至关重要的影响。随着模型深度的增加，训练深层神经网络面临诸多挑战，如梯度消失和梯度爆炸问题。为了应对这些挑战，残差连接和各种归一化方法被引入并广泛应用于现代深度学习模型中。本文将首先介绍残差连接和两种架构，分别是 pre-norm 和 post-norm。随后介绍四种常见的方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析为何当前主流的大模型倾向于采用 RMSNorm 与 Pre-Norm 结合的架构。\n残差连接 残差连接（Residual Connection） 是深度神经网络中的一项关键创新，它构成了残差网络（ResNet）(He, et al., 2015) 的核心。残差连接是一种重要的架构设计，目的是缓解深层网络训练中的梯度消失问题，并促进信息在网络中的流动。它通过引入快捷路径（Shortcut/Skip Connection），允许信息直接从浅层传递到深层，从而增强模型的表达能力和训练稳定性。\nFig. 1. Residual learning: a building block. (Image source: He, et al., 2015)\n在标准的残差连接中，输入 $x_l$ 经过一系列变换函数 $\\text{F}(\\cdot)$ 后，与原始输入 $x_l$ 相加，形成输出 $x_{l+1}$：\n$$ x_{l+1} = x_l + \\text{F}(x_l) $$其中：\n$x_l$ 是第 $l$ 层的输入。 $\\text{F}(x_l)$ 表示由一系列非线性变换（例如卷积层、全连接层、激活函数等）组成的残差函数。 $x_{l+1}$ 是第 $l+1$ 层的输出。 使用残差连接的结构有以下几个优势：\n缓解梯度消失： 通过快捷路径直接传递梯度，有效减少梯度在深层网络中的衰减，从而更容易训练更深的模型。 促进信息流动： 快捷路径允许信息更自由地在网络层之间流动，有助于网络学习更复杂的特征表示。 优化学习过程： 残差连接使得损失函数曲面更加平滑，优化模型的学习过程，使其更容易收敛到较好的解。 提升模型性能： 在图像识别、自然语言处理等多种深度学习任务中，使用残差连接的模型通常表现出更优越的性能。 Pre-Norm 与 Post-Norm 在讨论归一化方法时，Pre-Norm 和 Post-Norm 是两个关键的架构设计选择，尤其在 Transformer 模型中表现突出。以下将详细探讨两者的定义、区别及其对模型训练的影响。\n定义 Fig. 2. (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer. (Image source: Xiong, et al., 2020)\n从上图我可以直观看到，Post-Norm 和 Pre-Norm 的主要区别在于归一化层的位置：\nPost-Norm：传统的 Transformer 架构中，归一化层（如 LayerNorm）通常位于残差连接之后。\n$$ \\text{Post-Norm}: \\quad x_{l+1} = \\text{Norm}(x_l + \\text{F}(x_l)) $$ Pre-Norm：将归一化层放在残差连接之前。\n$$ \\text{Pre-Norm}: \\quad x_{l+1} = x_l + \\text{F}(\\text{Norm}(x_l)) $$ 对比分析 特性 Post-Norm Pre-Norm 归一化位置 残差连接之后 残差连接之前 梯度流动 可能导致梯度消失或爆炸，尤其在深层模型中 梯度更稳定，有助于训练深层模型 训练稳定性 难以训练深层模型，需要复杂的优化技巧 更容易训练深层模型，减少对学习率调度的依赖 信息传递 保留了原始输入的特性，有助于信息传递 可能导致输入特征的信息被压缩或丢失 模型性能 在浅层模型或需要强正则化效果时表现更优 在深层模型中表现更好，提升训练稳定性和收敛速度 实现复杂度 实现较为直接，但训练过程可能需要更多调优 实现简单，训练过程更稳定 Pre-Norm 和 Post-Norm 在模型训练中的差异可以从梯度反向传播的角度理解：\nPre-Norm：归一化操作在前，梯度在反向传播时能够更直接地传递到前面的层，减少了梯度消失的风险。但这也可能导致每一层的实际贡献被弱化，降低模型的实际有效深度。\nPost-Norm：归一化操作在后，有助于保持每一层的输出稳定，但在深层模型中，梯度可能会逐层衰减，导致训练困难。\nDeepNet (Wang, et al., 2022) 论文表明 Pre-Norm 在极深的 Transformer 模型中能够有效训练，而 Post-Norm 难以扩展到如此深度。\n归一化方法 在深度学习中，归一化方法种类繁多，不同的方法在不同的应用场景下表现各异。下面将详细介绍四种常见的归一化方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析它们的优劣势及适用场景。\nBatch Normalization Batch Normalization (Ioffe, et al., 2015) 旨在通过标准化每一批次的数据，使其均值为0，方差为1，从而缓解内部协变量偏移（Internal Covariate Shift）的问题。其数学表达式如下：\n$$ \\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta $$其中：\n$x_i$ 为输入向量中的第 $i$ 个样本。 $\\mu_{\\text{B}}$ 为当前批次的均值： $$ \\mu_{\\text{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i $$ 其中 $m$ 为批次大小。 $\\sigma_{\\text{B}}^2$ 为当前批次的方差： $$ \\sigma_{\\text{B}}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\text{B}})^2 $$ $\\epsilon$ 为一个极小的常数，用于防止分母为零。 $\\gamma$ 和 $\\beta$ 为可学习的缩放和平移参数。 优势：\n加速训练：通过标准化加速模型的收敛速度。 正则化效果：在一定程度上减少过拟合，降低了对 Dropout 等正则化技术的依赖。 减轻梯度消失问题：有助于缓解梯度消失，提高深层网络的训练效果。 缺点：\n对小批次不友好：在批次大小较小时，均值和方差的估计可能不稳定，影响归一化效果。 依赖批次大小：需要较大的批次才能获得良好的统计量估计，限制了在某些应用场景中的使用。 在某些网络结构中应用复杂：如循环神经网络（RNN），需要特殊处理以适应时间步的依赖性。 Layer Normalization Layer Normalization (Ba, et al., 2016) 通过在特征维度上进行归一化，使得每个样本的特征具有相同的均值和方差。其数学表达式如下：\n$$ \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta $$其中：\n$x$ 为输入向量。 $\\mu_{\\text{L}}$ 为特征维度的均值： $$ \\mu_{\\text{L}} = \\frac{1}{d} \\sum_{i=1}^{d} x_i $$ 其中 $d$ 为特征维度的大小。 $\\sigma_{\\text{L}}^2$ 为特征维度的方差： $$ \\sigma_{\\text{L}}^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu_{\\text{L}})^2 $$ $\\epsilon$ 为一个极小的常数，用于防止分母为零。 $\\gamma$ 和 $\\beta$ 为可学习的缩放和平移参数。 优势：\n对批次大小不敏感：适用于小批次或动态批次大小的场景，尤其在序列模型中表现优异。 适用于多种网络结构：在循环神经网络（RNN）和 Transformer 等模型中表现良好。 简化实现：无需依赖批次统计量，简化了在分布式训练中的实现。 缺点：\n计算量较大：相比 BatchNorm，计算均值和方差的开销稍高。 可能不如 BatchNorm 提升训练速度：在某些情况下，LayerNorm 的效果可能不如 BatchNorm 显著。 Weight Normalization Weight Normalization (Salimans, et al., 2016) 通过重新参数化神经网络中的权重向量来解耦其模长（norm）和方向（direction），从而简化优化过程并在一定程度上加速训练。其数学表达式如下：\n$$ w = \\frac{g}{\\lVert v \\rVert} \\cdot v $$$$ \\text{WeightNorm}(x) = w^T x + b $$其中：\n$w$ 是重新参数化后的权重向量。 $g$ 为可学习的标量缩放参数。 $v$ 为可学习的方向向量（与原始 $w$ 维度相同）。 $\\lVert v \\rVert$ 表示 $v$ 的欧几里得范数。 $x$ 为输入向量。 $b$ 为偏置项。 优势：\n简化优化目标：单独控制权重的模长与方向，有助于加速收敛。 稳定训练过程：在某些情况下，可减少梯度爆炸或消失问题。 实现不依赖批次大小：与输入数据的批次无关，适用性更广。 缺点：\n实现复杂度：需要对网络层进行重新参数化，可能带来额外的实现成本。 与其他归一化方法结合时需谨慎：如与 BatchNorm、LayerNorm 等同用时，需要调试和实验来确定最佳组合。 RMS Normalization RMS Normalization (Zhang, et al., 2019) 是一种简化的归一化方法，通过仅计算输入向量的均方根（RMS）进行归一化，从而减少计算开销。其数学表达式如下：\n$$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$其中：\n$x$ 为输入向量。 $d$ 为特征维度的大小。 $\\epsilon$ 为一个极小的常数，用于防止分母为零。 $\\gamma$ 为可学习的缩放参数。 优势：\n计算效率高：相比 LayerNorm 需要计算均值和方差，RMSNorm 仅需计算均方根，减少了计算开销。 训练稳定性：通过归一化输入，提升了模型的训练稳定性，使其在更大的学习率下仍能稳定训练。 资源优化：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。 简化实现：RMSNorm 的实现相对简单，便于在复杂模型中集成和优化，减少了工程实现的复杂性。 缺点：\n信息损失：仅使用均方根进行归一化，可能丢失部分信息，如均值信息。 适用性有限：在某些任务中，可能不如 BatchNorm 或 LayerNorm 表现优异。 代码示例 可以参考normalization.py\n归一化方法对比 以下两个表格对比了 BatchNorm、LayerNorm、WeightNorm 和 RMSNorm 四种归一化方法的主要特性：\nBatchNorm vs. LayerNorm 特性 BatchNorm (BN) LayerNorm (LN) 计算的统计量 批量的均值和方差 每个样本的均值和方差 操作维度 对批量数据的所有样本进行归一化 对每个样本的所有特征进行归一化 适用场景 适用于大批量数据，卷积神经网络 (CNN) 适用于小批量或序列数据，RNN 或 Transformer 是否依赖批量大小 强依赖批量大小 不依赖批量大小，适用于小批量或单样本任务 可学习的参数 缩放参数 $\\gamma$ 和平移参数 $\\beta$ 缩放参数 $\\gamma$ 和平移参数 $\\beta$ 公式 $\\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta$ $\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta$ 计算复杂度 需要计算批量的均值和方差 需要计算每个样本的均值和方差 使用示例 CNN, Vision Transformers RNN, Transformer, NLP WeightNorm vs. RMSNorm 特性 WeightNorm (WN) RMSNorm (RMS) 计算的统计量 分解权重向量的模长和方向 每个样本的均方根 (RMS) 操作维度 针对权重向量的维度进行重新参数化 对每个样本的所有特征进行归一化 适用场景 适用于需要更灵活的权重控制或加速收敛的场景 适用于需要高效计算的任务，如 RNN 或 Transformer 是否依赖批量大小 不依赖批量大小，与输入数据的维度无关 不依赖批量大小，适用于小批量或单样本任务 可学习的参数 标量缩放 $g$ 和方向向量 $v$ 缩放参数 $\\gamma$ 公式 $\\text{WeightNorm}(x) = w^T x + b$ $\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma$ 计算复杂度 参数重新分解和更新，额外开销较小，但需修改网络层实现 只需计算每个样本的均方根，计算较为高效 使用示例 深度网络的全连接层、卷积层等，提升训练稳定性和收敛速度 Transformer, NLP, 高效序列任务 通过上述对比，可以看出四种归一化方法各有优劣：\nBatchNorm 在大批量数据和卷积神经网络中表现优异，但对小批量敏感。 LayerNorm 适用于各种批量大小，尤其是在 RNN 和 Transformer 中效果显著。 WeightNorm 通过重新参数化权重向量，在一定程度上简化了优化过程并加速收敛。 RMSNorm 则在需要高效计算的场景下提供了一种轻量级的替代方案。 为什么当前主流 LLM 都使用 Pre-Norm 和 RMSNorm？ 近年来，随着大规模语言模型（LLM）如 GPT、LLaMA 和 Qwen 系列等的兴起，RMSNorm 和 Pre-Norm 已成为这些模型的标准选择。\nRMSNorm 的优势 Fig. 3. RMSNorm vs. LayerNorm: A Comparison of Time Consumption (Image source: Zhang, et al., 2019)\n计算效率更高\n减少运算量：只需计算输入向量的均方根（RMS），无需计算均值和方差。 加快训练速度：实际测试中，RMSNorm 显著缩短了训练时间（如上图由 665s 降至 501s），在大规模模型训练中尤其明显。 训练更稳定\n适应更大学习率：在保持稳定性的同时，能够使用更大学习率，加速模型收敛。 保持表达能力：通过适当的缩放参数 $\\gamma$ 简化归一化过程的同时，仍能维持模型的表现。 节省资源\n降低硬件需求：更少的计算开销既能提升速度，也能减少对硬件资源的占用，适合在资源受限环境中部署。 Pre-Norm 的优势 更易训练深层模型\n稳定梯度传播：在残差连接之前进行归一化，可有效缓解梯度消失或爆炸。 减少对复杂优化技巧的依赖：即使模型很深，训练过程依然稳定。 加速模型收敛\n高效的梯度流动：Pre-Norm 使梯度更容易传递到前面的层，整体收敛速度更快。 结论 残差连接和归一化方法在深度学习模型中扮演着至关重要的角色，不同的归一化方法和网络架构设计各有其适用场景和优缺点。通过引入残差连接，ResNet 成功地训练了极深的网络，显著提升了模型的表达能力和训练效率。同时，归一化方法如 BatchNorm、LayerNorm、WeightNorm 和 RMSNorm 各自提供了不同的优势，适应了不同的应用需求。\n随着模型规模的不断扩大，选择合适的归一化方法和网络架构设计变得尤为重要。RMSNorm 由于其高效的计算和良好的训练稳定性，结合 Pre-Norm 的架构设计，成为当前主流 LLM 的首选。这种组合不仅提升了模型的训练效率，还确保了在大规模参数下的训练稳定性和性能表现。\n参考文献 [1] He, Kaiming, et al. \u0026ldquo;Deep residual learning for image recognition.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[2] Xiong, Ruibin, et al. \u0026ldquo;On layer normalization in the transformer architecture.\u0026rdquo; International Conference on Machine Learning. PMLR, 2020.\n[3] Wang, Hongyu, et al. \u0026ldquo;Deepnet: Scaling transformers to 1,000 layers.\u0026rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).\n[4] Ioffe, Sergey. \u0026ldquo;Batch normalization: Accelerating deep network training by reducing internal covariate shift.\u0026rdquo; arXiv preprint arXiv:1502.03167 (2015).\n[5] Ba, Jimmy Lei. \u0026ldquo;Layer normalization.\u0026rdquo; arXiv preprint arXiv:1607.06450 (2016).\n[6] Salimans, Tim, and Durk P. Kingma. \u0026ldquo;Weight normalization: A simple reparameterization to accelerate training of deep neural networks.\u0026rdquo; Advances in neural information processing systems 29 (2016).\n[7] Zhang, Biao, and Rico Sennrich. \u0026ldquo;Root mean square layer normalization.\u0026rdquo; Advances in Neural Information Processing Systems 32 (2019).\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Feb 2025). 深度学习中的归一化.\nhttps://syhya.github.io/posts/2025-02-01-normalization\nOr\n@article{syhya2025normalization, title = \u0026#34;深度学习中的归一化\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-02-01-normalization\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-02-01-normalization/","summary":"\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e在深度学习中，网络架构的设计对模型的性能和训练效率有着至关重要的影响。随着模型深度的增加，训练深层神经网络面临诸多挑战，如梯度消失和梯度爆炸问题。为了应对这些挑战，残差连接和各种归一化方法被引入并广泛应用于现代深度学习模型中。本文将首先介绍残差连接和两种架构，分别是 pre-norm 和 post-norm。随后介绍四种常见的方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析为何当前主流的大模型倾向于采用 \u003cstrong\u003eRMSNorm\u003c/strong\u003e 与 \u003cstrong\u003ePre-Norm\u003c/strong\u003e 结合的架构。\u003c/p\u003e","title":"深度学习中的归一化"},{"content":"DeepSeek AI 近期发布 DeepSeek-R1 (DeepSeek-AI, 2025)，其推理性能在多个 benchmark 上已接近 OpenAI o1 (OpenAI, 2024)的水平，是开源社区成功复现 o1 的重要一步。R1 相关代码可以参考huggingface 尝试开源复现 open-r1 项目。以往的研究多依赖于海量的监督数据来提升大语言模型（Large Language Model, LLM）性能，但 DeepSeek-R1 及其早期实验 DeepSeek-R1-Zero 的成功，有力证明了纯粹大规模强化学习在提升 LLM 推理能力方面的潜力。其印证了 Richard Sutton 在 “The Bitter Lesson” 中提出的深刻见解:\nOne thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning. (Richard Sutton, 2019)\n符号 下面列举了文章所使用的数学公式，可以帮你更轻松阅读。\n符号 含义 \\( q \\) 或 \\( Q \\) 问题，用户提出的输入或指令 \\( o \\) 或 \\( O \\) 输出，模型生成的文本回复或答案 \\( t \\) token 索引，表示输出文本中的第 \\( t \\) 个 token 的位置 \\( o_t \\) 输出文本 \\( o \\) 中的第 \\( t \\) 个 token \\( o_{\u0026lt;t} \\) 输出文本 \\( o \\) 中前 \\( t-1 \\) 个 tokens \\( \u0026#124;o\u0026#124; \\) 输出文本 \\( o \\) 的长度，通常指 token 的数量 \\( G \\) 输出组的大小，在 GRPO 算法中，为每个问题采样的输出数量 \\( \\pi_\\theta, \\pi_{\\theta_{\\text{old}}}, \\pi_{\\text{ref}}, \\pi_{\\text{sft}} \\) 策略模型及其变体，用于生成文本输出或作为参考模型 \\( A_t, A_i \\) 优势函数与相对优势值 \\( \\varepsilon \\) 剪辑超参数, 用于限制重要性采样率的范围，保证策略更新的稳定性 \\( \\beta \\) 正则化系数，用于控制 KL 散度惩罚项在目标函数中的权重 \\( \\mathbb{D}_{KL} \\) KL 散度，衡量两个概率分布之间差异的度量，用于约束新策略与参考策略的距离 \\( \\mathcal{J}, \\mathcal{L} \\) 目标函数与损失函数 \\( \\mathbb{E} \\) 期望，表示对随机变量的平均值，在目标函数中表示对样本数据的平均 \\( P_{\\text{sft}}(Q, O) \\) SFT 数据集的分布，表示 \\( SFT \\) 数据集中问题 \\( Q \\) 和输出 \\( O \\) 的联合概率分布 \\( P_{\\text{sft}}(Q) \\) SFT 数据集中问题的分布，表示 \\( SFT \\) 数据集中问题 \\( Q \\) 的边缘概率分布 \\( \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\) 策略模型在给定问题 \\( q \\) 和之前生成的 tokens: \\( o_{\u0026lt;t} \\) 的条件下，生成第 \\( t \\) 个 token: \\( o_t \\) 条件概率 \\( \\mathbb{I}(o) \\) 判断输出 \\( o \\) 的答案是否为高质量的函数，高质量时为 1，否则为 0 \\( r(o) \\) 奖励函数，评估模型输出 \\( o \\) 质量的函数 \\( r_i \\) 第 \\( i \\) 个输出的奖励值 \\( \\nabla_{\\theta} \\) 梯度算子，表示对函数关于模型参数 \\( \\theta \\) 求梯度 \\( \\mathcal{N}(\\mu, 1) \\) 正态分布，均值为 \\( \\mu \\)，标准差为 1 \\( \\binom{a}{b} \\) 二项式系数，表示从 \\( a \\) 个元素中选择 \\( b \\) 个元素的组合数 \\( r(o) = \\frac{\\pi_{\\text{ref}}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\) 概率比值，参考模型与当前策略模型生成输出 \\( o \\) 的概率之比 训练流程概要 DeepSeek-R1 系列模型的训练是一个多阶段的过程，旨在构建具备卓越推理能力和通用语言能力的大型语言模型。整个训练流程从 DeepSeek-V3 (DeepSeek-AI, 2024) 模型出发，逐步迭代优化，最终得到不同版本的 DeepSeek-R1 模型。\nFig. 1. DeepSeek R1 Training Pipeline. (Image source: Harris Chan\u0026rsquo;s Tweet)\n如图1清晰的显示了 DeepSeek-R1 整个训练流程，主要分为以下几个关键阶段：\n基座模型与初步微调: 流程的起点是 DeepSeek-V3 Base 模型。首先，使用监督式微调 (SFT) 技术，在 冷启动长文本 CoT 数据 上对基础模型进行初步训练，赋予模型初步的推理能力。\n强化推理能力: 在 SFT 基础上，采用面向推理的强化学习方法，具体为组相对策略优化 (GRPO) 算法，并结合基于规则的奖励和CoT 语言一致性奖励，进一步提升模型的推理能力。\n推理数据生成与拒绝采样: 利用推理提示和拒绝采样技术，并以规则和让 DeepSeek-V3 模型进行评判数据质量，生成高质量的推理数据。\n非推理数据生成: 使用 CoT 提示方法，让 DeepSeek-V3 模型进行数据增强，生成非推理数据并且结合原始 SFT 数据，以提升模型的通用语言能力。\n蒸馏: 将推理数据和非推理数据结合，用于蒸馏训练。通过 SFT，将 DeepSeek-V3 的能力迁移到一系列小型模型 (Qwen 和 Llama 系列)，得到 DeepSeek-R1-Distill 系列模型。\n最终模型微调: 对 DeepSeek-V3 模型再次进行 SFT 和强化学习微调。强化学习阶段采用 推理和偏好奖励，并使用多样化的训练提示，最终得到 DeepSeek-R1 模型。\nDeepSeek-R1-Zero: 通过 GRPO 算法直接在 DeepSeek-V3 Base 上进行训练得到，作为其他模型的对比基准。\n接下来博主将深入分析 DeepSeek-R1 训练流程中的关键技术和方法。\nDeepSeek-R1-Zero PPO 近端策略优化 (Proximal Policy Optimization, PPO) (Schulman et al. 2017) 算法是一种广泛应用于强化学习的经典算法，在 InstructGPT(Ouyang et al. 2022) 论文中被证明是训练 LLM 强化学习微调阶段的有效且稳定的方法。\n强化学习核心思想是让智能体 (Agent) 在与环境的交互中学习，通过试错来最大化累积奖励。在LLM场景下，模型本身就是智能体，“环境” 可以理解为用户提出的问题和期望的回答方式。策略 (Policy) \\(\\pi_\\theta\\) 代表了智能体的行为准则，即给定一个输入 (例如问题 \\(q\\))，策略会输出一个动作 (例如生成文本 \\(o\\))。策略 \\(\\pi_\\theta\\) 通常由一个神经网络模型参数化，训练的目标是找到最优的参数 \\(\\theta\\)，使得策略能够生成高质量的输出。\nActor-Critic 框架是强化学习中常用的一种架构，PPO 也属于 Actor-Critic 算法。Actor-Critic 框架包含两个核心组件：\nActor (策略模型)：负责学习策略 \\(\\pi_\\theta\\)，即如何根据当前状态 (例如用户问题) 选择动作 (生成文本)。 Critic (价值模型)：负责评估 Actor 策略的好坏，通常通过学习一个价值函数 \\(V(s)\\) 或 \\(Q(s, a)\\) 来实现。价值函数预测在给定状态 \\(s\\) (或状态-动作对 \\((s, a)\\)) 下，未来能够获得的累积奖励的期望值。 PPO 的目标是改进策略模型 (Actor)，使其能够生成更高质量的输出，同时借助价值模型 (Critic) 来稳定训练过程。PPO 通过最大化以下目标函数来更新策略模型 \\(\\pi_{\\theta}\\)：\n\\[ \\mathcal{J}_{PPO}(\\theta) = \\mathbb{E}\\!\\Biggl[ \\min\\Bigl( \\underbrace{\\frac{\\pi_\\theta(a\\!\\mid\\!s)}{\\pi_{\\theta_{\\text{old}}}(a\\!\\mid\\!s)}}_{\\text{重要性采样率}}\\underbrace{A_t}_{\\text{优势函数}},\\, \\operatorname{clip}\\Bigl( \\underbrace{\\frac{\\pi_\\theta(a\\!\\mid\\!s)}{\\pi_{\\theta_{\\text{old}}}(a\\!\\mid\\!s)}}_{\\text{重要性采样率}}, 1-\\varepsilon,\\, 1+\\varepsilon \\Bigr)\\underbrace{A_t}_{\\text{优势函数}} \\Bigr) \\Biggr] \\]参数说明：\n期望 \\(\\mathbb{E}[\\cdot]\\)：表示对样本的平均。在实际训练中，我们会采样一批数据 (例如用户问题和模型生成的回答)，然后计算这批数据的平均目标函数值。\n重要性采样率：衡量当前策略 \\(\\pi_\\theta\\) 与旧策略 \\(\\pi_{\\theta_{\\text{old}}}\\) 在动作 \\(a\\) 上的概率比值。PPO 采用 近端策略更新 的思想，限制每次策略更新的幅度，避免策略变化过大导致训练不稳定。\n优势函数 \\(A_t\\)：评估在状态 \\(s\\) 下采取动作 \\(a\\) 相对于平均水平的优势。优势函数通常由 Critic 模型 (价值网络) 估计得到，可以是优势估计 (Advantage Estimation) 或 广义优势估计 (Generalized Advantage Estimation, GAE) 等方法。优势函数 \\(A_t\\) 越大，表示当前动作 \\(a\\) 越好，策略模型应该增加采取该动作的概率。\nclip：PPO 的核心机制之一，本质上可以看作是一个惩罚函数，用于限制重要性采样率的范围在 \\([1-\\varepsilon, 1+\\varepsilon]\\) 之间，其中 \\(\\varepsilon\\) 是一个超参数 (通常设置为 0.2)。剪辑操作防止策略更新步幅过大，提高训练的稳定性。\nclip 函数通过限制重要性采样率来惩罚过大或过小的策略更新幅度。\n当重要性采样率超出 \\([1-\\varepsilon, 1+\\varepsilon]\\) 范围时，clip 函数会将其限制在该范围内，从而降低目标函数的增益 (或减少损失)。 对于正向更新 (\\(A_t \u003e 0\\))： 如果重要性采样率过大 (超过 \\(1+\\varepsilon\\))，clip 会将其限制为 \\(1+\\varepsilon\\)，降低了实际的更新幅度，惩罚了过于激进的策略改进。 对于负向更新 (\\(A_t \u003c 0\\))： 如果重要性采样率过小 (小于 \\(1-\\varepsilon\\))，clip 会将其限制为 \\(1-\\varepsilon\\)，同样限制了更新幅度，避免策略发生剧烈变化。 目标函数取 clip 之前和 clip 之后的最小值，确保在重要性采样率超出范围时，PPO 会对策略更新进行惩罚，保证策略更新的“保守性”。\n在实际优化过程中，我们通常将 PPO 损失函数 \\(\\mathcal{L}_{PPO}(\\theta)\\) 定义为目标函数的负值，通过最小化损失来最大化目标函数：\n\\[ \\mathcal{L}_{PPO}(\\theta) = -\\,\\mathcal{J}_{PPO}(\\theta). \\]PPO 算法因其 简单有效、相对稳定 的特点，成为强化学习领域的基准算法之一，并在各种任务中取得了成功，包括大型语言模型的强化学习微调。PPO 通常被认为比早期的 TRPO 等方法更稳定，但在大模型上的具体应用仍需要细致的超参数调优。在大语言模型场景下，如果价值网络与策略网络完全分离且规模相当，势必会带来更多的计算与内存开销。为解决这些问题，DeepSeek 团队提出了组相对策略优化 (GRPO)算法。\nGRPO 组相对策略优化 (Group Relative Policy Optimization, GRPO) (Shao, et al. 2024) 是 DeepSeek 团队为训练 DeepSeek-R1-Zero 这样的大语言模型而专门设计的一种高效稳定的强化学习算法。GRPO 的核心创新在于摒弃了传统Actor-Critic 框架中对独立价值网络 (critic model) 的依赖，降低了计算成本，并提高了训练的稳定性。 从广义上讲，GRPO 可以被视为一种 Actor-Only 的强化学习方法。\nGRPO 的灵感来源于 相对评估 的思想。在许多实际场景中，我们往往更容易判断一组事物之间的相对好坏，而不是给出绝对的价值评估。例如，在评价一组学生的作业时，老师可能更容易比较不同作业之间的优劣，而不是给每份作业打一个绝对分数。GRPO 将这种相对评估的思想引入强化学习，通过 组内相对评分来构建基准 (baseline)，完全替代了对价值网络的依赖。\n具体而言，对于每个问题 \\( q \\)，GRPO 会从旧策略 \\( \\pi_{\\theta_{\\text{old}}} \\) 中采样一组输出 \\( \\{o_1, o_2, \\ldots, o_G\\} \\)，形成一个 输出组。然后，通过最大化下面的目标函数来更新策略模型 \\( \\pi_{\\theta} \\)：\n\\[ \\begin{aligned} \\mathcal{J}_{GRPO}(\\theta) \u0026 = \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O \\mid q)\\right] \\\\ \u0026 \\quad \\frac{1}{G} \\sum_{i=1}^G \\Biggl( \\min\\biggl( \\underbrace{\\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)}}_{\\text{重要性采样率}} \\,\\underbrace{A_i}_{\\text{相对优势值}},\\, \\operatorname{clip}\\Bigl( \\underbrace{\\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)}}_{\\text{重要性采样率}}, 1-\\varepsilon,\\, 1+\\varepsilon \\Bigr)\\,\\underbrace{A_i}_{\\text{相对优势值}} \\biggr) \\;-\\;\\beta\\,\\underbrace{\\mathbb{D}_{KL}\\bigl(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\bigr)}_{\\text{KL 散度惩罚项}} \\Biggr), \\end{aligned} \\]与 PPO 的目标函数类似，GRPO 的目标函数也包含重要性采样率和 clip，用于保证策略更新的稳定性。不同之处在于：\n相对优势值 \\(A_i\\)：GRPO 使用 相对优势值 \\(A_i\\) 代替 PPO 中的优势函数 \\(A_t\\)。相对优势值 \\(A_i\\) 是根据 组内奖励 计算得到的，无需价值网络估计。 KL 散度惩罚项 \\(\\mathbb{D}_{KL}\\bigl(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\bigr)\\)：为了进一步约束策略更新，GRPO 引入了 KL 散度惩罚项，限制新策略 \\(\\pi_\\theta\\) 与参考策略 \\(\\pi_{\\text{ref}}\\) 之间的差异过大。 Fig. 2. The comparison of PPO and GRPO. (Image source: DeepSeek-AI, 2024)\n从上图2我们可以看出GRPO 的核心创新在于 相对优势值 \\(A_i\\) 的计算方式。与 PPO 不同，GRPO 不依赖于独立的价值网络，而是直接利用 组内奖励 进行相对评估。对于每个输出组 \\( \\{o_1, o_2, \\ldots, o_G\\} \\)，GRPO 首先获取每个输出对应的奖励值 \\( \\{r_1, r_2, \\ldots, r_G\\} \\)。然后，根据以下公式计算相对优势值 \\( A_i \\)：\n\\[ A_i = \\frac{\\,r_i \\;-\\; \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})\\,}{ \\text{std}\\bigl(\\{r_1, r_2, \\ldots, r_G\\}\\bigr)}. \\]相对优势值 \\( A_i \\) 通过 标准化 组内奖励 \\( \\{r_1, r_2, \\ldots, r_G\\} \\) 得到，具有 零均值和单位方差，更好地反映了每个输出在组内的相对优劣程度。\nGRPO 采用 相对评估 的方式，具有以下优点：\n无需训练价值网络：避免了训练大规模价值网络带来的计算开销和不稳定性。 降低价值估计方差：相对评估关注组内输出的相对优劣，而不是绝对价值，降低了估计方差，提高了训练稳定性。 更符合奖励模型的比较特性：奖励模型通常基于比较数据训练，GRPO 的相对评估方式与之更契合。 更适用于序列生成任务的信用分配：即使奖励是稀疏的，GRPO 也能有效学习，因为它关注同组输出之间的相对好坏。 Schulman 无偏估计器 KL 散度 \\(\\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right)\\) 衡量了策略 \\(\\pi_\\theta\\) 相对于参考策略 \\(\\pi_{\\text{ref}}\\) 的信息损失，其标准定义为：\n\\[ \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) = \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\log \\frac{\\pi_\\theta(o \\mid q)}{\\pi_{\\text{ref}}(o \\mid q)} \\right]. \\]如前所述，直接计算上述期望在实际中面临挑战。为了解决这个问题，GRPO 采用了 Schulman 无偏估计器 (Schulman, 2020)。与公式中可能使用的 KL 散度惩罚项不同，我们使用以下无偏估计器来估计 \\(\\pi_\\theta\\) 和 \\(\\pi_{ref}\\) 之间的 KL 散度：\n$$ \\mathbb{D}_{K L}\\left[\\pi_{\\theta}| | \\pi_{r e f}\\right]=\\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}{\\pi_{\\theta}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}{\\pi_{\\theta}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}-1. $$为了理解这个估计器的优点，我们首先从数学上推导其无偏性。\n无偏性证明 为了简化符号，我们令 \\(r(o) = \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)}\\)。则 Schulman 估计器可以写成：\n\\[ \\hat{D}_{KL}(o) = r(o) - \\log r(o) - 1. \\]我们需要证明，当 \\(o\\) 从 \\(\\pi_\\theta(\\cdot|q)\\) 中采样时，\\(\\hat{D}_{KL}(o)\\) 的期望等于真实的 KL 散度 \\(\\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right)\\)。\n\\[ \\begin{aligned} \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} [\\hat{D}_{KL}(o)] \u0026= \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} [r(o) - \\log r(o) - 1] \\\\ \u0026= \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - 1 \\right] \\\\ \u0026= \\sum_{o} \\pi_\\theta(o \\mid q) \\left[ \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - 1 \\right] \\quad (\\text{离散情况，连续情况为积分}) \\\\ \u0026= \\sum_{o} \\left[ \\pi_{ref}(o \\mid q) - \\pi_\\theta(o \\mid q) \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\pi_\\theta(o \\mid q) \\right] \\\\ \u0026= \\underbrace{\\sum_{o} \\pi_{ref}(o \\mid q)}_{=1} - \\underbrace{\\sum_{o} \\pi_\\theta(o \\mid q) \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)}}_{=-\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})} - \\underbrace{\\sum_{o} \\pi_\\theta(o \\mid q)}_{=1} \\\\ \u0026= 1 - (-\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})) - 1 \\\\ \u0026= \\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref}). \\end{aligned} \\]因此，我们证明了 \\(\\hat{D}_{KL}(o)\\) 是 \\(\\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right)\\) 的无偏估计。\n三种 KL 散度估计器对比 为了直观理解三种估计器的差异，以下表格列出了它们的数学表达式，其中 \\( r(o) = \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\)：\n估计器 数学表达式 主要特点 k1 (朴素估计器) \\(\\hat{D}_{KL}^{(k1)}(o) = \\log \\frac{\\pi_\\theta(o \\mid q)}{\\pi_{ref}(o \\mid q)} = \\log \\frac{1}{r(o)}\\) 简单直接，对应 KL 散度定义；方差较高，估计结果波动较大。 k2 (平方对数比估计器) \\(\\hat{D}_{KL}^{(k2)}(o) = \\frac{1}{2} (\\log r(o))^2\\) 使用对数比的平方，始终为正，降低方差；引入偏差，尤其在分布差异大时。 k3 (Schulman 无偏估计器) \\(\\hat{D}_{KL}^{(k3)}(o) = r(o) - \\log r(o) - 1\\) 结合了比值 \\( r(o) \\) 和对数比 \\(\\log r(o)\\)；无偏，较低方差，估计稳定。 k1 (朴素估计器): 无偏简单直接，但方差较高，导致估计结果不稳定。 k2 (平方对数比估计器): 降低了方差，但引入了偏差，特别是在分布差异较大时偏差显著。 k3 (Schulman 无偏估计器): 兼具无偏性和较低的方差，提供了稳定的估计结果。 实验结果 为了评估三种 KL 散度估计器的性能，我们进行了数值实验，结果如下表所示。实验中，固定分布 \\( q = \\mathcal{N}(0, 1) \\)，通过改变分布 \\( p = \\mathcal{N}(\\mu, 1) \\) 的均值 \\(\\mu\\) 来控制真实的 KL 散度 \\(\\mathbb{D}_{KL}(p \\| q)\\)。使用5亿个样本进行 Monte Carlo 估计，并重复实验以获得稳定结果。\n实验代码可以参考 unbiased_kl_divergence.py\n真实 KL 散度 估计器 平均估计值 标准差 相对偏差 (%) 0.005 k1 0.005 0.1 0.0387 0.005 k2 0.005 0.0071 0.2415 0.005 k3 0.005 0.0071 -0.0082 0.125 k1 0.125 0.5 -0.0389 0.125 k2 0.1328 0.1875 6.2500 0.125 k3 0.125 0.1845 0.0072 0.5 k1 0.5 1 -0.0018 0.5 k2 0.625 0.866 25.0004 0.5 k3 0.5 0.8478 0.0052 朴素估计器 (k1):\n无偏性: 平均估计值与真实 KL 散度高度吻合，相对偏差接近 0%。 方差: 标准差高于 k3，且随着真实 KL 散度的增加而增大，导致估计结果不稳定。 平方对数比估计器 (k2):\n无偏性: 存在一定偏差，且偏差随真实 KL 散度的增加而显著增大（例如，真实 KL 为 0.5 时相对偏差达到 25%）。 方差: 在较低的真实 KL 散度下方差较低，但整体表现不稳定。 Schulman 无偏估计器 (k3):\n无偏性: 实验结果显示相对偏差极小，几乎为 0%，验证了其无偏性。 方差: 标准差明显低于 k1，且与 k1 相比在所有 KL 散度下均表现出更低的方差，尤其在较低 KL 散度时优势显著。 优点总结 无偏性: 理论和实验结果均表明，k3 是无偏估计器，能够准确反映真实的 KL 散度。 正定性: 估计值始终为非负，符合 KL 散度的性质。 较低的方差: 相较于 k1，k3 显著降低了估计方差，提供了更稳定的估计结果，尤其在 KL 散度较小时表现突出。 Schulman 无偏估计器 \\( \\hat{D}_{KL}^{(k3)}(o) = r(o) - \\log r(o) - 1 \\) 为 KL 散度提供了一种兼具无偏性和低方差的估计方法。其无偏性确保了估计的准确性，而较低的方差提升了估计的稳定性，特别适用于需要稳定梯度信号的强化学习场景，如策略优化。基于这些优势，GRPO 算法选择使用 k3 作为惩罚策略偏离的估计器，从而保证训练过程的稳定性和最终策略的性能。\n在实际优化中，GRPO 损失函数 \\(\\mathcal{L}_{GRPO}(\\theta)\\) 被定义为目标函数 \\(\\mathcal{J}_{GRPO}(\\theta)\\) 的负值，通过最小化损失函数 \\(\\mathcal{L}_{GRPO}(\\theta)\\) 来实现目标函数 \\(\\mathcal{J}_{GRPO}(\\theta)\\) 的最大化：\n\\[ \\mathcal{L}_{GRPO}(\\theta) = -\\,\\mathcal{J}_{GRPO}(\\theta) \\]PPO 与 GRPO 对比 为更清晰理解 PPO 和 GRPO 的异同，以下表格对两种算法进行对比：\n特性 PPO GRPO 是否 Actor-Critic 是 是 (广义上可以认为是 Actor-Only) 是否价值网络 需要独立的价值网络 (Critic) 无需独立的价值网络 优势函数估计 通过价值网络估计绝对优势值 通过组内奖励相对评估相对优势值 计算开销 较高，需要训练价值网络 较低，无需训练价值网络 训练稳定性 相对较好，但价值网络训练可能引入不稳定性 更好，避免了价值网络训练带来的不稳定性 算法复杂度 相对复杂，需要维护和更新策略网络和价值网络 相对简单，只需维护和更新策略网络 适用场景 广泛适用于各种强化学习任务，包括中小规模语言模型微调 特别适用于大语言模型的强化学习微调，注重效率和稳定性 信用分配 依赖价值网络进行时间差分学习，处理信用分配问题 依赖最终奖励和组内相对评估，也可辅助中间奖励 方差问题 价值网络估计可能引入方差 组内相对优势估计在小组规模下可能存在方差，可通过增大组规模等缓解 从表中可以看出，PPO 是一种通用且强大的强化学习算法，但其训练价值网络的机制在大语言模型场景下带来了额外的计算负担和潜在的不稳定性。GRPO 通过引入组相对评分，巧妙地规避了对价值网络的需求，在保证性能的同时，显著降低了计算成本，并提升了训练稳定性。这使得 GRPO 成为在训练资源不多的情况下训练 DeepSeek-R1-Zero 这样 LLM 的理想选择。\n代码生成评估指标 代码生成会采用更严谨的测试方法。通过编译器执行模型生成的代码，并使用预定义的测试用例进行多次单元测试，以判断代码的正确性。常用的评估指标包括 pass@k(Chen et al. 2021) 和 cons@N(OpenAI, 2024)。\npass@k: 衡量模型在生成 k 个代码样本时，至少有一个样本能够通过所有预定义测试用例的概率。\npass@k 有偏估计公式 \\[ \\text{Simplified pass@k} = \\frac{1}{P} \\sum_{i=1}^{P} C_i \\]\\[ C_i = \\begin{cases} 1 \u0026 \\text{如果生成的 k 个样本中至少有一个是正确的} \\\\ 0 \u0026 \\text{如果生成的 k 个样本全部都不正确} \\end{cases} \\]参数说明:\n\\( P \\): 评估的问题总数。 \\( C_i \\): 对于第 \\(i\\) 个问题，如果生成的 \\(k\\) 个样本中至少有一个是正确的，则 \\(C_i = 1\\)，否则 \\(C_i = 0\\)。 \\( \\sum_{i=1}^{P} C_i \\): 表示在所有 \\(P\\) 个问题中，被 “解决” 的问题总数。 \\( \\frac{1}{P} \\sum_{i=1}^{P} C_i \\): 表示 “解决” 问题的比例，即准确率。 公式含义: 这种简化方法直接计算 生成 k 个样本后，至少有一个样本正确的比例。 虽然这种方法提供的是 pass@k 的 有偏估计，可能会略微高估真实值，但它在实践中非常常用，因为它 直观、易于计算，并且在样本量足够大时，能够提供对模型性能的合理近似。尤其在工业界和快速评估场景中，这种简化方法非常实用。\n然而，LLM 在推理解码时会受到 temperature、top_p（核采样概率）、top_k（候选词数量）和repetition_penalty 等参数的影响。这些参数会使代码生成结果有随机性和多样性，并且当样本 K 比较少的如果设置随机过高的参数，会影响 pass@k 的评估结果。因此，采用无偏估计方法能够更准确地反映模型的真实性能。\npass@k 的无偏估计公式 \\[ \\text { pass @ } k:=\\underset{\\text { Problems }}{\\mathbb{E}}\\left[1-\\frac{\\binom{n-c}{k}}{\\binom{n}{k}}\\right] \\]参数说明:\n\\( n \\): 为每个问题生成的代码样本总数。 \\( c \\): 在 \\( n \\) 个样本中，能够通过所有单元测试的正确样本数。 \\( k \\): pass@\\(k\\) 指标中的参数 \\(k\\)，表示我们考虑的生成样本数量。 \\( \\binom{a}{b} \\): 表示二项式系数，计算从 \\(a\\) 个元素中选择 \\(b\\) 个元素的组合数。 \\( \\underset{\\text { Problems }}{\\mathbb{E}} \\): 表示对所有评估问题的期望值（平均值）。 公式含义:\n公式实际上计算的是至少有一个正确样本的概率。公式 \\( \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\) 计算的是在生成的 \\(n\\) 个样本中，随机抽取 \\(k\\) 个样本，且这 \\(k\\) 个样本都不正确的概率。我们用 1 减去这个概率，就得到了 在 \\(n\\) 个样本中，随机抽取 \\(k\\) 个样本，且这 \\(k\\) 个样本中至少有一个正确的概率， 这就是 pass@\\(k\\) 指标的含义。 这个公式提供的是 pass@k 的 无偏估计，更适用于学术研究等需要精确评估的场景。 在实际计算中，通常会生成远大于 \\(k\\) 的样本数 \\(n\\) (例如论文中使用 \\(n=200\\), \\(k \\leq 100\\))，以更稳定地估计 pass@\\(k\\)。 pass@k 简化乘积形式 为了更方便数值计算，原始公式还可以转化为以下乘积形式，它仍然是无偏估计，并能避免数值溢出问题：\n\\[ \\text { pass @ } k = \\underset{\\text { Problems }}{\\mathbb{E}}\\left[1 - \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i}\\right] \\]推导过程:\n至少有一个正确样本的反面是所有 k 个样本都不正确。 因此，pass@k 等于 1 减去 所有 k 个样本都不正确的概率。\n考虑不放回抽样的场景。假设我们从 \\(n\\) 个样本中抽取 \\(k\\) 个样本，要计算这 \\(k\\) 个样本都不正确的概率。总共有 \\(n\\) 个样本，其中 \\(n-c\\) 个是不正确的。\n第一次抽取时，抽到不正确样本的概率为 \\( \\frac{n-c}{n} \\)。\n在第一次抽取到不正确样本的条件下，第二次抽取时，剩余 \\(n-1\\) 个样本中，有 \\(n-c-1\\) 个不正确样本。因此，第二次仍然抽到不正确样本的条件概率为 \\( \\frac{n-c-1}{n-1} \\)。\n以此类推，第 \\(i\\) 次抽取时 ( \\(i\\) 从 1 到 \\(k\\) )，在之前 \\(i-1\\) 次都抽到不正确样本的条件下，第 \\(i\\) 次仍然抽到不正确样本的条件概率为 \\( \\frac{n-c-(i-1)}{n-(i-1)} = \\frac{n-c-i+1}{n-i+1} \\)。 为了与公式中的索引 \\(i=0\\) 对齐，我们将索引改为从 \\(i=0\\) 到 \\(k-1\\)，则第 \\(i+1\\) 次抽取时 ( \\(i\\) 从 0 到 \\(k-1\\) )，条件概率为 \\( \\frac{n-c-i}{n-i} \\)。\n将这 \\(k\\) 次抽取的条件概率连乘，即可得到所有 \\(k\\) 个样本都不正确的概率：\n\\[ P(\\text{所有 k 个样本都不正确}) = \\frac{n-c}{n} \\times \\frac{n-c-1}{n-1} \\times \\cdots \\times \\frac{n-c-k+1}{n-k+1} = \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i} \\] 最终，pass@k 的简化公式为：\n\\[ \\text { pass @ } k = \\underset{\\text { Problems }}{\\mathbb{E}}\\left[1 - \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i}\\right] \\] 这个乘积形式的公式，避免了直接计算可能数值很大的二项式系数，更易于理解和数值计算，尤其是在编程实现时，可以逐项累乘，有效防止数值溢出。\ncons@N cons@N: 通过生成 N 个样本，并从中选择出现频率最高的答案作为最终答案，评估该答案的准确率。在 DeepSeek-R1-Zero 的评估中，使用了 cons@64，即生成 64 个样本，并取其中出现次数最多的答案作为最终答案进行评估。\n\\[ \\text{cons@N} = \\frac{1}{P} \\sum_{i=1}^{P} \\mathbb{I}(\\text{ConsensusAnswer}_i \\text{ is correct}) \\]参数说明:\n\\( P \\)：评估的问题总数。 \\( \\text{ConsensusAnswer}_i \\)：通过多数投票得到的 共识答案。 \\( \\mathbb{I}(\\text{ConsensusAnswer}_i \\text{ is correct}) \\)：指示函数，若共识答案正确，则为 1，否则为 0。 公式含义： 计算在所有评估问题中，共识答案正确的比例。通过增加生成样本数 \\(N\\)，并采用多数投票策略，cons@N 指标能够更稳定和可靠地评估模型的平均性能。在模型生成结果存在一定随机性的情况下，该指标可以验证模型输出的一致性和准确性。\n奖励模型 奖励模型在 LLM 的研发中至关重要，主要应用于以下关键环节：\n基于人类反馈的强化学习: 在基于人类反馈的强化学习（RLHF）流程中，奖励模型用于评估模型生成结果的质量，并为后续的强化学习提供奖励信号。\n拒绝采样的关键工具: 在拒绝采样过程中，奖励模型对大量候选结果进行评分，筛选出高质量样本用于监督微调（SFT）。拒绝采样是自动化样本工程的重要方法，而奖励模型是其核心组成部分。\n业务场景中的判别器: 在实际应用中，奖励模型作为 LLM 输出结果的判别器或校验器，评估生成结果的质量。只有得分超过预设阈值的结果才会输出，否者进行再生成或降级处理，提高输出的可靠性和安全性。\nORM 与 PRM Fig. 3. Outcome reward vs Process reward. (Image source: Zeng et al. 2024)\n当前奖励模型主要分为两种范式：结果奖励模型(Outcome Reward Model, ORM) 和 过程奖励模型(Process Reward Model, PRM)。上图3直观的展示了这两种奖励模型的区别。以下表格也对比了这两种模型的主要特性：\n特性 ORM PRM 定义 对模型生成的完整结果进行整体评分 在内容生成过程中，对每一步或每个阶段进行细粒度评分 主要优势 简单直接，易于实现 对整体结果进行全面评估 提供更精细的奖励信号 有助于指导模型生成过程的每个步骤 主要劣势 方差较高，估计结果波动较大 缺乏过程中的反馈 训练和应用更为复杂 可能引入偏差，尤其在分布差异大时 适用场景 需要整体评估生成结果的任务 需要细粒度控制生成过程的任务，如分步推理或复杂生成任务 避免奖励欺骗的能力 中等，依赖于整体评分的准确性 较低，可通过优化每一步的奖励而非整体表现来作弊 训练复杂度 较低，无需对生成过程进行额外的监督 较高，需要在生成的每一步进行评分，增加了计算和数据需求 可解释性 高，评分基于最终结果 较低，评分涉及生成过程的多个步骤，难以全面理解每一步的评分依据 为了训练 DeepSeek-R1-Zero，DeepSeek 团队选择了ORM，而非PRM。此选择基于以下考虑：\n避免奖励欺骗\nPRM在大规模 RL 训练中，容易被智能体利用，导致奖励欺骗（Reward Hacking）（Gao et al. 2022）。模型可能采取“旁门左道”的策略以最大化奖励，而非提升推理能力。基于规则的奖励系统通过明确且可解释的规则，有效避免了奖励欺骗问题。\n基于规则的奖励系统在问题场景复杂或需要创造性回答时，可能难以覆盖所有类型的问题，规则设计可能存在漏洞被模型利用。\n降低训练复杂度\n训练 PRM 需要大量计算资源和数据，增加了训练流程的复杂性。而基于规则的奖励系统无需额外训练，规则一旦确定即可直接应用，简化了训练流程。基于规则的奖励系统特别适合自动判分或目标明确的任务，如数学题、LeetCode 编程题及对输出格式有明确要求的任务。对于开放式对话或创意类任务，则可能需要结合人类反馈或训练好的奖励模型。\n奖励机制 DeepSeek-R1-Zero 的奖励系统采用双重奖励机制，通过预定义的规则进行自动化评估，确保评估过程的高效性和实时性。这套系统主要包含以下两种类型的奖励：\n1. 准确性奖励\n定义： 衡量模型输出结果的正确性，是奖励系统中最关键的部分。 实现方式： 根据不同任务类型采用不同的验证方法： 数学问题： 验证最终答案是否与标准答案一致。 代码生成： 通过编译器执行模型生成的代码，并使用预设的单元测试用例进行多次测试，判断代码的正确性。 目的： 引导模型生成准确、可靠的输出结果。 2. 格式奖励\n定义： 为了提升模型输出的可读性和结构性，方便后续分析和评估而引入的奖励机制。 评估方式： 在强化学习训练过程中，通过预定义的规则系统进行自动化评估。 目的： 鼓励模型生成结构化的输出，例如包含思考过程和最终答案，使其更易于理解和分析。 DeepSeek-R1-Zero 的奖励函数 \\(r(o)\\) 由准确性奖励和格式奖励加权求和构成：\n$$ r(o) = r_{\\text{accuracy}}(o) + \\lambda \\cdot r_{\\text{format_effective}}(o) $$其中，有效格式奖励 \\(r_{\\text{format_effective}}(o)\\) 的计算方式如下：\n$$ r_{\\text{format_effective}}(o) = \\begin{cases} r_{\\text{format}}(o) \u0026 \\text{如果 } o \\text{ 的基本格式符合要求} \\\\ 0 \u0026 \\text{如果 } o \\text{ 的基本格式不符合要求} \\end{cases} $$基础格式奖励 \\(r_{\\text{format}}(o)\\) 则根据格式规范的符合程度进行分级：\n$$ r_{\\text{format}}(o) = \\begin{cases} R_{\\text{format_full}} \u0026 \\text{如果 } o \\text{ 的格式完全符合规范} \\\\ R_{\\text{format_partial}} \u0026 \\text{如果 } o \\text{ 的格式部分符合规范} \\\\ 0 \u0026 \\text{如果 } o \\text{ 的格式不符合规范} \\end{cases} $$训练模板 为了引导基模型遵循指定的指令，DeepSeek 团队设计了一个简洁而有效的训练模板。该模板要求模型首先生成推理过程（放在 \u0026lt;think\u0026gt; 和 \u0026lt;/think\u0026gt; 标签之间），然后再提供最终答案（放在 \u0026lt;answer\u0026gt; 和 \u0026lt;/answer\u0026gt; 标签之间）。这种结构化的格式，不仅确保了输出的可读性，更使研究人员能够清晰地观察模型在 RL 训练过程中的推理过程，从而更准确地评估模型的学习进展。\n角色 提示内容 助手回复 用户 prompt (用户提出的问题) 助手： \u0026lt;think\u0026gt; 推理过程 \u0026lt;/think\u0026gt; \u0026lt;answer\u0026gt; 答案 \u0026lt;/answer\u0026gt; \u0026lt;think\u0026gt; 和 \u0026lt;/think\u0026gt; (思维过程标签): 用于包裹模型的中间推理步骤，清晰展示模型的思考过程，便于理解模型的推理逻辑和进行错误分析。 \u0026lt;answer\u0026gt; 和 \u0026lt;/answer\u0026gt; (最终答案标签): 用于包裹模型的最终答案，方便程序自动化提取答案部分，进行高效的评估和后续处理。 评估流程 准确性评估： 评估模型输出 \\(o\\) 的答案是否正确，计算准确性奖励 \\(r_{\\text{accuracy}}(o)\\)。 基本格式检查： 检查输出 \\(o\\) 的基本格式是否符合预定义要求，例如是否包含必要的标签 \u0026lt;think\u0026gt; 和 \u0026lt;answer\u0026gt;，以及标签是否正确闭合和嵌套。 有效格式奖励判断： 基本格式不符合： 有效格式奖励 \\(r_{\\text{format_effective}}(o) = 0\\)。 基本格式符合： 进一步评估格式规范程度，计算基础格式奖励 \\(r_{\\text{format}}(o)\\)。 最终奖励计算： 将准确性奖励 \\(r_{\\text{accuracy}}(o)\\) 和有效格式奖励 \\(r_{\\text{format_effective}}(o)\\) 进行线性加权求和，得到最终奖励 \\(r(o)\\)。 通过结合准确性奖励和格式奖励，DeepSeek-R1-Zero 的奖励系统不仅关注模型输出的正确性，更重视输出结果的结构化和可读性。这使得模型不仅能够给出正确的答案，还能展现其思考过程，使其更像一个具备推理能力的智能体，而不仅仅是一个简单的答案输出机器。\n基准测试 Fig. 4. Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. (Image source: DeepSeek-AI, 2025)\n图4展示了不同模型在多项基准测试上的表现。在 AIME 2024 基准测试中，DeepSeek-R1-Zero 模型的 pass@1 分数达到了71.0%，此外 cons@64 分数为86.7%，与 OpenAI o1-0912 模型相当。\n自我反思 Fig. 5. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)\n图5展示了随着训练过程的深入，DeepSeek-R1-Zero 逐渐涌现出自我进化能力，模型不仅能够通过自我反思重新评估此前的推理步骤，还能主动探索替代的求解路径。此外模型能够根据问题的复杂程度，自适应地增加推理过程中生成的 token 数量，以实现更长、更深入的思考时间。总的来说这种动态且自发的行为显著增强了模型的推理能力，使其能够更加高效且准确地解决更复杂、更具挑战性的任务。\nFig. 6. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: DeepSeek-AI, 2025)\n图6展示了模型 DeepSeek-R1-Zero 出现了顿悟时刻（aha moment）在此阶段，本质上是模型在推理过程中，突然意识到之前的思考路径是错误的，并迅速调整了思考方向，最终得出了正确的答案。此现象同样表明模型在推理过程中具备了一定的自我反思能力。\nDeepSeek-R1 训练流程 为了在 DeepSeek-R1-Zero 的基础上进一步提升模型性能，DeepSeek 团队采用了 多阶段训练 策略，并将 冷启动数据 融入到训练流程中。DeepSeek-R1 的训练流程主要包括以下四个阶段，体现了从初步策略初始化到全面能力提升的进阶之路：\n冷启动: 利用高质量的思维链 (Chain-of-Thought, CoT) 数据，对 DeepSeek-V3-Base 基模型进行初步的监督微调，为后续强化学习奠定基础。\n面向推理的强化学习: 在冷启动模型基础上，应用强化学习算法，专注于增强模型在推理密集型任务中的能力。\n拒绝采样与监督微调: 通过拒绝采样技术筛选高质量推理数据，并结合非推理数据进行监督微调，进一步提升模型推理能力和通用能力。\n面向所有场景的强化学习: 综合考虑推理和非推理任务，进行第二阶段强化学习，使模型与人类偏好对齐，提升在更广泛场景下的表现。\n冷启动 在 DeepSeek-R1 的训练流程中，冷启动阶段至关重要，它如同引擎的点火器，为后续复杂的强化学习过程奠定坚实基础。监督微调是冷启动阶段的核心技术。\n训练目标 冷启动阶段的目标明确而关键：利用高质量的 CoT 数据，对 DeepSeek-V3-Base 基模型进行初步微调。这次微调旨在快速赋予模型以下核心能力：\n初步推理能力： 引导模型学习模仿人类的推理过程，为更复杂的推理打下基础。 良好文本生成质量： 确保模型输出文本的流畅性和自然度，提升用户体验。 这些 CoT 数据如同模型的“启动燃料”，帮助模型快速掌握人类的推理模式，并为后续强化学习提供良好的策略初始化，有效避免 RL 训练初期从零开始探索的低效和不稳定性。\n数据构建 为了构建高质量的冷启动数据，DeepSeek 团队进行了多方探索，最终整合了以下高效方法：\nFew-shot Prompting： 利用少量高质量的示例，引导模型生成更长、更具深度和逻辑性的 CoT 数据。 模型生成 + 反思验证： 直接 Prompt 模型生成答案，并加入反思和验证环节，确保答案的质量和推理的正确性。 优化 R1-Zero 输出： 收集 DeepSeek-R1-Zero 模型的输出，通过人工标注和优化，提升数据的可读性和整体质量。 通过上述策略，DeepSeek 团队积累了数千条高质量的冷启动数据，并以此为基础对 DeepSeek-V3-Base 进行了微调，作为强化学习的坚实起点。\n冷启动优点 相比于直接以 DeepSeek-R1-Zero 作为起点，冷启动数据带来了多项显著优势，为后续训练奠定了更优的基础：\n显著提升可读性：\nDeepSeek-R1-Zero 的输出存在可读性挑战，例如语言混合、缺乏结构化格式等。 冷启动数据特别设计了更易读的输出模式，包括： 添加摘要： 在回复末尾添加精炼的摘要，快速提炼核心结论。 过滤不良回复： 去除不友好或低质量的回复，确保数据纯净度。 结构化输出格式： 采用 | special_token | \u0026lt;reasoning_process\u0026gt; | special_token | \u0026lt;summary\u0026gt; 格式，清晰呈现推理过程和总结。 性能显著提升：\n通过精心设计融入人类先验知识的数据模式，DeepSeek 团队观察到模型性能相较 R1-Zero 有了显著提升。 这进一步验证了迭代训练是提升推理模型性能的有效路径。 更优的策略初始化：\n冷启动阶段的 SFT 核心在于策略初始化。 策略初始化是构建 Reasoing LLM，例如 OpenAI o1 系列的关键步骤。通过学习高质量 CoT 数据，模型初步掌握人类推理模式，并具备生成结构化推理过程的能力，为后续强化学习训练奠定坚实基础，避免了从零开始探索的困境。 监督微调 监督微调 (Supervised Fine-tuning, SFT) 的核心目标是通过在有监督标注的数据上微调模型，使其预测结果尽可能接近真实标签。 这旨在提升模型在特定任务和指令执行方面的能力。\n损失函数 SFT 的训练目标是最小化模型预测与真实标签之间的差异。损失函数通常采用交叉熵损失 (Cross-Entropy Loss)，也称为负对数似然 (Negative Log-Likelihood)，用于衡量模型预测 token 分布与真实 token 分布之间的差异。 为了平衡不同长度输出序列的贡献，我们通常会将损失函数归一化到每个 token 的平均损失。\n损失函数公式如下：\n\\[ \\mathcal{L}_{SFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\log \\pi_\\theta\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]参数说明:\n\\(\\mathcal{L}_{SFT}(\\theta)\\)：SFT 损失函数，通过调整模型参数 \\(\\theta\\) 最小化该函数。 \\(\\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}[\\cdot]\\)：在 SFT 数据集分布 \\(P_{sft}(Q, O)\\) 上的期望。 \\(P_{sft}(Q, O)\\)：SFT 数据集分布，\\(q\\) 代表问题 (Query)，\\(o\\) 代表对应的标准答案输出 (Output)。 \\((q, o)\\)：从 SFT 数据集中采样的 问题-答案对。 \\(|o|\\)：标准答案输出的 token 长度。 \\(o_t\\)：标准答案输出的第 \\(t\\) 个 token。 \\(o_{\u0026lt;t} \\)：标准答案输出的前 \\(t-1\\) 个 tokens。 \\(\\pi_\\theta\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\)：给定问题 \\(q\\) 和前文 \\(o_{\u0026lt;t} \\)，模型预测 token \\(o_t\\) 的概率。 \\(\\frac{1}{|o|}\\): 长度归一化因子，将总损失除以输出序列长度，得到每个 token 的平均损失。 SFT 损失函数旨在惩罚模型预测与标准答案之间的偏差。 对于给定的问题 \\(q\\) 和标准答案 \\(o\\)，损失函数计算模型预测答案 \\(o\\) 中每个 token \\(o_t\\) 的概率 \\(\\pi_\\theta(o_t | q, o_{\u0026lt;t} )\\)。通过除以输出长度 \\(|o|\\)，损失函数被归一化为每个 token 的平均负对数似然。\n模型准确预测标准答案 token 时，\\(\\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\approx 1\\)，\\(\\log \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\approx 0\\)，损失值接近最小值。 模型预测偏离标准答案时，\\(\\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\) 较小，\\(\\log \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\) 为负数且绝对值较大，损失值增大。 最小化 SFT 损失函数的过程，就是让模型学习生成与训练数据集中标准答案尽可能相似文本的过程。从负对数似然角度看，目标是找到最优模型参数 \\(\\theta\\)，最大化模型生成训练数据答案 \\(o\\) 的概率，等价于最小化生成答案 \\(o\\) 的负对数似然。高质量的 CoT 数据蕴含人类对推理和结果的偏好，因此 SFT 也可视为让模型学习并拟合人类推理偏好的过程。\n梯度 SFT 损失函数的梯度用于指导模型参数更新，以降低损失值。 损失函数关于模型参数 \\(\\theta\\) 的梯度为：\n\\[ \\nabla_{\\theta} \\mathcal{L}_{SFT} = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]参数说明:\n\\(\\nabla_{\\theta} \\mathcal{L}_{SFT}\\)：SFT 损失函数关于参数 \\(\\theta\\) 的梯度，指示损失函数值下降最快的方向。 \\(\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\): token 概率对数 \\(\\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\) 关于参数 \\(\\theta\\) 的梯度。 \\(\\frac{1}{|o|}\\): 长度归一化因子，与损失函数保持一致，梯度也是每个 token 平均损失的梯度。 实际计算梯度时，通常使用随机梯度下降算法，沿着梯度下降方向更新模型参数，逐步最小化损失函数，提升模型生成标准答案的准确性。\n梯度系数\n在 SFT 阶段，梯度系数通常设置为 1，这意味着所有训练样本对模型参数的更新贡献相同，模型平等地学习每个示例，力求最小化在整个数据集上的平均损失。\n数据来源与人类偏好 数据来源：SFT 数据集主要由高质量的长链思维 (CoT) 示例构成，代表了期望模型学习的“标准答案”，用于指导损失函数最小化。 数据可能来自人工标注或更强大的模型生成。可参考 Open-o1 项目的 SFT 数据集 OpenO1-SFT，包含长 CoT 回复。 人类偏好：在 SFT 阶段，人类选择可以被视为隐式的奖励函数。 高质量 CoT 数据体现了人类对模型推理和输出的期望，模型通过学习这些数据，最小化与人类期望输出的偏差，从而拟合人类偏好。 面向推理的强化学习 在冷启动微调后，DeepSeek 团队通过强化学习 (RL) 进一步提升模型在推理密集型任务（如编码、数学、科学和逻辑推理）中的能力。 此阶段的核心在于最大化奖励函数，引导模型学习更有效的推理策略。\n奖励函数 为了解决推理过程中 CoT 语言混合问题，DeepSeek 团队引入了语言一致性奖励，并将其与任务奖励结合，构成总奖励函数：\n\\[ r(o) = r_{\\text{task}}(o) + \\alpha \\cdot r_{\\text{lang_consistency}}(o) \\]参数说明:\n\\(r(o)\\)：总奖励函数，RL 训练的目标是最大化该函数。 \\(r_{\\text{task}}(o)\\)：基于任务完成情况的任务奖励，衡量模型推理的准确性。 \\(r_{\\text{lang_consistency}}(o)\\)：语言一致性奖励，衡量 CoT 输出的语言纯度。 \\(\\alpha\\)：超参数，平衡任务奖励和语言一致性奖励的权重。 总奖励函数是任务奖励和语言一致性奖励的加权和。最大化 \\(r(o)\\) 驱动模型在提升推理准确性的同时，保持 CoT 输出的语言一致性。 \\(\\alpha\\) 的作用是调整模型对语言一致性的重视程度。\n训练目标 通过最大化上述奖励函数，DeepSeek 团队在冷启动微调后的模型上进行 RL 训练，优化模型参数，使其在推理任务上获得更高的奖励值，最终提升推理能力。\nRFT 拒绝采样微调 (Rejection Sampling Fine-tuning, RFT) 旨在通过精炼训练数据提升模型通用能力。其核心思想是最小化选择性损失函数，引导模型学习高质量输出的生成模式。\n损失函数 RFT 采用拒绝采样策略，区分推理数据和非推理数据的生成与选择过程，构建高质量 SFT 数据集。训练目标是最小化以下损失函数：\n\\[ \\mathcal{L}_{RFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q) \\times \\pi_{sft}(O \\mid q)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\mathbb{I}(o) \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]其中，指示函数 \\(\\mathbb{I}(o)\\) 定义为：\n\\[ \\mathbb{I}(o) = \\begin{cases} 1, \u0026 \\text{如果输出 } o \\text{ 被判定为高质量} \\\\ 0, \u0026 \\text{否则} \\end{cases} \\]参数说明:\n\\(\\mathcal{L}_{RFT}(\\theta)\\): RFT 损失函数。 \\(P_{sft}(Q)\\): 问题 \\(q\\) 的分布。 \\(\\pi_{sft}(O \\mid q)\\): 给定问题 \\(q\\)，SFT 模型生成输出 \\(O\\) 的条件概率分布。 \\(\\mathbb{I}(o)\\): 指示函数，用于选择高质量答案。当输出 \\(o\\) 被判定为高质量时为 1，否则为 0。 RFT 损失函数基于交叉熵损失，通过指示函数 \\(\\mathbb{I}(o)\\) 选择性地学习高质量输出：\n高质量输出 (\\(\\mathbb{I}(o) = 1\\)): 损失函数退化为标准交叉熵损失，模型根据高质量答案的负对数似然更新参数，最小化模型预测与高质量答案的差异。 低质量输出 (\\(\\mathbb{I}(o) = 0\\)): 损失函数为零，低质量答案不参与参数更新。 RFT 通过最小化损失函数，引导模型专注于学习高质量答案的生成模式，实现选择性学习。\n数据生成 高质量数据 (推理数据): 通过 RL 模型生成候选答案，使用奖励模型（或 DeepSeek-V3 模型）评分，拒绝采样保留高分答案。 SFT 数据 (非推理数据): 复用 DeepSeek-V3 的 SFT 数据集及其生成流程。 训练过程 使用拒绝采样得到的高质量数据集，对 DeepSeek-V3-Base 模型进行监督微调，最小化 RFT 损失函数，提升模型推理和通用能力。\nRFT 迭代精炼数据和重训练模型，期望模型在每轮迭代学习更高质量数据模式，最终收敛到高质量输出模型。 迭代过程中，训练数据分布 \\(P_{sft}(Q, O)\\) 逐渐聚焦于高质量数据，使模型在损失最小化过程中不断提升生成高质量输出的能力。\nOnRFT 在线拒绝采样微调 (Online Rejection Sampling Fine-tuning, OnRFT) 目标与 RFT 类似，都是通过最小化选择性损失函数学习高质量输出模式。OnRFT 与 RFT 的主要区别在于数据采样方式，损失函数形式与 RFT 保持一致。OnRFT 的损失函数梯度为：\n\\[ \\nabla_{\\theta} \\mathcal{L}_{OnRFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q) \\times \\pi_{\\theta}(O \\mid q)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\mathbb{I}(o) \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]参数说明:\n\\(\\nabla_{\\theta} \\mathcal{L}_{OnRFT}\\): OnRFT 损失函数关于模型参数 \\(\\theta\\) 的梯度，指示损失函数下降方向。 \\(\\pi_{\\theta}(O \\mid q)\\): 给定问题 \\(q\\)，当前训练模型 生成输出 \\(O\\) 的条件概率分布。 RFT 与 OnRFT 对比 下面表格简单对比了 RFT 和OnRFT 的主要区别。\n特性 RFT OnRFT 数据生成方式 离线 (Offline) 在线 (Online) 数据生成模型 SFT 模型 \\(\\pi_{sft}\\) 当前训练模型 \\(\\pi_{\\theta}\\) 拒绝采样数据来源 预生成 SFT 数据集 训练时实时生成数据 数据循环 分离 在线循环 损失函数机制 选择性交叉熵损失，选择高质量输出学习 选择性交叉熵损失，选择高质量输出学习 训练数据分布变化 逐渐聚焦于高质量数据 动态变化，贴合当前模型能力 面向所有场景的强化学习 为了进一步对齐人类偏好，DeepSeek 团队进行了第二阶段 RL，目标是在最大化奖励函数的同时，提升模型的有用性 (Helpfulness) 和 无害性 (Harmlessness)，并兼顾推理能力。此阶段仍然是通过最大化奖励函数来指导模型训练，但奖励函数的设计更加复杂，以反映多维度的优化目标。\n此阶段的 RL 训练结合了：\n多样化的 Prompt 分布: 覆盖更广泛的场景，包括推理和通用任务。 多目标奖励信号: 推理数据: 沿用基于规则的任务奖励，侧重推理准确性。最大化任务奖励，引导模型最小化推理错误。 通用数据: 使用奖励模型捕捉人类对有用性和无害性的偏好。奖励模型的目标是学习人类偏好，并输出与人类偏好一致的奖励信号，RL 训练的目标是最大化奖励模型给出的奖励值，从而间接最小化模型输出与人类偏好之间的偏差。 蒸馏 为了将 DeepSeek-R1 的强大推理能力迁移到更高效的小型模型上，DeepSeek 团队采用了蒸馏（Distillation）（Hinton et al. 2015）技术。蒸馏过程主要包括以下步骤：\n数据生成: 利用训练好的 DeepSeek-R1 模型，生成约 80 万条高质量的推理数据。这些数据不仅包括推理密集型任务 (如数学题、编程题)，也涵盖了通用任务 (如问答、对话)，以保证蒸馏数据的多样性和覆盖面。\n模型微调: 将生成的 80 万条高质量推理数据，用于微调小型密集模型。蒸馏实验选择了 Qwen 和 Llama 系列模型作为 Student 模型，涵盖了从 1.5B 到 70B 参数的多种模型规模，以探索蒸馏技术在不同模型规模下的效果。选取的 Student 模型包括 Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, 和 Llama-3.3-70B-Instruct。\n性能评估: 在多个推理相关的 benchmark 中，对蒸馏后的模型进行全面的性能评估。评估结果旨在验证蒸馏技术是否能够有效地将大型模型的推理能力迁移到小型模型，并考察蒸馏后的小型模型在推理能力上是否能够达到甚至超越大型模型的水平。\nKL 散度蒸馏 除了直接使用 Teacher 模型生成的文本输出作为伪标签进行 SFT 蒸馏外，更严谨的方法是将 Teacher 模型生成的 token 概率分布 \\(\\pi_{\\text{teacher}}\\) 也纳入考虑。KL 散度蒸馏 是一种常用的方法，它不仅让 Student 模型学习 Teacher 模型的文本输出，也学习 Teacher 模型的 token 概率分布。通过最小化 Student 模型和 Teacher 模型输出概率分布之间的 KL 散度，可以更充分地将 Teacher 模型的知识迁移到 Student 模型中。但在实际工程中，直接使用 Teacher 模型的文本输出作为伪标签进行 SFT 蒸馏，通常也能取得足够好的效果，并且实现更简单。\n实验结果 实验结果如图7所示：\nFig. 7. Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. (Image source: DeepSeek-AI, 2025)\n实验结果表明，这种直接的 SFT 蒸馏方法能够显著提升小型模型的推理能力。值得注意的是，在除 CodeForces 之外的多个基准测试中，蒸馏后的 Llama-3.3-70B-Instruct 模型表现已超越 OpenAI-o1-mini。仅依靠 SFT 蒸馏更大的基座模型就能获得如此显著的提升，充分展现了这一方法在后续研究与应用中的潜力。\n讨论 DeepSeek-R1 在多阶段训练框架基础上，探索了 Reasoning Model 训练技术的简化路径，主要包括以下几点:\n线性化思维过程：CoT 替代 MCTS\n传统强化学习 AI，如围棋和象棋，曾依赖蒙特卡洛树搜索 (Monte Carlo Tree Search, MCTS)。DeepSeek-R1 等模型则探索使用自回归的链式思维方法简化推理过程，逐步摒弃了计算复杂度高的 MCTS。 CoT 将复杂推理分解为线性步骤，模型像解题一样逐步推理，而非 MCTS 的穷举式搜索。这种线性化思维降低了计算复杂度，更符合人类思维习惯，使模型更易学习复杂推理策略。 消除独立价值网络：简化 RL 架构\n传统强化学习 (如 PPO) 通常需要独立的策略网络和价值网络。DeepSeek-R1 等研究发现，强化的策略网络或简化的价值评估方法 (如 GRPO 的组相对评分) 可替代独立价值网络。 这简化了 RL 训练架构，降低了资源需求，提高了效率。表明大语言模型的策略网络已具备强大的价值评估能力，无需额外价值网络。 聚焦最终结果奖励：最小化奖励信号\nDeepSeek-R1 采用更加简单的 ORM 奖励策略，主要关注最终结果的准确性奖励，弱化中间推理步骤奖励。这种策略受 AlphaZero (Silver et al. 2017) 启发，后者仅关注胜负。 对于 Reasoning Model，最终结果奖励可能比 PRM 更有效，能帮助模型更自然地学习“思维方式”，减少繁琐的逐步监督。 增加思考时间：模型自发涌现深度思考\nDeepSeek-R1-Zero 训练中展现出自发 增加思考时间 的能力。模型随训练深入，根据问题复杂度自适应分配更多“思考时间”，生成更长推理序列。这种“思考时间”增加是模型 RL 训练中自发涌现的行为。 思考时间增加反映模型更深入探索和优化思维过程。复杂问题需要更多推理步骤才能找到答案。DeepSeek-R1-Zero 的自我进化能力印证了强化学习在提升模型推理能力方面的潜力。 总结 DeepSeek-R1 的成功展示了 RL 提升 LLM 推理能力的巨大潜力。DeepSeek-R1 采用的 GRPO 算法在计算效率、优化稳定性、奖励鲁棒性等方面优于 PPO 和 DPO，并通过简化模型架构降低了训练资源消耗。DeepSeek-R1 为开源 Reasoning Model 复现 o1 提供了一条值得参考的路径。\n参考文献 [1] Guo, Daya, et al. \u0026ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\u0026rdquo; arXiv preprint arXiv:2501.12948 (2025).\n[2] OpenAI. \u0026ldquo;Introducing OpenAI o1\u0026rdquo; OpenAI, 2024.\n[3] Jaech, Aaron, et al. \u0026ldquo;OpenAI o1 system card.\u0026rdquo; arXiv preprint arXiv:2412.16720 (2024).\n[4] Open-r1. HuggingFace, 2024.\n[5] Sutton, Richard. \u0026ldquo;The bitter lesson.\u0026rdquo; Incomplete Ideas (blog) 13.1 (2019): 38.\n[6] Liu A, et al. \u0026ldquo;Deepseek-v3 technical report.\u0026rdquo; arXiv preprint arXiv:2412.19437 (2024).\n[7] Schulman, John, et al. \u0026ldquo;Proximal policy optimization algorithms.\u0026rdquo; arXiv preprint arXiv:1707.06347 (2017).\n[8] Ouyang, Long, et al. \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in neural information processing systems 35 (2022): 27730-27744.\n[9] Shao, Zhihong, et al. \u0026ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\u0026rdquo; arXiv preprint arXiv:2402.03300 (2024).\n[10] J. Schulman. Approximating kl divergence, 2020.\n[11] Gao, Leo, John Schulman, and Jacob Hilton. \u0026ldquo;Scaling laws for reward model overoptimization.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n[12] Chen, Mark, et al. \u0026ldquo;Evaluating large language models trained on code.\u0026rdquo; arXiv preprint arXiv:2107.03374 (2021).\n[13] Learning to Reason with LLMs. OpenAI, 2024.\n[14] AMC. Mathematical Association of America (MAA), 2024.\n[15] Open-O1. Open-Source O1, 2024.\n[16] Zeng, Zhiyuan, et al. \u0026ldquo;Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective.\u0026rdquo; arXiv preprint arXiv:2412.14135 (2024).\n[17] Hinton, Geoffrey. \u0026ldquo;Distilling the Knowledge in a Neural Network.\u0026rdquo; arXiv preprint arXiv:1503.02531 (2015).\n[18] Silver, David, et al. \u0026ldquo;Mastering chess and shogi by self-play with a general reinforcement learning algorithm.\u0026rdquo; arXiv preprint arXiv:1712.01815 (2017).\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Jan 2025). OpenAI o1复现进展：DeepSeek-R1. https://syhya.github.io/posts/2025-01-27-deepseek-r1\nOr\n@article{syhya2025deepseekr1, title = \u0026#34;OpenAI o1复现进展：DeepSeek-R1\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-27-deepseek-r1\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-01-27-deepseek-r1/","summary":"\u003cp\u003eDeepSeek AI 近期发布 \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/2501.12948\"\u003eDeepSeek-AI, 2025\u003c/a\u003e)，其推理性能在多个 benchmark 上已接近 OpenAI o1 (\u003ca href=\"https://openai.com/o1/\"\u003eOpenAI, 2024\u003c/a\u003e)的水平，是开源社区成功复现 o1 的重要一步。R1 相关代码可以参考huggingface 尝试开源复现 \u003ca href=\"https://github.com/huggingface/open-r1\"\u003eopen-r1\u003c/a\u003e 项目。以往的研究多依赖于海量的监督数据来提升大语言模型（Large Language Model, LLM）性能，但 DeepSeek-R1 及其早期实验 DeepSeek-R1-Zero 的成功，有力证明了纯粹大规模强化学习在提升 LLM 推理能力方面的潜力。其印证了 Richard Sutton 在 “The Bitter Lesson” 中提出的深刻见解:\u003c/p\u003e","title":"OpenAI o1复现进展：DeepSeek-R1"},{"content":" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。\n引言 本篇文章将系统梳理LLaMA系列模型从LLaMA1到LLaMA3的发展历程，深入解析其模型架构、训练数据和训练方法，并通过对比表格揭示各版本的核心差异。\nLLaMA模型演进 大语言模型（LLM）在近年来取得了重大进展，Meta 在 2023 年相继推出了多代 LLaMA 模型系列，每一代在模型规模、训练数据以及应用场景上都有新的提升。本节将依次介绍 LLaMA1、LLaMA2、Code Llama、Llama Guard 和 LLaMA3 模型的主要特性与技术创新。\nLLaMA1 概述 LLaMA1（Touvron et al., 2023）于 2023 年 2 月发布，作为当时性能优异的开源模型，在学术界和工业界迅速引起广泛关注。\n特点 参数规模：提供 7B、13B、30B 和 65B 四个版本。 训练数据量：超过 1.4 万亿个 token。 训练资源：以 65B 模型为例，在 2048 张 A100 80GB GPU 上训练约 21 天。 性能优势：在大多数基准测试中，65B 模型超越了当时流行的 175B 参数 GPT-3。 技术细节 Pre-normalization \u0026amp; RMSNorm：采用预归一化（Pre-normalization）方案，使用 RMSNorm 替换传统的 LayerNorm，训练更稳定且速度更快。 FFN_SWiGLU：在前馈网络中引入 SWiGLU 结构，激活函数由 ReLU 更换为 SiLU，并对隐藏单元数量进行优化配置。 Rotary Embeddings (RoPE)：在每一层动态注入旋转位置嵌入信息，有利于长序列的建模能力。 应用场景 研究与开发：作为通用大模型的基础，用于各种 NLP 任务研究。 商业应用：可在客户服务、内容生成等场景中提升自动化水平。 教育与培训：为学术机构和教育平台提供新的教学和实验工具。 LLaMA2 概述 LLaMA2（Touvron et al., 2023）是 LLaMA 系列的第二代版本，于 2023 年中发布，相较于第一代在规模和性能上均有大幅提升。\n特点 参数规模：覆盖 7B、13B、34B、70B 四个不同规模。 训练数据量：从 1.4 万亿 token 扩展到 2 万亿 token，增幅约 40%。 上下文长度：支持 4096 个 token，是 LLaMA1 的两倍。 注意力机制：引入分组查询注意力（Group Query Attention, GQA）以提高推理效率和内存利用率。 技术细节 GQA（Group Query Attention）：对查询进行分组，可减少自注意力计算的开销，降低显存占用。 KV Cache：推理阶段采用 KV 缓存，提升解码速度，缩短推理时延。 应用场景 对话系统：更自然、准确地生成对话回复，改善用户交互体验。 内容生成：可应用于新闻、营销文案等高质量文本的自动化生成。 Code Llama Fig. 1. The Code Llama specialization pipeline. (Image source: [Rozière et al., 2023)\n概述 Code Llama（Rozière et al., 2023）基于 LLaMA2 进行额外训练与微调，专门面向代码生成、补全以及指令跟随任务，涵盖多种编程语言。\n特点 多种参数规模：提供 7B、13B、34B、70B 四类版本，可根据算力和应用需求选择。 训练规模： 7B、13B、34B 三个型号在约 5000 亿（500B）标记的代码数据基础上训练； 70B 型号在约 1 万亿（1T）标记的同源数据上训练。 支持长上下文：通过“长上下文微调”（LCFT）过程，能稳定处理多达 16k 乃至 100k tokens 的大型代码文件。 技术细节 架构继承：延续 LLaMA2 的 Transformer 架构，并针对代码领域做专门的目标任务优化。 数据构成：主要来源于公开可用的开源代码库，辅以少量与代码相关的自然语言内容，以保持通用理解能力。 填充中间（Fill-in-the-Middle, FIM）：7B、13B、70B 参数的基础模型直接支持在已有代码任意位置插入补全，方便 IDE 中的实时代码片段生成。 长上下文微调（LCFT）：在已训练的模型上使用更长序列（16k tokens）再次微调，重置 RoPE 参数，使模型在处理超过训练长度的输入时更稳定。 指令微调（Instruct Fine Tuning）：结合 Llama 2 安全指令数据与自监督生成的单元测试筛选数据，以提升模型对自然语言指令的理解和合规性。 应用场景 开发者工具：集成在 IDE 中用于智能补全、调试建议与文档注释，提升开发效率。 教育/培训：为初学者或教学平台提供示例代码、解题思路和习题解析。 商业化软件：与版本控制、CI/CD 等平台集成，为企业级开发提供自动化支持。 研究探索：在自动化测试、代码生成等领域带来新的算法与应用思路。 Llama Guard Fig. 2. Example task instructions for the Llama Guard prompt and response classification tasks. (Image source: Inan et al., 2023)\nFig. 3. Llama Guard 3 Vision classifies harmful content in the response classification task. (Image source: Chi et al., 2024)\n概述 Llama Guard（Inan et al., 2023）是 Meta 为 LLaMA2 及后续版本（如 LLaMA3）开发的安全增强模块，主要面向内容安全的评估与过滤，确保模型输出符合相关安全标准。\n特点 版本： Llama Guard 3 1B：面向基础文本内容的安全评估。 Llama Guard 3 8B：可处理更复杂的文本安全场景，专注于代码解释器滥用（S14）检测。 Llama Guard 3 Vision（Chi et al., 2024）：增强多模态处理能力，支持图像和文本的综合安全评估。 技术细节 多模态评估：通过特殊 \u0026lt;|image|\u0026gt; token 将图像信息与文本输入相结合，进行统一的安全审查。 安全类别：基于 ML Commons consortium 定义的 13 个安全类别（S1-S13），在 3.2 版本中新增针对“代码解释器滥用（S14）”的安全检测。 评估流程：用户将安全类别和对话内容作为输入提示，模型给出判定结果（安全或不安全）及违规类别。 应用场景 内容审核：自动化检测并过滤违反平台或法律规定的内容。 安全监控：在生产环境中实时监控信息流，防范有害或敏感内容传播。 多模态审核：对含图文混合的输入执行更加全面的安全审查。 LLaMA3 Fig. 4. The architecute of Llama 2 and 3. (Image source: Umar Jamil)\n概述 LLaMA3（Grattafiori et al., 2024）是 LLaMA 系列的第三代模型，在多语言、多模态、以及边缘设备部署方面均有提升，拥有从 1B 到 405B 等多种规模。\n特点 参数规模：1B、3B、11B、70B、90B 和 405B 六种版本，覆盖从轻量级到超大规模的多种需求。 训练数据量：累计 15 万亿 token，约为 LLaMA2 的 7.5 倍。 Tokenizer 更新：采用更高效的 tiktoken，词表从 32k 扩大至 128k。 上下文长度：可处理多达 128k tokens 的上下文。 多语言支持：覆盖 8 种语言，全面升级在跨语言环境下的适配能力。 多模态支持：11B 与 90B 版本提供视觉语言模型，可处理与图像结合的任务。 轻量级版本：1B 与 3B 通过剪枝和知识蒸馏技术，适合边缘与移动端部署。 技术细节 全面采用 GQA（Grouped Query Attention）：优化自注意力计算效率与内存使用。 训练方法多样化：结合监督微调(SFT)、拒绝采样(RS)、直接策略优化(DPO)等，以进一步提升模型推理与编码能力。 多模态模型：同时支持图像、视频和语音的多模态处理。 应用场景 高级对话系统：面对更广泛、更复杂的对话需求，提供自然、上下文一致的回复。 跨语言场景：为全球化应用提供多语言支持，覆盖更多人群和市场。 多模态任务：在图像理解、视觉问答等场景中发挥出色的多模态生成与推理能力。 边缘计算：1B 和 3B 版本可在算力有限的设备上运行，为 IoT 或移动端场景提供支持。 LLaMA 系列模型特性对比 特性 LLaMA1 LLaMA2 LLaMA3 发布时间 2023年2月 2023年7月 2024年4月 模型规模 7B、13B、30B、65B 7B、13B、34B、70B 1B、3B、11B、70B、90B、405B 训练数据量 1.4 万亿+ tokens 2 万亿 +tokens 15 万亿+ tokens 上下文长度 2048 tokens 4096 tokens 128k tokens Tokenizer SentencePiece BPE，32k 词汇表 SentencePiece BPE，32k 词汇表 tiktoken BPE，128k 词汇表 位置编码 RoPE RoPE RoPE 注意力机制和推理优化 Multi-Head Attention (MHA) Grouped Query Attention (GQA)+ kv cache Grouped Query Attention (GQA) + kv cache 归一化方法 RMSNorm RMSNorm RMSNorm 激活函数 SwiGLU SwiGLU SwiGLU 训练资源 2048 * A100 80GB 3.3M GPU hours on A100-80GB 16K H100 80GB 应用场景 通用语言理解与生成 通用语言理解与生成，推理效率进一步提升 多模态应用（图像、语音）、轻量级部署、边缘设备适配 关键技术解析 LLaMA3作为系列最新版本，集成了LLaMA1和LLaMA2的核心技术，并在此基础上进行了多项创新和优化。以下是LLaMA3所采用的所有关键技术的详细解析，包括数学公式和相关说明。\nRMS Normalization 在深度学习中，归一化技术在加速训练、提升模型性能和稳定性方面起着至关重要的作用。RMS Normalization (Zhang, et al., 2019) 是一种简化的归一化方法，通过仅计算输入向量的均方根（RMS）进行归一化，从而减少计算开销。其数学表达式如下：\n$$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$其中：\n\\( x \\) 为输入向量。 \\( d \\) 为特征维度的大小。 \\( \\epsilon \\) 为一个极小的常数，用于防止分母为零。 \\( \\gamma \\) 为可学习的缩放参数。 LLaMA3 中选择 RMSNorm 作为其归一化方法，主要基于以下考虑：\n计算效率：RMSNorm 相比 LayerNorm、BatchNorm 和WeightNorm 计算量更低，仅计算输入向量的均方根，适合 LLM 的高效训练。 训练稳定性：RMSNorm 在保持训练稳定性的同时，能够适应更大的学习率，促进模型的快速收敛。 资源优化：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。 简化实现：RMSNorm 的实现相对简单，便于在复杂模型中集成和优化。 关于各种 Norm 的对比和代码实现，可参考博客：Normalization in Deep Learning。\nFFN_SwiGLU Swish-Gated Linear Unit (Shazeer, 2020) 是 LLaMA 中用于增强前馈网络（Feed-Forward Network, FFN）非线性表达能力的关键技术。SwiGLU 结合了 Swish 激活函数和门控机制，显著提升了模型的表现力和性能。此外，与 PaLM (Chowdhery, 2022) 中使用的$4 d$隐藏维度不同，LLaMA 采用了 $\\frac{2}{3}d$ 的隐藏维度，从而在保持参数量和计算量不变的情况下，实现了更高的参数效率。\n$$ \\operatorname{FFN}_{\\mathrm{SwiGLU}}\\left(x, W_1, W_3, W_2\\right)=\\left(\\operatorname{Swish}\\left(x W_1\\right) \\otimes x W_3\\right) W_2 $$ 其中：\n\\( \\text{Swish}(x) = x \\cdot \\sigma(x) \\)（Swish 激活函数）。 \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)（Sigmoid 函数）。 \\( \\otimes \\) 表示逐元素相乘。 \\( W_1, W_2, W_3 \\) 为线性变换矩阵。 优势：\n增强非线性表达：SwiGLU 通过结合 Swish 激活函数与门控机制，能够更有效地捕捉复杂的模式和关系，提升 FFN 层的表达能力。 参数效率：采用 $\\frac{2}{3}d$ 的隐藏维度，在引入额外的线性变换矩阵的同时，保持了总参数量不变，实现了参数的高效利用。 性能提升：在多项基准测试中，FFN_SwiGLU 显著提升了模型的性能，尤其在处理复杂任务和长文本时表现尤为出色。例如，在文本生成和理解任务中，SwiGLU 帮助模型更好地理解上下文和长距离依赖关系。 实现细节：\n权重矩阵调整：为了保持与传统 FFN 层相同的参数量和计算量，SwiGLU 通过减少隐藏层的维度（例如，将隐藏层大小从 4d 调整为 $\\frac{2}{3}d$），在引入额外的线性变换矩阵的同时，确保整体模型的效率不受影响。 兼容性：SwiGLU 作为 GLU 家族的一员，能够无缝集成到现有的 Transformer 架构中，替代传统的 ReLU 或 GELU 激活函数，提升模型的整体性能。 实现代码可以参考这个文件：swiglu.py\nRotary Positional Embeddings (RoPE) Rotary Positional Embeddings (RoPE) 是LLaMA3中用于表示序列中位置关系的技术，通过对Query和Key向量应用旋转变换，增强了模型对相对位置信息的感知能力。\n优势：\n相对位置感知：RoPE能够自然地捕捉词汇之间的相对位置关系，提升了长距离依赖的建模效果。 计算效率高：无需额外的计算，位置编码与词向量的结合在计算上是高效的，适用于大规模模型。 适应不同长度的序列：RoPE可以灵活处理不同长度的输入序列，不受固定位置编码的限制。 兼容线性注意力：RoPE 可以与线性注意力机制结合，保持注意力计算的线性复杂度，进一步提升处理长序列的效率。 1. 背景简介 理解单词在序列中的位置关系对于成功训练大型语言模型（LLM）至关重要。循环神经网络（RNN）通过递归计算隐藏状态来自然地捕捉序列中的位置信息。然而，Transformer这类基于自注意力机制的模型由于其并行计算的特性，无法直接感知单词之间的相对位置关系，因此需要额外的位置编码来提供这一信息。\n位置编码的方法主要分为绝对位置编码和相对位置编码两大类。RoPE 则是一种创新性的绝对位置编码方法，旨在结合绝对位置编码和相对位置编码的优点，通过旋转变换实现相对位置感知。\n2. 位置编码的基本概念 2.1 绝对位置编码 绝对位置编码通过为每个位置生成一个固定或可训练的位置向量，并将其与词向量相加，从而为模型提供位置信息。常见的绝对位置编码方法包括：\n三角函数位置编码：例如，Vaswani 等人（2017）提出的使用正弦和余弦函数生成的位置编码。\n$$ p_i = \\left[\\sin\\left(\\frac{i}{10000^{2j/d}}\\right), \\cos\\left(\\frac{i}{10000^{2j/d}}\\right)\\right]_{j=1}^{d/2} $$ 其中，\\( p_i \\) 是位置 \\( i \\) 的位置编码向量，\\( d \\) 是词向量的维度。\n优点：\n实现简单，与词向量的结合方式直接。 缺点：\n无法自然地捕捉词汇之间的相对位置关系，限制了模型对长距离依赖的建模能力。 对于不同长度的序列，可能需要重新生成位置编码。 可训练位置编码：如BERT和GPT中使用的可训练位置编码。\n2.2 相对位置编码 相对位置编码旨在让模型关注词汇之间的相对距离，而不是绝对位置。这样，模型可以更灵活地处理不同长度的序列，并更有效地捕捉长距离依赖关系。\n常见方法：\nGoogle式相对位置编码：在论文《Self-Attention with Relative Position Representations》中，Shaw 等人（2018）提出了一种扩展自注意力机制以考虑相对位置的方法。\n优点：\n自然地捕捉词汇之间的相对位置信息，有助于长距离依赖的建模。 提高模型对序列长度的灵活适应能力。 缺点：\n实现相对复杂，尤其是在自注意力机制中的集成。 计算效率相对较低，特别是在处理长序列时。 3. 旋转位置编码（RoPE）的原理与实现 3.1 RoPE 的设计思路 RoPE 的核心理念是通过旋转变换将绝对位置信息转化为相对位置信息，从而结合绝对位置编码和相对位置编码的优点。具体而言，RoPE 通过将查询（Query）和键（Key）向量分别乘以与其位置相关的旋转矩阵，使得内积计算中自然地体现了相对位置关系。\n3.2 数学表达式与推导 3.2.1 2 维情况下的 RoPE 推导 为了更深入理解 RoPE 在二维情况下的工作原理，以下通过数学推导展示其如何实现相对位置编码。\n引言\n在二维空间中，向量的旋转可以通过复数的乘法来简化理解。RoPE 利用这一性质，通过对查询（Query）和键（Key）向量施加旋转变换，实现相对位置编码。\n复数与二维向量的对应关系\n一个复数 \\( z = a + ib \\) 可以表示为二维向量 \\( \\mathbf{v} = [a, b]^T \\)。 复数的乘法对应于二维向量的旋转和缩放。 $$ e^{i\\theta} = \\cos\\theta + i\\sin\\theta $$ 可以将复数旋转表示为二维向量的旋转矩阵。\nRoPE 的基本操作\n假设有二维的查询向量 \\( \\mathbf{q}_m \\) 和键向量 \\( \\mathbf{k}_n \\)，分别位于位置 \\( m \\) 和 \\( n \\)。RoPE 的目标是通过旋转变换，使得它们的内积仅依赖于相对位置 \\( m - n \\)。\n步骤 1：表示为复数\n$$ \\mathbf{q}_m = q_m^{(1)} + i q_m^{(2)} \\\\ \\mathbf{k}_n = k_n^{(1)} + i k_n^{(2)} $$步骤 2：应用旋转变换\n$$ f_q(\\mathbf{q}_m, m) = \\mathbf{q}_m \\cdot e^{im\\theta} = (q_m^{(1)} + i q_m^{(2)}) (\\cos(m\\theta) + i\\sin(m\\theta)) \\\\ f_k(\\mathbf{k}_n, n) = \\mathbf{k}_n \\cdot e^{in\\theta} = (k_n^{(1)} + i k_n^{(2)}) (\\cos(n\\theta) + i\\sin(n\\theta)) $$ 其中，\\( \\theta \\) 是一个预先定义的常数，用于控制旋转的速度。\n步骤 3：计算内积\n$$ \\langle f_q(\\mathbf{q}_m, m), f_k(\\mathbf{k}_n, n) \\rangle = \\text{Re}\\left[ f_q(\\mathbf{q}_m, m) \\cdot \\overline{f_k(\\mathbf{k}_n, n)} \\right] $$ 其中，\\( \\overline{f_k(\\mathbf{k}_n, n)} \\) 是 \\( f_k \\) 的共轭复数。\n$$ \\langle f_q(\\mathbf{q}_m, m), f_k(\\mathbf{k}_n, n) \\rangle = (q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m-n)\\theta) + (q_m^{(1)}k_n^{(2)} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta) $$ 这个结果表明，内积的计算中自然地引入了相对位置 \\( m - n \\) 的影响。\n步骤 4：将二维向量与旋转矩阵对应\n将复数形式转化为矩阵形式：\n$$ f_q(\\mathbf{q}_m, m) = R(m\\theta) \\mathbf{q}_m \\\\ f_k(\\mathbf{k}_n, n) = R(n\\theta) \\mathbf{k}_n $$$$ R(\\phi) = \\begin{bmatrix} \\cos\\phi \u0026 -\\sin\\phi \\\\ \\sin\\phi \u0026 \\cos\\phi \\end{bmatrix} $$$$ \\langle f_q(\\mathbf{q}_m, m), f_k(\\mathbf{k}_n, n) \\rangle = [q_m^{(1)}, q_m^{(2)}] R((n - m)\\theta) \\mathbf{k}_n $$ 进一步展开： $$ = q_m^{(1)} k_n^{(1)} \\cos((m - n)\\theta) + q_m^{(2)} k_n^{(2)} \\cos((m - n)\\theta) \\\nq_m^{(1)} k_n^{(2)} \\sin((m - n)\\theta) - q_m^{(2)} k_n^{(1)} \\sin((m - n)\\theta) $$ 与前述结果一致，验证了 RoPE 在二维情况下实现相对位置编码的有效性。 总结二维 RoPE 的实现步骤：\n表示向量为复数：将二维的查询和键向量表示为复数形式。 应用旋转变换：根据各自的位置 \\( m \\) 和 \\( n \\)，分别将查询和键向量旋转 \\( m\\theta \\) 和 \\( n\\theta \\) 的角度。 计算内积的实部：通过复数的乘法和内积运算，确保内积结果仅依赖于相对位置 \\( m - n \\)。 利用旋转矩阵：通过旋转矩阵的性质，将复数形式转化为矩阵形式，使得旋转操作更加直观和易于扩展到高维情况。 3.2.2 高维情况下的 RoPE 对于词向量维度 \\( d \\) 为偶数的情况，RoPE 通过将向量拆分为 \\( d/2 \\) 个二维子向量，并对每个子向量应用独立的旋转矩阵，从而实现高维度的旋转位置编码。\n$$ f_{\\{q, k\\}}(x_m, m) = R_{\\Theta, m}^d \\cdot W_{\\{q, k\\}} \\cdot x_m $$ 其中，\\( R_{\\Theta, m}^d \\) 是一个块对角矩阵，由 \\( d/2 \\) 个二维旋转矩阵组成，每个旋转矩阵对应一个不同的角度 \\( \\theta_i = 10000^{-2(i-1)/d} \\)。\n$$ R_{\\Theta, m}^d = \\begin{bmatrix} \\cos(m\\theta_1) \u0026 -\\sin(m\\theta_1) \u0026 \u0026 \u0026 \\\\ \\sin(m\\theta_1) \u0026 \\cos(m\\theta_1) \u0026 \u0026 \u0026 \\\\ \u0026 \u0026 \\cos(m\\theta_2) \u0026 -\\sin(m\\theta_2) \u0026 \\\\ \u0026 \u0026 \\sin(m\\theta_2) \u0026 \\cos(m\\theta_2) \u0026 \\\\ \u0026 \u0026 \u0026 \u0026 \\ddots \\\\ \\end{bmatrix} $$$$ q_m^\\top k_n = (R_{\\Theta, m}^d W_q x_m)^\\top (R_{\\Theta, n}^d W_k x_n) = x_m^\\top W_q^\\top R_{\\Theta, m}^{d\\top} R_{\\Theta, n}^d W_k x_n = x_m^\\top W_q^\\top R_{\\Theta, n-m}^d W_k x_n $$ 其中，\\( R_{\\Theta, n-m}^d = R_{\\Theta, m}^{d\\top} R_{\\Theta, n}^d \\)，体现了相对位置信息。\n3.3 RoPE 的性质 3.3.1 远程衰减特性\n通过选择 \\( \\theta_i = 10000^{-2(i-1)/d} \\)，RoPE 的内积计算结果随着相对位置 \\( |m - n| \\) 增大而衰减。这一特性符合自然语言中的直觉，即距离较远的单词对当前词的影响应当较小。\n3.3.2 兼容线性注意力机制\nRoPE 的旋转操作保持了向量的范数不变，使其可以与线性注意力机制（如 Performer）无缝结合，进一步提升模型在处理长序列时的计算效率。\n4. 总结 RoPE 通过旋转变换将绝对位置信息转化为相对位置信息，结合了绝对位置编码和相对位置编码的优点。其核心机制确保了模型对相对位置关系的感知能力，并且保持了高效的计算性能，特别是在处理长序列任务时表现出色。通过详细的数学推导，特别是在二维情况下的实现，RoPE 展示了其在自注意力机制中引入相对位置编码的有效性和灵活性。\nGrouped Query Attention (GQA) Grouped Query Attention (GQA) (Ainslie, 2023) 是 LLaMA3 中用于优化自注意力计算的关键技术。在大规模语言模型的推理过程中，每个注意力头（head）拥有独立的键（Key）和值（Value）参数会导致巨大的内存消耗。Grouped Query Attention (GQA) 旨在通过将多个查询（Query）头分组，并让每组共享一组键值头，从而在模型性能与推理效率之间取得更优的平衡。GQA 是 Multi-Head Attention (MHA) 和 Multi-Query Attention (MQA) 之间的一种折中方案：\nMHA：每个注意力头都有独立的 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)。 MQA：所有注意力头共享一组 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)。 GQA：将 \\(H\\) 个查询头划分为 \\(G\\) 组，每组共享一组 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)（其中 \\(1 \u003c G \u003c H\\)）。 1. 投影 (Projections) 给定输入序列 \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\)，首先通过线性变换投影得到查询、键和值矩阵：\n$$ \\mathbf{Q} = \\mathbf{X} W_Q, \\quad \\mathbf{K} = \\mathbf{X} W_K, \\quad \\mathbf{V} = \\mathbf{X} W_V, $$其中，\\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) 为可学习的投影矩阵。\n2. 头与分组 (Heads and Grouping) 头的切分：将 \\(\\mathbf{Q}\\)、\\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\) 分割成 \\(H\\) 个头，每个头的向量维度为 \\(d_{\\text{head}} = \\frac{d}{H}\\)。 $$ \\mathbf{Q} = [\\mathbf{Q}_1; \\mathbf{Q}_2; \\dots; \\mathbf{Q}_H], \\quad \\mathbf{K} = [\\mathbf{K}_1; \\mathbf{K}_2; \\dots; \\mathbf{K}_H], \\quad \\mathbf{V} = [\\mathbf{V}_1; \\mathbf{V}_2; \\dots; \\mathbf{V}_H] $$ 分组：将这 \\(H\\) 个查询头进一步划分为 \\(G\\) 组（\\(1 \u003c G \u003c H\\)）。对于第 \\(g\\) 组，包含 \\(\\frac{H}{G}\\) 个查询头，并共享一组键值头 \\(\\mathbf{K}^g\\) 和 \\(\\mathbf{V}^g\\)。 $$ \\mathcal{G} = \\left\\{ \\mathcal{G}_1, \\mathcal{G}_2, \\dots, \\mathcal{G}_G \\right\\}, \\quad |\\mathcal{G}_g| = \\frac{H}{G} \\quad \\forall g \\in \\{1, 2, \\dots, G\\} $$下图展示了 GQA 与传统 MHA 和 MQA 的对比，可见在 GQA 中，每组查询头公用一组键值头。\nFig. 5. Overview of Grouped Query Attention (GQA). (Image source: Ainslie et al., 2023)\n3. 组内注意力 (Intra-Group Attention) 对于第 \\(g\\) 组，令该组的查询向量为 \\(\\{\\mathbf{Q}_i\\}_{i \\in \\mathcal{G}_g}\\)，共享的键值向量为 \\(\\mathbf{K}^g\\) 和 \\(\\mathbf{V}^g\\)。组内注意力的计算公式为：\n$$ \\text{Attention}_g(\\mathbf{Q}_i, \\mathbf{K}^g, \\mathbf{V}^g) = \\text{softmax}\\left( \\frac{\\mathbf{Q}_i (\\mathbf{K}^g)^\\top}{\\sqrt{d_{\\text{head}}}} \\right) \\mathbf{V}^g $$其中，\\(\\sqrt{d_{\\text{head}}}\\) 为缩放因子，用于稳定梯度和数值计算。\n4. 拼接输出 (Concatenate \u0026amp; Output) 将所有组的注意力结果在通道维度上拼接，得到矩阵 \\(\\mathbf{O}\\)，然后通过线性变换矩阵 \\(W_O \\in \\mathbb{R}^{d \\times d}\\) 得到最终输出：\n$$ \\mathbf{O} = \\text{Concat}\\left( \\text{Attention}_1, \\text{Attention}_2, \\dots, \\text{Attention}_G \\right) W_O $$其中，\\(\\text{Concat}\\) 表示在通道维度上的拼接操作。\n更多关于注意力机制在 MHA、MQA 和 GQA 之间的详细对比及代码示例，可参考博客：Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA。\nBPE tiktoken tokenizer是LLaMA3采用的新一代分词器，相较于LLaMA2使用的SentencePiece BPE，tiktoken在以下方面有所改进：\n词汇表扩展：词汇表从32k扩展至128k，覆盖更多语言和专业术语，减少了分词次数，提升了生成质量。 编码效率：优化了编码算法，减少了分词时间，提高了处理速度。 生成质量：通过更细粒度的词汇表示，提升了模型生成文本的连贯性和准确性。 $$ \\text{Tokenize}(w) = \\text{BPE}(w) \\quad \\text{vs} \\quad \\text{Tokenize}(w) = \\text{tiktoken\\_BPE}(w) $$ 其中，\\( w \\) 为输入词汇，tiktoken_BPE通过更大词汇表减少了分词次数。 优势：\n减少分词次数：更大的词汇表使得更多词汇能作为单一token处理，减少了分词次数，提高了生成效率和质量。 提升生成质量：更细粒度的词汇表示，使模型在生成文本时能够更准确地表达复杂语义。 编码速度快：优化的编码算法提升了分词速度，适用于大规模模型的高效训练和推理。 轻量级模型 为了适应边缘设备和移动设备的需求，LLaMA3推出了1B和3B参数量的轻量级模型，采用以下技术：\n剪枝技术：通过系统性地移除网络中的冗余参数，减小模型规模，同时保持核心性能。 知识蒸馏：让小模型从大模型中学习，提升其在特定任务上的表现。 优化部署：针对移动设备的硬件架构进行优化，如针对Arm处理器的性能调优，确保模型在资源受限环境中的高效运行。 $$ \\text{Pruned\\_Model} = \\text{Prune}(\\text{Original\\_Model}, \\text{Pruning\\_Rate}) $$$$ \\text{Distilled\\_Model} = \\text{Distill}(\\text{Large\\_Model}, \\text{Small\\_Model}) $$ 其中，Prune表示剪枝操作，Distill表示知识蒸馏过程。 优势：\n适应资源受限设备：减小模型规模，使其适用于边缘设备和移动设备，推动了大语言模型的普及。 保持性能：通过剪枝和知识蒸馏技术，保持了模型的核心性能和表现。 高效运行：优化的模型结构和权重格式（如BFloat16）提升了计算效率，确保在移动设备上的高效运行。 训练方法 LLaMA3在训练数据和方法上进行了全面升级，采用了更大规模的数据和更先进的训练技术：\n预训练阶段：\n大规模数据扩展：训练数据量达到15万亿token，覆盖更多语言、专业领域和多模态数据，提升了模型的泛化能力和多语言支持。 扩展法则（Scaling Laws）： 根据Chinchilla扩展法则，优化模型的训练数据量和参数规模平衡，确保模型在关键任务上的最佳性能。 数学表达式： $$ \\text{Optimal Data} \\propto \\text{Model Size}^{4/3} $$ 这一公式指导了数据和模型规模的平衡，确保随着模型规模的增加，训练数据量也按比例增长，避免模型过拟合或欠拟合。 并行训练策略：\n数据并行：将训练数据分布到多个GPU上，提升数据处理速度。 模型并行：将模型的不同部分分布到多个GPU上，支持更大规模的模型训练。 流水并行：分阶段处理模型的不同部分，提高训练效率。 $$ \\text{Total Throughput} = \\text{Data Parallelism} \\times \\text{Model Parallelism} \\times \\text{Pipeline Parallelism} $$ 其中，总吞吐量（Total Throughput）是数据并行、模型并行和流水并行的乘积，显著提升了训练效率。 硬件优化：\n高效利用GPU：在16K GPU上实现每GPU超过400 TFLOPS的计算利用率，通过定制的24K GPU集群进行训练，确保训练过程的高效性和稳定性。 错误处理与存储优化： 自动错误检测与处理：确保训练过程的连续性和高效性。 可扩展存储系统：减少检查点和回滚的开销，提高数据存储效率。 微调阶段：\n多轮对齐步骤： 监督微调（SFT）：使用高质量的标注数据进一步优化模型性能。 拒绝采样（Rejection Sampling）：通过拒绝低质量内容，提升生成文本的质量。 近端策略优化（Proximal Policy Optimization, PPO）和直接策略优化（Direct Policy Optimization, DPO）：结合两者的优势，优化模型的生成策略，使其更符合人类偏好。 $$ \\mathcal{L}_{\\text{RLHF}} = \\mathbb{E}_{\\theta \\sim \\pi_{\\theta}} \\left[ r(s, a) \\right] $$ 其中，\\( \\mathcal{L}_{\\text{RLHF}} \\)为RLHF的损失函数，\\( \\pi_{\\theta} \\)为策略分布，\\( r(s, a) \\)为奖励函数。 多模态训练：\n视觉语言模型：结合图像和文本数据，提升模型在多模态任务中的表现。 代码数据扩展：增加代码token数量，提升模型在编程任务中的表现。 模型安全与质量控制：\n数据过滤pipeline： 启发式过滤器：基于规则的过滤，提高数据质量。 NSFW过滤器：去除不适内容，确保数据的安全性。 语义重复数据删除：使用语义分析技术，删除内容高度相似的数据。 文本分类器：预测数据质量，进一步优化数据集。 优化训练堆栈：\n高级训练堆栈：自动检测和处理训练过程中的错误，提升硬件可靠性。 性能调优：针对不同硬件平台进行优化，确保训练过程的高效性。 LLaMA3通过这些先进的训练方法和优化策略，显著提升了模型的性能和适应性，成为开源大语言模型领域的领先者。\n总结 LLaMA系列模型从LLaMA1到LLaMA3，体现了大规模预训练语言模型的技术进化与产业影响。通过不断扩展训练数据量、优化模型架构和引入先进的训练方法，LLaMA系列在性能、多语言支持和多模态能力等方面取得了显著的提升。其开源策略不仅推动了全球AI社区的创新和发展，也为产业应用提供了强大的技术支持。\n参考资料 Hendrycks and Gimpel, 2016 GLU Variants Improve Transformer LLaMA: Open and Efficient Foundation Language Models LLaMA2: Open Foundation and Fine-Tuned Chat Models meta-llama repo ","permalink":"https://syhya.github.io/zh/posts/2025-01-22-llama3/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e注意\u003c/strong\u003e: 本文\u003cstrong\u003e正在更新中\u003c/strong\u003e，内容只是\u003cstrong\u003e草稿版本\u003c/strong\u003e，并不完善，后续会有变动。请随时关注最新版本。\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e本篇文章将系统梳理LLaMA系列模型从LLaMA1到LLaMA3的发展历程，深入解析其模型架构、训练数据和训练方法，并通过对比表格揭示各版本的核心差异。\u003c/p\u003e","title":"Llama系列（长期更新中）"},{"content":"背景 Transformer (Vaswani et al., 2017）是一种基于编码器-解码器架构的模型。此模型在自然处理领域中展示了卓越的性能，随后一系列模型在此基础上进行了优化，例如仅使用编码器的 BERT (Devlin et al., 2018）或仅使用解码器的 GPT (Radford et al., 2018）系列，以及后续的大型语言模型如 LLaMA (Touvron et al., 2023）和 GPT-4 (OpenAI al., 2024）系列，这些模型大多采用了仅解码器的结构。\n符号 符号 含义 \\(B\\) 批量大小（Batch Size） \\(S\\) 序列长度（Sequence Length） \\(d\\) 隐藏维度 / 模型维度（Model Size / Hidden Dimension） \\(H\\) 注意力头数量（Number of Heads in Multi-Head Attention） \\(G\\) 分组数量（Group Number），用于分组查询注意力（GQA） \\(d_{\\text{head}} = \\frac{d}{H}\\) 每个注意力头的维度 \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) 输入序列，批量为 \\(B\\)，序列长度为 \\(S\\)，隐藏维度为 \\(d\\) \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{B \\times S \\times d}\\) 经过线性变换后的 Query、Key、Value 矩阵 \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) 分别对应生成 \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) 的可训练线性映射矩阵 \\(W_O \\in \\mathbb{R}^{d \\times d}\\) 多头 / 分组注意力输出后，用于映射回原始维度 \\(d\\) 的可训练线性映射矩阵 \\(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h \\in \\mathbb{R}^{B \\times S \\times d_{\\text{head}}}\\) 第 \\(h\\) 个注意力头对应的 Query、Key、Value 子矩阵 \\(\\mathbf{K}^*, \\mathbf{V}^*\\) 在多查询注意力（MQA）中，将所有头的 \\(\\mathbf{K}_h, \\mathbf{V}_h\\) 平均或合并后得到的共享 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\) \\(\\mathbf{q}, \\mathbf{k}\\in \\mathbb{R}^{d_{\\text{head}}}\\) 在缩放点积注意力的随机向量示例中，用于数学推导（中心极限定理）的单个查询向量和单个键向量 Transformer中的注意力机制 Transformer模型的核心在于自注意力机制（Self-Attention），它允许模型在处理序列数据时，动态地关注序列中的不同部分。具体来说，给定一个输入序列 \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\)（批大小 \\(B\\)，序列长度 \\(S\\)，隐藏维度 \\(d\\)），Transformer会通过三个线性层分别投影为查询（Query, \\(\\mathbf{Q}\\)）、键（Key, \\(\\mathbf{K}\\)）和值（Value, \\(\\mathbf{V}\\)）：\n\\[ \\mathbf{Q} = \\mathbf{X} W_Q, \\quad \\mathbf{K} = \\mathbf{X} W_K, \\quad \\mathbf{V} = \\mathbf{X} W_V \\]其中，\\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) 是可训练的权重矩阵。多头注意力通过将这些投影分成多个头，每个头负责不同的子空间表示，从而增强模型的表示能力。\n注意力机制有多种形式，Transformer 依赖于缩放点积注意力（Scaled Dot-Product Attention）：给定查询矩阵 \\(\\mathbf{Q}\\)、键矩阵 \\(\\mathbf{K}\\) 和值矩阵 \\(\\mathbf{V}\\)，输出是值向量的加权和，其中每个值的权重由查询与对应键的点积决定：\n\\[ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V} \\] Fig. 1. Scaled Dot-Product Attention. (Image source: Vaswani et al., 2017)\n多头注意力（MHA） 多头注意力（MHA）将 \\(\\mathbf{Q}\\)、\\(\\mathbf{K}\\)、\\(\\mathbf{V}\\) 分成多个头，每个头有独立的 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)，从而增加了模型的容量和灵活性：\n\\[ \\text{MHA}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]其中，每个头的计算为：\n\\[ \\text{head}_h = \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q}_h \\mathbf{K}_h^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V}_h \\] Fig. 2. Multi-Head Attention. (Image source: Vaswani et al., 2017)\n使用多头注意力机制的好处 捕捉不同的特征：单头注意力机制只能关注输入序列中的一种特征或模式，而多头注意力机制可以通过多个注意力头同时关注不同的特征或模式，使模型能够更全面地理解输入数据。 增强模型的表达能力：每个注意力头可以学习不同的表示方式，增强模型的表达能力。不同的注意力头可以关注输入序列的不同部分或不同关系，帮助模型更好地捕捉复杂的依赖关系。 提高稳定性和性能：多头注意力机制通过多个注意力头的平均或组合，减少单个注意力头的噪声和不稳定性，提高模型的稳定性和性能。 并行计算：多头注意力机制可以在计算上并行化，因为每个注意力头的计算是独立的。这有助于提高计算效率，特别是在使用GPU或TPU等硬件加速器时。 缩放点积注意力中的Softmax Softmax函数将一个向量 \\(\\mathbf{z} = [z_1, z_2, \\dots, z_n]\\) 转换为一个概率分布 \\(\\mathbf{y} = [y_1, y_2, \\dots, y_n]\\)，其定义如下：\n\\[ y_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)} \\quad \\text{对于} \\quad i = 1, 2, \\dots, n \\]在注意力机制中，softmax函数用于将缩放后的点积 \\(\\tfrac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\) 转换为注意力权重：\n\\[ \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr) = \\Bigl[ \\frac{\\exp\\Bigl(\\frac{Q_1 \\cdot K_1}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_1 \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)}, \\dots, \\frac{\\exp\\Bigl(\\frac{Q_S \\cdot K_S}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_S \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)} \\Bigr] \\]在 Transformer 的注意力机制中，缩放点积注意力公式中的缩放因子 \\(\\sqrt{d_{\\text{head}}}\\) 是为了确保在进行 softmax 之前，点积的结果不会因为向量维度的增加而变得过大。这主要有以下几个原因：\n防止梯度消失：通过缩放注意力得分，可以避免输入 softmax 函数的值过大，从而防止梯度在反向传播过程中出现消失的情况。\n数值不稳定性：缩放注意力得分可以使得 softmax 函数的输入值范围更加合理，避免数值过于极端，从而提升模型的数值稳定性和训练效果。特别是当向量维度较大时，未经缩放的点积结果可能导致 softmax 的指数函数值过大，进而引发溢出问题。\n数学解释：假设向量 \\(\\mathbf{q}\\) 和 \\(\\mathbf{k}\\) 的各分量独立同分布，均值为 0，方差为 1。它们的点积 \\(\\mathbf{q} \\cdot \\mathbf{k}\\) 的均值为 0，方差为 \\(d_{\\text{head}}\\)。为了防止点积的方差随维度 \\(d_{\\text{head}}\\) 增加而变大，需要对其进行缩放处理。通过将点积除以 \\(\\sqrt{d_{\\text{head}}}\\)，可以使缩放后的点积的方差为 1，与 \\(d_{\\text{head}}\\) 无关。\n根据统计学原理，当将随机变量除以一个常数时，其方差会按该常数的平方倒数缩放。因此，缩放因子 \\(\\tfrac{1}{\\sqrt{d_{\\text{head}}}}\\) 可以有效控制注意力得分的规模，从而提高数值稳定性。以下是详细推导过程：\n假设 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^{d_{\\text{head}}}\\)，各分量独立同分布，均值为 0，方差为 1，则它们的点积为：\n\\[ \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d_{\\text{head}}} q_i k_i \\]根据中心极限定理，当 \\(d_{\\text{head}}\\) 较大时，点积 \\(\\mathbf{q} \\cdot \\mathbf{k}\\) 近似服从均值为 0、方差为 \\(d_{\\text{head}}\\) 的正态分布：\n\\[ \\mathbf{q} \\cdot \\mathbf{k} \\sim \\mathcal{N}(0, d_{\\text{head}}) \\]为了使缩放后的点积具有单位方差，我们将点积除以 \\(\\sqrt{d_{\\text{head}}}\\)：\n\\[ \\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}} \\;\\sim\\; \\mathcal{N}\\!\\Bigl(0, \\frac{d_{\\text{head}}}{d_{\\text{head}}}\\Bigr) = \\mathcal{N}(0, 1) \\]因此，经过缩放后，点积 \\(\\tfrac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}}\\) 的方差恒为 1，与维度 \\(d_{\\text{head}}\\) 无关。这种缩放操作能够保持点积在一个稳定的范围内，避免 softmax 函数在计算中因输入值过大或过小而产生数值不稳定性。\n多查询注意力（MQA） 多查询注意力（MQA）(Shazeer, 2019) 通过让所有查询头（Query Heads）共享同一组键（Key）\\(\\mathbf{K}\\) 和值（Value）\\(\\mathbf{V}\\)，从而显著减少了显存带宽的需求。具体地，如果我们将传统多头注意力（MHA）中的所有 \\(\\mathbf{K}_h\\) 和 \\(\\mathbf{V}_h\\) 做如下平均：\n\\[ \\mathbf{K}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{K}_h, \\quad \\mathbf{V}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{V}_h, \\]其中 \\(H\\) 表示查询头的数量，\\(\\mathbf{K}_h\\) 和 \\(\\mathbf{V}_h\\) 分别表示第 \\(h\\) 个头对应的键和值。那么在推理过程中，每个头只需要使用同一个 \\(\\mathbf{K}^*\\) 和 \\(\\mathbf{V}^*\\)，从而大幅降低对显存带宽的占用。最后再将所有头输出拼接并映射回输出空间：\n\\[ \\text{MQA}(\\mathbf{Q}, \\mathbf{K}^*, \\mathbf{V}^*) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]由于键和值只保留了一组，MQA 推理速度更快，但在某些场景下，模型的表达能力和性能会受到一定限制。\n分组查询注意力（GQA） 分组查询注意力（GQA） (Ainslie, 2023) 是介于 MHA 和 MQA 之间的一种折中方案。它通过将查询头分为多个组，让每组共享一组 $\\mathbf{K}$ 和 $\\mathbf{V}$ 头，以在推理速度和模型性能之间取得平衡。每组包含 $\\frac{H}{G}$ 个查询头，每组共享一组 $\\mathbf{K}$ 和 $\\mathbf{V}$ 头。其具体流程如下：\n投影：将输入 $\\mathbf{X}$ 通过线性变换分别投影为 $\\mathbf{Q}$、$\\mathbf{K}$、$\\mathbf{V}$。 分组 Query：将 $\\mathbf{Q}$ 划分为 $H$ 个头后，再将这些头进一步划分为 $G$ 组。 分组 Key/Value：将 $\\mathbf{K}$ 和 $\\mathbf{V}$ 划分为 $G$ 组，每组共享一组 $\\mathbf{K}$ 和 $\\mathbf{V}$。 组内注意力：对每组的 $\\mathbf{Q}$ 与各自组共享的 $\\mathbf{K}$ 和 $\\mathbf{V}$ 进行注意力计算。 拼接输出：将各组的注意力结果在通道维度上拼接，最后通过线性层得到最终输出。 三种 Attention 方法之间的联系 Fig. 3. Overview of grouped-query method. (Image source: Ainslie et al., 2023)\n图3直观展示了这三种注意力机制的关系：多头注意力（MHA）为每个查询头都保留独立的 $\\mathbf{K}$ 和 $\\mathbf{V}$；多查询注意力（MQA）则所有查询头共享同一组 $\\mathbf{K}$ 和 $\\mathbf{V}$；分组查询注意力（GQA）则在两者之间，通过分组共享的方式兼顾速度与性能。\n当 $G=1$ 时：所有查询头共享同一组 $\\mathbf{K}$ 和 $\\mathbf{V}$。此时 GQA 退化为多查询注意力（MQA）。\n$\\mathbf{K}/\\mathbf{V}$ 头数量：$1$ 模型行为：所有头使用相同的 $\\mathbf{K}$ 和 $\\mathbf{V}$ 进行注意力计算，显著降低显存带宽需求。 当 $G=H$ 时：每个查询头都拥有独立的 $\\mathbf{K}$ 和 $\\mathbf{V}$。此时 GQA 退化为多头注意力（MHA）。\n$\\mathbf{K}/\\mathbf{V}$ 头数量：$H$ 模型行为：每个头使用完全独立的 $\\mathbf{K}$ 和 $\\mathbf{V}$，保留 MHA 的高模型容量和性能。 通过调整分组数量 $G$，GQA 在 MHA 和 MQA 之间实现了灵活切换，能够在保持较高模型性能的同时，兼顾推理速度的提升。\n实现代码片段 下面是使用 PyTorch 简单实现的 MHA 、MQA和 GQA 的代码, 其中 GQA采用了广播（Boardcast）和复制（Repeat）两种方法。此外 需要注意的是，在实际的 LLaMA3 源代码中，GQA 的实现还引入了 KV Cache。为简化示例，以下代码并未包含该部分。如果感兴趣，可以参考源代码 model.py 获取更完整的代码细节。\nMHA 代码片段 multi_head_attention.py\nimport math import torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # (nums_head * head_dim = hidden_dim) assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(dropout_rate) # Define linear projection layers self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): # x has shape: (batch_size, seq_len, hidden_dim) batch_size, seq_len, _ = x.size() # Q, K, V each has shape: (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) K = self.k_proj(x) V = self.v_proj(x) # Reshaping from (batch_size, seq_len, hidden_dim) to (batch_size, seq_len, nums_head, head_dim) # Then transpose to (batch_size, nums_head, seq_len, head_dim) # q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3) # [Another approach to do it] q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) k = K.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Matrix multiplication: (batch_size, nums_head, seq_len, head_dim) * (batch_size, nums_head, head_dim, seq_len) # Resulting shape: (batch_size, nums_head, seq_len, seq_len) # Note that the scaling factor uses head_dim, not hidden_dim. attention_val = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) print(f\u0026#34;attention_mask shape is {attention_mask.size()}\u0026#34;) if attention_mask is not None: # If attention_mask is provided, it should have shape (batch_size, nums_head, seq_len, seq_len). assert attention_val.size() == attention_mask.size() attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # Apply softmax along the last dimension to get attention weights. attention_weight = torch.softmax(attention_val, dim=-1) # Dropout on attention weights attention_weight = self.dropout(attention_weight) # Multiply attention weights with V: # (batch_size, nums_head, seq_len, seq_len) * (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v # Transpose back: (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) # # Note: The transpose operation changes the dimension ordering but does not change the memory layout, # resulting in a non-contiguous tensor. The contiguous() method makes the tensor contiguous in memory, # allowing subsequent view or reshape operations without error. output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) # output = output_mid.permute(0, 2, 1, 3).reshpae(batch_size, seq_len, self.hidden_dim) # # [Another approach to do it] output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 # attention_mask has shape: (batch_size, nums_head, seq_len, seq_len). # Here we use a lower-triangular mask to simulate causal masking. attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_head_attention = MultiHeadAttention(hidden_dim=hidden_dim, nums_head=nums_head) x_forward = multi_head_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) MQA 代码片段 multi_query_attention.py\nimport torch import torch.nn as nn import math class MultiQueryAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(p=dropout) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # For kv, project: hidden_dim -\u0026gt; head_dim self.k_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.v_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() Q, K, V = self.q_proj(x), self.k_proj(x), self.v_proj(x) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Broadcast k and v to match q\u0026#39;s dimensions for attention computation # k -\u0026gt; (batch_size, 1, seq_len, head_dim) # v -\u0026gt; (batch_size, 1, seq_len, head_dim) k = K.unsqueeze(1) v = V.unsqueeze(1) # (batch_size, head_num, seq_len, head_dim) * (batch_size, 1, head_dim, seq_len) # -\u0026gt; (batch_size, head_num, seq_len, seq_len) attention_val = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) attention_weight = torch.softmax(attention_val, dim=-1) print(f\u0026#34;attention_weight is {attention_weight}\u0026#34;) attention_weight = self.dropout(attention_weight) # (batch_size, head_num, seq_len, seq_len) * (batch_size, 1, seq_len, head_dim) # -\u0026gt; (batch_size, head_num, seq_len, head_dim) output_tmp = attention_weight @ v # -\u0026gt; (batch_size, seq_len, head_num, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_query_attention = MultiQueryAttention(hidden_dim=hidden_dim, nums_head=nums_head, dropout=0.2) x_forward = multi_query_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) GQA 代码片段 group_query_attention.py\nimport math import torch import torch.nn as nn class GQABroadcast(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # Total number of Q heads (H) self.nums_kv_head = nums_kv_head # Number of K, V heads (G, groups) assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head # Number of Q heads per group self.q_heads_per_group = nums_head // nums_kv_head self.dropout = nn.Dropout(dropout_rate) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # Projection output dimensions for K, V = nums_kv_head * head_dim self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask= None): batch_size, seq_len, _ = x.size() Q = self.q_proj(x) # (batch_size, seq_len, hidden_dim) K = self.k_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) V = self.v_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) # Q: (batch_size, seq_len, hidden_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2).contiguous() q = q.view(batch_size, self.nums_kv_head, self.q_heads_per_group, seq_len, self.head_dim) # K, V: (batch_size, seq_len, nums_kv_head * head_dim) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim # -\u0026gt; (batch_size, nums_kv_head, 1, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) # q: (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) * (batch_size, nums_kv_head, 1, head_dim, seq_len) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_val = q @ k.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) * (batch_size, nums_kv_head, 1, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) output_tmp = attention_weight @ v # (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = output_tmp.view(batch_size, self.nums_head, seq_len, self.head_dim) # (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) -\u0026gt; (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output class GQARepeat(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head self.nums_kv_head = nums_kv_head assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head self.q_head_per_group = nums_head // nums_kv_head self.q_proj = nn.Linear(hidden_dim, nums_head * self.head_dim) self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) self.dropout = nn.Dropout(dropout_rate) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() # (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) # (batch_size, seq_len, nums_kv_head * self.head_dim) K = self.k_proj(x) V = self.v_proj(x) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) # (batch_size, nums_head, seq_len, head_dim) k_repeat = k.repeat_interleave(self.q_head_per_group, dim=1) v_repeat = v.repeat_interleave(self.q_head_per_group, dim=1) # (batch_size, nums_head, seq_len, seq_len) attention_val = q @ k_repeat.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#39;-inf\u0026#39;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v_repeat # (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 16) batch_size, seq_len, hidden_dim = x.size() nums_head = 8 head_dim = hidden_dim // nums_head nums_kv_head = 4 q_heads_per_group = nums_head // nums_kv_head # v1: Boardcast # attention_mask_v1 has shape: (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_mask_v1 = torch.tril(torch.ones(batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len)) gqa_boradcast = GQABroadcast(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v1 = gqa_boradcast.forward(x, attention_mask=attention_mask_v1) # print(x_forward_v1) print(x_forward_v1.size()) # v2: Repeat # attention_mask_v2 has shape: (batch_size, nums_head, seq_len, seq_len) attention_mask_v2 = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) gqa_repeat = GQARepeat(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v2 = gqa_repeat.forward(x, attention_mask=attention_mask_v2) # print(x_forward_v2) print(x_forward_v2.size()) 时间与空间复杂度分析 说明：下文针对的是一次前向传播（forward propagation）的复杂度；在训练时，还需要额外考虑反向传播（backward propagation）与参数更新。反向传播不仅依赖前向传播保存的中间激活值，还需额外计算梯度和存储中间导数，通常使得总计算量和内存占用比前向传播高，导致训练耗时为前向传播的数倍。\n在分析不同注意力机制（MHA、MQA、GQA）时，我们主要关注它们在 自注意力（self-attention） 或 交叉注意力（cross-attention） 过程中，进行前向传播时的时间复杂度和空间复杂度。即使它们在实现细节上（例如是否共享 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)）有所不同，但从计算量和主要的缓存/显存使用角度来看，其量级大致保持一致。\n假设每个位置都会生成查询 \\(\\mathbf{Q}\\)、键 \\(\\mathbf{K}\\) 和值 \\(\\mathbf{V}\\) 的表征，且各矩阵按批量和头数拆分之后的形状如同下式所示：\n\\[ \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\;\\in\\; \\mathbb{R}^{B \\times H \\times S \\times d_{\\text{head}}} \\]时间复杂度分析 矩阵乘法的通用时间复杂度 对于形状为 $m \\times n$ 的矩阵 $\\mathbf{A}$ 与形状为 $n \\times p$ 的矩阵 $\\mathbf{B}$ 进行乘法 $\\mathbf{A}\\mathbf{B}$，其时间复杂度一般表示为：\n$$ \\mathcal{O}(m \\times n \\times p) $$在注意力机制的计算中，这一基本结论常用于分析 $\\mathbf{Q}\\mathbf{K}^\\top$ 以及注意力分数与 $\\mathbf{V}$ 的乘法等。\n自注意力计算的主要步骤及复杂度 点积计算 ($\\mathbf{Q}\\mathbf{K}^\\top$)\n$\\mathbf{Q}$ 形状：$B \\times H \\times S \\times d_{\\text{head}}$\n$\\mathbf{K}$ 形状：$B \\times H \\times S \\times d_{\\text{head}}$\n因此 $\\mathbf{Q}\\mathbf{K}^\\top$ 的结果形状为 $B \\times H \\times S \\times S$。\n具体的计算量可以视为：对每个批次、每个头，以及序列内所有位置对 $(S \\times S)$ 的点积，其中每个点积涉及 $d_{\\text{head}}$ 维度的乘加运算。\n故其时间复杂度为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S \\times S \\times d_{\\text{head}}\\bigr) \\;=\\; \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) $$ softmax 操作\n在得到的注意力分数矩阵 $B \\times H \\times S \\times S$ 上进行逐元素的 softmax 运算。\nsoftmax 对矩阵的每个元素执行指数与归一化操作，其复杂度一般为：\n$$ \\mathcal{O}(\\text{元素数}) = \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) $$ 相对于上一步的矩阵乘法，其依赖维度 $d_{\\text{head}}$ 的项可以忽略。因此常将其视为比矩阵乘法更小的开销。\n加权平均（注意力分数与 $\\mathbf{V}$ 的乘法）\n$\\mathbf{V}$ 形状：$B \\times H \\times S \\times d_{\\text{head}}$\n注意力分数矩阵形状：$B \\times H \\times S \\times S$\n将每个位置的注意力分数与对应的 $\\mathbf{V}$ 向量乘加之后，输出仍是 $B \\times H \\times S \\times d_{\\text{head}}$。\n其时间复杂度与 $\\mathbf{Q}\\mathbf{K}^\\top$ 的分析类似：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) $$ 将上述三步综合，最主要的开销来自两次矩阵乘法，各为 $\\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}})$。因此在一次完整前向计算时，量级可写为：\n$$ \\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}}) = \\mathcal{O}(B \\times S^2 \\times d). $$（这里用到了 $d_{\\text{head}} = \\frac{d}{H}$）\n增量解码/推理场景（KV Cache）下的时间复杂度 Fig. 4. KV cache example. (Image source: Efficient NLP YouTube Channel)\n参考图4在推理场景（尤其自回归生成）中，通常会使用 KV Cache 来缓存先前时刻的 $\\mathbf{K}$, $\\mathbf{V}$，从而避免重复计算。此时，每生成一个新 token（即处理一个新的时间步）只需：\n对新 token 计算 $\\mathbf{Q}$（及对应的 $\\mathbf{K}$, $\\mathbf{V}$）\n若只保留了投影权重，则新产生的 $\\mathbf{Q}$ 和当前时刻的 $\\mathbf{K}$, $\\mathbf{V}$ 仅涉及 $\\mathcal{O}(d^2)$ 参数乘法，但这是对单个 token而言，相对开销不大。 与已有 KV Cache 做注意力\nKV Cache 中存储了所有先前时刻的 $\\mathbf{K}$, $\\mathbf{V}$，形状约为：\n$$ B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} $$此时 $S_{\\text{past}}$ 表示已经生成的序列长度。\n新的 $\\mathbf{Q}$ 形状是 $B \\times H \\times 1 \\times d_{\\text{head}}$，故新 token 的注意力分数计算为：\n$$ \\mathbf{Q}\\mathbf{K}^\\top : \\; \\mathcal{O}\\bigl(B \\times H \\times 1 \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$ 同理，对 $\\mathbf{V}$ 的加权得到新 token 的输出，也有相同量级：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$ 更新 KV Cache\n将新产生的 $\\mathbf{K}$, $\\mathbf{V}$ 追加到 KV Cache 中，以备下一个时间步使用。此操作在时间复杂度上只是简单的 concat/append，主要在空间上会不断增长。 因此，在增量解码时，每个新 token 的计算量约为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$而不是一次性地进行 $S \\times S$ 规模的注意力计算。若要生成长度为 $S$ 的序列，总体时间在理想情况下也可归纳为\n$$ \\sum_{k=1}^{S} \\mathcal{O}\\bigl(B \\times H \\times k \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) $$与一次性计算的复杂度同阶，只是一次性计算与逐步计算的差异。每步只处理 1 个 token 的注意力时，峰值的临时计算量更小，也无需存储完整的 $S \\times S$ 注意力分数矩阵。\n时间复杂度总结 MHA（多头注意力）：头数多，但每个头分别计算 $\\mathbf{K}$, $\\mathbf{V}$。 MQA（多查询注意力）：多个头共享 $\\mathbf{K}$, $\\mathbf{V}$。 GQA（分组注意力）：将 $H$ 个头分成 $G$ 个组，每组共享一组 $\\mathbf{K}$, $\\mathbf{V}$。 不论 MHA / MQA / GQA，在 完整前向 下，它们的主要矩阵乘法复杂度均为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) $$而在增量推理场景（KV Cache）下，单步计算复杂度降低为\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$但需要在多步解码过程中维护并更新 KV Cache。\n空间复杂度分析 空间复杂度既包括模型参数（权重参数）的规模，也包括前向计算时需要的中间激活值（尤其是注意力得分矩阵、加权结果，以及可能的 KV Cache）的规模。\n模型参数规模 线性投影层的参数\n对输入向量（维度 $d$）投影到 $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ 的维度：\n$$ \\underbrace{d \\times d}_{\\mathbf{Q}\\text{的投影}} \\;+\\; \\underbrace{d \\times d}_{\\mathbf{K}\\text{的投影}} \\;+\\; \\underbrace{d \\times d}_{\\mathbf{V}\\text{的投影}} = 3d^2 $$ 一般而言，这些参数会再根据头数 $H$ 切分成多头的形式，但总和并不因为头数增加而改变。故其量级为 $\\mathcal{O}(d^2)$。\n输出合并层的参数\n将多头输出拼接后再投影回维度 $d$ 时，通常还会有一个 $d \\times d$ 的线性层。这也同样是 $\\mathcal{O}(d^2)$。\n因此，若单独把二者相加，有\n$$ 3d^2 + d^2 = 4d^2 $$ 仍然可记作 $\\mathcal{O}(d^2)$。\n前向计算的中间激活值 在进行训练或完整前向时，需要缓存如下主要张量：\n注意力分数矩阵\n形状为 $B \\times H \\times S \\times S$。无论使用 MHA、MQA 还是 GQA，每个头（或组）都需要计算与 $\\mathbf{Q}\\mathbf{K}^\\top$ 相关的注意力分数，其规模量级为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) $$ 加权后的输出\n形状为 $B \\times H \\times S \\times d_{\\text{head}}$，对应每个位置在前向计算中得到的注意力上下文向量。其量级为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S \\times d\\bigr) $$ 不同注意力机制下的 $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ 存储\n一般在反向传播时，需要缓存 $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ 的前向输出（或中间梯度）。若要显式存储，其形状及规模通常如下：\nMHA（多头注意力） $\\mathbf{Q}$: $B \\times H \\times S \\times d_{\\text{head}}$ $\\mathbf{K}$, $\\mathbf{V}$: $B \\times H \\times S \\times d_{\\text{head}}$ MQA（多查询注意力） $\\mathbf{Q}$: $B \\times H \\times S \\times d_{\\text{head}}$ $\\mathbf{K}$, $\\mathbf{V}$（共享）: $B \\times S \\times d$ GQA（分组注意力） $\\mathbf{Q}$: $B \\times H \\times S \\times d_{\\text{head}}$ $\\mathbf{K}$, $\\mathbf{V}$（分组共享）: $B \\times G \\times S \\times d_{\\text{head}}$, 其中 $G \\times d_{\\text{head}} = d$ 增量解码（KV Cache）下的空间消耗 在推理（增量解码）场景，往往会使用 KV Cache 来保存先前时刻的所有 Key、Value，以免反复计算。此时的存储结构通常是：\nKV Cache 维度（以 MHA 为例）：\n$$ \\mathbf{K}, \\mathbf{V} : B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} $$随着生成序列长度 $S_{\\text{past}}$ 的增长，KV Cache 会线性增大。\n单步注意力分数矩阵：\n由于每次只对新 token 进行注意力计算，分数矩阵的形状约为\n$$ B \\times H \\times 1 \\times S_{\\text{past}} $$显著小于训练时的 $B \\times H \\times S \\times S$。\n因此，增量解码时，大部分临时激活开销（如完整的 $S \\times S$ 矩阵）不再需要，但需要为 KV Cache 额外分配一份 $\\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}})$ 的显存，并随着序列长度增长而累积。\n综合空间复杂度 训练/完整前向\n主要激活值（注意力分数矩阵 + 输出 + Q, K, V 显式缓存）可合并表示为\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) $$当 $S$ 较大时，$B \\times H \\times S^2$ 常是主要瓶颈。\n推理/增量解码（KV Cache）\n无需完整的 $S^2$ 注意力分数矩阵，但需要一份\n$$ \\mathbf{K},\\mathbf{V}\\text{ Cache}: \\;\\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}) $$会随着解码步数 $S_{\\text{past}}$ 增长而线性增加。\n单次注意力分数仅为 $B \\times H \\times 1 \\times S_{\\text{past}}$ 的临时存储，量级显著小于训练场景。\n结论与对比 时间复杂度\n对于自注意力机制，无论是 MHA、MQA 还是 GQA，在完整前向场景下（训练时亦会包含该前向过程），主要的矩阵运算都保持相同量级：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) $$ 在 增量推理（KV Cache） 场景下，每个新 token 只需\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$的计算，但需要维护并更新 KV Cache。\n空间复杂度\n模型参数：三者都在 $\\mathcal{O}(d^2)$ 量级。\n中间激活值（训练/完整前向）：主要由注意力分数矩阵和输出决定，量级为\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) $$ 增量解码（KV Cache）：节省了 $S^2$ 大小的临时分数矩阵，但需要一份随着 $S_{\\text{past}}$ 增长的 K, V 缓存\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$ MQA / GQA 的优势\n虽然从大 $S$ 场景的理论时间复杂度看，MQA、GQA 与 MHA 并无数量级的差别，但它们在键、值共享（或分组共享）带来的实际带宽、缓存访存效率方面，往往能在工程实现中取得更好的显存和速度性能。 下表总结了 MHA、MQA 和 GQA 三种注意力机制的主要差异：\n特性 多头注意力 (MHA) 多查询注意力 (MQA) 分组查询注意力 (GQA) K/V头数量 与头数量相同（$H$） 单一K/V头 分组数（$G$）组，每组1个K/V头 推理时间 较慢 最快 较快，但略高于MQA 显存带宽需求 最高，$H$倍的K/V加载 最低，仅1个K/V头 介于MHA和MQA之间，$G$倍的K/V加载 模型容量 最高 最低 中等，取决于分组数$G$ 性能表现 最佳 略低于MHA 接近MHA，显著优于MQA 向上训练需求 无需 高，需要更多的稳定性和调整 较低，GQA模型在少量数据进行向上训练后即可稳定运行 适用场景 高性能需求但推理速度不敏感的应用 推理速度要求极高，且对模型性能要求较低的场景 需要在推理速度和模型性能之间取得平衡的应用 实验结果 性能测试 本实验在一台配备双 NVIDIA RTX 4090 GPU 的环境下进行，采用数据并行（Data Parallel, DP）方式，将批量大小（batch size）均匀拆分到两张 GPU 上。实验仅测试了前向传播的性能表现，包括平均延迟时间（Time_mean，单位：ms）和峰值显存占用（Peak_Mem_mean，单位：MB），以评估不同注意力机制（MHA、MQA 和 GQA）在推理阶段的资源需求和效率。\n实验代码请参考benchmark_attention.py。 测试基于 Llama3 8B 参数超参数设置 Fig. 5. Overview of the key hyperparameters of Llama 3. (Image source: Grattafiori et al., 2024)\n主要设置参数如下:\n总层数：32 层。 隐藏层维度：4096。 多头注意力总头数：32。 不同组数（nums_kv_head）配置：32（MHA）、1（MQA）、8（GQA-8）。 实验结果 本节主要介绍在不同序列长度（512、1024 和 1536）下，多头注意力（MHA）、多查询注意力（MQA）以及组查询注意力（GQA-8）的实验表现，包含时间延迟和显存占用两个方面的数据。为了方便对比，下表给出了三种注意力机制的具体测试结果。\nModel Size Method nums_kv_head Seq Length Time_mean (ms) Peak_Mem_mean (MB) Llama3 8B GQA-8 8 512 40.8777 2322.328 Llama3 8B MHA 32 512 53.0167 2706.375 Llama3 8B MQA 1 512 37.3592 2210.314 Llama3 8B GQA-8 8 1024 85.5850 6738.328 Llama3 8B MQA 1 1024 80.8002 6570.314 Llama3 8B MHA 32 1024 102.0514 7314.375 Llama3 8B GQA-8 8 1536 147.5949 13586.328 Llama3 8B MHA 32 1536 168.8142 14354.375 Llama3 8B MQA 1 1536 141.5059 13362.314 Fig. 6. Average Time Benchmark.\nFig. 7. Average Peak Memory Benchmark.\n在显存和时间开销敏感的场景下，MQA 和 GQA-8 是更高效的选择，其中 MQA 表现最优，但可能在模型性能能力上有所不足；GQA-8 则在效率和性能之间达到了良好的平衡。\nGQA论文实验结果 推理性能 Fig. 8. Inference time and performance comparison. (Image source: Ainslie et al., 2023)\nFig. 9. Additional Experimental Results. (Image source: Ainslie et al., 2023)\n从实验结果可以看出：\n推理速度：\nMHA-XXL 的推理时间显著高于 MHA-Large，主要由于其更大的头数量和模型规模。 MQA-XXL 和 GQA-8-XXL 相比 MHA-XXL，推理时间分别减少至约1/6和1/5。 性能表现：\nMHA-XXL 在所有任务上表现最佳，但推理时间较长。 MQA-XXL 在推理速度上具有优势，平均分仅略低于 MHA-XXL。 GQA-8-XXL 在推理速度上接近 MQA-XXL，但在性能上几乎与 MHA-XXL 持平，显示出 GQA 的高效性和优越性。 CheckPoint转化 Fig. 10. Ablation Study on Checkpoint Conversion Methods. (Image source: Ainslie et al., 2023)\n图10证明了均值池化方法在保留模型信息方面表现最佳，选择第一个头次之，随机初始化效果最差。均值池化有效地融合了多个 $\\mathbf{K}$ 和 $\\mathbf{V}$ 头的信息，保持了模型性能。\n向上训练比例 Fig. 11. Ablation Study on Uptraining Ratios. (Image source: Ainslie et al., 2023)\n图11展示了以MHA模型为基准，T5 XXL模型在MQA和GQA上的性能随着向上训练的数据量增加变化情况。\nGQA：即使在仅进行转换（无向上训练）的情况下，GQA已具备一定性能，随着向上训练比例增加，性能持续提升。 MQA：需要至少5%比例的预训练数据进行向上训练才能达到实用的性能，且随着比例增加，性能提升趋于平缓。 分组数量对推理速度的影响 Fig. 12. Effect of the Number of GQA Groups on Inference Speed. (Image source: Ainslie et al., 2023)\n从图12可以发现随着分组数的增加，推理时间略有上升，但相较于MHA，仍然保持显著的速度优势。选择适中的分组数比如8可以在速度和性能之间取得良好平衡。图3也显示了 llama3 从 7B 到 405B 参数的模型都是才采用8作为分组数（key/value heads = 8）。\n其他优化方法 除了注意力机制的优化，研究者们还提出了多种方法以提升Transformer模型的推理和训练效率：\nLoRA (HU et al., 2021): 通过在预训练模型的权重矩阵上添加低秩矩阵来实现高效的参数微调。 Flash Attention（Dao et al., 2022）：通过优化注意力计算，减少内存和计算开销。 量化技术 LLM.int8（Dettmers et al., 2022)和 GPTQ (Frantar et al., 2022)：通过降低模型权重和激活的精度，减少显存占用和计算成本。 模型蒸馏（Hinton et al., 2015）：通过训练小模型模仿大模型的行为，减小模型规模。 投机采样 Speculative Sampling（Chen et al., 2023）：通过并行生成和筛选，提升生成效率。 关键总结 向上训练方法能够有效利用已有的MHA模型的Checkpoint，通过少量的额外训练，将其转化为更高效的MQA或GQA模型，显著降低了训练成本。 分组查询注意力（GQA） 在推理效率和模型性能之间取得了良好的平衡，尤其适用于需要高效推理和高性能的应用场景。 实验结果表明，GQA能够在保持与MHA模型相近的性能的同时，显著提升推理速度，适合大规模模型部署和实时应用。 参考文献 [1] Vaswani A. Attention is all you need [J]. Advances in Neural Information Processing Systems, 2017.\n[2] Devlin J. Bert: Pre-training of deep bidirectional transformers for language understanding [J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] Radford A. Improving language understanding by generative pre-training [J]. 2018.\n[4] Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models [J]. arXiv preprint arXiv:2302.13971, 2023.\n[5] Achiam J, Adler S, Agarwal S, et al. Gpt-4 technical report [J]. arXiv preprint arXiv:2303.08774, 2023.\n[6] Shazeer N. Fast transformer decoding: One write-head is all you need [J]. arXiv preprint arXiv:1911.02150, 2019.\n[7] Ainslie J, Lee-Thorp J, de Jong M, et al. Gqa: Training generalized multi-query transformer models from multi-head checkpoints [J]. arXiv preprint arXiv:2305.13245, 2023.\n[8] Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models [J]. arXiv preprint arXiv:2106.09685, 2021.\n[9] Dettmers T, Lewis M, Belkada Y, et al. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale [J]. Advances in Neural Information Processing Systems, 2022, 35: 30318-30332.\n[10] Frantar E, Ashkboos S, Hoefler T, et al. Gptq: Accurate post-training quantization for generative pre-trained transformers [J]. arXiv preprint arXiv:2210.17323, 2022.\n[11] Hinton G. Distilling the Knowledge in a Neural Network [J]. arXiv preprint arXiv:1503.02531, 2015.\n[12] Chen C, Borgeaud S, Irving G, et al. Accelerating large language model decoding with speculative sampling [J]. arXiv preprint arXiv:2302.01318, 2023.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Jan 2025). Transformer注意力机制：MHA、MQA与GQA的对比.\nhttps://syhya.github.io/posts/2025-01-16-group-query-attention/\nOr\n@article{syhya2025gqa, title = \u0026#34;Transformer注意力机制：MHA、MQA与GQA的对比\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-16-group-query-attention/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-01-16-group-query-attention/","summary":"\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\n\u003cp\u003eTransformer (\u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eVaswani et al., 2017\u003c/a\u003e）是一种基于编码器-解码器架构的模型。此模型在自然处理领域中展示了卓越的性能，随后一系列模型在此基础上进行了优化，例如仅使用编码器的 BERT (\u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eDevlin et al., 2018\u003c/a\u003e）或仅使用解码器的 GPT (\u003ca href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\"\u003eRadford et al., 2018\u003c/a\u003e）系列，以及后续的大型语言模型如 LLaMA (\u003ca href=\"https://arxiv.org/abs/2302.13971\"\u003eTouvron et al., 2023\u003c/a\u003e）和 GPT-4 (\u003ca href=\"https://arxiv.org/abs/2303.08774\"\u003eOpenAI al., 2024\u003c/a\u003e）系列，这些模型大多采用了仅解码器的结构。\u003c/p\u003e","title":"Transformer注意力机制：MHA、MQA与GQA的对比"},{"content":"背景 随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。\n本文基于作者的工作经验，总结了如何在现有通用模型的基础上，通过数据准备、模型训练、部署、评估及持续迭代，构建具备特定领域知识的大语言模型。\n为什么要向基座模型注入领域知识 挑战一：有限的领域知识 现有的预训练模型（如 GPT-4、Llama 3）主要基于通用语料库进行训练，缺乏对小众语言或专有领域的深入理解，导致模型在处理编程代码时表现不佳。\n挑战二：数据安全与合规 企业在处理敏感数据时，必须遵循严格的数据主权和合规性要求。将私有业务数据上传至第三方云服务存在安全隐患，因此需要在本地环境中完成数据处理与模型训练。\n挑战三：OpenAI微调的局限 主流商用 API 的微调方案通常较为基础，难以实现深层次的对齐和优化。对于需要高度定制化的领域模型，这种方法难以满足需求。\n注入知识两种方法 在实际项目中，常见的将领域知识注入基座模型的方法主要包括 微调 (Fine-Tuning) 和 检索增强生成 (RAG)。下文将详细对比这些方法，以帮助选择最适合的策略。\n方法对比 微调 (Fine-Tuning) 核心思路\n通过持续预训练、监督微调和偏好对齐，直接更新模型参数，使其掌握特定领域知识和任务模式。\n技术细节\n继续预训练 (CPT)：在大量领域特定的无监督数据上继续预训练基座模型。 监督微调 (SFT)：使用高质量的标注数据进行有监督微调。 偏好对齐 (DPO)：通过用户反馈优化模型输出。 参数微调方法：使用全参数微调或者结合 LoRA 等 PEFT 方法冻结部分参数并添加 adapter。 优势\n深度定制：模型内部权重更新，能够深入理解领域知识。 无需依赖外部检索：推理时不需额外的知识库支持，减少延迟和总的 token 消耗。 提升整体性能：在特定领域任务上表现显著优于通用模型。 劣势\n高计算成本：需要大量计算资源进行训练，尤其是 CPT 阶段。 训练周期长：从数据准备到模型训练再到优化，需要较长时间。 灾难性遗忘：模型可能在学习新知识的同时，遗忘原有的通用能力。 检索增强生成 (RAG) 核心思路\n构建领域知识库，在推理阶段检索相关文档，辅助模型生成更准确的回答，无需直接改变模型参数。\n技术细节\n数据处理：对领域文档进行预处理，按块大小和重叠量切分。 向量化：使用文本嵌入模型将文本块转换为向量，存储在向量数据库中。 召回：推理时通过相似度搜索召回相关文档，作为上下文信息或 few-shot 示例提供给基座模型。 优势\n保持通用能力：模型参数不变，仍保留通用语言能力。 快速更新：知识库可动态更新，无需重新训练模型。 计算效率：避免大规模训练，节省计算资源。 劣势\n依赖知识库质量：检索到的文档质量直接影响回答质量。 推理速度：检索过程可能增加推理延迟，并且需要更多的 token。 知识覆盖有限：模型内部知识仍受限于基座模型的预训练数据。 模型与训练资源 基座模型 以 Llama 3 系列 为例，其具有以下特点：\n参数规模\nLlama 3 系列涵盖从 1B 到 405B 参数的模型，广泛支持多语言处理、代码生成、推理，以及视觉和文本任务。小型模型（1B 和 3B）经过专门优化，适合边缘和移动设备，支持最大 128K 的上下文窗口，可高效处理本地任务，例如摘要生成、指令执行和文本重写。\n多模态能力\nLlama 3 的视觉模型（11B 和 90B 参数）在图像理解任务上的表现优于许多封闭模型，同时支持图像、视频和语音的多模态处理。所有模型均支持微调，便于针对特定领域进行定制化开发。\n开源与社区支持\nLlama 3 系列模型及其权重以开源形式发布，可通过 llama.com 和 Hugging Face 平台 获取，为开发者提供便捷的访问和应用支持。\n数据集限制\n虽然 Llama 3 模型本身以开源形式发布，但其训练所使用的数据集并未开源。因此，严格来说，Llama 3 并非完全开源的模型。这一限制可能会在解决灾难性遗忘问题时带来挑战，因为难以获得与原始训练完全一致的数据集。\n训练资源 训练大型语言模型需要强大的计算资源和高效的分布式训练框架。\n硬件资源\nGPU 集群：建议使用 NVIDIA A100 或 H100 GPU，4卡或8卡配置，通过 NVLink 或 InfiniBand 提升通信带宽。 存储资源：高性能 SSD（如 NVMe）以支持快速的数据读写。 软件框架\n并行框架：DeepSpeed、Megatron-LM 等分布式训练框架，支持大规模模型训练。 推理框架：vLLM、ollama 等，优化推理速度和资源利用。 并行策略\n数据并行：适用于单卡可容纳模型的情况，通过 DeepSpeed 的 ZeRO Stage 0 实现。 模型并行、流水线并行和张量并行：单卡无法容纳时，采用 ZeRO Stage 1、2、3 进行优化，或使用 ZeRO-Infinity 将参数和优化器状态部分卸载到 CPU 或 NVMe。 DeepSpeed ZeRO 分片策略对比 为了更好地理解 DeepSpeed 的 ZeRO 分片策略，以下将分为不同的部分进行详细说明。\nZeRO Stage 分片策略 ZeRO Stage 描述 显存占用 训练速度 ZeRO-0 纯数据并行，不进行任何分片。所有优化器状态、梯度和参数在每张 GPU 上完全复制。 最高 最快 ZeRO-1 分片优化器状态（例如动量和二阶矩），减少显存占用，但梯度和参数仍为数据并行。 高 略慢于 ZeRO-0 ZeRO-2 分片优化器状态和梯度，在 ZeRO-1 的基础上进一步减少显存占用。 中 慢于 ZeRO-1 ZeRO-3 分片优化器状态、梯度和模型参数，显存占用最低，适合大规模模型。但需要在前向和后向时进行参数广播（All-Gather/All-Reduce），通信量显著增加。 低 明显慢于 ZeRO-2，取决于模型大小和网络带宽 Offload 策略 Offload 类型 描述 显存占用 训练速度 ZeRO-1 + CPU Offload 在 ZeRO-1 的基础上，将优化器状态卸载到 CPU 内存；可进一步降低 GPU 显存占用，但需要 CPU-GPU 数据传输，依赖 PCIe 带宽，且占用 CPU 内存。 中偏低 慢于 ZeRO-1，受 CPU 性能和 PCIe 带宽影响 ZeRO-2 + CPU Offload 在 ZeRO-2 的基础上，将优化器状态卸载到 CPU 内存；对较大模型进一步降低 GPU 显存占用，但会增加 CPU-GPU 数据传输开销。 较低 慢于 ZeRO-2，受 CPU 性能和 PCIe 带宽影响 ZeRO-3 + CPU Offload 在 ZeRO-3 的基础上，将优化器状态和模型参数卸载到 CPU；GPU 显存占用最小，但 CPU-GPU 通信量极大，且 CPU 带宽远小于 GPU-GPU 通信。 极低 非常慢 ZeRO-Infinity (NVMe Offload) 基于 ZeRO-3，将优化器状态、梯度和参数卸载到 NVMe，突破 CPU 内存限制，适合超大规模模型；性能高度依赖 NVMe 并行读写速度。 极低需 NVMe 支持 慢于 ZeRO-3，但通常优于 ZeRO-3 + CPU Offload 通信量与性能影响 ZeRO-0/1/2\n通信以 梯度同步 为主，使用 All-Reduce 操作，通信量相对较低。 ZeRO-3\n需要对模型参数进行 All-Gather/All-Reduce 操作，通信量显著增大，网络带宽成为关键瓶颈，前后传播时的参数广播进一步加剧通信负担。 CPU Offload（ZeRO-1/2/3 + CPU）\n卸载优化器状态或参数到 CPU，减少 GPU 显存占用。 通信量主要来自 CPU \u0026lt;-\u0026gt; GPU 数据传输，带宽远低于 GPU-GPU 通信，极易造成性能瓶颈，尤其在 ZeRO-3 场景下。 NVMe Offload（ZeRO-Infinity）\n在 ZeRO-3 的基础上进一步卸载至 NVMe，突破 CPU 内存限制以支持超大规模模型。 性能强烈依赖 NVMe I/O 带宽 和并行度，若 NVMe 速度足够高，通常优于 CPU Offload；但在 I/O 性能较弱或高延迟场景下，效果可能不佳。 硬件与配置影响 硬件限制\nPCIe 带宽、网络带宽、NVMe I/O 等对 Offload 性能有显著影响，需根据硬件环境选择最佳策略。 补充说明\nCPU Offload 利用 CPU 内存并通过 PCIe 传输数据；NVMe Offload 则将状态保存于 NVMe 设备。 NVMe Offload 在 NVMe I/O 性能充足 时通常优于 CPU Offload，但需避免因 I/O 性能不足导致的性能瓶颈。 与官方文档对照\n建议结合 DeepSpeed 官方文档 获取最新、最准确的配置参数和性能调优建议。 数据准备：决定训练成败的核心 数据质量直接决定了模型的性能。数据准备包括数据收集、清洗、去重、分类与配比、脱敏等步骤。\n预训练数据 数据来源 公开数据集：如：the-stack-v2、Common Crawl 等。 企业自有数据：内部文档、代码库、业务日志等。 网络爬虫：通过爬虫技术采集领域相关的网页内容。 数据规模 建议使用至少数亿到数十亿个 token，以确保模型能够充分学习领域知识。 当数据量不足时，模型效果可能受限，建议采用数据增强的方法来补充数据。 数据处理 数据预处理\n格式统一：对来自多个数据源的无标注大量语料进行处理，确保其格式一致。推荐使用高效的存储格式，如 Parquet，以提高数据读取和处理的效率。 数据去重\n检测方法：使用 MinHash、SimHash 或余弦相似度等算法进行近似重复检测。 处理粒度：可选择按句子、段落或文档级别去重，根据任务需求灵活调整。 相似度阈值：设定合理的相似度阈值（如 0.9），删除重复度高于阈值的文本，确保数据多样性。 数据清洗\n文本过滤：结合规则和模型评分器（如 BERT/RoBERTa）去除乱码、拼写错误和低质量文本。 格式化处理：优先使用 JSON 格式处理数据，确保代码、Markdown 和 LaTeX 等特殊格式的准确性。 数据脱敏\n隐私保护：匿名化或去除人名、电话号码、邮箱、密码等敏感信息，确保数据合规。 不合规内容过滤：剔除含有违法、色情或种族歧视等内容的数据块。 数据混合与配比\n比例控制：例如，将 70% 的领域特定数据与 30% 的通用数据相结合，避免模型遗忘通用能力。 任务类型：确保数据包含代码生成、问答对话、文档摘要、多轮对话和数学推理等多种任务类型。 数据顺序\n逐步引导：采用课程学习（Curriculum Learning）方法，从简单、干净的数据开始训练，逐步引入更复杂或噪声较高的数据，优化模型的学习效率和收敛路径。 语义连贯性：利用上下文预训练（In-context Pretraining）技术，将语义相似的文档拼接在一起，增强上下文一致性，提升模型的语义理解深度与泛化能力。 监督微调数据 数据格式 可采用 Alpaca 或 Vicuna 风格，比如结构化为 [instruction, input, output] 的单轮和多轮对话。\n规模：几千条到几十万条，具体根据项目需求和计算资源决定。 质量：确保数据的高质量和多样性，避免模型学习到错误或偏见。 数据构建 在数据构建过程中，我们首先收集日常业务数据，并与业务专家共同构建基础问题。随后，利用大语言模型进行数据增强，以提升数据的多样性和鲁棒性。以下是具体的数据增强策略：\n数据增强策略 表达多样化\n通过大语言模型对现有数据进行改写，采用同义词替换和语法变换等方法，增加数据的多样性。\n鲁棒性增强\n构建包含拼写错误、混合语言等输入的提示（Prompt），以模拟真实场景，同时确保生成答案的高质量。\n知识蒸馏\n利用 GPT-4、Claude 等大型语言模型进行知识蒸馏，生成符合需求的问答数据对。\n复杂任务设计\n针对复杂场景（如多轮对话、逻辑推理等），手动设计高质量数据，以覆盖模型的能力边界。\n数据生成管道\n构建自动化数据生成流水线，将数据生成、筛选、格式化和校验等环节集成，提高整体效率。\n关键要点 任务类型标注：每条数据标注明确的任务类型，便于后续精细化分析和调优。 多轮对话与话题切换：构建多轮对话中上下文关联与话题转换的数据，确保模型能够学习话题切换与上下文关联的能力。 思维链（Chain of Thought）策略：分类、推理等任务可先用 COT 生成过程性答案，提高准确率。 数据飞轮：上线后持续收集用户真实问题，结合真实需求迭代数据；定期清洗，确保质量与多样性。 偏好数据 数据格式 三元组结构：[prompt, chosen answer, rejected answer] 标注细节： 多模型采样：使用多个不同训练阶段或不同数据配比的模型生成回答，增加数据多样性。 编辑与优化：标注人员可对选择的回答进行小幅修改，确保回答质量。 采样策略 多模型采样：部署多个不同版本的模型，对同一 prompt 生成不同回答。 对比标注：由人工或自动化系统对生成的回答进行对比，选择更优的回答对。 关键要点 数据多样性与覆盖：确保偏好数据涵盖各种场景和任务，避免模型在特定情境下表现不佳。 高质量标注：偏好数据的质量直接影响模型的对齐效果，需确保标注准确且一致。 训练流程 一个完整的特定领域大语言模型训练流程通常包括 继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO) 三个主要步骤，最终实现模型的部署与持续优化。\n三种方法对比 训练方法概览 训练方法 主要目标 数据需求 典型应用场景 继续预训练 (CPT) 继续在大规模无监督语料上进行预训练，注入新领域知识 大量无标签文本（至少数亿到数十亿 Token） 补足领域知识，如法律、医疗、金融等专业文本 监督微调 (SFT) 在有监督标注的数据上进行微调，强化特定任务和指令执行能力 定制化标注数据（指令/对话对等），从几千到几十万条 各类特定任务，如代码生成、问答、文本改写、复杂指令执行等 直接偏好对齐 (DPO) 利用偏好数据（正例 chosen vs. 负例 rejected）直接对齐人类偏好 偏好数据：[prompt, chosen, rejected](规模相对较小) 对齐人类反馈，如回答风格、合规性、安全性等 优势与挑战 继续预训练 (CPT) 优势:\n更好领域覆盖，全面提升模型在特定领域的理解和生成能力。 无需额外手动标注。 挑战/局限:\n需要大量高质量领域数据。 训练成本高，需大规模算力与时间。 可能引入领域偏见，需谨慎处理数据质量与分布。 监督微调 (SFT) 优势:\n快速获取可用的任务执行能力。 显著提升模型对特定场景的准确性。 挑战/局限:\n数据标注成本较高。 需谨慎选择标注数据以避免过拟合。 微调后可能削弱模型的通用性。 直接偏好对齐 (DPO) 优势:\n无需单独训练 Reward Model。 数据量需求较小，调优效率高。 挑战/局限:\n需要可靠的偏好标注。 对复杂、多样化场景仍需持续迭代收集更多偏好数据。 易受偏好数据分布的限制。 通用训练要点与技术细节 在进行 CPT、SFT、DPO 三种训练方法时，存在许多通用的训练要点和技术细节。以下部分将这些通用内容进行统一描述，以便于更好地理解和应用。\n数据处理与准备 数据质量：无论是 CPT、SFT 还是 DPO，数据的质量都是至关重要的。需要确保数据的准确性、无歧义性和多样性。 数据格式：统一的数据格式有助于简化训练流程。例如，使用 JSON 或其他结构化格式来存储训练数据。 数据增强：通过 LLM 重写和优化等方式增加数据多样性，提升模型的泛化能力。 学习率与优化 学习率设置：通常采用比预训练阶段更小的学习率，如从 3e-4 降低到 3e-5，具体数值视任务和数据量而定。 学习率调度：使用 warm-up 策略（如前 10% 步骤线性递增），随后采用线性衰减或余弦退火策略，确保训练过程平稳。 优化器选择：根据模型规模和硬件资源选择合适的优化器，比如 AdamW。 训练策略 全参数微调：在资源允许的情况下，优先进行全参数微调，以确保模型能够全面掌握新知识。 参数高效微调（PEFT）：如 LoRA，适用于计算资源有限的场景，通过冻结部分参数并添加 adapter 实现高效微调。 混合精度训练：在支持的 GPU 上使用 bf16 或 fp16，降低显存占用，提高训练速度。 训练稳定性：采用梯度裁剪、正则化、dropout、权重衰减等技术，防止梯度爆炸和模型过拟合。 Flash Attention：利用 Flash Attention 技术优化注意力机制的计算效率，提升训练速度和降低显存占用。 监控与调优 收敛监控：实时监控训练集和验证集的 loss 曲线，确保模型逐步收敛，必要时调整学习率和其他超参数。 Checkpoint：定期保留 Checkpoint，防止意外中断导致全部训练进度丢失。 早停机制：防止模型过拟合，适时停止训练，保存最佳模型状态。 模型评估：在训练过程中定期进行评估，确保模型性能符合预期。 继续预训练 (CPT) 目标 通过在大量领域特定的无监督数据上继续预训练基座模型，注入新的领域知识，使模型更好地理解和生成特定领域的内容。\n训练要点 流式加载\n实施流式数据加载，以便在训练过程中动态读取数据，防止超过最大内存，训练中断。\n全参数微调\n在进行模型训练时，通常需要更新模型的全部参数，以确保模型能够全面掌握新知识。\n全量微调相较于参数高效微调（如 LoRA）在领域知识注入方面效果更佳，尤其在运算资源充足的情况下，建议优先选择全参数微调。\n监督微调 (SFT) 目标 通过高质量的标注数据，训练模型执行特定任务，如代码生成、代码修复、复杂指令执行等，提升模型的实用性和准确性。\n训练要点 Epoch 数量\n在数据量充足的情况下通常 1 ~ 4 个 epoch 即可见到显著效果。 如果数据量不够，可以考虑加大 epoch 数量，但要注意过拟合的风险，建议进行数据增强。 数据增强与多样性\n确保训练数据涵盖多种任务类型和指令表达方式，提升模型的泛化能力。 包含多轮对话和鲁棒性数据，增强模型应对真实用户场景的能力。 直接偏好对齐 (DPO) 目标 通过用户反馈和偏好数据，优化模型输出，使其更符合人类的期望和需求，包括回答风格、安全性和可读性等方面。\nDPO 的特点 直接优化\n不需要单独训练 Reward Model，直接通过 (chosen, rejected) 数据对模型进行对比学习。\n高效性\n相较于 PPO，DPO 需要更少的数据和计算资源即可达到相似甚至更好的效果。\n动态适应\n每次有新数据时，模型能立即适应，无需重新训练 Reward Model。\n训练要点 偏好数据的收集\n部署多个不同训练阶段或不同数据配比的模型，生成多样化的回答。 通过人工或自动化方式标注 chosen 和 rejected 回答对，确保数据的多样性和质量。 对比学习\n通过最大化 chosen 回答的概率，最小化 rejected 回答的概率，优化模型参数。\n迭代优化\n持续收集用户反馈，生成新的偏好数据，进行循环迭代，逐步提升模型性能。 结合数据飞轮机制，实现模型的持续进化与优化。 常见问题与解决方案 重复输出 (Repetitive Outputs)\n问题：模型生成内容重复，连续打印停不下来。\n解决方案：\n数据去重与清洗：确保训练数据不含大量重复内容。 检查 EOT（End-of-Token）设置：防止模型连接打印无法停止。 通过 SFT/DPO 进行对齐：优化模型输出质量。 调整解码策略：如增加 top_k、repetition penalty 和 temperature 参数。 灾难性遗忘 (Catastrophic Forgetting)\n问题：模型在微调过程中遗忘原有的通用能力，可以看作是在新的数据集上过拟合，原本模型参数空间变化过大。\n解决方案：\n混合一部分通用数据：保持模型的通用能力。 调低学习率：减少对原有知识的冲击。 增加 Dropout Rate 和 Weight Decay：避免过拟合。 采用 LoRA 等参数高效微调方法：避免大规模参数更新。 使用 RAG 辅助：结合外部知识库提升模型表现。 Chat Vector: 通过模型权重的简单算术操作，快速为模型注入对话和通用能力。 实体关系与推理路径理解不足\n问题：模型难以正确理解复杂的实体关系和推理路径。\n解决方案：\n引入 Chain-of-Thought (CoT) 数据与强化推理训练：\n通过分步推理训练提升模型的能力，结合 强化微调 和 o1/o3 的训练方法。 扩展训练数据覆盖面：\n引入更多包含复杂实体关系和推理路径的多样化场景数据。 结合知识图谱建模：\n利用 GraphRAG 强化模型对实体关系的理解与推理能力。 模型部署与评估 部署 推理框架\nollama：基于 llama.cpp 的本地推理部署，可快速启动 vLLM：主打高并发、多用户场景下的推理吞吐量优化 量化：将模型量化为 8-bit 或 4-bit，进一步降低推理成本，提高部署效率。 集成 RAG \u0026amp; 智能体\nRAG：结合向量知识库，实时检索相关文档或代码片段，辅助模型生成更精准的回答。 智能体：利用 Function Call 或多轮对话机制，让模型调用外部工具或进行复杂推理，提升互动性和实用性。 Langgraph：封装 RAG 和 多智能体工作流，构建定制化的对话系统或代码自动生成平台。 评估 评估指标\nCPT 阶段：使用领域内测试集，评估困惑度（Perplexity，PPL）或者交叉熵（Cross Entropy），衡量模型对新知识的掌握程度。 SFT / DPO 阶段： Human 或模型评测：通过人工评分或自动化工具，评估回答的准确性、连贯性、可读性和安全性。 代码生成：构建大规模单元测试集，评估 pass@k 指标，衡量代码生成的正确率。 通用能力：在常见 benchmark（如 MMLU、CMMLU）对模型进行测试，确保模型在通用任务上的表现下降不大。 解码超参数\n一致性：在评估过程中保持 top_k、top_p、temperature、max_new_tokens 等解码参数一致，确保评估结果的可比性。 网格搜索：在算力允许的情况下，对不同解码参数组合进行评估，选择最优的参数配置。 数据飞轮与持续迭代 数据飞轮机制\n实时收集用户日志\n收集线上用户的真实 prompt 和生成的回答，覆盖多样化的使用场景和任务类型。\n自动或人工标注\n对收集到的用户 prompt 和回答进行偏好标注，生成新的 (chosen, rejected) 数据对。\n迭代训练\n将新生成的偏好数据加入到下一轮的 SFT/DPO 训练中，不断优化模型的回答质量和用户体验。\n鲁棒性数据\n包含拼写错误、混合语言、模糊指令等数据，提升模型在真实场景下的鲁棒性和应对能力。\n持续优化\n反馈循环：利用用户反馈，持续改进训练数据和模型表现，实现模型的自我优化和进化。 多模型协同：部署多个版本的模型，生成多样化的回答，通过对比学习提升模型的综合能力。 结合意图识别和多智能体推理 使用意图分类模型让大模型判断用户输入意图类别。基于意图类别与上下文类型的映射，监督推理路径，然后根据推理路径进行多路召回。将这些信息提供给训练好的模型，生成最终结果。\n总结 通过 继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO) 的组合方法，能够有效地在基座大模型上注入特定领域的知识，构建出具备高效解决业务问题能力的闭源大语言模型。关键步骤如下：\n数据准备\n高质量的数据收集、清洗、去重和分类，确保训练数据的多样性与准确性。 结合数据脱敏策略，保护隐私与合规。 模型训练\n通过 CPT 注入领域知识，SFT 学习特定任务模式，DPO 优化模型输出符合人类偏好和安全。 利用高效的并行训练框架和调参技巧，提升训练效率和资源利用率。 部署与评估\n采用高效的推理框架，结合 RAG 和 Agent 实现知识增强和功能扩展。 通过多维度评估，确保模型在各个阶段的表现符合预期。 持续迭代\n构建数据飞轮，实时收集用户反馈，不断优化训练数据和模型表现。 集成 RAG 和 Agent，实现模型能力的持续提升与扩展。 最终，通过系统化的流程和技术手段，能够构建一个不仅具备深厚领域知识，还能灵活应对复杂业务需求的长生命周期 AI 系统。\n参考资料 DeepSpeed Megatron-LM ollama vLLM GraphRAG The Llama 3 Herd of Models ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages Evaluating Large Language Models Trained on Code Direct Preference Optimization: Your Language Model is Secretly a Reward Model 引用 引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Jan 2025). 构建特定领域的大语言模型.\nhttps://syhya.github.io/posts/2025-01-05-domain-llm-training\nOr\n@article{syhya2024domainllm, title = \u0026#34;构建特定领域的大语言模型\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-05-domain-llm-training/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/","summary":"\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\n\u003cp\u003e随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。\u003c/p\u003e","title":"构建特定领域的大语言模型"},{"content":"租用 GPU 还是购买 GPU？ 在构建深度学习工作环境之前，首先需要综合考虑 使用周期、预算、数据隐私 以及 维护成本。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。\n租用 GPU 的优点：\n无需一次性投入高额硬件成本 可根据项目需求弹性扩容 云厂商通常提供数据合规与安全保障，省去硬件运维烦恼 购买 GPU 的优点：\n长期大规模使用时，整体成本更低 对内部数据和模型有更高的隐私与可控性 硬件可随时调整、升级，部署更灵活 个人建议\n如果预算有限或只是初学阶段，可先使用 Colab、Kaggle 或云 GPU； 当算力需求和隐私需求上升时，再考虑自建多卡服务器或租用多机多卡集群。 背景 在 2023 年 9 月，为了在工作之余继续对大模型（LLM）进行探索和研究，我组装了一台 双 RTX 4090 的个人 AI 实验服务器。该服务器已运行近一年，整体体验如下：\n噪音：服务器放在脚边，满负荷训练时风扇噪音较大，但在日常推理或中等负载下可接受 推理性能：双卡共计 48GB 显存，采用 4bit 量化方案时可运行到 70B 级别的模型（如 Llama 70B、Qwen 72B） 训练性能：在使用 DeepSpeed 的分布式和 offload 技术（ZeRO-3 + CPU offload）后，可对 34B 左右的模型（如 CodeLlama 34B）进行微调 性价比：对于个人或小团队的日常实验和中小规模模型训练而言，该配置较为实用；但若进行超大规模模型的全参数训练，仍需更多专业卡（如多卡 A100 或 H100 集群） 下图展示了不同大小模型、不同训练方法对显存的需求： Fig. 1. Hardware Requirement. (Image source: LLaMA-Factory)\n搭建思路与配置详情 整机预算在 4 万元人民币（约 6000 美元） 左右，以下是我最终选用的配置清单，仅供参考：\n配件 型号 价格 (元) 显卡 RTX 4090 * 2 25098 主板 + CPU AMD R9 7900X + 微星 MPG X670E CARBON 5157.55 内存 美商海盗船(USCORSAIR) 48GB*2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + 三星 990PRO 4TB 4587 电源 美商海盗船 AX1600i 2699 风扇 追风者 T30 12cm P * 6 1066.76 散热 利民 Thermalright FC140 BLACK 419 机箱 PHANTEKS 追风者 620PC 全塔 897.99 显卡延长线 追风者 FL60 PCI-E4.0 *16 399 总计：约 42723.3 元\nGPU 选择 对于大模型研究，浮点运算性能（TFLOPS） 和 显存容量 是最核心的指标。专业卡（A100、H100 等）虽有更高显存以及 NVLink，但价格动辄数十万人民币，对个人用户并不友好。根据 Tim Dettmers 的调研，RTX 4090 在单位美元算力方面表现非常亮眼，且支持 BF16、Flash Attention 等新特性，因此成为高性价比的选择。\n散热方式：涡轮 vs 风冷 vs 水冷 散热方式 优点 缺点 适用场景 涡轮风扇 体积紧凑；适合并行多卡部署 噪音大、整体散热效率一般 企业服务器机柜、多卡密集部署 风冷散热 性能与噪音平衡佳、维护简单 显卡体型通常较大 家用或个人研究（主机摆放空间足够） 水冷散热 散热能力突出、满载噪音更低 可能会出现漏液、价格更高 对静音要求极高或极限超频场景 家用推荐：风冷卡 兼顾散热效率、噪音和维护成本；相对于涡轮卡和水冷卡更友好。\nCPU \u0026amp; 主板 在深度学习场景中，CPU 主要负责数据预处理、管道调度以及多进程/多线程并行管理，确保数据能够以高吞吐量、低延迟的方式传递到 GPU。因此，CPU 的核心需求主要包括 充足的 PCIe 通道 和 卓越的多线程性能。\nIntel：13/14 代 i9（如 13900K）拥有 20 条 PCIe 主通道，能够满足双卡 x8 + x8 的需求 AMD：Ryzen 7000/9000 系列（如 7950X）提供 28 条（可用 24 条）PCIe 通道，支持双卡 x8 + x8，并为 M.2 SSD 提供足够带宽 微星 MPG X670E CARBON 主板 扩展性：支持 PCIe 5.0 和 DDR5 内存，具备充足的未来升级空间 稳定性：高规格供电设计，保障 CPU 与多显卡的稳定运行 接口丰富：支持多块 M.2 SSD 和 USB4，满足多样化使用需求 AMD Ryzen 9 7900X 特点 核心与线程：12 核心、24 线程，在深度学习场景中的数据预处理和多任务处理方面表现强劲 PCIe 带宽：提供 28 条（可用 24 条）PCIe 5.0 通道，可轻松支持双卡 x8 + x8，并为 M.2 SSD 提供高速带宽 能效比：基于 Zen 4 架构，性能与能耗平衡优秀，适合高性能计算需求 主板选购要点 空间布局 RTX 4090 尺寸庞大且卡槽较厚，需确认主板是否能同时容纳两张显卡；若存在空间或散热冲突，可使用显卡延长线竖放第二张卡。 PCIe 通道拆分 主板需至少支持双 PCIe 4.0 x8 + x8 配置，以避免出现 x16 + x2 的情况。x16 + x2 的带宽分配会显著限制第二块 GPU 的数据传输能力，进而影响 GPU 与 CPU 之间的数据交换效率。在大模型训练中，这种带宽瓶颈可能导致性能显著下降，严重影响整体训练效率。 扩展性 在双显卡插满的情况下，仍需确保主板具有足够的 M.2 SSD 插槽和外设接口 综合扩展性、性能与性价比等因素，我最终选择了 AMD Ryzen 9 7900X 搭配 微星 MPG X670E CARBON 主板 的组合。通过显卡延长线解决了 4090 双卡过厚导致的插槽冲突问题。\nBIOS 设置建议 内存优化 开启 XMP/EXPO（对应 Intel/AMD）以提升内存频率，增强带宽性能。 超频调整 如果需要进一步提升性能，可在 BIOS 中启用 PBO（Precision Boost Overdrive） 或 Intel Performance Tuning，并结合系统监控工具观察稳定性。 温度与稳定性 避免过度超频，注意控制温度，避免因崩溃或过热导致系统不稳定。 内存 深度学习训练中，内存会被大量占用用于数据加载、模型优化状态储存（尤其在多 GPU Zero-stage 并行场景下）。内存容量最好 ≥ 显存总容量的两倍。本配置中，使用了 48GB * 2（共 96GB），满足日常多任务和分布式训练的需求，减少内存不足导致的频繁 swap。\n硬盘 优先选用 M.2 NVMe SSD：其读写性能更优，对加载超大模型权重、缓存中间文件、训练日志等都有显著速度提升 容量建议 ≥ 2TB：随着大模型文件越来越庞大，2TB 往往很快就会被占满，建议根据自身需求选 4TB 或更多 SSD 品牌：三星、海力士或西部数据等主流大厂都拥有稳定的高端产品线 电源 双 4090 满载时整机功耗可达 900W~1000W 左右，CPU、主板和硬盘等还需额外功率余量。通常建议选择 1500W 以上 的铂金或钛金电源，以确保在高负载下电流供给稳定、降低电压波动带来的系统不稳定。\n我在此使用美商海盗船 AX1600i（数字电源），可以通过软件监控实时功耗，并提供充足冗余。\n散热与风扇 我采用 风冷 方案，包括：\nCPU 散热器：利民 FC140（双塔式气冷方案，兼顾了较高的散热效率和相对低噪音） 机箱风扇：追风者 T30 12cm * 6，保持机箱内部正压或者稍微正压的风道布局，保证显卡和供电模块的进风顺畅 在 GPU 长时间高负载训练（如分布式训练大型模型）时，机箱内的风道管理和风扇配置非常重要。建议使用监控软件及时查看 CPU、GPU、主板供电模块温度，适度调节风扇转速。\n散热进阶\n若对静音有更高要求，可考虑 Hybrid 散热（半水冷方案）或更精细的风扇调速曲线。 适度清理机箱灰尘、使用防尘网并定期更换导热硅脂也能提升散热和稳定性。 机箱 RTX 4090 体型巨大，且双卡堆叠时需要充足的内部空间和散热风道。全塔机箱能提供更好的走线空间和气流组织。我选用了 PHANTEKS 追风者 620PC，除了体型大、空间充裕外，也自带良好的线缆管理通道。\n装机完成后的图片如下：\nFig. 2. Computer\n系统与软件环境 操作系统方面强烈推荐 Linux，例如 Ubuntu 22.04 LTS，因其对 CUDA、NVIDIA 驱动以及常见深度学习框架有更好的支持和兼容性。大致流程如下：\n安装 OS：使用 Ubuntu 或其他 Linux 系统即可。 安装 NVIDIA 驱动：确保 nvidia-smi 能正确识别两张 4090:\nFig. 3. nvidia-smi Output\n安装 CUDA 工具链：通过 nvcc -V 确认版本信息:\nFig. 4. nvcc -V Output\n安装 cuDNN：确保深度学习框架可以调用 GPU 加速卷积和 RNN 等操作 测试框架：使用 PyTorch、TensorFlow 或 JAX 简单测试模型推理/训练是否正常 Docker 容器化： 利用 nvidia-container-toolkit 让容器直接访问 GPU 资源，避免主机环境污染。 在多机多卡环境下，还能结合 Kubernetes、Ray 或 Slurm 等进行集群调度与资源管理。 常用工具与框架推荐 训练框架\nLLaMA-Factory：对大语言模型训练/推理流程有较好封装，新手友好 DeepSpeed：支持大模型分布式训练、多种并行策略和优化功能 Megatron-LM：NVIDIA 官方的大规模语言模型训练框架，适合多机多卡场景 监控 \u0026amp; 可视化\nWeights \u0026amp; Biases 或 TensorBoard：实时监控训练过程中的损失函数、学习率等指标，支持远程可视化 推理工具\nollama：基于 llama.cpp 的本地推理部署，可快速启动 vLLM：主打高并发、多用户场景下的推理吞吐量优化 Framework Ollama vLLM 作用 简易本地部署 LLM 高并发 / 高吞吐的 LLM 推理 多请求处理 并发数增加时，推理速度下降明显 并发数增大也能保持较高吞吐 16 路并发 ~17 秒/请求 ~9 秒/请求 吞吐对比 Token 生成速度较慢 Token 生成速度可提升约 2 倍 极限并发 32 路以上并发时，性能衰减较严重 仍能平稳处理高并发 适用场景 个人项目或低并发应用 企业级或多用户并发访问 WebUI\nOpen-WebUI：基于 Web 界面的多合一 AI 界面，支持多种后端推理（ollama、OpenAI API 等），便于快速原型和可视化 进阶建议 开发与调试效率\n使用 SSH 工具提升远程开发效率，制作自定义容器镜像减少环境配置时间。\n量化与剪枝\n通过 4bit、8bit 量化和剪枝技术，减少模型参数和显存需求，优化推理性能。\n混合精度训练\n使用 BF16 或 FP16 提升训练速度，结合 GradScaler 提高数值稳定性。\nCPU 协同优化\n使用多线程、多进程或 RAM Disk 缓存优化数据加载，支持流式加载大规模预训练数据集。\n多机集群部署\n通过 InfiniBand 或高速以太网搭建集群，使用 Kubernetes 实现高效资源调度。\n总结 通过以上配置与思路，我成功搭建了一台 双卡 RTX 4090 深度学习主机。它在 推理 和 中小规模微调 场景中表现良好，对于想要在个人或小团队环境下进行大模型（LLM）科研或应用探索的人来说，这种方案兼具 性价比 与 灵活性。当然，如果要大规模全参数训练上百亿乃至上千亿参数的大模型，依然需要更多 GPU（如多卡 A100/H100 集群）。\n就个人体验而言，双 4090 在预算范围内提供了较好的训练与推理性能，可以满足绝大部分中小规模研究与实验需求，值得有条件的个人或小团队参考。\n参考资料 Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe 通道规格 AMD R5 7600X PCIe 通道规格 MSI MPG X670E CARBON 规格 nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI 版权声明与引用 声明：本文所涉及的配置清单、价格与建议仅供技术交流与研究参考。实际购买与部署请结合个人预算和需求进行综合评估。若因参考或采纳文中信息导致任何直接或间接后果，本文作者恕不承担责任。\n引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Dec 2024). 基于双卡 RTX 4090 搭建家用深度学习主机. https://syhya.github.io/posts/2024-12-21-build-gpu-server\nOr\n@article{syhya2024build, title = \u0026#34;基于双卡 RTX 4090 搭建家用深度学习主机\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Dec\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2024-12-21-build-gpu-server/\u0026#34; ","permalink":"https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/","summary":"\u003ch2 id=\"租用-gpu-还是购买-gpu\"\u003e租用 GPU 还是购买 GPU？\u003c/h2\u003e\n\u003cp\u003e在构建深度学习工作环境之前，首先需要综合考虑 \u003cstrong\u003e使用周期\u003c/strong\u003e、\u003cstrong\u003e预算\u003c/strong\u003e、\u003cstrong\u003e数据隐私\u003c/strong\u003e 以及 \u003cstrong\u003e维护成本\u003c/strong\u003e。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。\u003c/p\u003e","title":"基于双卡 RTX 4090 搭建家用深度学习主机"},{"content":"摘要 股票市场是金融市场的重要组成部分，近些年来，股票市场蓬勃发展，股票价格预测和量化投资策略研究吸引了许多领域的研究学者。其中最近几年随着人工智能和机器学习的发展，学者们从传统的统计学模型迁移到了人工智能算法，尤其是在深度学习热潮掀起后，神经网络在股票价格预测和量化投资策略研究中取得了不错的效果。深度学习的目标是学习多层次的特征，通过组合低级特征构建抽象的高级特征，从而挖掘数据的分布式特征表示，基于此进行复杂的非线性建模，从而实现预测任务。其中 RNN 被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价，交易信息都是序列数据，因此之前有很多研究者，基于 RNN 来预测股票价格。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题，之后出现了诸如 GRU，Peephole LSTM，BiLSTM 等 LSTM 的变体。但传统的股票预测模型有些并未考虑时间因素，有些仅考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。\n本文实验第一部分通过利用国内浦发银行和国外 IBM 的股票数据，分别建立了 LSTM，GRU，BiLSTM 的股票预测模型，通过比较这三种深度学习模型最后预测的结果，发现对于两个数据集都是 BiLSTM 模型优于其他模型，有更好的预测准确率。第二部分通过使用 A 股全市场的股票数据，并先使用 LightGBM 模型进行对 50 个因子的篮选，选出重要程度最高的 10 个因子。之后再用 BiLSTM 模型选取进行因子组合，建立量化投资策略，最后对该策略进行实证与回测，发现该策略优于市场基准指数，说明了 BiLSTM 模型在股票价格预测和量化投资的实际应用价值。\n关键词：量化投资；深度学习；神经网络模型；多因子选股；BiLSTM\n第一章 绪论 1.1 研究背景与意义 1.1.1 研究背景 从 1970 年逐渐兴起，量化投资进入各投资者的视野，一场新的革命就此拉开序幕，改变了从前被动管理和有效市场假说主导的投资组合管理局面。有效市场假说认为，在市场有效的前提下，股票价格能反映市场的所有信息，不存在超额收益。被动投资管理的投资理念是市场是有效的，更加注重资产类别，最常见的方法是购买指数基金并关注已发布指数的表现。而主动投资管理主要依赖于投资者对于市场和个股的主观判定，根据能够获取的公开数据，通过将数学模型应用于金融领域，对股票进行评判，从而构建投资组合获取收益。量化投资通过对大量历史数据进行统计处理，挖掘投资机会，规避主观因素，受到越来越多投资者的追捧。量化投资兴起以后，人们也逐渐利用各种技术来进行股票价格的预测，从而更好地建立量化投资策略。早期国内外学者采用统计学方法建模，来预测股票价格，如指数平均法，多元回归法，自回归平均移动模型等，但是由于股票市场受多种因素影响，同时数据量很大，导致股票预测难度很大，各种统计学模型预测效果并不令人满意。\n近几年来，机器学习，深度学习和神经网络等相关技术不断发展，为股票价格预测和量化策略的构建提供了技术支持，不少学者通过随机森林法，神经网络，支持向量机和卷积神经网络等方法完成新的突破。股票市场足够的历史数据加之以多元的技术支撑，为股票价格预测和量化策略的构建提供了有利条件。\n1.1.2 研究意义 从国家经济体系和金融市场的长远发展来看，对于股票价格预测模型和量化投资策略的研究必不可少。我国起步较晚，金融市场不够成熟，金融工具不够丰富，市场效率较低，但是近几年来国家逐渐放宽政策，大力建设金融市场，为量化投资的发展提供“温床”，发展量化投资及新兴金融技术可以提供我们国家金融市场弯道超车的机会。并且股票价格指数作为一项重要的经济指标，对于我国经济发展起着晴雨表的作用。\n从个人投资者和机构投资者的角度来看，构建股票价格预测模型和量化投资策略模型提高了市场的有效性。个人投资者的专业知识不够完善，投资行为具有一定的盲目性，构建相关模型，为其提供参考，能够减少判断失误的概率，改变个人投资者在资本市场处于相对弱势的地位。并且对于机构投资者而言，理性客观的模型加之以个人经验的判断，提高了决策的正确性，为投资行为提供了新的方向。\n综上，结合我国目前的发展现状，本文选取个股进行股票价格预测模型和 A 股全市场的股票进行量化策略研究有重要的现实研究意义。\n1.2 研究综述 White（1988）$^{[1]}$ 使用 BP 神经网络来预测 IBM 股票的日收益率。然而，由于 BP 神经网络模型易受梯度爆炸的影响，导致模型无法收敛到全局最小值，从而无法实现准确的预测。\nKimoto（1990）$^{[2]}$ 使用模块化神经网络技术开发了一个用于东证股价指数（Tokyo Stock Exchange Prices Indexes，TOPIX）预测的系统。该系统不仅成功预测了东京证券交易所的 TOPIX，还通过基于预测结果的股票交易模拟，实现了一定程度的盈利。\nG．Peter Zhang（2003）$^{[3]}$ 对差分整合移动平均自回归（Autoregressive Integrated Moving Average，ARIMA）模型和人工神经网络（Artificial Neural Network，ANN）模型在时间序列预测中的性能进行了对比研究。结果显示，ANN 模型在时间序列预测的精度上显著优于 ARIMA 模型。\nRyo Akita（2016）$^{[4]}$ 选取消费者物价指数、市盈率以及报纸上的各种事件作为特征，利用段落向量和 LSTM 网络构建了一个金融时间序列预测模型。通过东京证券交易所五十家上市公司的实际数据，验证了该模型在股票开盘价预测方面的有效性。\nKunihiro Miyazaki（2017）$^{[5]}$ 通过提取股票日线图像及每 30 分钟的股票价格数据，构建了一个针对东证核心 30 指数（Topix Core 30）及其成分股涨跌预测的模型。研究对比了多种模型，包括逻辑回归（Logistic Regression, LR）、随机森林（Random Forest, RF）、多层感知器（Multilayer Perceptron, MLP）、LSTM、CNN、PCA-CNN 和 CNN-LSTM。结果表明，LSTM 在预测性能上最优，CNN 表现一般，但结合 CNN 和 LSTM 的混合模型可以提升预测精度。\nTaewook Kim（2019）$^{[6]}$ 提出了一个 LSTM-CNN 混合模型，用于结合股票价格时间序列与股票价格图像两种数据表示形式的特征，以预测 S\u0026amp;P 500 指数的股价。研究表明，LSTM-CNN 模型在股价预测方面优于单一模型，并且这种预测对于构建量化投资策略具有一定的实际意义。\n1.3 论文的创新点 本文股票预测方面具有以下创新点：\n分别选用国内 A 股上海浦东发展银行和国外美股 IBM 的数据进行研究，避免单一市场研究的局限性。并且传统的 BP 模型从未考虑时间因素，要么像 LSTM 模型考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。实证过程中并与 LSTM 模型，GRU 模型进行对比，说明其能够提升预测准确率。 股票价格预测模型采用股票多特征进行训练，包括开盘价，闭盘价，最高价和交易量等特征，相对于单特征预测股票收盘价理论上更加精确，能更好地比较 LSTM，GRU 和 BiLSTM 对于股票的预测效果。 本文量化策略研究方面具有以下创新点：\n未使用市面上已有的常见因子，使用自己通过遗传规划算法（Genetic Programming，GP）和人工数据挖掘得到的多个价量因子，并通过 LightGBM 模型进行对 50 个因子的筛选，选出重要程度最高的 10 个因子。 传统的量化投资模型一般用 LSTM 模型和 CNN 模型构建量化投资策略，本文使用 A 股全市场的数据，利用 BiLSTM 模型选取进行因子组合，建立量化投资策略，最后对该策略进行实证与回测，发现该策略优于市场基准指数（中证全指），说明了 BiLSTM 模型在股票价格预测和量化投资的实际应用价值。 1.4 论文研究框架 本文基于深度学习算法分别进行了股票价格预测和量化策略研究，本文的具体研究框架如 Fig. 1 所示：\nFig. 1. Research Framework.\n本文具体研究框架内容如下：\n第一章为绪论。本章首先对股票价格预测和量化策略研究的研究意义和研究背景进行了介绍。随后对研究现状进行了综述，然后说明了本文相比现有的研究的创新点，最后简要阐述了本文的研究框架。\n第二章为相关理论基础。本章对本文研究中涉及到的深度学习模型和量化选股的基本理论进行了介绍。深度学习模型小节依次介绍了 RNN，LSTM，GRU 和 BiLSTM 这四个深度学习模型，其中重点介绍了 LSTM 模型的内部结构。量化选股理论小节依次介绍了均值－方差模型，资本资产定价模型，套利定价理论，多因子模型，Fama－French 三因子模型和五因子模型。本小节从各种金融理论和模型发展脉络中介绍了多因子量化选股的历程。\n第三章为 LSTM，GRU 和 BiLSTM 在股票价格预测中比较研究。本章首先介绍了实验所用国内及国外股票的数据集，然后对于数据进行归一化和数据划分的预处理步骤。紧接着说明了本章所使用 LSTM，GRU 和 BiLSTM 这三个模型的网络结构，模型的编译和超参数设置，并进行了实验得到实验结果。最后对实验结果进行分析和本章小结。\n第四章为基于 LightGBM-BiLSTM 的量化投资模型研究。本章首先大致介绍了实验步骤，然后分别介绍了实验所用的股票数据和因子数据。之后再对因子依次进行缺失值处理，去极值，因子标准化和因子中性化处理得到清洗后的因子。随后再利用 LightGBM 和 BiLSTM 分别进行因子选择和因子组合，最后根据得到的模型进行量化策略构建，并对量化策略进行回测。\n第五章为总结与展望。本章对于本文关于股票价格预测与量化投资策略的主要研究内容进行了总结，之后针对目前研究所存在的不足，对未来研究的方向进行了展望。\n第二章 相关理论基础 2.1 深度学习模型 2.1.1 RNN 循环神经网络（Recurrent Neural Network，RNN）被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价和交易信息都是序列数据，因此之前有很多工作，基于 RNN 来预测股票价格。RNN 采用十分简单的重复模块的链式结构，例如单个 tanh 层。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题。Fig. 2 是 RNN 结构图。\nFig. 2. RNN Structure Diagram. (Image source: Understanding LSTM Networks)\n2.1.2 LSTM 长短时记忆网络（Long Short-Term Memory，LSTM）是一种特殊的 RNN，能够学习长期依赖关系。它们是由 Hochreiter \u0026amp; Schmidhuber（1997）$^{[7]}$ 提出的，并在随后的工作中被许多人改进和推广。由于独特的设计结构，LSTM 有着间隙长度相对不敏感的特点，并且解决了传统 RNN 的梯度消失和梯度爆炸的问题。相对于传统 RNN 和隐马尔可夫模型（Hidden Markov Model，HMM）等其他时间序列模型，LSTM 能更好地处理和预测时间序列中间隔和延迟非常长的重要事件。因此，LSTM 广泛应用在机器翻译和语音识别的领域。\nLSTM 被明确设计为避免长期依赖问题。所有的递归神经网络都具有神经网络的重复模块链的形式，而 LSTM 对 RNN 的结构进行了改进。LSTM 并没有采用单一神经网络层，而是采用一种特殊的方式进行交互的四层结构。\nFig. 3. LSTM Structure Diagram 1. (Image source: Understanding LSTM Networks)\nFig. 4. LSTM Structure Diagram 2. (Image source: Understanding LSTM Networks)\n如 Fig.3 所示，黑线用来表示传输一个节点的输出向量到另一个节点的输入向量。神经网络层（Neural network layer）是带有 $\\sigma$ 激活函数或者 tanh 激活函数的处理模块；逐点运算（Pointwise operation）是代表向量与向量之间进行点乘运算；向量传输（Vector transfer）是表示信息传递方向；汇合（Concatenate）和复制（Copy）分别用两个黑线合在一起和两个黑线分开来表示信息的汇合和信息的复制。\n下面我们以 LSTM 为例，进行其结构详细的说明。\n遗忘门（forget gate） Fig. 5. Forget Gate Calculation (Image source: Understanding LSTM Networks)\n$$ f_{t} = \\sigma\\left(W_{f} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{f}\\right) $$参数说明：\n$h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $\\sigma$ ：sigmoid 激活函数 $W_{f}$ ：遗忘门的权重矩阵 $b_{f}$ ：遗忘门的偏差向量参数 第一步如 Fig.5 所示，是一个决定从细胞状态中丢弃的信息的过程。该过程由 sigmoid 函数计算得到 $f_{t}$ 的值（$f_{t}$ 的范围在 0 到 1 之间，其中 0 代表完全不通过，1 代表完全通过）来决定细胞状态 $C_{t-1}$ 通过或者部分通过。\n输入门（input gate） Fig. 6. Input Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} i_{t} \u0026= \\sigma\\left(W_{i} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{i}\\right) \\\\ \\tilde{C}_{t} \u0026= \\tanh\\left(W_{C} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{C}\\right) \\end{aligned} $$参数说明：\n$h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $\\sigma$ ：sigmoid 激活函数 $W_{i}$ ：输入门的权重矩阵 $b_{i}$ ：输入门的偏差向量参数 $W_{C}$ ：细胞状态的权重矩阵 $b_{C}$ ：细胞状态的偏差向量参数 $\\tanh$ ：tanh 激活函数 第二步如 Fig.6 所示，通过 sigmoid 函数计算我们要在细胞状态中存储什么信息，接下来通过一个 $\\tanh$ 层创建候选向量 $\\tilde{C}_{t}$，该向量将会被加到细胞的状态中。\nFig. 7. Current Cell State Calculation (Image source: Understanding LSTM Networks)\n$$ C_{t} = f_{t} * C_{t-1} + i_{t} * \\tilde{C}_{t} $$参数说明：\n$C_{t-1}$ ：上一时刻的细胞状态 $\\tilde{C}_{t}$ ：临时细胞状态 $i_{t}$ ：输入门的值 $f_{t}$ ：遗忘门的值 第三步如 Fig.7 所示，当前时刻的细胞状态 $C_t$ 通过结合遗忘门和输入门的作用计算得到。\n遗忘门 $f_t$ 对上一时刻的细胞状态 $C_{t-1}$ 进行加权，以丢弃不需要的信息。 输入门 $i_t$ 对候选细胞状态 $\\tilde{C}_t$ 进行加权，决定引入多少新信息。\n最终，两个部分相加，更新得出当前时刻的细胞状态 $C_t$。 输出门（output gate） Fig. 8. Output Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} o_{t} \u0026= \\sigma\\left(W_{o} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{o}\\right) \\\\ h_{t} \u0026= o_{t} * \\tanh\\left(C_{t}\\right) \\end{aligned} $$参数说明：\n$o_{t}$ ：输出门的值 $\\sigma$ ：sigmoid 激活函数 $W_{o}$ ：输出门的权重矩阵 $h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $b_{o}$ ：输出门的偏差向量参数 $h_{t}$ ：当前时刻的输出 $\\tanh$ ：tanh 激活函数 $C_{t}$ ：当前时刻的细胞状态 第四步如 Fig.8 所示，使用 sigmoid 函数计算输出门的值，最后通过 tanh 激活函数将这一时刻的细胞状态 $C_{t}$ 进行处理并与输出门的值 $o_{t}$ 相乘得到当前时刻的输出 $h_{t}$。\n2.1.3 GRU K. Cho（2014）$^{[8]}$ 提出了门控循环单元（Gated Recurrent Unit，GRU）。GRU 主要是在 LSTM 的基础上进行了简化和调整，将 LSTM 原有的遗忘门、输入门和输出门合并为更新门（update gate）和重置门（reset gate）。此外，GRU 还将细胞状态与隐藏状态合并，从而减少了模型的复杂性，同时在某些任务中仍能够达到与 LSTM 相当的性能。\n该模型在训练数据集比较大的情况下可以节省很多时间，在某些较小和较不频繁的数据集上表现出更好的性能$^{[9][10]}$。\nFig. 9. GRU Structure Diagram (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} z_{t} \u0026= \\sigma\\left(W_{z} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ r_{t} \u0026= \\sigma\\left(W_{r} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ \\tilde{h}_{t} \u0026= \\tanh\\left(W \\cdot \\left[r_{t} * h_{t-1}, x_{t}\\right]\\right) \\\\ h_{t} \u0026= \\left(1 - z_{t}\\right) * h_{t-1} + z_{t} * \\tilde{h}_{t} \\end{aligned} $$参数说明：\n$z_{t}$ ：更新门的值 $r_{t}$ ：重置门的值 $W_{z}$ ：更新门的权重矩阵 $W_{r}$ ：重置门的权重矩阵 $\\tilde{h}_{t}$ ：临时的输出 2.1.4 BiLSTM 双向长短时记忆网络（Bidirectional Long Short-Term Memory，BiLSTM）是由前向的 LSTM 与后向的 LSTM 结合成的。BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，能够学习到对时间有长期依赖性的信息，与单向 LSTM 相比可以更好地考虑未来数据的反向影响。Fig. 10 是 BiLSTM 结构图。\nFig. 10. BiLSTM Structure Diagram. (Image source: Baeldung)\n2.2 量化选股理论 2.2.1 均值－方差模型 量化选股策略起源于 20 世纪 50 年代，Markowitz（1952）$^{[11]}$ 提出了均值－方差模型（Mean-Variance Model）。该模型不仅奠定了现代投资组合理论的基础，将投资风险量化，还建立了一个描述风险和预期收益率的具体模型。它打破了以往仅对投资组合进行定性分析而缺乏定量分析的局面，将数学模型成功引入金融投资领域。\n$$ \\begin{aligned} \\mathrm{E}\\left(R_{p}\\right) \u0026= \\sum_{i=1}^{n} w_{i} \\mathrm{E}\\left(R_{i}\\right) \\\\ \\sigma_{p}^{2} \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\operatorname{Cov}\\left(R_{i}, R_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\sigma_{i} \\sigma_{j} \\rho_{ij} \\\\ \\sigma_{i} \u0026= \\sqrt{\\operatorname{Var}\\left(R_{i}\\right)}, \\quad \\rho_{ij} = \\operatorname{Corr}\\left(R_{i}, R_{j}\\right) \\end{aligned} $$$$ \\min \\sigma_{p}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} \\mathrm{E}\\left(R_{i}\\right) w_{i} = \\mu_{p}, \\quad \\sum_{i=1}^{n} w_{i} = 1 $$$$ \\begin{aligned} \\Omega \u0026= \\begin{pmatrix} \\sigma_{11} \u0026 \\cdots \u0026 \\sigma_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\sigma_{n1} \u0026 \\cdots \u0026 \\sigma_{nn} \\end{pmatrix} = \\begin{pmatrix} \\operatorname{Var}\\left(R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Cov}\\left(R_{1}, R_{n}\\right) \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\operatorname{Cov}\\left(R_{n}, R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Var}\\left(R_{n}\\right) \\end{pmatrix} \\\\ \\Omega^{-1} \u0026= \\begin{pmatrix} v_{11} \u0026 \\cdots \u0026 v_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ v_{n1} \u0026 \\cdots \u0026 v_{nn} \\end{pmatrix} \\\\ w_{i} \u0026= \\frac{1}{D}\\left(\\mu_{p} \\sum_{j=1}^{n} v_{ij}\\left(C \\mathrm{E}\\left(R_{j}\\right) - A\\right) + \\sum_{j=1}^{n} v_{ij}\\left(B - A \\mathrm{E}\\left(R_{j}\\right)\\right)\\right), \\quad i = 1, \\ldots, n \\end{aligned} $$$$ \\begin{aligned} A \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{j}\\right), \\quad B = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{i}\\right) \\mathrm{E}\\left(R_{j}\\right), \\quad C = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij}, \\quad D = BC - A^{2} \u003e 0 \\\\ \\sigma_{p}^{2} \u0026= \\frac{C \\mu_{p}^{2} - 2A \\mu_{p} + B}{D} \\end{aligned} $$其中：\n$\\mathrm{E}\\left(R_{p}\\right)$ 和 $\\mu_{p}$ 是投资组合 $p$ 的预期收益率 $\\mathrm{E}\\left(R_{i}\\right)$ 是资产 $i$ 的预期收益率 $\\sigma_{i}$ 是资产 $i$ 的标准差 $\\sigma_{j}$ 是资产 $j$ 的标准差 $w_{i}$ 是资产 $i$ 在投资组合中的比例 $\\sigma_{p}^{2}$ 是投资组合 $p$ 的方差 $\\rho_{ij}$ 是资产 $i$ 和资产 $j$ 之间的相关系数 通过以上公式$^{[12]}$，我们可以构建投资组合，来让我们的投资组合在一定的期望收益率的条件下，非系统风险降低到最小。\n2.2.2 资本资产定价模型 William Sharpe（1964）$^{[13]}$、John Lintner（1965）$^{[14]}$ 和 Jan Mossin（1966）$^{[15]}$ 提出了资本资产定价模型（Capital Asset Pricing Model，CAPM）。该模型认为，一项资产的预期收益与该资产的风险度量 $\\beta$ 值相关。这一模型通过简单的线性关系，将资产的预期收益率与市场风险联系起来，使得 Markowitz（1952）$^{[11]}$ 的投资组合选择理论更贴近现实世界，同时为多因子选股模型的建立奠定了理论基础。\n根据资本资产定价模型，对于一个给定的资产 $i$，它的预期收益率和市场投资组合的预期收益率之间的关系可以表示为：\n$$ E\\left(r_{i}\\right) = r_{f} + \\beta_{im}\\left[E\\left(r_{m}\\right) - r_{f}\\right] $$其中：\n$E\\left(r_{i}\\right)$ 是资产 $i$ 的预期收益率 $r_{f}$ 是无风险利率 $\\beta_{im}$（Beta）是资产 $i$ 的系统性风险系数，$\\beta_{im} = \\frac{\\operatorname{Cov}\\left(r_{i}, r_{m}\\right)}{\\operatorname{Var}\\left(r_{m}\\right)}$ $E\\left(r_{m}\\right)$ 是市场投资组合 $m$ 的预期收益率 $E\\left(r_{m}\\right) - r_{f}$ 是市场风险溢价 2.2.3 套利定价理论和多因子模型 Ross（1976）$^{[16]}$ 提出了套利定价理论（Arbitrage Pricing Theory，APT）。该理论认为，套利行为是形成市场均衡价格的决定性因素，通过在收益率形成过程中引入一系列因子构建线性相关关系，克服了资本资产定价模型（CAPM）的局限性，为后续研究者提供了重要的理论指导。\n套利定价理论被认为是多因子模型（Multiple-Factor Model，MFM）的理论基础，是资产价格模型的重要组成部分，也是资产价格理论的基石之一。多因子模型的一般表达形式为：\n$$ r_{j} = a_{j} + \\lambda_{j1} f_{1} + \\lambda_{j2} f_{2} + \\cdots + \\lambda_{jn} f_{n} + \\delta_{j} $$其中：\n$r_{j}$ 是资产 $j$ 的收益率 $a_{j}$ 是资产 $j$ 的常数 $f_{n}$ 是系统性因素 $\\lambda_{jn}$ 是因子载荷 $\\delta_{j}$ 是随机误差 2.2.4 Fama－French 三因子模型和五因子模型 Fama（1992）和 French（1992）$^{[17]}$ 使用横截面回归与时间序列结合的方法发现，股票市场的 $\\beta$ 值无法解释不同股票回报率的差异，而上市公司的市值、账面市值比和市盈率等可以显著解释股票回报率的差异。他们认为超额收益是对 CAPM 中 $\\beta$ 未能反映的风险因素的补偿，由此提出了 Fama－French 三因子模型。这三个因子分别为：\n市场风险溢价因子（Market Risk Premium）\n表示市场整体的系统性风险，即市场投资组合的预期收益减去无风险利率的差值。 衡量投资者承担系统性风险（即无法通过分散投资消除的风险）所期望的超额回报。 计算公式为：\n$$ \\text{Market Risk Premium} = E(R_m) - R_f $$ 其中 $E(R_m)$ 是市场的预期收益率，$R_f$ 是无风险利率。 市值因子（Size, SMB: Small Minus Big）\n表示小市值股票与大市值股票之间的收益差异。 小市值股票通常风险更高，但历史数据显示，其预期收益也往往高于大市值股票。 计算公式为：\n$$ SMB = R_{\\text{Small}} - R_{\\text{Big}} $$ 反映了市场对小市值股票的额外风险溢价的补偿。 账面市值比因子（Value, HML: High Minus Low）\n反映高账面市值比（即“价值型股票”）与低账面市值比（即“成长型股票”）之间的收益差异。 高账面市值比的股票通常定价较低（被市场低估），但长期来看可能获得较高回报。 计算公式为：\n$$ HML = R_{\\text{High}} - R_{\\text{Low}} $$ 低账面市值比的股票可能因市场对其过于乐观的预期而被高估。 该模型将 APT 模型中的因子具体化，并得出结论：投资小市值、高成长的股票具有高风险高收益的特性。Fama－French 三因子模型被广泛应用于现代投资行为的分析和实践中。\n随后，Fama（2015）和 French（2015）$^{[18]}$ 对三因子模型进行了扩展，新增了以下两个因子：\n盈利水平因子（Profitability, RMW: Robust Minus Weak）\n反映高盈利公司与低盈利公司之间的收益差异。 盈利能力强的公司（高 ROE、净利润率）更可能提供稳定且较高的回报。 计算公式为：\n$$ RMW = R_{\\text{Robust}} - R_{\\text{Weak}} $$ 投资水平因子（Investment, CMA: Conservative Minus Aggressive）\n反映保守型投资公司与激进型投资公司之间的收益差异。 激进型公司（扩张迅速，资本开支较高）通常伴随着更大的经营风险，而保守型公司（资本支出相对稳健）表现出更高的稳定性和收益。 计算公式为：\n$$ CMA = R_{\\text{Conservative}} - R_{\\text{Aggressive}} $$ Fama-French 三因子模型公式为：\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\epsilon_i $$Fama-French 五因子模型公式为：\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\beta_{i,RMW} \\cdot RMW + \\beta_{i,CMA} \\cdot CMA + \\epsilon_i $$其中：\n$R_i$: 股票 $i$ 的预期收益率 $R_f$: 无风险收益率 $R_m$: 市场组合的预期收益率 $R_m - R_f$: 市场风险溢价因子 $SMB$: 小市值减去大市值股票收益 $HML$: 高账面市值比减去低账面市值比股票收益 $RMW$: 高盈利能力减去低盈利能力股票收益 $CMA$: 低投资活动减去高投资活动股票收益 $\\beta_{i,*}$: 股票 $i$ 对应因子的敏感度 $\\epsilon_i$: 回归残差 2.2.5 模型对比表格 以下表格总结了 均值-方差模型、资本资产定价模型 (CAPM)、套利定价理论 (APT) 和 Fama-French 模型 的核心内容及因子来源：\n模型 核心内容 因子来源 均值-方差模型 投资组合理论的基础，通过期望收益和协方差矩阵优化投资组合。 投资组合中资产的期望收益和协方差矩阵 资本资产定价模型 (CAPM) 通过市场风险因子（$\\beta$）解释资产收益，奠定多因子模型理论基础。 市场因子 $\\beta$ 套利定价理论 (APT) 多因子框架，允许多个经济变量解释资产收益，例如通胀率、利率等。 多因子（宏观经济变量，如通胀率、利率等） Fama-French 三因子模型 增加市值因子和账面市值比因子，改进对资产收益的解释能力。 市场因子、SMB（市值因子）、HML（账面市值比因子） Fama-French 五因子模型 在三因子模型基础上增加盈利因子和投资因子，进一步完善资产定价模型。 市场因子、SMB、HML、RMW（盈利因子）、CMA（投资因子） 以下表格总结了这些模型的优点及不足：\n模型 优点 不足 均值-方差模型 提供了系统化投资组合优化方法，奠定现代投资理论基础。 仅针对收益和方差进行优化，未明确风险补偿的来源。 资本资产定价模型 (CAPM) 简单易用，通过市场风险解释收益差异，为多因子模型提供理论基础。 假设单因子（市场风险）决定收益，忽略其他系统性风险因子。 套利定价理论 (APT) 允许多个因子解释资产收益，减少对单因子假设的依赖，更灵活。 未明确具体因子，实操性较低，仅提供框架。 Fama-French 三因子模型 通过增加市值因子和账面市值比因子，显著提高了对资产收益的解释能力。 忽略了盈利能力和投资行为等其他因子。 Fama-French 五因子模型 在三因子模型基础上增加盈利因子和投资因子，更全面地捕捉影响资产收益的关键变量。 模型复杂度较高，对数据要求高，仍可能遗漏某些潜在因子。 第三章 LSTM，GRU 和 BiLSTM 在股票价格预测中比较研究 3.1 实验数据介绍 国内外很多学者的研究以本国的股票指数为主，对于不同市场的单个股票研究相对较少。并且很少有研究将 LSTM、GRU、BiLSTM 这三个模型进行对比研究。因此本文分别选择国内 A 股上海浦东发展银行（简称浦发银行，代码600000）和美股 International Business Machines Corporation（简称 IBM）进行研究，这样更能准确地对我们使用的三个模型进行对比。其中浦发银行采用 2008 年 1 月 1 日到 2020 年 12 月 31 日的股票数据，共有 3114 条有效数据，数据来源于 Tushare 金融大数据平台。我们选取该数据集的日期（date）、开盘价（open）、收盘价（close）、最高价（high）、最低价（low）和成交量（volume）这 6 个特征进行实验。浦发银行的数据集除日期作为时间序列的索引以外，其他 5 个特征均作为自变量。IBM 采用 1990 年 1 月 2 日到 2018 年 11 月 15 日的股票数据，共有 7278 条有效数据，数据来源于雅虎财经。我们选取该数据集的日期（date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、调整后的收盘价（Adj Close）和成交量（volume）这 7 个特征进行实验。IBM 的数据集除日期作为时间序列的索引以外，其他 6 个特征均作为自变量。本次实验选择收盘价（close）作为被预测的变量。表3.1.1和表3.1.2分别是两个数据集的部分数据。\n3.1.1 浦发银行数据集部分展示 date open close high low volume code 2008-01-02 9.007 9.101 9.356 8.805 131583.90 600000 2008-01-03 9.007 8.645 9.101 8.426 211346.56 600000 2008-01-04 8659 9.009 9.111 8.501 139249.67 600000 2008-01-07 8.970 9.515 9.593 8.953 228043.01 600000 2008-01-08 9.551 9.583 9.719 9.517 161255.31 600000 2008-01-09 9.583 9.663 9.772 9.432 102510.92 600000 2008-01-10 9.701 9.680 9.836 9.602 217966.25 600000 2008-01-11 9.670 10.467 10.532 9.670 231544.21 600000 2008-01-14 10.367 10.059 10.433 10.027 142918.39 600000 2008-01-15 10.142 10.051 10.389 10.006 161221.52 600000 数据来源：Tushare\n3.1.2 IBM数据集部分展示 Date Open High Low Close Adj Close Volume 1990-01-02 23.6875 24.5313 23.6250 24.5000 6.590755 7041600 1990-01-03 24.6875 24.8750 24.5938 24.7188 6.649599 9464000 1990-01-04 24.7500 25.0938 24.7188 25.0000 6.725261 9674800 1990-01-05 24.9688 25.4063 24.8750 24.9375 6.708448 7570000 1990-01-08 24.8125 25.2188 24.8125 25.0938 6.750481 4625200 1990-01-09 25.1250 25.3125 24.8438 24.8438 6.683229 7048000 1990-01-10 24.8750 25.0000 24.6563 24.7500 6.658009 5945600 1990-01-11 24.8750 25.0938 24.8438 24.9688 6.716855 5905600 1990-01-12 24.6563 24.8125 24.4063 24.4688 6.582347 5390800 1990-01-15 24.4063 24.5938 24.3125 24.5313 6.599163 4035600 数据来源：雅虎财经\n3.2 实验数据预处理 3.2.1 数据的归一化 实验中各个特征在单位和量级上存在差异，比如股票价格和成交量之间量级差异巨大，会对我们实验最终预测的结果产生影响。因此我们采用 sklearn.preprocessing 库中的 MinMaxScaler 方法将数据的特征缩放至 0 到 1 之间。这样既能提升模型精度，也能提升模型收敛速度。归一化公式：\n$$ x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)} $$其中 $x^{\\prime}$ 是归一化后的数据，$x$ 是原始数据， $\\min (x)$ 是原始数据集的最小值， $\\max (x)$ 是原始数据集的最大值。之后在我们的实验过程中获得预测结果之后，还要将数据进行反归一化处理，之后才能进行对于股票价格的预测和模型评估。\n3.2.2 数据的划分 此处分别将浦发银行和 IBM 的整个实验数据集送入，设置循环核时间步（timestep）都为 60，每个时间步输入特征个数分别为 5 和 6。这样可以输入前 60 个交易日的数据，预测出第 61 天的收盘价。使我们的数据集符合之后要比较的三种神经网络模型输入的要求，依次是送入样本数，循环核时间展开步数和每个时间步输入特征个数。之后我们再将浦发银行的数据集按照 2488：311：255 的比例将归一化的数据集划分为训练集，验证集，测试集三个部分。将 IBM 的数据集按照 6550：364：304 的比例将归一化的数据集划分为训练集，验证集，测试集三个部分。我们这里划分出验证集的目的是为了方便进行调整模型的超参数以便优化各个模型之后再进行比较。\n3.3 模型网络结构 本文通过大量反复试验最终各个模型设置的网络结构如下表所示，其中层与层之间使用循环神经网络默认的 tanh 和 linear 作为激活函数，并且为了防止过拟合加入 Dropout，Dropout 的丢弃比例（rate）取值为 0.2。LSTM 和 GRU 每个循环层的神经元个数为 50，BiLSTM 循环层的神经元的个数为 100。LSTM、GRU、BiLSTM 每个模型分别采用四层 LSTM、GRU、BiLSTM 和一层全连接层，其中每层网络之间都设置了一个 Dropout。\n3.3.1 IBM的LSTM网络结构 Layer(type) Output Shape Param# lstm_1 (LSTM) (None, 60, 50) 11400 dropout_1 (Dropout) (None, 60, 50) 0 lstm_2 (LSTM) (None, 60, 50) 20200 dropout_2 (Dropout) (None, 60, 50) 0 lstm_3 (LSTM) (None, 60, 50) 20200 dropout_3 (Dropout) (None, 60, 50) 0 lstm_4 (LSTM) (None, 50) 20200 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params：72,051\nTrainable params：72,051\nNon-trainable params：0\n3.3.2 IBM的GRU网络结构 Layer(type) Output Shape Param# gru_1 (GRU) (None, 60, 50) 8550 dropout_1 (Dropout) (None, 60, 50) 0 gru_2 (GRU) (None, 60, 50) 15150 dropout_2 (Dropout) (None, 60, 50) 0 gru_3 (GRU) (None, 60, 50) 15150 dropout_3 (Dropout) (None, 60, 50) 0 gru_4 (GRU) (None, 50) 15150 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params：54,051\nTrainable params：54,051\nNon-trainable params：0\n3.3.3 IBM的BiLSTM网络结构 Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 60, 100) 22800 dropout_1 (Dropout) (None, 60, 100) 0 bidirectional_2 (Bidirection) (None, 60, 100) 60400 dropout_2 (Dropout) (None, 60, 100) 0 bidirectional_3 (Bidirection) (None, 60, 100) 60400 dropout_3 (Dropout) (None, 60, 100) 0 bidirectional_4 (Bidirection) (None, 100) 60400 dropout_4 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 1) 101 Total params：204,101\nTrainable params：204,101\nNon-trainable params：0\n3.4 模型和编译及超参数设置 本文模型在以验证集的损失函数最小为目标，进行不断的超参数调试之后，对于浦发银行的三个模型都选用 epochs=100，batch_size=32；对于 IBM 的三个模型都选用 epochs=50，batch_size=32。其中优化器都采用自适应矩估计（Adaptive moment estimation，Adam）$^{[19]}$。使用其 keras 包中的默认值，即 lr=0.001、beta_1=0.9、beta_2=0.999、epsilon=1e-08 和 decay=0.0。损失函数采用均方误差（Mean Square Error，MSE）。\n参数解释：\nlr：学习率 beta_1：一阶矩估计的指数衰减率 beta_2：二阶矩估计的指数衰减率 epsilon：模糊因子 decay：每次更新后的学习率衰减值 3.5 实验结果与分析 首先简单介绍一下模型使用评价的这几个指标。计算公式如下：\n均方误差（Mean Square Error，MSE）： $$ M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2} $$ 均方根误差（Root Mean Squared Error，RMSE)： $$ R M S E=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}} $$ 平均绝对误差（Mean Absolute Error，MAE）： $$ M A E=\\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-\\hat{Y}_{i}\\right| $$ \\( R^2 \\)（R Squared）： $$ \\begin{gathered} \\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} \\\\ R^{2}=1-\\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\end{gathered} $$其中：$n$ 是样本的数量，$Y$ 是股票实际的收盘价，$\\hat{Y}_{i}$ 是股票预测的收盘价， $\\bar{Y}$ 是股票平均的收盘价。MSE，RMSE 和 MAE 越小该模型越精确。 $R^{2}$ 评价模型系数拟合优度越大越好。\n3.5.1 浦发银行实验结果 LSTM GRU BiLSTM MSE 0.059781 0.069323 0.056454 RMSE 0.244501 0.263292 0.237601 MAE 0.186541 0.202665 0.154289 R-squared 0.91788 0.896214 0.929643 比较三个模型的评价指标，我们可以发现在浦发银行测试集上 BiLSTM 模型的 MSE、RMSE 和 MAE 都小于 LSTM 模型和 GRU 模型，而 R-Squared 都大于 LSTM 模型和 GRU 模型。我们通过对比 RMSE 发现，BiLSTM 相较于 LSTM 在验证集上有 2.90%的性能提升，BiLSTM 相较于 GRU 在验证集上有 10.81%的性能提升。\n3.5.2 IBM实验结果 LSTM GRU BiLSTM MSE 18.01311 12.938584 11.057501 RMSE 4.244186 3.597024 3.325282 MAE 3.793223 3.069033 2.732075 R-squared 0.789453 0.851939 0.883334 比较三个模型的评价指标，我们可以发现在 IBM 测试集上 BiLSTM 模型的 MSE、RMSE 和 MAE 都小于 LSTM 模型和 GRU 模型，而 R-Squared 都大于 LSTM 模型和 GRU 模型。我们通过对比 RMSE 发现，BiLSTM 相较于 LSTM 在验证集上有 27.63%的性能提升，BiLSTM 相较于 GRU 在验证集上有 8.17%的性能提升。\n3.6 本章小结 本章先是介绍了实验所选用的浦发银行和 IBM 两个数据集以及选用的特征，之后对数据集进行了归一化、数据划分的预处理步骤。同时详细的说明了实验所使用 LSTM、GRU 和 BiLSTM 模型的网络结构和超参数。最后得到了每个模型的损失函数图像和一系列的拟合图形。比较了模型的多个评价指标和拟合图像最终得到 BiLSTM 模型能够更好地对股票价格进行预测，为我们下一章研究 LightGBM-BiLSTM 的量化投资策略奠定了基础。\n第四章 基于 LightGBM-BiLSTM 的量化投资模型研究 4.1 实验步骤 Fig. 11. LightGBM-BiLSTM Diagram.\n如 Fig.11 所示，本实验先从因子库中选取 50 个因子。之后对因子依次进行去极值、标准化和缺损值填充的因子清洗步骤。再利用 LightGBM 模型进行因子选择，根据因子重要性进行排序得到前十的因子作为本横截面挑选出来的因子。紧接着使用 BiLSTM 建立多因子模型，最后再进行回测分析。\n4.2 实验数据 本文采用的行情数据来源于 Tushare。具体数据集的特征如下表所示。\n4.2.1 股票数据集包含的特征 名称 类型 描述 ts_code str 股票代码 trade_date str 交易日期 open float 开盘价 high float 最高价 low float 最低价 close float 收盘价 pre_close float 昨收价 change float 涨跌额 pct_chg float 涨跌幅（未复权） vol float 成交量（手） amount float 成交额（千元） A股全市场日线数据集包含5,872,309行数据，即包含5,872,309个样本。如表4.2.1所示，A股全市场日线数据集数据集有以下11个特征，分别依次为股票代码（ts_code）、交易日期（trade_date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、昨收价（pre_close）、涨跌额（change）、换手率（turnover_rate）、交易金额（amount）、总市值（total_mv）和复权因子（adj_factor）。\n4.2.2 A股全市场日线数据集部分展示 ts_code trade_date open high low close pre_close change vol amount 600613.SH 20120104 8.20 8.20 7.84 7.86 8.16 -0.30 4762.98 3854.1000 600690.SH 20120104 9.00 9.17 8.78 8.78 8.93 -0.15 142288.41 127992.6050 300277.SZ 20120104 22.90 22.98 20.81 20.88 22.68 -1.80 12212.39 26797.1370 002403.SZ 20120104 8.87 8.90 8.40 8.40 8.84 -0.441 10331.97 9013.4317 300179.SZ 20120104 19.99 20.32 19.20 19.50 19.96 -0.46 1532.31 3008.0594 600000.SH 20120104 8.54 8.56 8.39 8.41 8.49 -0.08 342013.79 290229.5510 300282.SZ 20120104 22.90 23.33 21.02 21.02 23.35 -2.33 38408.60 86216.2356 002319.SZ 20120104 9.74 9.95 9.38 9.41 9.73 -0.32 4809.74 4671.4803 601991.SH 20120104 5.17 5.39 5.12 5.25 5.16 0.09 145268.38 76547.7490 000780.SZ 20120104 10.42 10.49 10.00 10.00 10.30 -0.30 20362.30 20830.1761 [5872309 rows x 11 columns]\n中证全指日线数据集包含5,057行数据，即包含5,057个样本。如表4.2.2所示，中证全指日线数据集有以下7个特征，分别依次为交易日期（trade_date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、交易量（volume）和昨收价（pre_close）。\n4.2.3 中证全指日线数据集部分展示 trade_date open high low close volume pre_close 2006-11-24 1564.3560 1579.3470 1549.9790 1576.1530 7.521819e+09 1567.0910 2006-11-27 1574.1130 1598.7440 1574.1130 1598.7440 7.212786e+09 1581.1530 2006-11-28 1597.7200 1604.7190 1585.3620 1596.8400 7.025637e+09 1598.7440 2006-11-29 1575.3030 1620.2870 1575.3030 1617.9880 7.250354e+09 1596.8400 2006-11-30 1621.4280 1657.3230 1621.4280 1657.3230 9.656888e+09 1617.9880 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 2020-11-11 5477.8870 5493.5867 5422.9110 5425.8017 5.604086e+10 5494.1042 2020-11-12 5439.2296 5454.3452 5413.9659 5435.1379 4.594251e+10 5425.8017 2020-11-13 5418.2953 5418.3523 5364.2031 5402.7702 4.688916e+10 5435.1379 2020-11-16 5422.3565 5456.7264 5391.9232 5456.7264 5.593672e+10 5402.7702 2020-11-17 5454.0696 5454.0696 5395.6052 5428.0765 5.857009e+10 5456.7264 [5057 rows x 7 columns]\n下表4.2.4是原始的因子部分数据。依次经过上述因子缺失值填充、因子去极值、因子标准化和因子中性化这 4 个因子清洗的步骤后，得到如表4.2.5展示的经过因子清洗后的因子部分数据。\n4.2.4 原始的因子数据 trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 4.2.5 清洗后的因子数据 sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.2.6 因子数据 价量因子构建 本文使用如下方式构建价量因子，构建价量因子的基础要素有两点：首先是基础字段，其次是算子。如表4.2.1 所示，基础字段包括日频的最高价（high），最低价（low），开盘价（open），收盘价（close），上一日收盘价（pre_close），成交量（vol），涨跌（pct_chg），换手率（turnover_rate），交易金额（amount），总市值（total_mv）和复权因子（adj_factor）。\n4.2.7 基础字段表 编号 字段名 意义 high 最高价 当日成交订单中最高的价格 low 最低价 当日成交订单中最低的价格 open 开盘价 当日集合竞价成交的价格 close 收盘价 当日最后一笔成交订单的价格 pre_close 上一日收盘价 上一日最后一笔成交订单的价格 vol 成交 全天成交的股票数 pct_chg 涨跌 本日证券涨跌 turnover_rate 换手率 本日证券的换手率 amount 成交金额 全天成交的金额 total_mv 总市值 总股本数乘以当时股价得出的股票总价值 adj_factor 复权因子 权息修复比例 本文通过 gplearn提供的基础算子集和自己定义的一些特殊算子，得到如表所示的算子列表。\n4.2.8 算子列表 算子 名称 定义 add(x, y) 和 \\( x + y\\)；点运算 \\(\\operatorname{div}(x, y)\\) 除 \\( x / y\\)；点运算 \\(\\operatorname{mul}(x, y)\\) 乘 \\( x \\cdot y\\)；点运算 \\(\\operatorname{sub}(x, y)\\) 减 \\( x - y\\)；点运算 neg(x) 负 \\(-x\\)；点运算 \\(\\log(x)\\) 对数 \\(\\log(x)\\)；点运算 max(x, y) 最大值 \\(x, y\\) 中数值较大的数；点运算 \\(\\min(x, y)\\) 最小值 \\(x, y\\) 中数值较小的数；点运算 delta_d(x) d 日差值 当日的 \\(x\\) 值减去 d 日前的 \\(x\\) 值；时序运算 delay_d(x) d 日延时 d 日前的 \\(x\\) 值；时序运算 Corr_d(x, y) d 日相关性 d 日 \\(x\\) 值和 d 日 \\(y\\) 值的相关性；时序运算 Max_d(x) d 日最大值 d 日 \\(x\\) 值的最大值；时序运算 Min_d(x) d 日最小值 d 日 \\(x\\) 值的最小值；时序运算 sort_d(x) d 日排序位置 d 日 \\(x\\) 值的排序值；时序运算 Argsortmin_d(x) d 日最小值位置 d 日 \\(x\\) 值的最小值的位置；时序运算 Argsortmax_d(x) d 日最大值位置 d 日 \\(x\\) 值的最大值的位置；时序运算 \\(\\operatorname{inv}(x)\\) 倒数 \\( 1 / x\\)；点运算 Std_d(x) d 日方差 d 日 \\(x\\) 值的方差；时序运算 abs(x) 绝对值 \\(\\lvert x\\rvert\\)；点运算 4.2.9 遗传规划 遗传规划（Genetic Programming, GP）的核心思想是使用进化算法在算子（operators）与基础字段（terminals）组合而成的巨大搜索空间中，自动“进化”出具有较强预测能力的因子表达式。对于本文中的因子挖掘来说，GP 的主要目标是从表4.2.7中的基础字段和表4.2.8中的算子所能组合成的所有可能表达式中，搜索并找到那些能对下一期股票收益有较好预测效果的因子。GP 的核心流程可分为以下几个步骤：\n初始化（Initialization） 定义算子集与基础字段\n算子集（operators）如表4.2.8所示，包括加、减、乘、除、对数、绝对值、延时、移动最大/最小值、移动相关系数等运算。 基础字段（terminals）如表4.2.7所示，包括开盘价、收盘价、最高价、最低价、成交量、复权因子等。\n这些算子和基础字段可以视作因子表达式树中的“节点”，其中基础字段为叶子节点（终端节点），算子为内部节点。 随机生成初始种群\n在初始化阶段，根据给定的算子集与字段集，随机“拼接”生成一系列因子表达式（可表示为若干语法树或表达式树），形成初始种群。 例如，可能随机产生\n\\[ \\text{因子1}: \\mathrm{Max\\_5}\\bigl(\\mathrm{add}(\\mathrm{vol}, \\mathrm{close})\\bigr), \\quad \\text{因子2}: \\mathrm{sub}\\bigl(\\mathrm{adj\\_factor}, \\mathrm{neg}(\\mathrm{turnover\\_rate})\\bigr), \\dots \\] 每个因子表达式都将对应一个个体（individual）。 适应度函数（Fitness Function） 度量因子的预测能力\n针对每个表达式（个体），我们需要评估它对未来收益或其他目标的预测能力。具体来说，可以在下一期股票收益 \\( r^{T+1} \\) 与当前期因子暴露度 \\( x_k^T \\) 之间，计算其相关系数（IC）或更综合的指标 IR（Information Ratio）来衡量。 设定目标\n若我们希望因子具有更高的相关性（IC），则可令适应度函数为 \\(\\lvert \\rho(x_k^T, r^{T+1})\\rvert\\)； 若我们希望因子的 IR 更高，则可设定适应度函数为 IR 值。 因子 IC 或 IR 越高，该表达式的“适应度”就越高。 \\[ \\text{Fitness} \\bigl(F(x)\\bigr) \\;=\\; \\begin{cases} \\lvert \\rho(x_k^T, r^{T+1})\\rvert \\quad \u0026\\text{(IC最大化)},\\\\[6pt] \\mathrm{IR}(x_k^T) \\quad \u0026\\text{(IR最大化)}. \\end{cases} \\] 其中 \\(\\rho(\\cdot)\\) 表示相关系数，\\(\\mathrm{IR}(\\cdot)\\) 为 IR 指标。\n选择（Selection）、交叉（Crossover）与变异（Mutation） 选择（Selection）\n根据适应度函数的结果，将因子适应度高的表达式“保留”或“繁衍”，适应度较低的表达式则被淘汰。 这类似于生物进化中的“优胜劣汰”。 交叉（Crossover）\n将若干适应度较高的表达式（父本）随机选取一部分“节点”进行交换，从而得到新的表达式（子本）。 在表达式树结构中，可以将子树 A 与子树 B 互换，从而产生新的后代表达式。 例如，若表达式树 \\(\\mathrm{FactorA}\\) 的某个子树与表达式树 \\(\\mathrm{FactorB}\\) 的对应子树相交换，就生成了两个新的表达式。 变异（Mutation）\n以一定概率对表达式的某些节点进行随机变更，比如： 更换节点的算子（例如将 \\(\\mathrm{add}\\) 换为 \\(\\mathrm{sub}\\)）， 替换终端节点的基础字段（例如将 \\(\\mathrm{vol}\\) 换为 \\(\\mathrm{close}\\)）， 或随机改变运算参数（如移动窗口长度、平滑因子等）。 变异可以增加群体的多样性，避免过早收敛或陷入局部最优。 迭代进化（Iteration） 循环执行\n将选择、交叉、变异的操作反复执行多代（generations）。 每一代都产生一个新的因子表达式种群，并对其进行适应度评估。 收敛与终止\n当进化达到预先设定的停止条件（如迭代次数、适应度阈值等）时，算法终止。 通常我们会选出若干个最终适应度较高的因子表达式，将它们视为进化结果。 数学表征：搜索最优因子表达式 将上述过程抽象成下式，可以简单表示因子的搜索目标：\n\\[ F(x) \\;=\\; \\mathrm{GP}\\bigl(\\{\\text{operators}\\}, \\{\\text{terminals}\\}\\bigr), \\] 表示通过 GP 算法在给定算子集（operators）和基础字段集（terminals）上搜索出一个函数 \\(F(x)\\)。从最优化的角度看，我们希望找到：\n\\[ \\max_{F} \\bigl\\lvert \\rho(F^T, r^{T+1}) \\bigr\\rvert \\quad \\text{或者} \\quad \\max_{F} \\; \\mathrm{IR}\\bigl(F\\bigr), \\] 其中\n\\(\\rho(\\cdot)\\) 表示因子与下一期收益的相关系数（IC）， \\(\\mathrm{IR}(\\cdot)\\) 表示该因子的 IR 指标。 在实际应用中，我们会给定一段回测期，对每一代的候选因子进行打分（IC/IR 评估），通过选择、交叉和变异的迭代过程不断“进化”出更优质的因子。\n通过以上步骤，我们最终能够在庞大的算子组合与基础字段组合的搜索空间中，自动挖掘到一批对未来收益有较强预测能力、且具有较好稳健性（如 IR 较高）的因子表达式。\n4.2.10 挖掘出的部分因子 因子名 定义 0 Max＿25(add(turnover_rate, vol)) 1 Max＿30(vol) 2 Max＿25(turnover_rate) 3 Max＿35(add(vol, close)) 4 Max＿30(turnover_rate) 5 sub(Min＿20(neg(pre_close)), div(vol, adj_factor)) 6 Max＿60(max(vol, adj_factor)) 7 Max＿50(amount) 8 div(vol, neg(close)) 9 min(ArgSortMin＿25(pre_close), neg(vol)) 10 neg(max(vol, turnover_rate)) 11 mul(amount, neg(turnover_rate)) 12 inv(add(ArgSortMax＿40(change), inv(pct_chg))) 13 Std＿40(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))) 14 div(log(total_mv),amount) 15 div(neg(Max＿5(amount)), Min＿20(ArgSort＿60(high))) 16 Corr＿30(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))), add(log(Max＿10(pre_close)), high)) 17 ArgSort＿60(neg(turnover_rate)) \u0026hellip; \u0026hellip; 这些因子均是通过遗传规划从算子列表（表4.2.8）与基础字段列表（表4.2.7）中组合而得，具有不同的数学表达形式。\n因子有效性检验 当我们得到挖掘的因子之后，需要对因子进行有效性检验，常见的检验指标有信息系数（Information Coefficient，IC）和信息比率（Information Ratio，IR）。\n信息系数（IC）描述的是所选股票下期收益率和本期因子暴露度的线性相关程度，可以反应该因子进行收益率预测的稳健性 **信息比率（IR）**是超额收益的均值与超额收益的标准差之比，信息比率与夏普比率类似，主要区别在于夏普比率使用无风险收益作为基准，而信息比率使用风险指数作为基准。夏普比率有助于确定投资组合的绝对收益，信息比率有助于确定投资组合的相对收益。当我们计算了 IC 之后，可以根据 IC 的值再对 IR 进行计算。当 IR 大于 0.5 时，因子稳定获取超额收益能力较强。 实际计算中，因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 值一般是指所选股票第 \\(T\\) 期的因子 \\(k\\) 上的暴露度 \\( x_k^T \\) 与所选股票第 \\(T+1\\) 期的收益率 \\( r^{T+1} \\) 的相关系数；因子 \\(k\\) 的 \\( \\mathrm{IR} \\) 值为因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 的均值除以因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 的标准差，计算公式如下：\n$$ \\begin{gathered} I C=\\rho_{x_{k}^{T}, r^{T+1}}=\\frac{\\operatorname{cov}\\left(x_{k}^{T}, r^{T+1}\\right)}{\\sigma_{x_{k}^{T}} \\sigma_{r^{T+1}}}=\\frac{\\mathrm{E}\\left(x_{k}^{T} * r^{T+1}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right) \\mathrm{E}\\left(r^{T+1}\\right)}{\\sqrt{\\mathrm{E}\\left(\\left(x_{k}^{T}\\right)^{2}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right)^{2}} \\cdot \\sqrt{\\mathrm{E}\\left(\\left(r^{T+1}\\right)^{2}\\right)-\\mathrm{E}\\left(r^{T+1}\\right)^{2}}} \\\\ I R=\\frac{\\overline{I C}}{\\sigma_{I C}} \\end{gathered} $$其中：\n$x_{k}^{T}$ ：所选股票第 $T$ 期的因子 $k$ 上的暴露度 $r^{T+1}$ ：所选股票第 $T+1$ 期的收益率 $\\overline{I C}: I C$ 的均值 本文采用 IR 判断因子好坏，通过对大量不同的算子和基础数据的组合以及 IC 和 IR 的“篮选”，文章得到了本文所选用的 50 个价量因子。经过 IR 检测，按 IR 由高到低排序得到如下图所示的表格。从下表中我们可以看出来所选的 50 个价量因子的 IR 都大于 0.5 ，说明这些因子稳定获取超额收益能力较强。\n4.2.11 因子 IR 检验表 因子名 IR 因子名 IR 0 3.11 25 2.73 1 2.95 26 2.71 2 2.95 27 2.70 3 2.95 28 2.69 4 2.95 29 2.69 5 2.94 30 2.69 6 2.94 31 2.68 7 2.94 32 2.68 8 2.93 33 2.68 9 2.93 34 2.68 10 2.93 35 2.67 11 2.92 36 2.67 12 2.91 37 2.66 13 2.89 38 2.65 14 2.86 39 2.65 15 2.83 40 2.65 16 2.83 41 2.65 17 2.83 42 2.64 18 2.79 43 2.63 19 2.78 44 2.63 20 2.78 45 2.62 21 2.76 46 2.62 22 2.75 47 2.62 从该表可见，在所筛选的因子中，所有因子的 IR 均大于 0.5，具有较强且稳定的获取超额收益的能力。\n4.3 因子清洗 4.3.1 因子缺失值处理和去极值 对于因子的缺失值处理的方法有个案剔除法，均值替换法，回归替换法等方法。本文采用较为简单的均值替换法对缺损值进行处理，即利用因子的平均值来替代缺失的数据。对于因子去极值有中位数去极值，百分比去极值和 $3 \\sigma$ 去极值等方法。本文采用的是 $3 \\sigma$ 去极值法，该方法是利用统计学上的 $3 \\sigma$ 原则，将把离该因子均值三个标准差以上的极值因子转化到刚好离均值三个标准差的位置，具体计算公式如下：\n$$ X_i^{\\prime}= \\begin{cases} \\bar{X}+3 \\sigma \u0026 \\text{if } X_i \u003e \\bar{X} + 3 \\sigma \\\\ \\bar{X}-3 \\sigma \u0026 \\text{if } X_i \u003c \\bar{X} - 3 \\sigma \\\\ X_i \u0026 \\text{if } \\bar{X} - 3 \\sigma \u003c X_i \u003c \\bar{X} + 3 \\sigma \\end{cases} $$其中：\n$X_{i}$ ：因子处理之前的值 $\\bar{X}$ ：因子序列的均值 $\\sigma$ ：因子序列的标准差 $X_{i}^{\\prime}$ ：去极值后的因子的值 4.3.2 因子的标准化 本文实验选取了多个因子，并且各个因子量纲并不完全相同，为了我们方便进行比较和回归，我们还需对因子进行标准化处理。目前常用的具体的标准化方法有 Min－Max标准化，Z－score 标准化和 Decimal scaling 小数定标标准化等。本文选择 Z－score 标准化的方法。通过原始数据的均值和标准差进行数据的标准化，经过处理的数据符合标准正态分布，即均值为 0 ，标准差为 1 ，其标准化后的数值大小有正有负，得到标准正态分布曲线。\n本文采用 Z－score 标准化公式如下：\n$$ \\tilde{x}=\\frac{x_{i}-u}{\\sigma} $$其中：\n$x_{i}$ ：因子的原值 $u$ ：因子序列的均值 $\\sigma$ ：因子序列的标准差 $\\tilde{x}$ ：标准化后的因子值 4.3.3 因子的中性化 因子中性化是为了剔除其他因素对我们所选因子的影响，使我们构建量化投资策略组合所选择的股票更加分散，而不是集中在特定的行业或者市值的股票上，可以更好地分担投资组合的风险和解决因子多重共线性的问题。市值和行业是影响股票收益最主要的两种自变量，所以在进行因子清洗的过程中，还必须考虑市值和行业的影响。本文实证中我们采用仅纳入行业因子，而将市场因子包含在行业因子中的方法。针对因子的单因子回归模型见公式(31)，我们将以下回归模型的残差项作为因子中性化后的新的因子值。\n$$ \\tilde{r}_{j}^{t}=\\sum_{s=1}^{s} X_{j s}^{t} \\tilde{f}_{s}^{t}+X_{j k}^{t} \\tilde{f}_{k}^{t}+\\tilde{u}_{j}^{t} $$其中：\n$\\tilde{r}_{j}^{t}$ ：股票 $j$ 在第 $t$ 期的收益率 $X_{j s}^{t}$ ：股票 $j$ 在第 $t$ 期在行业 $s$ 上的暴露度 $\\tilde{f}_{s}^{t}$ ：行业在第 $t$ 期的收益率 $X_{j k}^{t}$ ：股票 $j$ 在第 $t$ 期在因子 $k$ 上的暴露度 $\\tilde{f}_{k}^{t}$ ：因子 $k$ 在第 $t$ 期的收益率 $\\tilde{u}_j^t$ ：一个 $0-1$ 哑变量，即如果股票 $j$ 属于行业 $s$ ，则暴露度为 1 ，否则为 0 在本文中，并不会对公司所属行业进行按照比例拆分，即股票 $j$ 只能属于一个特定的行业 $s$ ，在行业 $s$ 上的暴露度为 1 ，在其他所有行业的暴露度为 0 。本文使用申万宏源行业分类标准，具体分类情况依次为农林牧渔，采掘，化工，钢铁，有色金属，电子元器件，家用电器，食品饮料，纺织服装，轻工制造，医药生物，公用事业，交通运输，房地产，商业贸易，餐饮旅游，综合，建筑材料，建筑装饰，电器设备，国防军工，计算机，传媒，通信，银行，非银金融，汽车和机械设备这 28 类。下表为 2021 年 2 月 5 日的申万指数一级行业历史行情图。\n4.3.3.1 2021年2月5日的申万指数一级行业历史行情图 指数代码 指数名称 发布日期 开盘指数 最高指数 最低指数 收盘指数 成交量(亿殴) 成交额(亿元) 涨跌幅(%) 801010 农林牧渔 2021/2/5 0:00 4111.43 4271.09 4072.53 4081.81 15.81 307.82 -0.3 801020 采掘 2021/2/5 0:00 2344.62 2357.33 2288.97 2289.41 18.06 115.6 -2.25 801030 化工 2021/2/5 0:00 4087.77 4097.59 3910.67 3910.67 55.78 778.85 -3.95 801040 钢铁 2021/2/5 0:00 2253.78 2268.17 2243.48 2250.81 11.61 48.39 -1.02 801050 有色金属 2021/2/5 0:00 4212.1 4250.59 4035.99 4036.74 45.41 593.92 -4.43 801080 电子元器件 2021/2/5 0:00 4694.8 4694.8 4561.95 4561.95 52.67 850.79 -2.78 801110 家用电器 2021/2/5 0:00 10033.82 10171.26 9968.93 10096.83 8.55 149.18 0.83 801120 食品饮料 2021/2/5 0:00 30876.33 31545.02 30649.57 30931.69 11.32 657.11 0.47 801130 纺织服装 2021/2/5 0:00 1614.48 1633.89 1604.68 1607.63 6.28 57.47 -0.39 801140 轻工制造 2021/2/5 0:00 2782.07 2791.88 2735.48 2737.24 15.28 176.16 -1.35 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 数据来源：申银万国\n下表是原始的因子部分数据。依次经过上述因子缺失值填充、因子去极值、因子标准化和因子中性化这 4 个因子清洗的步骤后，得到如表展示的经过因子清洗后的因子部分数据。\n4.3.3.2 原始的因子数据 trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 4.3.3.3 清洗后的因子数据 sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.4 基于 LightGBM 的因子选择 4.4.1 GBDT Friedman（2001）$^{[20]}$ 提出的梯度提升决策树（Gradient Boosting Decision Tree，GBDT）是一种基于迭代的回归型决策树。其主要思想是通过逐步添加弱分类器（通常是决策树）来优化模型，使得整体模型能够最小化损失函数。GBDT 的模型可以表示为：\n$$ \\hat{y} = \\sum_{m=1}^{M} \\gamma_m h_m(\\mathbf{x}) $$其中：\n\\( M \\) 是迭代次数， \\( \\gamma_m \\) 是第 \\( m \\) 个弱分类器的权重， \\( h_m(\\mathbf{x}) \\) 是第 \\( m \\) 个决策树模型。 GBDT 的训练过程通过逐步拟合负梯度方向来最小化损失函数，具体更新公式为：\n$$ \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{N} L\\left(y_i, \\hat{y}_{i}^{(m-1)} + \\gamma h_m(\\mathbf{x}_i)\\right) $$其中，\\( L \\) 是损失函数，\\( y_i \\) 是真实值，\\( \\hat{y}_{i}^{(m-1)} \\) 是第 \\( m-1 \\) 次迭代后的预测值。\n4.4.2 LightGBM 轻量级梯度提升机（Light Gradient Boosting Machine，LightGBM)$^{[21]}$ 是一个高效实现 GBDT 算法的框架，最初由 Microsoft 开发，作为一个免费开源的分布式梯度提升框架。LightGBM 基于决策树算法，广泛应用于排名、分类及其他机器学习任务，开发重点在于性能和可伸缩性。其主要优势包括高效率的并行训练、更快的训练速度、更低的内存消耗、更好的准确率，以及支持分布式计算和快速处理海量数据$^{[22]}$。\nLightGBM 的核心算法基于以下优化目标：\n$$ L = \\sum_{i=1}^{N} l(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(h_m) $$其中，\\( l \\) 是损失函数，\\( \\Omega \\) 是正则化项，用于控制模型复杂度，通常表示为：\n$$ \\Omega(h_m) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 $$这里，\\( T \\) 是树的叶子数，\\( w_j \\) 是第 \\( j \\) 个叶子的权重，\\( \\gamma \\) 和 \\( \\lambda \\) 是正则化参数。\nLightGBM 采用基于梯度的单边采样（Gradient-based One-Side Sampling，GOSS）和互斥特征捆绑（Exclusive Feature Bundling，EFB）等技术，显著提升了训练效率和模型性能。\n在本研究中，训练过程中使用的损失函数为均方误差（Mean Squared Error，MSE），其定义为：\n$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$其中，\\( y \\) 为真实收益率，\\( \\hat{y} \\) 为模型预测的收益率，\\( N \\) 为样本数量。\n4.4.3 算法流程 本小节算法的具体流程如下：\n数据准备：使用一年的每只股票的 50 个因子数据（A 股全市场数据）和历史未来一个月的收益率作为特征。\n模型训练：利用网格搜索法（Grid Search）优化 LightGBM 模型的超参数，训练模型以预测未来一个月的收益率。模型训练流程如图4.12所示。\n$$ \\text{参数优化：} \\quad \\theta^* = \\arg\\min_\\theta \\sum_{i=1}^{N} L(y_i, \\hat{y}_i(\\theta)) $$其中，\\( \\theta \\) 表示模型的超参数集合，\\( \\theta^* \\) 为最优参数。\n因子重要性计算：使用 LightGBM 的 feature_importances_ 方法计算各因子的特征重要性。特征重要性主要通过两个指标衡量：\nSplit：该特征在所有树中被用于分裂的次数。 Gain：该特征在所有分裂中带来的总增益（即损失函数的减少量）。 因子的特征重要性可以表示为：\n$$ \\text{Importance}_{\\text{split}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\mathbb{I}(f \\text{ 被用于第 } j \\text{ 个叶节点的分裂}) $$$$ \\text{Importance}_{\\text{gain}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\Delta L_{m,j} \\cdot \\mathbb{I}(f \\text{ 被用于第 } j \\text{ 个叶节点的分裂}) $$其中，\\( \\mathbb{I} \\) 是指示函数，\\( \\Delta L_{m,j} \\) 是因子 \\( f \\) 在第 \\( m \\) 棵树的第 \\( j \\) 个分裂中带来的损失减少量。\n因子筛选：根据模型计算的因子重要性进行排序，选择前十个重要性最高的因子作为本横截面分析中使用的因子。所选因子的重要性如表4.4.4所示。\n4.4.4 部分所选因子重要性排序 importance feature_name trade_date 35 factor_35 2010-08-11 27 factor_27 2010-08-11 33 factor_33 2010-08-11 20 factor_20 2010-08-11 24 factor_24 2010-08-11 45 factor_45 2010-08-11 37 factor_37 2010-08-11 49 factor_49 2010-08-11 19 factor_19 2010-08-11 47 factor_47 2010-08-11 22 factor_22 2010-09-09 20 factor_20 2010-09-09 30 factor_30 2010-09-09 24 factor_24 2010-09-09 4.4.5 代码实现片段 以下是训练过程所使用的部分代码，用于因子选择。\nfeature_choice def feature_choice( self, days=21, is_local=False ): if is_local: feature_info = pd.read_hdf(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;)) else: factors = self.get_env().query_data(Factors_Data) factors = factors[ factors[COM_DATE] \u0026gt;= \u0026#39;2010-01-01\u0026#39; ] trade_list = list(set(factors[COM_DATE])) trade_list.sort() if len(trade_list) % days == 0: n = int(len(trade_list) / days) - 7 else: n = int(len(trade_list) / days) - 6 feature_info = pd.DataFrame() begin_index = 147 feature = list(factors.columns) feature.remove(COM_SEC) feature.remove(COM_DATE) feature.remove(Ret) for i in range(n): end_date = days * i + begin_index - 21 begin_date = days * i trade_date = days * i + begin_index print(trade_list[trade_date]) train_data = factors[ (factors[COM_DATE] \u0026lt;= trade_list[end_date]) \u0026amp; (factors[COM_DATE] \u0026gt;= trade_list[begin_date]) ] model = lgb.LGBMRegressor() model.fit(train_data[feature], train_data[Ret]) feature_info_cell = pd.DataFrame(columns=Info_Fields) feature_info_cell[Importance] = model.feature_importances_ feature_info_cell[Feature_Name] = model.feature_name_ feature_info_cell = feature_info_cell.sort_values(by=Importance).tail(10) feature_info_cell[COM_DATE] = trade_list[trade_date] feature_info = pd.concat( [feature_info, feature_info_cell], axis=0 ) h = pd.HDFStore(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;), \u0026#39;w\u0026#39;) h[\u0026#39;data\u0026#39;] = feature_info h.close() self.get_env().add_data(feature_info, Feature_Info) pass 通过上述流程，利用 LightGBM 高效地筛选出对预测未来收益率最具影响力的因子，从而提升模型的预测能力和解释性。\n4.5 基于 BiLSTM 的因子组合 本小节使用 BiLSTM 进行因子组合。BiLSTM 的具体原理在第二章已经介绍了，这里不再赘述。下面先介绍一下使用模型的具体网络结构，本文通过大量反复试验最终 BiLSTM 设置的网络结构如表4.5.1所示。其中层与层之间使用循环神经网络默认的 tanh 和 linear 作为激活函数。并且为了防止过拟合加入 Dropout，但是如果 Dropout 使用过大的丢弃比例会出现欠拟合的现象，因此 Dropout 的丢弃比例取值为 0.01。最终模型的 BiLSTM 循环层的神经元个数为 100，采用一层 BiLSTM 层和三层全连接层，其中 BiLSTM 层和第一个全连接层之间设置了一个 Dropout。\n4.5.1 BiLSTM的网络结构 Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 100) 24400 dropout_1 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 256) 25856 dropout_2 (Dropout) (None, 256) 0 dense_2 (Dense) (None, 64) 16448 dense_3 (Dense) (None, 1) 0 Total params：66,769\nTrainable params：66,769\nNon-trainable params：0\n因为本次实验使用数据的数据量较大，因此选用 epochs=400，batch_size=1024。模型的损失函数采用均方误差（Mean Square Error，MSE）。其中优化器采用随机梯度下降（Stochastic Gradient Descent，SGD）。随机梯度下降相对于梯度下降（Gradient Descent，GD）有在信息冗余的情况下更能有效地利用信息，前期迭代效果卓越，适合处理大样本的数据这三个优势 $^{[23]}$。由于本实验训练数据量较大，使用 SGD 的话每次仅用一个样本来迭代，训练的速度很快，可以大大减少我们训练所花费的时间。使用其 keras 包中的默认值，即 lr=0.01、momentum=0.0、decay=0.0 和 nesterov=False。\n参数解释：\nlr：学习率 momentum：动量参数 decay：每次更新后的学习率衰减值 nesterov：确定是否使用 Nesterov 动量 4.5.2 算法流程 本小节算法的具体流程如下：\n使用一年的每只股票的 10 个因子（LightGBM 选出来的因子）和历史未来一个月的收益率的 A 股全市场数据作为特征。 以一年每支股票未来一个月的收益率为预测目标，利用 BiLSTM 进行训练，如 Fig. 12 所示。 Fig. 12. Rolling Window\n一个月的样本外数据的实时因子数据通过训练好的 BiLSTM 模型，得到实时的未来一月的每只股票预期收益率。收益率如表4.11所示。 4.5.3 部分股票预测收益率表 sec_code trade_date y_hat 000001.SZ 2011/5/26 0.0424621 000002.SZ 2011/5/26 -0.1632174 000004.SZ 2011/5/26 -0.0642319 000005.SZ 2011/5/26 0.08154649 000006.SZ 2011/5/26 0.00093213 000007.SZ 2011/5/26 -0.073218 000008.SZ 2011/5/26 -0.0464256 000009.SZ 2011/5/26 -0.091549 000010.SZ 2011/5/26 0.08154649 000011.SZ 2011/5/26 -0.1219943 000012.SZ 2011/5/26 -0.1448984 000014.SZ 2011/5/26 0.09038845 000016.SZ 2011/5/26 -0.11225 4.5.4 代码实现片段 以下是训练过程所使用的部分代码，用于构建BiLSTM训练网络。\nbuild_net_blstm def build_net_blstm(self): model = ks.Sequential() model.add( ks.layers.Bidirectional(ks.layers.LSTM( 50 ),input_shape=(11,10)) ) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(256)) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(64)) model.add(ks.layers.Dense(1)) model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;mse\u0026#39;) model.summary() self.set_model(model) 4.6 量化策略和策略回测 4.6.1 回测指标 下面先对策略的一些常见回测指标进行介绍。评价指标包括累计收益率（Total Rate of Return）、年化收益率（Annualized Rate of Return）、年化波动率（Annualized volatility）、夏普比率（Sharpe ratio）、最大回撤率 (Maximum Drawdown，MDD)、年化换手率（Annualized turnover rate）和年化交易成本率（Annualized transaction cost rate），其中假定一年股市开盘252天，无风险利率默认为0.035，手续费默认0.002。\n累计收益率（Total Rate of Return）：在其他指标相同的情况下，累计收益率越大说明该策略越好，越能带来更大的收益。公式如下： $$ \\text{Total Rate of Return} = r_{p} = \\frac{P_{1} - P_{0}}{P_{0}} $$$P_{1}$ ：最终股票和现金的总价值\n$P_{0}$ ：初始股票和现金的总价值\n年化收益率（Annualized Rate of Return）：是将累计总收益率换算成以年为单位的几何平均收益率。在其他指标相同的情况下，年化收益率越大，代表该策略越好。公式如下： $$ \\text{Annualized Rate of Return} = R_{p} = \\left(1 + r_{p}\\right)^{\\frac{252}{t}} - 1 $$$r_{p}$ ：累计收益率\n$t$ ：投资策略执行的天数\n年化波动率（Annualized volatility）：定义为对象资产的年回报率的对数值的标准差。年化波动率用来衡量策略的风险性，波动率越大代表策略的风险越高。公式如下： $$ \\begin{aligned} \\text{Annualized volatility} = \\sigma_{p} \u0026= \\sqrt{\\frac{252}{t-1} \\sum_{i=1}^{t}\\left(r_{d} - \\bar{r}_{d}\\right)^{2}} \\\\ \\bar{r}_{d} \u0026= \\frac{1}{t} \\sum_{i=1}^{t} r_{d_{i}} \\end{aligned} $$$r_{d_{i}}$ ：第 $i$ 天日收益率\n$\\bar{r}_{d}$ ：日平均收益率\n$t$ ：投资策略执行的天数\n夏普比率（Sharpe ratio）：是由 Sharpe（1966）$^{[24]}$ 提出的。它代表投资者额外承受一单位风险，所获得的超额收益$^{[25]}$。这里给出年化的夏普比率计算公式： $$ S = \\frac{R_{p} - R_{f}}{\\sigma_{p}} $$$R_{p}$ ：年化收益率\n$R_{f}$ ：无风险收益率\n$\\sigma_{p}$ ：年化波动率\n最大回撤率 (Maximum Drawdown，MDD)：表示表示我们策略运行期间股票和现金的总价值走到最低点时的收益率回撤幅度的最大值。最大回测率用来策略最极端可能的亏损情况。 $$ MDD = \\frac{\\max \\left(V_{x} - V_{y}\\right)}{V_{x}} $$$V_{x}$ 和 $V_{y}$ 分别为策略组合在第 $x$ 天和第 $y$ 天的股票和现金的总价值，且 $x \u0026lt; y$。\n年化换手率（Annualized turnover rate）：用来衡量对投资组合里面的股票买卖的频繁程度。越大说明改投资组合换仓越频繁，交易成本也会越大。 $$ \\text{change} = \\frac{N \\times 252}{t} $$$t$ ：投资策略执行的天数\n$N$ ：总共买进和卖出的次数\n年化交易成本率（Annualized transaction cost rate）：用来衡量投资组合策略的交易成本，越大说明交易成本越高。 $$ c = \\left(1 + \\text{commison}\\right)^{\\text{change}} - 1 $$change：年化换手率\ncommison：手续费\n4.6.2 策略及回测结果 本文量化交易策略采用每隔一个月进行换仓（即调仓周期为28个交易日），每次换仓采取等额持股的方式买入 BiLSTM 预测出的预期收益率最高的25支股票，卖出原本所持有的股票。本文的回测时间和规则如下：\n回测时间：从 2012 年 1 月到 2020 年 10 月。 回测股票池：全 A 股，剔除特别处理（Special treatment，ST）股票。 交易手续费：买入时支付给券商交易佣金千分之二，卖出时支付给券商交易佣金千分之二，其中单笔交易佣金不满5元券商按5元收取交易佣金。 买卖规则：当天开盘涨停股票不能买入，跌停股票不能卖出。 4.6.2.1 策略回测结果 累计收益率 年化收益率 年化波动率 夏普比率 最大回撤 年化换手率 年化交易成本率 策略 701.00% 29.18% 33.44% 0.77 51.10% 51.10% 11.35% 基准 110.40% 9.70% 26.01% 0.24 58.49% 58.49% 0.00% Fig. 22. Net Profit Curve\n回测结果如上表和 Fig.22 所示。我的策略采用的是本章所介绍的 LightGBM-BiLSTM 量化策略。基准采用的中证全指（000985）。由上面的结果可以看到，本策略累计收益率为701.00%，远高于基准110.40%；年化收益率为29.18%，远高于基准9.70%；夏普率为0.77，高于基准0.24。这三项回测指标说明 LightGBM-BiLSTM 量化策略确实能够给投资者带来更大的收益。本策略年化波动率为33.44%大于基准26.01%，最大回撤为51.10%小于基准58.49%，这两项回测指标说明 LightGBM-BiLSTM 量化策略存在一定的风险，特别是很难抵御系统性风险的冲击。年化换手率为11.35%，年化交易成本率为2.29%，说明我们策略不是高频交易策略，交易成本较小。从收益曲线图可以看出 LightGBM-BiLSTM 量化策略在前两年的收益率和基准相差不大，并没有特别的优势。但从2015年4月左右开始 LightGBM-BiLSTM 量化策略的收益率明显好于基准的收益率。总体而言，该 LightGBM-BiLSTM 量化策略的收益率十分可观，但仍然存在一定的风险。\n第五章 总结与展望 5.1 总结 本文首先介绍了基于深度学习的股票价格预测和量化策略研究的研究背景和研究意义，然后分别介绍了股票价格预测和量化投资策略国内外的研究现状，之后说明了本文的创新点和研究框架。接着本论文在相关理论基础章节大致介绍了本文用到的深度学习模型和量化投资的发展历程。重点介绍了 LSTM，GRU，BiLSTM 这三个模型的基本结构，基本原理和特点。\n随后，本文利用浦发银行和 IBM 的日频数据，通过一系列的数据处理过程和特征提取来对数据进行预处理。然后介绍了 LSTM，GRU，BiLSTM 这三个模型的具体网络结构以及超参数的设定。紧接着我们使用 LSTM，GRU，BiLSTM 分别进行两只股票收盘价的预测和模型评估比较。实验结果表明对于两只股票而言都是 BiLSTM 预测效果更加准确。\n最后，本论文为了进一步说明 BiLSTM 在金融上的运用价值，构建了基于 LightGBM－BiLSTM 的量化投资模型。选取 A 股全市场的股票和多个因子依次进行因子清洗，基于 LightGBM 的因子选择和基于 LSTM 的因子组合等过程。接着，我们构建一定的投资策略并通过累计收益率，年化收益率，年化波动率和夏普比率等评估指标与基准的持有中证全指进行对比。通过对比发现 LightGBM－BiLSTM 量化投资模型能带来更好的收益，说明了利用深度学习构建量化投资策略的有效性。\n5.2 展望 本文虽然分别对比 LSTM，GRU，BiLSTM 这三个模型预测股票收盘价的效果和基于 LightGBM－BiLSTM 量化投资策略取得了一定的成果，但本文研究仍有一些不足之处。结合本文的研究成果，可以进一步进行以下研究和改进：\n预测目标多样化：本文在预测股票价格方面，选取的股票收盘价作为预测目标，虽然这一结果最直观，但 Bachelier（1900）$^{[26]}$ 提出的随机游走假说（Random Walk Hypothesis，RWH）认为股票的价格服从随机漫步，是不可预测的。虽然之后有许多行为经济学家证明这一观点不完全正确，但这也同时说明单纯预测股票的收盘价难度和可解释性不那么强 $^{[27][28]}$。因此可以选择股票波动率预测，股票涨跌判断和股票收益率预测等作为未来的研究的方向。 模型多样化对比：本文在预测股票价格方面，对比了 LSTM，GRU 和 BiLSTM 这三种循环神经网络模型并且说明了 BiLSTM 预测效果比较好，但仍然缺少和其他更多不同模型的对比研究。因此未来可以深入研究与 Autoregressive Integrated Moving Average (ARIMA)，卷积神经网络（Convolutional Neural Networks，CNN），深度神经网络（Deep Neural Networks，DNN）, CNN－LSTM, Transformer 和 TimeGPT 等单一或复合模型之间的对比。 因子多样化：本文在构建量化投资策略方面使用的因子都是技术面的价量因子，因子的种类单一。未来可以选择财务因子，情绪因子，成长因子等不同种类的因子，从而提高策略的性能。同时未来研究还可以适当的加入择时策略，在预测大盘上涨时增加仓位，在预测大盘下跌时减少仓位，赚取贝塔（beta，$\\beta$）的钱。 投资组合优化：本文的因子组合过程仍然不完善，未来可以利用二次规划的方法对投资组合进行优化。 高频交易策略研究：本文的量化投资策略方法采取的是低频交易的策略，未来可以利用股票的 tick 数据来研究高频策略和超高频策略。 参考文献 [1] White, H. “Economic prediction using neural networks: The case of IBM daily stock returns.” Proc. of ICNN. 1988, 2: 451-458.\n[2] Kimoto, T., Asakawa, K., Yoda, M., et al. “Stock market prediction system with modular neural networks.” Proc. of 1990 IJCNN International Joint Conference on Neural Networks. IEEE, 1990: 1-6.\n[3] Zhang, G. P. “Time series forecasting using a hybrid ARIMA and neural network model.” Neurocomputing. 2003, 50: 159-175.\n[4] Akita, R., Yoshihara, A., Matsubara, T., et al. “Deep learning for stock prediction using numerical and textual information.” Proc. of 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS). IEEE, 2016: 1-6.\n[5] 宮崎邦洋, 松尾豊. “Deep Learning を用いた株価予測の分析.” 人工知能学会全国大会論文集 第31回全国大会. 一般社団法人 人工知能学会, 2017: 2D3OS19a3-2D3OS19a3.\n[6] Kim, T., Kim, H. Y. “Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data.” PLoS ONE. 2019, 14(2): e0212320.\n[7] Hochreiter, S., Schmidhuber, J. “Long short-term memory.” Neural Computation. 1997, 9(8): 1735-1780.\n[8] Cho, K., Van Merriënboer, B., Gulcehre, C., et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078. 2014.\n[9] Chung, J., Gulcehre, C., Cho, K. H., et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” arXiv preprint arXiv:1412.3555. 2014.\n[10] Gruber, N., Jockisch, A. “Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?” Frontiers in Artificial Intelligence. 2020, 3(40): 1-6.\n[11] Markowitz, H. “Portfolio Selection.” The Journal of Finance. 1952, 7(1): 77-91. doi:10.2307/2975974.\n[12] Merton, R. C. “An analytic derivation of the efficient portfolio frontier.” Journal of Financial and Quantitative Analysis. 1972: 1851-1872.\n[13] Sharpe, W. F. “Capital asset prices: A theory of market equilibrium under conditions of risk.” The Journal of Finance. 1964, 19(3): 425-442.\n[14] Lintner, J. “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.” Review of Economics and Statistics. 1965, 47(1): 13-37.\n[15] Mossin, J. “Equilibrium in a capital asset market.” Econometrica: Journal of the Econometric Society. 1966: 768-783.\n[16] Ross, S. A. “The arbitrage theory of capital asset pricing.” Journal of Economic Theory. 1976, 13(3): 341-60.\n[17] Fama, E. F., French, K. R. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics. 1993, 33(1): 3-56.\n[18] Fama, E. F., French, K. R. “A five-factor asset pricing model.” Journal of Financial Economics. 2015, 116(1): 1-22.\n[19] Kingma, D. P., Ba, J. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980. 2014.\n[20] Friedman, J. H. “Greedy function approximation: A gradient boosting machine.” Annals of Statistics. 2001: 1189-1232.\n[21] Kopitar, L., Kocbek, P., Cilar, L., et al. “Early detection of type 2 diabetes mellitus using machine learning-based prediction models.” Scientific Reports. 2020, 10(1): 1-12.\n[22] Ke, G., Meng, Q., Finley, T., et al. “Lightgbm: A highly efficient gradient boosting decision tree.” Advances in Neural Information Processing Systems. 2017, 30: 3146-3154.\n[23] Bottou, L., Curtis, F. E., Nocedal, J. “Optimization methods for large-scale machine learning.” SIAM Review. 2018, 60(2): 223-311.\n[24] Sharpe, W. F. “Mutual fund performance.” The Journal of Business. 1966, 39(1): 119-138.\n[25] Sharpe, W. F. “The sharpe ratio.” Journal of Portfolio Management. 1994, 21(1): 49-58.\n[26] Bachelier, L. “Théorie de la spéculation.” Annales Scientifiques de l\u0026rsquo;École Normale Supérieure. 1900, 17: 21-86.\n[27] Fromlet, H. “Behavioral finance-theory and practical application: Systematic analysis of departures from the homo oeconomicus paradigm are essential for realistic financial research and analysis.” Business Economics. 2001: 63-69.\n[28] Lo, A. W. “The adaptive markets hypothesis.” The Journal of Portfolio Management. 2004, 30(5): 15-29.\n参考博客 Colah\u0026rsquo;s Blog. (2015, August 27). Understanding LSTM Networks. 引用 引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Apr 2021). 基于深度学习的股票价格预测和量化策略.\nhttps://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\nOr\n@article{syhya2021stockprediction, title = \u0026#34;基于深度学习的股票价格预测和量化策略\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2021\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2021-04-21-deep-learning-stock-prediction/","summary":"\u003ch2 id=\"摘要\"\u003e摘要\u003c/h2\u003e\n\u003cp\u003e股票市场是金融市场的重要组成部分，近些年来，股票市场蓬勃发展，股票价格预测和量化投资策略研究吸引了许多领域的研究学者。其中最近几年随着人工智能和机器学习的发展，学者们从传统的统计学模型迁移到了人工智能算法，尤其是在深度学习热潮掀起后，神经网络在股票价格预测和量化投资策略研究中取得了不错的效果。深度学习的目标是学习多层次的特征，通过组合低级特征构建抽象的高级特征，从而挖掘数据的分布式特征表示，基于此进行复杂的非线性建模，从而实现预测任务。其中 RNN 被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价，交易信息都是序列数据，因此之前有很多研究者，基于 RNN 来预测股票价格。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题，之后出现了诸如 GRU，Peephole LSTM，BiLSTM 等 LSTM 的变体。但传统的股票预测模型有些并未考虑时间因素，有些仅考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。\u003c/p\u003e","title":"基于深度学习的股票价格预测和量化策略"}]