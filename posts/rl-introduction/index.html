<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deep Reinforcement Learning (Ongoing Updates) | Yue Shui Blog</title><meta name=keywords content="Deep Learning,AI,Reinforcement Learning"><meta name=description content="
Note: This article is currently being updated. The content is in draft version and may change. Please check back for the latest version.
Notations

  
      
          Symbol
          Meaning
      
  
  
      
          \(s, s', S_t, S_{t+1}\)
          State, next state, state at time \(t\), state at time \(t+1\)
      
      
          \(o, o_t\)
          Observation, observation at time \(t\)
      
      
          \(a, a', A_t, A_{t+1}\)
          Action, next action, action at time \(t\), action at time \(t+1\)
      
      
          \(r, r_t\)
          Immediate reward, reward at time \(t\)
      
      
          \(G_t\)
          Return at time \(t\)
      
      
          \(R(\tau)\)
          Return of a trajectory \(\tau\)
      
      
          \(\mathcal{S}\)
          Set of all possible states
      
      
          \(\mathcal{A}\)
          Set of all possible actions
      
      
          \(\mathcal{R}\)
          Set of all possible rewards
      
      
          \(\pi(a\mid s), \pi_\theta(a\mid s)\)
          Policy (stochastic), parameterized policy
      
      
          \(\mu(s), \mu_\theta(s)\)
          Policy (deterministic), parameterized policy
      
      
          \(\theta, \phi, w\)
          Policy or value function parameters
      
      
          \(\gamma\)
          Discount factor
      
      
          \(J(\pi)\)
          Expected return of policy \(\pi\)
      
      
          \(V_\pi(s)\)
          State-value function for policy \(\pi\)
      
      
          \(Q_\pi(s,a)\)
          Action-value function for policy \(\pi\)
      
      
          \(V_*(s)\)
          Optimal state-value function
      
      
          \(Q_*(s,a)\)
          Optimal action-value function
      
      
          \(A_\pi(s,a)\)
          Advantage function for policy \(\pi\)
      
      
          \(P(s'\mid s,a)\)
          Transition probability function
      
      
          \(R(s,a,s')\)
          Reward function
      
      
          \(\rho_0(s)\)
          Start-state distribution
      
      
          \(\tau\)
          Trajectory
      
      
          \(D\)
          Replay memory
      
      
          \(\alpha\)
          Learning rate, temperature parameter (in SAC)
      
      
          \(\lambda\)
          Eligibility trace parameter
      
      
          \(\epsilon\)
          Exploration parameter (e.g., in \(\epsilon\)-greedy), clipping parameter (in PPO)
      
  

What is Reinforcement Learning?
Definition
"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/rl-introduction/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/rl-introduction/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/rl-introduction/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Deep Reinforcement Learning (Ongoing Updates)"><meta property="og:description" content=" Note: This article is currently being updated. The content is in draft version and may change. Please check back for the latest version.
Notations Symbol Meaning \(s, s', S_t, S_{t+1}\) State, next state, state at time \(t\), state at time \(t+1\) \(o, o_t\) Observation, observation at time \(t\) \(a, a', A_t, A_{t+1}\) Action, next action, action at time \(t\), action at time \(t+1\) \(r, r_t\) Immediate reward, reward at time \(t\) \(G_t\) Return at time \(t\) \(R(\tau)\) Return of a trajectory \(\tau\) \(\mathcal{S}\) Set of all possible states \(\mathcal{A}\) Set of all possible actions \(\mathcal{R}\) Set of all possible rewards \(\pi(a\mid s), \pi_\theta(a\mid s)\) Policy (stochastic), parameterized policy \(\mu(s), \mu_\theta(s)\) Policy (deterministic), parameterized policy \(\theta, \phi, w\) Policy or value function parameters \(\gamma\) Discount factor \(J(\pi)\) Expected return of policy \(\pi\) \(V_\pi(s)\) State-value function for policy \(\pi\) \(Q_\pi(s,a)\) Action-value function for policy \(\pi\) \(V_*(s)\) Optimal state-value function \(Q_*(s,a)\) Optimal action-value function \(A_\pi(s,a)\) Advantage function for policy \(\pi\) \(P(s'\mid s,a)\) Transition probability function \(R(s,a,s')\) Reward function \(\rho_0(s)\) Start-state distribution \(\tau\) Trajectory \(D\) Replay memory \(\alpha\) Learning rate, temperature parameter (in SAC) \(\lambda\) Eligibility trace parameter \(\epsilon\) Exploration parameter (e.g., in \(\epsilon\)-greedy), clipping parameter (in PPO) What is Reinforcement Learning? Definition "><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-31T12:00:00+08:00"><meta property="article:modified_time" content="2025-01-31T12:00:00+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="AI"><meta property="article:tag" content="Reinforcement Learning"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Deep Reinforcement Learning (Ongoing Updates)"><meta name=twitter:description content="
Note: This article is currently being updated. The content is in draft version and may change. Please check back for the latest version.
Notations

  
      
          Symbol
          Meaning
      
  
  
      
          \(s, s', S_t, S_{t+1}\)
          State, next state, state at time \(t\), state at time \(t+1\)
      
      
          \(o, o_t\)
          Observation, observation at time \(t\)
      
      
          \(a, a', A_t, A_{t+1}\)
          Action, next action, action at time \(t\), action at time \(t+1\)
      
      
          \(r, r_t\)
          Immediate reward, reward at time \(t\)
      
      
          \(G_t\)
          Return at time \(t\)
      
      
          \(R(\tau)\)
          Return of a trajectory \(\tau\)
      
      
          \(\mathcal{S}\)
          Set of all possible states
      
      
          \(\mathcal{A}\)
          Set of all possible actions
      
      
          \(\mathcal{R}\)
          Set of all possible rewards
      
      
          \(\pi(a\mid s), \pi_\theta(a\mid s)\)
          Policy (stochastic), parameterized policy
      
      
          \(\mu(s), \mu_\theta(s)\)
          Policy (deterministic), parameterized policy
      
      
          \(\theta, \phi, w\)
          Policy or value function parameters
      
      
          \(\gamma\)
          Discount factor
      
      
          \(J(\pi)\)
          Expected return of policy \(\pi\)
      
      
          \(V_\pi(s)\)
          State-value function for policy \(\pi\)
      
      
          \(Q_\pi(s,a)\)
          Action-value function for policy \(\pi\)
      
      
          \(V_*(s)\)
          Optimal state-value function
      
      
          \(Q_*(s,a)\)
          Optimal action-value function
      
      
          \(A_\pi(s,a)\)
          Advantage function for policy \(\pi\)
      
      
          \(P(s'\mid s,a)\)
          Transition probability function
      
      
          \(R(s,a,s')\)
          Reward function
      
      
          \(\rho_0(s)\)
          Start-state distribution
      
      
          \(\tau\)
          Trajectory
      
      
          \(D\)
          Replay memory
      
      
          \(\alpha\)
          Learning rate, temperature parameter (in SAC)
      
      
          \(\lambda\)
          Eligibility trace parameter
      
      
          \(\epsilon\)
          Exploration parameter (e.g., in \(\epsilon\)-greedy), clipping parameter (in PPO)
      
  

What is Reinforcement Learning?
Definition
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Deep Reinforcement Learning (Ongoing Updates)","item":"https://syhya.github.io/posts/rl-introduction/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deep Reinforcement Learning (Ongoing Updates)","name":"Deep Reinforcement Learning (Ongoing Updates)","description":" Note: This article is currently being updated. The content is in draft version and may change. Please check back for the latest version.\nNotations Symbol Meaning \\(s, s', S_t, S_{t+1}\\) State, next state, state at time \\(t\\), state at time \\(t+1\\) \\(o, o_t\\) Observation, observation at time \\(t\\) \\(a, a', A_t, A_{t+1}\\) Action, next action, action at time \\(t\\), action at time \\(t+1\\) \\(r, r_t\\) Immediate reward, reward at time \\(t\\) \\(G_t\\) Return at time \\(t\\) \\(R(\\tau)\\) Return of a trajectory \\(\\tau\\) \\(\\mathcal{S}\\) Set of all possible states \\(\\mathcal{A}\\) Set of all possible actions \\(\\mathcal{R}\\) Set of all possible rewards \\(\\pi(a\\mid s), \\pi_\\theta(a\\mid s)\\) Policy (stochastic), parameterized policy \\(\\mu(s), \\mu_\\theta(s)\\) Policy (deterministic), parameterized policy \\(\\theta, \\phi, w\\) Policy or value function parameters \\(\\gamma\\) Discount factor \\(J(\\pi)\\) Expected return of policy \\(\\pi\\) \\(V_\\pi(s)\\) State-value function for policy \\(\\pi\\) \\(Q_\\pi(s,a)\\) Action-value function for policy \\(\\pi\\) \\(V_*(s)\\) Optimal state-value function \\(Q_*(s,a)\\) Optimal action-value function \\(A_\\pi(s,a)\\) Advantage function for policy \\(\\pi\\) \\(P(s'\\mid s,a)\\) Transition probability function \\(R(s,a,s')\\) Reward function \\(\\rho_0(s)\\) Start-state distribution \\(\\tau\\) Trajectory \\(D\\) Replay memory \\(\\alpha\\) Learning rate, temperature parameter (in SAC) \\(\\lambda\\) Eligibility trace parameter \\(\\epsilon\\) Exploration parameter (e.g., in \\(\\epsilon\\)-greedy), clipping parameter (in PPO) What is Reinforcement Learning? Definition ","keywords":["Deep Learning","AI","Reinforcement Learning"],"articleBody":" Note: This article is currently being updated. The content is in draft version and may change. Please check back for the latest version.\nNotations Symbol Meaning \\(s, s', S_t, S_{t+1}\\) State, next state, state at time \\(t\\), state at time \\(t+1\\) \\(o, o_t\\) Observation, observation at time \\(t\\) \\(a, a', A_t, A_{t+1}\\) Action, next action, action at time \\(t\\), action at time \\(t+1\\) \\(r, r_t\\) Immediate reward, reward at time \\(t\\) \\(G_t\\) Return at time \\(t\\) \\(R(\\tau)\\) Return of a trajectory \\(\\tau\\) \\(\\mathcal{S}\\) Set of all possible states \\(\\mathcal{A}\\) Set of all possible actions \\(\\mathcal{R}\\) Set of all possible rewards \\(\\pi(a\\mid s), \\pi_\\theta(a\\mid s)\\) Policy (stochastic), parameterized policy \\(\\mu(s), \\mu_\\theta(s)\\) Policy (deterministic), parameterized policy \\(\\theta, \\phi, w\\) Policy or value function parameters \\(\\gamma\\) Discount factor \\(J(\\pi)\\) Expected return of policy \\(\\pi\\) \\(V_\\pi(s)\\) State-value function for policy \\(\\pi\\) \\(Q_\\pi(s,a)\\) Action-value function for policy \\(\\pi\\) \\(V_*(s)\\) Optimal state-value function \\(Q_*(s,a)\\) Optimal action-value function \\(A_\\pi(s,a)\\) Advantage function for policy \\(\\pi\\) \\(P(s'\\mid s,a)\\) Transition probability function \\(R(s,a,s')\\) Reward function \\(\\rho_0(s)\\) Start-state distribution \\(\\tau\\) Trajectory \\(D\\) Replay memory \\(\\alpha\\) Learning rate, temperature parameter (in SAC) \\(\\lambda\\) Eligibility trace parameter \\(\\epsilon\\) Exploration parameter (e.g., in \\(\\epsilon\\)-greedy), clipping parameter (in PPO) What is Reinforcement Learning? Definition Reinforcement Learning (RL) is a branch of machine learning that trains an agent to take a series of actions (\\(a_t\\)) in an environment, transitioning through different states (\\(s_t\\)) to achieve a long-terms goal.\nUnlike supervised learning, which relies on human-labeled data, RL depends on the interaction between the agent and the environment. After each action, the agent receives a reward (\\(r_t\\)) as feedback. The objective of the agent is to learn a policy $\\pi(s)$, which is a strategy for selecting actions, in order to maximize the total reward (\\(\\sum_{t=0}^{T} r_t\\)).\nApplications RL has achieved remarkable successes in various domains, including:\nGame Playing: Mastering complex games like Go (AlphaGo, AlphaGo Zero) and video games (Atari, Dota 2). Robotics: Controlling robots for tasks like navigation, manipulation, and locomotion. Autonomous Driving: Developing self-driving vehicles that can perceive their environment and make driving decisions. Resource Management: Optimizing resource allocation in areas like energy management and traffic control. Personalized Recommendations: Creating recommendation systems that adapt to user preferences over time. Policy A policy \\(\\pi\\) is the strategy an agent employs to decide which action to take in each state. It is the cornerstone of an RL agent, defining its behavior. Policies can be either deterministic or stochastic.\nDeterministic Policy \\(\\pi(s)\\): Maps each state to a single, specific action. For a given state \\(s\\), the policy \\(\\pi(s)\\) always selects the same action \\(a\\): $$ \\pi(s) = a $$ Stochastic Policy \\(\\pi(a \\mid s)\\): Provides a probability distribution over possible actions for each state. For a given state \\(s\\), \\(\\pi(a \\mid s)\\) represents the probability of choosing action \\(a\\). The agent samples an action based on this distribution: $$ \\pi(a \\mid s) = \\mathbb{P}_\\pi[A = a \\mid S = s] $$ In Deep Reinforcement Learning, policies are typically represented by sophisticated function approximators, such as neural networks. These networks, parameterized by weights \\(\\theta\\), learn to map states (or observations) to actions (or action probabilities). The parameterized policies are denoted as:\n\\(\\pi_\\theta(s)\\) for deterministic policies \\(\\pi_\\theta(a \\mid s)\\) for stochastic policies Trajectories A trajectory (also called episode) \\(\\tau\\) is a sequence of states and actions that unfold in an environment:\n\\[ \\tau = (s_0, a_0, s_1, a_1, \\ldots) \\]The initial state of the environment, \\(s_0\\), is sampled from a predefined start-state distribution, often represented as \\(\\rho_0\\):\n\\[ s_0 \\sim \\rho_0(\\cdot) \\]State transitions describe how the environment evolves from one state to the next, specifically from state \\(s_t\\) at time \\(t\\) to state \\(s_{t+1}\\) at time \\(t+1\\). These transitions are governed by the natural laws of the environment and are influenced solely by the most recent action taken \\(a_t\\). The transition dynamics can be either:\nDeterministic:\n\\[ s_{t+1} = f(s_t, a_t) \\] Stochastic:\n\\[ s_{t+1} \\sim P(\\cdot \\mid s_t, a_t) \\] Reward and Return In reinforcement learning, rewards and returns are fundamental concepts that guide an agent’s learning process by providing feedback on its actions.\nReward A reward is a scalar signal received by the agent after taking an action in a specific state. It serves as immediate feedback to indicate the desirability of the action taken. The reward function, denoted by \\( R \\), maps state-action pairs to real numbers:\n\\[ r_t = R(s_t, a_t) \\]where:\n\\( r_t \\) is the reward received at time step \\( t \\), \\( s_t \\) is the state at time step \\( t \\), \\( a_t \\) is the action taken at time step \\( t \\). The reward function encapsulates the goals of the agent by assigning higher rewards to desirable outcomes and lower (or negative) rewards to undesirable ones.\nReturn The return, often denoted by \\( G_t \\), represents the total accumulated future rewards from a specific time step onward. It quantifies the long-term benefit of actions taken by the agent. The return is defined as the sum of discounted rewards:\n\\[ G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\]where:\n\\( \\gamma \\in [0, 1) \\) is the discount factor that determines the present value of future rewards, \\( r_{t+k} \\) is the reward received \\( k \\) time steps after \\( t \\). The discount factor \\( \\gamma \\) balances the importance of immediate versus future rewards:\nA higher \\( \\gamma \\) (close to 1) makes the agent strive for long-term rewards. A lower \\( \\gamma \\) (close to 0) makes the agent prioritize immediate rewards. Why Use a Discount Factor? The discount factor \\( \\gamma \\) serves several important purposes in reinforcement learning:\nHandling Uncertainty of Future Rewards:\nHigher Uncertainty: Future rewards are often more uncertain than immediate rewards. For example, in the stock market, predicting long-term returns is more challenging due to market volatility. Preference for Immediate Benefits:\nHuman Behavior: As humans, we might prefer to enjoy rewards today rather than waiting for them years later. This preference is naturally modeled by discounting future rewards. Mathematical Convenience:\nFinite Computation: Discounting allows us to compute returns without needing to track future steps indefinitely, simplifying calculations and algorithms. Avoiding Infinite Loops:\nTermination Assurance: In environments with the possibility of infinite loops in state transitions, discounting ensures that the return remains finite, preventing issues with infinite sums. Finite Horizon Return In scenarios with a finite number of time steps \\( T \\), the return is calculated up to the terminal time step:\n\\[ G_t = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k} \\]This is common in episodic tasks where the interaction between the agent and the environment terminates after a certain number of steps.\nTransition Function and Reward Function In Reinforcement Learning (RL), a model encapsulates the agent’s representation of how the environment behaves. This model typically includes two core components: the transition function and the reward function.\nTransition Function The transition function \\( P(s' \\mid s, a) \\) specifies the probability of moving from state \\( s \\) to state \\( s' \\) after taking action \\( a \\). Formally:\n\\[ P(s' \\mid s, a) = \\mathbb{P}(S_{t+1} = s' \\mid S_t = s, A_t = a) \\] Deterministic Environment: In a deterministic setting, the transition function assigns a probability of 1 to a single specific next state and 0 to all others:\n\\[ P(s' \\mid s, a) = \\begin{cases} 1 \u0026 \\text{if } s' = f(s, a),\\\\ 0 \u0026 \\text{otherwise}. \\end{cases} \\] Stochastic Environment: In a stochastic environment, the transition function defines a probability distribution over possible next states:\n\\[ P(s' \\mid s, a) = \\text{Probability of transitioning to } s' \\text{ from } s \\text{ by taking action } a. \\] Reward Function The reward function \\( R(s, a, s') \\) specifies the immediate reward obtained after transitioning from state \\( s \\) to state \\( s' \\) via action \\( a \\). It provides essential feedback that guides the agent’s learning. Formally:\n\\[ R(s, a, s') = \\mathbb{E}\\bigl[ R_{t+1} \\mid S_t = s, A_t = a, S_{t+1} = s' \\bigr] \\]Depending on the problem, the reward function might depend on:\nState-Action-State:\n\\[ R(s, a, s') = \\text{Immediate reward after transitioning from } s \\text{ to } s' \\text{ using } a. \\] State-Action:\n\\[ R(s, a) = \\mathbb{E}\\bigl[ R_{t+1} \\mid S_t = s, A_t = a \\bigr]. \\] State Only:\n\\[ R(s) = \\mathbb{E}\\bigl[ R_{t+1} \\mid S_t = s \\bigr]. \\] Value Function Value functions quantify the expected return (sum of discounted future rewards) starting from a given state (or state-action pair). They are central to most RL methods. There are two key types:\nState-Value Function The state-value function \\( V_\\pi(s) \\) measures the expected return when starting in state \\( s \\) and following policy \\( \\pi \\) thereafter:\n\\[ V_\\pi(s) = \\mathbb{E}_\\pi\\bigl[ G_t \\mid S_t = s \\bigr] = \\mathbb{E}_\\pi\\Bigl[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\;\\big\\vert\\; S_t = s \\Bigr]. \\]Action-Value Function (Q-function) The action-value function \\( Q_\\pi(s, a) \\) measures the expected return when starting in state \\( s \\), taking action \\( a \\), and thereafter following policy \\( \\pi \\):\n\\[ Q_\\pi(s, a) = \\mathbb{E}_\\pi\\bigl[ G_t \\mid S_t = s, A_t = a \\bigr] = \\mathbb{E}_\\pi\\Bigl[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\;\\big\\vert\\; S_t = s, A_t = a \\Bigr]. \\]Advantage Function The advantage function \\( A_\\pi(s, a) \\) indicates how much better (or worse) taking action \\( a \\) in state \\( s \\) is compared to the average action under policy \\( \\pi \\). It is defined as:\n\\[ A_\\pi(s, a) = Q_\\pi(s, a) - V_\\pi(s). \\]Significance of the Advantage Function Variance Reduction: In policy gradient methods, using the advantage function can reduce the variance of gradient estimates, leading to more stable learning. Policy Improvement: It highlights which actions are better or worse than the average in a given state, providing a clearer signal for policy updates. Optimal Value and Policy The ultimate goal in RL is to find an optimal policy \\( \\pi_* \\) that maximizes the expected return in the long run. Correspondingly, we define optimal value functions.\nOptimal State-Value Function The optimal state-value function \\( V_*(s) \\) is the maximum state-value attainable at state \\( s \\) over all possible policies:\n\\[ V_*(s) = \\max_{\\pi} V_\\pi(s). \\]Optimal Action-Value Function The optimal action-value function \\( Q_*(s, a) \\) is the maximum action-value attainable for the state-action pair \\( (s, a) \\) over all possible policies:\n\\[ Q_*(s, a) = \\max_{\\pi} Q_\\pi(s, a). \\]Optimal Policy An optimal policy \\( \\pi_* \\) is any policy that achieves these optimal value functions. Formally, for all \\( s \\) and \\( a \\):\n\\[ V_{\\pi_*}(s) = V_*(s), \\quad\\text{and}\\quad Q_{\\pi_*}(s, a) = Q_*(s, a). \\]Challenges in Finding the Optimal Policy Complexity: Finding \\( \\pi_* \\), \\( V_*(s) \\), or \\( Q_*(s, a) \\) can be computationally expensive, especially for large or continuous state and action spaces. Approximation: Many RL algorithms approximate these optimal functions or directly learn an (approximately) optimal policy through techniques such as dynamic programming, Monte Carlo methods, or temporal-difference learning. A Taxonomy of RL Algorithms Now that we’ve gone through the basics of RL terminology and notation, we can delve into the richer material: the landscape of algorithms in modern RL, along with the trade-offs involved in their design.\nOverview Creating an accurate and comprehensive taxonomy of modern RL algorithms is challenging due to the modularity and complexity of these algorithms. To keep this introduction digestible, we focus on the foundational design choices in deep RL algorithms, the trade-offs these choices entail, and contextualize some prominent modern algorithms within this framework.\nKey Insights from the RL Algorithms Taxonomy The comparative table provides a structured overview of various Reinforcement Learning (RL) approaches. Below, we break down the table contents into detailed key insights to better understand the distinctions, strengths, and challenges associated with each category of RL algorithms.\n1. Model-Free RL: Policy Optimization Key Idea:\nPolicy Optimization methods focus on directly learning a parameterized policy \\( \\pi_\\theta(a \\mid s) \\) by maximizing a performance objective \\( J(\\pi) \\). These methods often incorporate learning a value function \\( V^\\pi(s) \\) to facilitate more effective policy updates.\nOn/Off-Policy Setting:\nPrimarily on-policy, meaning updates are based on data collected from the most recent policy.\nStrengths:\nStability: By directly optimizing the policy objective, these methods tend to exhibit stable and reliable convergence. Simplicity in Updates: Utilizes gradient-based approaches, making the update process conceptually straightforward. Weaknesses:\nSample Inefficiency: Requires fresh interaction data from the current policy for each update, which can be resource-intensive. High Variance: Gradient estimates can exhibit high variance, potentially slowing down the learning process. Representative Examples:\nA2C, A3C, PPO, TRPO\n2. Model-Free RL: Q-Learning Key Idea:\nQ-Learning methods aim to learn an approximate Q-function \\( Q_\\theta(s, a) \\) that estimates the optimal action-value function \\( Q^*(s, a) \\). The policy is then derived by selecting actions that maximize the Q-values.\nOn/Off-Policy Setting:\nPrimarily off-policy, allowing the use of data collected from any policy during training.\nStrengths:\nSample Efficiency: Can reuse past experiences effectively, making better use of available data. Straightforward Objective: Relies on Bellman backups, providing a clear and direct learning target. Weaknesses:\nStability Issues: Susceptible to divergence and instability, especially when combined with function approximation. Indirect Performance Optimization: Optimizes the Q-function rather than the policy directly, which can complicate the learning process. Representative Examples:\nDQN, C51, QR-DQN\n3. Model-Based RL Key Idea:\nModel-Based methods involve using or learning a model of the environment’s dynamics (state transitions and rewards) to facilitate planning or generate additional training data.\nOn/Off-Policy Setting:\nCan be on-policy or off-policy depending on the specific algorithm design.\nStrengths:\nHigh Sample Efficiency: Leveraging a model allows for planning and generating synthetic data, reducing the need for extensive real-world interactions. Forward Planning: Enables the agent to “think ahead” by simulating future states and rewards, leading to more informed decision-making. Weaknesses:\nModel Bias: Inaccuracies in the learned model can lead to suboptimal or even detrimental policy performance in the real environment. Implementation Complexity: Incorporating a model adds layers of complexity, making these methods harder to implement and tune effectively. Representative Examples:\nMBMF, MBVE, AlphaZero, World Models\n4. Hybrid / In-Between Approaches Key Idea:\nHybrid methods blend elements from policy optimization, Q-Learning, and planning. For instance, they may learn both a Q-function and a policy simultaneously or embed planning mechanisms directly into the policy structure.\nOn/Off-Policy Setting:\nVaries across different algorithms; some are off-policy, others are on-policy, and some employ a mixed approach.\nStrengths:\nBalanced Strengths: Capable of harnessing the advantages of multiple RL paradigms, such as the stability of policy optimization and the sample efficiency of Q-Learning. Enhanced Data Utilization: Can effectively reuse data while maintaining stable policy updates, leading to improved overall performance. Weaknesses:\nImplementation Complexity: Managing multiple components (e.g., separate networks for policy and value functions) increases the complexity of the algorithm. Inherited Failure Modes: Risks arising from combining different methods can lead to compounded instability or other issues from each constituent approach. Representative Examples:\nDDPG, SAC, I2A (Imagination-Augmented Agents)\nSummary of Insights Diverse Strategies: RL algorithms can be broadly categorized into model-free and model-based approaches, each with distinct methodologies and trade-offs. Hybrid methods seek to combine these strategies to leverage their respective strengths.\nPolicy Optimization vs. Q-Learning:\nPolicy Optimization offers stability and direct optimization but at the cost of sample efficiency. Q-Learning provides greater sample efficiency through data reuse but may suffer from stability issues. Model-Based Advantages and Challenges:\nWhile model-based methods can significantly enhance sample efficiency and enable forward planning, they are often hindered by the difficulty of accurately modeling complex environments and the increased complexity of implementation. Hybrid Approaches as a Middle Ground:\nBy integrating aspects of both model-free and model-based methods, hybrid algorithms aim to achieve a balance between stability, sample efficiency, and performance. However, this integration introduces additional complexity and potential points of failure. Representative Algorithms:\nUnderstanding where prominent algorithms like PPO, DQN, AlphaZero, and SAC fit within this taxonomy helps in selecting the appropriate method based on the specific requirements and constraints of the task at hand. Others Markov Decision Processes (MDPs) In reinforcement learning, the interaction between an agent and its environment is often formalized as a Markov Decision Process (MDP). MDPs provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. An MDP is defined by a 5-tuple \\(\\langle \\mathcal{S}, \\mathcal{A}, P, R, \\gamma \\rangle\\):\n\\(\\mathcal{S}\\): A set of possible states. This is the set of all possible situations the agent can be in. \\(\\mathcal{A}\\): A set of possible actions. These are the actions the agent can take in each state. \\(P(s'\\mid s,a)\\): The transition probability function. It defines the probability of transitioning to a next state \\(s'\\) from the current state \\(s\\) when action \\(a\\) is taken. This function encapsulates the environment’s dynamics. \\(R(s,a,s')\\): The reward function. It defines the reward received by the agent after transitioning from state \\(s\\) to \\(s'\\) due to action \\(a\\). This function specifies the immediate feedback from the environment. \\(\\gamma \\in [0, 1]\\): The discount factor. It is a value between 0 and 1 that discounts future rewards. A discount factor closer to 0 makes the agent prioritize immediate rewards, while a factor closer to 1 makes it value future rewards more. The defining characteristic of an MDP is the Markov property, which states that the future state and reward depend only on the current state and action, and not on the history of past states and actions. Formally, for any time step \\(t\\):\n\\[ \\mathbb{P}\\bigl[S_{t+1} \\mid S_t, A_t\\bigr] = \\mathbb{P}\\bigl[S_{t+1} \\mid S_1, A_1, S_2, A_2, \\ldots, S_t, A_t\\bigr] \\]This property simplifies the problem significantly as the agent only needs to consider the current state to make optimal decisions, without needing to remember the entire history.\nIn an MDP, the agent’s goal is to find a policy \\(\\pi\\) that maximizes the expected cumulative discounted reward, starting from some initial state distribution \\(\\rho_0(s)\\). The sequence of states, actions, and rewards generated by an agent interacting with an MDP is called a trajectory or episode:\n\\[ \\tau = \\bigl(S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \\ldots\\bigr) \\]The first state \\(S_0\\) is sampled from the start-state distribution \\(\\rho_0(\\cdot)\\). Subsequent states are determined by the transition probabilities \\(P(s'\\mid s,a)\\), and rewards are given by the reward function \\(R(s,a,s')\\). Actions \\(A_t\\) are chosen by the agent according to its policy \\(\\pi(a\\mid s)\\).\nBellman Equations Bellman equations are a set of equations that lie at the heart of dynamic programming and reinforcement learning. They decompose the value function into two parts: the immediate reward and the discounted value of the next state. These equations express a recursive relationship that value functions must satisfy. There are two main types of Bellman equations: Bellman Expectation Equations and Bellman Optimality Equations.\nBellman Expectation Equations\nBellman Expectation Equations are used for policy evaluation, i.e., calculating the value functions \\(V_\\pi(s)\\) and \\(Q_\\pi(s,a)\\) for a given policy \\(\\pi\\). They express the value of a state (or state-action pair) in terms of the expected immediate reward and the expected value of the next state, assuming the agent follows policy \\(\\pi\\). Bellman Expectation Equation for State-Value Function \\(\\bigl(V_\\pi(s)\\bigr)\\):\n\\[ V_\\pi(s) = \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_{t}=s\\bigr] \\] Expanding this expectation: \\[ V_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\Bigl(R(s, a) + \\gamma V_\\pi(s')\\Bigr) \\] Derivation:\n\\[ \\begin{aligned} V_\\pi(s) \u0026= \\mathbb{E}_{\\pi}\\bigl[G_t \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots) \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_t=s\\bigr] \\\\ \u0026= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\bigl(R(s,a) + \\gamma V_\\pi(s')\\bigr) \\end{aligned} \\] Bellman Expectation Equation for Action-Value Function \\(\\bigl(Q_\\pi(s,a)\\bigr)\\):\n\\[ Q_\\pi(s,a) = \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma Q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t=s, A_t=a\\bigr] \\] Expanding this expectation: \\[ Q_\\pi(s,a) = R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\mid s')\\, Q_\\pi(s',a') \\] Derivation:\n\\[ \\begin{aligned} Q_\\pi(s,a) \u0026= \\mathbb{E}_{\\pi}\\bigl[G_t \\mid S_t=s, A_t=a\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_t=s, A_t=a\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma \\mathbb{E}_{a' \\sim \\pi} Q_\\pi(S_{t+1}, a') \\mid S_t=s, A_t=a\\bigr] \\\\ \u0026= R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\mid s') Q_\\pi(s',a') \\end{aligned} \\] Bellman Optimality Equations\nBellman Optimality Equations specify the conditions for optimality for value functions \\(V_*(s)\\) and \\(Q_*(s,a)\\). They express the optimal value of a state (or state-action pair) in terms of the optimal values of successor states, assuming optimal actions are taken. Bellman Optimality Equation for Optimal State-Value Function \\(\\bigl(V_*(s)\\bigr)\\):\n\\[ V_*(s) = \\max_{a \\in \\mathcal{A}} \\mathbb{E}\\bigl[R_{t+1} + \\gamma V_*(S_{t+1}) \\mid S_t=s, A_t=a\\bigr] \\] Expanding this expectation: \\[ V_*(s) = \\max_{a \\in \\mathcal{A}} \\Bigl(R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a)\\, V_*(s')\\Bigr) \\] Explanation: To achieve the optimal value in state \\(s\\), we should choose the action \\(a\\) that maximizes the sum of the immediate reward \\(R(s,a)\\) and the discounted optimal value of the next state \\(V_*(s')\\).\nBellman Optimality Equation for Optimal Action-Value Function \\(\\bigl(Q_*(s,a)\\bigr)\\):\n\\[ Q_*(s,a) = \\mathbb{E}\\bigl[R_{t+1} + \\gamma \\max_{a'} Q_*(S_{t+1}, a') \\mid S_t=s, A_t=a\\bigr] \\] Expanding this expectation: \\[ Q_*(s,a) = R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a)\\, \\max_{a' \\in \\mathcal{A}} Q_*(s',a') \\] Explanation: The optimal Q-value for a state-action pair \\((s,a)\\) is the immediate reward \\(R(s,a)\\) plus the discounted maximum Q-value achievable from the next state \\(s'\\), considering all possible actions \\(a'\\) in \\(s'\\).\nThese Bellman equations form the basis for many reinforcement learning algorithms, providing a way to compute and improve value functions and policies.\n2. Fundamental Approaches Dynamic Programming Dynamic Programming (DP) provides a collection of algorithms that can be used to compute optimal policies in MDPs, given a complete model of the environment. DP methods are particularly useful when the environment is fully known, meaning we have access to the transition probabilities \\(P(s'\\mid s,a)\\) and the reward function \\(R(s,a,s')\\). DP algorithms are based on the principle of optimality and utilize Bellman equations to find optimal policies and value functions.\nPolicy Evaluation (Prediction)\nPolicy Evaluation, also known as the prediction problem, aims to compute the state-value function \\(V_\\pi(s)\\) for a given policy \\(\\pi\\). It uses the Bellman Expectation Equation for \\(V_\\pi(s)\\) iteratively. Iterative Policy Evaluation Algorithm:\nInitialize \\(V_0(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). For each iteration \\(k+1\\): For each state \\(s \\in \\mathcal{S}\\): \\[ V_{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s', r} P(s', r \\mid s, a)\\Bigl(r + \\gamma V_k(s')\\Bigr) \\] Repeat until convergence, i.e., until \\(V_{k+1}(s) \\approx V_k(s)\\) for all \\(s\\). Explanation: In each iteration, the value function for each state is updated based on the expected rewards and values of successor states, weighted by the policy and transition probabilities. This process is repeated until the value function converges, meaning the updates become very small.\nPolicy Improvement\nPolicy Improvement is the process of creating a better policy \\(\\pi'\\) from a given policy \\(\\pi\\). The idea is to act greedily with respect to the value function \\(V_\\pi\\) (or action-value function \\(Q_\\pi\\)) of the current policy. Greedy Policy Improvement: For each state \\(s \\in \\mathcal{S}\\), choose a new action \\(a'\\) that maximizes the action-value function \\(Q_\\pi(s,a)\\): \\[ \\pi'(s) = \\arg\\max_{a \\in \\mathcal{A}} Q_\\pi(s,a) \\] where \\[ Q_\\pi(s,a) = \\mathbb{E}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_t=s, A_t=a\\bigr] = \\sum_{s', r} P(s', r\\mid s, a)\\bigl(r + \\gamma V_\\pi(s')\\bigr). \\] Policy Improvement Theorem: If we improve the policy greedily with respect to \\(V_\\pi\\), the new policy \\(\\pi'\\) is guaranteed to be no worse than \\(\\pi\\), i.e., \\(V_{\\pi'}(s) \\ge V_\\pi(s)\\) for all \\(s \\in \\mathcal{S}\\). If improvement is strict for any state, then \\(\\pi'\\) is strictly better than \\(\\pi\\). Policy Iteration\nPolicy Iteration combines policy evaluation and policy improvement in an iterative process to find an optimal policy. It starts with an arbitrary policy, iteratively evaluates its value function, and then improves the policy based on this value function. Policy Iteration Algorithm: Initialization: Initialize a policy \\(\\pi_0\\) (e.g., randomly). Policy Evaluation: Compute the state-value function \\(V_{\\pi_k}\\) for the current policy \\(\\pi_k\\) using iterative policy evaluation. Policy Improvement: Create a new policy \\(\\pi_{k+1}\\) by acting greedily with respect to \\(V_{\\pi_k}\\): \\[ \\pi_{k+1}(s) = \\arg\\max_{a \\in \\mathcal{A}} Q_{\\pi_k}(s,a) \\] Iteration: Repeat steps 2 and 3 until policy improvement no longer yields a change in the policy, i.e., \\(\\pi_{k+1} = \\pi_k\\). At this point, the policy \\(\\pi_k\\) is guaranteed to be an optimal policy \\(\\pi_*\\), and \\(V_{\\pi_k} = V_*\\). Generalized Policy Iteration (GPI): The general idea of iteratively performing policy evaluation and policy improvement, which underlies many RL algorithms, including policy iteration and value iteration. Monte-Carlo Methods Monte-Carlo (MC) methods are a class of model-free reinforcement learning algorithms. Unlike Dynamic Programming, MC methods do not require a complete model of the environment. Instead, they learn directly from episodes of experience. An episode is a complete sequence of states, actions, and rewards from a start state to a terminal state. MC methods are used for both prediction (estimating value functions) and control (finding optimal policies).\nValue Estimation Monte Carlo methods estimate value functions by averaging the returns observed in actual or simulated episodes. The return \\(G_t\\) is the total discounted reward from time step \\(t\\) onwards in an episode: \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t} R_T, \\] where \\(T\\) is the terminal time step of the episode. Monte Carlo Value Estimation Algorithm:\nInitialize \\(V(s)\\) arbitrarily for all \\(s \\in \\mathcal{S}\\). Initialize \\(N(s) = 0\\) and \\(SumOfReturns(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). For each episode: Generate an episode following policy \\(\\pi\\): \\(S_1, A_1, R_2, S_2, A_2, \\ldots, S_T\\). For each state \\(S_t\\) visited in the episode: \\[ N(S_t) \\leftarrow N(S_t) + 1 \\] \\[ SumOfReturns(S_t) \\leftarrow SumOfReturns(S_t) + G_t \\] \\[ V(S_t) = \\frac{SumOfReturns(S_t)}{N(S_t)} \\] Explanation: For each episode, we calculate the return \\(G_t\\) for each state \\(S_t\\) visited in that episode. We then average these returns across all episodes to estimate \\(V(s)\\). We can use either first-visit MC, where we only consider the first visit to a state in an episode, or every-visit MC, where we consider every visit. Action-Value Function Estimation: MC methods can be extended to estimate action-value functions \\(Q_\\pi(s,a)\\) similarly, by averaging returns following each state-action pair \\((s,a)\\).\nTemporal-Difference (TD) Learning Temporal-Difference (TD) learning is a class of model-free reinforcement learning methods that learn directly from raw experience without a model of the environment, similar to Monte Carlo methods. However, TD learning has a key advantage: it can learn from incomplete episodes by bootstrapping, meaning it updates value function estimates based on other estimates, without waiting for the final outcome of an episode. TD learning is central to modern reinforcement learning algorithms.\nBootstrapping\nBootstrapping is a core concept in TD learning. It means that TD methods update their estimates based in part on other estimates. In value function learning, TD methods update the value of a state based on the estimated value of the next state. This is in contrast to Monte Carlo methods, which wait until the end of an episode to calculate the actual return and use that as the target for updates. Bootstrapping allows TD learning to be more sample-efficient and to learn online, without needing to wait for the end of episodes. Value Estimation\nThe simplest TD method for prediction is TD(0), also known as one-step TD learning. TD(0) updates the value function \\(V(S_t)\\) towards a TD target, which is an estimate of the return based on the immediate reward \\(R_{t+1}\\) and the current estimate of the value of the next state \\(V(S_{t+1})\\). TD(0) Update Rule for State-Value Function: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha \\Bigl(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\Bigr), \\] where \\(\\alpha\\) is the learning rate, controlling the step size of the update. The term \\(\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\) is called the TD error, representing the difference between the TD target and the current estimate.\nExplanation: The TD(0) update moves the value function \\(V(S_t)\\) in the direction of the TD target. If the TD error is positive, it means the current estimate \\(V(S_t)\\) is lower than the TD target, so we increase \\(V(S_t)\\). If the TD error is negative, we decrease \\(V(S_t)\\).\nSARSA: On-Policy TD Control\nSARSA (State-Action-Reward-State-Action) is an on-policy TD control algorithm for learning action-value functions \\(Q_\\pi(s,a)\\). It learns the Q-function for the policy that is being used to explore the environment. “On-policy” means that SARSA learns about the policy it is currently following. SARSA Algorithm: Initialize \\(Q(s,a)\\) arbitrarily for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}\\). For each episode: Initialize state \\(S_t\\). Choose action \\(A_t\\) using a policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy). Repeat for each step of episode: Take action \\(A_t\\), observe reward \\(R_{t+1}\\) and next state \\(S_{t+1}\\). Choose next action \\(A_{t+1}\\) using policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy). Update Q-value for state-action pair \\((S_t, A_t)\\): [ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\Bigl(R_{t+1} \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\\Bigr). ] \\(S_t \\leftarrow S_{t+1}\\), \\(A_t \\leftarrow A_{t+1}\\). Until \\(S_{t+1}\\) is terminal. Explanation: SARSA updates the Q-value for the state-action pair \\((S_t, A_t)\\) based on the reward \\(R_{t+1}\\) and the Q-value of the next state-action pair \\((S_{t+1}, A_{t+1})\\). The action \\(A_{t+1}\\) is chosen using the same policy that is being evaluated and improved, hence “on-policy”. Q-Learning: Off-Policy TD Control\nQ-Learning is an off-policy TD control algorithm. It learns an estimate of the optimal action-value function \\(Q_*(s,a)\\), independent of the policy being followed. “Off-policy” means that Q-learning can learn about an optimal policy even while following a different, possibly exploratory, policy. Q-Learning Algorithm: Initialize \\(Q(s,a)\\) arbitrarily for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}\\). For each episode: Initialize state \\(S_t\\). Repeat for each step of episode: Choose action \\(A_t\\) using a policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy). Take action \\(A_t\\), observe reward \\(R_{t+1}\\) and next state \\(S_{t+1}\\). Update Q-value for state-action pair \\((S_t, A_t)\\): [ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\Bigl(R_{t+1} \\gamma \\max_{a’} Q(S_{t+1}, a’) - Q(S_t, A_t)\\Bigr). ] \\(S_t \\leftarrow S_{t+1}\\). Until \\(S_{t+1}\\) is terminal. Explanation: Q-learning updates the Q-value for \\((S_t, A_t)\\) based on the reward \\(R_{t+1}\\) and the maximum Q-value achievable in the next state \\(S_{t+1}\\), regardless of which action is actually taken in \\(S_{t+1}\\). This “max” operation makes Q-learning off-policy because it learns about the optimal policy while potentially following a different behavior policy for exploration. Deep Q-Networks (DQN)\nDeep Q-Networks (DQN) is a groundbreaking algorithm that combines Q-learning with deep neural networks to handle high-dimensional state spaces, such as images. DQN addresses the instability and convergence issues that arise when using nonlinear function approximation (like neural networks) with bootstrapping and off-policy learning. DQN introduces two key techniques to stabilize Q-learning: Experience Replay: DQN stores transitions \\(\\bigl(s_t, a_t, r_t, s_{t+1}\\bigr)\\) in a replay memory (buffer) \\(D\\). Instead of updating Q-values online from sequential experiences, DQN samples mini-batches of transitions randomly from \\(D\\) to perform updates. This breaks the correlation between consecutive samples and smooths the data distribution over updates, improving stability. Periodically Updated Target Network: DQN uses two Q-networks: a Q-network \\(Q(s,a; \\theta)\\) with parameters \\(\\theta\\) that are being trained, and a target Q-network \\(Q(s,a; \\theta^-)\\) with parameters \\(\\theta^-\\) that are periodically updated to be the same as \\(\\theta\\) (e.g., every \\(C\\) steps) and kept frozen in between. The target network is used to compute the TD target in the Q-learning update, which stabilizes learning by reducing oscillations and divergence. DQN Loss Function: The loss function for DQN is the mean squared error between the TD target and the current Q-value: \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim U(D)}\\Bigl[\\bigl(y - Q(s,a;\\theta)\\bigr)^2\\Bigr], \\] where \\[ y = r + \\gamma \\max_{a'} Q(s',a'; \\theta^-) \\] is the TD target, and \\(\\bigl(s,a,r,s'\\bigr)\\) is a transition sampled from the replay memory \\(D\\). \\(U(D)\\) denotes uniform sampling from \\(D\\). DQN Algorithm Outline: Initialize replay memory \\(D\\) to capacity \\(N\\). Initialize Q-network \\(Q\\) with random weights \\(\\theta\\). Initialize target Q-network \\(\\hat{Q}\\) with weights \\(\\theta^- = \\theta\\). For each episode: Initialize state \\(s_1\\). For \\(t = 1, \\ldots, T\\): Select action \\(a_t\\) using \\(\\epsilon\\)-greedy policy based on \\(Q(s_t,\\cdot;\\theta)\\). Execute action \\(a_t\\), observe reward \\(r_t\\) and next state \\(s_{t+1}\\). Store transition \\(\\bigl(s_t,a_t,r_t,s_{t+1}\\bigr)\\) in \\(D\\). Sample random mini-batch of transitions from \\(D\\). Compute TD targets \\(y_j = r_j + \\gamma \\max_{a'} \\hat{Q}(s_j',a';\\theta^-)\\) (or \\(y_j = r_j\\) if episode terminates). Perform gradient descent step to minimize \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_j\\Bigl[\\bigl(y_j - Q(s_j,a_j;\\theta)\\bigr)^2\\Bigr] \\] with respect to \\(\\theta\\). Every \\(C\\) steps, reset \\(\\theta^- = \\theta\\). Combining TD and Monte-Carlo Learning n-step TD Learning\nn-step TD learning methods bridge the gap between one-step TD learning (like TD(0)) and Monte Carlo methods by looking ahead \\(n\\) steps to estimate the return. The n-step return \\(G_t^{(n)}\\) is defined as: \\[ G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n}). \\] For \\(n=1\\), \\(G_t^{(1)} = R_{t+1} + \\gamma V(S_{t+1})\\), which is the TD target. As \\(n \\to \\infty\\), \\(G_t^{(n)} \\to G_t\\), the Monte Carlo return. n-step TD Update Rule: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha \\bigl(G_t^{(n)} - V(S_t)\\bigr). \\] Benefits of n-step TD: Balances bias and variance: n-step returns have lower variance than MC returns and lower bias than 1-step TD targets. Can learn faster than MC methods and be more stable than 1-step TD methods in some environments. TD(\\(\\lambda\\))\nTD(\\(\\lambda\\)) methods generalize n-step TD learning by averaging n-step returns over all possible values of \\(n\\), weighted by a factor \\(\\lambda^{n-1}\\). The \\(\\lambda\\)-return \\(G_t^{(\\lambda)}\\) is defined as: \\[ G_t^{(\\lambda)} = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t^{(n)}, \\] where \\(\\lambda \\in [0,1]\\) is the trace-decay parameter. When \\(\\lambda = 0\\), \\(G_t^{(\\lambda)} = G_t^{(1)}\\), which reduces to TD(0). When \\(\\lambda = 1\\), \\(G_t^{(\\lambda)} = G_t^{(\\infty)} = G_t\\), which becomes the Monte Carlo return. TD(\\(\\lambda\\)) Update Rule: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha \\bigl(G_t^{(\\lambda)} - V(S_t)\\bigr). \\] Eligibility Traces: TD(\\(\\lambda\\)) can be implemented efficiently using eligibility traces, which provide a mechanism to assign credit to past states and actions for observed rewards. Eligibility traces maintain a short-term memory of visited states or state-action pairs, allowing updates to propagate back through time more efficiently than with n-step returns alone. Policy Gradient Methods Policy Gradient methods are a class of model-free reinforcement learning algorithms that directly learn and optimize the policy \\(\\pi_\\theta(a\\mid s)\\) without explicitly learning a value function (though value functions are often used to assist policy gradient methods, as in Actor-Critic methods). Policy gradient methods directly search for an optimal policy by optimizing the policy parameters \\(\\theta\\) using gradient ascent on the expected return \\(J(\\theta)\\).\nPolicy Gradient Theorem The Policy Gradient Theorem provides an analytical expression for the gradient of the performance objective \\(J(\\theta)\\) with respect to the policy parameters \\(\\theta\\). This theorem is crucial because it allows us to compute the gradient and perform gradient ascent to improve the policy. The policy gradient theorem states: \\[ \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\Bigl[\\nabla_{\\theta} \\ln \\pi(a\\mid s,\\theta)\\, Q_\\pi(s,a)\\Bigr]. \\] This equation is fundamental to policy gradient methods. It shows that the gradient of the performance objective can be estimated by averaging over trajectories sampled from the policy, where for each time step in a trajectory, we compute the product of the action-value function \\(Q_\\pi(s,a)\\) and the gradient of the log-policy \\(\\nabla_{\\theta} \\ln \\pi(a\\mid s,\\theta)\\).\nProof of Policy Gradient Theorem: (Detailed derivation already covered in the “Policy Gradient Theorem” section above.)\nAdvanced Policy Gradient Algorithms REINFORCE\nREINFORCE, also known as Monte Carlo Policy Gradient, is a basic policy gradient algorithm that directly implements the policy gradient theorem. It uses Monte Carlo estimates of the action-value function \\(Q_\\pi(s,a)\\) to update the policy parameters. In REINFORCE, the return \\(G_t\\) from an entire episode is used as an unbiased estimate of \\(Q_\\pi(S_t,A_t)\\). REINFORCE Algorithm: Initialize policy parameters \\(\\theta\\) randomly. For each episode: Generate an episode trajectory \\(\\tau = (S_1, A_1, R_2, S_2, A_2, \\ldots, S_T)\\) following policy \\(\\pi_\\theta\\). For each time step \\(t = 1, 2, \\ldots, T\\): Calculate the return \\[ G_t = \\sum_{k=0}^{T-t} \\gamma^k R_{t+k+1}. \\] Update policy parameters using gradient ascent: \\[ \\theta \\leftarrow \\theta + \\alpha\\, G_t\\, \\nabla_{\\theta} \\ln \\pi\\bigl(A_t \\mid S_t,\\theta\\bigr). \\] Explanation: REINFORCE updates the policy parameters in the direction that increases the probability of actions that led to higher returns in the episode. The gradient update is proportional to \\(G_t\\) and \\(\\nabla_{\\theta} \\ln \\pi(A_t\\mid S_t,\\theta)\\). Actor-Critic Variants\nA2C (Advantage Actor-Critic): Advantage Actor-Critic (A2C) is the synchronous, deterministic counterpart to Asynchronous Advantage Actor-Critic (A3C). A2C runs multiple agents in parallel environments and waits for all agents to complete their steps before performing a synchronized update of the global network. This synchronous update often leads to more stable and efficient learning, especially when using GPUs for training. A2C typically uses the advantage function \\(A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)\\) to reduce variance and improve the gradient signal. A3C (Asynchronous Advantage Actor-Critic): Asynchronous Advantage Actor-Critic (A3C) is a parallel, asynchronous framework where multiple independent agents (actors) run in parallel environments to collect experience and update a shared global network asynchronously. A3C is known for its efficiency in utilizing multi-core CPUs. It typically uses the advantage function \\(A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)\\) to reduce variance in policy gradient updates. Off-Policy Policy Gradient\nOff-Policy Policy Gradient methods learn a policy using data generated by a different policy, called the behavior policy, rather than the policy being optimized, called the target policy. This is in contrast to on-policy methods, where the behavior policy and target policy are the same. Off-policy methods offer several advantages: Sample Efficiency: They can reuse past experiences stored in a replay buffer, making learning more data-efficient. Exploration: They can use a behavior policy that is more exploratory than the target policy, facilitating better exploration of the environment. Flexibility: They allow learning from diverse datasets, including data collected from human experts or other agents. Importance Sampling: A key technique in off-policy policy gradient methods is importance sampling. It is used to correct for the mismatch between the distribution of data generated by the behavior policy and the distribution under the target policy. For policy gradient, importance sampling weights are used to adjust the gradient estimates. For example, in off-policy policy gradient updates, the gradient is weighted by the ratio \\(\\frac{\\pi_\\theta(a\\mid s)}{\\beta(a\\mid s)}\\), where \\(\\beta\\) is the behavior policy. Advantage Actor-Critic (A2C)\nAdvantage Actor-Critic (A2C) is the synchronous counterpart to A3C, running multiple agents in parallel environments and performing synchronized updates. It uses the advantage function to reduce variance in the policy gradient estimates, leading to more stable and efficient learning. Deterministic Policy Gradient (DPG)\nDeterministic Policy Gradient (DPG) is a policy gradient algorithm designed for continuous action spaces. Unlike stochastic policy gradient methods that learn a probability distribution over actions, DPG learns a deterministic policy \\(\\mu_\\theta(s)\\) that directly outputs a specific action for each state. DPG is based on the Deterministic Policy Gradient Theorem, which provides a way to compute the gradient of the performance objective for deterministic policies. Deterministic Policy Gradient Theorem: \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho^\\mu}\\bigl[\\nabla_a Q_\\mu(s,a)\\,\\nabla_\\theta \\mu_\\theta(s)\\bigr] \\Bigm|_{a = \\mu_\\theta(s)}, \\] where \\(\\rho^\\mu(s)\\) is the state distribution under the deterministic policy \\(\\mu_\\theta(s)\\), and \\(Q_\\mu(s,a)\\) is the action-value function for policy \\(\\mu_\\theta(s)\\). Deep Deterministic Policy Gradient (DDPG)\nDeep Deterministic Policy Gradient (DDPG) is an off-policy actor-critic algorithm that combines DPG with techniques from DQN to enable deep reinforcement learning in continuous action spaces. DDPG is essentially a deep learning version of DPG, incorporating experience replay and target networks to stabilize training. Key Features of DDPG: Actor-Critic Architecture: Uses two neural networks: an actor network \\(\\mu_\\theta(s)\\) and a critic network \\(Q_w(s,a)\\). Experience Replay: Stores transitions in a replay buffer and samples mini-batches for updates, similar to DQN. Target Networks: Uses target networks for both actor \\(\\mu_{\\theta^-}(s)\\) and critic \\(Q(s,a; w^-)\\), updated slowly by soft updates. Deterministic Policy: Learns a deterministic policy, well-suited for continuous action spaces. Exploration: Uses Ornstein-Uhlenbeck process or Gaussian noise added to actions for exploration. Distributed Distributional DDPG (D4PG)\nDistributed Distributional DDPG (D4PG) is an extension of DDPG that incorporates: Distributional Critic: Learns a distribution over returns instead of a single Q-value. N-step Returns: Uses n-step TD targets to reduce variance. Multiple Distributed Parallel Actors: Collects experience in parallel, improving data throughput. Prioritized Experience Replay (PER): Samples transitions with probabilities proportional to their TD errors. Multi-Agent DDPG (MADDPG)\nMulti-Agent DDPG (MADDPG) extends DDPG to multi-agent environments, addressing non-stationarity where multiple agents learn simultaneously. Centralized Critic, Decentralized Actors: Each agent’s critic has access to global information, while each actor only sees its local observations. Learning with Other Agents’ Policies: Helps handle changing dynamics as other agents learn. Trust Region Policy Optimization (TRPO)\nTrust Region Policy Optimization (TRPO) is an on-policy policy gradient algorithm that ensures monotonic policy improvement by constraining the size of each policy update, measured by KL divergence. TRPO Objective: \\[ \\max_{\\theta}\\,\\mathbb{E}_{s \\sim \\rho^{\\pi_{\\theta_\\text{old}}},\\,a \\sim \\pi_{\\theta_\\text{old}}} \\Bigl[\\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_\\text{old}}(a\\mid s)}\\,\\hat{A}_{\\theta_\\text{old}}(s,a)\\Bigr] \\] subject to a KL-divergence constraint \\(\\le \\delta\\). Explanation: TRPO keeps the new policy \\(\\pi_\\theta\\) within a “trust region” of the old policy \\(\\pi_{\\theta_\\text{old}}\\), preventing large, destabilizing updates. Proximal Policy Optimization (PPO)\nProximal Policy Optimization (PPO) simplifies TRPO and is more practical to implement while maintaining similar performance. PPO uses a clipped surrogate objective to bound the policy update. PPO Clipped Objective: \\[ J^{\\mathrm{CLIP}}(\\theta) = \\mathbb{E}\\Bigl[\\min\\bigl(r(\\theta)\\,\\hat{A}_{\\theta_\\text{old}}(s,a),\\,\\mathrm{clip}\\bigl(r(\\theta),1-\\epsilon,1+\\epsilon\\bigr)\\,\\hat{A}_{\\theta_\\text{old}}(s,a)\\bigr)\\Bigr], \\] where \\(r(\\theta) = \\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_\\text{old}}(a\\mid s)}\\). Explanation: By clipping the ratio \\(r(\\theta)\\), PPO avoids excessively large updates that could harm performance. Phasic Policy Gradient (PPG)\nPhasic Policy Gradient (PPG) is an on-policy algorithm that separates policy and value function updates into distinct phases, improving sample efficiency and stability. Policy Phase: Optimizes a PPO-like objective. Auxiliary Phase: Improves the value function and keeps policy from drifting too far. Benefits: More stable updates and better sample reuse compared to PPO. Actor-Critic with Experience Replay (ACER)\nActor-Critic with Experience Replay (ACER) is an off-policy actor-critic algorithm combining experience replay, Retrace for stable off-policy Q-value estimation, truncated importance weights with bias correction, and an efficient TRPO-like update. Key Ideas: Off-policy correction via truncated importance sampling. Retrace for unbiased and low-variance Q-value estimation. Trust region updates for stable learning. Actor-Critic using Kronecker-Factored Trust Region (ACKTR)\nActor-Critic using Kronecker-Factored Trust Region (ACKTR) leverages Kronecker-Factored Approximate Curvature (K-FAC) for more efficient and stable updates. K-FAC: Approximates the Fisher information matrix via Kronecker products for natural gradient updates. Natural Gradient: Helps optimize in directions that consider curvature, often improving convergence speed and stability. Soft Actor-Critic (SAC)\nSoft Actor-Critic (SAC) is an off-policy actor-critic algorithm featuring maximum entropy RL. It maximizes both return and policy entropy, encouraging exploration and robustness. Maximum Entropy Objective: \\[ J(\\pi) = \\sum_{t=1}^{T} \\mathbb{E}\\bigl[r(s_t,a_t) + \\alpha\\,\\mathcal{H}\\bigl(\\pi_\\theta(\\cdot\\mid s_t)\\bigr)\\bigr], \\] where \\(\\alpha\\) is a temperature parameter. Soft Q-function and Soft Value Function: Incorporates entropy terms into the Bellman backup. Automatic Temperature Adjustment: \\(\\alpha\\) can be learned to balance exploration and exploitation. Twin Delayed DDPG (TD3)\nTwin Delayed DDPG (TD3) is an improvement over DDPG that addresses overestimation bias and improves stability. Clipped Double Q-learning: Uses two critics and takes the minimum of both Q-values to reduce overestimation. Delayed Policy Updates: Updates the policy less frequently than the critics. Target Policy Smoothing: Adds noise to target actions for smoother Q-value estimates. Stein Variational Policy Gradient (SVPG)\nStein Variational Policy Gradient (SVPG) uses Stein Variational Gradient Descent (SVGD) to maintain an ensemble of policy “particles,” encouraging diversity. SVGD: Iteratively updates a set of particles to approximate a target distribution. Kernel Function: Encourages particles to spread out, improving exploration. Importance Weighted Actor-Learner Architectures (IMPALA)\nImportance Weighted Actor-Learner Architectures (IMPALA) is a highly scalable RL framework with decoupled actors and learners, using V-trace to handle off-policy corrections. Decoupled Architecture: Multiple actors generate data in parallel, sending trajectories to a central learner. V-trace: Corrects off-policy data from actors running slightly outdated policies. High Throughput: Achieves efficient large-scale training in complex environments. ","wordCount":"7230","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-01-31T12:00:00+08:00","dateModified":"2025-01-31T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/rl-introduction/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Deep Reinforcement Learning (Ongoing Updates)</h1><div class=post-meta><span title='2025-01-31 12:00:00 +0800 +0800'>2025-01-31</span>&nbsp;·&nbsp;34 min&nbsp;·&nbsp;7230 words&nbsp;·&nbsp;Yue Shui</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#notations>Notations</a></li><li><a href=#what-is-reinforcement-learning>What is Reinforcement Learning?</a><ul><li><a href=#definition>Definition</a></li><li><a href=#applications>Applications</a></li><li><a href=#policy>Policy</a></li><li><a href=#trajectories>Trajectories</a></li><li><a href=#reward-and-return>Reward and Return</a><ul><li><a href=#reward>Reward</a></li><li><a href=#return>Return</a></li><li><a href=#why-use-a-discount-factor>Why Use a Discount Factor?</a></li><li><a href=#finite-horizon-return>Finite Horizon Return</a></li></ul></li><li><a href=#transition-function-and-reward-function>Transition Function and Reward Function</a><ul><li><a href=#transition-function>Transition Function</a></li><li><a href=#reward-function>Reward Function</a></li></ul></li><li><a href=#value-function>Value Function</a><ul><li><a href=#state-value-function>State-Value Function</a></li><li><a href=#action-value-function-q-function>Action-Value Function (Q-function)</a></li></ul></li><li><a href=#advantage-function>Advantage Function</a><ul><li><a href=#significance-of-the-advantage-function>Significance of the Advantage Function</a></li></ul></li><li><a href=#optimal-value-and-policy>Optimal Value and Policy</a><ul><li><a href=#optimal-state-value-function>Optimal State-Value Function</a></li><li><a href=#optimal-action-value-function>Optimal Action-Value Function</a></li><li><a href=#optimal-policy>Optimal Policy</a></li><li><a href=#challenges-in-finding-the-optimal-policy>Challenges in Finding the Optimal Policy</a></li></ul></li></ul></li><li><a href=#a-taxonomy-of-rl-algorithms>A Taxonomy of RL Algorithms</a><ul><li><a href=#overview>Overview</a></li></ul></li><li><a href=#key-insights-from-the-rl-algorithms-taxonomy>Key Insights from the RL Algorithms Taxonomy</a><ul><li><a href=#1-model-free-rl-policy-optimization>1. Model-Free RL: Policy Optimization</a></li><li><a href=#2-model-free-rl-q-learning>2. Model-Free RL: Q-Learning</a></li><li><a href=#3-model-based-rl>3. Model-Based RL</a></li><li><a href=#4-hybrid--in-between-approaches>4. Hybrid / In-Between Approaches</a></li><li><a href=#summary-of-insights>Summary of Insights</a></li></ul></li><li><a href=#others>Others</a><ul><li><a href=#markov-decision-processes-mdps>Markov Decision Processes (MDPs)</a></li><li><a href=#bellman-equations>Bellman Equations</a></li></ul></li><li><a href=#2-fundamental-approaches>2. Fundamental Approaches</a><ul><li><a href=#dynamic-programming>Dynamic Programming</a></li><li><a href=#monte-carlo-methods>Monte-Carlo Methods</a></li><li><a href=#temporal-difference-td-learning>Temporal-Difference (TD) Learning</a></li><li><a href=#combining-td-and-monte-carlo-learning>Combining TD and Monte-Carlo Learning</a></li><li><a href=#policy-gradient-methods>Policy Gradient Methods</a></li><li><a href=#advanced-policy-gradient-algorithms>Advanced Policy Gradient Algorithms</a></li></ul></li></ul></nav></div></details></div><div class=post-content><blockquote><p><strong>Note</strong>: This article <strong>is currently being updated</strong>. The content is in <strong>draft version</strong> and may change. Please check back for the latest version.</p></blockquote><h2 id=notations>Notations<a hidden class=anchor aria-hidden=true href=#notations>#</a></h2><table><thead><tr><th style=text-align:left>Symbol</th><th style=text-align:left>Meaning</th></tr></thead><tbody><tr><td style=text-align:left>\(s, s', S_t, S_{t+1}\)</td><td style=text-align:left>State, next state, state at time \(t\), state at time \(t+1\)</td></tr><tr><td style=text-align:left>\(o, o_t\)</td><td style=text-align:left>Observation, observation at time \(t\)</td></tr><tr><td style=text-align:left>\(a, a', A_t, A_{t+1}\)</td><td style=text-align:left>Action, next action, action at time \(t\), action at time \(t+1\)</td></tr><tr><td style=text-align:left>\(r, r_t\)</td><td style=text-align:left>Immediate reward, reward at time \(t\)</td></tr><tr><td style=text-align:left>\(G_t\)</td><td style=text-align:left>Return at time \(t\)</td></tr><tr><td style=text-align:left>\(R(\tau)\)</td><td style=text-align:left>Return of a trajectory \(\tau\)</td></tr><tr><td style=text-align:left>\(\mathcal{S}\)</td><td style=text-align:left>Set of all possible states</td></tr><tr><td style=text-align:left>\(\mathcal{A}\)</td><td style=text-align:left>Set of all possible actions</td></tr><tr><td style=text-align:left>\(\mathcal{R}\)</td><td style=text-align:left>Set of all possible rewards</td></tr><tr><td style=text-align:left>\(\pi(a\mid s), \pi_\theta(a\mid s)\)</td><td style=text-align:left>Policy (stochastic), parameterized policy</td></tr><tr><td style=text-align:left>\(\mu(s), \mu_\theta(s)\)</td><td style=text-align:left>Policy (deterministic), parameterized policy</td></tr><tr><td style=text-align:left>\(\theta, \phi, w\)</td><td style=text-align:left>Policy or value function parameters</td></tr><tr><td style=text-align:left>\(\gamma\)</td><td style=text-align:left>Discount factor</td></tr><tr><td style=text-align:left>\(J(\pi)\)</td><td style=text-align:left>Expected return of policy \(\pi\)</td></tr><tr><td style=text-align:left>\(V_\pi(s)\)</td><td style=text-align:left>State-value function for policy \(\pi\)</td></tr><tr><td style=text-align:left>\(Q_\pi(s,a)\)</td><td style=text-align:left>Action-value function for policy \(\pi\)</td></tr><tr><td style=text-align:left>\(V_*(s)\)</td><td style=text-align:left>Optimal state-value function</td></tr><tr><td style=text-align:left>\(Q_*(s,a)\)</td><td style=text-align:left>Optimal action-value function</td></tr><tr><td style=text-align:left>\(A_\pi(s,a)\)</td><td style=text-align:left>Advantage function for policy \(\pi\)</td></tr><tr><td style=text-align:left>\(P(s'\mid s,a)\)</td><td style=text-align:left>Transition probability function</td></tr><tr><td style=text-align:left>\(R(s,a,s')\)</td><td style=text-align:left>Reward function</td></tr><tr><td style=text-align:left>\(\rho_0(s)\)</td><td style=text-align:left>Start-state distribution</td></tr><tr><td style=text-align:left>\(\tau\)</td><td style=text-align:left>Trajectory</td></tr><tr><td style=text-align:left>\(D\)</td><td style=text-align:left>Replay memory</td></tr><tr><td style=text-align:left>\(\alpha\)</td><td style=text-align:left>Learning rate, temperature parameter (in SAC)</td></tr><tr><td style=text-align:left>\(\lambda\)</td><td style=text-align:left>Eligibility trace parameter</td></tr><tr><td style=text-align:left>\(\epsilon\)</td><td style=text-align:left>Exploration parameter (e.g., in \(\epsilon\)-greedy), clipping parameter (in PPO)</td></tr></tbody></table><h2 id=what-is-reinforcement-learning>What is Reinforcement Learning?<a hidden class=anchor aria-hidden=true href=#what-is-reinforcement-learning>#</a></h2><h3 id=definition>Definition<a hidden class=anchor aria-hidden=true href=#definition>#</a></h3><p><img alt="alt text" loading=lazy src=/posts/rl-introduction/image.png></p><p>Reinforcement Learning (RL) is a branch of machine learning that trains an <strong>agent</strong> to take a series of <strong>actions</strong> (\(a_t\)) in an <strong>environment</strong>, transitioning through different <strong>states</strong> (\(s_t\)) to achieve a long-terms goal.</p><p>Unlike supervised learning, which relies on human-labeled data, RL depends on the interaction between the agent and the environment. After each action, the agent receives a <strong>reward</strong> (\(r_t\)) as feedback. The objective of the agent is to learn a <strong>policy</strong> $\pi(s)$, which is a strategy for selecting actions, in order to <strong>maximize the total reward</strong> (\(\sum_{t=0}^{T} r_t\)).</p><h3 id=applications>Applications<a hidden class=anchor aria-hidden=true href=#applications>#</a></h3><p>RL has achieved remarkable successes in various domains, including:</p><ul><li><strong>Game Playing</strong>: Mastering complex games like Go (AlphaGo, AlphaGo Zero) and video games (Atari, Dota 2).</li><li><strong>Robotics</strong>: Controlling robots for tasks like navigation, manipulation, and locomotion.</li><li><strong>Autonomous Driving</strong>: Developing self-driving vehicles that can perceive their environment and make driving decisions.</li><li><strong>Resource Management</strong>: Optimizing resource allocation in areas like energy management and traffic control.</li><li><strong>Personalized Recommendations</strong>: Creating recommendation systems that adapt to user preferences over time.</li></ul><h3 id=policy>Policy<a hidden class=anchor aria-hidden=true href=#policy>#</a></h3><p>A <strong>policy</strong> \(\pi\) is the strategy an agent employs to decide which action to take in each state. It is the cornerstone of an RL agent, defining its behavior. Policies can be either <strong>deterministic</strong> or <strong>stochastic</strong>.</p><ul><li><p><strong>Deterministic Policy</strong> \(\pi(s)\): Maps each state to a single, specific action. For a given state \(s\), the policy \(\pi(s)\) always selects the same action \(a\):</p>$$
\pi(s) = a
$$</li><li><p><strong>Stochastic Policy</strong> \(\pi(a \mid s)\): Provides a probability distribution over possible actions for each state. For a given state \(s\), \(\pi(a \mid s)\) represents the probability of choosing action \(a\). The agent samples an action based on this distribution:</p>$$
\pi(a \mid s) = \mathbb{P}_\pi[A = a \mid S = s]
$$</li><li><p>In <strong>Deep Reinforcement Learning</strong>, policies are typically represented by sophisticated function approximators, such as <strong>neural networks</strong>. These networks, parameterized by weights \(\theta\), learn to map states (or observations) to actions (or action probabilities). The parameterized policies are denoted as:</p><ul><li>\(\pi_\theta(s)\) for <strong>deterministic policies</strong></li><li>\(\pi_\theta(a \mid s)\) for <strong>stochastic policies</strong></li></ul></li></ul><h3 id=trajectories>Trajectories<a hidden class=anchor aria-hidden=true href=#trajectories>#</a></h3><p>A <strong>trajectory</strong> (also called <strong>episode</strong>) \(\tau\) is a sequence of states and actions that unfold in an environment:</p>\[
\tau = (s_0, a_0, s_1, a_1, \ldots)
\]<p>The initial state of the environment, \(s_0\), is sampled from a predefined start-state distribution, often represented as \(\rho_0\):</p>\[
s_0 \sim \rho_0(\cdot)
\]<p>State transitions describe how the environment evolves from one state to the next, specifically from state \(s_t\) at time \(t\) to state \(s_{t+1}\) at time \(t+1\). These transitions are governed by the natural laws of the environment and are influenced solely by the most recent action taken \(a_t\). The transition dynamics can be either:</p><ul><li><p><strong>Deterministic</strong>:</p>\[
s_{t+1} = f(s_t, a_t)
\]</li><li><p><strong>Stochastic</strong>:</p>\[
s_{t+1} \sim P(\cdot \mid s_t, a_t)
\]</li></ul><h3 id=reward-and-return>Reward and Return<a hidden class=anchor aria-hidden=true href=#reward-and-return>#</a></h3><p>In reinforcement learning, <strong>rewards</strong> and <strong>returns</strong> are fundamental concepts that guide an agent&rsquo;s learning process by providing feedback on its actions.</p><h4 id=reward>Reward<a hidden class=anchor aria-hidden=true href=#reward>#</a></h4><p>A <strong>reward</strong> is a scalar signal received by the agent after taking an action in a specific state. It serves as immediate feedback to indicate the desirability of the action taken. The reward function, denoted by \( R \), maps state-action pairs to real numbers:</p>\[
r_t = R(s_t, a_t)
\]<p>where:</p><ul><li>\( r_t \) is the reward received at time step \( t \),</li><li>\( s_t \) is the state at time step \( t \),</li><li>\( a_t \) is the action taken at time step \( t \).</li></ul><p>The reward function encapsulates the goals of the agent by assigning higher rewards to desirable outcomes and lower (or negative) rewards to undesirable ones.</p><h4 id=return>Return<a hidden class=anchor aria-hidden=true href=#return>#</a></h4><p>The <strong>return</strong>, often denoted by \( G_t \), represents the <strong>total accumulated future rewards</strong> from a specific time step onward. It quantifies the long-term benefit of actions taken by the agent. The return is defined as the sum of discounted rewards:</p>\[
G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}
\]<p>where:</p><ul><li>\( \gamma \in [0, 1) \) is the <strong>discount factor</strong> that determines the present value of future rewards,</li><li>\( r_{t+k} \) is the reward received \( k \) time steps after \( t \).</li></ul><p>The discount factor \( \gamma \) balances the importance of immediate versus future rewards:</p><ul><li>A <strong>higher \( \gamma \)</strong> (close to 1) makes the agent strive for long-term rewards.</li><li>A <strong>lower \( \gamma \)</strong> (close to 0) makes the agent prioritize immediate rewards.</li></ul><h4 id=why-use-a-discount-factor>Why Use a Discount Factor?<a hidden class=anchor aria-hidden=true href=#why-use-a-discount-factor>#</a></h4><p>The discount factor \( \gamma \) serves several important purposes in reinforcement learning:</p><ol><li><p><strong>Handling Uncertainty of Future Rewards</strong>:</p><ul><li><strong>Higher Uncertainty</strong>: Future rewards are often more uncertain than immediate rewards. For example, in the stock market, predicting long-term returns is more challenging due to market volatility.</li></ul></li><li><p><strong>Preference for Immediate Benefits</strong>:</p><ul><li><strong>Human Behavior</strong>: As humans, we might prefer to enjoy rewards today rather than waiting for them years later. This preference is naturally modeled by discounting future rewards.</li></ul></li><li><p><strong>Mathematical Convenience</strong>:</p><ul><li><strong>Finite Computation</strong>: Discounting allows us to compute returns without needing to track future steps indefinitely, simplifying calculations and algorithms.</li></ul></li><li><p><strong>Avoiding Infinite Loops</strong>:</p><ul><li><strong>Termination Assurance</strong>: In environments with the possibility of infinite loops in state transitions, discounting ensures that the return remains finite, preventing issues with infinite sums.</li></ul></li></ol><h4 id=finite-horizon-return>Finite Horizon Return<a hidden class=anchor aria-hidden=true href=#finite-horizon-return>#</a></h4><p>In scenarios with a finite number of time steps \( T \), the return is calculated up to the terminal time step:</p>\[
G_t = \sum_{k=0}^{T-t-1} \gamma^k r_{t+k}
\]<p>This is common in episodic tasks where the interaction between the agent and the environment terminates after a certain number of steps.</p><h3 id=transition-function-and-reward-function>Transition Function and Reward Function<a hidden class=anchor aria-hidden=true href=#transition-function-and-reward-function>#</a></h3><p>In Reinforcement Learning (RL), a <strong>model</strong> encapsulates the agent’s representation of how the environment behaves. This model typically includes two core components: the <strong>transition function</strong> and the <strong>reward function</strong>.</p><h4 id=transition-function>Transition Function<a hidden class=anchor aria-hidden=true href=#transition-function>#</a></h4><p>The <strong>transition function</strong> \( P(s' \mid s, a) \) specifies the probability of moving from state \( s \) to state \( s' \) after taking action \( a \). Formally:</p>\[
P(s' \mid s, a) = \mathbb{P}(S_{t+1} = s' \mid S_t = s, A_t = a)
\]<ul><li><p><strong>Deterministic Environment</strong>: In a deterministic setting, the transition function assigns a probability of 1 to a single specific next state and 0 to all others:</p>\[
P(s' \mid s, a) =
\begin{cases}
1 & \text{if } s' = f(s, a),\\
0 & \text{otherwise}.
\end{cases}
\]</li><li><p><strong>Stochastic Environment</strong>: In a stochastic environment, the transition function defines a probability distribution over possible next states:</p>\[
P(s' \mid s, a) = \text{Probability of transitioning to } s' \text{ from } s \text{ by taking action } a.
\]</li></ul><h4 id=reward-function>Reward Function<a hidden class=anchor aria-hidden=true href=#reward-function>#</a></h4><p>The <strong>reward function</strong> \( R(s, a, s') \) specifies the immediate reward obtained after transitioning from state \( s \) to state \( s' \) via action \( a \). It provides essential feedback that guides the agent&rsquo;s learning. Formally:</p>\[
R(s, a, s') = \mathbb{E}\bigl[ R_{t+1} \mid S_t = s, A_t = a, S_{t+1} = s' \bigr]
\]<p>Depending on the problem, the reward function might depend on:</p><ul><li><p><strong>State-Action-State</strong>:</p>\[
R(s, a, s') = \text{Immediate reward after transitioning from } s \text{ to } s' \text{ using } a.
\]</li><li><p><strong>State-Action</strong>:</p>\[
R(s, a) = \mathbb{E}\bigl[ R_{t+1} \mid S_t = s, A_t = a \bigr].
\]</li><li><p><strong>State Only</strong>:</p>\[
R(s) = \mathbb{E}\bigl[ R_{t+1} \mid S_t = s \bigr].
\]</li></ul><h3 id=value-function>Value Function<a hidden class=anchor aria-hidden=true href=#value-function>#</a></h3><p><strong>Value functions</strong> quantify the expected return (sum of discounted future rewards) starting from a given state (or state-action pair). They are central to most RL methods. There are two key types:</p><h4 id=state-value-function>State-Value Function<a hidden class=anchor aria-hidden=true href=#state-value-function>#</a></h4><p>The <strong>state-value function</strong> \( V_\pi(s) \) measures the expected return when starting in state \( s \) and following policy \( \pi \) thereafter:</p>\[
V_\pi(s) = \mathbb{E}_\pi\bigl[ G_t \mid S_t = s \bigr]
= \mathbb{E}_\pi\Bigl[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \;\big\vert\; S_t = s \Bigr].
\]<h4 id=action-value-function-q-function>Action-Value Function (Q-function)<a hidden class=anchor aria-hidden=true href=#action-value-function-q-function>#</a></h4><p>The <strong>action-value function</strong> \( Q_\pi(s, a) \) measures the expected return when starting in state \( s \), taking action \( a \), and thereafter following policy \( \pi \):</p>\[
Q_\pi(s, a) = \mathbb{E}_\pi\bigl[ G_t \mid S_t = s, A_t = a \bigr]
= \mathbb{E}_\pi\Bigl[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \;\big\vert\; S_t = s, A_t = a \Bigr].
\]<h3 id=advantage-function>Advantage Function<a hidden class=anchor aria-hidden=true href=#advantage-function>#</a></h3><p>The <strong>advantage function</strong> \( A_\pi(s, a) \) indicates how much better (or worse) taking action \( a \) in state \( s \) is compared to the average action under policy \( \pi \). It is defined as:</p>\[
A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s).
\]<h4 id=significance-of-the-advantage-function>Significance of the Advantage Function<a hidden class=anchor aria-hidden=true href=#significance-of-the-advantage-function>#</a></h4><ul><li><strong>Variance Reduction</strong>: In policy gradient methods, using the advantage function can reduce the variance of gradient estimates, leading to more stable learning.</li><li><strong>Policy Improvement</strong>: It highlights which actions are better or worse than the average in a given state, providing a clearer signal for policy updates.</li></ul><h3 id=optimal-value-and-policy>Optimal Value and Policy<a hidden class=anchor aria-hidden=true href=#optimal-value-and-policy>#</a></h3><p>The ultimate goal in RL is to find an <strong>optimal policy</strong> \( \pi_* \) that maximizes the expected return in the long run. Correspondingly, we define <strong>optimal value functions</strong>.</p><h4 id=optimal-state-value-function>Optimal State-Value Function<a hidden class=anchor aria-hidden=true href=#optimal-state-value-function>#</a></h4><p>The <strong>optimal state-value function</strong> \( V_*(s) \) is the maximum state-value attainable at state \( s \) over all possible policies:</p>\[
V_*(s) = \max_{\pi} V_\pi(s).
\]<h4 id=optimal-action-value-function>Optimal Action-Value Function<a hidden class=anchor aria-hidden=true href=#optimal-action-value-function>#</a></h4><p>The <strong>optimal action-value function</strong> \( Q_*(s, a) \) is the maximum action-value attainable for the state-action pair \( (s, a) \) over all possible policies:</p>\[
Q_*(s, a) = \max_{\pi} Q_\pi(s, a).
\]<h4 id=optimal-policy>Optimal Policy<a hidden class=anchor aria-hidden=true href=#optimal-policy>#</a></h4><p>An <strong>optimal policy</strong> \( \pi_* \) is any policy that achieves these optimal value functions. Formally, for all \( s \) and \( a \):</p>\[
V_{\pi_*}(s) = V_*(s), \quad\text{and}\quad Q_{\pi_*}(s, a) = Q_*(s, a).
\]<h4 id=challenges-in-finding-the-optimal-policy>Challenges in Finding the Optimal Policy<a hidden class=anchor aria-hidden=true href=#challenges-in-finding-the-optimal-policy>#</a></h4><ul><li><strong>Complexity</strong>: Finding \( \pi_* \), \( V_*(s) \), or \( Q_*(s, a) \) can be computationally expensive, especially for large or continuous state and action spaces.</li><li><strong>Approximation</strong>: Many RL algorithms approximate these optimal functions or directly learn an (approximately) optimal policy through techniques such as dynamic programming, Monte Carlo methods, or temporal-difference learning.</li></ul><h2 id=a-taxonomy-of-rl-algorithms>A Taxonomy of RL Algorithms<a hidden class=anchor aria-hidden=true href=#a-taxonomy-of-rl-algorithms>#</a></h2><p>Now that we’ve gone through the basics of RL terminology and notation, we can delve into the richer material: the landscape of algorithms in modern RL, along with the trade-offs involved in their design.</p><p><img alt="A non-exhaustive, but useful taxonomy of algorithms in modern RL." loading=lazy src=/posts/rl-introduction/rl_algorithms_9_15.svg></p><h3 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h3><p>Creating an accurate and comprehensive taxonomy of modern RL algorithms is challenging due to the modularity and complexity of these algorithms. To keep this introduction digestible, we focus on the foundational design choices in deep RL algorithms, the trade-offs these choices entail, and contextualize some prominent modern algorithms within this framework.</p><h2 id=key-insights-from-the-rl-algorithms-taxonomy>Key Insights from the RL Algorithms Taxonomy<a hidden class=anchor aria-hidden=true href=#key-insights-from-the-rl-algorithms-taxonomy>#</a></h2><p>The comparative table provides a structured overview of various Reinforcement Learning (RL) approaches. Below, we break down the table contents into detailed key insights to better understand the distinctions, strengths, and challenges associated with each category of RL algorithms.</p><h3 id=1-model-free-rl-policy-optimization>1. Model-Free RL: Policy Optimization<a hidden class=anchor aria-hidden=true href=#1-model-free-rl-policy-optimization>#</a></h3><ul><li><p><strong>Key Idea</strong>:<br>Policy Optimization methods focus on directly learning a parameterized policy \( \pi_\theta(a \mid s) \) by maximizing a performance objective \( J(\pi) \). These methods often incorporate learning a value function \( V^\pi(s) \) to facilitate more effective policy updates.</p></li><li><p><strong>On/Off-Policy Setting</strong>:<br>Primarily <strong>on-policy</strong>, meaning updates are based on data collected from the most recent policy.</p></li><li><p><strong>Strengths</strong>:</p><ul><li><strong>Stability</strong>: By directly optimizing the policy objective, these methods tend to exhibit stable and reliable convergence.</li><li><strong>Simplicity in Updates</strong>: Utilizes gradient-based approaches, making the update process conceptually straightforward.</li></ul></li><li><p><strong>Weaknesses</strong>:</p><ul><li><strong>Sample Inefficiency</strong>: Requires fresh interaction data from the current policy for each update, which can be resource-intensive.</li><li><strong>High Variance</strong>: Gradient estimates can exhibit high variance, potentially slowing down the learning process.</li></ul></li><li><p><strong>Representative Examples</strong>:<br>A2C, A3C, PPO, TRPO</p></li></ul><h3 id=2-model-free-rl-q-learning>2. Model-Free RL: Q-Learning<a hidden class=anchor aria-hidden=true href=#2-model-free-rl-q-learning>#</a></h3><ul><li><p><strong>Key Idea</strong>:<br>Q-Learning methods aim to learn an approximate Q-function \( Q_\theta(s, a) \) that estimates the optimal action-value function \( Q^*(s, a) \). The policy is then derived by selecting actions that maximize the Q-values.</p></li><li><p><strong>On/Off-Policy Setting</strong>:<br>Primarily <strong>off-policy</strong>, allowing the use of data collected from any policy during training.</p></li><li><p><strong>Strengths</strong>:</p><ul><li><strong>Sample Efficiency</strong>: Can reuse past experiences effectively, making better use of available data.</li><li><strong>Straightforward Objective</strong>: Relies on Bellman backups, providing a clear and direct learning target.</li></ul></li><li><p><strong>Weaknesses</strong>:</p><ul><li><strong>Stability Issues</strong>: Susceptible to divergence and instability, especially when combined with function approximation.</li><li><strong>Indirect Performance Optimization</strong>: Optimizes the Q-function rather than the policy directly, which can complicate the learning process.</li></ul></li><li><p><strong>Representative Examples</strong>:<br>DQN, C51, QR-DQN</p></li></ul><h3 id=3-model-based-rl>3. Model-Based RL<a hidden class=anchor aria-hidden=true href=#3-model-based-rl>#</a></h3><ul><li><p><strong>Key Idea</strong>:<br>Model-Based methods involve using or learning a model of the environment&rsquo;s dynamics (state transitions and rewards) to facilitate planning or generate additional training data.</p></li><li><p><strong>On/Off-Policy Setting</strong>:<br>Can be <strong>on-policy</strong> or <strong>off-policy</strong> depending on the specific algorithm design.</p></li><li><p><strong>Strengths</strong>:</p><ul><li><strong>High Sample Efficiency</strong>: Leveraging a model allows for planning and generating synthetic data, reducing the need for extensive real-world interactions.</li><li><strong>Forward Planning</strong>: Enables the agent to &ldquo;think ahead&rdquo; by simulating future states and rewards, leading to more informed decision-making.</li></ul></li><li><p><strong>Weaknesses</strong>:</p><ul><li><strong>Model Bias</strong>: Inaccuracies in the learned model can lead to suboptimal or even detrimental policy performance in the real environment.</li><li><strong>Implementation Complexity</strong>: Incorporating a model adds layers of complexity, making these methods harder to implement and tune effectively.</li></ul></li><li><p><strong>Representative Examples</strong>:<br>MBMF, MBVE, AlphaZero, World Models</p></li></ul><h3 id=4-hybrid--in-between-approaches>4. Hybrid / In-Between Approaches<a hidden class=anchor aria-hidden=true href=#4-hybrid--in-between-approaches>#</a></h3><ul><li><p><strong>Key Idea</strong>:<br>Hybrid methods blend elements from policy optimization, Q-Learning, and planning. For instance, they may learn both a Q-function and a policy simultaneously or embed planning mechanisms directly into the policy structure.</p></li><li><p><strong>On/Off-Policy Setting</strong>:<br>Varies across different algorithms; some are off-policy, others are on-policy, and some employ a mixed approach.</p></li><li><p><strong>Strengths</strong>:</p><ul><li><strong>Balanced Strengths</strong>: Capable of harnessing the advantages of multiple RL paradigms, such as the stability of policy optimization and the sample efficiency of Q-Learning.</li><li><strong>Enhanced Data Utilization</strong>: Can effectively reuse data while maintaining stable policy updates, leading to improved overall performance.</li></ul></li><li><p><strong>Weaknesses</strong>:</p><ul><li><strong>Implementation Complexity</strong>: Managing multiple components (e.g., separate networks for policy and value functions) increases the complexity of the algorithm.</li><li><strong>Inherited Failure Modes</strong>: Risks arising from combining different methods can lead to compounded instability or other issues from each constituent approach.</li></ul></li><li><p><strong>Representative Examples</strong>:<br>DDPG, SAC, I2A (Imagination-Augmented Agents)</p></li></ul><h3 id=summary-of-insights>Summary of Insights<a hidden class=anchor aria-hidden=true href=#summary-of-insights>#</a></h3><ol><li><p><strong>Diverse Strategies</strong>: RL algorithms can be broadly categorized into model-free and model-based approaches, each with distinct methodologies and trade-offs. Hybrid methods seek to combine these strategies to leverage their respective strengths.</p></li><li><p><strong>Policy Optimization vs. Q-Learning</strong>:</p><ul><li><strong>Policy Optimization</strong> offers stability and direct optimization but at the cost of sample efficiency.</li><li><strong>Q-Learning</strong> provides greater sample efficiency through data reuse but may suffer from stability issues.</li></ul></li><li><p><strong>Model-Based Advantages and Challenges</strong>:</p><ul><li>While model-based methods can significantly enhance sample efficiency and enable forward planning, they are often hindered by the difficulty of accurately modeling complex environments and the increased complexity of implementation.</li></ul></li><li><p><strong>Hybrid Approaches as a Middle Ground</strong>:</p><ul><li>By integrating aspects of both model-free and model-based methods, hybrid algorithms aim to achieve a balance between stability, sample efficiency, and performance. However, this integration introduces additional complexity and potential points of failure.</li></ul></li><li><p><strong>Representative Algorithms</strong>:</p><ul><li>Understanding where prominent algorithms like PPO, DQN, AlphaZero, and SAC fit within this taxonomy helps in selecting the appropriate method based on the specific requirements and constraints of the task at hand.</li></ul></li></ol><h2 id=others>Others<a hidden class=anchor aria-hidden=true href=#others>#</a></h2><h3 id=markov-decision-processes-mdps>Markov Decision Processes (MDPs)<a hidden class=anchor aria-hidden=true href=#markov-decision-processes-mdps>#</a></h3><p>In reinforcement learning, the interaction between an agent and its environment is often formalized as a <strong>Markov Decision Process (MDP)</strong>. MDPs provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. An MDP is defined by a 5-tuple \(\langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle\):</p><ul><li>\(\mathcal{S}\): A set of possible <strong>states</strong>. This is the set of all possible situations the agent can be in.</li><li>\(\mathcal{A}\): A set of possible <strong>actions</strong>. These are the actions the agent can take in each state.</li><li>\(P(s'\mid s,a)\): The <strong>transition probability function</strong>. It defines the probability of transitioning to a next state \(s'\) from the current state \(s\) when action \(a\) is taken. This function encapsulates the environment&rsquo;s dynamics.</li><li>\(R(s,a,s')\): The <strong>reward function</strong>. It defines the reward received by the agent after transitioning from state \(s\) to \(s'\) due to action \(a\). This function specifies the immediate feedback from the environment.</li><li>\(\gamma \in [0, 1]\): The <strong>discount factor</strong>. It is a value between 0 and 1 that discounts future rewards. A discount factor closer to 0 makes the agent prioritize immediate rewards, while a factor closer to 1 makes it value future rewards more.</li></ul><p>The defining characteristic of an MDP is the <strong>Markov property</strong>, which states that the future state and reward depend only on the current state and action, and not on the history of past states and actions. Formally, for any time step \(t\):</p>\[
\mathbb{P}\bigl[S_{t+1} \mid S_t, A_t\bigr]
= \mathbb{P}\bigl[S_{t+1} \mid S_1, A_1, S_2, A_2, \ldots, S_t, A_t\bigr]
\]<p>This property simplifies the problem significantly as the agent only needs to consider the current state to make optimal decisions, without needing to remember the entire history.</p><p><img alt="Markov Decision Process Diagram" loading=lazy src=path/to/mdp_diagram.png></p><p>In an MDP, the agent&rsquo;s goal is to find a policy \(\pi\) that maximizes the expected cumulative discounted reward, starting from some initial state distribution \(\rho_0(s)\). The sequence of states, actions, and rewards generated by an agent interacting with an MDP is called a <strong>trajectory</strong> or <strong>episode</strong>:</p>\[
\tau = \bigl(S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \ldots\bigr)
\]<p>The first state \(S_0\) is sampled from the start-state distribution \(\rho_0(\cdot)\). Subsequent states are determined by the transition probabilities \(P(s'\mid s,a)\), and rewards are given by the reward function \(R(s,a,s')\). Actions \(A_t\) are chosen by the agent according to its policy \(\pi(a\mid s)\).</p><h3 id=bellman-equations>Bellman Equations<a hidden class=anchor aria-hidden=true href=#bellman-equations>#</a></h3><p><strong>Bellman equations</strong> are a set of equations that lie at the heart of dynamic programming and reinforcement learning. They decompose the value function into two parts: the immediate reward and the discounted value of the next state. These equations express a recursive relationship that value functions must satisfy. There are two main types of Bellman equations: <strong>Bellman Expectation Equations</strong> and <strong>Bellman Optimality Equations</strong>.</p><ul><li><p><strong>Bellman Expectation Equations</strong></p><ul><li>Bellman Expectation Equations are used for <strong>policy evaluation</strong>, i.e., calculating the value functions \(V_\pi(s)\) and \(Q_\pi(s,a)\) for a given policy \(\pi\). They express the value of a state (or state-action pair) in terms of the expected immediate reward and the expected value of the next state, assuming the agent follows policy \(\pi\).<ul><li><p><strong>Bellman Expectation Equation for State-Value Function</strong> \(\bigl(V_\pi(s)\bigr)\):</p>\[
V_\pi(s)
= \mathbb{E}_{\pi}\bigl[R_{t+1} + \gamma V_\pi(S_{t+1}) \mid S_{t}=s\bigr]
\]<p>Expanding this expectation:</p>\[
V_\pi(s)
= \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{s' \in \mathcal{S}} P(s' \mid s,a)
\Bigl(R(s, a) + \gamma V_\pi(s')\Bigr)
\]<p><strong>Derivation</strong>:</p>\[
\begin{aligned}
V_\pi(s)
&= \mathbb{E}_{\pi}\bigl[G_t \mid S_t=s\bigr] \\
&= \mathbb{E}_{\pi}\bigl[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \mid S_t=s\bigr] \\
&= \mathbb{E}_{\pi}\bigl[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \cdots) \mid S_t=s\bigr] \\
&= \mathbb{E}_{\pi}\bigl[R_{t+1} + \gamma G_{t+1} \mid S_t=s\bigr] \\
&= \mathbb{E}_{\pi}\bigl[R_{t+1} + \gamma V_\pi(S_{t+1}) \mid S_t=s\bigr] \\
&= \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{s' \in \mathcal{S}} P(s' \mid s,a)
\bigl(R(s,a) + \gamma V_\pi(s')\bigr)
\end{aligned}
\]</li><li><p><strong>Bellman Expectation Equation for Action-Value Function</strong> \(\bigl(Q_\pi(s,a)\bigr)\):</p>\[
Q_\pi(s,a)
= \mathbb{E}_{\pi}\bigl[R_{t+1} + \gamma Q_\pi(S_{t+1}, A_{t+1}) \mid S_t=s, A_t=a\bigr]
\]<p>Expanding this expectation:</p>\[
Q_\pi(s,a)
= R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s,a)
\sum_{a' \in \mathcal{A}} \pi(a' \mid s')\, Q_\pi(s',a')
\]<p><strong>Derivation</strong>:</p>\[
\begin{aligned}
Q_\pi(s,a)
&= \mathbb{E}_{\pi}\bigl[G_t \mid S_t=s, A_t=a\bigr] \\
&= \mathbb{E}_{\pi}\bigl[R_{t+1} + \gamma V_\pi(S_{t+1}) \mid S_t=s, A_t=a\bigr] \\
&= \mathbb{E}_{\pi}\bigl[R_{t+1} + \gamma \mathbb{E}_{a' \sim \pi} Q_\pi(S_{t+1}, a')
\mid S_t=s, A_t=a\bigr] \\
&= R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s,a) \sum_{a' \in \mathcal{A}}
\pi(a' \mid s') Q_\pi(s',a')
\end{aligned}
\]</li></ul></li></ul></li><li><p><strong>Bellman Optimality Equations</strong></p><ul><li>Bellman Optimality Equations specify the conditions for optimality for value functions \(V_*(s)\) and \(Q_*(s,a)\). They express the optimal value of a state (or state-action pair) in terms of the optimal values of successor states, assuming optimal actions are taken.<ul><li><p><strong>Bellman Optimality Equation for Optimal State-Value Function</strong> \(\bigl(V_*(s)\bigr)\):</p>\[
V_*(s)
= \max_{a \in \mathcal{A}} \mathbb{E}\bigl[R_{t+1} + \gamma V_*(S_{t+1}) \mid S_t=s, A_t=a\bigr]
\]<p>Expanding this expectation:</p>\[
V_*(s)
= \max_{a \in \mathcal{A}} \Bigl(R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s,a)\, V_*(s')\Bigr)
\]<p><strong>Explanation</strong>: To achieve the optimal value in state \(s\), we should choose the action \(a\) that maximizes the sum of the immediate reward \(R(s,a)\) and the discounted optimal value of the next state \(V_*(s')\).</p></li><li><p><strong>Bellman Optimality Equation for Optimal Action-Value Function</strong> \(\bigl(Q_*(s,a)\bigr)\):</p>\[
Q_*(s,a)
= \mathbb{E}\bigl[R_{t+1} + \gamma \max_{a'} Q_*(S_{t+1}, a') \mid S_t=s, A_t=a\bigr]
\]<p>Expanding this expectation:</p>\[
Q_*(s,a)
= R(s,a) + \gamma \sum_{s' \in \mathcal{S}} P(s' \mid s,a)\, \max_{a' \in \mathcal{A}} Q_*(s',a')
\]<p><strong>Explanation</strong>: The optimal Q-value for a state-action pair \((s,a)\) is the immediate reward \(R(s,a)\) plus the discounted maximum Q-value achievable from the next state \(s'\), considering all possible actions \(a'\) in \(s'\).</p></li></ul></li></ul></li></ul><p>These Bellman equations form the basis for many reinforcement learning algorithms, providing a way to compute and improve value functions and policies.</p><h2 id=2-fundamental-approaches>2. Fundamental Approaches<a hidden class=anchor aria-hidden=true href=#2-fundamental-approaches>#</a></h2><h3 id=dynamic-programming>Dynamic Programming<a hidden class=anchor aria-hidden=true href=#dynamic-programming>#</a></h3><p><strong>Dynamic Programming (DP)</strong> provides a collection of algorithms that can be used to compute optimal policies in MDPs, given a complete model of the environment. DP methods are particularly useful when the environment is fully known, meaning we have access to the transition probabilities \(P(s'\mid s,a)\) and the reward function \(R(s,a,s')\). DP algorithms are based on the principle of optimality and utilize Bellman equations to find optimal policies and value functions.</p><ul><li><p><strong>Policy Evaluation (Prediction)</strong></p><ul><li><strong>Policy Evaluation</strong>, also known as the prediction problem, aims to compute the state-value function \(V_\pi(s)\) for a given policy \(\pi\). It uses the Bellman Expectation Equation for \(V_\pi(s)\) iteratively.<ul><li><p><strong>Iterative Policy Evaluation Algorithm</strong>:</p><ol><li>Initialize \(V_0(s) = 0\) for all \(s \in \mathcal{S}\).</li><li>For each iteration \(k+1\):<ul><li>For each state \(s \in \mathcal{S}\):
\[
V_{k+1}(s)
= \sum_{a \in \mathcal{A}} \pi(a \mid s)
\sum_{s', r} P(s', r \mid s, a)\Bigl(r + \gamma V_k(s')\Bigr)
\]</li></ul></li><li>Repeat until convergence, i.e., until \(V_{k+1}(s) \approx V_k(s)\) for all \(s\).</li></ol></li><li><p><strong>Explanation</strong>: In each iteration, the value function for each state is updated based on the expected rewards and values of successor states, weighted by the policy and transition probabilities. This process is repeated until the value function converges, meaning the updates become very small.</p></li></ul></li></ul></li><li><p><strong>Policy Improvement</strong></p><ul><li><strong>Policy Improvement</strong> is the process of creating a better policy \(\pi'\) from a given policy \(\pi\). The idea is to act greedily with respect to the value function \(V_\pi\) (or action-value function \(Q_\pi\)) of the current policy.<ul><li><strong>Greedy Policy Improvement</strong>:
For each state \(s \in \mathcal{S}\), choose a new action \(a'\) that maximizes the action-value function \(Q_\pi(s,a)\):
\[
\pi'(s) = \arg\max_{a \in \mathcal{A}} Q_\pi(s,a)
\]
where
\[
Q_\pi(s,a)
= \mathbb{E}\bigl[R_{t+1} + \gamma V_\pi(S_{t+1}) \mid S_t=s, A_t=a\bigr]
= \sum_{s', r} P(s', r\mid s, a)\bigl(r + \gamma V_\pi(s')\bigr).
\]</li><li><strong>Policy Improvement Theorem</strong>: If we improve the policy greedily with respect to \(V_\pi\), the new policy \(\pi'\) is guaranteed to be no worse than \(\pi\), i.e., \(V_{\pi'}(s) \ge V_\pi(s)\) for all \(s \in \mathcal{S}\). If improvement is strict for any state, then \(\pi'\) is strictly better than \(\pi\).</li></ul></li></ul></li><li><p><strong>Policy Iteration</strong></p><ul><li><strong>Policy Iteration</strong> combines policy evaluation and policy improvement in an iterative process to find an optimal policy. It starts with an arbitrary policy, iteratively evaluates its value function, and then improves the policy based on this value function.<ul><li><strong>Policy Iteration Algorithm</strong>:<ol><li><strong>Initialization</strong>: Initialize a policy \(\pi_0\) (e.g., randomly).</li><li><strong>Policy Evaluation</strong>: Compute the state-value function \(V_{\pi_k}\) for the current policy \(\pi_k\) using iterative policy evaluation.</li><li><strong>Policy Improvement</strong>: Create a new policy \(\pi_{k+1}\) by acting greedily with respect to \(V_{\pi_k}\):
\[
\pi_{k+1}(s)
= \arg\max_{a \in \mathcal{A}} Q_{\pi_k}(s,a)
\]</li><li><strong>Iteration</strong>: Repeat steps 2 and 3 until policy improvement no longer yields a change in the policy, i.e., \(\pi_{k+1} = \pi_k\). At this point, the policy \(\pi_k\) is guaranteed to be an optimal policy \(\pi_*\), and \(V_{\pi_k} = V_*\).</li></ol></li><li><strong>Generalized Policy Iteration (GPI)</strong>: The general idea of iteratively performing policy evaluation and policy improvement, which underlies many RL algorithms, including policy iteration and value iteration.</li></ul></li></ul></li></ul><h3 id=monte-carlo-methods>Monte-Carlo Methods<a hidden class=anchor aria-hidden=true href=#monte-carlo-methods>#</a></h3><p><strong>Monte-Carlo (MC) methods</strong> are a class of model-free reinforcement learning algorithms. Unlike Dynamic Programming, MC methods do not require a complete model of the environment. Instead, they learn directly from <strong>episodes</strong> of experience. An episode is a complete sequence of states, actions, and rewards from a start state to a terminal state. MC methods are used for both prediction (estimating value functions) and control (finding optimal policies).</p><ul><li><strong>Value Estimation</strong><ul><li>Monte Carlo methods estimate value functions by averaging the <strong>returns</strong> observed in actual or simulated episodes. The <strong>return</strong> \(G_t\) is the total discounted reward from time step \(t\) onwards in an episode:
\[
G_t = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{T-t} R_T,
\]
where \(T\) is the terminal time step of the episode.<ul><li><p><strong>Monte Carlo Value Estimation Algorithm</strong>:</p><ol><li>Initialize \(V(s)\) arbitrarily for all \(s \in \mathcal{S}\).</li><li>Initialize \(N(s) = 0\) and \(SumOfReturns(s) = 0\) for all \(s \in \mathcal{S}\).</li><li>For each episode:<ul><li>Generate an episode following policy \(\pi\): \(S_1, A_1, R_2, S_2, A_2, \ldots, S_T\).</li><li>For each state \(S_t\) visited in the episode:
\[
N(S_t) \leftarrow N(S_t) + 1
\]
\[
SumOfReturns(S_t) \leftarrow SumOfReturns(S_t) + G_t
\]
\[
V(S_t) = \frac{SumOfReturns(S_t)}{N(S_t)}
\]</li></ul></li></ol><ul><li><strong>Explanation</strong>: For each episode, we calculate the return \(G_t\) for each state \(S_t\) visited in that episode. We then average these returns across all episodes to estimate \(V(s)\). We can use either <strong>first-visit MC</strong>, where we only consider the first visit to a state in an episode, or <strong>every-visit MC</strong>, where we consider every visit.</li></ul></li><li><p><strong>Action-Value Function Estimation</strong>: MC methods can be extended to estimate action-value functions \(Q_\pi(s,a)\) similarly, by averaging returns following each state-action pair \((s,a)\).</p></li></ul></li></ul></li></ul><h3 id=temporal-difference-td-learning>Temporal-Difference (TD) Learning<a hidden class=anchor aria-hidden=true href=#temporal-difference-td-learning>#</a></h3><p><strong>Temporal-Difference (TD) learning</strong> is a class of model-free reinforcement learning methods that learn directly from raw experience without a model of the environment, similar to Monte Carlo methods. However, TD learning has a key advantage: it can learn from <strong>incomplete episodes</strong> by bootstrapping, meaning it updates value function estimates based on other estimates, without waiting for the final outcome of an episode. TD learning is central to modern reinforcement learning algorithms.</p><ul><li><p><strong>Bootstrapping</strong></p><ul><li><strong>Bootstrapping</strong> is a core concept in TD learning. It means that TD methods update their estimates based in part on other estimates. In value function learning, TD methods update the value of a state based on the estimated value of the next state. This is in contrast to Monte Carlo methods, which wait until the end of an episode to calculate the actual return and use that as the target for updates. Bootstrapping allows TD learning to be more sample-efficient and to learn online, without needing to wait for the end of episodes.</li></ul></li><li><p><strong>Value Estimation</strong></p><ul><li>The simplest TD method for prediction is <strong>TD(0)</strong>, also known as one-step TD learning. TD(0) updates the value function \(V(S_t)\) towards a <strong>TD target</strong>, which is an estimate of the return based on the immediate reward \(R_{t+1}\) and the current estimate of the value of the next state \(V(S_{t+1})\).<ul><li><p><strong>TD(0) Update Rule for State-Value Function</strong>:</p>\[
V(S_t) \leftarrow V(S_t) + \alpha \Bigl(R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\Bigr),
\]<p>where \(\alpha\) is the learning rate, controlling the step size of the update. The term \(\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\) is called the <strong>TD error</strong>, representing the difference between the TD target and the current estimate.</p></li><li><p><strong>Explanation</strong>: The TD(0) update moves the value function \(V(S_t)\) in the direction of the TD target. If the TD error is positive, it means the current estimate \(V(S_t)\) is lower than the TD target, so we increase \(V(S_t)\). If the TD error is negative, we decrease \(V(S_t)\).</p></li></ul></li></ul></li><li><p><strong>SARSA: On-Policy TD Control</strong></p><ul><li><strong>SARSA (State-Action-Reward-State-Action)</strong> is an <strong>on-policy</strong> TD control algorithm for learning action-value functions \(Q_\pi(s,a)\). It learns the Q-function for the policy that is being used to explore the environment. &ldquo;On-policy&rdquo; means that SARSA learns about the policy it is currently following.<ul><li><strong>SARSA Algorithm</strong>:<ol><li>Initialize \(Q(s,a)\) arbitrarily for all \(s \in \mathcal{S}, a \in \mathcal{A}\).</li><li>For each episode:<ul><li>Initialize state \(S_t\).</li><li>Choose action \(A_t\) using a policy derived from \(Q\) (e.g., \(\epsilon\)-greedy).</li><li>Repeat for each step of episode:<ol><li>Take action \(A_t\), observe reward \(R_{t+1}\) and next state \(S_{t+1}\).</li><li>Choose next action \(A_{t+1}\) using policy derived from \(Q\) (e.g., \(\epsilon\)-greedy).</li><li>Update Q-value for state-action pair \((S_t, A_t)\):
[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Bigl(R_{t+1}<ul><li>\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\Bigr).
]</li></ul></li><li>\(S_t \leftarrow S_{t+1}\), \(A_t \leftarrow A_{t+1}\).</li></ol></li><li>Until \(S_{t+1}\) is terminal.</li></ul></li></ol></li><li><strong>Explanation</strong>: SARSA updates the Q-value for the state-action pair \((S_t, A_t)\) based on the reward \(R_{t+1}\) and the Q-value of the next state-action pair \((S_{t+1}, A_{t+1})\). The action \(A_{t+1}\) is chosen using the same policy that is being evaluated and improved, hence &ldquo;on-policy&rdquo;.</li></ul></li></ul></li><li><p><strong>Q-Learning: Off-Policy TD Control</strong></p><ul><li><strong>Q-Learning</strong> is an <strong>off-policy</strong> TD control algorithm. It learns an estimate of the optimal action-value function \(Q_*(s,a)\), independent of the policy being followed. &ldquo;Off-policy&rdquo; means that Q-learning can learn about an optimal policy even while following a different, possibly exploratory, policy.<ul><li><strong>Q-Learning Algorithm</strong>:<ol><li>Initialize \(Q(s,a)\) arbitrarily for all \(s \in \mathcal{S}, a \in \mathcal{A}\).</li><li>For each episode:<ul><li>Initialize state \(S_t\).</li><li>Repeat for each step of episode:<ol><li>Choose action \(A_t\) using a policy derived from \(Q\) (e.g., \(\epsilon\)-greedy).</li><li>Take action \(A_t\), observe reward \(R_{t+1}\) and next state \(S_{t+1}\).</li><li>Update Q-value for state-action pair \((S_t, A_t)\):
[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Bigl(R_{t+1}<ul><li>\gamma \max_{a&rsquo;} Q(S_{t+1}, a&rsquo;) - Q(S_t, A_t)\Bigr).
]</li></ul></li><li>\(S_t \leftarrow S_{t+1}\).</li></ol></li><li>Until \(S_{t+1}\) is terminal.</li></ul></li></ol></li><li><strong>Explanation</strong>: Q-learning updates the Q-value for \((S_t, A_t)\) based on the reward \(R_{t+1}\) and the <strong>maximum</strong> Q-value achievable in the next state \(S_{t+1}\), regardless of which action is actually taken in \(S_{t+1}\). This &ldquo;max&rdquo; operation makes Q-learning off-policy because it learns about the optimal policy while potentially following a different behavior policy for exploration.</li></ul></li></ul></li><li><p><strong>Deep Q-Networks (DQN)</strong></p><ul><li><strong>Deep Q-Networks (DQN)</strong> is a groundbreaking algorithm that combines Q-learning with deep neural networks to handle high-dimensional state spaces, such as images. DQN addresses the instability and convergence issues that arise when using nonlinear function approximation (like neural networks) with bootstrapping and off-policy learning. DQN introduces two key techniques to stabilize Q-learning:<ul><li><strong>Experience Replay</strong>: DQN stores transitions \(\bigl(s_t, a_t, r_t, s_{t+1}\bigr)\) in a <strong>replay memory</strong> (buffer) \(D\). Instead of updating Q-values online from sequential experiences, DQN samples mini-batches of transitions randomly from \(D\) to perform updates. This breaks the correlation between consecutive samples and smooths the data distribution over updates, improving stability.</li><li><strong>Periodically Updated Target Network</strong>: DQN uses two Q-networks: a <strong>Q-network</strong> \(Q(s,a; \theta)\) with parameters \(\theta\) that are being trained, and a <strong>target Q-network</strong> \(Q(s,a; \theta^-)\) with parameters \(\theta^-\) that are periodically updated to be the same as \(\theta\) (e.g., every \(C\) steps) and kept frozen in between. The target network is used to compute the TD target in the Q-learning update, which stabilizes learning by reducing oscillations and divergence.</li><li><strong>DQN Loss Function</strong>: The loss function for DQN is the mean squared error between the TD target and the current Q-value:
\[
\mathcal{L}(\theta)
= \mathbb{E}_{(s,a,r,s') \sim U(D)}\Bigl[\bigl(y - Q(s,a;\theta)\bigr)^2\Bigr],
\]
where
\[
y = r + \gamma \max_{a'} Q(s',a'; \theta^-)
\]
is the TD target, and \(\bigl(s,a,r,s'\bigr)\) is a transition sampled from the replay memory \(D\). \(U(D)\) denotes uniform sampling from \(D\).</li><li><strong>DQN Algorithm Outline</strong>:<ol><li>Initialize replay memory \(D\) to capacity \(N\).</li><li>Initialize Q-network \(Q\) with random weights \(\theta\).</li><li>Initialize target Q-network \(\hat{Q}\) with weights \(\theta^- = \theta\).</li><li>For each episode:<ul><li>Initialize state \(s_1\).</li><li>For \(t = 1, \ldots, T\):<ol><li>Select action \(a_t\) using \(\epsilon\)-greedy policy based on \(Q(s_t,\cdot;\theta)\).</li><li>Execute action \(a_t\), observe reward \(r_t\) and next state \(s_{t+1}\).</li><li>Store transition \(\bigl(s_t,a_t,r_t,s_{t+1}\bigr)\) in \(D\).</li><li>Sample random mini-batch of transitions from \(D\).</li><li>Compute TD targets \(y_j = r_j + \gamma \max_{a'} \hat{Q}(s_j',a';\theta^-)\)
(or \(y_j = r_j\) if episode terminates).</li><li>Perform gradient descent step to minimize
\[
\mathcal{L}(\theta)
= \mathbb{E}_j\Bigl[\bigl(y_j - Q(s_j,a_j;\theta)\bigr)^2\Bigr]
\]
with respect to \(\theta\).</li><li>Every \(C\) steps, reset \(\theta^- = \theta\).</li></ol></li></ul></li></ol></li></ul></li></ul></li></ul><h3 id=combining-td-and-monte-carlo-learning>Combining TD and Monte-Carlo Learning<a hidden class=anchor aria-hidden=true href=#combining-td-and-monte-carlo-learning>#</a></h3><ul><li><p><strong>n-step TD Learning</strong></p><ul><li>n-step TD learning methods bridge the gap between one-step TD learning (like TD(0)) and Monte Carlo methods by looking ahead \(n\) steps to estimate the return. The <strong>n-step return</strong> \(G_t^{(n)}\) is defined as:
\[
G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n}).
\]
For \(n=1\), \(G_t^{(1)} = R_{t+1} + \gamma V(S_{t+1})\), which is the TD target. As \(n \to \infty\), \(G_t^{(n)} \to G_t\), the Monte Carlo return.</li><li><strong>n-step TD Update Rule</strong>:
\[
V(S_t) \leftarrow V(S_t) + \alpha \bigl(G_t^{(n)} - V(S_t)\bigr).
\]</li><li><strong>Benefits of n-step TD</strong>:<ul><li>Balances bias and variance: n-step returns have lower variance than MC returns and lower bias than 1-step TD targets.</li><li>Can learn faster than MC methods and be more stable than 1-step TD methods in some environments.</li></ul></li></ul></li><li><p><strong>TD(\(\lambda\))</strong></p><ul><li><strong>TD(\(\lambda\))</strong> methods generalize n-step TD learning by averaging n-step returns over all possible values of \(n\), weighted by a factor \(\lambda^{n-1}\). The <strong>\(\lambda\)-return</strong> \(G_t^{(\lambda)}\) is defined as:
\[
G_t^{(\lambda)}
= (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)},
\]
where \(\lambda \in [0,1]\) is the <strong>trace-decay parameter</strong>.<ul><li>When \(\lambda = 0\), \(G_t^{(\lambda)} = G_t^{(1)}\), which reduces to TD(0).</li><li>When \(\lambda = 1\), \(G_t^{(\lambda)} = G_t^{(\infty)} = G_t\), which becomes the Monte Carlo return.</li></ul></li><li><strong>TD(\(\lambda\)) Update Rule</strong>:
\[
V(S_t) \leftarrow V(S_t) + \alpha \bigl(G_t^{(\lambda)} - V(S_t)\bigr).
\]</li><li><strong>Eligibility Traces</strong>: TD(\(\lambda\)) can be implemented efficiently using <strong>eligibility traces</strong>, which provide a mechanism to assign credit to past states and actions for observed rewards. Eligibility traces maintain a short-term memory of visited states or state-action pairs, allowing updates to propagate back through time more efficiently than with n-step returns alone.</li></ul></li></ul><h3 id=policy-gradient-methods>Policy Gradient Methods<a hidden class=anchor aria-hidden=true href=#policy-gradient-methods>#</a></h3><p><strong>Policy Gradient methods</strong> are a class of model-free reinforcement learning algorithms that directly learn and optimize the policy \(\pi_\theta(a\mid s)\) without explicitly learning a value function (though value functions are often used to assist policy gradient methods, as in Actor-Critic methods). Policy gradient methods directly search for an optimal policy by optimizing the policy parameters \(\theta\) using gradient ascent on the expected return \(J(\theta)\).</p><ul><li><strong>Policy Gradient Theorem</strong><ul><li><p>The <strong>Policy Gradient Theorem</strong> provides an analytical expression for the gradient of the performance objective \(J(\theta)\) with respect to the policy parameters \(\theta\). This theorem is crucial because it allows us to compute the gradient and perform gradient ascent to improve the policy. The policy gradient theorem states:</p>\[
\nabla_{\theta} J(\theta)
= \mathbb{E}_{\pi_\theta}\Bigl[\nabla_{\theta} \ln \pi(a\mid s,\theta)\, Q_\pi(s,a)\Bigr].
\]<p>This equation is fundamental to policy gradient methods. It shows that the gradient of the performance objective can be estimated by averaging over trajectories sampled from the policy, where for each time step in a trajectory, we compute the product of the action-value function \(Q_\pi(s,a)\) and the gradient of the log-policy \(\nabla_{\theta} \ln \pi(a\mid s,\theta)\).</p></li><li><p><strong>Proof of Policy Gradient Theorem</strong>:
(Detailed derivation already covered in the &ldquo;Policy Gradient Theorem&rdquo; section above.)</p></li></ul></li></ul><h3 id=advanced-policy-gradient-algorithms>Advanced Policy Gradient Algorithms<a hidden class=anchor aria-hidden=true href=#advanced-policy-gradient-algorithms>#</a></h3><ul><li><p><strong>REINFORCE</strong></p><ul><li><strong>REINFORCE</strong>, also known as Monte Carlo Policy Gradient, is a basic policy gradient algorithm that directly implements the policy gradient theorem. It uses Monte Carlo estimates of the action-value function \(Q_\pi(s,a)\) to update the policy parameters. In REINFORCE, the return \(G_t\) from an entire episode is used as an unbiased estimate of \(Q_\pi(S_t,A_t)\).<ul><li><strong>REINFORCE Algorithm</strong>:<ol><li>Initialize policy parameters \(\theta\) randomly.</li><li>For each episode:<ul><li>Generate an episode trajectory \(\tau = (S_1, A_1, R_2, S_2, A_2, \ldots, S_T)\) following policy \(\pi_\theta\).</li><li>For each time step \(t = 1, 2, \ldots, T\):<ol><li>Calculate the return
\[
G_t = \sum_{k=0}^{T-t} \gamma^k R_{t+k+1}.
\]</li><li>Update policy parameters using gradient ascent:
\[
\theta \leftarrow \theta + \alpha\, G_t\, \nabla_{\theta} \ln \pi\bigl(A_t \mid S_t,\theta\bigr).
\]</li></ol></li></ul></li></ol></li><li><strong>Explanation</strong>: REINFORCE updates the policy parameters in the direction that increases the probability of actions that led to higher returns in the episode. The gradient update is proportional to \(G_t\) and \(\nabla_{\theta} \ln \pi(A_t\mid S_t,\theta)\).</li></ul></li></ul></li><li><p><strong>Actor-Critic Variants</strong></p><ul><li><strong>A2C (Advantage Actor-Critic)</strong>: Advantage Actor-Critic (A2C) is the synchronous, deterministic counterpart to Asynchronous Advantage Actor-Critic (A3C). A2C runs multiple agents in parallel environments and waits for all agents to complete their steps before performing a synchronized update of the global network. This synchronous update often leads to more stable and efficient learning, especially when using GPUs for training. A2C typically uses the advantage function \(A_\pi(s,a) = Q_\pi(s,a) - V_\pi(s)\) to reduce variance and improve the gradient signal.</li><li><strong>A3C (Asynchronous Advantage Actor-Critic)</strong>: Asynchronous Advantage Actor-Critic (A3C) is a parallel, asynchronous framework where multiple independent agents (actors) run in parallel environments to collect experience and update a shared global network asynchronously. A3C is known for its efficiency in utilizing multi-core CPUs. It typically uses the advantage function \(A_\pi(s,a) = Q_\pi(s,a) - V_\pi(s)\) to reduce variance in policy gradient updates.</li></ul></li><li><p><strong>Off-Policy Policy Gradient</strong></p><ul><li><strong>Off-Policy Policy Gradient methods</strong> learn a policy using data generated by a different policy, called the <strong>behavior policy</strong>, rather than the policy being optimized, called the <strong>target policy</strong>. This is in contrast to <strong>on-policy</strong> methods, where the behavior policy and target policy are the same. Off-policy methods offer several advantages:<ul><li><strong>Sample Efficiency</strong>: They can reuse past experiences stored in a replay buffer, making learning more data-efficient.</li><li><strong>Exploration</strong>: They can use a behavior policy that is more exploratory than the target policy, facilitating better exploration of the environment.</li><li><strong>Flexibility</strong>: They allow learning from diverse datasets, including data collected from human experts or other agents.</li></ul></li><li><strong>Importance Sampling</strong>: A key technique in off-policy policy gradient methods is <strong>importance sampling</strong>. It is used to correct for the mismatch between the distribution of data generated by the behavior policy and the distribution under the target policy. For policy gradient, importance sampling weights are used to adjust the gradient estimates. For example, in off-policy policy gradient updates, the gradient is weighted by the ratio \(\frac{\pi_\theta(a\mid s)}{\beta(a\mid s)}\), where \(\beta\) is the behavior policy.</li></ul></li><li><p><strong>Advantage Actor-Critic (A2C)</strong></p><ul><li>Advantage Actor-Critic (A2C) is the synchronous counterpart to A3C, running multiple agents in parallel environments and performing synchronized updates. It uses the advantage function to reduce variance in the policy gradient estimates, leading to more stable and efficient learning.</li></ul></li><li><p><strong>Deterministic Policy Gradient (DPG)</strong></p><ul><li><strong>Deterministic Policy Gradient (DPG)</strong> is a policy gradient algorithm designed for continuous action spaces. Unlike stochastic policy gradient methods that learn a probability distribution over actions, DPG learns a <strong>deterministic policy</strong> \(\mu_\theta(s)\) that directly outputs a specific action for each state. DPG is based on the Deterministic Policy Gradient Theorem, which provides a way to compute the gradient of the performance objective for deterministic policies.<ul><li><strong>Deterministic Policy Gradient Theorem</strong>:
\[
\nabla_\theta J(\theta)
= \mathbb{E}_{s \sim \rho^\mu}\bigl[\nabla_a Q_\mu(s,a)\,\nabla_\theta \mu_\theta(s)\bigr]
\Bigm|_{a = \mu_\theta(s)},
\]
where \(\rho^\mu(s)\) is the state distribution under the deterministic policy \(\mu_\theta(s)\), and \(Q_\mu(s,a)\) is the action-value function for policy \(\mu_\theta(s)\).</li></ul></li></ul></li><li><p><strong>Deep Deterministic Policy Gradient (DDPG)</strong></p><ul><li><strong>Deep Deterministic Policy Gradient (DDPG)</strong> is an off-policy actor-critic algorithm that combines DPG with techniques from DQN to enable deep reinforcement learning in continuous action spaces. DDPG is essentially a deep learning version of DPG, incorporating experience replay and target networks to stabilize training.<ul><li><strong>Key Features of DDPG</strong>:<ul><li><strong>Actor-Critic Architecture</strong>: Uses two neural networks: an actor network \(\mu_\theta(s)\) and a critic network \(Q_w(s,a)\).</li><li><strong>Experience Replay</strong>: Stores transitions in a replay buffer and samples mini-batches for updates, similar to DQN.</li><li><strong>Target Networks</strong>: Uses target networks for both actor \(\mu_{\theta^-}(s)\) and critic \(Q(s,a; w^-)\), updated slowly by soft updates.</li><li><strong>Deterministic Policy</strong>: Learns a deterministic policy, well-suited for continuous action spaces.</li><li><strong>Exploration</strong>: Uses Ornstein-Uhlenbeck process or Gaussian noise added to actions for exploration.</li></ul></li></ul></li></ul></li><li><p><strong>Distributed Distributional DDPG (D4PG)</strong></p><ul><li><strong>Distributed Distributional DDPG (D4PG)</strong> is an extension of DDPG that incorporates:<ul><li><strong>Distributional Critic</strong>: Learns a distribution over returns instead of a single Q-value.</li><li><strong>N-step Returns</strong>: Uses n-step TD targets to reduce variance.</li><li><strong>Multiple Distributed Parallel Actors</strong>: Collects experience in parallel, improving data throughput.</li><li><strong>Prioritized Experience Replay (PER)</strong>: Samples transitions with probabilities proportional to their TD errors.</li></ul></li></ul></li><li><p><strong>Multi-Agent DDPG (MADDPG)</strong></p><ul><li><strong>Multi-Agent DDPG (MADDPG)</strong> extends DDPG to multi-agent environments, addressing non-stationarity where multiple agents learn simultaneously.<ul><li><strong>Centralized Critic, Decentralized Actors</strong>: Each agent’s critic has access to global information, while each actor only sees its local observations.</li><li><strong>Learning with Other Agents’ Policies</strong>: Helps handle changing dynamics as other agents learn.</li></ul></li></ul></li><li><p><strong>Trust Region Policy Optimization (TRPO)</strong></p><ul><li><strong>Trust Region Policy Optimization (TRPO)</strong> is an on-policy policy gradient algorithm that ensures monotonic policy improvement by constraining the size of each policy update, measured by KL divergence.<ul><li><strong>TRPO Objective</strong>:
\[
\max_{\theta}\,\mathbb{E}_{s \sim \rho^{\pi_{\theta_\text{old}}},\,a \sim \pi_{\theta_\text{old}}}
\Bigl[\frac{\pi_\theta(a\mid s)}{\pi_{\theta_\text{old}}(a\mid s)}\,\hat{A}_{\theta_\text{old}}(s,a)\Bigr]
\]
subject to a KL-divergence constraint \(\le \delta\).</li><li><strong>Explanation</strong>: TRPO keeps the new policy \(\pi_\theta\) within a &ldquo;trust region&rdquo; of the old policy \(\pi_{\theta_\text{old}}\), preventing large, destabilizing updates.</li></ul></li></ul></li><li><p><strong>Proximal Policy Optimization (PPO)</strong></p><ul><li><strong>Proximal Policy Optimization (PPO)</strong> simplifies TRPO and is more practical to implement while maintaining similar performance. PPO uses a clipped surrogate objective to bound the policy update.<ul><li><strong>PPO Clipped Objective</strong>:
\[
J^{\mathrm{CLIP}}(\theta)
= \mathbb{E}\Bigl[\min\bigl(r(\theta)\,\hat{A}_{\theta_\text{old}}(s,a),\,\mathrm{clip}\bigl(r(\theta),1-\epsilon,1+\epsilon\bigr)\,\hat{A}_{\theta_\text{old}}(s,a)\bigr)\Bigr],
\]
where \(r(\theta) = \frac{\pi_\theta(a\mid s)}{\pi_{\theta_\text{old}}(a\mid s)}\).</li><li><strong>Explanation</strong>: By clipping the ratio \(r(\theta)\), PPO avoids excessively large updates that could harm performance.</li></ul></li></ul></li><li><p><strong>Phasic Policy Gradient (PPG)</strong></p><ul><li><strong>Phasic Policy Gradient (PPG)</strong> is an on-policy algorithm that separates policy and value function updates into distinct phases, improving sample efficiency and stability.<ul><li><strong>Policy Phase</strong>: Optimizes a PPO-like objective.</li><li><strong>Auxiliary Phase</strong>: Improves the value function and keeps policy from drifting too far.</li><li><strong>Benefits</strong>: More stable updates and better sample reuse compared to PPO.</li></ul></li></ul></li><li><p><strong>Actor-Critic with Experience Replay (ACER)</strong></p><ul><li><strong>Actor-Critic with Experience Replay (ACER)</strong> is an off-policy actor-critic algorithm combining experience replay, Retrace for stable off-policy Q-value estimation, truncated importance weights with bias correction, and an efficient TRPO-like update.<ul><li><strong>Key Ideas</strong>:<ul><li>Off-policy correction via truncated importance sampling.</li><li>Retrace for unbiased and low-variance Q-value estimation.</li><li>Trust region updates for stable learning.</li></ul></li></ul></li></ul></li><li><p><strong>Actor-Critic using Kronecker-Factored Trust Region (ACKTR)</strong></p><ul><li><strong>Actor-Critic using Kronecker-Factored Trust Region (ACKTR)</strong> leverages Kronecker-Factored Approximate Curvature (K-FAC) for more efficient and stable updates.<ul><li><strong>K-FAC</strong>: Approximates the Fisher information matrix via Kronecker products for natural gradient updates.</li><li><strong>Natural Gradient</strong>: Helps optimize in directions that consider curvature, often improving convergence speed and stability.</li></ul></li></ul></li><li><p><strong>Soft Actor-Critic (SAC)</strong></p><ul><li><strong>Soft Actor-Critic (SAC)</strong> is an off-policy actor-critic algorithm featuring <strong>maximum entropy</strong> RL. It maximizes both return and policy entropy, encouraging exploration and robustness.<ul><li><strong>Maximum Entropy Objective</strong>:
\[
J(\pi) = \sum_{t=1}^{T} \mathbb{E}\bigl[r(s_t,a_t) + \alpha\,\mathcal{H}\bigl(\pi_\theta(\cdot\mid s_t)\bigr)\bigr],
\]
where \(\alpha\) is a temperature parameter.</li><li><strong>Soft Q-function and Soft Value Function</strong>: Incorporates entropy terms into the Bellman backup.</li><li><strong>Automatic Temperature Adjustment</strong>: \(\alpha\) can be learned to balance exploration and exploitation.</li></ul></li></ul></li><li><p><strong>Twin Delayed DDPG (TD3)</strong></p><ul><li><strong>Twin Delayed DDPG (TD3)</strong> is an improvement over DDPG that addresses overestimation bias and improves stability.<ul><li><strong>Clipped Double Q-learning</strong>: Uses two critics and takes the minimum of both Q-values to reduce overestimation.</li><li><strong>Delayed Policy Updates</strong>: Updates the policy less frequently than the critics.</li><li><strong>Target Policy Smoothing</strong>: Adds noise to target actions for smoother Q-value estimates.</li></ul></li></ul></li><li><p><strong>Stein Variational Policy Gradient (SVPG)</strong></p><ul><li><strong>Stein Variational Policy Gradient (SVPG)</strong> uses Stein Variational Gradient Descent (SVGD) to maintain an ensemble of policy &ldquo;particles,&rdquo; encouraging diversity.<ul><li><strong>SVGD</strong>: Iteratively updates a set of particles to approximate a target distribution.</li><li><strong>Kernel Function</strong>: Encourages particles to spread out, improving exploration.</li></ul></li></ul></li><li><p><strong>Importance Weighted Actor-Learner Architectures (IMPALA)</strong></p><ul><li><strong>Importance Weighted Actor-Learner Architectures (IMPALA)</strong> is a highly scalable RL framework with decoupled actors and learners, using <strong>V-trace</strong> to handle off-policy corrections.<ul><li><strong>Decoupled Architecture</strong>: Multiple actors generate data in parallel, sending trajectories to a central learner.</li><li><strong>V-trace</strong>: Corrects off-policy data from actors running slightly outdated policies.</li><li><strong>High Throughput</strong>: Achieves efficient large-scale training in complex environments.</li></ul></li></ul></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-02-01-normalization/><span class=title>« Prev</span><br><span>Normalization in Deep Learning</span>
</a><a class=next href=https://syhya.github.io/posts/2025-01-27-deepseek-r1/><span class=title>Next »</span><br><span>OpenAI o1 Replication Progress: DeepSeek-R1</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Deep Reinforcement Learning (Ongoing Updates) on x" href="https://x.com/intent/tweet/?text=Deep%20Reinforcement%20Learning%20%28Ongoing%20Updates%29&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2frl-introduction%2f&amp;hashtags=DeepLearning%2cAI%2cReinforcementLearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Deep Reinforcement Learning (Ongoing Updates) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2frl-introduction%2f&amp;title=Deep%20Reinforcement%20Learning%20%28Ongoing%20Updates%29&amp;summary=Deep%20Reinforcement%20Learning%20%28Ongoing%20Updates%29&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2frl-introduction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Deep Reinforcement Learning (Ongoing Updates) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2frl-introduction%2f&title=Deep%20Reinforcement%20Learning%20%28Ongoing%20Updates%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Deep Reinforcement Learning (Ongoing Updates) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2frl-introduction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Deep Reinforcement Learning (Ongoing Updates) on whatsapp" href="https://api.whatsapp.com/send?text=Deep%20Reinforcement%20Learning%20%28Ongoing%20Updates%29%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2frl-introduction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Deep Reinforcement Learning (Ongoing Updates) on telegram" href="https://telegram.me/share/url?text=Deep%20Reinforcement%20Learning%20%28Ongoing%20Updates%29&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2frl-introduction%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Deep Reinforcement Learning (Ongoing Updates) on ycombinator" href="https://news.ycombinator.com/submitlink?t=Deep%20Reinforcement%20Learning%20%28Ongoing%20Updates%29&u=https%3a%2f%2fsyhya.github.io%2fposts%2frl-introduction%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>