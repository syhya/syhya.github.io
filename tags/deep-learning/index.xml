<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Deep Learning on Yue Shui Blog</title><link>https://syhya.github.io/tags/deep-learning/</link><description>Recent content in Deep Learning on Yue Shui Blog</description><generator>Hugo -- 0.140.1</generator><language>en-us</language><lastBuildDate>Sat, 21 Dec 2024 12:00:00 +0800</lastBuildDate><atom:link href="https://syhya.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Building a Home Deep Learning Rig with Dual RTX 4090 GPUs</title><link>https://syhya.github.io/posts/2024-12-21-build-gpu-server/</link><pubDate>Sat, 21 Dec 2024 12:00:00 +0800</pubDate><guid>https://syhya.github.io/posts/2024-12-21-build-gpu-server/</guid><description>&lt;h2 id="rent-a-gpu-or-buy-your-own">Rent a GPU or Buy Your Own?&lt;/h2>
&lt;p>Before setting up a deep learning environment, consider &lt;strong>usage duration&lt;/strong>, &lt;strong>budget&lt;/strong>, &lt;strong>data privacy&lt;/strong>, and &lt;strong>maintenance overhead&lt;/strong>. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.&lt;/p></description></item></channel></rss>