---
title: "RAG"
date: 2025-03-03T12:00:00+08:00
lastmod: 2025-03-03T12:00:00+08:00
author: Yue Shui
categories: ["技术博客"]
tags: [深度学习, 向量召回]
readingTime: 60
toc: true
ShowToc: true
TocOpen: false
draft: false
type: "posts"
math: true
---





# **检索增强生成（RAG）技术发展进展**

## A. RAG 概述

**RAG 定义与动机：**检索增强生成（Retrieval-Augmented Generation, RAG）是一种将大语言模型（LLM）的生成能力与外部检索系统相结合的技术。传统LLM在参数中存储了大量知识，但无法轻易更新知识且易产生事实幻觉。RAG 通过在生成答案时动态检索相关资料，为 LLM 提供**非参数化记忆**，使其输出基于最新的真实信息 ([[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401#:~:text=far%20been%20only%20investigated%20for,intensive%20NLP))。换言之，RAG 模型由预训练的生成模型（**参数记忆**）和一个可查询的知识库（**非参数记忆**）组成，查询知识库以获取事实依据来增强回答。这种架构赋予 LLM 实时获取外部知识的能力，使其能够轻松更新知识、引用资料来源并降低虚构错误 ([RAG 101: Demystifying Retrieval-Augmented Generation Pipelines | NVIDIA Technical Blog](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/#:~:text=Empowering%20LLM%20solutions%20with%20real,data%20access)) ([RAG 101: Demystifying Retrieval-Augmented Generation Pipelines | NVIDIA Technical Blog](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/#:~:text=Mitigating%20LLM%20hallucinations))。

**核心组件与工作流程：**典型的 RAG 管道包含离线和在线两个阶段 ([RAG 101: Demystifying Retrieval-Augmented Generation Pipelines | NVIDIA Technical Blog](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/#:~:text=Building%20and%20deploying%20your%20first,RAG%20pipeline))。离线阶段包括**数据预处理**和**索引建立**：(1) **数据预处理（Chunking）**：将长文档拆分为适当大小的片段，并进行清洗、标注等；(2) **Embedding**：使用预训练模型将每个文档片段编码为向量表示；(3) **向量存储**：将文档向量加载到向量数据库（如 FAISS、Milvus 等）中以支持相似度检索。在线阶段执行**检索与生成**：(4) **检索**：对用户查询进行编码，搜索向量数据库获取最高相似度的若干文档片段（也可结合BM25等关键词搜索）；(5) **生成**：将检索到的内容与原始查询一起提供给LLM，生成最终回答。整个流程如图所示 ([RAG 101: Demystifying Retrieval-Augmented Generation Pipelines | NVIDIA Technical Blog](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/))。通过这种“**先检索、后生成**”的闭环，RAG 确保LLM有据可依地回答问题，显著减少了模型幻觉，并支持随时更新知识库而无需重新训练模型 ([RAG 101: Demystifying Retrieval-Augmented Generation Pipelines | NVIDIA Technical Blog](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/#:~:text=Data%20is%20constantly%20changing%20in,time%20and%20personalized%20data)) ([RAG 101: Demystifying Retrieval-Augmented Generation Pipelines | NVIDIA Technical Blog](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/#:~:text=Mitigating%20LLM%20hallucinations))。

 ([RAG 101: Demystifying Retrieval-Augmented Generation Pipelines | NVIDIA Technical Blog](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/))*RAG 工作流程示意图：离线将企业知识文档预处理并向量化存储到向量数据库，在线针对用户查询检索相关文档并与LLM交互生成答案。*

**示例代码：**下例展示了使用向量检索和LLM来实现问答的简化 RAG 流程：

```python
# 利用已有向量数据库 retriever 查找相关文档
retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})
# 构建LLM问答链，结合检索器
qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)
# 针对用户问题运行 RAG 问答
query = "What are the core components of a RAG pipeline?"
result = qa_chain.run(query)
print(result)  # 输出包含基于知识库的答案
``` 

上述代码使用检索器获取与查询相关的文档片段，并将其交给LLM生成回答，从而实现“检索增强”的问答功能。

## B. RAG 与模型微调的对比

**“检索” vs “学习”思路：**RAG 技术代表了一种“搜索而非记忆”的范式。传统做法是通过**模型微调**将新知识融合进模型参数（学习），而 RAG 则在推理时**即时检索**所需知识（搜索）。这两者在知识更新、成本和实时性上各有特点：

- **知识更新速度：**微调模型需要收集新数据并重新训练，往往耗时长且更新周期慢。而 RAG 只需向知识库中添加或修改文档即可使模型掌握新知识，更新几乎是即时的 ([RAG 101: Demystifying Retrieval-Augmented Generation Pipelines | NVIDIA Technical Blog](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/#:~:text=Empowering%20LLM%20solutions%20with%20real,data%20access))。例如，当有新法规发布时，使用 RAG 的系统可以立即搜索到新法规文本并回答相关问题；而纯微调模型需要重新训练才能掌握该法规内容。

- **计算和存储成本：**微调大型模型不仅需要强算力支持，还会增加模型参数量，占用更多内存。相比之下，RAG 将知识存储在外部向量库中，LLM 无需“记住”所有知识，仅在需要时检索，因而基模型可保持较小规模。这降低了部署成本，并且知识库可以针对不同领域复用，不必为每个领域训练独立模型。

- **实时性与时效性：**微调后的模型在部署后知识是静态的，无法反映训练后出现的新信息。而 RAG 拥有连接实时数据库或搜索引擎的能力，能处理事实不断变化的应用场景（如新闻问答、财报分析），显著增强了系统的时效性 ([RAG 101: Demystifying Retrieval-Augmented Generation Pipelines | NVIDIA Technical Blog](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/#:~:text=Empowering%20LLM%20solutions%20with%20real,data%20access))。

Richard Sutton 在 2019 年提出的“苦涩的教训”中指出：**利用大规模计算进行搜索和学习，比起人工将知识硬编码进模型，更能带来长期进步** ([‘The Bitter Lesson’ of AI is not too bitter – AI in Media and Society](https://www.macloo.com/ai/2024/03/11/the-bitter-lesson-of-ai-is-not-too-bitter/#:~:text=tl%3Bdr%3A%20%E2%80%9CLeveraging%20human%20knowledge%E2%80%9D%20has,This%20is%20the%20bitter%20lesson)) ([‘The Bitter Lesson’ of AI is not too bitter – AI in Media and Society](https://www.macloo.com/ai/2024/03/11/the-bitter-lesson-of-ai-is-not-too-bitter/#:~:text=,%E2%80%94Richard%20S.%20Sutton))。这一观点契合了 RAG 的优势——通过检索（search）扩展知识，而非试图让模型记住所有知识。RAG 利用通用的检索+生成框架，就像互联网搜索引擎+文本生成器一样，可以灵活应对新知识需求。而纯粹依赖模型微调的方式，好比每次都训练一个新模型来回答问题，不仅低效且难以跟上知识爆炸的节奏。 ([‘The Bitter Lesson’ of AI is not too bitter – AI in Media and Society](https://www.macloo.com/ai/2024/03/11/the-bitter-lesson-of-ai-is-not-too-bitter/#:~:text=,%E2%80%94Richard%20S.%20Sutton))

当然，RAG 与微调各有适用场景：微调适合在特定任务上达到极致性能，且不需要频繁更新知识的情况；RAG 则在知识更新频繁、需要覆盖广泛开放领域知识时表现出独特优势。二者也可结合使用，在基础模型微调一定领域知识的同时，通过 RAG 补充最新的信息，实现精准度与时效性的兼顾。

## C. GraphRAG：引入知识图谱的检索增强生成

**基本概念：**GraphRAG 是对传统 RAG 的拓展，核心思想是将**知识图谱（Knowledge Graph）**融入 RAG 流程，以提升检索的质量和多跳推理能力。知识图谱由**实体（节点）**及其**关系（边）**构成，天然适合表示异构且关联密集的知识 ([[2501.00309] Retrieval-Augmented Generation with Graphs (GraphRAG)](https://arxiv.org/abs/2501.00309#:~:text=enhances%20downstream%20task%20execution%20by,specific%20relational%20knowledge%2C%20poses))。相比将知识拆成孤立文本片段的向量检索，GraphRAG 通过图谱显式建模实体之间的关联语义，让检索更“懂”数据的内在结构 ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=Enter%20GraphRAG%3A%20a%20paradigm%20that,for%20enterprise%20and%20research%20applications)) ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=GraphRAG%20combines%20the%20strengths%20of,edge%20technologies))。简而言之，GraphRAG = **RAG + 知识图谱**：LLM在生成答案时，不仅参考检索到的文本片段，还利用图谱提供的深层语义关系，为回答提供更丰富的上下文。

**工作流程：**GraphRAG 包含**构建知识图谱**和**图谱增强检索**两个阶段 ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=,Knowledge%20Graph)) ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=1.%20Traditional%20Retrieval%3A%20,insights%20derived%20from%20the%20graph))：

- **图谱构建（索引阶段）：**首先，从原始语料中抽取实体和关系，构建知识图谱。具体步骤包括：①**实体与关系抽取**：利用 NLP 技术从文本片段中识别重要实体（如人名、地名、专业术语等）以及它们之间的关系 ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=,Knowledge%20Graph)) ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=))。例如，在文本“Elon Musk 创立了 SpaceX”，GraphRAG 会提取实体“Elon Musk”和“SpaceX”，关系为“创立（founded）”。②**知识图谱创建**：将抽取的实体作为节点、关系作为边加入图谱中。如果有结构化数据（数据库、表格）也可直接导入节点/边 ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=The%20graph%20is%20populated%20using%3A))。对于非结构化文本，通过以上过程得到初始图谱。③**图谱优化**：可选地，对图谱执行社群划分、层次聚类等操作，将密切相关的实体聚为社区，并生成每个社区的摘要描述 ([What is GraphRAG? Enhancing RAG with Knowledge Graphs - Zilliz blog](https://zilliz.com/blog/graphrag-explained-enhance-rag-with-knowledge-graphs#:~:text=graph)) ([What is GraphRAG? Enhancing RAG with Knowledge Graphs - Zilliz blog](https://zilliz.com/blog/graphrag-explained-enhance-rag-with-knowledge-graphs#:~:text=4,contextual%20information%20for%20subsequent%20queries))。这一步骤将图谱组织成层次结构（如全局主题–子主题），为不同粒度的检索做准备。

- **检索与生成（查询阶段）：**GraphRAG 查询阶段一般提供**全局检索**和**局部检索**两种模式，用于处理不同类型的问题 ([What is GraphRAG? Enhancing RAG with Knowledge Graphs - Zilliz blog](https://zilliz.com/blog/graphrag-explained-enhance-rag-with-knowledge-graphs#:~:text=Querying))。①**全局检索（Global Search）：**面对涉及全局概念或话题汇总的问题（如“数据集中最主要的5个主题是什么？”），GraphRAG 会利用社区级别的图谱摘要来检索，从宏观层面获取答案要点。②**局部检索（Local Search）：**对于涉及特定实体的详细提问（如“Elon Musk 创立了哪些公司？”），GraphRAG 将在知识图谱中定位该实体节点，然后沿着相关边遍历一到两跳，搜集邻近相关实体的信息 ([GraphRAG: Unlocking LLM discovery on narrative private data - Microsoft Research](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/#:~:text=In%20comparison%2C%20the%20GraphRAG%20approach,two%20entities%20in%20the%20graph)) ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=))。接着，对通过图谱找到的相关文本片段和关系说明，一并提供给LLM生成答案 ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=))。

**优势分析：**融合知识图谱使 RAG 的检索“从平面走向立体”。传统 RAG 基于文本相似度找资料，容易局限于字面相关，而 GraphRAG 能通过图谱**多跳关联**找到隐藏的关联信息，实现更复杂的推理 ([GraphRAG: Unlocking LLM discovery on narrative private data - Microsoft Research](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/#:~:text=Retrieval,not%20trained%20on%20and%20has)) ([GraphRAG: Unlocking LLM discovery on narrative private data - Microsoft Research](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/#:~:text=For%20example%3A))。例如，对于问题“*Elon Musk 创立的公司的名称是什么？*”，普通 RAG 可能需要多轮检索（先检索Elon Musk信息，再检索其公司的信息）；而 GraphRAG 则可在知识图谱中直接从“Elon Musk”节点出发找到所有“创立”关系的目标节点，一步获取其创建的公司列表 ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=)) ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=,missing%20context%20to%20retrieved%20documents))。GraphRAG 还提供**结果溯源**能力：图谱关系往往来自可靠数据源，LLM 可引用图谱中记录的来源以提供依据，增强回答的可信度 ([GraphRAG: Unlocking LLM discovery on narrative private data - Microsoft Research](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/#:~:text=By%20using%20the%20LLM,answers%20and%20capturing%20evidence%20provenance)) ([GraphRAG: Unlocking LLM discovery on narrative private data - Microsoft Research](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/#:~:text=audit%20the%20LLM%E2%80%99s%20output%20directly,against%20the%20original%20source%20material))。

**劣势与挑战：**GraphRAG 的引入也带来额外开销：构建和维护知识图谱在大规模场景下可能非常复杂且耗时 ([[2412.06206] SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://arxiv.org/abs/2412.06206#:~:text=shared%20entities%2C%20and%20generates%20recursive,improvement%20in%20average%20F1%20scores))。抽取实体和关系的准确性直接决定图谱质量，如果NLP抽取有误可能引入噪声关系。同时，知识图谱通常只涵盖特定领域实体，对于开放领域的问题可能需要结合传统向量检索补全信息。因此，实际系统中常会混合使用图谱检索和文本检索，以兼顾召回率和精确性。例如微软研究提出的 GraphRAG 框架允许将图谱查询与关键词搜索结合，在图谱无法覆盖时退回文本搜索 ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=,Cypher))。

**GraphRAG 实现示例：**下面以伪代码示意 GraphRAG 处理多跳问答的流程：

```python
# 构建知识图谱（离线阶段）
knowledge_graph = build_knowledge_graph(corpus)  # 从语料抽取三元组构建KG

# 图谱增强的查询（在线阶段）
query = "Elon Musk 创立了哪些公司？"
# 1. 从问题识别涉及的实体（这里是 Elon Musk）
entities = extract_entities(query)  # -> ["Elon Musk"]
# 2. 在知识图谱中获取相关子图（找到 Elon Musk 节点的一度邻居及其关系）
subgraph = knowledge_graph.get_subgraph(entities, hop=1)
# 3. 提取子图中相关实体的说明文本（如公司名称、简介）
context_texts = retrieve_texts_from(subgraph.nodes)
# 4. 将图谱上下文与原始问题一起送入 LLM 生成答案
answer = llm.generate(f"问题: {query}\n知识:{context_texts}\n回答:")
print(answer)  # 输出 "Elon Musk 创立了 Zip2、X.com（即 PayPal）、Tesla、SpaceX、Neuralink 和 The Boring Company。"
``` 

以上流程中，GraphRAG 借助知识图谱快速完成了跨越多文档的知识关联和检索 ([Supercharging Retrieval-Augmented Generation (RAG) with Knowledge Graphs: A Deep Dive into GraphRAG](https://blog.adyog.com/2024/12/25/supercharging-retrieval-augmented-generation-rag-with-knowledge-graphs-a-deep-dive-into-graphrag/#:~:text=,missing%20context%20to%20retrieved%20documents))。这展示了 GraphRAG 在复杂问答场景下相较传统 RAG 的优势。

## D. 多模态 RAG

**动机：**现实应用中有大量知识存在于非纯文本形式，如图片、PDF文件、表格、音频等。**多模态 RAG**旨在扩展检索增强生成框架，使其不仅能处理文本，还能从不同模态的数据中检索信息并生成回答 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=A%20retrieval,and%20other%20forms%20of%20information))。例如，一个企业内的文档库可能包含扫描的合同图片、含有图表的PDF报告、数据表格等——多模态 RAG 可以统一查询这些异构数据源，为用户问题提供完整答案。

**视觉语言模型 (VLM)：**多模态 RAG 的关键在于引入**视觉语言模型**，即能够同时理解图像与文本的模型 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=LLMs%20are%20designed%20to%20understand%2C,answering%2C%20and%20more))。这些模型（如 CLIP、BLIP-2、GPT-4V 等）可以将图像和文本映射到同一语义空间，或者在生成时处理文本和视觉输入。VLM 使 RAG 系统能够“看”图片、“读”图表，从而将视觉信息纳入生成过程。

**多模态检索方法：**扩展到多模态后，有两种主要的检索策略：

- **方法1：OCR + 文本检索。**对于图片或 PDF，通过光学字符识别（OCR）或解析工具先提取其中的文本内容，将非文本数据转换为文本描述或元数据 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=Another%20option%20is%20to%20pick,modalities%20in%20the%20primary%20modality)) ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=For%20example%2C%20say%20your%20application,the%20images%20for%20later%20use))。然后，将这些提取的文本与普通文档一起建立向量索引进行检索。这种方式的优点是充分利用现有成熟的文本向量模型，无需训练新的多模态模型，实施相对简单 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=The%20key%20benefit%20here%20is,some%20nuance%20from%20the%20image))。例如，一张产品说明书PDF可提取出文字说明、表格数据，图像则可用一句描述代替；检索时和其他文本一样处理。缺点在于可能丢失**视觉结构**信息（如图像布局、曲线走向） ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=The%20key%20benefit%20here%20is,some%20nuance%20from%20the%20image))。另外OCR过程需保证准确，否则错误文字将影响检索效果。

- **方法2：直接多模态嵌入。**利用多模态模型（如 OpenAI CLIP）将图像和文本**共同嵌入到同一向量空间** ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=Embed%20all%20modalities%20into%20the,same%20vector%20space)) ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=In%20the%20case%20of%20images,for%20all%20question%20and%20answering))。这样，无论查询是文字还是一张图，都能在统一向量空间中找到相似的对象。例如，用户给出一张设备故障照片作为查询，系统可通过 CLIP 找到向量邻近的相关维修手册段落图像或文本说明 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=A%20retrieval,and%20other%20forms%20of%20information))。这种方式避免了信息损失，在一定程度上保留了图像的细节语义。然而挑战在于需要高质量的多模态模型支持，而且不同模态向量的分布对齐需要仔细校准 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=The%20tradeoff%20in%20this%20situation,in%20images%20and%20complex%20tables))。同时，直接存储大量图像向量也带来算力和存储消耗。

**混合检索与重排序：**实践中，还可以结合以上两种方法。例如，对图像既存其OCR的文本向量，又存视觉特征向量；检索阶段分别查询两种索引并融合结果。在最终将检索结果交给LLM前，可利用**重排序模型**对多模态结果做排序优化（如先用跨模态BERT对图像描述和查询计算相关性评分） ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=Have%20separate%20stores%20for%20different,modalities))。这一系列流程确保多模态 RAG 尽可能找到相关信息并准确传递给生成模型。

**应用示例：**想象一个法律助手系统，用户提问：“请根据以下专利附图解释该发明的工作原理。” 系统需要读懂PDF中的电路图和说明文字。多模态 RAG 会先用OCR提取附图说明文字，并调用图像标注模型生成电路图的简要描述，然后将两部分内容与用户问题一起提交给LLM整合回答。这种方式下，LLM 得到了来自图像和文本的联合知识，可以生成完整的解释答复。

**OCR + RAG vs 多模态嵌入对比：**归纳来说，OCR方案实现简单、依赖成熟技术，但可能遗漏视觉信号；而直接多模态方案潜力更大，能捕获复杂图像信息，但实现成本和技术难度较高 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=The%20tradeoff%20in%20this%20situation,in%20images%20and%20complex%20tables))。在实际工程中，可根据需求权衡取舍：如果应用场景中图像主要承载文字信息（如扫描件），OCR 足矣；但如果图像本身蕴含大量无法用三言两语描述的内容（如医学影像），则需要多模态模型直接参与。 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=In%20the%20case%20of%20images,for%20all%20question%20and%20answering))

## E. Agentic RAG：具备智能体特性的 RAG

**概念引入：**随着需求的提升，研究者开始探索让 RAG 系统具备“智能体”（Agent）能力，即不仅进行单轮的检索和回答，还能像人类顾问一样**规划多步推理、调用工具、反思调整**。这便诞生了“Agentic RAG”理念 ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=match%20at%20L867%20Agentic%20RAG%3A,Augmented%20Generation%20and%20Autonomy)) ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=While%20RAG%20focuses%20on%20factual,By%20integrating%20these%20two%20models))。Agentic RAG 将 RAG 从静态问答扩展到**自主、多步骤**的交互过程 ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=may%20not%20be%20enough,foundational%20principles%2C%20and%20use%20cases)) ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=Agentic%20RAG%20innovates%20the%20retrieval,making%20tasks%20without%20experiencing%20hallucinations))。简单来说，在 Agentic RAG 中，LLM 不再一次性给出最终答案，而是可以在内部展开一系列思考和行动，例如多轮检索、结果校验、步骤规划，然后逐步形成答案。这使得 RAG 能应对更复杂的任务，例如推理题、决策规划、多源信息综合等 ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=Agentic%20RAG%20innovates%20the%20retrieval,making%20tasks%20without%20experiencing%20hallucinations)) ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=agentic%20RAG%20seeks%20to%20develop,making%20tasks%20without%20experiencing%20hallucinations))。

**核心模式：**业界总结了提升 LLM Agent 能力的四种关键设计模式 ([Agentic Design Patterns Part 2: Reflection](https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/#:~:text=Last%20week%2C%20I%20described%20four,lead%20to%20surprising%20performance%20gains))——**反思 (Reflection)**、**规划 (Planning)**、**工具使用 (Tool Use)** 和**多智能体协作 (Multi-agent Collaboration)**。Agentic RAG 系统往往结合这几个要素来强化多步推理能力：

- **反思 (Reflection)：**让模型反思自己生成的内容，发现并纠正错误。 ([Agentic Design Patterns Part 2: Reflection](https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/#:~:text=You%20may%20have%20had%20the,is%20the%20crux%20of%20Reflection)) ([Agentic Design Patterns Part 2: Reflection](https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/#:~:text=Here%E2%80%99s%20code%20intended%20for%20task,for%20how%20to%20improve%20it))具体做法是在LLM产出初始答案后，追加一个“批判性思考”步骤：要求LLM检查之前的回答是否不正确或不完整，并给出改进建议，然后据此生成修正后的答案。这个过程类似人类写完答案后自我检查再修改。例如，模型第一次回答代码有bug，通过反思模式，它能自我测试发现错误并修复，再给出正确代码。Self-RAG ([Agentic RAG - Definition and Low-code Implementation | RAGFlow](https://ragflow.io/blog/agentic-rag-definition-and-low-code-implementation#:~:text=Self,the%20following%20two%20major%20components)) ([Agentic RAG - Definition and Low-code Implementation | RAGFlow](https://ragflow.io/blog/agentic-rag-definition-and-low-code-implementation#:~:text=Self,step%20reasoning))正是应用反思的雏形：模型检索完资料作答后，评估答案相关性，不足则重新检索，直到答案充分为止。

- **规划 (Planning)：**在回答复杂问题前，先让模型制定解决问题的计划或步骤清单 ([Agentic RAG - Definition and Low-code Implementation | RAGFlow](https://ragflow.io/blog/agentic-rag-definition-and-low-code-implementation#:~:text=1.%20Open,step%20reasoning.%20Adaptive)) ([Agentic RAG - Definition and Low-code Implementation | RAGFlow](https://ragflow.io/blog/agentic-rag-definition-and-low-code-implementation#:~:text=2.%20Multi,for%20answering%20the%20complex%20questions))。这样模型在脑海中有了“解题思路”再行动，避免了漫无目的的尝试。例如，对于“请总结并比较文件A和文件B的异同”这样的问题，LLM可以先规划：“步骤1：阅读文件A摘要；步骤2：阅读文件B摘要；步骤3：分别列出要点；步骤4：比较要点异同。” 然后按计划逐步检索或提取，再最终形成答案。规划使模型能进行**多跳问题分解**，尤其在问句本身隐含多个子问题时，能逐一解决再汇总 ([Agentic RAG - Definition and Low-code Implementation | RAGFlow](https://ragflow.io/blog/agentic-rag-definition-and-low-code-implementation#:~:text=2.%20Multi,for%20answering%20the%20complex%20questions))。

- **工具使用 (Tool Use)：**赋予 LLM 调用外部工具的能力，以完成其单靠语言模型无法胜任的操作 ([RAGENTIC: RAG-Enhanced Multi-Agent Architecture | Microsoft Community Hub](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/ragentic-rag-enhanced-multi-agent-architecture/4287132#:~:text=understanding%2C%20Workflow%20Optimization%20and%20Multi,a%20wide%20range%20of%20actions)) ([RAGENTIC: RAG-Enhanced Multi-Agent Architecture | Microsoft Community Hub](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/ragentic-rag-enhanced-multi-agent-architecture/4287132#:~:text=Planning%2C%20Memory%2C%20Tool%2C%20and%20Action,documents%20through%20RAGs%20framework%20to))。典型工具包括：搜索引擎（获取实时信息）、计算器（执行精确计算）、翻译器、数据库查询等。在 Agentic RAG 中，LLM 可以根据需要决定调用何种工具，并将工具返回的结果纳入后续对话。例如，当被问及“今天纽约的气温比昨天下降了多少度”时，Agent 会意识到需要查询天气数据（使用搜索工具）和做减法计算（使用计算器），最终给出准确结果。工具使用让 RAG 能处理**超出LLM训练知识范围**或**需要精密计算**的问题，大大扩展了系统能力。

- **多智能体协作 (Multi-agent Collaboration)：**不是只有单一Agent工作，而是**多个Agent分工合作**完成任务 ([What is Agentic AI Multi-Agent Pattern? - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2024/11/agentic-ai-multi-agent-pattern/#:~:text=Vidhya%20www,agent%20systems)) ([Top 4 Agentic AI Design Patterns - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2024/10/agentic-design-patterns/#:~:text=Discover%204%20essential%20Agentic%20Design,driven%20applications.%204.7))。每个Agent可以是一个LLM实例，扮演不同角色。例如，一个Agent负责分析用户意图和规划任务，另一个Agent专注检索资料，还有Agent汇总撰写答案。Agents之间通过对话或共享内存交换信息，迭代完善结果。这种架构利用“**专家Agent**”提高复杂任务的效率和准确性。例如，在法律问答场景，可设置“律师Agent”负责法律条文检索，“解释Agent”负责用通俗语言生成答案，两者协作完成用户咨询。多Agent系统也有助于并行处理子任务，提高响应速度 ([What is Agentic AI Multi-Agent Pattern? - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2024/11/agentic-ai-multi-agent-pattern/#:~:text=many%20tools%20to%20handle,agent%20systems))。需要注意的是，多Agent通信和协调是一大挑战，研究者也在探索如何避免它们相互误导或陷入无效对话。

**Agentic RAG 实践：**在复杂场景中，上述模式往往结合出现。例如一个 Agentic RAG 系统需要回答科研分析类的问题：“根据以下实验结果，提出可能的改进方案，并评估可行性。” 该系统可能这样运作：首先**规划**解决方案框架，然后调用**检索工具**查找相关研究，接着每找到一部分证据就**反思**整合到方案中，最后两个Agent相互**校对**方案可行性。这种多步流程超出了传统单轮RAG的范畴，但借助 Agentic 策略，LLM 能更系统性地完成任务。 ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=The%20motivation%20behind%20agentic%20RAG,in%20a%20continuous%20feedback%20loop)) ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=In%20these%20contexts%2C%20merely%20retrieving,in%20a%20continuous%20feedback%20loop))

总之，Agentic RAG 将 LLM 从被动答题的状态转变为主动问题求解者。它把人类解决复杂问题的思维过程（计划→行动→检查→协作）引入了人工智能模型中。这一方向2024年迅速兴起，被认为是让 AI 变得更“聪明”、“可靠”的关键，许多开源框架如 LangChain Agents、AutoGen、Camel 等都在探索将这些能力应用于实际系统中。

## F. RAG 的挑战与解决方案

尽管 RAG 展示了强大的能力，但在实践中仍面临一些挑战。研究者和工程师们也提出了相应的改进方案来应对这些问题：

**1. 非结构化多模态文档解析：**正如前文讨论的多模态 RAG，现实中的 PDF、扫描图片等文档往往版面复杂、包含文字和图形混排。这给解析带来难度：如何保持段落顺序？表格如何处理？图片说明如何与正文关联？传统OCR得到的一串文本可能打乱原文结构，影响理解。因此需要引入**版面理解**技术，如 LayoutLM 等版面感知模型，或者使用专门的解析库（如 Unstructured.io）提取文档的层次结构（标题、段落、表格单元格等）。针对包含图像的PDF，可以先检测图像区域，用图像caption模型生成描述，然后把描述嵌入到PDF文本中作为补充。 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=Why%20is%20multimodality%20hard%3F)) ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=For%20instance%2C%20consider%20images%20,pond%2C%20ocean%2C%20trees%2C%20and%20sand))这些方法虽增加了一些预处理成本，但能极大提高后续检索和生成对非纯文本数据的理解力。

**2. 检索召回率：**RAG 系统的答案质量高度依赖于检索阶段找回的文档是否包含关键信息。然而仅靠**单一检索方式**可能存在漏召回的风险。例如，向量检索对语义匹配敏感，但如果用户问题措辞独特、与知识库表述不一致，可能无法找到其实相关的内容；而传统关键词检索（BM25）对同义表达不敏感，也可能漏掉重要结果。为提高召回率，现代 RAG 趋向采用**混合检索（Hybrid Search）**：同时执行稀疏向量（BM25）和密集向量检索，将二者结果取并集或融合评分 ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=)) ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=%28TF,resources%20for%20training%20and%20inference))。实证研究表明，混合检索往往能取得比单一方法更高的 Recall@K ([On Complementarity Objectives for Hybrid Retrieval - ACL Anthology](https://aclanthology.org/2023.acl-long.746.pdf#:~:text=Anthology%20aclanthology,Questions%2C%20where%20a%20yellow)) ([hybrid search using both BGE and BM25 #17 - GitHub](https://github.com/FlagOpen/FlagEmbedding/issues/17#:~:text=Their%20results%20show%20that%20the,In%20particular))。此外，引入**重排序模型**也是常用策略：先粗检索出较多候选文档（例如20篇），然后用一个更精细的交互模型（如跨编码器BERT或ColBERT ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=The%20retriever%20module%20is%20central,resources%20for%20training%20and%20inference))）对这20篇进行相关度重新打分，选出最相关的前几篇供LLM使用。这种“**双阶段检索**”在开放域问答里已证明有效，可显著提升检索精度，减少答案遗漏。

**3. 语义鸿沟问题：**所谓语义鸿沟，指的是用户复杂问题所需的信息往往分散在多条孤立片段中，**单一片段不足以回答**。如果检索模块仅根据语义相似度取回几段零散文本，LLM 可能难以综合它们给出正确结论。这在需要多跳推理的问题中尤为突出。例如，“击败篡位者 Allectus 的人之子的名字？”这个问题涉及三层关系，需要跨越多个资料片段 ([What is GraphRAG? Enhancing RAG with Knowledge Graphs - Zilliz blog](https://zilliz.com/blog/graphrag-explained-enhance-rag-with-knowledge-graphs#:~:text=For%20example%2C%20consider%20this%20question%3A,who%20defeated%20the%20usurper%20Allectus%3F%E2%80%9D))。为弥合语义鸿沟，近来提出了多种改进方法：

- **GraphRAG：**前文已述，GraphRAG使用知识图谱显式连接相关实体，使模型能够通过图谱进行多跳检索和推理 ([GraphRAG: Unlocking LLM discovery on narrative private data - Microsoft Research](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/#:~:text=Retrieval,not%20trained%20on%20and%20has)) ([GraphRAG: Unlocking LLM discovery on narrative private data - Microsoft Research](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/#:~:text=For%20example%3A))。GraphRAG 特别适用于 **“连接点”缺失** 的情况——当答案需要将两个看似无关的事实关联起来时，图谱中的中间实体可以作为桥梁帮助模型“连点成线”。

- **RAPTOR：**2024 年提出的 RAPTOR（Recursive Abstractive Processing for Tree-Organized Retrieval）方法提供了一种不同思路 ([[2401.18059] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059#:~:text=%3E%20Abstract%3ARetrieval,retrieval%20with%20recursive%20summaries%20offers)) ([[2401.18059] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059#:~:text=retrieve%20only%20short%20contiguous%20chunks,augmented%20LMs%20on%20several))。它通过**递归摘要**构建文档树：将长文档分段并生成每段摘要，再对摘要进行进一步聚合，形成层次化的**摘要树** ([[2401.18059] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059#:~:text=retrieve%20only%20short%20contiguous%20chunks,augmented%20LMs%20on%20several))。查询时，RAPTOR 不仅检索原始文本片段，还可以检索不同抽象层级的摘要节点，从而一次获取对长文档**全局上下文**的概览 ([[2401.18059] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059#:~:text=of%20recursively%20embedding%2C%20clustering%2C%20and,performance%20on%20the%20QuALITY%20benchmark))。实验显示，这种方法在需要跨越长内容、多步推理的QA任务上效果显著提升，在QuALITY等长文阅读理解基准上将准确率提高了20个百分点 ([[2401.18059] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059#:~:text=Controlled%20experiments%20show%20that%20retrieval,in%20absolute%20accuracy))。RAPTOR 援引了“先粗后细”的检索理念：先获取宏观摘要锁定方向，再深入细节检索具体内容，从而克服了单段检索看局部、缺全局的问题。

- **SiReRAG：**这是另一项最新工作（Nan Zhang 等，2024） ([[2412.06206] SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://arxiv.org/abs/2412.06206#:~:text=,explore%20some%20variances%20to%20construct)) ([[2412.06206] SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://arxiv.org/abs/2412.06206#:~:text=SiReRAG%20extracts%20propositions%20and%20entities,improvement%20in%20average%20F1%20scores))。SiReRAG 名称源自 **Similar & Related RAG**，其核心在于同时考虑**语义相似度**和**知识关联度**两种视角来组织和检索文档 ([[2412.06206] SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://arxiv.org/abs/2412.06206#:~:text=,explore%20some%20variances%20to%20construct))。具体实现上，SiReRAG 一方面像 RAPTOR 那样基于语义相似度构建**相似性树**（similarity tree），另一方面抽取文中的**命题和实体**，通过共享实体将命题连接成**关联性树**（relatedness tree） ([[2412.06206] SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://arxiv.org/abs/2412.06206#:~:text=reasoning,methods%20on%20three%20multihop%20datasets)) ([[2412.06206] SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://arxiv.org/abs/2412.06206#:~:text=SiReRAG%20extracts%20propositions%20and%20entities,improvement%20in%20average%20F1%20scores))。然后将两棵树展开融合为统一的索引。当用户提问时，系统既会检索语义上相近的内容，也会检索在知识图谱上相关的内容，确保多跳所需的背景不会被遗漏。SiReRAG 在多跳数据集（如HotpotQA）上相比现有方法平均提升约1.9%的F1，结合重排序后提升可达7.8% ([[2412.06206] SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://arxiv.org/abs/2412.06206#:~:text=shared%20entities%2C%20and%20generates%20recursive,improvement%20in%20average%20F1%20scores))。这证明同时建模“相似”与“相关”是增强复杂知识检索的有效途径。

通过上述改进，RAG 系统正变得更加强大：它能解析多模态数据、混合检索提升召回，并利用知识图谱或分层摘要来进行更聪明的检索和推理。这些技术的融合，大大拓宽了 RAG 在真实世界复杂场景下的适用性。

## G. 未来趋势展望

**2024 年的标志性进展：**在过去一年里，RAG 领域涌现出诸多突破，使这一技术更加成熟完善：

- **多模态检索全面开花：**越来越多的应用开始将图像、表格等纳入 RAG 管道。一些企业级解决方案（如 Azure Cognitive Search 的 RAG、多模态 LlamaIndex 等）支持对 PDF、图片直接建立索引并检索 ([An Easy Introduction to Multimodal Retrieval-Augmented Generation | NVIDIA Technical Blog](https://developer.nvidia.com/blog/an-easy-introduction-to-multimodal-retrieval-augmented-generation/#:~:text=A%20retrieval,and%20other%20forms%20of%20information))。同时，有了 CLIP、BLIP-2、GPT-4V 等强大多模态模型，多模态 RAG 从研究走向落地，催生了不少如智能客服、医学影像问答等场景的新应用。

- **检索精度提升：ColBERT 及重排序模型应用：**为了弥补 dense retriever 与 lexical retriever 各自的不足，重排序技术在 RAG 中得到重视。ColBERT 等上下文交互式检索模型通过**细粒度匹配**提高了检索精度，被集成到开源 RAG 框架中，用于对初始检索结果进行精排，减少了无关片段干扰 ([RAG, AI Agents, and Agentic RAG: An In-Depth Review and Comparative Analysis | DigitalOcean](https://www.digitalocean.com/community/conceptual-articles/rag-ai-agents-agentic-rag-comparative-analysis#:~:text=The%20retriever%20module%20is%20central,resources%20for%20training%20and%20inference))。2024 年也看到跨编码器（cross-encoder）在检索阶段的广泛应用，很多问答系统在LLM生成前会调用一个mini-LLM来验证检索内容与查询的相关性，确保LLM收到的是高质量上下文。这种两段式架构已成为最佳实践之一。

- **知识图谱融合与 GraphRAG 落地：**知识驱动的 RAG 在产业界受到关注。微软开源了 GraphRAG 方案 ([GitHub - microsoft/graphrag: A modular graph-based Retrieval-Augmented Generation (RAG) system](https://github.com/microsoft/graphrag#:~:text=Overview))并在企业私有数据分析中验证了其价值 ([GraphRAG: Unlocking LLM discovery on narrative private data - Microsoft Research](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/#:~:text=GraphRAG%20uses%20LLM,For%20example)) ([GraphRAG: Unlocking LLM discovery on narrative private data - Microsoft Research](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/#:~:text=To%20address%20this%2C%20the%20tech,previously%20applied%20to%20private%20datasets))。一些商业产品开始支持将企业自有知识图谱与LLM对接，实现更精准的企业智能问答。GraphRAG 相关的研究也激增（2024年底有了第一篇GraphRAG综述论文 ([[2501.00309] Retrieval-Augmented Generation with Graphs (GraphRAG)](https://arxiv.org/abs/2501.00309#:~:text=Given%20the%20broad%20applicability%2C%20the,discuss%20research%20challenges%20and%20brainstorm))）, 预示这一方向未来的潜力。

- **开源工具与社区：**诸如 LangChain、LlamaIndex、Haystack 等开源项目在2024年持续进化，提供了丰富的 RAG 模板和组件，大幅降低了开发门槛。不少面向RAG的向量库（如 Milvus、Weaviate）也优化了与LLM集成的接口。社区还涌现出了许多教程和最佳实践博客，帮助开发者避开常见陷阱（如检索结果格式化、token 限制等）。可以说，RAG 生态系统在这一年里趋于完善稳定。

**2025 年的发展趋势：**展望未来，RAG 技术有望在以下几个方向取得进一步突破：

- **Agentic RAG 大放异彩：**让 RAG 系统具备智能体能力的探索将继续深入。我们预计会出现更多**自主 Agent**驱动的 RAG 框架，支持复杂对话、多轮质询、自动纠错等功能。一些研究会致力于**agent 权限控制与可靠性**，以保证Agent决策可控、安全。同时，多Agent协作也可能催生新颖的应用形态，例如由多个专用Agent组成的“AI团队”为用户解决复杂任务。在实际产品中，Agentic RAG 有望用于客户咨询、数据分析助理等需要综合多步推理的场景，让AI的回答更加**连贯**且**靠谱**。

- **LLM 与知识库融合深化：**未来的LLM可能在训练阶段就引入检索机制，实现更紧密的**“生成-检索”融合**。例如，类似 Retrieval Transformer 的架构可能让模型在生成每个token时动态访问知识库（一些早期探索如 RETRO 模型已证明可行）。此外，**知识图谱+LLM融合**也会是热点方向。一种可能性是开发**图谱感知的LLM**，能以图结构形式输入知识（比如对Graph数据有更长的记忆或特殊处理），使模型直接在图上推理而非仅文本推理。知识库（KG、数据库）与LLM的交互接口也将标准化，涌现更多类似 GraphQL+LLM、SQL+LLM 这样的融合方案。

- **多模态与跨模态推理：**随着多模态基础模型能力提升，RAG 将扩展到**更多模态**：如音频、视频的检索与问答。2025年可能见到支持视频片段检索然后总结的 RAG 系统，或是能听取用户语音提问并在内部检索图像+文本再语音回答的全链路系统。跨模态推理（比如“看图回答涉及图中文本的问题”）将变得更可靠。我们或许还会看到将**传感器数据**（物联网流数据、实时监控）接入RAG，从而把 LLM 应用于实时决策支持——这会对检索的速度和准确性提出更高要求，倒逼向量数据库和ANN算法进一步优化。

- **个性化与持续学习：**未来的 RAG 系统可能更加注重**用户个性化**和**持续知识更新**。一方面，通过分析用户历史查询和反馈，系统可以调整检索排序，更精准地提供符合用户偏好的信息。另一方面，RAG 的知识库有望引入**增量学习**机制：根据用户新提的问题或上传的资料，不断丰富内部知识库，并在必要时对检索模块做轻量调优。这种自适应能力将使 RAG 越用越智能。

- **性能与效率优化：**尽管功能在增强，2025年的RAG也需要在性能上更上一层楼，包括降低**延迟**、优化**内存占用**、提升对海量知识的扩展性等。我们可能看到更轻量的LLM被用于RAG以加快响应（如压缩蒸馏模型专门做检索问答），以及更高效的倒排索引与ANN融合技术。隐私安全方面，对本地部署、加密检索等的需求会上升，RAG 在这些约束下的实现也会有所突破。

总的来说，RAG 技术正朝着“**更聪明、更全面、更高效**”的方向演进。从最初用于开放域问答的一招鲜技巧，发展到如今多模态、多跳推理、Agent驱动的复杂AI系统框架，RAG 展现出强大的生命力。可以预见，在2025年及以后，RAG将成为构建各类认知智能应用的**核心范式**之一，为各行业的知识处理与决策支持带来革命性变化。

**参考文献：**

1. *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*. arXiv:2005.11401  ([[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401#:~:text=%3E%20Abstract%3ALarge%20pre,augmented%20generation%20%28RAG%29)) ([[2005.11401] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401#:~:text=models%20which%20combine%20pre,extract%20architectures))

2. *The Bitter Lesson.* (Richard Sutton, 2019) *IncompleteIdeas Blog.*

3. *Graph Retrieval-Augmented Generation: A Survey.* arXiv:2501.00309  ([[2501.00309] Retrieval-Augmented Generation with Graphs (GraphRAG)](https://arxiv.org/abs/2501.00309#:~:text=enhances%20downstream%20task%20execution%20by,specific%20relational%20knowledge%2C%20poses)) ([[2501.00309] Retrieval-Augmented Generation with Graphs (GraphRAG)](https://arxiv.org/abs/2501.00309#:~:text=techniques%20is%20urgently%20desired,maintained%20at%20this%20https%20URL))

4. *RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval.* arXiv:2401.18059  ([[2401.18059] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059#:~:text=%3E%20Abstract%3ARetrieval,retrieval%20with%20recursive%20summaries%20offers)) ([[2401.18059] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059#:~:text=Controlled%20experiments%20show%20that%20retrieval,in%20absolute%20accuracy))

5. *SiReRAG: Indexing Similar and Related Information for Multihop Reasoning.* arXiv:2412.06206  ([[2412.06206] SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://arxiv.org/abs/2412.06206#:~:text=,explore%20some%20variances%20to%20construct)) ([[2412.06206] SiReRAG: Indexing Similar and Related Information for Multihop Reasoning](https://arxiv.org/abs/2412.06206#:~:text=shared%20entities%2C%20and%20generates%20recursive,solution%2C%20SiReRAG%20enhances%20existing%20reranking))

6. *Learning Transferable Visual Models from Natural Language Supervision.* arXiv:2103.00020 (CLIP).

7. *ReAct: Synergizing Reasoning and Acting in Language Models.* arXiv:2210.03629.

