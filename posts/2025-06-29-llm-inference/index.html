<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Large Language Model Inference | Yue Shui Blog</title><meta name=keywords content="LLM,Inference,Quantization,Pruning,Knowledge Distillation,KV Cache,Attention,Speculative Decoding,FlashAttention,vLLM,Transformer,Sparsity,Mixture of Experts"><meta name=description content="In recent years, Large Language Models (LLMs) have achieved revolutionary breakthroughs in fields such as natural language processing, code generation, and even multimodal interaction. However, the powerful capabilities of these models come at the cost of enormous computational and memory overhead, especially during the inference stage. Efficiently deploying and running these models, which have billions or even trillions of parameters, has become a core challenge in scaling LLM technology for real-world applications."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-06-29-llm-inference/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-06-29-llm-inference/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-06-29-llm-inference/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-06-29-llm-inference/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Large Language Model Inference"><meta property="og:description" content="In recent years, Large Language Models (LLMs) have achieved revolutionary breakthroughs in fields such as natural language processing, code generation, and even multimodal interaction. However, the powerful capabilities of these models come at the cost of enormous computational and memory overhead, especially during the inference stage. Efficiently deploying and running these models, which have billions or even trillions of parameters, has become a core challenge in scaling LLM technology for real-world applications."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-29T12:00:00+08:00"><meta property="article:modified_time" content="2025-09-01T18:02:26+08:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Inference"><meta property="article:tag" content="Quantization"><meta property="article:tag" content="Pruning"><meta property="article:tag" content="Knowledge Distillation"><meta property="article:tag" content="KV Cache"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Large Language Model Inference"><meta name=twitter:description content="In recent years, Large Language Models (LLMs) have achieved revolutionary breakthroughs in fields such as natural language processing, code generation, and even multimodal interaction. However, the powerful capabilities of these models come at the cost of enormous computational and memory overhead, especially during the inference stage. Efficiently deploying and running these models, which have billions or even trillions of parameters, has become a core challenge in scaling LLM technology for real-world applications."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Large Language Model Inference","item":"https://syhya.github.io/posts/2025-06-29-llm-inference/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Large Language Model Inference","name":"Large Language Model Inference","description":"In recent years, Large Language Models (LLMs) have achieved revolutionary breakthroughs in fields such as natural language processing, code generation, and even multimodal interaction. However, the powerful capabilities of these models come at the cost of enormous computational and memory overhead, especially during the inference stage. Efficiently deploying and running these models, which have billions or even trillions of parameters, has become a core challenge in scaling LLM technology for real-world applications.\n","keywords":["LLM","Inference","Quantization","Pruning","Knowledge Distillation","KV Cache","Attention","Speculative Decoding","FlashAttention","vLLM","Transformer","Sparsity","Mixture of Experts"],"articleBody":"In recent years, Large Language Models (LLMs) have achieved revolutionary breakthroughs in fields such as natural language processing, code generation, and even multimodal interaction. However, the powerful capabilities of these models come at the cost of enormous computational and memory overhead, especially during the inference stage. Efficiently deploying and running these models, which have billions or even trillions of parameters, has become a core challenge in scaling LLM technology for real-world applications.\nThe challenges of LLM inference primarily stem from two aspects:\nHuge Memory Footprint: In addition to the model parameters themselves, the inference process requires storing a large amount of intermediate state, especially the KV Cache. For example, for a request with a batch size of 512 and a sequence length of 2048, its KV cache can be as large as 3TB, several times the size of the model itself. Furthermore, the computational complexity of self-attention grows quadratically with the sequence length. Low parallelizability: The text generation of LLMs is inherently an autoregressive process, meaning tokens are generated one by one, with the generation of the next token depending on all previously generated tokens. This serial nature makes the decoding process difficult to parallelize efficiently. Principles of Token Generation To better understand the optimization techniques that follow, we first need to understand how large models generate text and identify the key bottlenecks in their inference process.\nAutoregressive Generation Mainstream large language models like GPT use a Decoder-Only Transformer architecture and generate text in an autoregressive manner. The basic idea is that the probability of a text sequence can be decomposed into a product of a series of conditional probabilities. Given an initial context word sequence $W_0$ (usually the user’s input prompt), the model predicts the next word (token) one at a time, adding the newly generated word to the context as input for the next prediction step. This process can be represented by the following formula:\n$$ P(w_{1:T} | W_0) = \\prod_{t=1}^{T} P(w_t | w_{1:t-1}, W_0), \\text{ with } w_{1:0} = \\emptyset $$where $w_t$ is the word generated at time step $t$, and $w_{1:t-1}$ is the sequence of all words generated before time step $t$. The generation process continues until the model produces a special end-of-sequence (EOS) token or reaches a predefined maximum length $T$.\nPrefilling and Decoding The autoregressive nature of generation dictates that LLM inference can be clearly divided into two stages: the Prefilling stage and the Decoding stage.\nFig. 1. The Prefilling and Decoding Stages of LLM Inference. (Image source: Zhou et al., 2024)\nPrefilling Stage: In this stage, the model processes the entire input prompt in parallel (e.g., “I, like, natural, language” in Figure 1) and computes the probability distribution for the first output token (“Processing”). This stage is characterized by high parallelism, as all input tokens can be fed into the Transformer model at once. This allows compute-intensive operations (like matrix multiplication) to fully utilize the parallel processing power of GPUs, making it compute-bound.\nDecoding Stage: In this stage, the model generates subsequent tokens one by one. Each time a token is generated, it is appended to the end of the existing sequence and used as input for the next prediction. This process is serial because the generation of the next token depends on the previous one. Consequently, this stage is memory-bound, with the main bottleneck being the loading of the massive model weights from GPU memory, rather than the computation itself.\nFig. 2. Illustration of the memory variation through time (latency) during one generation process. Note that the author ignores the activation size in this figure for simplification. (Image source: Zhou et al., 2024)\nTo accelerate the decoding process, modern LLM inference frameworks widely adopt the KV Cache technique. In the Transformer’s self-attention mechanism, each token needs to interact with all preceding tokens. To avoid recomputing the Key (K) and Value (V) vectors for all previous tokens when generating each new token, the system caches these computed K and V values. This cache is the KV Cache.\nAs shown in Figure 2, the size of the KV Cache grows linearly as the generated sequence lengthens. For a model with billions of parameters and long sequences, the KV Cache can occupy several gigabytes or even tens of gigabytes of VRAM. This makes VRAM the scarcest resource in LLM inference, severely limiting the number of requests the system can handle simultaneously (i.e., the batch size), which directly impacts inference throughput. Therefore, how to efficiently manage and optimize the KV Cache is one of the core problems in LLM inference optimization.\nDecoding Strategies At each decoding step, the model outputs a probability distribution over the entire vocabulary. The method used to select the next token from this distribution is determined by the decoding strategy (or token generation strategy). Different strategies significantly affect the quality, creativity, and coherence of the generated text.\nGreedy Search Greedy search is the simplest decoding strategy. At each time step $t$, it always selects the token with the highest probability as the output:\n$$ w_t = \\underset{w}{\\operatorname{argmax}} P(w | w_{1:t-1}) $$This method greatly reduces computational complexity and produces results quickly, but it has clear limitations. Because it only makes locally optimal choices at each step, greedy search can easily get stuck in local optima, overlooking globally better possibilities. This often results in generated text that is dull, repetitive, and lacks diversity and creativity.\nFig. 3. At each time step, greedy search selects the token with the highest conditional probability. (Image source: d2l-en, 2019)\nCode Implementation:\nimport torch import torch.nn.functional as F def greedy_search(model, input_ids, max_len=20, eos_token_id=2): \"\"\" A simple implementation of Greedy Search. `model` should be a function that takes input_ids and returns logits. \"\"\" generated_sequence = input_ids for _ in range(max_len): # Get logits for the last token logits = model(generated_sequence) next_token_logits = logits[:, -1, :] # Select the token with the highest probability next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1) # Append the new token to the sequence generated_sequence = torch.cat([generated_sequence, next_token], dim=1) # Stop if EOS token is generated if next_token.item() == eos_token_id: break return generated_sequence Beam Search To overcome the local optima problem of greedy search, beam search maintains $k$ (num_beams or beam width) most likely candidate sequences (called “beams”) at each decoding step. In the next step, it expands based on these $k$ sequences and again selects the $k$ new sequences with the highest overall probability. Finally, the algorithm selects the candidate sequence with the highest overall probability from all completed sequences as the final output.\nFig. 4. The process of beam search (beam size $=2$; maximum length of an output sequence $=3$ ). The candidate output sequences are $A, C, A B, C E, A B D$, and $C E D$. (Image source: d2l-en, 2019)\nThis approach expands the search space, effectively reducing the impact of local optima and typically generating higher-quality, more coherent text. However, the essence of beam search is still to choose the path with the highest overall probability, which makes it prone to producing high-frequency, common expressions in open-ended generation tasks, potentially lacking creativity and diverse output.\nTemperature Sampling Fig. 5. Illustration of Temperature Sampling. (Image source: Big Hummingbird Blogs, 2024)\nUnlike deterministic search methods, sampling methods introduce randomness, making the generated text more diverse and creative. The most basic sampling method is to sample directly from the model’s probability distribution. Temperature sampling adjusts the shape of the original probability distribution using a temperature coefficient $T$, applied to the Softmax function. The temperature coefficient controls the flatness of the token probability distribution output by the LLM. A higher temperature makes the distribution flatter and the output more random, while a lower temperature makes the distribution more extreme and the output more stable.\n$$ P_T(w_i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)} $$where $z_i$ is the logit output by the model for word $w_i$.\nWhen $T \\to 1$, the probability distribution remains unchanged. When $T \u003c 1$ (cooling), the distribution becomes “sharper,” making high-probability words more likely to be selected, and the generated result is closer to greedy search. When $T \u003e 1$ (heating), the distribution becomes “flatter,” giving low-probability words a chance to be selected, resulting in more diverse and random output. Top-K Sampling Top-K sampling (Fan et al., 2018) retains only the $K$ most probable candidate words before sampling, then renormalizes and samples from this set of $K$ words. This effectively prevents the model from sampling from extremely low-probability words, avoiding the generation of incoherent text. However, its drawback is that the value of $K$ is fixed and cannot adapt dynamically to different probability distributions.\nFig. 6. Illustration of Top-K Sampling. (Image source: Big Hummingbird Blogs, 2024)\nTop-p (Nucleus) Sampling Top-p sampling (Holtzman et al., 2019) uses a dynamic method to select the set of candidate words. It starts with the most probable word and accumulates their probabilities until the sum exceeds a preset threshold $p$ (e.g., 0.9). The model then samples only from this dynamically generated, minimal set of candidate words $V_{\\text{top-p}}$. This method balances text coherence and creativity and is currently one of the most commonly used and effective strategies for open-ended text generation.\nFig. 7. Illustration of Top-p Sampling. (Image source: Big Hummingbird Blogs, 2024)\nCombined Sampling Code Implementation (Top-K, Top-p, Temperature):\nimport torch import torch.nn.functional as F @torch.no_grad() def generate_with_sampling(model, idx, max_new_tokens, temperature=1.0, top_k=None, top_p=None, eos_token_id=2): for _ in range(max_new_tokens): # Crop context if it's too long idx_cond = idx if idx.size(1) \u003c= model.config.max_position_embeddings else idx[:, -model.config.max_position_embeddings:] # Forward pass to get logits logits = model(idx_cond).logits[:, -1, :] # Apply temperature if temperature \u003e 0: logits = logits / temperature # Apply Top-K filtering if top_k is not None: v, _ = torch.topk(logits, min(top_k, logits.size(-1))) logits[logits \u003c v[:, [-1]]] = -float('Inf') # Apply Top-p (Nucleus) filtering if top_p is not None: sorted_logits, sorted_indices = torch.sort(logits, descending=True) cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) # Remove tokens with cumulative probability above the threshold sorted_indices_to_remove = cumulative_probs \u003e top_p # Shift the indices to the right to keep the first token above the threshold sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 indices_to_remove = sorted_indices[sorted_indices_to_remove] logits.scatter_(1, indices_to_remove, -float('Inf')) # Convert logits to probabilities and sample probs = F.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) # Append sampled index and check for EOS idx = torch.cat((idx, idx_next), dim=1) if idx_next.item() == eos_token_id: break return idx Speculative Decoding Speculative Decoding (Leviathan et al., 2023) is an innovative acceleration technique that aims to achieve the generation quality of a large model at the speed of a small model, thereby reducing latency without sacrificing quality.\nIt uses a small, fast Draft Model to generate multiple (e.g., $k$) candidate tokens at once. Then, the large Target Model performs a single forward pass to validate these $k$ tokens in parallel. If the tokens predicted by the draft model match those of the target model, they are accepted, effectively generating multiple tokens with a single forward pass. If they don’t match, the subsequent predictions from the draft model are discarded, and the target model’s prediction is used for correction.\nFig. 8. Overview of online speculative decoding (OSD) framework: For each prompt, the draft model suggests multiple tokens and the target model performs the verification. (Image source: Liu et al., 2024)\nAs long as there is some consistency between the predictions of the draft and target models, speculative decoding can significantly reduce generation latency. Variations include Self-speculative decoding, which uses the early layers of the model itself as the draft model.\nHeuristic Strategies Best-of-N / Majority Vote: These methods improve the quality and robustness of the final result by generating multiple candidate answers. Best-of-N: The LLM generates N answers, which are then scored by an independent evaluation model (Verifier) or a Reward Model. The answer with the highest score (Best) is selected as the final output. Majority Vote / Self-Consistency: The LLM generates multiple different reasoning paths (Chain-of-Thought) and answers for the same question. The most consistent answer is then selected through a majority vote. This method is particularly effective for tasks requiring complex reasoning. Overview of Optimization Methods Now that we understand the basic principles of inference, let’s delve into how to optimize this process. The main goals of inference optimization are to reduce latency, increase throughput, and decrease memory footprint. Existing techniques can be broadly categorized into three areas: model compression, memory and computation optimization, and efficient model architectures.\nTypically, the goals of model inference optimization include:\nReducing the model’s memory footprint by using fewer GPU devices and less VRAM. Reducing computational complexity by decreasing the number of floating-point operations (FLOPs) required. Reducing inference latency to make the model run faster. To reduce the cost of inference in terms of memory and time, several methods can be employed:\nApplying various parallelization techniques to scale the model across a large number of GPUs. By intelligently parallelizing model components and data, it’s possible to run models with trillions of parameters. Memory Offloading, which moves temporarily unused data to the CPU and reads it back when needed. This helps reduce memory usage but increases latency. Smart Batching Strategies; for example, EffectiveTransformer packs consecutive sequences together to eliminate padding in batches. Network compression techniques, such as pruning, quantization, and distillation. Models with fewer parameters or lower bit-width naturally require less memory and run faster. Improvements specific to model architectures. Many architectural changes, especially to the attention layer, help speed up Transformer decoding. You can refer to a previous post on Large Model Training for different types of training parallelization and memory-saving designs, including CPU memory offloading. This article will focus on network compression techniques and architectural improvements for Transformer models.\nKnowledge Distillation Knowledge Distillation (KD) (Hinton et al., 2015) is a direct method for building a smaller model to accelerate inference by transferring the knowledge from a pre-trained, expensive model (the “teacher model”) to a smaller, cheaper model (the “student model”). There are few restrictions on the student model’s architecture, other than requiring its output space to match the teacher’s to construct a suitable learning objective.\nFig. 9. The generic framework of teacher-student knowledge distillation training. (Image source: Gou et al., 2020)\nGiven a dataset, the student model learns to mimic the teacher’s output through a distillation loss function. Neural networks typically have a softmax layer; for example, an LLM outputs a probability distribution over tokens. Let $\\mathbf{z}_t$ and $\\mathbf{z}_s$ be the pre-softmax logits of the teacher and student models, respectively. The distillation loss minimizes the difference between the two softmax outputs, both with a high temperature $T$. When ground truth labels $\\mathbf{y}$ are available, we can combine this with a supervised learning objective (e.g., cross-entropy) that operates on the ground truth labels and the student’s soft logits.\n$$ \\mathcal{L}_{\\mathrm{KD}}=\\mathcal{L}_{\\text {distill }}\\left(\\operatorname{softmax}\\left(\\mathbf{z}_t, T\\right), \\operatorname{softmax}\\left(\\mathbf{z}_s, T\\right)\\right)+\\lambda \\mathcal{L}_{\\mathrm{CE}}\\left(\\mathbf{y}, \\mathbf{z}_s\\right) $$where $\\lambda$ is a hyperparameter that balances learning from soft and hard targets. A common choice for $\\mathcal{L}_{\\text {distill}}$ is KL-divergence or cross-entropy.\nAn early success story is DistilBERT (Sanh et al. 2019), which reduced BERT’s parameters by 40% while retaining 97% of its performance on downstream fine-tuning tasks and running 71% faster. DistilBERT’s pre-training loss is a combination of a soft distillation loss, a supervised training loss (masked language modeling loss $\\mathcal{L}_{\\text{MLM}}$ in BERT), and a special cosine embedding loss to align the hidden state vectors of the teacher and student models.\nFig. 10. The performance of DistilBERT (Image source: Sanh et al., 2019)\nDistillation can be easily combined with quantization, pruning, or sparsification techniques, where the teacher model is the original full-precision, dense model, and the student model is quantized, pruned, or sparsified to achieve higher sparsity.\nQuantization To further improve model performance during inference, we can go beyond low-precision floating-point numbers and use quantization. Quantization converts the model’s floating-point weights into low-bit integer representations, such as 8-bit integers (INT8) or even 4-bit integers (INT4).\nThere are generally two ways to apply quantization to deep neural networks:\nPost-Training Quantization (PTQ): The model is first trained to convergence, and then its weights are converted to a lower precision without further training. This method is typically low-cost to implement compared to training. Quantization-Aware Training (QAT): Quantization is applied during pre-training or further fine-tuning. QAT can achieve better performance but requires additional computational resources and access to representative training data. Precision Comparison In the field of deep learning, numerical precision determines the delicate balance between computational speed and model performance. Understanding the pros and cons of different floating-point and integer formats is key to optimizing the performance of large-scale models. Floating-point numbers are represented in a computer with three parts:\nSign: Indicates whether the number is positive or negative. Exponent: Determines the dynamic range of the number. Mantissa (or Significand): Determines the precision of the number. For convenience, we often refer to the mantissa as the fraction. Fig. 11. fp32 vs fp16 vs bf16 (Image source: Raschka, 2023)\nType Total Bits Sign Bits Exponent Bits Mantissa Bits Characteristics FP64 (Double-precision) 64 1 11 52 Extremely high precision, widely used in scientific computing, but computationally expensive and memory-intensive, rarely used in deep learning. FP32 (Single-precision) 32 1 8 23 Standard format for deep learning training, moderate speed, larger memory footprint. FP16 (Half-precision) 16 1 5 10 Faster computation, half the memory footprint of FP32, but limited dynamic range, prone to numerical overflow. BF16 (Brain Floating Point) 16 1 8 7 Same dynamic range as FP32, avoids overflow, better suited for LLMs, slightly lower precision than FP16. While pure FP16 precision is fast and memory-efficient, its limited dynamic range makes it highly susceptible to numerical overflow and underflow, which can make training unstable or even prevent convergence. Therefore, using Mixed-Precision Training is crucial.\nQuantization maps floating-point numbers to integers, further reducing computational complexity and memory footprint. Specifically:\nINT8: Occupies only 1/4 of the memory of FP32, significantly accelerating inference speed, but may slightly reduce model accuracy. INT4: A more extreme compression scheme, better suited for devices with extremely limited resources or inference scenarios requiring very high throughput. Challenges in Transformer Quantization Many studies on Transformer model quantization share a common finding: simple low-precision (e.g., 8-bit) post-training quantization leads to a significant performance drop. This is mainly due to the high dynamic range of activation values, and a simple activation quantization strategy cannot maintain the model’s performance.\nFig. 12. Only quantizing model weights to 8-bit while keeping activation at full precision (W8A32) achieves much better results when activations are quantized to 8-bit irrespective of whether weights are in lower precision (W8A8 and W32A8). (Image source: Bondarenko et al. 2021)\nBondarenko et al. (2021) found in experiments with small BERT models that the input and output of the FFN (feed-forward network) have very different dynamic ranges due to strong outliers in the output tensor. Therefore, per-tensor quantization of the FFN’s residual sum can lead to significant errors.\nAs model sizes grow to billions of parameters, large-magnitude outlier features begin to appear in all Transformer layers, causing simple low-bit quantization to fail. Researchers observed this phenomenon in the OPT (Zhang et al. 2022) model, which is larger than 6.7B parameters. Larger models have more layers with extreme outliers, and these outlier features have a significant impact on model performance. In a few dimensions, the magnitude of activation outliers can be about 100 times larger than most other values.\nFig. 13. The mean zero-shot accuracy over a set of language tasks (WinoGrande, HellaSwag, PIQA, LAMBADA) of OPT models of increasing sizes. (Image source: Dettmers et al. 2022)\nPost-Training Quantization (PTQ) Mixed-precision quantization The most direct way to solve the aforementioned quantization challenges is to implement quantization with different precisions for weights and activations.\nGOBO (Zadeh et al. 2020) was one of the first models to apply post-training quantization to BERT. It assumes that the model weights of each layer follow a Gaussian distribution and thus detects outliers by tracking the mean and standard deviation of each layer. Outlier features are kept in their original form, while other values are divided into multiple bins, storing only the corresponding bin index and centroid value.\nFig. 14. The pseudocode for the GOBO algorithm. (Image source: Zadeh et al. 2020)\nBased on the observation that only certain activation layers in BERT (e.g., the residual connection after the FFN) cause large performance drops, Bondarenko et al. (2021) adopted mixed-precision quantization, using 16-bit quantization for problematic activations and 8-bit for others.\nLLM.int8() (Dettmers et al. 2022) achieves mixed-precision quantization through two mixed-precision decompositions:\nSince matrix multiplication consists of a series of independent inner products between row and column vectors, we can apply independent quantization to each inner product: each row and column is scaled by its absolute maximum value and then quantized to INT8. Outlier activation features (e.g., 20 times larger than other dimensions) are kept in FP16 format, but they only account for a small fraction of the total weights. How to identify outliers is empirical. Fig. 15. Two mixed-precision decompositions of LLM.int8(). (Image source: Dettmers et al. 2022)\nQuantization at fine-grained granularity Fig. 16. Comparison of quantization at different granularities. $d$ is the model size / hidden state dimension and $h$ is the number of heads in one MHSA (multi-head self-attention) component. (Image source: Lilian, 2023)\nSimply quantizing the entire weight matrix of a layer (“per-tensor” or “per-layer” quantization) is the easiest to implement but cannot achieve good quantization granularity.\nQ-BERT (Shen, et al. 2020) applies group-wise quantization to a fine-tuned BERT model, treating the individual matrix $W$ corresponding to each head in the MHSA (multi-head self-attention) as a group, and then applies Hessian-based mixed-precision quantization.\nPer-embedding group (PEG) (Bondarenko et al. 2021) activation quantization is motivated by the observation that outliers only appear in a few dimensions of the $d$ (hidden state/model size) dimension. Per-embedding quantization is quite computationally expensive. In contrast, PEG quantization divides the activation tensor into several uniformly sized groups along the embedding dimension, where elements in the same group share quantization parameters. To ensure that all outliers are assigned to the same group, they apply a deterministic range-based permutation of the embedding dimensions, where dimensions are sorted by their value range.\nZeroQuant (Yao et al. 2022) uses group-wise quantization for weights (same as Q-BERT) and token-wise quantization for activations. To avoid expensive quantization and dequantization computations, ZeroQuant builds custom kernels that fuse the quantization operation with its preceding operation.\nSecond-order information for quantization Q-BERT (Shen, et al. 2020) developed Hessian AWare Quantization (HAWQ) (Dong, et al. 2019) for its mixed-precision quantization. The motivation is that parameters with a higher Hessian spectrum (i.e., larger top eigenvalues) are more sensitive to quantization and thus require higher precision. This is essentially a method for identifying outliers.\nFrom another perspective, the quantization problem is an optimization problem. Given a weight matrix $\\mathbf{W}$ and an input matrix $\\mathbf{X}$, we want to find a quantized weight matrix $\\hat{\\mathbf{W}}$ that minimizes the mean squared error (MSE):\n$$ \\hat{\\mathbf{W}}^* = \\arg \\min_{\\hat{\\mathbf{W}}} |\\mathbf{W}\\mathbf{X} - \\hat{\\mathbf{W}}\\mathbf{X}| $$GPTQ (Frantar et al. 2022) builds on the OBC (Optimal Brain Compression) (Frantar et al. 2022) method, treating the weight matrix $\\mathbf{W}$ as a set of row vectors $\\mathbf{w}$ and quantizing each row independently. GPTQ iteratively quantizes more weights, which are chosen greedily to minimize the quantization error. The update for the selected weights has a closed-form solution that utilizes the Hessian matrix.\nFig. 17. The pseudocode for the GPTQ algorithm. (Image source: Frantar et al. 2022)\nGPTQ can reduce the bit-width of weights in OPT-175B to 3-bit or 4-bit without much performance loss, but it only applies to model weights, not activations.\nOutlier smoothing Fig. 18. Magnitude of the input activations and weights of a linear layer in OPT-13B before and after SmoothQuant (Image source: Xiao et al. 2022)\nFrom the figure above, we can see that in Transformer models, activations are harder to quantize than weights. There are three main characteristics:\nActivations are harder to quantize than weights: Quantizing weights to INT8/INT4 barely affects accuracy, but activations are more sensitive. Outliers amplify the difficulty of activation quantization: Extreme values in activations are about 100 times larger than normal values. Direct INT8 quantization would crush most small values to zero. Outliers are fixed in a few channels: These extreme values are consistently concentrated in specific channels, leading to a highly uneven distribution across channels. SmoothQuant (Xiao et al. 2022) proposes a clever solution by smoothing outlier features from activations to weights through a mathematically equivalent transformation, and then quantizing both weights and activations (W8A8). Therefore, SmoothQuant has better hardware efficiency than mixed-precision quantization.\nFig. 19. SmoothQuant migrates the scale variance from activations to weights offline to reduce the difficulty of activation quantization. Both the resulting new weight and activation matrices are easy to quantize. (Image source: Xiao et al. 2022)\nConsidering a per-channel smoothing factor $\\mathbf{s}$, SmoothQuant scales the weights according to the following formula:\n$$ \\mathbf{Y} = (\\mathbf{X}\\text{diag}(\\mathbf{s})^{-1}) \\cdot (\\text{diag}(\\mathbf{s})\\mathbf{W}) = \\hat{\\mathbf{X}}\\hat{\\mathbf{W}} $$The smoothing factor can be easily fused into the parameters of the preceding layer offline. A hyperparameter $\\alpha$ controls the degree to which the quantization difficulty is migrated from activations to weights: $\\mathbf{s} = \\max(|\\mathbf{X}_j|)^\\alpha / \\max(|\\mathbf{W}_j|)^{1-\\alpha}$. The paper finds that for many LLMs, $\\alpha=0.5$ is an optimal choice in experiments. For models with more significant outliers in activations, $\\alpha$ can be adjusted to be larger.\nQuantization-Aware Training (QAT) Quantization-Aware Training (QAT) integrates the quantization operation into the pre-training or fine-tuning process, directly learning a low-bit representation of the model weights. It achieves higher performance at the cost of additional training time and computational resources.\nCommon QAT methods include:\nDirect Fine-tuning: The model is first quantized once, and then further fine-tuned on the original pre-training dataset or a representative training dataset. This makes the model sensitive to quantization errors and allows it to actively compensate, thereby improving the performance of the quantized model. The training objective can be the same as the original pre-training objective (e.g., negative log-likelihood NLL or masked language modeling MLM in language models) or a task-specific objective (e.g., cross-entropy for classification tasks). A typical implementation is QLoRA, which achieves efficient fine-tuning by combining a low-bit (e.g., 4-bit) base model with full-precision LoRA adapters.\nKnowledge Distillation: A full-precision model acts as the teacher, and a low-precision model acts as the student. A distillation loss guides the student model to approach the teacher’s performance. The Layer-by-layer Knowledge Distillation (LKD) technique used by ZeroQuant (Yao et al. 2022) is an example of this method. It quantizes the model weights layer by layer, with each quantized layer using the corresponding full-precision layer as a teacher, minimizing the mean squared error (MSE) between their weight computation results to improve performance.\n$$ \\mathcal{L}_{L K D, k}=M S E\\left(L_k \\cdot L_{k-1} \\cdot L_{k-2} \\cdot \\ldots \\cdot L_1(\\boldsymbol{X})-\\widehat{L}_k \\cdot L_{k-1} \\cdot L_{k-2} \\cdot \\ldots \\cdot L_1(\\boldsymbol{X})\\right) $$Pruning Network pruning reduces model size by removing unimportant model weights or connections, thereby achieving model compression while maintaining performance as much as possible. Depending on the implementation, pruning can be divided into unstructured pruning and structured pruning.\nUnstructured pruning: Not limited to a specific pattern, it can discard weights or connections at any position in the network, thus disrupting the original structural regularity of the network. Because the resulting sparse patterns are difficult to adapt to modern hardware architectures, this method usually cannot effectively improve actual inference efficiency.\nStructured pruning: Maintains the network’s structure by trimming entire structures (such as convolutional kernels, channels, or layers). The pruned network is still suitable for dense matrix computations optimized by existing hardware, thus significantly improving actual inference performance. In this article, we focus on structured pruning to achieve efficient sparse structures in Transformer models.\nA typical workflow for network pruning includes the following three steps:\nTrain a full, dense network until convergence. Prune the network by removing redundant structures or weights. (Optional) Further fine-tune the network to recover the performance of the pruned model. The Lottery Ticket Hypothesis One of the theoretical foundations of pruning is the Lottery Ticket Hypothesis (LTH) (Frankle \u0026 Carbin, 2019). This hypothesis posits that a randomly initialized dense neural network contains certain sparse subnetworks (the “winning tickets”) that, when trained in isolation, can achieve performance comparable to or even better than the full network.\nThe core idea of LTH is that not all parameters are equally important. Only a small fraction of the parameters in a model play a crucial role. This suggests that a large number of parameters are not primarily for solving overfitting but mainly provide a sufficient initialization search space for high-performance subnetworks to be discovered.\nTo test this hypothesis, Frankle and Carbin proposed the following experimental steps:\nRandomly initialize a dense neural network with initial weights $\\theta_0$. Train the full network to achieve good performance, with final parameters $\\theta$. Prune the trained parameters $\\theta$ to generate a sparse mask $m$. Select the “winning ticket” subnetwork, with initial parameters defined as $m \\odot \\theta_0$. The experiment found that by using only the small number of “winning ticket” parameters selected in step 1 and training them from their original random initial values, the model could still achieve almost the same accuracy as the original network.\nThis result indicates that the vast initial parameter space is not necessary for the final deployed model but provides a large number of initial possibilities during the training phase, allowing the network to discover high-performing sparse structures. This also explains why, although a pruned model is significantly smaller, training the same sparse structure from scratch is often difficult to succeed.\nPruning Strategies Magnitude pruning is the simplest yet quite effective pruning method—weights with the smallest absolute values are pruned. In fact, some studies (Gale et al. 2019) have found that simple magnitude pruning methods can achieve comparable or better results than complex pruning methods like variational dropout (Molchanov et al. 2017) and $l_0$ regularization (Louizos et al. 2017). Magnitude pruning is easy to apply to large models and achieves fairly consistent performance across a wide range of hyperparameters.\nGradual Magnitude Pruning (GMP) (Zhu \u0026 Gupta, 2017) is based on the idea that large sparse models can achieve better performance than small but dense models. It proposes gradually increasing the sparsity of the network during training. At each training step, weights with the smallest absolute values are masked to zero to achieve a desired sparsity level $s$, and the masked weights do not receive gradient updates during backpropagation. The desired sparsity level $s$ increases with the number of training steps. The GMP process is sensitive to the learning rate schedule, which should be higher than that used in dense network training but not so high that it fails to converge.\nIterative pruning (Renda et al. 2020) iterates through step 2 (pruning) and step 3 (retraining) multiple times: in each iteration, only a small fraction of weights are pruned, and then the model is retrained. This process is repeated until the desired sparsity level is reached.\nRetraining The retraining step can be simple fine-tuning, using the same pre-training data or other task-specific datasets.\nThe Lottery Ticket Hypothesis proposes a retraining technique called weight rewinding: after pruning, the unpruned weights are re-initialized to their original values from early in training, and then retrained using the same learning rate schedule.\nLearning rate rewinding (Renda et al. 2020) only resets the learning rate to its early value, while the unpruned weights remain unchanged from the end of the previous training phase. They observed that (1) on various networks and datasets, retraining with weight rewinding is superior to retraining with fine-tuning; and (2) in all tested scenarios, learning rate rewinding is comparable or superior to weight rewinding.\nSparsity Sparsity is an effective way to scale model capacity while maintaining computational efficiency for model inference. Here we consider two types of sparsity for Transformers:\nSparsified dense layers, including self-attention and FFN layers. Sparse model architectures; i.e., by introducing Mixture-of-Experts (MoE) components. N:M Sparsity via Pruning N:M sparsity is a structured sparse pattern that works well with modern GPU hardware optimizations, where N out of every M consecutive elements are zero. For example, the Nvidia A100’s sparse tensor cores support 2:4 sparsity for faster inference.\nFig. 20. The illustration of achieving N:M structure sparsity. (Image source: Zhou et al. 2021)\nTo sparsify a dense neural network to follow an N:M structured sparse pattern, Nvidia recommends a three-step conventional workflow for training the pruned network: train -\u003e prune to meet 2:4 sparsity -\u003e retrain.\nPermutations can provide more options during pruning to preserve large-magnitude parameters or satisfy special constraints like N:M sparsity. The result of a matrix multiplication does not change as long as the paired axes of two matrices are permuted in the same order. For example:\n(1) Within a self-attention module, if the same permutation order is applied to axis 1 of the query embedding matrix $\\mathbf{Q}$ and axis 0 of the key embedding matrix $\\mathbf{K}^\\top$, the final matrix multiplication result of $\\mathbf{Q}\\mathbf{K}^\\top$ will remain unchanged.\nFig. 21. Illustration of the same permutation on $\\mathbf{Q}$ (axis 1) and $\\mathbf{K}^\\top$ (axis 0) to keep the results of a self-attention module unchanged. (Image source: Lilian, 2023)\n(2) Within an FFN layer containing two MLP layers and a ReLU non-linearity, we can permute axis 1 of the first linear weight matrix $\\mathbf{W}_1$ and axis 0 of the second linear weight matrix $\\mathbf{W}_2$ in the same order.\nFig. 22. Illustration of the same permutation on $\\mathbf{W}_1$ (axis 1) and $\\mathbf{W}_2$ (axis 0) to keep the FFN layer’s output unchanged. For simplicity, the bias terms are skipped but the same permutation should be applied to them too. (Image source: Lilian, 2023)\nTo enforce N:M structured sparsity, we divide the columns of a matrix into segments of M columns (called “stripes”). We can easily observe that the order of columns within each stripe and the order of the stripes themselves have no effect on the N:M sparsity constraint.\nChannel Permutations Channel Permutations (Pool \u0026 Yu, 2021) employs an iterative greedy algorithm to find the optimal permutation that maximizes the weight magnitude for N:M sparsity. All pairs of channels are speculatively swapped, and only the swap that results in the largest increase in magnitude is adopted, thus generating a new permutation and ending a single iteration. The greedy algorithm may only find a local optimum, so they introduce two techniques to escape local optima:\nBounded regressions: In practice, randomly swap two channels for a fixed number of times. The solution search is limited to a depth of one channel swap to keep the search space broad and shallow. Narrow, deep search: Select multiple stripes and optimize them simultaneously. Fig. 23. Algorithm for finding the best permutation for N:M sparsity greedily and iteratively. (Image source: Pool \u0026 Yu, 2021)\nIf the network is permuted before pruning, it can achieve better performance compared to pruning it in its default channel order.\nSTE \u0026 SR-STE The Straight-Through Estimator (STE) (Bengio et al. 2013) computes the gradient of the dense parameters with respect to the pruned network $\\widetilde{W}$, $\\partial\\mathcal{L}/\\partial\\widetilde{W}$, and applies it to the dense network $W$ as an approximation: $$ W_{t+1} \\leftarrow W_t - \\gamma \\frac{\\partial\\mathcal{L}}{\\partial\\widetilde{W}} $$Sparse-refined STE (SR-STE) (Zhou et al. 2021) extends the STE method to train a model with N:M sparsity from scratch. It is commonly used for backpropagation updates in model quantization and is adapted for magnitude pruning and sparse parameter updates. The dense weights $W$ are updated as follows: $$ W_{t+1} \\leftarrow W_t - \\gamma \\frac{\\partial\\mathcal{L}}{\\partial\\widetilde{W}} + \\lambda_W(\\overline{\\mathcal{E}} \\odot W_t) $$ where $\\overline{\\mathcal{E}}$ is the mask matrix of $\\widetilde{W}$, and $\\odot$ is element-wise multiplication. SR-STE aims to prevent large changes in the binary mask by (1) limiting the values of weights pruned in $\\widetilde{W}_t$, and (2) boosting the weights that are not pruned in $\\widetilde{W}_t$.\nFig. 24. Comparison of STE and SR-STE. $\\odot$ is element-wise product; $\\otimes$ is matrix multiplication. (Image source: Zhou et al. 2021)\nTop-KAST The Top-K Always Sparse Training (Top-KAST) (Jayakumar et al. 2021) method, unlike STE or SR-STE, can maintain constant sparsity in both the forward and backward passes without needing dense parameters or dense gradients for the forward pass.\nAt a training step $t$, Top-KAST proceeds as follows:\nSparse Forward Pass: Select a subset of parameters $A^t \\subset \\Theta$, containing the top $K$ parameters of each layer sorted by magnitude, limited to the top $D$ proportion of weights. In the parameterization $\\alpha^t$ at time $t$, if a parameter is not in $A^t$ (the active weights), its value is zero. $$ \\alpha_i^t = \\begin{cases} \\theta_i^t \u0026 \\text{if } i \\in A^t = \\{i \\mid \\theta_i^t \\in \\text{TopK}(\\theta^t, D)\\} \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$ where $\\text{TopK}(\\theta, x)$ selects the top $x$ proportion of weights from $\\theta$ based on magnitude.\nSparse Backward Pass: The gradient is then applied to a larger subset of parameters $B \\subset \\Theta$, where $B$ contains a $(D+M)$ proportion of weights and $A \\subset B$. Updating a larger proportion of weights allows for more effective exploration of different pruning masks, making it more likely to cause permutations in the top $D$ proportion of active weights. $$ \\Delta\\theta_i^t = \\begin{cases} -\\eta \\nabla_{\\alpha_t} \\mathcal{L}(y, x, \\alpha^t)_i \u0026 \\text{if } i \\in B^t = \\{i \\mid \\theta_i^t \\in \\text{TopK}(\\theta^t, D+M)\\} \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$ Training is divided into two phases, and the additional coordinates in the set $B \\setminus A$ control how much exploration is introduced. The amount of exploration is expected to decrease gradually during the training process, and the mask will eventually stabilize.\nFig. 25. The pruning mask of Top-KAST stabilizes in time. (Image source: Jayakumar et al. 2021)\nTo prevent the “rich get richer” phenomenon, Top-KAST penalizes the magnitude of active weights through an L2 regularization loss to encourage the exploration of new items. Parameters in $B \\setminus A$ are penalized more than those in $A$ to set a higher selection threshold for a stable mask during updates. $$ L_{\\text{penalty}}(\\alpha_i^t) = \\begin{cases} |\\theta_i^t| \u0026 \\text{if } i \\in A^t \\\\ |\\theta_i^t|/D \u0026 \\text{if } i \\in B^t \\setminus A^t \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$Sparsified Transformer Scaling Transformer (Jaszczur et al. 2021) sparsifies the self-attention and FFN layers in the Transformer architecture, achieving a 37x speedup in single-sample inference.\nFig. 26. Decoding speed of a single token for Terraformer with 17B parameters is 37x faster than a dense baseline model. (Image source: Jaszczur et al. 2021)\nSparse FFN Layer: Each FFN layer contains 2 MLPs and a ReLU. Because ReLU introduces a large number of zero values, they enforce a fixed structure on the activations, forcing only one non-zero value in a block of $N$ elements. The sparse pattern is dynamic and different for each token. $$ \\begin{aligned} Y_{\\text{sparse}} \u0026= \\max(0, xW_1 + b_1) \\odot \\text{Controller}(x) \\\\ \\text{SparseFFN}(x) \u0026= Y_{\\text{sparse}} W_2 + b_2 \\\\ \\text{Controller}(x) \u0026= \\arg\\max(\\text{Reshape}(xC_1C_2, (-1, N))) \\end{aligned} $$ where each activation in $Y_{\\text{sparse}}$ corresponds to a column in $W_1$ and a row in $W_2$. The controller is a low-rank bottleneck dense layer, $C_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{lowrank}}}, C_2 \\in \\mathbb{R}^{d_{\\text{lowrank}} \\times d_{\\text{ff}}}$ and $d_{\\text{lowrank}} = d_{\\text{model}}/N$. It uses arg max at inference time to select which columns should be non-zero, and the Gumbel-softmax trick during training. Because we can compute $\\text{Controller}(x)$ before loading the FFN weight matrices, we know which columns will be zeroed out and thus choose not to load them into memory to speed up inference.\nFig. 27. (a) Sparse FFN layer; columns in red are not loaded in memory for faster inference. (b) Sparse FFN controller for 1:4 sparsity. (Image source: Jaszczur et al. 2021)\nSparse QKV (Attention) Layer: In the attention layer, the dimension $d_{\\text{model}}$ is divided into $S$ modules, each of size $M = d_{\\text{model}}/S$. To ensure that each sub-part can access any part of the embedding, Scaling Transformer introduces a multiplicative layer (i.e., a layer that multiplies inputs from multiple neural network layers element-wise), which can represent any permutation but contains fewer parameters than a dense layer.\nGiven an input vector $x \\in \\mathbb{R}^{d_{\\text{model}}}$, the multiplicative layer outputs $y \\in \\mathbb{R}^{S \\times M}$: $$ y_{s,m} = \\sum_i x_i D_{i,s} E_{i,m} \\quad \\text{where } D \\in \\mathbb{R}^{d_{\\text{model}} \\times S}, E \\in \\mathbb{R}^{d_{\\text{model}} \\times M} $$ The output of the multiplicative layer is a tensor of size $\\mathbb{R}^{\\text{batch size} \\times \\text{length} \\times S \\times M}$. It is then processed by a 2D convolutional layer, where length and $S$ are treated as the height and width of an image. Such a convolutional layer further reduces the number of parameters and computation time of the attention layer.\nFig. 28. (a) A multiplicative layer is introduced to enable partitions to access any part of an embedding. (b) Combination of multiplicative dense layer and 2-D convolutional layer reduces the number of parameters and computation time of the attention layer. (Image source: Jaszczur et al. 2021)\nTo better handle long sequences, Scaling Transformer is further equipped with LSH (Locality-Sensitive Hashing) attention and FFN block recurrence from Reformer (Kitaev, et al. 2020).\nMixture of Experts Mixture-of-Experts (MoE) models consist of multiple “expert” networks, where each input sample activates only a subset of these experts for computation.\nFig. 29. Dense Transformer vs Sparse Expert Transformer. (Image source: Fedus et al. 2022)\nDense Model: All input tokens are processed using the same feed-forward network (FFN) parameters. While the structure is simple and easy to train, its computational cost increases rapidly as the model size grows.\nSparse Expert Model: Each input token is independently routed to a few experts among many for processing. This sparse routing mechanism allows the model to have more unique parameters without a significant increase in overall computational cost, thus improving parameter efficiency and scalability, and effectively reducing the computational cost during inference.\nRouting Strategy Improvements The MoE layer has a routing network that assigns a subset of experts to each input token. In traditional MoE models, the routing strategy routes each token to its preferred expert in the order they appear in the natural sequence. If a token is routed to an expert that has already reached its capacity, the token is marked as “overflowed” and skipped.\nVision MoE (V-MoE) (Riquelme et al. 2021) adds MoE layers to the ViT (Vision Transformer). It achieves previous SOTA performance with only half the inference computation. V-MoE can be scaled up to 15B parameters. Their experiments used $k=2$, 32 experts, and placed an expert layer every 2 layers (meaning MoE was placed in every other layer).\nDue to the limited capacity of each expert, some important and informative tokens might be dropped if they appear too late (e.g., the order of words in a sentence, or the order of image patches). To avoid this drawback of the traditional routing scheme, V-MoE employs Batch Priority Routing (BPR), which first assigns experts to tokens with high priority. BPR computes a priority score for each token before expert assignment (the maximum or sum of the top-k router scores) and changes the order of tokens accordingly. This ensures that the expert capacity buffer will be filled with critical tokens first.\nFig. 30. How image patches are discarded according to priority scores when $C \u003c 1$. (Image source: Riquelme et al. 2021)\nWhen $C \\le 0.5$, BPR performs much better than traditional routing, as the model starts to drop a large number of tokens. It allows the model to compete with dense networks even at fairly low capacities.\nWhen studying how to interpret the association between image classes and experts, they observed that early MoE layers are more general, while later MoE layers may specialize in a few image classes.\nTask MoE (Task-level Mixture-of-Experts) (Kudugunta et al. 2021) considers task information and routes tokens at the task level rather than the word or token level in machine translation. They use MNMT (Multilingual Neural Machine Translation) as an example and group translation tasks based on the target language or language pair.\nToken-level routing is dynamic, with routing decisions made independently for each token. Therefore, at inference time, the server needs to pre-load all experts. In contrast, task-level routing is static for a given fixed task, so an inference server for a task only needs to pre-load $k$ experts (assuming top-k routing). According to their experiments, Task MoE can achieve similar performance gains as Token MoE compared to a dense model baseline, with 2.6x higher peak throughput and only 1.6% of the decoder size.\nTask-level MoE essentially classifies the task distribution based on predefined heuristic rules and incorporates this human knowledge into the router. When such heuristic rules do not exist (e.g., for a general sentence completion task), how to utilize Task MoE is less straightforward.\nPR-MoE (Pyramid residual MoE) (Rajbhandari et al. 2022) has each token pass through a fixed MLP and a selected expert. Observing that MoE is more beneficial in later layers, PR-MoE employs more experts in the later layers. The DeepSpeed library implements a flexible multi-expert, multi-data parallel system to support training PR-MoE with varying numbers of experts.\nFig. 31. Illustration of PR-MoE architecture in comparison with a standard MoE. (Image source: Rajbhandari et al. 2022)\nKernel Improvement Expert networks can be hosted on different devices. However, as the number of GPUs increases, the number of experts per GPU decreases, and the communication between experts (“All-to-all”) becomes more expensive. All-to-all communication between experts across multiple GPUs relies on NCCL’s P2P API, which cannot saturate the bandwidth of high-speed links (like NVLink, HDR InfiniBand) at a large scale because individual data blocks become smaller as more nodes are used. Existing all-to-all algorithms perform poorly in large-scale scenarios with small workloads. There are several kernel improvements that enable more efficient MoE computation, such as making all-to-all communication cheaper/faster.\nDeepSpeed (Rajbhandari et al. 2022) and TUTEL (Hwang et al. 2022) both implement a tree-based hierarchical all-to-all algorithm that first runs an intra-node all-to-all, followed by an inter-node all-to-all. It reduces the number of communication hops from $O(G)$ to $O(G_{\\text{node}} + G/G_{\\text{node}})$, where $G$ is the total number of GPU nodes and $G_{\\text{node}}$ is the number of GPU cores per node. Although the communication volume is doubled in this implementation, it achieves better scalability in small-batch, large-scale scenarios because the bottleneck is latency rather than communication bandwidth.\nDynaMoE (Kossmann et al. 2022) uses dynamic recompilation to adapt computational resources to the dynamic workload among experts. The RECOMPILE mechanism compiles the computation graph from scratch and reallocates resources only when needed. It measures the number of samples assigned to each expert and dynamically adjusts their capacity factor $C$ to reduce memory and computational requirements at runtime. Based on the observation that sample-expert assignments converge early in training, a sample assignment cache is introduced after convergence, and then RECOMPILE is used to eliminate dependencies between the gating network and the experts.\nArchitectural Optimization Efficient Transformers The survey paper Efficient Transformers (Tay et al. 2020) reviews a series of Transformer architectures with improvements in computational and memory efficiency. Readers interested in this topic can read the original paper.\nFig. 32. Categorization of efficient transformer models. (Image source: Tay et al. 2020)\nKV Cache Optimization Multi-Query Attention (MQA) \u0026 Grouped-Query Attention (GQA): In standard Multi-Head Attention (MHA), each head has its own set of Key and Value projection matrices. MQA (Shazeer, 2019) proposed having all Query heads share the same set of Key and Value heads, which greatly reduces the size of the KV Cache. GQA (Ainslie et al., 2023) is a compromise between MHA and MQA, grouping query heads so that heads within a group share a set of K/V, achieving a good balance between performance and effectiveness.\nvLLM(Kwon et al., 2023) introduced PagedAttention, inspired by virtual memory and paging in operating systems. It divides the KV Cache into fixed-size blocks, which can be stored non-contiguously in physical VRAM. A “block table” manages the mapping from logical blocks to physical blocks. For a detailed introduction to vLLM, you can refer to my previous blog post vLLM: High-Throughput and Memory-Efficient LLM Serving Engine. This method almost completely eliminates memory fragmentation (both internal and external), bringing VRAM utilization close to 100%. More importantly, through a Copy-on-Write mechanism, it can efficiently share the KV Cache across requests, greatly increasing throughput for complex decoding scenarios like parallel sampling and Beam Search.\nFlashAttention FlashAttention(Dao et al., 2022) is an IO-aware exact attention algorithm. It recognizes that the main bottleneck in standard Attention implementations is the data transfer between GPU HBM (High-Bandwidth Memory) and SRAM (on-chip high-speed cache). FlashAttention uses Tiling and Recomputation techniques to fuse the entire Attention computation into a single CUDA kernel, avoiding the need to write and read the huge $N \\times N$ attention matrix to and from HBM. This dramatically reduces memory access, thereby speeding up Attention computation by several times without sacrificing accuracy. FlashAttention-2(Dao, 2023) further optimizes parallelism and hardware utilization. Fig. 33. FlashAttention uses tiling to avoid materializing the large N × N attention matrix on slow GPU HBM, achieving up to 7.6× speedup over PyTorch. (Image source: Dao et al., 2022)\nReferences [1] Zhou, Zixuan, et al. “A survey on efficient inference for large language models.” arXiv preprint arXiv:2404.14294 (2024).\n[2] Zhang, Aston, et al. “Dive into Deep Learning.”. Cambridge University Press, 2023.\n[3] Big Hummingbird Blogs. (2024). “A Visual Explanation of LLM Hyperparameters.” Blog post.\n[4] Fan, Angela, Mike Lewis, and Yann Dauphin. “Hierarchical neural story generation.” arXiv preprint arXiv:1805.04833 (2018). int arXiv:1805.04832.\n[5] Holtzman, Ari, et al. “The curious case of neural text degeneration.” arXiv preprint arXiv:1904.09751 (2019).\n[6] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. “Fast inference from transformers via speculative decoding.” International Conference on Machine Learning. PMLR, 2023.\n[7] Liu, Xiaoxuan, et al. “Online speculative decoding.” arXiv preprint arXiv:2310.07177 (2023).\n[8] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015).\n[9] Gou, Jianping, et al. “Knowledge distillation: A survey.” International Journal of Computer Vision 129.6 (2021): 1789-1819.\n[10] Sanh, Victor, et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.” arXiv preprint arXiv:1910.01108 (2019).\n[11] Raschka, S. (2023). “Accelerating Large Language Models with Mixed-Precision Techniques.” Blog post.\n[12] Bondarenko, Yelysei, Markus Nagel, and Tijmen Blankevoort. “Understanding and overcoming the challenges of efficient transformer quantization.” arXiv preprint arXiv:2109.12948 (2021).\n[13] Zhang, S., Roller, S., Goyal, N., et al. (2022). “OPT: Open pre-trained transformer language models.” arXiv preprint arXiv:2205.01068.\n[14] Dettmers, T., et al. (2022). “LLM.int8(): 8-bit matrix multiplication for transformers at scale.” arXiv preprint arXiv:2208.07339.\n[15] Zadeh, V. K., et al. (2020). “GOBO: Quantizing attention-based NLP models for low latency and energy efficient inference.” arXiv preprint arXiv:2005.03842.\n[16] Weng, L. (2023). “Large Transformer Model Inference Optimization.” Lil’Log blog post.\n[17] Shen, Z., Dong, Z., Ye, J., et al. (2020). “Q-BERT: Hessian-based ultra-low-precision quantization of BERT.” arXiv preprint arXiv:1909.05840.\n[18] Dong, Z., Yao, Z., Gholami, A., et al. (2019). “HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision” arXiv preprint arXiv:1905.03696.\n[19] Yao, Z., et al. (2022). “ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers.” arXiv preprint arXiv:2206.01861.\n[20] Frantar, E., et al. (2022). “GPTQ: Accurate post-training quantization for generative pre-trained transformers.” arXiv preprint arXiv:2210.17323.\n[21] Xiao, G., \u0026 Lin, J. (2022). “SmoothQuant: Accurate and efficient post-training quantization for large language models.” arXiv preprint arXiv:2211.10438.\n[22] Frankle, J., \u0026 Carbin, M. (2019). “The lottery ticket hypothesis: Finding sparse, trainable neural networks.” arXiv preprint arXiv:1803.03635.\n[23] Gale, T., Elsen, E., \u0026 Hooker, S. (2019). “The state of sparsity in deep neural networks.” arXiv preprint arXiv:1902.09574.\n[24] Molchanov, D., Ashukha, A., \u0026 Vetrov, D. (2017). “Variational dropout sparsifies deep neural networks.” arXiv preprint arXiv:1701.05369.\n[25] Louizos, Christos, Max Welling, and Diederik P. Kingma. “Learning sparse neural networks through $ L_0 $ regularization.” arXiv preprint arXiv:1712.01312 (2017).\n[26] Zhu, M., \u0026 Gupta, S. (2017). “To prune, or not to prune: Exploring the efficacy of pruning for model compression.” arXiv preprint arXiv:1710.01878.\n[27] Renda, A., Frankle, J., \u0026 Carbin, M. (2020). “Comparing rewinding and fine-tuning in neural network pruning.” arXiv preprint arXiv:2003.02389.\n[28] Nvidia. (2020). “NVIDIA A100 Tensor Core GPU.” Nvidia Blog.\n[29] Zhou, A., \u0026 Ma, X. (2021). “Learning N:M fine-grained structured sparse neural networks from scratch.” arXiv preprint arXiv:2102.04010.\n[30] Pool, J., \u0026 Yu, F. (2021). “Channel permutations for N:M structured sparsity.” Advances in Neural Information Processing Systems 34.\n[31] Bengio, Y., Léonard, N., \u0026 Courville, A. (2013). “Estimating or propagating gradients through stochastic neurons for conditional computation.” arXiv preprint arXiv:1308.3432.\n[32] Jayakumar, S. M., Pascanu, R., Rae, J., et al. (2021). “Top-KAST: Top-K always sparse training.” arXiv preprint arXiv:2106.03517.\n[33] Jaszczur, S., et al. (2021). “Sparse is enough in scaling transformers.” Advances in Neural Information Processing Systems 34.\n[34] Kitaev, N., Kaiser, Ł., \u0026 Levskaya, A. (2020). “Reformer: The efficient transformer.” arXiv preprint arXiv:2001.04451.\n[35] Fedus, W., et al. (2022). “A review of sparse expert models in deep learning.” arXiv preprint arXiv:2209.01667.\n[36] Riquelme, C., et al. (2021). “Scaling vision with sparse mixture of experts.” Advances in Neural Information Processing Systems 34: 8583-8595.\n[37] Kudugunta, S., Lepikhin, D., Heafield, K., et al. (2021). “Beyond domain adaptation: Multi-task mixture-of-experts for zero-shot generalization.” arXiv preprint arXiv:2110.03742.\n[38] Rajbhandari, S., et al. (2022). “DeepSpeed-MoE: Advancing mixture-of-experts inference and training to power next-generation AI scale.” arXiv preprint arXiv:2201.05596.\n[39] Hwang, I., et al. (2022). “Tutel: Adaptive mixture-of-experts at scale.” arXiv preprint arXiv:2206.03382.\n[40] Kossmann, F., et al. (2022). “Optimizing Mixture of Experts using Dynamic Recompilations” arXiv preprint arXiv:2205.01848.\n[41] Tay, Y., et al. (2020). “Efficient transformers: A survey.” arXiv preprint arXiv:2009.06732.\n[42] Shazeer, N. (2019). “Fast transformer decoding: One write-head is all you need.” arXiv preprint arXiv:1911.02150.\n[43] Ainslie, J., et al. (2023). “GQA: Training generalized multi-query transformer models from multi-head checkpoints.” arXiv preprint arXiv:2305.13245.\n[44] Kwon, W., et al. (2023). “Efficient memory management for large language model serving with PagedAttention.” Proceedings of the 29th Symposium on Operating Systems Principles.\n[45] Dao, T., et al. (2022). “FlashAttention: Fast and memory-efficient exact attention with IO-awareness.” Advances in Neural Information Processing Systems 35: 16344-16359.\n[46] Dao, T. (2023). “FlashAttention-2: Faster attention with better parallelism and work partitioning.” arXiv preprint arXiv:2307.08691.\n[47] Pope, R., et al. (2022). “Efficiently scaling transformer inference.” arXiv preprint arXiv:2211.05102.\n[48] von Platen, P. (2020). “How to generate text: Using different decoding methods for language generation with Transformers.” Hugging Face Blog.\nCitation Citation: When reproducing or citing the content of this article, please credit the original author and source.\nCited as:\nYue Shui. (Jun 2025). Large Language Model Inference. https://syhya.github.io/posts/2025-06-29-llm-inference\nOr\n@article{syhya2025llminferencesurvey, title = \"Large Language Model Inference\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Jun\", url = \"https://syhya.github.io/posts/2025-06-29-llm-inference\" } ","wordCount":"9025","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-06-29T12:00:00+08:00","dateModified":"2025-09-01T18:02:26+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-06-29-llm-inference/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Large Language Model Inference</h1><div class=post-meta><span title='2025-06-29 12:00:00 +0800 +0800'>2025-06-29</span>&nbsp;·&nbsp;43 min&nbsp;·&nbsp;9025 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-06-29-llm-inference/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#principles-of-token-generation>Principles of Token Generation</a><ul><li><a href=#autoregressive-generation>Autoregressive Generation</a></li><li><a href=#prefilling-and-decoding>Prefilling and Decoding</a></li><li><a href=#decoding-strategies>Decoding Strategies</a><ul><li><a href=#greedy-search>Greedy Search</a></li><li><a href=#beam-search>Beam Search</a></li><li><a href=#temperature-sampling>Temperature Sampling</a></li><li><a href=#top-k-sampling>Top-K Sampling</a></li><li><a href=#top-p-nucleus-sampling>Top-p (Nucleus) Sampling</a></li><li><a href=#speculative-decoding>Speculative Decoding</a></li><li><a href=#heuristic-strategies>Heuristic Strategies</a></li></ul></li></ul></li><li><a href=#overview-of-optimization-methods>Overview of Optimization Methods</a></li><li><a href=#knowledge-distillation>Knowledge Distillation</a></li><li><a href=#quantization>Quantization</a><ul><li><a href=#precision-comparison>Precision Comparison</a></li><li><a href=#challenges-in-transformer-quantization>Challenges in Transformer Quantization</a></li><li><a href=#post-training-quantization-ptq>Post-Training Quantization (PTQ)</a><ul><li><a href=#mixed-precision-quantization>Mixed-precision quantization</a></li><li><a href=#quantization-at-fine-grained-granularity>Quantization at fine-grained granularity</a></li><li><a href=#second-order-information-for-quantization>Second-order information for quantization</a></li><li><a href=#outlier-smoothing>Outlier smoothing</a></li></ul></li><li><a href=#quantization-aware-training-qat>Quantization-Aware Training (QAT)</a></li></ul></li><li><a href=#pruning>Pruning</a><ul><li><a href=#the-lottery-ticket-hypothesis>The Lottery Ticket Hypothesis</a></li><li><a href=#pruning-strategies>Pruning Strategies</a></li><li><a href=#retraining>Retraining</a></li></ul></li><li><a href=#sparsity>Sparsity</a><ul><li><a href=#nm-sparsity-via-pruning>N:M Sparsity via Pruning</a></li><li><a href=#channel-permutations>Channel Permutations</a></li><li><a href=#ste--sr-ste>STE & SR-STE</a></li><li><a href=#top-kast>Top-KAST</a></li><li><a href=#sparsified-transformer>Sparsified Transformer</a></li><li><a href=#mixture-of-experts>Mixture of Experts</a><ul><li><a href=#routing-strategy-improvements>Routing Strategy Improvements</a></li><li><a href=#kernel-improvement>Kernel Improvement</a></li></ul></li></ul></li><li><a href=#architectural-optimization>Architectural Optimization</a><ul><li><a href=#efficient-transformers>Efficient Transformers</a></li><li><a href=#kv-cache-optimization>KV Cache Optimization</a></li><li><a href=#flashattention>FlashAttention</a></li><li><a href=#references>References</a></li></ul></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><p>In recent years, Large Language Models (LLMs) have achieved revolutionary breakthroughs in fields such as natural language processing, code generation, and even multimodal interaction. However, the powerful capabilities of these models come at the cost of enormous computational and memory overhead, especially during the inference stage. Efficiently deploying and running these models, which have billions or even trillions of parameters, has become a core challenge in scaling LLM technology for real-world applications.</p><p>The challenges of LLM inference primarily stem from two aspects:</p><ol><li><strong>Huge Memory Footprint</strong>: In addition to the model parameters themselves, the inference process requires storing a large amount of intermediate state, especially the <strong>KV Cache</strong>. For example, for a request with a batch size of 512 and a sequence length of 2048, its KV cache can be as large as 3TB, several times the size of the model itself. Furthermore, the computational complexity of self-attention grows quadratically with the sequence length.</li><li><strong>Low parallelizability</strong>: The text generation of LLMs is inherently an <strong>autoregressive</strong> process, meaning tokens are generated one by one, with the generation of the next token depending on all previously generated tokens. This serial nature makes the decoding process difficult to parallelize efficiently.</li></ol><h2 id=principles-of-token-generation>Principles of Token Generation<a hidden class=anchor aria-hidden=true href=#principles-of-token-generation>#</a></h2><p>To better understand the optimization techniques that follow, we first need to understand how large models generate text and identify the key bottlenecks in their inference process.</p><h3 id=autoregressive-generation>Autoregressive Generation<a hidden class=anchor aria-hidden=true href=#autoregressive-generation>#</a></h3><p>Mainstream large language models like GPT use a Decoder-Only Transformer architecture and generate text in an autoregressive manner. The basic idea is that the probability of a text sequence can be decomposed into a product of a series of conditional probabilities. Given an initial context word sequence $W_0$ (usually the user&rsquo;s input prompt), the model predicts the next word (token) one at a time, adding the newly generated word to the context as input for the next prediction step. This process can be represented by the following formula:</p>$$
P(w_{1:T} | W_0) = \prod_{t=1}^{T} P(w_t | w_{1:t-1}, W_0), \text{ with } w_{1:0} = \emptyset
$$<p>where $w_t$ is the word generated at time step $t$, and $w_{1:t-1}$ is the sequence of all words generated before time step $t$. The generation process continues until the model produces a special end-of-sequence (EOS) token or reaches a predefined maximum length $T$.</p><h3 id=prefilling-and-decoding>Prefilling and Decoding<a hidden class=anchor aria-hidden=true href=#prefilling-and-decoding>#</a></h3><p>The autoregressive nature of generation dictates that LLM inference can be clearly divided into two stages: the <strong>Prefilling stage</strong> and the <strong>Decoding stage</strong>.</p><figure class=align-center><img loading=lazy src=prefilling_decoding.png#center alt="Fig. 1. The Prefilling and Decoding Stages of LLM Inference. (Image source: Zhou et al., 2024)" width=100%><figcaption><p>Fig. 1. The Prefilling and Decoding Stages of LLM Inference. (Image source: <a href=https://arxiv.org/abs/2404.14294>Zhou et al., 2024</a>)</p></figcaption></figure><ol><li><p><strong>Prefilling Stage</strong>: In this stage, the model processes the entire input prompt in parallel (e.g., &ldquo;I, like, natural, language&rdquo; in Figure 1) and computes the probability distribution for the first output token (&ldquo;Processing&rdquo;). This stage is characterized by <strong>high parallelism</strong>, as all input tokens can be fed into the Transformer model at once. This allows compute-intensive operations (like matrix multiplication) to fully utilize the parallel processing power of GPUs, making it <strong>compute-bound</strong>.</p></li><li><p><strong>Decoding Stage</strong>: In this stage, the model generates subsequent tokens one by one. Each time a token is generated, it is appended to the end of the existing sequence and used as input for the next prediction. This process is <strong>serial</strong> because the generation of the next token depends on the previous one. Consequently, this stage is <strong>memory-bound</strong>, with the main bottleneck being the loading of the massive model weights from GPU memory, rather than the computation itself.</p></li></ol><figure class=align-center><img loading=lazy src=inference_memory.png#center alt="Fig. 2. Illustration of the memory variation through time (latency) during one generation process. Note that the author ignores the activation size in this figure for simplification. (Image source: Zhou et al., 2024)" width=100%><figcaption><p>Fig. 2. Illustration of the memory variation through time (latency) during one generation process. Note that the author ignores the activation size in this figure for simplification. (Image source: <a href=https://arxiv.org/abs/2404.14294>Zhou et al., 2024</a>)</p></figcaption></figure><p>To accelerate the decoding process, modern LLM inference frameworks widely adopt the <strong>KV Cache</strong> technique. In the Transformer&rsquo;s self-attention mechanism, each token needs to interact with all preceding tokens. To avoid recomputing the Key (K) and Value (V) vectors for all previous tokens when generating each new token, the system caches these computed K and V values. This cache is the KV Cache.</p><p>As shown in Figure 2, the size of the KV Cache grows linearly as the generated sequence lengthens. For a model with billions of parameters and long sequences, the KV Cache can occupy several gigabytes or even tens of gigabytes of VRAM. This makes VRAM the scarcest resource in LLM inference, severely limiting the number of requests the system can handle simultaneously (i.e., the batch size), which directly impacts inference throughput. Therefore, <strong>how to efficiently manage and optimize the KV Cache is one of the core problems in LLM inference optimization</strong>.</p><h3 id=decoding-strategies>Decoding Strategies<a hidden class=anchor aria-hidden=true href=#decoding-strategies>#</a></h3><p>At each decoding step, the model outputs a probability distribution over the entire vocabulary. The method used to select the next token from this distribution is determined by the decoding strategy (or token generation strategy). Different strategies significantly affect the quality, creativity, and coherence of the generated text.</p><h4 id=greedy-search>Greedy Search<a hidden class=anchor aria-hidden=true href=#greedy-search>#</a></h4><p>Greedy search is the simplest decoding strategy. At each time step $t$, it always selects the token with the highest probability as the output:</p>$$
w_t = \underset{w}{\operatorname{argmax}} P(w | w_{1:t-1})
$$<p>This method greatly reduces computational complexity and produces results quickly, but it has clear limitations. Because it only makes locally optimal choices at each step, greedy search can easily get stuck in local optima, overlooking globally better possibilities. This often results in generated text that is dull, repetitive, and lacks diversity and creativity.</p><figure class=align-center><img loading=lazy src=greedy_search.svg#center alt="Fig. 3. At each time step, greedy search selects the token with the highest conditional probability. (Image source: d2l-en, 2019)" width=50%><figcaption><p>Fig. 3. At each time step, greedy search selects the token with the highest conditional probability. (Image source: <a href=https://d2l.ai/chapter_recurrent-modern/beam-search.html#id1>d2l-en, 2019</a>)</p></figcaption></figure><p><strong>Code Implementation</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>greedy_search</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>max_len</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>eos_token_id</span><span class=o>=</span><span class=mi>2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    A simple implementation of Greedy Search.
</span></span></span><span class=line><span class=cl><span class=s2>    `model` should be a function that takes input_ids and returns logits.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>generated_sequence</span> <span class=o>=</span> <span class=n>input_ids</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Get logits for the last token</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>generated_sequence</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>next_token_logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Select the token with the highest probability</span>
</span></span><span class=line><span class=cl>        <span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>next_token_logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Append the new token to the sequence</span>
</span></span><span class=line><span class=cl>        <span class=n>generated_sequence</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>generated_sequence</span><span class=p>,</span> <span class=n>next_token</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Stop if EOS token is generated</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>next_token</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>==</span> <span class=n>eos_token_id</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>break</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>generated_sequence</span>
</span></span></code></pre></div><h4 id=beam-search>Beam Search<a hidden class=anchor aria-hidden=true href=#beam-search>#</a></h4><p>To overcome the local optima problem of greedy search, beam search maintains $k$ (<code>num_beams</code> or beam width) most likely candidate sequences (called &ldquo;beams&rdquo;) at each decoding step. In the next step, it expands based on these $k$ sequences and again selects the $k$ new sequences with the highest overall probability. Finally, the algorithm selects the candidate sequence with the highest overall probability from all completed sequences as the final output.</p><figure class=align-center><img loading=lazy src=beam_search.svg#center alt="Fig. 4. The process of beam search (beam size $=2$; maximum length of an output sequence $=3$ ). The candidate output sequences are $A, C, A B, C E, A B D$, and $C E D$. (Image source: d2l-en, 2019)" width=100%><figcaption><p>Fig. 4. The process of beam search (beam size $=2$; maximum length of an output sequence $=3$ ). The candidate output sequences are $A, C, A B, C E, A B D$, and $C E D$. (Image source: <a href=https://d2l.ai/chapter_recurrent-modern/beam-search.html#id1>d2l-en, 2019</a>)</p></figcaption></figure><p>This approach expands the search space, effectively reducing the impact of local optima and typically generating higher-quality, more coherent text. However, the essence of beam search is still to choose the path with the highest overall probability, which makes it prone to producing high-frequency, common expressions in open-ended generation tasks, potentially lacking creativity and diverse output.</p><h4 id=temperature-sampling>Temperature Sampling<a hidden class=anchor aria-hidden=true href=#temperature-sampling>#</a></h4><figure class=align-center><img loading=lazy src=temperature.png#center alt="Fig. 5. Illustration of Temperature Sampling. (Image source: Big Hummingbird Blogs, 2024)" width=80%><figcaption><p>Fig. 5. Illustration of Temperature Sampling. (Image source: <a href=https://www.bighummingbird.com/blogs/llm-hyperparameter>Big Hummingbird Blogs, 2024</a>)</p></figcaption></figure><p>Unlike deterministic search methods, sampling methods introduce randomness, making the generated text more diverse and creative. The most basic sampling method is to sample directly from the model&rsquo;s probability distribution. <strong>Temperature sampling</strong> adjusts the shape of the original probability distribution using a temperature coefficient $T$, applied to the Softmax function. The temperature coefficient controls the flatness of the token probability distribution output by the LLM. A higher temperature makes the distribution flatter and the output more random, while a lower temperature makes the distribution more extreme and the output more stable.</p>$$
P_T(w_i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
$$<p>where $z_i$ is the logit output by the model for word $w_i$.</p><ul><li>When $T \to 1$, the probability distribution remains unchanged.</li><li>When $T &lt; 1$ (cooling), the distribution becomes &ldquo;sharper,&rdquo; making high-probability words more likely to be selected, and the generated result is closer to greedy search.</li><li>When $T > 1$ (heating), the distribution becomes &ldquo;flatter,&rdquo; giving low-probability words a chance to be selected, resulting in more diverse and random output.</li></ul><h4 id=top-k-sampling>Top-K Sampling<a hidden class=anchor aria-hidden=true href=#top-k-sampling>#</a></h4><p><strong>Top-K sampling</strong> (<a href=https://arxiv.org/abs/1805.04833>Fan et al., 2018</a>) retains only the $K$ most probable candidate words before sampling, then renormalizes and samples from this set of $K$ words. This effectively prevents the model from sampling from extremely low-probability words, avoiding the generation of incoherent text. However, its drawback is that the value of $K$ is fixed and cannot adapt dynamically to different probability distributions.</p><figure class=align-center><img loading=lazy src=top_k.png#center alt="Fig. 6. Illustration of Top-K Sampling. (Image source: Big Hummingbird Blogs, 2024)" width=80%><figcaption><p>Fig. 6. Illustration of Top-K Sampling. (Image source: <a href=https://www.bighummingbird.com/blogs/llm-hyperparameter>Big Hummingbird Blogs, 2024</a>)</p></figcaption></figure><h4 id=top-p-nucleus-sampling>Top-p (Nucleus) Sampling<a hidden class=anchor aria-hidden=true href=#top-p-nucleus-sampling>#</a></h4><p><strong>Top-p sampling</strong> (<a href=https://arxiv.org/abs/1904.09751>Holtzman et al., 2019</a>) uses a dynamic method to select the set of candidate words. It starts with the most probable word and accumulates their probabilities until the sum exceeds a preset threshold $p$ (e.g., 0.9). The model then samples only from this dynamically generated, minimal set of candidate words $V_{\text{top-p}}$. This method balances text coherence and creativity and is currently one of the most commonly used and effective strategies for open-ended text generation.</p><figure class=align-center><img loading=lazy src=top_p.png#center alt="Fig. 7. Illustration of Top-p Sampling. (Image source: Big Hummingbird Blogs, 2024)" width=80%><figcaption><p>Fig. 7. Illustration of Top-p Sampling. (Image source: <a href=https://www.bighummingbird.com/blogs/llm-hyperparameter>Big Hummingbird Blogs, 2024</a>)</p></figcaption></figure><p><strong>Combined Sampling Code Implementation (Top-K, Top-p, Temperature)</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@torch.no_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_with_sampling</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>top_p</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>eos_token_id</span><span class=o>=</span><span class=mi>2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_new_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Crop context if it&#39;s too long</span>
</span></span><span class=line><span class=cl>        <span class=n>idx_cond</span> <span class=o>=</span> <span class=n>idx</span> <span class=k>if</span> <span class=n>idx</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>max_position_embeddings</span> <span class=k>else</span> <span class=n>idx</span><span class=p>[:,</span> <span class=o>-</span><span class=n>model</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>max_position_embeddings</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Forward pass to get logits</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>idx_cond</span><span class=p>)</span><span class=o>.</span><span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Apply temperature</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>temperature</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span> <span class=o>/</span> <span class=n>temperature</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Apply Top-K filtering</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>top_k</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>v</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>topk</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=nb>min</span><span class=p>(</span><span class=n>top_k</span><span class=p>,</span> <span class=n>logits</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span><span class=p>[</span><span class=n>logits</span> <span class=o>&lt;</span> <span class=n>v</span><span class=p>[:,</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]]]</span> <span class=o>=</span> <span class=o>-</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;Inf&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>        <span class=c1># Apply Top-p (Nucleus) filtering</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>top_p</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>sorted_logits</span><span class=p>,</span> <span class=n>sorted_indices</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>descending</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>cumulative_probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>sorted_logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>            <span class=c1># Remove tokens with cumulative probability above the threshold</span>
</span></span><span class=line><span class=cl>            <span class=n>sorted_indices_to_remove</span> <span class=o>=</span> <span class=n>cumulative_probs</span> <span class=o>&gt;</span> <span class=n>top_p</span>
</span></span><span class=line><span class=cl>            <span class=c1># Shift the indices to the right to keep the first token above the threshold</span>
</span></span><span class=line><span class=cl>            <span class=n>sorted_indices_to_remove</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=mi>1</span><span class=p>:]</span> <span class=o>=</span> <span class=n>sorted_indices_to_remove</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>sorted_indices_to_remove</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>            <span class=n>indices_to_remove</span> <span class=o>=</span> <span class=n>sorted_indices</span><span class=p>[</span><span class=n>sorted_indices_to_remove</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span><span class=o>.</span><span class=n>scatter_</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>indices_to_remove</span><span class=p>,</span> <span class=o>-</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;Inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Convert logits to probabilities and sample</span>
</span></span><span class=line><span class=cl>        <span class=n>probs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>idx_next</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Append sampled index and check for EOS</span>
</span></span><span class=line><span class=cl>        <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>idx</span><span class=p>,</span> <span class=n>idx_next</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>idx_next</span><span class=o>.</span><span class=n>item</span><span class=p>()</span> <span class=o>==</span> <span class=n>eos_token_id</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>break</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>idx</span>
</span></span></code></pre></div><h4 id=speculative-decoding>Speculative Decoding<a hidden class=anchor aria-hidden=true href=#speculative-decoding>#</a></h4><p><strong>Speculative Decoding</strong> (<a href=https://arxiv.org/abs/2211.17192>Leviathan et al., 2023</a>) is an innovative acceleration technique that aims to achieve the generation quality of a large model at the speed of a small model, thereby reducing latency without sacrificing quality.</p><p>It uses a small, fast Draft Model to generate multiple (e.g., $k$) candidate tokens at once. Then, the large Target Model performs a single forward pass to validate these $k$ tokens in parallel. If the tokens predicted by the draft model match those of the target model, they are accepted, effectively generating multiple tokens with a single forward pass. If they don&rsquo;t match, the subsequent predictions from the draft model are discarded, and the target model&rsquo;s prediction is used for correction.</p><figure class=align-center><img loading=lazy src=online_speculative_decoding.png#center alt="Fig. 8. Overview of online speculative decoding (OSD) framework: For each prompt, the draft model suggests multiple tokens and the target model performs the verification. (Image source: Liu et al., 2024)" width=100%><figcaption><p>Fig. 8. Overview of online speculative decoding (OSD) framework: For each prompt, the draft model suggests multiple tokens and the target model performs the verification. (Image source: <a href=https://arxiv.org/abs/2310.07177>Liu et al., 2024</a>)</p></figcaption></figure><p>As long as there is some consistency between the predictions of the draft and target models, speculative decoding can significantly reduce generation latency. Variations include <strong>Self-speculative decoding</strong>, which uses the early layers of the model itself as the draft model.</p><h4 id=heuristic-strategies>Heuristic Strategies<a hidden class=anchor aria-hidden=true href=#heuristic-strategies>#</a></h4><ul><li><strong>Best-of-N / Majority Vote</strong>: These methods improve the quality and robustness of the final result by generating multiple candidate answers.<ul><li><strong>Best-of-N</strong>: The LLM generates N answers, which are then scored by an independent evaluation model (Verifier) or a Reward Model. The answer with the highest score (Best) is selected as the final output.</li><li><strong>Majority Vote / Self-Consistency</strong>: The LLM generates multiple different reasoning paths (Chain-of-Thought) and answers for the same question. The most consistent answer is then selected through a majority vote. This method is particularly effective for tasks requiring complex reasoning.</li></ul></li></ul><h2 id=overview-of-optimization-methods>Overview of Optimization Methods<a hidden class=anchor aria-hidden=true href=#overview-of-optimization-methods>#</a></h2><p>Now that we understand the basic principles of inference, let&rsquo;s delve into how to optimize this process. The main goals of inference optimization are to <strong>reduce latency</strong>, <strong>increase throughput</strong>, and <strong>decrease memory footprint</strong>. Existing techniques can be broadly categorized into three areas: model compression, memory and computation optimization, and efficient model architectures.</p><p>Typically, the goals of model inference optimization include:</p><ul><li><strong>Reducing the model&rsquo;s memory footprint</strong> by using fewer GPU devices and less VRAM.</li><li><strong>Reducing computational complexity</strong> by decreasing the number of floating-point operations (FLOPs) required.</li><li><strong>Reducing inference latency</strong> to make the model run faster.</li></ul><p>To reduce the cost of inference in terms of memory and time, several methods can be employed:</p><ol><li><strong>Applying various parallelization techniques</strong> to scale the model across a large number of GPUs. By intelligently parallelizing model components and data, it&rsquo;s possible to run models with trillions of parameters.</li><li><strong>Memory Offloading</strong>, which moves temporarily unused data to the CPU and reads it back when needed. This helps reduce memory usage but increases latency.</li><li><strong>Smart Batching Strategies</strong>; for example, EffectiveTransformer packs consecutive sequences together to eliminate padding in batches.</li><li><strong>Network compression techniques</strong>, such as pruning, quantization, and distillation. Models with fewer parameters or lower bit-width naturally require less memory and run faster.</li><li><strong>Improvements specific to model architectures</strong>. Many architectural changes, especially to the attention layer, help speed up Transformer decoding.</li></ol><p>You can refer to a previous post on <a href=https://syhya.github.io/posts/2025-03-01-train-llm/>Large Model Training</a> for different types of training parallelization and memory-saving designs, including CPU memory offloading. This article will focus on network compression techniques and architectural improvements for Transformer models.</p><h2 id=knowledge-distillation>Knowledge Distillation<a hidden class=anchor aria-hidden=true href=#knowledge-distillation>#</a></h2><p>Knowledge Distillation (KD) (<a href=https://arxiv.org/abs/1503.02531>Hinton et al., 2015</a>) is a direct method for building a smaller model to accelerate inference by transferring the knowledge from a pre-trained, expensive model (the &ldquo;teacher model&rdquo;) to a smaller, cheaper model (the &ldquo;student model&rdquo;). There are few restrictions on the student model&rsquo;s architecture, other than requiring its output space to match the teacher&rsquo;s to construct a suitable learning objective.</p><figure class=align-center><img loading=lazy src=knowledge_distillation.png#center alt="Fig. 9. The generic framework of teacher-student knowledge distillation training. (Image source: Gou et al., 2020)" width=90%><figcaption><p>Fig. 9. The generic framework of teacher-student knowledge distillation training. (Image source: <a href=https://arxiv.org/abs/2006.05525>Gou et al., 2020</a>)</p></figcaption></figure><p>Given a dataset, the student model learns to mimic the teacher&rsquo;s output through a distillation loss function. Neural networks typically have a softmax layer; for example, an LLM outputs a probability distribution over tokens. Let $\mathbf{z}_t$ and $\mathbf{z}_s$ be the pre-softmax logits of the teacher and student models, respectively. The distillation loss minimizes the difference between the two softmax outputs, both with a high temperature $T$. When ground truth labels $\mathbf{y}$ are available, we can combine this with a supervised learning objective (e.g., cross-entropy) that operates on the ground truth labels and the student&rsquo;s soft logits.</p>$$
\mathcal{L}_{\mathrm{KD}}=\mathcal{L}_{\text {distill }}\left(\operatorname{softmax}\left(\mathbf{z}_t, T\right), \operatorname{softmax}\left(\mathbf{z}_s, T\right)\right)+\lambda \mathcal{L}_{\mathrm{CE}}\left(\mathbf{y}, \mathbf{z}_s\right)
$$<p>where $\lambda$ is a hyperparameter that balances learning from soft and hard targets. A common choice for $\mathcal{L}_{\text {distill}}$ is KL-divergence or cross-entropy.</p><p>An early success story is <strong>DistilBERT</strong> (<a href=https://arxiv.org/abs/1910.01108>Sanh et al. 2019</a>), which reduced BERT&rsquo;s parameters by 40% while retaining 97% of its performance on downstream fine-tuning tasks and running 71% faster. DistilBERT&rsquo;s pre-training loss is a combination of a soft distillation loss, a supervised training loss (masked language modeling loss $\mathcal{L}_{\text{MLM}}$ in BERT), and a special cosine embedding loss to align the hidden state vectors of the teacher and student models.</p><figure class=align-center><img loading=lazy src=DistilBERT.png#center alt="Fig. 10. The performance of DistilBERT (Image source: Sanh et al., 2019)" width=90%><figcaption><p>Fig. 10. The performance of DistilBERT (Image source: <a href=https://arxiv.org/abs/1910.01108>Sanh et al., 2019</a>)</p></figcaption></figure><p>Distillation can be easily combined with <strong>quantization</strong>, <strong>pruning</strong>, or <strong>sparsification</strong> techniques, where the teacher model is the original full-precision, dense model, and the student model is quantized, pruned, or sparsified to achieve higher sparsity.</p><h2 id=quantization>Quantization<a hidden class=anchor aria-hidden=true href=#quantization>#</a></h2><p>To further improve model performance during inference, we can go beyond low-precision floating-point numbers and use <strong>quantization</strong>. Quantization converts the model&rsquo;s floating-point weights into low-bit integer representations, such as 8-bit integers (INT8) or even 4-bit integers (INT4).</p><p>There are generally two ways to apply quantization to deep neural networks:</p><ol><li><strong>Post-Training Quantization (PTQ)</strong>: The model is first trained to convergence, and then its weights are converted to a lower precision without further training. This method is typically low-cost to implement compared to training.</li><li><strong>Quantization-Aware Training (QAT)</strong>: Quantization is applied during pre-training or further fine-tuning. QAT can achieve better performance but requires additional computational resources and access to representative training data.</li></ol><h3 id=precision-comparison>Precision Comparison<a hidden class=anchor aria-hidden=true href=#precision-comparison>#</a></h3><p>In the field of deep learning, numerical precision determines the delicate balance between computational speed and model performance. Understanding the pros and cons of different floating-point and integer formats is key to optimizing the performance of large-scale models. Floating-point numbers are represented in a computer with three parts:</p><ul><li><strong>Sign</strong>: Indicates whether the number is positive or negative.</li><li><strong>Exponent</strong>: Determines the dynamic range of the number.</li><li><strong>Mantissa (or Significand)</strong>: Determines the precision of the number. For convenience, we often refer to the mantissa as the fraction.</li></ul><figure class=align-center><img loading=lazy src=combined_float_diagrams.png#center alt="Fig. 11. fp32 vs fp16 vs bf16 (Image source: Raschka, 2023)" width=70%><figcaption><p>Fig. 11. fp32 vs fp16 vs bf16 (Image source: <a href=https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html>Raschka, 2023</a>)</p></figcaption></figure><table><thead><tr><th>Type</th><th>Total Bits</th><th>Sign Bits</th><th>Exponent Bits</th><th>Mantissa Bits</th><th>Characteristics</th></tr></thead><tbody><tr><td><a href=https://en.wikipedia.org/wiki/Double-precision_floating-point_format><strong>FP64 (Double-precision)</strong></a></td><td>64</td><td>1</td><td>11</td><td>52</td><td>Extremely high precision, widely used in scientific computing, but computationally expensive and memory-intensive, rarely used in deep learning.</td></tr><tr><td><a href=https://en.wikipedia.org/wiki/Single-precision_floating-point_format><strong>FP32 (Single-precision)</strong></a></td><td>32</td><td>1</td><td>8</td><td>23</td><td>Standard format for deep learning training, moderate speed, larger memory footprint.</td></tr><tr><td><a href=https://en.wikipedia.org/wiki/Half-precision_floating-point_format><strong>FP16 (Half-precision)</strong></a></td><td>16</td><td>1</td><td>5</td><td>10</td><td>Faster computation, half the memory footprint of FP32, but limited dynamic range, prone to numerical overflow.</td></tr><tr><td><a href=https://cloud.google.com/tpu/docs/bfloat16><strong>BF16 (Brain Floating Point)</strong></a></td><td>16</td><td>1</td><td>8</td><td>7</td><td>Same dynamic range as FP32, avoids overflow, better suited for LLMs, slightly lower precision than FP16.</td></tr></tbody></table><p>While pure FP16 precision is fast and memory-efficient, its limited dynamic range makes it highly susceptible to numerical overflow and underflow, which can make training unstable or even prevent convergence. Therefore, using <a href=https://syhya.github.io/posts/2025-03-01-train-llm/#mixed-precision-training>Mixed-Precision Training</a> is crucial.</p><p>Quantization maps floating-point numbers to integers, further reducing computational complexity and memory footprint. Specifically:</p><ul><li><strong>INT8</strong>: Occupies only 1/4 of the memory of FP32, significantly accelerating inference speed, but may slightly reduce model accuracy.</li><li><strong>INT4</strong>: A more extreme compression scheme, better suited for devices with extremely limited resources or inference scenarios requiring very high throughput.</li></ul><h3 id=challenges-in-transformer-quantization>Challenges in Transformer Quantization<a hidden class=anchor aria-hidden=true href=#challenges-in-transformer-quantization>#</a></h3><p>Many studies on Transformer model quantization share a common finding: simple low-precision (e.g., 8-bit) post-training quantization leads to a significant performance drop. This is mainly due to the <strong>high dynamic range of activation values</strong>, and a simple activation quantization strategy cannot maintain the model&rsquo;s performance.</p><figure class=align-center><img loading=lazy src=glue_benchmark.png#center alt="Fig. 12. Only quantizing model weights to 8-bit while keeping activation at full precision (W8A32) achieves much better results when activations are quantized to 8-bit irrespective of whether weights are in lower precision (W8A8 and W32A8). (Image source: Bondarenko et al. 2021)"><figcaption><p>Fig. 12. Only quantizing model weights to 8-bit while keeping activation at full precision (<code>W8A32</code>) achieves much better results when activations are quantized to 8-bit irrespective of whether weights are in lower precision (<code>W8A8</code> and <code>W32A8</code>). (Image source: <a href=https://arxiv.org/abs/2109.12948>Bondarenko et al. 2021</a>)</p></figcaption></figure><p><a href=https://arxiv.org/abs/2109.12948>Bondarenko et al. (2021)</a> found in experiments with small BERT models that the input and output of the FFN (feed-forward network) have very different dynamic ranges due to strong <strong>outliers</strong> in the output tensor. Therefore, per-tensor quantization of the FFN&rsquo;s residual sum can lead to significant errors.</p><p>As model sizes grow to billions of parameters, <strong>large-magnitude outlier features</strong> begin to appear in all Transformer layers, causing simple low-bit quantization to fail. Researchers observed this phenomenon in the <strong>OPT</strong> (<a href=https://arxiv.org/abs/2205.01068>Zhang et al. 2022</a>) model, which is larger than 6.7B parameters. Larger models have more layers with extreme outliers, and these outlier features have a significant impact on model performance. In a few dimensions, the magnitude of activation outliers can be about 100 times larger than most other values.</p><figure class=align-center><img loading=lazy src=int8_outliner.png#center alt="Fig. 13. The mean zero-shot accuracy over a set of language tasks (WinoGrande, HellaSwag, PIQA, LAMBADA) of OPT models of increasing sizes. (Image source: Dettmers et al. 2022)" width=80%><figcaption><p>Fig. 13. The mean zero-shot accuracy over a set of language tasks (WinoGrande, HellaSwag, PIQA, LAMBADA) of OPT models of increasing sizes. (Image source: <a href=https://arxiv.org/abs/2208.07339>Dettmers et al. 2022</a>)</p></figcaption></figure><h3 id=post-training-quantization-ptq>Post-Training Quantization (PTQ)<a hidden class=anchor aria-hidden=true href=#post-training-quantization-ptq>#</a></h3><h4 id=mixed-precision-quantization>Mixed-precision quantization<a hidden class=anchor aria-hidden=true href=#mixed-precision-quantization>#</a></h4><p>The most direct way to solve the aforementioned quantization challenges is to implement quantization with different precisions for weights and activations.</p><p><strong>GOBO</strong> (<a href=https://arxiv.org/abs/2005.03842>Zadeh et al. 2020</a>) was one of the first models to apply post-training quantization to BERT. It assumes that the model weights of each layer follow a Gaussian distribution and thus detects outliers by tracking the mean and standard deviation of each layer. Outlier features are kept in their original form, while other values are divided into multiple bins, storing only the corresponding bin index and centroid value.</p><figure class=align-center><img loading=lazy src=gobo.png#center alt="Fig. 14. The pseudocode for the GOBO algorithm. (Image source: Zadeh et al. 2020)" width=80%><figcaption><p>Fig. 14. The pseudocode for the GOBO algorithm. (Image source: <a href=https://arxiv.org/abs/2005.03842>Zadeh et al. 2020</a>)</p></figcaption></figure><p>Based on the observation that only certain activation layers in BERT (e.g., the residual connection after the FFN) cause large performance drops, <a href=https://arxiv.org/abs/2109.12948>Bondarenko et al. (2021)</a> adopted mixed-precision quantization, using 16-bit quantization for problematic activations and 8-bit for others.</p><p><strong>LLM.int8()</strong> (<a href=https://arxiv.org/abs/2208.07339>Dettmers et al. 2022</a>) achieves mixed-precision quantization through two mixed-precision decompositions:</p><ol><li>Since matrix multiplication consists of a series of independent inner products between row and column vectors, we can apply independent quantization to each inner product: each row and column is scaled by its absolute maximum value and then quantized to INT8.</li><li>Outlier activation features (e.g., 20 times larger than other dimensions) are kept in FP16 format, but they only account for a small fraction of the total weights. How to identify outliers is empirical.</li></ol><figure class=align-center><img loading=lazy src=llm_int8_quantization.png#center alt="Fig. 15. Two mixed-precision decompositions of LLM.int8(). (Image source: Dettmers et al. 2022)" width=100%><figcaption><p>Fig. 15. Two mixed-precision decompositions of <code>LLM.int8()</code>. (Image source: <a href=https://arxiv.org/abs/2208.07339>Dettmers et al. 2022</a>)</p></figcaption></figure><h4 id=quantization-at-fine-grained-granularity>Quantization at fine-grained granularity<a hidden class=anchor aria-hidden=true href=#quantization-at-fine-grained-granularity>#</a></h4><figure class=align-center><img loading=lazy src=quantization_granularity.png#center alt="Fig. 16. Comparison of quantization at different granularities. $d$ is the model size / hidden state dimension and $h$ is the number of heads in one MHSA (multi-head self-attention) component. (Image source: Lilian, 2023)" width=100%><figcaption><p>Fig. 16. Comparison of quantization at different granularities. $d$ is the model size / hidden state dimension and $h$ is the number of heads in one MHSA (multi-head self-attention) component. (Image source: <a href=https://lilianweng.github.io/posts/2023-01-10-inference-optimization/>Lilian, 2023</a>)</p></figcaption></figure><p>Simply quantizing the entire weight matrix of a layer (&ldquo;per-tensor&rdquo; or &ldquo;per-layer&rdquo; quantization) is the easiest to implement but cannot achieve good quantization granularity.</p><p><strong>Q-BERT</strong> (<a href=https://arxiv.org/abs/1909.05840>Shen, et al. 2020</a>) applies <strong>group-wise quantization</strong> to a fine-tuned BERT model, treating the individual matrix $W$ corresponding to each head in the MHSA (multi-head self-attention) as a group, and then applies Hessian-based mixed-precision quantization.</p><p><strong>Per-embedding group (PEG)</strong> (<a href=https://arxiv.org/abs/2109.12948>Bondarenko et al. 2021</a>) activation quantization is motivated by the observation that outliers only appear in a few dimensions of the $d$ (hidden state/model size) dimension. Per-embedding quantization is quite computationally expensive. In contrast, PEG quantization divides the activation tensor into several uniformly sized groups along the embedding dimension, where elements in the same group share quantization parameters. To ensure that all outliers are assigned to the same group, they apply a deterministic range-based permutation of the embedding dimensions, where dimensions are sorted by their value range.</p><p><strong>ZeroQuant</strong> (<a href=https://arxiv.org/abs/2206.01861>Yao et al. 2022</a>) uses group-wise quantization for weights (same as Q-BERT) and <strong>token-wise quantization</strong> for activations. To avoid expensive quantization and dequantization computations, ZeroQuant builds custom kernels that fuse the quantization operation with its preceding operation.</p><h4 id=second-order-information-for-quantization>Second-order information for quantization<a hidden class=anchor aria-hidden=true href=#second-order-information-for-quantization>#</a></h4><p><strong>Q-BERT</strong> (<a href=https://arxiv.org/abs/1909.05840>Shen, et al. 2020</a>) developed <strong>Hessian AWare Quantization (HAWQ)</strong> (<a href=https://arxiv.org/abs/1905.03696>Dong, et al. 2019</a>) for its mixed-precision quantization. The motivation is that parameters with a higher Hessian spectrum (i.e., larger top eigenvalues) are more sensitive to quantization and thus require higher precision. This is essentially a method for identifying outliers.</p><p>From another perspective, the quantization problem is an optimization problem. Given a weight matrix $\mathbf{W}$ and an input matrix $\mathbf{X}$, we want to find a quantized weight matrix $\hat{\mathbf{W}}$ that minimizes the mean squared error (MSE):</p>$$
\hat{\mathbf{W}}^* = \arg \min_{\hat{\mathbf{W}}} |\mathbf{W}\mathbf{X} - \hat{\mathbf{W}}\mathbf{X}|
$$<p><a href=https://github.com/IST-DASLab/gptq><strong>GPTQ</strong></a> (<a href=https://arxiv.org/abs/2210.17323>Frantar et al. 2022</a>) builds on the <strong>OBC (Optimal Brain Compression)</strong> (<a href=https://arxiv.org/abs/2208.11580>Frantar et al. 2022</a>) method, treating the weight matrix $\mathbf{W}$ as a set of row vectors $\mathbf{w}$ and quantizing each row independently. GPTQ iteratively quantizes more weights, which are chosen greedily to minimize the quantization error. The update for the selected weights has a closed-form solution that utilizes the Hessian matrix.</p><figure class=align-center><img loading=lazy src=gptq.png#center alt="Fig. 17. The pseudocode for the GPTQ algorithm. (Image source: Frantar et al. 2022)" width=100%><figcaption><p>Fig. 17. The pseudocode for the GPTQ algorithm. (Image source: <a href=https://arxiv.org/abs/2210.17323>Frantar et al. 2022</a>)</p></figcaption></figure><p>GPTQ can reduce the bit-width of weights in OPT-175B to <strong>3-bit</strong> or <strong>4-bit</strong> without much performance loss, but it only applies to model weights, not activations.</p><h4 id=outlier-smoothing>Outlier smoothing<a hidden class=anchor aria-hidden=true href=#outlier-smoothing>#</a></h4><figure class=align-center><img loading=lazy src=migrate_quantization_difficulty.png#center alt="Fig. 18. Magnitude of the input activations and weights of a linear layer in OPT-13B before and after SmoothQuant (Image source: Xiao et al. 2022)" width=100%><figcaption><p>Fig. 18. Magnitude of the input activations and weights of a linear layer in OPT-13B before and after SmoothQuant (Image source: <a href=https://arxiv.org/abs/2211.10438>Xiao et al. 2022</a>)</p></figcaption></figure><p>From the figure above, we can see that in Transformer models, activations are harder to quantize than weights. There are three main characteristics:</p><ol><li><strong>Activations are harder to quantize than weights</strong>: Quantizing weights to INT8/INT4 barely affects accuracy, but activations are more sensitive.</li><li><strong>Outliers amplify the difficulty of activation quantization</strong>: Extreme values in activations are about 100 times larger than normal values. Direct INT8 quantization would crush most small values to zero.</li><li><strong>Outliers are fixed in a few channels</strong>: These extreme values are consistently concentrated in specific channels, leading to a highly uneven distribution across channels.</li></ol><p><strong>SmoothQuant</strong> (<a href=https://arxiv.org/abs/2211.10438>Xiao et al. 2022</a>) proposes a clever solution by <strong>smoothing outlier features from activations to weights through a mathematically equivalent transformation</strong>, and then quantizing both weights and activations (<code>W8A8</code>). Therefore, SmoothQuant has better hardware efficiency than mixed-precision quantization.</p><figure class=align-center><img loading=lazy src=smooth_quant.png#center alt="Fig. 19. SmoothQuant migrates the scale variance from activations to weights offline to reduce the difficulty of activation quantization. Both the resulting new weight and activation matrices are easy to quantize. (Image source: Xiao et al. 2022)" width=80%><figcaption><p>Fig. 19. SmoothQuant migrates the scale variance from activations to weights offline to reduce the difficulty of activation quantization. Both the resulting new weight and activation matrices are easy to quantize. (Image source: <a href=https://arxiv.org/abs/2211.10438>Xiao et al. 2022</a>)</p></figcaption></figure><p>Considering a per-channel smoothing factor $\mathbf{s}$, SmoothQuant scales the weights according to the following formula:</p>$$
\mathbf{Y} = (\mathbf{X}\text{diag}(\mathbf{s})^{-1}) \cdot (\text{diag}(\mathbf{s})\mathbf{W}) = \hat{\mathbf{X}}\hat{\mathbf{W}}
$$<p>The smoothing factor can be easily fused into the parameters of the preceding layer offline. A hyperparameter $\alpha$ controls the degree to which the quantization difficulty is migrated from activations to weights: $\mathbf{s} = \max(|\mathbf{X}_j|)^\alpha / \max(|\mathbf{W}_j|)^{1-\alpha}$. The paper finds that for many LLMs, $\alpha=0.5$ is an optimal choice in experiments. For models with more significant outliers in activations, $\alpha$ can be adjusted to be larger.</p><h3 id=quantization-aware-training-qat>Quantization-Aware Training (QAT)<a hidden class=anchor aria-hidden=true href=#quantization-aware-training-qat>#</a></h3><p>Quantization-Aware Training (QAT) integrates the quantization operation into the pre-training or fine-tuning process, directly learning a low-bit representation of the model weights. It achieves higher performance at the cost of additional training time and computational resources.</p><p>Common QAT methods include:</p><ul><li><p><strong>Direct Fine-tuning</strong>: The model is first quantized once, and then further <strong>fine-tuned</strong> on the original pre-training dataset or a representative training dataset. This makes the model sensitive to quantization errors and allows it to actively compensate, thereby improving the performance of the quantized model. The training objective can be the same as the original pre-training objective (e.g., negative log-likelihood NLL or masked language modeling MLM in language models) or a task-specific objective (e.g., cross-entropy for classification tasks). A typical implementation is <a href=https://syhya.github.io/posts/2025-03-01-train-llm/#qlora>QLoRA</a>, which achieves efficient fine-tuning by combining a low-bit (e.g., 4-bit) base model with full-precision LoRA adapters.</p></li><li><p><strong>Knowledge Distillation</strong>: A full-precision model acts as the teacher, and a low-precision model acts as the student. A <strong>distillation loss</strong> guides the student model to approach the teacher&rsquo;s performance. The Layer-by-layer Knowledge Distillation (LKD) technique used by <strong>ZeroQuant</strong> (<a href=https://arxiv.org/abs/2206.01861>Yao et al. 2022</a>) is an example of this method. It quantizes the model weights layer by layer, with each quantized layer using the corresponding full-precision layer as a teacher, minimizing the mean squared error (MSE) between their weight computation results to improve performance.</p></li></ul>$$
\mathcal{L}_{L K D, k}=M S E\left(L_k \cdot L_{k-1} \cdot L_{k-2} \cdot \ldots \cdot L_1(\boldsymbol{X})-\widehat{L}_k \cdot L_{k-1} \cdot L_{k-2} \cdot \ldots \cdot L_1(\boldsymbol{X})\right)
$$<h2 id=pruning>Pruning<a hidden class=anchor aria-hidden=true href=#pruning>#</a></h2><p>Network pruning reduces model size by removing unimportant model weights or connections, thereby achieving model compression while maintaining performance as much as possible. Depending on the implementation, pruning can be divided into <strong>unstructured pruning</strong> and <strong>structured pruning</strong>.</p><ul><li><p><strong>Unstructured pruning</strong>: Not limited to a specific pattern, it can discard weights or connections at any position in the network, thus disrupting the original structural regularity of the network. Because the resulting sparse patterns are difficult to adapt to modern hardware architectures, this method usually cannot effectively improve actual inference efficiency.</p></li><li><p><strong>Structured pruning</strong>: Maintains the network&rsquo;s structure by trimming entire structures (such as convolutional kernels, channels, or layers). The pruned network is still suitable for dense matrix computations optimized by existing hardware, thus significantly improving actual inference performance. In this article, we focus on structured pruning to achieve efficient sparse structures in Transformer models.</p></li></ul><p>A typical workflow for network pruning includes the following three steps:</p><ol><li>Train a full, dense network until convergence.</li><li>Prune the network by removing redundant structures or weights.</li><li>(Optional) Further fine-tune the network to recover the performance of the pruned model.</li></ol><h3 id=the-lottery-ticket-hypothesis>The Lottery Ticket Hypothesis<a hidden class=anchor aria-hidden=true href=#the-lottery-ticket-hypothesis>#</a></h3><p>One of the theoretical foundations of pruning is the <strong>Lottery Ticket Hypothesis (LTH)</strong> (<a href=https://arxiv.org/abs/1803.03635>Frankle & Carbin, 2019</a>). This hypothesis posits that a randomly initialized dense neural network contains certain sparse subnetworks (the &ldquo;winning tickets&rdquo;) that, when trained in isolation, can achieve performance comparable to or even better than the full network.</p><p>The core idea of LTH is that not all parameters are equally important. Only a small fraction of the parameters in a model play a crucial role. This suggests that a large number of parameters are not primarily for solving overfitting but mainly provide a sufficient initialization search space for high-performance subnetworks to be discovered.</p><p>To test this hypothesis, Frankle and Carbin proposed the following experimental steps:</p><ol><li>Randomly initialize a dense neural network with initial weights $\theta_0$.</li><li>Train the full network to achieve good performance, with final parameters $\theta$.</li><li>Prune the trained parameters $\theta$ to generate a sparse mask $m$.</li><li>Select the &ldquo;winning ticket&rdquo; subnetwork, with initial parameters defined as $m \odot \theta_0$.</li></ol><p>The experiment found that by using only the small number of &ldquo;winning ticket&rdquo; parameters selected in step 1 and training them from their original random initial values, the model could still achieve almost the same accuracy as the original network.</p><p>This result indicates that the vast initial parameter space is not necessary for the final deployed model but provides a large number of initial possibilities during the training phase, allowing the network to discover high-performing sparse structures. This also explains why, although a pruned model is significantly smaller, training the same sparse structure from scratch is often difficult to succeed.</p><h3 id=pruning-strategies>Pruning Strategies<a hidden class=anchor aria-hidden=true href=#pruning-strategies>#</a></h3><p><strong>Magnitude pruning</strong> is the simplest yet quite effective pruning method—weights with the smallest absolute values are pruned. In fact, some studies (<a href=https://arxiv.org/abs/1902.09574>Gale et al. 2019</a>) have found that simple magnitude pruning methods can achieve comparable or better results than complex pruning methods like variational dropout (<a href=https://arxiv.org/abs/1701.05369>Molchanov et al. 2017</a>) and $l_0$ regularization (<a href=https://arxiv.org/abs/1712.01312>Louizos et al. 2017</a>). Magnitude pruning is easy to apply to large models and achieves fairly consistent performance across a wide range of hyperparameters.</p><p><strong>Gradual Magnitude Pruning (GMP)</strong> (<a href=https://arxiv.org/abs/1710.01878>Zhu & Gupta, 2017</a>) is based on the idea that large sparse models can achieve better performance than small but dense models. It proposes gradually increasing the sparsity of the network during training. At each training step, weights with the smallest absolute values are masked to zero to achieve a desired sparsity level $s$, and the masked weights do not receive gradient updates during backpropagation. The desired sparsity level $s$ increases with the number of training steps. The GMP process is sensitive to the learning rate schedule, which should be higher than that used in dense network training but not so high that it fails to converge.</p><p><strong>Iterative pruning</strong> (<a href=https://arxiv.org/abs/2003.02389>Renda et al. 2020</a>) iterates through step 2 (pruning) and step 3 (retraining) multiple times: in each iteration, only a small fraction of weights are pruned, and then the model is retrained. This process is repeated until the desired sparsity level is reached.</p><h3 id=retraining>Retraining<a hidden class=anchor aria-hidden=true href=#retraining>#</a></h3><p>The retraining step can be simple fine-tuning, using the same pre-training data or other task-specific datasets.</p><p>The <strong>Lottery Ticket Hypothesis</strong> proposes a retraining technique called <strong>weight rewinding</strong>: after pruning, the unpruned weights are re-initialized to their original values from early in training, and then retrained using the same learning rate schedule.</p><p><strong>Learning rate rewinding</strong> (<a href=https://arxiv.org/abs/2003.02389>Renda et al. 2020</a>) only resets the learning rate to its early value, while the unpruned weights remain unchanged from the end of the previous training phase. They observed that (1) on various networks and datasets, retraining with weight rewinding is superior to retraining with fine-tuning; and (2) in all tested scenarios, learning rate rewinding is comparable or superior to weight rewinding.</p><h2 id=sparsity>Sparsity<a hidden class=anchor aria-hidden=true href=#sparsity>#</a></h2><p>Sparsity is an effective way to scale model capacity while maintaining computational efficiency for model inference. Here we consider two types of sparsity for Transformers:</p><ul><li>Sparsified dense layers, including self-attention and FFN layers.</li><li>Sparse model architectures; i.e., by introducing Mixture-of-Experts (MoE) components.</li></ul><h3 id=nm-sparsity-via-pruning>N:M Sparsity via Pruning<a hidden class=anchor aria-hidden=true href=#nm-sparsity-via-pruning>#</a></h3><p><strong>N:M sparsity</strong> is a structured sparse pattern that works well with modern GPU hardware optimizations, where N out of every M consecutive elements are zero. For example, the <a href=https://www.nvidia.com/en-us/data-center/a100/>Nvidia A100</a>&rsquo;s sparse tensor cores support 2:4 sparsity for faster inference.</p><figure class=align-center><img loading=lazy src=sparsity.png#center alt="Fig. 20. The illustration of achieving N:M structure sparsity. (Image source: Zhou et al. 2021)" width=100%><figcaption><p>Fig. 20. The illustration of achieving N:M structure sparsity. (Image source: <a href=https://arxiv.org/abs/2102.04010>Zhou et al. 2021</a>)</p></figcaption></figure><p>To sparsify a dense neural network to follow an N:M structured sparse pattern, Nvidia recommends a three-step conventional workflow for training the pruned network: train -> prune to meet 2:4 sparsity -> retrain.</p><p>Permutations can provide more options during pruning to preserve large-magnitude parameters or satisfy special constraints like N:M sparsity. The result of a matrix multiplication does not change as long as the paired axes of two matrices are permuted in the same order. For example:</p><p>(1) Within a self-attention module, if the same permutation order is applied to axis 1 of the query embedding matrix $\mathbf{Q}$ and axis 0 of the key embedding matrix $\mathbf{K}^\top$, the final matrix multiplication result of $\mathbf{Q}\mathbf{K}^\top$ will remain unchanged.</p><figure class=align-center><img loading=lazy src=permutation_attention.png#center alt="Fig. 21. Illustration of the same permutation on $\mathbf{Q}$ (axis 1) and $\mathbf{K}^\top$ (axis 0) to keep the results of a self-attention module unchanged. (Image source: Lilian, 2023)" width=90%><figcaption><p>Fig. 21. Illustration of the same permutation on $\mathbf{Q}$ (axis 1) and $\mathbf{K}^\top$ (axis 0) to keep the results of a self-attention module unchanged. (Image source: <a href=https://lilianweng.github.io/posts/2023-01-10-inference-optimization/>Lilian, 2023</a>)</p></figcaption></figure><p>(2) Within an FFN layer containing two MLP layers and a ReLU non-linearity, we can permute axis 1 of the first linear weight matrix $\mathbf{W}_1$ and axis 0 of the second linear weight matrix $\mathbf{W}_2$ in the same order.</p><figure class=align-center><img loading=lazy src=permutation_ffn.png#center alt="Fig. 22. Illustration of the same permutation on $\mathbf{W}_1$ (axis 1) and $\mathbf{W}_2$ (axis 0) to keep the FFN layer&rsquo;s output unchanged. For simplicity, the bias terms are skipped but the same permutation should be applied to them too. (Image source: Lilian, 2023)" width=100%><figcaption><p>Fig. 22. Illustration of the same permutation on $\mathbf{W}_1$ (axis 1) and $\mathbf{W}_2$ (axis 0) to keep the FFN layer&rsquo;s output unchanged. For simplicity, the bias terms are skipped but the same permutation should be applied to them too. (Image source: <a href=https://lilianweng.github.io/posts/2023-01-10-inference-optimization/>Lilian, 2023</a>)</p></figcaption></figure><p>To enforce N:M structured sparsity, we divide the columns of a matrix into segments of M columns (called &ldquo;stripes&rdquo;). We can easily observe that the order of columns within each stripe and the order of the stripes themselves have no effect on the N:M sparsity constraint.</p><h3 id=channel-permutations>Channel Permutations<a hidden class=anchor aria-hidden=true href=#channel-permutations>#</a></h3><p><strong>Channel Permutations</strong> (<a href=https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html>Pool & Yu, 2021</a>) employs an iterative greedy algorithm to find the optimal permutation that maximizes the weight magnitude for N:M sparsity. All pairs of channels are speculatively swapped, and only the swap that results in the largest increase in magnitude is adopted, thus generating a new permutation and ending a single iteration. The greedy algorithm may only find a local optimum, so they introduce two techniques to escape local optima:</p><ol><li><strong>Bounded regressions</strong>: In practice, randomly swap two channels for a fixed number of times. The solution search is limited to a depth of one channel swap to keep the search space broad and shallow.</li><li><strong>Narrow, deep search</strong>: Select multiple stripes and optimize them simultaneously.</li></ol><figure class=align-center><img loading=lazy src=greedy_permulation_search.png#center alt="Fig. 23. Algorithm for finding the best permutation for N:M sparsity greedily and iteratively. (Image source: Pool & Yu, 2021)"><figcaption><p>Fig. 23. Algorithm for finding the best permutation for N:M sparsity greedily and iteratively. (Image source: <a href=https://proceedings.neurips.cc/paper/2021/hash/1415fe9c182320c8d536c9493793334d-Abstract.html>Pool & Yu, 2021</a>)</p></figcaption></figure><p>If the network is permuted before pruning, it can achieve better performance compared to pruning it in its default channel order.</p><h3 id=ste--sr-ste>STE & SR-STE<a hidden class=anchor aria-hidden=true href=#ste--sr-ste>#</a></h3><p>The <strong>Straight-Through Estimator (STE)</strong> (<a href=https://arxiv.org/abs/1308.3432>Bengio et al. 2013</a>) computes the gradient of the dense parameters with respect to the pruned network $\widetilde{W}$, $\partial\mathcal{L}/\partial\widetilde{W}$, and applies it to the dense network $W$ as an approximation:</p>$$
W_{t+1} \leftarrow W_t - \gamma \frac{\partial\mathcal{L}}{\partial\widetilde{W}}
$$<p><strong>Sparse-refined STE (SR-STE)</strong> (<a href=https://arxiv.org/abs/2102.04010>Zhou et al. 2021</a>) extends the STE method to train a model with N:M sparsity from scratch. It is commonly used for backpropagation updates in model quantization and is adapted for magnitude pruning and sparse parameter updates. The dense weights $W$ are updated as follows:</p>$$
W_{t+1} \leftarrow W_t - \gamma \frac{\partial\mathcal{L}}{\partial\widetilde{W}} + \lambda_W(\overline{\mathcal{E}} \odot W_t)
$$<p>where $\overline{\mathcal{E}}$ is the mask matrix of $\widetilde{W}$, and $\odot$ is element-wise multiplication. SR-STE aims to prevent large changes in the binary mask by (1) limiting the values of weights pruned in $\widetilde{W}_t$, and (2) boosting the weights that are not pruned in $\widetilde{W}_t$.</p><figure class=align-center><img loading=lazy src=sr-ste.png#center alt="Fig. 24. Comparison of STE and SR-STE. $\odot$ is element-wise product; $\otimes$ is matrix multiplication. (Image source: Zhou et al. 2021)" width=100%><figcaption><p>Fig. 24. Comparison of STE and SR-STE. $\odot$ is element-wise product; $\otimes$ is matrix multiplication. (Image source: <a href=https://arxiv.org/abs/2102.04010>Zhou et al. 2021</a>)</p></figcaption></figure><h3 id=top-kast>Top-KAST<a hidden class=anchor aria-hidden=true href=#top-kast>#</a></h3><p>The <strong>Top-K Always Sparse Training (Top-KAST)</strong> (<a href=https://arxiv.org/abs/2106.03517>Jayakumar et al. 2021</a>) method, unlike STE or SR-STE, can maintain constant sparsity in both the forward and backward passes without needing dense parameters or dense gradients for the forward pass.</p><p>At a training step $t$, Top-KAST proceeds as follows:</p><ol><li><p><strong>Sparse Forward Pass</strong>: Select a subset of parameters $A^t \subset \Theta$, containing the top $K$ parameters of each layer sorted by magnitude, limited to the top $D$ proportion of weights. In the parameterization $\alpha^t$ at time $t$, if a parameter is not in $A^t$ (the active weights), its value is zero.</p>$$
\alpha_i^t = \begin{cases} \theta_i^t & \text{if } i \in A^t = \{i \mid \theta_i^t \in \text{TopK}(\theta^t, D)\} \\ 0 & \text{otherwise} \end{cases}
$$<p>where $\text{TopK}(\theta, x)$ selects the top $x$ proportion of weights from $\theta$ based on magnitude.</p></li><li><p><strong>Sparse Backward Pass</strong>: The gradient is then applied to a larger subset of parameters $B \subset \Theta$, where $B$ contains a $(D+M)$ proportion of weights and $A \subset B$. Updating a larger proportion of weights allows for more effective exploration of different pruning masks, making it more likely to cause permutations in the top $D$ proportion of active weights.</p>$$
\Delta\theta_i^t = \begin{cases} -\eta \nabla_{\alpha_t} \mathcal{L}(y, x, \alpha^t)_i & \text{if } i \in B^t = \{i \mid \theta_i^t \in \text{TopK}(\theta^t, D+M)\} \\ 0 & \text{otherwise} \end{cases}
$$</li></ol><p>Training is divided into two phases, and the additional coordinates in the set $B \setminus A$ control how much exploration is introduced. The amount of exploration is expected to decrease gradually during the training process, and the mask will eventually stabilize.</p><figure class=align-center><img loading=lazy src=top_kast.png#center alt="Fig. 25. The pruning mask of Top-KAST stabilizes in time. (Image source: Jayakumar et al. 2021)" width=100%><figcaption><p>Fig. 25. The pruning mask of Top-KAST stabilizes in time. (Image source: <a href=https://proceedings.neurips.cc/paper/2020/hash/47d1e990583c9c67424d369f3414728e-Abstract.html>Jayakumar et al. 2021</a>)</p></figcaption></figure><p>To prevent the &ldquo;rich get richer&rdquo; phenomenon, Top-KAST penalizes the magnitude of active weights through an L2 regularization loss to encourage the exploration of new items. Parameters in $B \setminus A$ are penalized more than those in $A$ to set a higher selection threshold for a stable mask during updates.</p>$$
L_{\text{penalty}}(\alpha_i^t) = \begin{cases} |\theta_i^t| & \text{if } i \in A^t \\ |\theta_i^t|/D & \text{if } i \in B^t \setminus A^t \\ 0 & \text{otherwise} \end{cases}
$$<h3 id=sparsified-transformer>Sparsified Transformer<a hidden class=anchor aria-hidden=true href=#sparsified-transformer>#</a></h3><p><strong>Scaling Transformer</strong> (<a href=https://arxiv.org/abs/2111.12763>Jaszczur et al. 2021</a>) sparsifies the self-attention and FFN layers in the Transformer architecture, achieving a 37x speedup in single-sample inference.</p><figure class=align-center><img loading=lazy src=sparsified_transformer_speed.png#center alt="Fig. 26. Decoding speed of a single token for Terraformer with 17B parameters is 37x faster than a dense baseline model. (Image source: Jaszczur et al. 2021)" width=70%><figcaption><p>Fig. 26. Decoding speed of a single token for Terraformer with 17B parameters is 37x faster than a dense baseline model. (Image source: <a href=https://arxiv.org/abs/2111.12763>Jaszczur et al. 2021</a>)</p></figcaption></figure><p><strong>Sparse FFN Layer</strong>: Each FFN layer contains 2 MLPs and a ReLU. Because ReLU introduces a large number of zero values, they enforce a fixed structure on the activations, forcing only one non-zero value in a block of $N$ elements. The sparse pattern is dynamic and different for each token.</p>$$
\begin{aligned}
Y_{\text{sparse}} &= \max(0, xW_1 + b_1) \odot \text{Controller}(x) \\
\text{SparseFFN}(x) &= Y_{\text{sparse}} W_2 + b_2 \\
\text{Controller}(x) &= \arg\max(\text{Reshape}(xC_1C_2, (-1, N)))
\end{aligned}
$$<p>where each activation in $Y_{\text{sparse}}$ corresponds to a column in $W_1$ and a row in $W_2$. The controller is a low-rank bottleneck dense layer, $C_1 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{lowrank}}}, C_2 \in \mathbb{R}^{d_{\text{lowrank}} \times d_{\text{ff}}}$ and $d_{\text{lowrank}} = d_{\text{model}}/N$. It uses <code>arg max</code> at inference time to select which columns should be non-zero, and the Gumbel-softmax trick during training. Because we can compute $\text{Controller}(x)$ before loading the FFN weight matrices, we know which columns will be zeroed out and thus choose <strong>not to load them into memory</strong> to speed up inference.</p><figure class=align-center><img loading=lazy src=sparse_ffn.png#center alt="Fig. 27. (a) Sparse FFN layer; columns in red are not loaded in memory for faster inference. (b) Sparse FFN controller for 1:4 sparsity. (Image source: Jaszczur et al. 2021)" width=100%><figcaption><p>Fig. 27. (a) Sparse FFN layer; columns in red are not loaded in memory for faster inference. (b) Sparse FFN controller for 1:4 sparsity. (Image source: <a href=https://arxiv.org/abs/2111.12763>Jaszczur et al. 2021</a>)</p></figcaption></figure><p><strong>Sparse QKV (Attention) Layer</strong>: In the attention layer, the dimension $d_{\text{model}}$ is divided into $S$ modules, each of size $M = d_{\text{model}}/S$. To ensure that each sub-part can access any part of the embedding, Scaling Transformer introduces a <strong>multiplicative layer</strong> (i.e., a layer that multiplies inputs from multiple neural network layers element-wise), which can represent any permutation but contains fewer parameters than a dense layer.</p><p>Given an input vector $x \in \mathbb{R}^{d_{\text{model}}}$, the multiplicative layer outputs $y \in \mathbb{R}^{S \times M}$:</p>$$
y_{s,m} = \sum_i x_i D_{i,s} E_{i,m} \quad \text{where } D \in \mathbb{R}^{d_{\text{model}} \times S}, E \in \mathbb{R}^{d_{\text{model}} \times M}
$$<p>The output of the multiplicative layer is a tensor of size $\mathbb{R}^{\text{batch size} \times \text{length} \times S \times M}$. It is then processed by a 2D convolutional layer, where <code>length</code> and $S$ are treated as the height and width of an image. Such a convolutional layer further reduces the number of parameters and computation time of the attention layer.</p><figure class=align-center><img loading=lazy src=sparse_qkv.png#center alt="Fig. 28. (a) A multiplicative layer is introduced to enable partitions to access any part of an embedding. (b) Combination of multiplicative dense layer and 2-D convolutional layer reduces the number of parameters and computation time of the attention layer. (Image source: Jaszczur et al. 2021)" width=100%><figcaption><p>Fig. 28. (a) A multiplicative layer is introduced to enable partitions to access any part of an embedding. (b) Combination of multiplicative dense layer and 2-D convolutional layer reduces the number of parameters and computation time of the attention layer. (Image source: <a href=https://arxiv.org/abs/2111.12763>Jaszczur et al. 2021</a>)</p></figcaption></figure><p>To better handle long sequences, Scaling Transformer is further equipped with LSH (Locality-Sensitive Hashing) attention and FFN block recurrence from <strong>Reformer</strong> (<a href=https://arxiv.org/abs/2001.04451>Kitaev, et al. 2020</a>).</p><h3 id=mixture-of-experts>Mixture of Experts<a hidden class=anchor aria-hidden=true href=#mixture-of-experts>#</a></h3><p><a href=https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/#mixture-of-experts-model><strong>Mixture-of-Experts (MoE)</strong></a> models consist of multiple &ldquo;expert&rdquo; networks, where each input sample activates only a subset of these experts for computation.</p><figure class=align-center><img loading=lazy src=dense_sparse_model.png#center alt="Fig. 29. Dense Transformer vs Sparse Expert Transformer. (Image source: Fedus et al. 2022)" width=100%><figcaption><p>Fig. 29. Dense Transformer vs Sparse Expert Transformer. (Image source: <a href=https://arxiv.org/abs/2209.01667>Fedus et al. 2022</a>)</p></figcaption></figure><ul><li><p><strong>Dense Model</strong>: All input tokens are processed using the same feed-forward network (FFN) parameters. While the structure is simple and easy to train, its computational cost increases rapidly as the model size grows.</p></li><li><p><strong>Sparse Expert Model</strong>: Each input token is independently routed to a few experts among many for processing. This sparse routing mechanism allows the model to have more unique parameters without a significant increase in overall computational cost, thus improving parameter efficiency and scalability, and effectively reducing the computational cost during inference.</p></li></ul><h4 id=routing-strategy-improvements>Routing Strategy Improvements<a hidden class=anchor aria-hidden=true href=#routing-strategy-improvements>#</a></h4><p>The MoE layer has a routing network that assigns a subset of experts to each input token. In traditional MoE models, the routing strategy routes each token to its preferred expert in the order they appear in the natural sequence. If a token is routed to an expert that has already reached its capacity, the token is marked as &ldquo;overflowed&rdquo; and skipped.</p><p><strong>Vision MoE (V-MoE)</strong> (<a href=https://arxiv.org/abs/2106.05974>Riquelme et al. 2021</a>) adds MoE layers to the ViT (Vision Transformer). It achieves previous SOTA performance with only half the inference computation. V-MoE can be scaled up to 15B parameters. Their experiments used $k=2$, 32 experts, and placed an expert layer every 2 layers (meaning MoE was placed in every other layer).</p><p>Due to the limited capacity of each expert, some important and informative tokens might be dropped if they appear too late (e.g., the order of words in a sentence, or the order of image patches). To avoid this drawback of the traditional routing scheme, V-MoE employs <strong>Batch Priority Routing (BPR)</strong>, which first assigns experts to tokens with high priority. BPR computes a priority score for each token before expert assignment (the maximum or sum of the top-k router scores) and changes the order of tokens accordingly. This ensures that the expert capacity buffer will be filled with critical tokens first.</p><figure class=align-center><img loading=lazy src=v_moe_bpr.png#center alt="Fig. 30. How image patches are discarded according to priority scores when $C < 1$. (Image source: Riquelme et al. 2021)" width=100%><figcaption><p>Fig. 30. How image patches are discarded according to priority scores when $C &lt; 1$. (Image source: <a href=https://arxiv.org/abs/2106.05974>Riquelme et al. 2021</a>)</p></figcaption></figure><p>When $C \le 0.5$, BPR performs much better than traditional routing, as the model starts to drop a large number of tokens. It allows the model to compete with dense networks even at fairly low capacities.</p><p>When studying how to interpret the association between image classes and experts, they observed that early MoE layers are more general, while later MoE layers may specialize in a few image classes.</p><p><strong>Task MoE (Task-level Mixture-of-Experts)</strong> (<a href=https://arxiv.org/abs/2110.03742>Kudugunta et al. 2021</a>) considers task information and routes tokens at the <strong>task level</strong> rather than the word or token level in machine translation. They use MNMT (Multilingual Neural Machine Translation) as an example and group translation tasks based on the target language or language pair.</p><p>Token-level routing is dynamic, with routing decisions made independently for each token. Therefore, at inference time, the server needs to pre-load all experts. In contrast, task-level routing is <strong>static</strong> for a given fixed task, so an inference server for a task only needs to pre-load $k$ experts (assuming top-k routing). According to their experiments, Task MoE can achieve similar performance gains as Token MoE compared to a dense model baseline, with 2.6x higher peak throughput and only 1.6% of the decoder size.</p><p>Task-level MoE essentially classifies the task distribution based on predefined heuristic rules and incorporates this human knowledge into the router. When such heuristic rules do not exist (e.g., for a general sentence completion task), how to utilize Task MoE is less straightforward.</p><p><strong>PR-MoE (Pyramid residual MoE)</strong> (<a href=https://arxiv.org/abs/2201.05596>Rajbhandari et al. 2022</a>) has each token pass through a fixed MLP and a selected expert. Observing that MoE is more beneficial in later layers, PR-MoE employs more experts in the later layers. The DeepSpeed library implements a flexible multi-expert, multi-data parallel system to support training PR-MoE with varying numbers of experts.</p><figure class=align-center><img loading=lazy src=pr_moe.png#center alt="Fig. 31. Illustration of PR-MoE architecture in comparison with a standard MoE. (Image source: Rajbhandari et al. 2022)" width=100%><figcaption><p>Fig. 31. Illustration of PR-MoE architecture in comparison with a standard MoE. (Image source: <a href=https://arxiv.org/abs/2201.05596>Rajbhandari et al. 2022</a>)</p></figcaption></figure><h4 id=kernel-improvement>Kernel Improvement<a hidden class=anchor aria-hidden=true href=#kernel-improvement>#</a></h4><p>Expert networks can be hosted on different devices. However, as the number of GPUs increases, the number of experts per GPU decreases, and the communication between experts (&ldquo;All-to-all&rdquo;) becomes more expensive. All-to-all communication between experts across multiple GPUs relies on NCCL&rsquo;s P2P API, which cannot saturate the bandwidth of high-speed links (like NVLink, HDR InfiniBand) at a large scale because individual data blocks become smaller as more nodes are used. Existing all-to-all algorithms perform poorly in large-scale scenarios with small workloads. There are several kernel improvements that enable more efficient MoE computation, such as making all-to-all communication cheaper/faster.</p><p><strong>DeepSpeed</strong> (<a href=https://arxiv.org/abs/2201.05596>Rajbhandari et al. 2022</a>) and <strong>TUTEL</strong> (<a href=https://arxiv.org/abs/2206.03382>Hwang et al. 2022</a>) both implement a tree-based <strong>hierarchical all-to-all algorithm</strong> that first runs an intra-node all-to-all, followed by an inter-node all-to-all. It reduces the number of communication hops from $O(G)$ to $O(G_{\text{node}} + G/G_{\text{node}})$, where $G$ is the total number of GPU nodes and $G_{\text{node}}$ is the number of GPU cores per node. Although the communication volume is doubled in this implementation, it achieves better scalability in small-batch, large-scale scenarios because the bottleneck is latency rather than communication bandwidth.</p><p><strong>DynaMoE</strong> (<a href=https://arxiv.org/abs/2205.01848>Kossmann et al. 2022</a>) uses <strong>dynamic recompilation</strong> to adapt computational resources to the dynamic workload among experts. The <code>RECOMPILE</code> mechanism compiles the computation graph from scratch and reallocates resources only when needed. It measures the number of samples assigned to each expert and dynamically adjusts their capacity factor $C$ to reduce memory and computational requirements at runtime. Based on the observation that sample-expert assignments converge early in training, a <strong>sample assignment cache</strong> is introduced after convergence, and then <code>RECOMPILE</code> is used to eliminate dependencies between the gating network and the experts.</p><h2 id=architectural-optimization>Architectural Optimization<a hidden class=anchor aria-hidden=true href=#architectural-optimization>#</a></h2><h3 id=efficient-transformers>Efficient Transformers<a hidden class=anchor aria-hidden=true href=#efficient-transformers>#</a></h3><p>The survey paper <strong>Efficient Transformers</strong> (<a href=https://arxiv.org/abs/2009.06732>Tay et al. 2020</a>) reviews a series of Transformer architectures with improvements in computational and memory efficiency. Readers interested in this topic can read the original paper.</p><figure class=align-center><img loading=lazy src=efficient_transformers.png#center alt="Fig. 32. Categorization of efficient transformer models. (Image source: Tay et al. 2020)" width=100%><figcaption><p>Fig. 32. Categorization of efficient transformer models. (Image source: <a href=https://arxiv.org/abs/2009.06732>Tay et al. 2020</a>)</p></figcaption></figure><h3 id=kv-cache-optimization>KV Cache Optimization<a hidden class=anchor aria-hidden=true href=#kv-cache-optimization>#</a></h3><ul><li><p><strong>Multi-Query Attention (MQA) & Grouped-Query Attention (GQA)</strong>: In standard Multi-Head Attention (MHA), each head has its own set of Key and Value projection matrices. MQA (<a href=https://arxiv.org/abs/1911.02150>Shazeer, 2019</a>) proposed having all Query heads share the same set of Key and Value heads, which greatly reduces the size of the KV Cache. GQA (<a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>) is a compromise between MHA and MQA, grouping query heads so that heads within a group share a set of K/V, achieving a good balance between performance and effectiveness.</p></li><li><p><strong>vLLM</strong>(<a href=https://arxiv.org/abs/2309.06180>Kwon et al., 2023</a>) introduced <strong>PagedAttention</strong>, inspired by virtual memory and paging in operating systems. It divides the KV Cache into fixed-size blocks, which can be stored non-contiguously in physical VRAM. A &ldquo;block table&rdquo; manages the mapping from logical blocks to physical blocks. For a detailed introduction to vLLM, you can refer to my previous blog post <a href=https://syhya.github.io/posts/2025-05-17-vllm/>vLLM: High-Throughput and Memory-Efficient LLM Serving Engine</a>. This method almost completely eliminates memory fragmentation (both internal and external), bringing VRAM utilization close to 100%. More importantly, through a Copy-on-Write mechanism, it can efficiently share the KV Cache across requests, greatly increasing throughput for complex decoding scenarios like parallel sampling and Beam Search.</p></li></ul><h3 id=flashattention>FlashAttention<a hidden class=anchor aria-hidden=true href=#flashattention>#</a></h3><ul><li><strong>FlashAttention</strong>(<a href=https://arxiv.org/abs/2205.14135>Dao et al., 2022</a>) is an IO-aware exact attention algorithm. It recognizes that the main bottleneck in standard Attention implementations is the data transfer between GPU HBM (High-Bandwidth Memory) and SRAM (on-chip high-speed cache). FlashAttention uses <strong>Tiling</strong> and <strong>Recomputation</strong> techniques to fuse the entire Attention computation into a single CUDA kernel, avoiding the need to write and read the huge $N \times N$ attention matrix to and from HBM. This dramatically reduces memory access, thereby speeding up Attention computation by several times without sacrificing accuracy. <strong>FlashAttention-2</strong>(<a href=https://arxiv.org/abs/2307.08691>Dao, 2023</a>) further optimizes parallelism and hardware utilization.</li></ul><figure class=align-center><img loading=lazy src=flash_attention.png#center alt="Fig. 33. FlashAttention uses tiling to avoid materializing the large N × N attention matrix on slow GPU HBM, achieving up to 7.6× speedup over PyTorch. (Image source: Dao et al., 2022)" width=100%><figcaption><p>Fig. 33. FlashAttention uses tiling to avoid materializing the large N × N attention matrix on slow GPU HBM, achieving up to 7.6× speedup over PyTorch. (Image source: <a href=https://arxiv.org/abs/2205.14135>Dao et al., 2022</a>)</p></figcaption></figure><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>[1] Zhou, Zixuan, et al. <a href=https://arxiv.org/abs/2404.14294>“A survey on efficient inference for large language models.”</a> arXiv preprint arXiv:2404.14294 (2024).</p><p>[2] Zhang, Aston, et al. <a href=https://d2l.ai/>“Dive into Deep Learning.”</a>. Cambridge University Press, 2023.</p><p>[3] Big Hummingbird Blogs. (2024). <a href=https://www.bighummingbird.com/blogs/llm-hyperparameter>“A Visual Explanation of LLM Hyperparameters.”</a> Blog post.</p><p>[4] Fan, Angela, Mike Lewis, and Yann Dauphin. <a href=https://arxiv.org/abs/1805.04833>“Hierarchical neural story generation.”</a> arXiv preprint arXiv:1805.04833 (2018).
int arXiv:1805.04832.</p><p>[5] Holtzman, Ari, et al. <a href=https://arxiv.org/abs/1904.09751>“The curious case of neural text degeneration.”</a> arXiv preprint arXiv:1904.09751 (2019).</p><p>[6] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. <a href=https://arxiv.org/abs/2211.17192>“Fast inference from transformers via speculative decoding.”</a> International Conference on Machine Learning. PMLR, 2023.</p><p>[7] Liu, Xiaoxuan, et al. <a href=https://arxiv.org/abs/2310.07177>“Online speculative decoding.”</a> arXiv preprint arXiv:2310.07177 (2023).</p><p>[8] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. <a href=https://arxiv.org/abs/1503.02531>“Distilling the knowledge in a neural network.”</a> arXiv preprint arXiv:1503.02531 (2015).</p><p>[9] Gou, Jianping, et al. <a href=https://arxiv.org/abs/2006.05525>“Knowledge distillation: A survey.”</a> International Journal of Computer Vision 129.6 (2021): 1789-1819.</p><p>[10] Sanh, Victor, et al. <a href=https://arxiv.org/abs/1910.01108>“DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.”</a> arXiv preprint arXiv:1910.01108 (2019).</p><p>[11] Raschka, S. (2023). <a href=https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html>“Accelerating Large Language Models with Mixed-Precision Techniques.”</a> Blog post.</p><p>[12] Bondarenko, Yelysei, Markus Nagel, and Tijmen Blankevoort. <a href=https://arxiv.org/abs/2109.12948>&ldquo;Understanding and overcoming the challenges of efficient transformer quantization.&rdquo;</a> arXiv preprint arXiv:2109.12948 (2021).</p><p>[13] Zhang, S., Roller, S., Goyal, N., et al. (2022). <a href=https://arxiv.org/abs/2205.01068>“OPT: Open pre-trained transformer language models.”</a> arXiv preprint arXiv:2205.01068.</p><p>[14] Dettmers, T., et al. (2022). <a href=https://arxiv.org/abs/2208.07339>“LLM.int8(): 8-bit matrix multiplication for transformers at scale.”</a> arXiv preprint arXiv:2208.07339.</p><p>[15] Zadeh, V. K., et al. (2020). <a href=https://arxiv.org/abs/2005.03842>“GOBO: Quantizing attention-based NLP models for low latency and energy efficient inference.”</a> arXiv preprint arXiv:2005.03842.</p><p>[16] Weng, L. (2023). <a href=https://lilianweng.github.io/posts/2023-01-10-inference-optimization/>“Large Transformer Model Inference Optimization.”</a> Lil’Log blog post.</p><p>[17] Shen, Z., Dong, Z., Ye, J., et al. (2020). <a href=https://arxiv.org/abs/1909.05840>“Q-BERT: Hessian-based ultra-low-precision quantization of BERT.”</a> arXiv preprint arXiv:1909.05840.</p><p>[18] Dong, Z., Yao, Z., Gholami, A., et al. (2019). <a href=https://arxiv.org/abs/1905.03696>“HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision”</a> arXiv preprint arXiv:1905.03696.</p><p>[19] Yao, Z., et al. (2022). <a href=https://arxiv.org/abs/2206.01861>“ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers.”</a> arXiv preprint arXiv:2206.01861.</p><p>[20] Frantar, E., et al. (2022). <a href=https://arxiv.org/abs/2210.17323>“GPTQ: Accurate post-training quantization for generative pre-trained transformers.”</a> arXiv preprint arXiv:2210.17323.</p><p>[21] Xiao, G., & Lin, J. (2022). <a href=https://arxiv.org/abs/2211.10438>“SmoothQuant: Accurate and efficient post-training quantization for large language models.”</a> arXiv preprint arXiv:2211.10438.</p><p>[22] Frankle, J., & Carbin, M. (2019). <a href=https://arxiv.org/abs/1803.03635>“The lottery ticket hypothesis: Finding sparse, trainable neural networks.”</a> arXiv preprint arXiv:1803.03635.</p><p>[23] Gale, T., Elsen, E., & Hooker, S. (2019). <a href=https://arxiv.org/abs/1902.09574>“The state of sparsity in deep neural networks.”</a> arXiv preprint arXiv:1902.09574.</p><p>[24] Molchanov, D., Ashukha, A., & Vetrov, D. (2017). <a href=https://arxiv.org/abs/1701.05369>“Variational dropout sparsifies deep neural networks.”</a> arXiv preprint arXiv:1701.05369.</p><p>[25] Louizos, Christos, Max Welling, and Diederik P. Kingma. <a href=https://arxiv.org/abs/1712.01312>&ldquo;Learning sparse neural networks through $ L_0 $ regularization.&rdquo;</a> arXiv preprint arXiv:1712.01312 (2017).</p><p>[26] Zhu, M., & Gupta, S. (2017). <a href=https://arxiv.org/abs/1710.01878>“To prune, or not to prune: Exploring the efficacy of pruning for model compression.”</a> arXiv preprint arXiv:1710.01878.</p><p>[27] Renda, A., Frankle, J., & Carbin, M. (2020). <a href=https://arxiv.org/abs/2003.02389>“Comparing rewinding and fine-tuning in neural network pruning.”</a> arXiv preprint arXiv:2003.02389.</p><p>[28] Nvidia. (2020). <a href=https://www.nvidia.com/en-us/data-center/a100/>“NVIDIA A100 Tensor Core GPU.”</a> Nvidia Blog.</p><p>[29] Zhou, A., & Ma, X. (2021). <a href=https://arxiv.org/abs/2102.04010>“Learning N:M fine-grained structured sparse neural networks from scratch.”</a> arXiv preprint arXiv:2102.04010.</p><p>[30] Pool, J., & Yu, F. (2021). <a href=https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html>“Channel permutations for N:M structured sparsity.”</a> Advances in Neural Information Processing Systems 34.</p><p>[31] Bengio, Y., Léonard, N., & Courville, A. (2013). <a href=https://arxiv.org/abs/1308.3432>“Estimating or propagating gradients through stochastic neurons for conditional computation.”</a> arXiv preprint arXiv:1308.3432.</p><p>[32] Jayakumar, S. M., Pascanu, R., Rae, J., et al. (2021). <a href=https://arxiv.org/abs/2106.03517>“Top-KAST: Top-K always sparse training.”</a> arXiv preprint arXiv:2106.03517.</p><p>[33] Jaszczur, S., et al. (2021). <a href=https://arxiv.org/abs/2111.12763>“Sparse is enough in scaling transformers.”</a> Advances in Neural Information Processing Systems 34.</p><p>[34] Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020). <a href=https://arxiv.org/abs/2001.04451>“Reformer: The efficient transformer.”</a> arXiv preprint arXiv:2001.04451.</p><p>[35] Fedus, W., et al. (2022). <a href=https://arxiv.org/abs/2209.01667>“A review of sparse expert models in deep learning.”</a> arXiv preprint arXiv:2209.01667.</p><p>[36] Riquelme, C., et al. (2021). <a href=https://arxiv.org/abs/2106.05974>“Scaling vision with sparse mixture of experts.”</a> Advances in Neural Information Processing Systems 34: 8583-8595.</p><p>[37] Kudugunta, S., Lepikhin, D., Heafield, K., et al. (2021). <a href=https://arxiv.org/abs/2110.03742>“Beyond domain adaptation: Multi-task mixture-of-experts for zero-shot generalization.”</a> arXiv preprint arXiv:2110.03742.</p><p>[38] Rajbhandari, S., et al. (2022). <a href=https://arxiv.org/abs/2201.05596>“DeepSpeed-MoE: Advancing mixture-of-experts inference and training to power next-generation AI scale.”</a> arXiv preprint arXiv:2201.05596.</p><p>[39] Hwang, I., et al. (2022). <a href=https://arxiv.org/abs/2206.03382>“Tutel: Adaptive mixture-of-experts at scale.”</a> arXiv preprint arXiv:2206.03382.</p><p>[40] Kossmann, F., et al. (2022). <a href=https://arxiv.org/abs/2205.01848>“Optimizing Mixture of Experts using Dynamic Recompilations”</a> arXiv preprint arXiv:2205.01848.</p><p>[41] Tay, Y., et al. (2020). <a href=https://arxiv.org/abs/2009.06732>“Efficient transformers: A survey.”</a> arXiv preprint arXiv:2009.06732.</p><p>[42] Shazeer, N. (2019). <a href=https://arxiv.org/abs/1911.02150>“Fast transformer decoding: One write-head is all you need.”</a> arXiv preprint arXiv:1911.02150.</p><p>[43] Ainslie, J., et al. (2023). <a href=https://arxiv.org/abs/2305.13245>“GQA: Training generalized multi-query transformer models from multi-head checkpoints.”</a> arXiv preprint arXiv:2305.13245.</p><p>[44] Kwon, W., et al. (2023). <a href=https://arxiv.org/abs/2309.06180>“Efficient memory management for large language model serving with PagedAttention.”</a> Proceedings of the 29th Symposium on Operating Systems Principles.</p><p>[45] Dao, T., et al. (2022). <a href=https://arxiv.org/abs/2205.14135>“FlashAttention: Fast and memory-efficient exact attention with IO-awareness.”</a> Advances in Neural Information Processing Systems 35: 16344-16359.</p><p>[46] Dao, T. (2023). <a href=https://arxiv.org/abs/2307.08691>“FlashAttention-2: Faster attention with better parallelism and work partitioning.”</a> arXiv preprint arXiv:2307.08691.</p><p>[47] Pope, R., et al. (2022). <a href=https://arxiv.org/abs/2211.05102>“Efficiently scaling transformer inference.”</a> arXiv preprint arXiv:2211.05102.</p><p>[48] von Platen, P. (2020). <a href=https://huggingface.co/blog/how-to-generate>“How to generate text: Using different decoding methods for language generation with Transformers.”</a> Hugging Face Blog.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reproducing or citing the content of this article, please credit the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Jun 2025). Large Language Model Inference.
<a href=https://syhya.github.io/posts/2025-06-29-llm-inference>https://syhya.github.io/posts/2025-06-29-llm-inference</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025llminferencesurvey</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;Large Language Model Inference&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Jun&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-06-29-llm-inference&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/tags/inference/>Inference</a></li><li><a href=https://syhya.github.io/tags/quantization/>Quantization</a></li><li><a href=https://syhya.github.io/tags/pruning/>Pruning</a></li><li><a href=https://syhya.github.io/tags/knowledge-distillation/>Knowledge Distillation</a></li><li><a href=https://syhya.github.io/tags/kv-cache/>KV Cache</a></li><li><a href=https://syhya.github.io/tags/attention/>Attention</a></li><li><a href=https://syhya.github.io/tags/speculative-decoding/>Speculative Decoding</a></li><li><a href=https://syhya.github.io/tags/flashattention/>FlashAttention</a></li><li><a href=https://syhya.github.io/tags/vllm/>VLLM</a></li><li><a href=https://syhya.github.io/tags/transformer/>Transformer</a></li><li><a href=https://syhya.github.io/tags/sparsity/>Sparsity</a></li><li><a href=https://syhya.github.io/tags/mixture-of-experts/>Mixture of Experts</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-08-24-gpt5/><span class=title>« Prev</span><br><span>gpt-oss & GPT-5</span>
</a><a class=next href=https://syhya.github.io/posts/2025-05-17-vllm/><span class=title>Next »</span><br><span>vLLM: High-Throughput, Memory-Efficient LLM Serving</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Inference on x" href="https://x.com/intent/tweet/?text=Large%20Language%20Model%20Inference&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-06-29-llm-inference%2f&amp;hashtags=LLM%2cInference%2cQuantization%2cPruning%2cKnowledgeDistillation%2cKVCache%2cAttention%2cSpeculativeDecoding%2cFlashAttention%2cvLLM%2cTransformer%2cSparsity%2cMixtureofExperts"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Inference on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-06-29-llm-inference%2f&amp;title=Large%20Language%20Model%20Inference&amp;summary=Large%20Language%20Model%20Inference&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-06-29-llm-inference%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Inference on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-06-29-llm-inference%2f&title=Large%20Language%20Model%20Inference"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Inference on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-06-29-llm-inference%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Inference on whatsapp" href="https://api.whatsapp.com/send?text=Large%20Language%20Model%20Inference%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-06-29-llm-inference%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Inference on telegram" href="https://telegram.me/share/url?text=Large%20Language%20Model%20Inference&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-06-29-llm-inference%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Inference on ycombinator" href="https://news.ycombinator.com/submitlink?t=Large%20Language%20Model%20Inference&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-06-29-llm-inference%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>