<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scaling Laws | Yue Shui Blog</title><meta name=keywords content="Deep Learning,AI,LLM,Scaling Laws,Test-Time Compute,Reinforcement Learning,Reward Model,Compute-Optimal"><meta name=description content="From the evolution of the GPT series, researchers have gradually realized that as long as model parameters, training data, and compute resources are continuously scaled up, the performance of large models improves along a stable and predictable path. This predictability is characterized by Scaling Laws, which provide the theoretical foundation and practical confidence for high-cost pre-training. As model scale, alignment techniques, and inference-time compute co-evolve, the boundaries of AI capabilities are being systematically pushed. Scaling laws are not only the foundation for building next-generation models but also a key methodology for continuously improving model capabilities under compute constraints."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-11-19-scaling-law/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-11-19-scaling-law/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-11-19-scaling-law/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-11-19-scaling-law/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Scaling Laws"><meta property="og:description" content="From the evolution of the GPT series, researchers have gradually realized that as long as model parameters, training data, and compute resources are continuously scaled up, the performance of large models improves along a stable and predictable path. This predictability is characterized by Scaling Laws, which provide the theoretical foundation and practical confidence for high-cost pre-training. As model scale, alignment techniques, and inference-time compute co-evolve, the boundaries of AI capabilities are being systematically pushed. Scaling laws are not only the foundation for building next-generation models but also a key methodology for continuously improving model capabilities under compute constraints."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-19T12:00:00+08:00"><meta property="article:modified_time" content="2025-12-03T10:00:00+00:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Scaling Laws"><meta property="article:tag" content="Test-Time Compute"><meta property="article:tag" content="Reinforcement Learning"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Scaling Laws"><meta name=twitter:description content="From the evolution of the GPT series, researchers have gradually realized that as long as model parameters, training data, and compute resources are continuously scaled up, the performance of large models improves along a stable and predictable path. This predictability is characterized by Scaling Laws, which provide the theoretical foundation and practical confidence for high-cost pre-training. As model scale, alignment techniques, and inference-time compute co-evolve, the boundaries of AI capabilities are being systematically pushed. Scaling laws are not only the foundation for building next-generation models but also a key methodology for continuously improving model capabilities under compute constraints."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Scaling Laws","item":"https://syhya.github.io/posts/2025-11-19-scaling-law/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scaling Laws","name":"Scaling Laws","description":"From the evolution of the GPT series, researchers have gradually realized that as long as model parameters, training data, and compute resources are continuously scaled up, the performance of large models improves along a stable and predictable path. This predictability is characterized by Scaling Laws, which provide the theoretical foundation and practical confidence for high-cost pre-training. As model scale, alignment techniques, and inference-time compute co-evolve, the boundaries of AI capabilities are being systematically pushed. Scaling laws are not only the foundation for building next-generation models but also a key methodology for continuously improving model capabilities under compute constraints.\n","keywords":["Deep Learning","AI","LLM","Scaling Laws","Test-Time Compute","Reinforcement Learning","Reward Model","Compute-Optimal"],"articleBody":"From the evolution of the GPT series, researchers have gradually realized that as long as model parameters, training data, and compute resources are continuously scaled up, the performance of large models improves along a stable and predictable path. This predictability is characterized by Scaling Laws, which provide the theoretical foundation and practical confidence for high-cost pre-training. As model scale, alignment techniques, and inference-time compute co-evolve, the boundaries of AI capabilities are being systematically pushed. Scaling laws are not only the foundation for building next-generation models but also a key methodology for continuously improving model capabilities under compute constraints.\nScaling Laws Scaling Laws (Kaplan et al., 2020) are the foundation for understanding how Large Language Model (LLM) performance improves with resource investment. It was the first to systematically reveal the Power-Law relationship between model performance and three core factors: model parameter count $N$, dataset size $D$, and training compute $C$. A basic inverse power-law relationship can be expressed as:\n$$ y = a \\left( \\frac{1}{x} \\right)^p $$In a log-log plot, this relationship appears as a straight line, which is the iconic chart seen in most scaling law papers. For LLMs, y is typically the model’s test loss, while x is a scale variable of interest (e.g., parameter count).\nBasic Laws Fig. 1. Language modeling performance improves smoothly with model size, dataset size, and training compute. (Image source: Kaplan et al., 2020)\nWhen the other two factors are not bottlenecks, the model’s test loss $L$ exhibits a power-law relationship with model parameters $N$, dataset size $D$, and compute $C$, respectively. These relationships can be precisely modeled mathematically:\nModel Size (N): With sufficient data, loss decreases as the number of non-embedding parameters $N$ increases. $$ L(N)=\\left(N_{\\mathrm{c}} / N\\right)^{\\alpha_N} ; \\alpha_N \\sim 0.076, \\quad N_{\\mathrm{c}} \\sim 8.8 \\times 10^{13} \\text { (non-embedding parameters) } $$ Dataset Size (D): For LLMs trained to convergence (via early stopping), loss decreases as dataset size $D$ increases. $$ L(D)=\\left(D_{\\mathrm{c}} / D\\right)^{\\alpha_D} ; \\alpha_D \\sim 0.095, \\quad D_{\\mathrm{c}} \\sim 5.4 \\times 10^{13} \\text { (tokens) } $$ Training Compute (C): With optimal model size and sufficient data, loss decreases as the minimum compute $C_{\\min}$ increases. $$ L\\left(C_{\\min }\\right)=\\left(C_{\\mathrm{c}}^{\\min } / C_{\\min }\\right)^{\\alpha_C^{\\min }} ; \\alpha_C^{\\min } \\sim 0.050, \\quad C_{\\mathrm{c}}^{\\min } \\sim 3.1 \\times 10^8 \\text { (PF-days) } $$Here, $N_c, D_c, C_c^{\\min}$ are constants, while $\\alpha_N, \\alpha_D, \\alpha_C^{\\min}$ are scaling exponents that determine the rate of performance improvement. This work also uncovered several critical findings:\nLarger models are more sample-efficient: To reach the same level of loss, larger models require fewer training samples. When the compute budget increases, priority should be given to increasing model parameters $N$ rather than data volume $D$ (suggesting $N \\propto C^{0.73}, D \\propto C^{0.27}$). Fig. 2. Optimal parameters for compute-efficient training. (Image source: Kaplan et al., 2020)\nModel shape matters little: Provided the total parameter count is fixed, specific architectural designs such as depth vs. width have a minimal impact on performance compared to scale itself. C ≈ 6ND: In Decoder-only Transformer models, training compute (FLOPs) can be estimated via a simple formula. With $N$ parameters, a forward pass processes one token using approximately $2N$ FLOPs, and the backward pass is roughly twice the forward pass. Thus, the cost to train per token is approximately $6N$ FLOPs. If the training corpus contains $D$ tokens, the total training compute is approximately $6ND$. Multimodal Scaling Laws for Autoregressive Generative Modeling (Henighan et al., 2020) verified that scaling laws are not unique to language models but also apply to multimodal models. Autoregressive Transformers in image, video, and multimodal tasks all show predictable power-law performance improvements with scale.\nFig. 3. Smooth scaling of reducible loss across different domains (Image, Video, Multimodal). (Image source: Henighan et al., 2020)\nChinchilla Chinchilla (Hoffmann et al., 2022) revised OpenAI’s conclusions. By training over 400 models, they found that OpenAI did not identify the true optimum due to the use of fixed learning rate schedules.\nThe Chinchilla scaling law states that to achieve Compute-Optimality, model parameters $N$ and training tokens $D$ should be scaled equally. That is: $$ N \\propto C^{0.5}, \\quad D \\propto C^{0.5} $$This implies that under a given compute budget, most existing large models (such as GPT-3, Gopher) are undertrained. Chinchilla (70B), trained following this law (using the same compute as Gopher 280B but with 4x the data), outperformed Gopher across the board.\nFig. 4. Training curve envelope and optimal model size/token count projections. (Image source: Hoffmann et al., 2022)\nGPT-4 In the GPT-4 technical report (OpenAI, 2023), OpenAI demonstrated a crucial application of Scaling Laws: predicting the final performance of large models using small models. They accurately predicted GPT-4’s final loss using models with only 1/10,000th the compute. Their fitting formula introduces an irreducible loss $c$:\n$$ L(C)=a C^b+c $$Parameter Definitions:\n$C$: Training compute. $L(C)$：Model loss under compute $C$. $a$：Scale Coefficient, controls the overall magnitude of loss reduction as compute increases. $b$：Scaling Exponent, determines the rate of loss reduction; a larger exponent means faster reduction. $c$：Irreducible Loss, reflecting the inherent entropy of the data, i.e., the lower bound of error that cannot be reduced regardless of compute investment. Fig. 5. GPT-4’s final loss on an internal code-token dataset aligns with a power-law trend extrapolated from smaller models when training compute is normalized. (Image source: OpenAI, 2023)\nIn a plot with normalized training compute on the x-axis and bits-per-word on the y-axis, the loss points from small models exhibit a highly stable power-law linear relationship. The power-law curve fitted from these points (without using any GPT-4 data) falls almost precisely on GPT-4’s final loss.\nRM Scaling Laws In the RLHF pipeline, the Reward Model (RM) acts as a proxy for human preference. However, the RM is not a perfect discriminator. Goodhart’s law states: When a measure becomes a target, it ceases to be a good measure. Overoptimizing this proxy RM during LLM training leads to performance degradation on the true objective.\nScaling Laws for Reward Model Overoptimization (Gao et al., 2023) used a synthetic data setup, employing a 6B parameter “Gold RM” as the true reward $R$, and using different “Proxy RMs” ranging from 3M to 3B parameters as optimization targets.\nFig. 6. Scaling laws for reward model overoptimization for Best-of-N and RL. (Image source: Gao et al., 2023)\nThe KL divergence from the initial policy to the optimized policy is defined as:\n$$ \\mathrm{KL}:=D_{\\mathrm{KL}}\\left(\\pi \\| \\pi_{\\mathrm{init}}\\right) $$And a distance function is defined as:\n$$ d:=\\sqrt{D_{\\mathrm{KL}}\\left(\\pi \\| \\pi_{\\mathrm{init}}\\right)} $$The change in the Gold RM score $R$ follows different functional forms depending on the optimization method:\n$$ R_{\\mathrm{BoN}}(d) = d(\\alpha_{\\mathrm{BoN}} - \\beta_{\\mathrm{BoN}} d) $$Best-of-N (BoN) Rejection Sampling exhibits a quadratic decay relationship (increasing then decreasing). In the early stages of optimization, the true reward increases, but after passing a certain optimal point, the reward drops significantly as optimization deepens.\n$$ R_{\\mathrm{RL}}(d) = d(\\alpha_{\\mathrm{RL}} - \\beta_{\\mathrm{RL}} \\log d) $$RL has a decay term of $\\log d$, meaning the decline is significantly slower than BoN, exhibiting a logarithmic decay relationship.\nFig. 7. The values of $\\alpha_{\\text {bon }}$, $\\beta_{\\text {bon }}$ and $\\beta_{\\mathrm{RL}}$ in the BoN and RL overoptimization scaling laws for both proxy (dashed line) and gold (solid line) rewards as they scale with parameter count. (Image source: Gao et al., 2023)\nThe coefficients $\\alpha$ and $\\beta$ in the above formulas describe the initial optimization efficiency and the severity of overoptimization, leading to the following conclusions:\nRM Parameter Count is Critical: Increasing the RM parameters effectively increases $\\alpha$ and decreases $\\beta$, thereby mitigating overoptimization and achieving higher Gold RM peaks. RL is More Robust to Overoptimization than BoN: Measured by KL divergence, RL is slower in both optimization and overoptimization compared to BoN. Test-time Scaling In addition to investing compute during training, model performance can also be improved by increasing computation at inference time—a paradigm known as Test-Time Scaling (Snell et al., 2024). OpenAI demonstrated this phenomenon in o1 (OpenAI, 2025): scaling both train-time compute and test-time compute consistently improves model performance, particularly on complex reasoning tasks such as mathematics and programming.\nFig. 8. Additional RL training and additional test-time compute improves competitive mathematics performance. (Image source: OpenAI, 2025)\nParallel Sampling: e.g., Best-of-N or Majority Voting. The model generates multiple independent samples for the same question, and the best answer is selected via a verifier or voting. Sequential Refinement: e.g., Self-Correction. The model iteratively improves based on the previous output. Efficiency Trade-off Fig. 9. Compute-Optimal Scaling for Iterative Self-Refinement and Search: Evaluating Efficiency Against Best-of-N Baselines and Analyzing the Trade-offs Between Test-Time Compute and Pretraining Scaling (Image source: Snell et al., 2024)\nFrom the figure above, we can draw the following conclusions:\nThe optimal strategy depends on problem difficulty, and there is a limit to returns: Although compute-optimal strategies are generally more efficient than the Best-of-N baseline (saving ~4x compute), their specific efficacy is highly dependent on task complexity.\nEasy Problems: The model’s initial answer is usually directionally correct and only needs local fine-tuning. In this case, sequential refinement is the most efficient strategy, quickly fixing details with minimal compute cost. Hard Problems: Although parallel search can explore more paths, as problem difficulty increases, the return on simply increasing test-time inference (whether search or refinement) rapidly shows diminishing marginal returns. For extremely hard problems, test-time compute struggles to break through the ceiling of the model’s capabilities. Test-time compute cannot fully replace pre-training; base capability remains fundamental: Test-time Compute and Pretraining Compute are not a 1:1 equivalent exchange.\nPrerequisite for Exchange ($Y \\ll X$): Increasing inference time is only cost-effective when the inference token budget ($Y$) is far smaller than the pre-training token budget ($X$), and the problems are of low-to-medium difficulty. Irreplaceability of Base Models: Once the problem becomes hard or inference demands are too high, the reasoning tricks (search/refinement) of small models cannot bridge the huge capability gap. The broad knowledge and strong reasoning capabilities provided by scaling up pre-training remain the decisive foundation for solving complex problems. Simple test-time scaling s1: Simple test-time scaling (Muennighoff et al., 2025) curated the s1K dataset containing 1,000 questions paired with reasoning traces. They then performed supervised fine-tuning (SFT) using Qwen2.5-32B-Instruct as the base model, employing a Budget Forcing method to control the length of output tokens to study test-time scaling.\nFig. 10. Budget forcing in s1-32B: suppressing the end-of-thinking delimiter prompts the model to continue after “…is 2.”, triggering a self-correction via “Wait”. (Image source: Muennighoff et al., 2025)\nLengthening: When the model attempts to end thinking, suppress the end token and force append words like Wait to encourage the model to reflect and double-check. Shortening: When the thinking process exceeds a preset token length, force append end tokens (e.g., or \"Final Answer:\") to prompt the model to output a conclusion immediately. Fig. 11. Sequential and parallel test-time scaling. (Image source: Muennighoff et al., 2025)\nThe results above show that thinking time (Token count) extended via Budget Forcing in both parallel and sequential scaling methods is significantly positively correlated with downstream task accuracy.\nScale RL ScaleRL (Khatri et al., 2025) proposes a systematic method for analyzing and predicting RL scaling in LLMs. Unlike the common power-law relationship in pre-training, the performance of RL training (e.g., expected reward $R_C$ on the validation set) follows a Sigmoidal saturation curve as compute $C$ increases. This curve can be described by the following formula:\n$$ \\overbrace{R_C-R_0}^{\\text {Reward Gain }}=\\overbrace{\\left(A-R_0\\right)}^{\\text {Asymptotic Reward Gain }} \\times \\frac{1}{\\underbrace{1+\\left(C_{\\text {mid }} / C\\right)^B}_{\\text {Compute Efficiency }}} $$Parameter Definitions:\n$R_C$: Expected reward (or Pass Rate) under compute $C$. $R_0$: Initial model performance. $A$: Asymptotic Performance. Represents the theoretical maximum performance the model can achieve under infinite compute. $B$: Scaling Exponent (Efficiency). Controls the steepness of the curve; a larger value means the model reaches the performance ceiling faster. $C_{mid}$: The compute required to achieve half of the total gain. Fig. 12. Interpretation of the sigmoidal scaling curve for RL. (Image source: Khatri et al., 2025)\nThe figure above demonstrates the value of this framework: It allows researchers to fit the curve using early training data at a smaller compute scale, thereby predicting the final performance and efficiency of an RL recipe under a larger compute budget, reducing the cost of algorithm exploration.\nThrough large-scale ablation experiments on various RL design choices (such as loss functions, advantage normalization, data curriculum, etc.), several key principles were summarized:\nPerformance ceilings are not universal: Different RL algorithms (e.g., GRPO, DAPO, CISPO) have different asymptotic performance limits $A$. Choosing the correct loss function is critical. Decoupling of Efficiency and Ceiling: Many common RL tricks, such as advantage normalization, data curriculum, and length penalties, mainly affect compute efficiency ($B$ and $C_{\\text{mid}}$), but have little impact on the performance ceiling $A$. Stable and Predictable Scaling: A carefully designed, stable RL recipe follows a predictable sigmoidal trajectory. Even when scaled to the level of 100,000 GPU hours, its performance remains highly consistent with the curve fitted from early data. Fig. 13. ScaleRL demonstrates more scalable performance compared to other prevalent RL methods. (Image source: Khatri et al., 2025)\nReferences [1] Kaplan, Jared, et al. “Scaling laws for neural language models.” arXiv preprint arXiv:2001.08361 (2020).\n[2] Henighan, Tom, et al. “Scaling laws for autoregressive generative modeling.” arXiv preprint arXiv:2010.14701 (2020).\n[3] Hoffmann, Jordan, et al. “Training compute-optimal large language models.” arXiv preprint arXiv:2203.15556 (2022).\n[4] Achiam, Josh, et al. “Gpt-4 technical report.” arXiv preprint arXiv:2303.08774 (2023).\n[5] Gao, Leo, John Schulman, and Jacob Hilton. “Scaling laws for reward model overoptimization.” International Conference on Machine Learning. PMLR, 2023.\n[6] Snell, Charlie, et al. “Scaling llm test-time compute optimally can be more effective than scaling model parameters.” arXiv preprint arXiv:2408.03314 (2024).\n[7] El-Kishky, Ahmed, et al. “Competitive programming with large reasoning models.” arXiv preprint arXiv:2502.06807 (2025).\n[8] Muennighoff, Niklas, et al. “s1: Simple test-time scaling.” Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing. 2025.\n[9] Khatri, Devvrit, et al. “The art of scaling reinforcement learning compute for llms.” arXiv preprint arXiv:2510.13786 (2025).\nCitation Citation: When reproducing or citing the content of this article, please credit the original author and source.\nCited as:\nYue Shui. (Nov 2025). Scaling Laws. https://syhya.github.io/posts/2025-11-19-scaling-law/\nOr\n@article{yue_shui_scaling_laws_2025, title = {Scaling Laws}, author = {Yue Shui}, journal = {syhya.github.io}, year = {2025}, month = {November}, url = {https://syhya.github.io/posts/2025-11-19-scaling-law/} } ","wordCount":"2365","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-11-19T12:00:00+08:00","dateModified":"2025-12-03T10:00:00Z","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-11-19-scaling-law/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Scaling Laws</h1><div class=post-meta><span title='2025-11-19 12:00:00 +0800 +0800'>Created:&nbsp;2025-11-19</span>&nbsp;·&nbsp;Updated:&nbsp;2025-12-03&nbsp;·&nbsp;12 min&nbsp;·&nbsp;2365 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-11-19-scaling-law/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#scaling-laws>Scaling Laws</a><ul><li><a href=#basic-laws>Basic Laws</a></li><li><a href=#multimodal>Multimodal</a></li><li><a href=#chinchilla>Chinchilla</a></li><li><a href=#gpt-4>GPT-4</a></li></ul></li><li><a href=#rm-scaling-laws>RM Scaling Laws</a></li><li><a href=#test-time-scaling>Test-time Scaling</a><ul><li><a href=#efficiency-trade-off>Efficiency Trade-off</a></li><li><a href=#simple-test-time-scaling>Simple test-time scaling</a></li></ul></li><li><a href=#scale-rl>Scale RL</a></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><p>From the evolution of the GPT series, researchers have gradually realized that as long as model parameters, training data, and compute resources are continuously scaled up, the performance of large models improves along a stable and predictable path. This predictability is characterized by <strong>Scaling Laws</strong>, which provide the theoretical foundation and practical confidence for high-cost pre-training. As model scale, alignment techniques, and inference-time compute co-evolve, the boundaries of AI capabilities are being systematically pushed. Scaling laws are not only the foundation for building next-generation models but also a key methodology for continuously improving model capabilities under compute constraints.</p><h2 id=scaling-laws>Scaling Laws<a hidden class=anchor aria-hidden=true href=#scaling-laws>#</a></h2><p><strong>Scaling Laws</strong> (<a href=https://arxiv.org/abs/2001.08361>Kaplan et al., 2020</a>) are the foundation for understanding how Large Language Model (LLM) performance improves with resource investment. It was the first to systematically reveal the <a href=https://en.wikipedia.org/wiki/Power_law>Power-Law</a> relationship between model performance and three core factors: model parameter count $N$, dataset size $D$, and training compute $C$. A basic inverse power-law relationship can be expressed as:</p>$$
y = a \left( \frac{1}{x} \right)^p
$$<p>In a log-log plot, this relationship appears as a straight line, which is the iconic chart seen in most scaling law papers. For LLMs, <code>y</code> is typically the model&rsquo;s test loss, while <code>x</code> is a scale variable of interest (e.g., parameter count).</p><h3 id=basic-laws>Basic Laws<a hidden class=anchor aria-hidden=true href=#basic-laws>#</a></h3><figure class=align-center><a href=scaling_laws.png data-fancybox=gallery><img loading=lazy src=scaling_laws.png#center alt="Fig. 1. Language modeling performance improves smoothly with model size, dataset size, and training compute. (Image source: Kaplan et al., 2020)" width=100%></a><figcaption><p>Fig. 1. Language modeling performance improves smoothly with model size, dataset size, and training compute. (Image source: <a href=https://arxiv.org/abs/2001.08361>Kaplan et al., 2020</a>)</p></figcaption></figure><p>When the other two factors are not bottlenecks, the model&rsquo;s test loss $L$ exhibits a power-law relationship with model parameters $N$, dataset size $D$, and compute $C$, respectively. These relationships can be precisely modeled mathematically:</p><ol><li><strong>Model Size (N)</strong>: With sufficient data, loss decreases as the number of non-embedding parameters $N$ increases.</li></ol>$$
L(N)=\left(N_{\mathrm{c}} / N\right)^{\alpha_N} ; \alpha_N \sim 0.076, \quad N_{\mathrm{c}} \sim 8.8 \times 10^{13} \text { (non-embedding parameters) }
$$<ol start=2><li><strong>Dataset Size (D)</strong>: For LLMs trained to convergence (via early stopping), loss decreases as dataset size $D$ increases.</li></ol>$$
L(D)=\left(D_{\mathrm{c}} / D\right)^{\alpha_D} ; \alpha_D \sim 0.095, \quad D_{\mathrm{c}} \sim 5.4 \times 10^{13} \text { (tokens) }
$$<ol start=3><li><strong>Training Compute (C)</strong>: With optimal model size and sufficient data, loss decreases as the minimum compute $C_{\min}$ increases.</li></ol>$$
L\left(C_{\min }\right)=\left(C_{\mathrm{c}}^{\min } / C_{\min }\right)^{\alpha_C^{\min }} ; \alpha_C^{\min } \sim 0.050, \quad C_{\mathrm{c}}^{\min } \sim 3.1 \times 10^8 \text { (PF-days) }
$$<p>Here, $N_c, D_c, C_c^{\min}$ are constants, while $\alpha_N, \alpha_D, \alpha_C^{\min}$ are scaling exponents that determine the rate of performance improvement. This work also uncovered several critical findings:</p><ul><li><strong>Larger models are more sample-efficient</strong>: To reach the same level of loss, larger models require fewer training samples. When the compute budget increases, priority should be given to increasing model parameters $N$ rather than data volume $D$ (suggesting $N \propto C^{0.73}, D \propto C^{0.27}$).</li></ul><figure class=align-center><a href=scaling_law_optimal_parameters.png data-fancybox=gallery><img loading=lazy src=scaling_law_optimal_parameters.png#center alt="Fig. 2. Optimal parameters for compute-efficient training. (Image source: Kaplan et al., 2020)" width=100%></a><figcaption><p>Fig. 2. Optimal parameters for compute-efficient training. (Image source: <a href=https://arxiv.org/abs/2001.08361>Kaplan et al., 2020</a>)</p></figcaption></figure><ul><li><strong>Model shape matters little</strong>: Provided the total parameter count is fixed, specific architectural designs such as depth vs. width have a minimal impact on performance compared to scale itself.</li><li><strong>C ≈ 6ND</strong>: In Decoder-only Transformer models, training compute (FLOPs) can be estimated via a simple formula. With $N$ parameters, a forward pass processes one token using approximately $2N$ FLOPs, and the backward pass is roughly twice the forward pass. Thus, the cost to train per token is approximately $6N$ FLOPs. If the training corpus contains $D$ tokens, the total training compute is approximately $6ND$.</li></ul><h3 id=multimodal>Multimodal<a hidden class=anchor aria-hidden=true href=#multimodal>#</a></h3><p><strong>Scaling Laws for Autoregressive Generative Modeling</strong> (<a href=https://arxiv.org/abs/2010.14701>Henighan et al., 2020</a>) verified that scaling laws are not unique to language models but also apply to <strong>multimodal models</strong>. Autoregressive Transformers in image, video, and multimodal tasks all show predictable power-law performance improvements with scale.</p><figure class=align-center><a href=autoregressive_scaling_law.png data-fancybox=gallery><img loading=lazy src=autoregressive_scaling_law.png#center alt="Fig. 3. Smooth scaling of reducible loss across different domains (Image, Video, Multimodal). (Image source: Henighan et al., 2020)" width=100%></a><figcaption><p>Fig. 3. Smooth scaling of reducible loss across different domains (Image, Video, Multimodal). (Image source: <a href=https://arxiv.org/abs/2010.14701>Henighan et al., 2020</a>)</p></figcaption></figure><h3 id=chinchilla>Chinchilla<a hidden class=anchor aria-hidden=true href=#chinchilla>#</a></h3><p><strong>Chinchilla</strong> (<a href=https://arxiv.org/abs/2203.15556>Hoffmann et al., 2022</a>) revised OpenAI&rsquo;s conclusions. By training over 400 models, they found that OpenAI did not identify the true optimum due to the use of fixed learning rate schedules.</p><p>The Chinchilla scaling law states that <strong>to achieve Compute-Optimality, model parameters $N$ and training tokens $D$ should be scaled equally</strong>. That is:</p>$$ N \propto C^{0.5}, \quad D \propto C^{0.5} $$<p>This implies that under a given compute budget, most existing large models (such as GPT-3, Gopher) are <em>undertrained</em>. Chinchilla (70B), trained following this law (using the same compute as Gopher 280B but with 4x the data), outperformed Gopher across the board.</p><figure class=align-center><a href=Chinchilla.png data-fancybox=gallery><img loading=lazy src=Chinchilla.png#center alt="Fig. 4. Training curve envelope and optimal model size/token count projections. (Image source: Hoffmann et al., 2022)" width=100%></a><figcaption><p>Fig. 4. Training curve envelope and optimal model size/token count projections. (Image source: <a href=https://arxiv.org/abs/2203.15556>Hoffmann et al., 2022</a>)</p></figcaption></figure><h3 id=gpt-4>GPT-4<a hidden class=anchor aria-hidden=true href=#gpt-4>#</a></h3><p>In the <strong>GPT-4</strong> technical report (<a href=https://arxiv.org/abs/2303.08774>OpenAI, 2023</a>), OpenAI demonstrated a crucial application of Scaling Laws: <strong>predicting the final performance of large models using small models</strong>. They accurately predicted GPT-4&rsquo;s final loss using models with only 1/10,000th the compute. Their fitting formula introduces an <strong>irreducible loss</strong> $c$:</p>$$
L(C)=a C^b+c
$$<p><strong>Parameter Definitions:</strong></p><ul><li>$C$: Training compute.</li><li>$L(C)$：Model loss under compute $C$.</li><li>$a$：<strong>Scale Coefficient</strong>, controls the overall magnitude of loss reduction as compute increases.</li><li>$b$：<strong>Scaling Exponent</strong>, determines the rate of loss reduction; a larger exponent means faster reduction.</li><li>$c$：<strong>Irreducible Loss</strong>, reflecting the inherent entropy of the data, i.e., the lower bound of error that cannot be reduced regardless of compute investment.</li></ul><figure class=align-center><a href=gpt4_loss.png data-fancybox=gallery><img loading=lazy src=gpt4_loss.png#center alt="Fig. 5. GPT-4’s final loss on an internal code-token dataset aligns with a power-law trend extrapolated from smaller models when training compute is normalized. (Image source: OpenAI, 2023)" width=100%></a><figcaption><p>Fig. 5. GPT-4’s final loss on an internal code-token dataset aligns with a power-law trend extrapolated from smaller models when training compute is normalized. (Image source: <a href=https://arxiv.org/abs/2303.08774>OpenAI, 2023</a>)</p></figcaption></figure><p>In a plot with normalized training compute on the x-axis and bits-per-word on the y-axis, the loss points from small models exhibit a highly stable power-law linear relationship. The power-law curve fitted from these points (without using any GPT-4 data) falls almost precisely on GPT-4&rsquo;s final loss.</p><h2 id=rm-scaling-laws>RM Scaling Laws<a hidden class=anchor aria-hidden=true href=#rm-scaling-laws>#</a></h2><p>In the RLHF pipeline, the Reward Model (RM) acts as a proxy for human preference. However, the RM is not a perfect discriminator. <a href=https://en.wikipedia.org/wiki/Goodhart%27s_law>Goodhart&rsquo;s law</a> states: <strong>When a measure becomes a target, it ceases to be a good measure</strong>. Overoptimizing this proxy RM during LLM training leads to performance degradation on the true objective.</p><p><strong>Scaling Laws for Reward Model Overoptimization</strong> (<a href=https://arxiv.org/abs/2210.10760>Gao et al., 2023</a>) used a synthetic data setup, employing a 6B parameter &ldquo;Gold RM&rdquo; as the true reward $R$, and using different &ldquo;Proxy RMs&rdquo; ranging from 3M to 3B parameters as optimization targets.</p><figure class=align-center><a href=rm_scaling_laws.png data-fancybox=gallery><img loading=lazy src=rm_scaling_laws.png#center alt="Fig. 6. Scaling laws for reward model overoptimization for Best-of-N and RL. (Image source: Gao et al., 2023)" width=100%></a><figcaption><p>Fig. 6. Scaling laws for reward model overoptimization for Best-of-N and RL. (Image source: <a href=https://arxiv.org/abs/2210.10760>Gao et al., 2023</a>)</p></figcaption></figure><p>The KL divergence from the initial policy to the optimized policy is defined as:</p>$$
\mathrm{KL}:=D_{\mathrm{KL}}\left(\pi \| \pi_{\mathrm{init}}\right)
$$<p>And a distance function is defined as:</p>$$
d:=\sqrt{D_{\mathrm{KL}}\left(\pi \| \pi_{\mathrm{init}}\right)}
$$<p>The change in the Gold RM score $R$ follows different functional forms depending on the optimization method:</p>$$
R_{\mathrm{BoN}}(d) = d(\alpha_{\mathrm{BoN}} - \beta_{\mathrm{BoN}} d)
$$<p><strong>Best-of-N (BoN) Rejection Sampling</strong> exhibits a <strong>quadratic decay</strong> relationship (increasing then decreasing). In the early stages of optimization, the true reward increases, but after passing a certain optimal point, the reward drops significantly as optimization deepens.</p>$$
R_{\mathrm{RL}}(d) = d(\alpha_{\mathrm{RL}} - \beta_{\mathrm{RL}} \log d)
$$<p><strong>RL</strong> has a decay term of $\log d$, meaning the decline is significantly slower than BoN, exhibiting a <strong>logarithmic decay</strong> relationship.</p><figure class=align-center><a href=rm_size.png data-fancybox=gallery><img loading=lazy src=rm_size.png#center alt="Fig. 7. The values of $\alpha_{\text {bon }}$, $\beta_{\text {bon }}$ and $\beta_{\mathrm{RL}}$ in the BoN and RL overoptimization scaling laws for both proxy (dashed line) and gold (solid line) rewards as they scale with parameter count. (Image source: Gao et al., 2023)" width=100%></a><figcaption><p>Fig. 7. The values of $\alpha_{\text {bon }}$, $\beta_{\text {bon }}$ and $\beta_{\mathrm{RL}}$ in the BoN and RL overoptimization scaling laws for both proxy (dashed line) and gold (solid line) rewards as they scale with parameter count. (Image source: <a href=https://arxiv.org/abs/2210.10760>Gao et al., 2023</a>)</p></figcaption></figure><p>The coefficients $\alpha$ and $\beta$ in the above formulas describe the initial optimization efficiency and the severity of overoptimization, leading to the following conclusions:</p><ol><li><strong>RM Parameter Count is Critical</strong>: Increasing the RM parameters effectively increases $\alpha$ and decreases $\beta$, thereby mitigating overoptimization and achieving higher Gold RM peaks.</li><li><strong>RL is More Robust to Overoptimization than BoN</strong>: Measured by KL divergence, RL is slower in both optimization and overoptimization compared to BoN.</li></ol><h2 id=test-time-scaling>Test-time Scaling<a hidden class=anchor aria-hidden=true href=#test-time-scaling>#</a></h2><p>In addition to investing compute during training, model performance can also be improved by increasing computation at inference time—a paradigm known as <strong>Test-Time Scaling</strong> (<a href=https://arxiv.org/abs/2408.03314>Snell et al., 2024</a>). OpenAI demonstrated this phenomenon in <strong>o1</strong> (<a href=https://arxiv.org/abs/2502.06807>OpenAI, 2025</a>): scaling both <strong>train-time compute</strong> and <strong>test-time compute</strong> consistently improves model performance, particularly on complex reasoning tasks such as mathematics and programming.</p><figure class=align-center><a href=o1_reasoning.png data-fancybox=gallery><img loading=lazy src=o1_reasoning.png#center alt="Fig. 8. Additional RL training and additional test-time compute improves competitive mathematics performance. (Image source: OpenAI, 2025)" width=100%></a><figcaption><p>Fig. 8. Additional RL training and additional test-time compute improves competitive mathematics performance. (Image source: <a href=https://arxiv.org/abs/2502.06807>OpenAI, 2025</a>)</p></figcaption></figure><ul><li><strong>Parallel Sampling</strong>: e.g., Best-of-N or Majority Voting. The model generates multiple independent samples for the same question, and the best answer is selected via a verifier or voting.</li><li><strong>Sequential Refinement</strong>: e.g., Self-Correction. The model iteratively improves based on the previous output.</li></ul><h3 id=efficiency-trade-off>Efficiency Trade-off<a hidden class=anchor aria-hidden=true href=#efficiency-trade-off>#</a></h3><figure class=align-center><a href=test_time_scaling.png data-fancybox=gallery><img loading=lazy src=test_time_scaling.png#center alt="Fig. 9. Compute-Optimal Scaling for Iterative Self-Refinement and Search: Evaluating Efficiency Against Best-of-N Baselines and Analyzing the Trade-offs Between Test-Time Compute and Pretraining Scaling (Image source: Snell et al., 2024)" width=100%></a><figcaption><p>Fig. 9. Compute-Optimal Scaling for Iterative Self-Refinement and Search: Evaluating Efficiency Against Best-of-N Baselines and Analyzing the Trade-offs Between Test-Time Compute and Pretraining Scaling (Image source: <a href=https://arxiv.org/abs/2408.03314>Snell et al., 2024</a>)</p></figcaption></figure><p>From the figure above, we can draw the following conclusions:</p><ol><li><p><strong>The optimal strategy depends on problem difficulty, and there is a limit to returns</strong>: Although compute-optimal strategies are generally more efficient than the Best-of-N baseline (saving ~4x compute), their specific efficacy is highly dependent on task complexity.</p><ul><li><strong>Easy Problems:</strong> The model&rsquo;s initial answer is usually directionally correct and only needs local fine-tuning. In this case, sequential refinement is the most efficient strategy, quickly fixing details with minimal compute cost.</li><li><strong>Hard Problems:</strong> Although parallel search can explore more paths, as problem difficulty increases, the return on simply increasing test-time inference (whether search or refinement) rapidly shows <strong>diminishing marginal returns</strong>. For extremely hard problems, test-time compute struggles to break through the ceiling of the model&rsquo;s capabilities.</li></ul></li><li><p><strong>Test-time compute cannot fully replace pre-training; base capability remains fundamental</strong>: Test-time Compute and Pretraining Compute are not a 1:1 equivalent exchange.</p><ul><li><strong>Prerequisite for Exchange ($Y \ll X$):</strong> Increasing inference time is only cost-effective when the inference token budget ($Y$) is far smaller than the pre-training token budget ($X$), and the problems are of low-to-medium difficulty.</li><li><strong>Irreplaceability of Base Models:</strong> Once the problem becomes hard or inference demands are too high, the reasoning tricks (search/refinement) of small models cannot bridge the huge capability gap. The broad knowledge and strong reasoning capabilities provided by scaling up pre-training remain the decisive foundation for solving complex problems.</li></ul></li></ol><h3 id=simple-test-time-scaling>Simple test-time scaling<a hidden class=anchor aria-hidden=true href=#simple-test-time-scaling>#</a></h3><p><strong>s1: Simple test-time scaling</strong> (<a href=https://arxiv.org/abs/2501.19393>Muennighoff et al., 2025</a>) curated the <a href=https://huggingface.co/datasets/simplescaling/s1K>s1K</a> dataset containing 1,000 questions paired with reasoning traces. They then performed supervised fine-tuning (SFT) using Qwen2.5-32B-Instruct as the base model, employing a <strong>Budget Forcing</strong> method to control the length of output tokens to study test-time scaling.</p><figure class=align-center><a href=s1_sample.png data-fancybox=gallery><img loading=lazy src=s1_sample.png#center alt="Fig. 10. Budget forcing in s1-32B: suppressing the end-of-thinking delimiter prompts the model to continue after “&mldr;is 2.”, triggering a self-correction via “Wait”. (Image source: Muennighoff et al., 2025)" width=100%></a><figcaption><p>Fig. 10. Budget forcing in s1-32B: suppressing the end-of-thinking delimiter prompts the model to continue after “&mldr;is 2.”, triggering a self-correction via “Wait”. (Image source: <a href=https://arxiv.org/abs/2501.19393>Muennighoff et al., 2025</a>)</p></figcaption></figure><ul><li><strong>Lengthening</strong>: When the model attempts to end thinking, suppress the end token and force append words like <code>Wait</code> to encourage the model to reflect and double-check.</li><li><strong>Shortening</strong>: When the thinking process exceeds a preset token length, force append end tokens (e.g., <code>&lt;/think></code> or <code>"Final Answer:"</code>) to prompt the model to output a conclusion immediately.</li></ul><figure class=align-center><a href=s1_result.png data-fancybox=gallery><img loading=lazy src=s1_result.png#center alt="Fig. 11. Sequential and parallel test-time scaling. (Image source: Muennighoff et al., 2025)" width=100%></a><figcaption><p>Fig. 11. Sequential and parallel test-time scaling. (Image source: <a href=https://arxiv.org/abs/2501.19393>Muennighoff et al., 2025</a>)</p></figcaption></figure><p>The results above show that thinking time (Token count) extended via Budget Forcing in both parallel and sequential scaling methods is significantly positively correlated with downstream task accuracy.</p><h2 id=scale-rl>Scale RL<a hidden class=anchor aria-hidden=true href=#scale-rl>#</a></h2><p><strong>ScaleRL</strong> (<a href=https://arxiv.org/abs/2510.13786>Khatri et al., 2025</a>) proposes a systematic method for analyzing and predicting RL scaling in LLMs. Unlike the common power-law relationship in pre-training, the performance of RL training (e.g., expected reward $R_C$ on the validation set) follows a <strong>Sigmoidal saturation curve</strong> as compute $C$ increases. This curve can be described by the following formula:</p>$$
\overbrace{R_C-R_0}^{\text {Reward Gain }}=\overbrace{\left(A-R_0\right)}^{\text {Asymptotic Reward Gain }} \times \frac{1}{\underbrace{1+\left(C_{\text {mid }} / C\right)^B}_{\text {Compute Efficiency }}}
$$<p><strong>Parameter Definitions:</strong></p><ul><li>$R_C$: Expected reward (or Pass Rate) under compute $C$.</li><li>$R_0$: Initial model performance.</li><li>$A$: <strong>Asymptotic Performance</strong>. Represents the theoretical maximum performance the model can achieve under infinite compute.</li><li>$B$: <strong>Scaling Exponent (Efficiency)</strong>. Controls the steepness of the curve; a larger value means the model reaches the performance ceiling faster.</li><li>$C_{mid}$: The compute required to achieve half of the total gain.</li></ul><figure class=align-center><a href=rl_scaling_fit.png data-fancybox=gallery><img loading=lazy src=rl_scaling_fit.png#center alt="Fig. 12. Interpretation of the sigmoidal scaling curve for RL. (Image source: Khatri et al., 2025)" width=100%></a><figcaption><p>Fig. 12. Interpretation of the sigmoidal scaling curve for RL. (Image source: <a href=https://arxiv.org/abs/2510.13786>Khatri et al., 2025</a>)</p></figcaption></figure><p>The figure above demonstrates the value of this framework: It allows researchers to fit the curve using early training data at a smaller compute scale, thereby <strong>predicting</strong> the final performance and efficiency of an RL recipe under a larger compute budget, reducing the cost of algorithm exploration.</p><p>Through large-scale ablation experiments on various RL design choices (such as loss functions, advantage normalization, data curriculum, etc.), several key principles were summarized:</p><ol><li><strong>Performance ceilings are not universal</strong>: Different RL algorithms (e.g., GRPO, DAPO, CISPO) have different asymptotic performance limits $A$. Choosing the correct loss function is critical.</li><li><strong>Decoupling of Efficiency and Ceiling</strong>: Many common RL tricks, such as advantage normalization, data curriculum, and length penalties, mainly affect compute efficiency ($B$ and $C_{\text{mid}}$), but have little impact on the performance ceiling $A$.</li><li><strong>Stable and Predictable Scaling</strong>: A carefully designed, stable RL recipe follows a predictable sigmoidal trajectory. Even when scaled to the level of 100,000 GPU hours, its performance remains highly consistent with the curve fitted from early data.</li></ol><figure class=align-center><a href=scale_rl_result.png data-fancybox=gallery><img loading=lazy src=scale_rl_result.png#center alt="Fig. 13. ScaleRL demonstrates more scalable performance compared to other prevalent RL methods. (Image source: Khatri et al., 2025)" width=90%></a><figcaption><p>Fig. 13. ScaleRL demonstrates more scalable performance compared to other prevalent RL methods. (Image source: <a href=https://arxiv.org/abs/2510.13786>Khatri et al., 2025</a>)</p></figcaption></figure><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Kaplan, Jared, et al. <a href=https://arxiv.org/abs/2001.08361>&ldquo;Scaling laws for neural language models.&rdquo;</a> arXiv preprint arXiv:2001.08361 (2020).</p><p>[2] Henighan, Tom, et al. <a href=https://arxiv.org/abs/2010.14701>&ldquo;Scaling laws for autoregressive generative modeling.&rdquo;</a> arXiv preprint arXiv:2010.14701 (2020).</p><p>[3] Hoffmann, Jordan, et al. <a href=https://arxiv.org/abs/2203.15556>&ldquo;Training compute-optimal large language models.&rdquo;</a> arXiv preprint arXiv:2203.15556 (2022).</p><p>[4] Achiam, Josh, et al. <a href=https://arxiv.org/abs/2303.08774>&ldquo;Gpt-4 technical report.&rdquo;</a> arXiv preprint arXiv:2303.08774 (2023).</p><p>[5] Gao, Leo, John Schulman, and Jacob Hilton. <a href=https://arxiv.org/abs/2210.10760>&ldquo;Scaling laws for reward model overoptimization.&rdquo;</a> International Conference on Machine Learning. PMLR, 2023.</p><p>[6] Snell, Charlie, et al. <a href=https://arxiv.org/abs/2408.03314>&ldquo;Scaling llm test-time compute optimally can be more effective than scaling model parameters.&rdquo;</a> arXiv preprint arXiv:2408.03314 (2024).</p><p>[7] El-Kishky, Ahmed, et al. <a href=https://arxiv.org/abs/2502.06807>&ldquo;Competitive programming with large reasoning models.&rdquo;</a> arXiv preprint arXiv:2502.06807 (2025).</p><p>[8] Muennighoff, Niklas, et al. <a href=https://arxiv.org/abs/2501.19393>&ldquo;s1: Simple test-time scaling.&rdquo;</a> Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing. 2025.</p><p>[9] Khatri, Devvrit, et al. <a href=https://arxiv.org/abs/2510.13786>&ldquo;The art of scaling reinforcement learning compute for llms.&rdquo;</a> arXiv preprint arXiv:2510.13786 (2025).</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reproducing or citing the content of this article, please credit the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Nov 2025). Scaling Laws.
<a href=https://syhya.github.io/posts/2025-11-19-scaling-law/>https://syhya.github.io/posts/2025-11-19-scaling-law/</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>yue_shui_scaling_laws_2025</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>{Scaling Laws}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>{Yue Shui}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>{syhya.github.io}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>{November}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>{https://syhya.github.io/posts/2025-11-19-scaling-law/}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/tags/scaling-laws/>Scaling Laws</a></li><li><a href=https://syhya.github.io/tags/test-time-compute/>Test-Time Compute</a></li><li><a href=https://syhya.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://syhya.github.io/tags/reward-model/>Reward Model</a></li><li><a href=https://syhya.github.io/tags/compute-optimal/>Compute-Optimal</a></li></ul><nav class=paginav><a class=next href=https://syhya.github.io/posts/2025-09-30-agentic-rl/><span class=title>Next »</span><br><span>Agentic RL</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on x" href="https://x.com/intent/tweet/?text=Scaling%20Laws&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-11-19-scaling-law%2f&amp;hashtags=DeepLearning%2cAI%2cLLM%2cScalingLaws%2cTest-TimeCompute%2cReinforcementLearning%2cRewardModel%2cCompute-Optimal"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-11-19-scaling-law%2f&amp;title=Scaling%20Laws&amp;summary=Scaling%20Laws&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-11-19-scaling-law%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-11-19-scaling-law%2f&title=Scaling%20Laws"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-11-19-scaling-law%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on whatsapp" href="https://api.whatsapp.com/send?text=Scaling%20Laws%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-11-19-scaling-law%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on telegram" href="https://telegram.me/share/url?text=Scaling%20Laws&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-11-19-scaling-law%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on ycombinator" href="https://news.ycombinator.com/submitlink?t=Scaling%20Laws&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-11-19-scaling-law%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>