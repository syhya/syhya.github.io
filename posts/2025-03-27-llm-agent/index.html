<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Large Language Model Agents | Yue Shui Blog</title><meta name=keywords content="Large Language Model,AI,Agent,Reinforcement Learning,Planning,Memory,Tool Use,Deep Research,ReAct,Reflexion,WebVoyager,OpenAI Operator,CoT,ToT,Workflow"><meta name=description content="Agents
Since the release of ChatGPT by OpenAI in October 2022, and with the emergence of subsequent projects like AutoGPT and AgentGPT, LLM-related agents have become a research hotspot and a practical application direction in AI in recent years. This article will introduce the basic concepts, core technologies, and latest application progress of agents.
LLM Agent
A Large Language Model Agent (LLM agent) utilizes an LLM as its core &ldquo;brain&rdquo; and combines modules like planning, memory, and external tools to automate the execution of complex tasks."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-03-27-llm-agent/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-03-27-llm-agent/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-03-27-llm-agent/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-03-27-llm-agent/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Large Language Model Agents"><meta property="og:description" content="Agents Since the release of ChatGPT by OpenAI in October 2022, and with the emergence of subsequent projects like AutoGPT and AgentGPT, LLM-related agents have become a research hotspot and a practical application direction in AI in recent years. This article will introduce the basic concepts, core technologies, and latest application progress of agents.
LLM Agent A Large Language Model Agent (LLM agent) utilizes an LLM as its core “brain” and combines modules like planning, memory, and external tools to automate the execution of complex tasks."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-27T10:00:00+00:00"><meta property="article:modified_time" content="2025-09-02T22:00:47+08:00"><meta property="article:tag" content="Large Language Model"><meta property="article:tag" content="AI"><meta property="article:tag" content="Agent"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Planning"><meta property="article:tag" content="Memory"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Large Language Model Agents"><meta name=twitter:description content="Agents
Since the release of ChatGPT by OpenAI in October 2022, and with the emergence of subsequent projects like AutoGPT and AgentGPT, LLM-related agents have become a research hotspot and a practical application direction in AI in recent years. This article will introduce the basic concepts, core technologies, and latest application progress of agents.
LLM Agent
A Large Language Model Agent (LLM agent) utilizes an LLM as its core &ldquo;brain&rdquo; and combines modules like planning, memory, and external tools to automate the execution of complex tasks."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Large Language Model Agents","item":"https://syhya.github.io/posts/2025-03-27-llm-agent/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Large Language Model Agents","name":"Large Language Model Agents","description":"Agents Since the release of ChatGPT by OpenAI in October 2022, and with the emergence of subsequent projects like AutoGPT and AgentGPT, LLM-related agents have become a research hotspot and a practical application direction in AI in recent years. This article will introduce the basic concepts, core technologies, and latest application progress of agents.\nLLM Agent A Large Language Model Agent (LLM agent) utilizes an LLM as its core \u0026ldquo;brain\u0026rdquo; and combines modules like planning, memory, and external tools to automate the execution of complex tasks.\n","keywords":["Large Language Model","AI","Agent","Reinforcement Learning","Planning","Memory","Tool Use","Deep Research","ReAct","Reflexion","WebVoyager","OpenAI Operator","CoT","ToT","Workflow"],"articleBody":"Agents Since the release of ChatGPT by OpenAI in October 2022, and with the emergence of subsequent projects like AutoGPT and AgentGPT, LLM-related agents have become a research hotspot and a practical application direction in AI in recent years. This article will introduce the basic concepts, core technologies, and latest application progress of agents.\nLLM Agent A Large Language Model Agent (LLM agent) utilizes an LLM as its core “brain” and combines modules like planning, memory, and external tools to automate the execution of complex tasks.\nUser Request: The user interacts with the agent by inputting tasks through prompts. Agent: The system’s brain, composed of one or more LLMs, responsible for overall coordination and task execution. Planning: Decomposes complex tasks into smaller sub-tasks, formulates an execution plan, and continuously optimizes the results through reflection. Memory: Includes short-term memory (capturing task information in real-time using in-context learning) and long-term memory (using external vector stores to save and retrieve key information, ensuring information continuity for long-running tasks). Tools: Integrates external tools such as calculators, web search, and code interpreters to call external data, execute code, and obtain the latest information. Fig. 1. The illustration of LLM Agent Framework. (Image source: DAIR.AI, 2024)\nRL Agent The goal of Reinforcement Learning (RL) is to train an agent to take a series of actions ($a_t$) in a given environment. During interaction, the agent transitions from one state ($s_t$) to the next and receives a reward ($r_t$) from the environment after each action. This interaction generates a complete trajectory ($\\tau$), typically represented as:\n$$ \\tau = \\{(s_0, a_0, r_0), (s_1, a_1, r_1), \\dots, (s_T, a_T, r_T)\\}. $$The agent’s objective is to learn a policy ($\\pi$), which is a rule for selecting actions in each state, to maximize the expected cumulative reward, usually expressed as:\n$$ \\max_{\\pi} \\, \\mathbb{E}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right], $$ where $\\gamma \\in [0,1]$ is the discount factor, used to balance short-term and long-term rewards.\nFig. 2. The agent-environment interaction. (Image source: Sutton \u0026 Barto, 2018)\nIn the context of LLMs, the model can be viewed as an agent, and the “environment” can be understood as the user’s input and the desired response format:\nState ($s_t$): Can be the current dialogue context or the user’s question. Action ($a_t$): The text output by the model (answer, generated content, etc.). Reward ($r_t$): Feedback from the user or the system (e.g., user satisfaction, automatic scoring by a reward model). Trajectory ($\\tau$): The entire sequence of text interactions from the beginning to the end of a conversation, which can be used to evaluate the model’s overall performance. Policy ($\\pi$): The rule by which the LLM generates text in each state (dialogue context), typically determined by the model’s parameters. For LLMs, they are traditionally pre-trained on massive offline datasets. In the post-training reinforcement learning phase, the model is trained with feedback from humans or other models to produce high-quality text that better aligns with human preferences or task requirements.\nComparison The table below shows the differences between the two:\nComparison Dimension LLM Agent RL Agent Core Principle Automates complex tasks through planning, memory, and tools. Continuously optimizes its policy to maximize long-term rewards through a trial-and-error feedback loop with the environment. Optimization Method Does not directly update model parameters. Performance is improved mainly through context extension, external memory, and tools. Continuously and frequently updates policy model parameters, relying on reward signals from the environment for optimization. Interaction Method Interacts with users or external systems using natural language, flexibly calling various tools to obtain external information. Interacts with a real or simulated environment, which provides rewards or penalties, forming a closed feedback loop. Objective Decomposes complex tasks and completes them with the help of external resources, focusing on the quality and accuracy of the task outcome. Maximizes long-term rewards, seeking an optimal balance between short-term and long-term returns. As research progresses, the combination of LLM and RL agents presents more possibilities, such as:\nUsing reinforcement learning methods to train Reasoning LLMs (like o1/o3), making them more suitable as base models for LLM agents. Simultaneously, recording the data and feedback from LLM agents executing tasks to provide rich training data for Reasoning LLMs, thereby enhancing model performance. Prompt Engineering Prompt Engineering, also known as In-Context Prompting, is the technique of optimizing input prompts to guide an LLM to produce the desired output. Its core objective is to control the model’s behavior through effective communication without updating the model’s weights.\nZero-Shot Prompting Zero-Shot Prompting directly provides the model with task instructions without any examples. This method relies entirely on the knowledge and instruction-following capabilities the model learned during its pre-training phase. For example, for sentiment analysis:\nFig. 3. Zero-Shot Prompting.\nFor models that have undergone instruction fine-tuning, such as GPT-5 or Claude 4, they can understand and execute these direct instructions very well.\nFew-Shot Prompting Few-Shot Prompting provides a set of high-quality examples in the prompt, with each example containing an input and the desired output. Through these examples, the model can better understand the user’s intent and the specific requirements of the task, leading to better performance than zero-shot prompting. However, a drawback of this method is that it consumes more of the context window length. For example, providing a few sentiment analysis examples:\nFig. 4. Few-Shot Prompting.\nAutomatic Prompt Construction Automatic Prompt Engineer (APE) (Zhou et al. 2022) is a method that searches through a pool of candidate instructions generated by the model. It filters the candidate set and ultimately selects the highest-scoring instruction based on a chosen scoring function.\nFig. 5. Automatic Prompt Engineer (APE) workflow. (Image source: Zhou et al. 2022)\nAutomatic Chain-of-Thought (Auto-CoT) (Zhang et al. 2022) proposes a method for automatically constructing Chain-of-Thought examples, aiming to solve the problem of manual prompt design being time-consuming and potentially suboptimal. Its core idea is to sample questions through clustering techniques and then leverage the LLM’s own zero-shot reasoning capabilities to automatically generate reasoning chains, thereby constructing diverse, high-quality examples.\nFig. 6. Overview of the Auto-CoT method. (Image source: Zhang et al. 2022)\nAuto-CoT consists of two main stages:\nQuestion Clustering: Embeds the questions in the dataset and runs an algorithm like $k$-means for clustering. This step aims to group similar questions into the same cluster to ensure the diversity of subsequently sampled questions. Demonstration Selection \u0026 Rationale Generation: Selects one or more representative questions from each cluster (e.g., choosing the question closest to the cluster centroid). Then, it uses a Zero-Shot CoT prompt to have the LLM generate reasoning chains for these selected questions. These automatically generated “question-rationale” pairs form the final few-shot prompt used for task execution. Knowledge Enhancement When dealing with knowledge-intensive or commonsense reasoning tasks, relying solely on the LLM’s parametric knowledge is often insufficient and can lead to incorrect or outdated answers. To address this issue, researchers have proposed two main approaches:\nGenerated Knowledge Prompting (Liu et al. 2022) is a method where the model is first prompted to generate relevant knowledge before making a prediction. The core idea is that when a task requires commonsense or external information, the model may make mistakes due to a lack of context. If the model is first guided to generate knowledge related to the input and then answers based on that knowledge, the accuracy of its reasoning can be improved.\nFig. 7. Overview of the Generated Knowledge Prompting. (Image source: Liu et al. 2022)\nKnowledge Generation: Based on the input, the model first generates relevant factual knowledge. Knowledge Integration: The generated knowledge is combined with the original question to form a new prompt. Answer Prediction: The model provides an answer based on the enhanced input. Retrieval Augmented Generation (RAG) (Lewis et al. 2021) is a method that combines information retrieval with text generation to tackle knowledge-intensive tasks. Its core idea is that relying solely on an LLM’s parametric (static) knowledge can easily lead to factual errors. By introducing retrieval from external knowledge bases, the factual consistency and timeliness of the generated results can be improved.\nFig. 8. Overview of the Retrieval Augmented Generation. (Image source: Lewis et al. 2021)\nRetrieval: Retrieves relevant documents from an external knowledge source (e.g., Wikipedia or a private knowledge base). Augmentation: Concatenates the retrieved documents with the original input to serve as the prompt context. Generation: A generation model (the original paper used a pre-trained seq2seq model, but today LLMs are mainstream) outputs the answer based on the augmented prompt. Multimodal Chain-of-Thought Prompting Multimodal CoT Prompting (MCoT) (Zhang et al. 2023) integrates text and visual information into the reasoning process, breaking the limitation of traditional CoT, which relies solely on the language modality. Its framework is divided into two stages:\nRationale Generation: Generates an explanatory reasoning chain based on multimodal information (text + image). Answer Inference: Uses the generated rationale as an aid to infer the final answer. Fig. 9. Overview of our Multimodal-CoT framework. (Image source: Zhang et al. 2023)\nActive Prompt Active Prompt (Diao et al. 2023) addresses the limitation of traditional CoT methods that rely on a fixed set of manually annotated examples. The problem is that fixed examples are not necessarily optimal for all tasks, which can lead to poor generalization. Active Prompt introduces an active learning strategy to adaptively select and update the best task-relevant examples, thereby improving the model’s reasoning performance.\nFig. 10. Illustrations of active prompting framework. (Image source: Diao et al. 2023)\nUncertainty Estimation: With or without a small number of manual CoT examples, the LLM generates k answers for training questions (in the paper, k=5), and an uncertainty metric is calculated based on the variance of these answers. Selection: Based on the uncertainty level, the most uncertain questions are selected. Annotation: The selected questions are manually annotated to provide new, high-quality CoT examples. Inference: The newly annotated examples are used for inference, improving the model’s performance on the target task. Planning: Task Decomposition The core components of an LLM Agent include planning, memory, and tool use. These components work together to enable the agent to autonomously execute complex tasks.\nFig. 11. Overview of a LLM-powered autonomous agent system. (Image source: Weng, 2017)\nPlanning is crucial for the successful execution of complex tasks. It can be approached in different ways depending on the complexity and the need for iterative refinement. In simple scenarios, the planning module can use an LLM to outline a detailed plan in advance, including all necessary sub-tasks. This step ensures that the agent systematically performs task decomposition and follows a clear logical flow from the outset.\nChain of Thought Chain of Thought (CoT) (Wei et al. 2022) works by generating a series of short sentences step-by-step to describe the reasoning process; these sentences are called reasoning steps. Its purpose is to explicitly show the model’s reasoning path, helping it better handle complex reasoning tasks. The figure below shows the difference between few-shot prompting (left) and CoT prompting (right). The few-shot prompt leads to an incorrect answer, while the CoT method guides the model to state its reasoning process step-by-step, more clearly reflecting the model’s logical process and thus improving the answer’s accuracy and interpretability.\nFig. 12. The comparison example of few-shot prompting and CoT prompting. (Image source: Wei et al. 2022)\nZero-Shot CoT (Kojima et al. 2022) is a follow-up to CoT that proposes an extremely simple zero-shot prompting method. They found that by simply appending the phrase Let's think step by step to the end of the question, the LLM can produce a chain of thought, leading to more accurate answers.\nFig. 13. The comparison example of few-shot prompting and CoT prompting. (Image source: Kojima et al. 2022)\nSelf-Consistency Sampling Self-consistency sampling (Wang et al. 2022a) generates multiple diverse answers by sampling multiple times from the same prompt with a temperature \u003e 0 and then selecting the best answer from the set. The core idea is to improve the final answer’s accuracy and robustness by sampling multiple reasoning paths and then taking a majority vote. The criteria for selecting the best answer can vary for different tasks, but majority voting is generally used as a universal solution. For tasks that are easy to verify, such as programming problems, the answers can be validated by running them through an interpreter and using unit tests. This is an optimization of CoT, and when used in combination, it can significantly improve the model’s performance on complex reasoning tasks.\nFig. 14. Overview of the Self-Consistency Method for Chain-of-Thought Reasoning. (Image source: Wang et al. 2022a)\nHere are some subsequent optimization works:\n(Wang et al. 2022b) later used another ensemble learning method for optimization, increasing randomness by changing the order of examples or replacing human-written reasoning with model-generated ones, followed by majority voting. Fig. 15. An overview of different ways of composing rationale-augmented ensembles, depending on how the randomness of rationales is introduced. (Image source: Wang et al. 2022b)\nIf training samples only provide the correct answer without the reasoning, the STaR (Self-Taught Reasoner) (Zelikman et al. 2022) method can be used: (1) Have the LLM generate reasoning chains, and keep only the reasoning for answers that are correct. (2) Fine-tune the model with the generated reasoning, iterating until convergence. Note that a high temperature can easily lead to results with the correct answer but incorrect reasoning. If there is no ground truth, majority voting can be considered the “correct answer.” Fig. 16. An overview of STaR and a STaR-generated rationale on CommonsenseQA. (Image source: Zelikman et al. 2022)\n(Fu et al. 2023) found that more complex examples (with more reasoning steps) can improve model performance. When separating reasoning steps, a newline character \\n works better than step i, ., or ;. Additionally, using a complexity-based consistency strategy, which only performs majority voting on the top-$k$ most complex generated reasoning chains, can further optimize the model’s output. It was also shown that replacing Q: with Question: in the prompt provides an additional performance boost. Fig. 17. Sensitivity analysis on step formatting. Complex prompts consistently lead to better performance with regard to different step formatting. (Image source: Fu et al. 2023)\nTree of Thoughts Tree of Thoughts (ToT) (Yao et al. 2023) expands on CoT by exploring multiple reasoning possibilities at each step. It first decomposes a problem into multiple thought steps and generates several different ideas at each step, forming a tree structure. The search process can use Breadth-First Search (BFS) or Depth-First Search (DFS), and each state is evaluated by a classifier (or by having the LLM score it) or through majority voting. It involves three main steps:\nExpand: Generate one or more candidate solutions. Score: Measure the quality of the candidate solutions. Prune: Keep the top-$k$ best candidate solutions. If no solution is found (or if the quality of the candidates is not high enough), the process backtracks to the expansion step.\nFig. 18. Schematic illustrating various approaches to problem solving with LLMs. (Image source: Yao et al. 2023)\nPlanning: Self-Reflexion Self-Reflexion is a key factor that enables an agent to achieve iterative improvement by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\nReAct The ReAct (Reason + Act) (Yao et al. 2023) framework achieves seamless integration of reasoning and acting in LLMs by combining task-specific discrete actions with the language space. This design not only allows the model to interact with the environment by calling external interfaces like the Wikipedia search API but also to generate detailed reasoning trajectories in natural language to solve complex problems.\nThe ReAct prompt template includes explicit thought steps, with the basic format as follows:\nThought: ... Action: ... Observation: ... ...(Repeated many times) Fig. 19. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023)\nAs seen in the figure below, ReAct significantly outperforms the baseline Action-only method in both knowledge-intensive and decision-making tasks, demonstrating its advantages in enhancing reasoning effectiveness and interactive performance.\nFig. 20. PaLM-540B prompting results on HotpotQA and Fever. (Image source: Yao et al. 2023)\nReflexion Reflexion (Shinn et al. 2023) enables an LLM to iteratively improve and optimize its decisions through self-feedback and dynamic memory.\nThis method essentially borrows ideas from reinforcement learning. In the traditional Actor-Critic model, the Actor selects an action $a_t$ based on the current state $s_t$, while the Critic provides an evaluation (e.g., a value function $V(s_t)$ or an action-value function $Q(s_t, a_t)$) and gives feedback to the Actor for policy optimization. Correspondingly, in Reflexion’s three main components:\nActor: Played by the LLM, it outputs text and corresponding actions based on the environment’s state (including context and historical information). This can be denoted as:\n$$ a_t = \\pi_\\theta(s_t), $$where $\\pi_\\theta$ represents the policy based on parameters $\\theta$ (i.e., the LLM’s weights or prompt). The Actor interacts with the environment and produces a trajectory $\\tau = {(s_1,a_1,r_1), \\dots, (s_T,a_T,r_T)}$.\nEvaluator: Similar to the Critic, the Evaluator receives the trajectory generated by the Actor and outputs a reward signal $r_t$. In the Reflexion framework, the Evaluator can analyze the trajectory using pre-designed heuristic rules or an additional LLM to generate rewards. For example:\n$$ r_t = R(\\tau_t), $$where $R(\\cdot)$ is the reward function based on the current trajectory $\\tau_t$.\nSelf-Reflection: This module adds a self-regulating feedback mechanism on top of the Actor-Critic model. It integrates the current trajectory $\\tau$, reward signals ${r_t}$, and historical experience from long-term memory, using its language generation capabilities to produce self-improvement suggestions for the next decision. This feedback is then written to external memory, providing richer context for the Actor’s subsequent decisions, thus achieving iterative optimization similar to updating policy parameters $\\theta$ through dynamic adjustment of the prompt, without updating the LLM’s internal parameters.\nFig. 21. (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm. (Image source: Shinn et al. 2023)\nThe core loop and algorithm of Reflexion are described as follows:\nInitialization\nInstantiate the Actor, Evaluator, and Self-Reflection models simultaneously (all can be implemented by LLMs), denoted as $M_a, M_e, M_{sr}$ respectively. Initialize the policy $\\pi_\\theta$ (including the Actor’s model parameters or prompt, and initial memory). The Actor first generates an initial trajectory $\\tau_0$ according to the current policy $\\pi_\\theta$. After evaluation by $M_e$, $M_{sr}$ generates the first self-reflection text and stores it in long-term memory. Generate Trajectory\nIn each iteration, $M_a$ reads the current long-term memory and environmental observations, sequentially outputting actions ${a_1, a_2, \\ldots}$, interacting with the environment, and receiving corresponding feedback to form a new trajectory $\\tau_t$. $\\tau_t$ can be considered the short-term memory for this task, used only in the current iteration. Evaluation\n$M_e$ outputs rewards or scores ${r_1, r_2, \\ldots}$ based on the trajectory $\\tau_t$ (i.e., the sequence of the Actor’s behaviors and environmental feedback). This step corresponds to internal feedback from $M_e$ or results directly provided by the external environment. Self-Reflection\nThe $M_{sr}$ module synthesizes the trajectory $\\tau_t$ and reward signals ${r_t}$ to generate self-correction or improvement suggestions $\\mathrm{sr}_t$ at the language level. The reflection text can be seen as an analysis of errors or as providing new inspirational ideas, and it is stored in long-term memory. In practice, we can vectorize the feedback information and store it in a vector database. Update and Repeat\nAfter appending the latest self-reflection text $\\mathrm{sr}_t$ to long-term memory, the Actor can use RAG to retrieve relevant historical information in the next iteration to adjust its policy. Repeat the above steps until $M_e$ determines the task is completed or the maximum number of rounds is reached. In this loop, Reflexion relies on the continuous accumulation of self-reflection + long-term memory to improve decision-making, rather than directly modifying model parameters. Below are examples of Reflexion applied to decision-making, programming, and reasoning tasks:\nFig. 22. Reflexion works on decision-making, programming, and reasoning tasks. (Image source: Shinn et al. 2023)\nIn an experiment with 100 HotPotQA questions, a comparison between the CoT method and a method with episodic memory showed that the Reflexion method, with an added self-reflection step at the end, significantly improved its search, information retrieval, and reasoning capabilities.\nFig. 23. Comparative Analysis of Chain-of-Thought (CoT) and ReAct on the HotPotQA Benchmark. (Image source: Shinn et al. 2023)\nDeepSeek R1 DeepSeek-R1 (DeepSeek-AI, 2025) represents a major breakthrough in the open-source community’s efforts to replicate OpenAI’s o1 (OpenAI, 2024), successfully training an advanced reasoning model with deep reflection capabilities through reinforcement learning techniques.\nFor a detailed training process and technical implementation of DeepSeek R1, please refer to my previous blog post: Progress in Replicating OpenAI o1: DeepSeek-R1.\nA key transformation during the training of DeepSeek-R1-Zero is that as training progresses, the model gradually emerges with an outstanding self-evolution capability. This capability is manifested in three core aspects:\nSelf-Reflection: The model can look back and critically evaluate previous reasoning steps. Proactive Exploration: When it finds the current problem-solving path to be suboptimal, it can autonomously search for and try alternative solutions. Dynamic Thought Adjustment: It adaptively adjusts the number of generated tokens based on the complexity of the problem, achieving a deeper thought process. This dynamic and spontaneous reasoning behavior significantly enhances the model’s ability to solve complex problems, enabling it to tackle challenging tasks more efficiently and accurately.\nFig. 24. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)\nA typical “aha moment” also emerged during the training of DeepSeek-R1-Zero. At this critical stage, the model suddenly realized during the reasoning process that its previous line of thought was flawed, and it quickly adjusted its thinking direction, ultimately leading to the correct answer. This phenomenon strongly demonstrates that the model has developed powerful self-correction and reflection capabilities in its reasoning process, similar to the “aha” experience in human thinking.\nFig. 25. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: DeepSeek-AI, 2025)\nMemory Human Memory Memory refers to the process of acquiring, storing, retaining, and retrieving information. Human memory is primarily divided into the following three categories:\nFig. 26. Categorization of human memory. (Image source: Weng, 2017)\nSensory Memory: Used to briefly retain sensory information after the original stimulus (visual, auditory, tactile, etc.) has disappeared, typically lasting for milliseconds or seconds. Sensory memory is further divided into:\nIconic Memory: The transient image or visual impression retained by the visual channel, generally lasting 0.25–0.5 seconds, used to form visual continuity in video or animation scenes. Echoic Memory: The brief storage of auditory information, which can last for several seconds, allowing a person to replay recently heard sentences or sound clips. Haptic Memory: Used to retain brief tactile or force information, generally lasting from milliseconds to seconds, such as the brief finger sensations when typing on a keyboard or reading Braille. Short-Term Memory: Stores the information we are currently conscious of.\nLasts for about 20–30 seconds, with a capacity typically of 7±2 items. Responsible for the temporary processing and maintenance of information during complex cognitive tasks like learning and reasoning. Long-Term Memory: Can store information for days to decades, with a virtually unlimited capacity. Long-term memory is divided into:\nExplicit Memory: Can be consciously recalled, including episodic memory (personal experiences, event details) and semantic memory (facts and concepts). Implicit Memory: Unconscious memory, primarily related to skills and habits, such as riding a bike or touch typing. These three types of human memory are intertwined and together form our cognition and understanding of the world. When building LLM Agents, we can draw inspiration from this classification of human memory:\nSensory Memory corresponds to the LLM’s embedding representations of raw input data (such as text, images, and videos). Short-Term Memory corresponds to the LLM’s in-context learning, limited by the model’s context window max_tokens. When the conversation length exceeds the window, earlier information is truncated. Long-Term Memory corresponds to an external vector store or database, where the Agent can retrieve historical information on demand using RAG technology. LLM Agent Memory When an Agent engages in multi-turn interactions with a user or executes multi-step tasks, it can utilize different forms of memory and environmental information to complete its workflow.\nFig. 27. An overview of the sources, forms, and operations of the memory in LLM-based agents. (Image source: Zhang et al. 2024)\nTextual Memory\nFull Interaction: Records all dialogue and action trajectories, helping the Agent trace back the context. Recent Interaction: Retains only the dialogue content highly relevant to the current task, reducing unnecessary context usage. Retrieved Interaction: The Agent can retrieve historical dialogues or records related to the current task from an external knowledge base and integrate them into the current context. External Knowledge: When the Agent encounters a knowledge gap, it can retrieve and acquire additional information through APIs or external storage. Parametric Memory\nFine-tuning: Expands the model’s internal knowledge by injecting new information or knowledge into the LLM. Knowledge Editing: Modifies or updates existing knowledge at the model level, achieving dynamic adjustment of the model’s internal parametric memory. Environment\nRepresents the entities and context involved when the Agent interacts with users and external systems, such as the user Alice, or accessible tools or interfaces (e.g., a ticket booking system, a streaming platform). Agent\nThe LLM Agent is responsible for read and write operations, i.e., reading information from the external environment or knowledge base and writing new actions or content. It also includes a series of management functions, such as merging, reflecting, and forgetting, to dynamically maintain short-term and long-term memory. Another example is an Agent completing two different but related tasks, requiring the use of both short-term and long-term memory:\nTask A: Play a video: The Agent records the current plan, actions, and environmental state (e.g., searching, clicking, playing the video) in its short-term memory. This information is stored in memory and the LLM’s context window. Task B: Download a game: The Agent utilizes its long-term memory related to Arcane and League of Legends to quickly find a way to download the game. The figure shows the Agent searching on Google; we can consider Google’s knowledge base as an external knowledge source. All new search, click, and download actions are also updated in the short-term memory. Fig. 28: Illustration of short-term memory and long-term memory in an LLM-brained GUI agent. (Image source: Zhang et al. 2024)\nCommon memory elements and their corresponding storage methods can be summarized in the following table:\nMemory Element Memory Type Description Storage Medium / Method Action Short-Term Memory Historical action trajectory (e.g., clicking buttons, entering text) Memory, LLM Context Window Plan Short-Term Memory The plan for the next operation generated in the previous or current step Memory, LLM Context Window Execution Result Short-Term Memory The result returned after an action, error messages, and environmental feedback Memory, LLM Context Window Environment State Short-Term Memory Available buttons, page titles, system status, etc., in the current UI environment Memory, LLM Context Window Self-Experience Long-Term Memory Historical task trajectories and execution steps Database, Disk Self-Guidance Long-Term Memory Guiding rules and best practices summarized from historical successful trajectories Database, Disk External Knowledge Long-Term Memory External knowledge bases, documents, or other data sources to assist in task completion External Database, Vector Retrieval Task Success Metrics Long-Term Memory Records of task success rates, failure rates, etc., for improvement and analysis Database, Disk Furthermore, researchers have proposed new training and storage methods to enhance the memory capabilities of LLMs:\nLongMem (Language Models Augmented with Long-Term Memory) (Wang, et al. 2023) enables LLMs to remember long historical information. It adopts a decoupled network architecture, freezing the original LLM parameters as a memory encoder, while using an Adaptive Residual Side-Network (SideNet) as a memory retriever for memory checking and reading.\nFig. 29. Overview of the memory caching and retrieval flow of LongMem. (Image source: Wang, et al. 2023)\nIt mainly consists of three parts: Frozen LLM, Residual SideNet, and Cached Memory Bank. Its workflow is as follows:\nFirst, a long text sequence is split into fixed-length segments. Each segment is encoded layer by layer in the Frozen LLM, and at the $m$-th layer, the attention’s $K, V \\in \\mathbb{R}^{H \\times M \\times d}$ vector pairs are extracted and cached in the Cached Memory Bank. When a new input sequence is encountered, the model retrieves the top-$k$ most relevant key-value pairs from the long-term memory bank based on the current input’s query-key. These are then integrated into the subsequent language generation process. Meanwhile, the memory bank removes the oldest content to ensure the availability of the latest contextual information. The Residual SideNet fuses the hidden layer outputs of the frozen LLM with the retrieved historical key-values during inference, enabling effective modeling and utilization of context from ultra-long texts. Through this decoupled design, LongMem can flexibly schedule massive amounts of historical information without expanding its native context window, balancing both speed and long-term memory capabilities.\nTool Use Tool use is an important component of LLM Agents. By empowering LLMs with the ability to call external tools, their capabilities are significantly expanded: they can not only generate natural language but also obtain real-time information, perform complex calculations, and interact with various systems (such as databases, APIs, etc.), effectively breaking through the limitations of their pre-trained knowledge and avoiding the inefficiency of reinventing the wheel.\nTraditional LLMs primarily rely on pre-trained data for text generation, which makes them deficient in areas like mathematical operations, data retrieval, and real-time information updates. Through tool calling, models can:\nEnhance computational ability: For example, by calling a specialized calculator tool like Wolfram, the model can perform more precise mathematical calculations, compensating for its own arithmetic shortcomings. Obtain real-time information: Using search engines like Google, Bing, or database APIs, the model can access the latest information, ensuring the timeliness and accuracy of the generated content. Increase information credibility: With the support of external tools, the model can cite real data sources, reducing the risk of fabricating information and improving overall credibility. Improve system transparency: Tracking API call records can help users understand the model’s decision-making process, providing a degree of interpretability. Currently, various LLM applications based on tool calling have emerged, utilizing different strategies and architectures to cover everything from simple tasks to complex multi-step reasoning.\nToolformer Toolformer (Schick, et al. 2023) is an LLM that can use external tools through simple APIs. It is trained by fine-tuning the GPT-J model, requiring only a few examples for each API. The tools Toolformer learns to call include a question-answering system, Wikipedia search, a calculator, a calendar, and a translation system:\nFig. 30. Examples of inputs and outputs for all APIs used. (Image source: Schick, et al. 2023)\nHuggingGPT HuggingGPT (Shen, et al. 2023) is a framework that uses ChatGPT as a task planner. It selects available models from HuggingFace by reading their descriptions to complete user tasks and summarizes the results based on their execution.\nFig. 31. Illustration of how HuggingGPT works. (Image source: Shen, et al. 2023)\nThe system consists of the following four stages:\nTask Planning: Parses the user’s request into multiple sub-tasks. Each task has four attributes: task type, ID, dependencies, and parameters. The paper uses few-shot prompting to guide the model in task decomposition and planning. Model Selection: Assigns each sub-task to different expert models, using a multiple-choice format to determine the most suitable model. Due to the limited context length, models need to be initially filtered based on the task type. Task Execution: The expert models execute their assigned specific tasks and record the results, which are then passed to the LLM for further processing. Response Generation: Receives the execution results from each expert model and finally outputs a summary answer to the user. LLM Agent Applications Generative Agent The Generative Agent (Park, et al. 2023) experiment simulates realistic human behavior with 25 virtual characters driven by large language models in a sandbox environment. Its core design integrates memory, retrieval, reflection, and planning/reaction mechanisms, allowing agents to record and review their experiences and extract key information to guide future actions and interactions.\nFig. 32. The screenshot of generative agent sandbox. (Image source: Park, et al. 2023)\nThe entire system uses a long-term memory module to record all observed events, a retrieval model to extract information based on recency, importance, and relevance, and a reflection mechanism to generate high-level inferences, ultimately translating these outcomes into concrete actions. This simulation demonstrates emergent behaviors such as information diffusion, relationship memory, and social event coordination, providing a realistic simulation of human behavior for interactive applications.\nFig. 25. The generative agent architecture. (Park, et al. 2023)\nWebVoyager WebVoyager (He et al. 2024) is an autonomous web interaction agent based on large multimodal models, capable of controlling the mouse and keyboard for web browsing. WebVoyager uses the classic ReAct loop. In each interaction step, it views a browser screenshot annotated with a method similar to SoM (Set-of-Marks) (Yang, et al. 2023), which provides interaction cues by placing numerical labels on web elements, and then decides on the next action. This combination of visual annotation and the ReAct loop allows users to interact with web pages using natural language. For a concrete example, you can refer to the WebVoyager code using the LangGraph framework.\nFig. 33. The overall workflow of WebVoyager. (Image source: He et al. 2024)\nOpenAI Operator Operator (OpenAI, 2025) is an AI agent recently released by OpenAI, designed to autonomously execute web tasks. Operator can interact with web pages like a human user, completing specified tasks by typing, clicking, and scrolling. The core technology behind Operator is the Computer-Using Agent (CUA) (OpenAI, 2025). CUA combines the visual capabilities of GPT-4o with enhanced reasoning abilities gained through reinforcement learning, and it has been specially trained to interact with graphical user interfaces (GUIs), including buttons, menus, and text boxes that users see on the screen.\nFig. 34. Overview of OpenAI CUA. (Image source: OpenAI, 2025)\nCUA operates in an iterative loop consisting of three stages:\nPerception: CUA “observes” the web page content by capturing browser screenshots. This vision-based input allows it to understand the page’s layout and elements. Reasoning: Using a chain-of-thought reasoning process, CUA evaluates the next action based on the current and previous screenshots and the actions already taken. This reasoning ability enables it to track task progress, review intermediate steps, and make adjustments as needed. Action: CUA interacts with the browser by simulating mouse and keyboard operations (such as clicking, typing, and scrolling). This allows it to perform a wide range of web tasks without needing specific API integrations. The difference between CUA and the pre-existing WebVoyager is that CUA is an agent specifically trained with reinforcement learning, rather than a fixed-flow workflow built by directly calling GPT-4o. Although CUA is still in its early stages and has certain limitations, it has achieved state-of-the-art results on the following benchmarks.\nFig. 35. OpenAI CUA Benchmark Results. (Image source: OpenAI, 2025)\nDeep Research Deep Research is essentially a report generation system: given a user’s query, the system uses an LLM as its core agent to generate a structured and detailed report through multiple rounds of iterative information retrieval and analysis. Currently, the implementation logic of various Deep Research systems can be mainly divided into two approaches: Workflow Agent and RL Agent.\nWorkflow Agent vs RL Agent The Workflow Agent approach relies on developers to pre-design workflows and manually craft prompts to organize the entire report generation process. Its main features include:\nTask Decomposition and Flow Orchestration: The system breaks down the user’s query into several sub-tasks, such as generating an outline, information retrieval, and content summarization, and then executes them in a predetermined sequence. Fixed Process: The calls and interactions between different stages are pre-defined, similar to building a static flowchart or a directed acyclic graph (DAG), ensuring that each step has a clear responsibility. Dependence on Manual Design: This method heavily relies on the experience of engineers, who improve output quality through repeated prompt tuning. It is highly applicable but has limited flexibility. The LangGraph framework can be used to build and orchestrate workflows in the form of a graph.\nFig. 36. A workflow of the LangGraph. (Image source: LangGraph, 2025)\nThe following table compares 5 common workflow and agent patterns:\nPattern Core Mechanism Advantages Limitations Use Cases Prompt Chaining Sequentially calls LLMs, passing results step-by-step Suitable for phased reasoning, more accurate results Fixed process, high latency Document generation (outline → content), translation polishing Parallelization Splits sub-tasks for parallel processing, or multi-model voting Increases speed, more robust results Sub-tasks must be independent, high resource consumption Parallel content moderation, multi-model code detection Routing First classifies, then assigns to different models/processes Highly targeted, improves efficiency Effectiveness depends on classification accuracy Customer service query routing, dynamic model size selection Evaluator-Optimizer Generate → Evaluate → Optimize iteratively Improves result quality, suitable for tasks with standards High cost, multiple iterations increase latency Translation optimization, multi-round retrieval refinement Orchestrator-Worker Central orchestration, dynamically decomposes and schedules sub-tasks Flexible, can handle complex tasks Complex architecture, high scheduling cost Multi-file code modification, real-time research integration Agent LLM makes autonomous decisions, calls tools based on environmental feedback Highly flexible, adapts to dynamic environments Unpredictable, cost and security need control Autonomous research agents, interactive problem-solving Currently, there are several open-source projects on GitHub that have implemented workflow-based Deep Research Agents, such as GPT Researcher and open deep research.\nFig. 37. An overview of the open deep research. (Image source: LangChain, 2025)\nThe RL Agent is an alternative approach that uses RL to train a reasoning model to optimize the agent’s multi-round search, analysis, and report writing process. Its main features include:\nAutonomous Decision-Making Capability: The system is trained through reinforcement learning, allowing the agent to autonomously judge, make decisions, and adjust its strategy when facing complex search and content integration tasks, thereby generating reports more efficiently. Continuous Optimization: Using a reward mechanism to score and provide feedback on the generation process, the agent can continuously iterate and optimize its own policy, improving the overall quality from task decomposition to the final report. Reduced Manual Intervention: Compared to fixed processes that rely on manually crafted prompts, the reinforcement learning training approach reduces dependence on manual design, making it more suitable for handling variable and complex real-world application scenarios. The table below summarizes the main differences between these two approaches:\nFeature Workflow Agent RL Agent Process Design Pre-designed fixed workflow with clear task decomposition and flow orchestration End-to-end learning, with the agent making autonomous decisions and dynamically adjusting the process Autonomous Decision-Making Relies on manually designed prompts; the decision process is fixed and immutable Through reinforcement learning, the agent can autonomously judge, decide, and optimize its strategy Manual Intervention Requires extensive manual design and tuning of prompts; significant manual intervention Reduces manual intervention, achieving automatic feedback and continuous optimization through a reward mechanism Flexibility \u0026 Adaptability Weaker adaptability to complex or changing scenarios; limited extensibility Better suited for variable and complex real-world scenarios, with high flexibility Optimization Mechanism Optimization mainly relies on engineers’ experience and adjustments; lacks an end-to-end feedback mechanism Utilizes reward feedback from reinforcement learning for continuous, automated performance improvement Implementation Difficulty Relatively straightforward to implement, but requires tedious process design and maintenance Requires training data and computational resources; higher initial development investment, but better long-term results Training Required No additional training needed; relies solely on manually constructed processes and prompts Requires training the agent through reinforcement learning to achieve autonomous decision-making OpenAI Deep Research OpenAI Deep Research (OpenAI, 2025) is an intelligent agent officially released by OpenAI in February 2025. Designed for complex scenarios, it can automatically search, filter, analyze, and integrate multi-source information to ultimately generate high-quality comprehensive reports. The system is built on o3 as its core base model and incorporates reinforcement learning methods, significantly improving the accuracy and robustness of its multi-round iterative search and reasoning processes.\nCompared to traditional ChatGPT plugin-based search or conventional RAG techniques, OpenAI Deep Research has the following outstanding advantages:\nReinforcement Learning-Driven Iterative Reasoning Leveraging the o3 reasoning model and reinforcement learning training strategies, the agent can continuously optimize its reasoning path during multi-round search and summarization, effectively reducing the risk of distortion caused by error accumulation.\nMulti-Source Information Integration and Cross-Validation Breaking the limitations of a single search engine, it can simultaneously call upon various authoritative data sources such as specific databases and professional knowledge bases, forming more reliable research conclusions through cross-validation.\nHigh-Quality Report Generation The training phase introduces an LLM-as-a-judge scoring mechanism and strict evaluation criteria, enabling the system to self-evaluate when outputting reports, thereby generating more clearly structured and rigorously argued professional texts.\nTraining Process The training process for OpenAI Deep Research utilized a browser interaction dataset specifically tailored for research scenarios. Through these datasets, the model mastered core browsing functions—including searching, clicking, scrolling, and file parsing—and also learned to use Python tools in a sandboxed environment for computation, data analysis, and visualization. Furthermore, with reinforcement learning training on these browsing tasks, the model can efficiently perform information retrieval, integration, and reasoning across a vast number of websites, quickly locating key information or generating comprehensive research reports.\nThese training datasets include both objective tasks with ground-truth answers that can be automatically scored, as well as open-ended tasks equipped with detailed scoring rubrics. During training, the model’s responses are rigorously compared against the ground truth or scoring criteria, and the model generates CoT thought processes to allow an evaluation model to provide feedback.\nAdditionally, the training process reused the safety datasets accumulated during the o1 model’s training phase and was specifically supplemented with safety training data for Deep Research scenarios, ensuring that the model strictly adheres to relevant compliance and safety requirements during automated search and browsing.\nPerformance The model achieved state-of-the-art results on the Humanity’s Last Exam benchmark (Phan, et al. 2025), which evaluates AI’s ability to answer expert-level questions across various professional domains.\nFig. 38. Humanity’s Last Exam Benchmark Results. (Image source: OpenAI, 2025)\nFuture Directions Intelligent agents show vast promise, but to achieve reliable and widespread application, the following key challenges still need to be addressed:\nContext Window Limitations: The limited context window of LLMs restricts the amount of information they can process, affecting long-term planning and memory capabilities and reducing task coherence. Current research is exploring external memory mechanisms and context compression techniques to enhance long-term memory and complex information processing abilities. Currently, OpenAI’s latest model, GPT-4.5 (OpenAI, 2025), has a maximum context window of 128k tokens.\nInterface Standardization and Interoperability: The current natural language-based interaction with tools suffers from inconsistent formatting. The Model Context Protocol (MCP) (Anthropic, 2024) aims to unify the interaction between LLMs and applications through an open standard, reducing development complexity and improving system stability and cross-platform compatibility.\nTask Planning and Decomposition Capabilities: Agents struggle to formulate coherent plans for complex tasks, effectively decompose sub-tasks, and lack the ability to dynamically adjust in unexpected situations. More powerful planning algorithms, self-reflection mechanisms, and dynamic policy adjustment methods are needed to flexibly respond to uncertain environments.\nComputational Resources and Economic Viability: Deploying large model agents is costly due to multiple API calls and intensive computation, limiting their use in some practical scenarios. Optimization directions include more efficient model architectures, quantization techniques, inference optimization, caching strategies, and intelligent scheduling mechanisms. With the development of specialized GPU hardware like the NVIDIA DGX B200 and distributed technologies, computational efficiency is expected to improve significantly.\nSecurity and Privacy Protection: Agents face security risks such as prompt injection and need robust authentication, permission control, input validation, and sandboxed environments. For multimodal inputs and external tools, data anonymization, the principle of least privilege, and audit logs must be strengthened to meet security and privacy compliance requirements.\nDecision Transparency and Explainability: The difficulty in explaining agent decisions limits their application in high-stakes domains. Enhancing explainability requires the development of visualization tools, chain-of-thought tracking, and decision rationale generation mechanisms to improve decision transparency, build user trust, and meet regulatory requirements.\nReferences [1] DAIR.AI. “LLM Agents.” Prompt Engineering Guide, 2024.\n[2] Sutton, Richard S., and Andrew G. Barto. “Reinforcement Learning: An Introduction.” MIT Press, 2018.\n[3] Weng, Lilian. “LLM-powered Autonomous Agents.” Lil’Log, 2023.\n[4] Zhou, Yongchao, et al. “Large language models are human-level prompt engineers.” The eleventh international conference on learning representations. 2022.\n[5] Zhang, Zhuosheng, et al. “Automatic chain of thought prompting in large language models.” arXiv preprint arXiv:2210.03493 (2022).\n[6] Liu, Jiacheng, et al. “Generated knowledge prompting for commonsense reasoning.” arXiv preprint arXiv:2110.08387 (2021).\n[7] Lewis, Patrick, et al. “Retrieval-augmented generation for knowledge-intensive nlp tasks.” Advances in neural information processing systems 33 (2020): 9459-9474.\n[8] Zhang, Zhuosheng, et al. “Multimodal chain-of-thought reasoning in language models.” arXiv preprint arXiv:2302.00923 (2023).\n[9] Diao, Shizhe, et al. “Active prompting with chain-of-thought for large language models.” arXiv preprint arXiv:2302.12246 (2023).\n[10] Wei, Jason, et al. “Chain-of-thought prompting elicits reasoning in large language models.” Advances in neural information processing systems 35 (2022): 24824-24837.\n[11] Kojima, Takeshi, et al. “Large language models are zero-shot reasoners.” Advances in neural information processing systems 35 (2022): 22199-22213.\n[12] Wang, Xuezhi, et al. “Self-consistency improves chain of thought reasoning in language models.” arXiv preprint arXiv:2203.11171 (2022).\n[13] Wang, Xuezhi, et al. “Rationale-augmented ensembles in language models.” arXiv preprint arXiv:2207.00747 (2022).\n[14] Zelikman, Eric, et al. “Star: Bootstrapping reasoning with reasoning.” Advances in Neural Information Processing Systems 35 (2022): 15476-15488.\n[15] Fu, Yao, et al. “Complexity-based prompting for multi-step reasoning.” arXiv preprint arXiv:2210.00720 (2022).\n[16] Yao, Shunyu, et al. “Tree of thoughts: Deliberate problem solving with large language models.” Advances in neural information processing systems 36 (2023): 11809-11822.\n[17] Yao, Shunyu, et al. “React: Synergizing reasoning and acting in language models.” International Conference on Learning Representations (ICLR). 2023.\n[18] Shinn, Noah, et al. “Reflexion: Language agents with verbal reinforcement learning.” Advances in Neural Information Processing Systems 36 (2023): 8634-8652.\n[19] Guo, Daya, et al. “Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.” arXiv preprint arXiv:2501.12948 (2025).\n[20] OpenAI. “Introducing OpenAI o1” OpenAI, 2024.\n[21] Zhang, Zeyu, et al. “A survey on the memory mechanism of large language model based agents.” arXiv preprint arXiv:2404.13501 (2024).\n[22] Zhang, Chaoyun, et al. “Large language model-brained gui agents: A survey.” arXiv preprint arXiv:2411.18279 (2024).\n[23] Wang, Weizhi, et al. “Augmenting language models with long-term memory.” Advances in Neural Information Processing Systems 36 (2023): 74530-74543.\n[24] Schick, Timo, et al. “Toolformer: Language models can teach themselves to use tools.” Advances in Neural Information Processing Systems 36 (2023): 68539-68551.\n[25] Shen, Yongliang, et al. “Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.” Advances in Neural Information Processing Systems 36 (2023): 38154-38180.\n[26] Park, Joon Sung, et al. “Generative agents: Interactive simulacra of human behavior.” Proceedings of the 36th annual acm symposium on user interface software and technology. 2023.\n[27] He, Hongliang, et al. “WebVoyager: Building an end-to-end web agent with large multimodal models.” arXiv preprint arXiv:2401.13919 (2024).\n[28] Yang, Jianwei, et al. “Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v.” arXiv preprint arXiv:2310.11441 (2023).\n[29] OpenAI. “Introducing Operator.” OpenAI, 2025.\n[30] OpenAI. “Computer-Using Agent.” OpenAI, 2025.\n[31] OpenAI. “Introducing Deep Research.” OpenAI, 2025.\n[32] Phan, Long, et al. “Humanity’s Last Exam.” arXiv preprint arXiv:2501.14249 (2025).\n[33] OpenAI. “Introducing GPT-4.5.” OpenAI, 2025.\n[34] Anthropic. “Introducing the Model Context Protocol.” Anthropic, 2024.\n[35] LangGraph. “A workflow of the LangGraph.” LangGraph Tutorials, 2025.\n[36] Assaf Elovic. “GPT Researcher” GitHub Repository, 2025.\n[37] LangChain. “Open Deep Research” GitHub Repository, 2025.\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui.(Mar 2025). Large Language Model Agents. https://syhya.github.io/posts/2025-03-27-llm-agent\nOr\n@article{syhya2025llm-agent, title = \"Large Language Model Agents\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Mar\", url = \"https://syhya.github.io/posts/2025-03-27-llm-agent\" } ","wordCount":"7905","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-03-27T10:00:00Z","dateModified":"2025-09-02T22:00:47+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-03-27-llm-agent/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Large Language Model Agents</h1><div class=post-meta><span title='2025-03-27 10:00:00 +0000 UTC'>Created:&nbsp;2025-03-27</span>&nbsp;·&nbsp;Updated:&nbsp;2025-09-02&nbsp;·&nbsp;38 min&nbsp;·&nbsp;7905 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-03-27-llm-agent/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#agents>Agents</a><ul><li><a href=#llm-agent>LLM Agent</a></li><li><a href=#rl-agent>RL Agent</a></li><li><a href=#comparison>Comparison</a></li></ul></li><li><a href=#prompt-engineering>Prompt Engineering</a><ul><li><a href=#zero-shot-prompting>Zero-Shot Prompting</a></li><li><a href=#few-shot-prompting>Few-Shot Prompting</a></li><li><a href=#automatic-prompt-construction>Automatic Prompt Construction</a></li></ul></li><li><a href=#knowledge-enhancement>Knowledge Enhancement</a><ul><li><a href=#multimodal-chain-of-thought-prompting>Multimodal Chain-of-Thought Prompting</a></li><li><a href=#active-prompt>Active Prompt</a></li></ul></li><li><a href=#planning-task-decomposition>Planning: Task Decomposition</a><ul><li><a href=#chain-of-thought>Chain of Thought</a></li><li><a href=#self-consistency-sampling>Self-Consistency Sampling</a></li><li><a href=#tree-of-thoughts>Tree of Thoughts</a></li></ul></li><li><a href=#planning-self-reflexion>Planning: Self-Reflexion</a><ul><li><a href=#react>ReAct</a></li><li><a href=#reflexion>Reflexion</a></li><li><a href=#deepseek-r1>DeepSeek R1</a></li></ul></li><li><a href=#memory>Memory</a><ul><li><a href=#human-memory>Human Memory</a></li><li><a href=#llm-agent-memory>LLM Agent Memory</a></li></ul></li><li><a href=#tool-use>Tool Use</a><ul><li><a href=#toolformer>Toolformer</a></li><li><a href=#hugginggpt>HuggingGPT</a></li></ul></li><li><a href=#llm-agent-applications>LLM Agent Applications</a><ul><li><a href=#generative-agent>Generative Agent</a></li><li><a href=#webvoyager>WebVoyager</a></li><li><a href=#openai-operator>OpenAI Operator</a></li></ul></li><li><a href=#deep-research>Deep Research</a><ul><li><a href=#workflow-agent-vs-rl-agent>Workflow Agent vs RL Agent</a></li><li><a href=#openai-deep-research>OpenAI Deep Research</a><ul><li><a href=#training-process>Training Process</a></li><li><a href=#performance>Performance</a></li></ul></li></ul></li><li><a href=#future-directions>Future Directions</a></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><h2 id=agents>Agents<a hidden class=anchor aria-hidden=true href=#agents>#</a></h2><p>Since the release of ChatGPT by OpenAI in October 2022, and with the emergence of subsequent projects like <a href=https://github.com/Significant-Gravitas/AutoGPT>AutoGPT</a> and <a href=https://github.com/reworkd/AgentGPT>AgentGPT</a>, LLM-related agents have become a research hotspot and a practical application direction in AI in recent years. This article will introduce the basic concepts, core technologies, and latest application progress of agents.</p><h3 id=llm-agent>LLM Agent<a hidden class=anchor aria-hidden=true href=#llm-agent>#</a></h3><p>A <strong>Large Language Model Agent (LLM agent)</strong> utilizes an LLM as its core &ldquo;brain&rdquo; and combines modules like planning, memory, and external tools to automate the execution of complex tasks.</p><ul><li><strong>User Request:</strong> The user interacts with the agent by inputting tasks through prompts.</li><li><strong>Agent:</strong> The system&rsquo;s brain, composed of one or more LLMs, responsible for overall coordination and task execution.</li><li><strong>Planning:</strong> Decomposes complex tasks into smaller sub-tasks, formulates an execution plan, and continuously optimizes the results through reflection.</li><li><strong>Memory:</strong> Includes short-term memory (capturing task information in real-time using in-context learning) and long-term memory (using external vector stores to save and retrieve key information, ensuring information continuity for long-running tasks).</li><li><strong>Tools:</strong> Integrates external tools such as calculators, web search, and code interpreters to call external data, execute code, and obtain the latest information.</li></ul><figure class=align-center><img loading=lazy src=llm_agent.png#center alt="Fig. 1. The illustration of LLM Agent Framework. (Image source: DAIR.AI, 2024)" width=70%><figcaption><p>Fig. 1. The illustration of LLM Agent Framework. (Image source: <a href=https://www.promptingguide.ai/research/llm-agents#llm-agent-framework>DAIR.AI, 2024</a>)</p></figcaption></figure><h3 id=rl-agent>RL Agent<a hidden class=anchor aria-hidden=true href=#rl-agent>#</a></h3><p>The goal of <strong>Reinforcement Learning (RL)</strong> is to train an agent to take a series of actions ($a_t$) in a given environment. During interaction, the agent transitions from one state ($s_t$) to the next and receives a reward ($r_t$) from the environment after each action. This interaction generates a complete trajectory ($\tau$), typically represented as:</p>$$
\tau = \{(s_0, a_0, r_0), (s_1, a_1, r_1), \dots, (s_T, a_T, r_T)\}.
$$<p>The agent&rsquo;s objective is to learn a policy ($\pi$), which is a rule for selecting actions in each state, to <strong>maximize the expected cumulative reward</strong>, usually expressed as:</p>$$
\max_{\pi} \, \mathbb{E}\left[\sum_{t=0}^{T} \gamma^t r_t\right],
$$<p>where $\gamma \in [0,1]$ is the discount factor, used to balance short-term and long-term rewards.</p><figure class=align-center><img loading=lazy src=rl_agent.png#center alt="Fig. 2. The agent-environment interaction. (Image source: Sutton & Barto, 2018)" width=80%><figcaption><p>Fig. 2. The agent-environment interaction. (Image source: <a href=http://incompleteideas.net/book/the-book.html>Sutton & Barto, 2018</a>)</p></figcaption></figure><p>In the context of <strong>LLMs</strong>, the model can be viewed as an agent, and the &ldquo;environment&rdquo; can be understood as the user&rsquo;s input and the desired response format:</p><ul><li><strong>State ($s_t$)</strong>: Can be the current dialogue context or the user&rsquo;s question.</li><li><strong>Action ($a_t$)</strong>: The text output by the model (answer, generated content, etc.).</li><li><strong>Reward ($r_t$)</strong>: Feedback from the user or the system (e.g., user satisfaction, automatic scoring by a reward model).</li><li><strong>Trajectory ($\tau$)</strong>: The entire sequence of text interactions from the beginning to the end of a conversation, which can be used to evaluate the model&rsquo;s overall performance.</li><li><strong>Policy ($\pi$)</strong>: The rule by which the LLM generates text in each state (dialogue context), typically determined by the model&rsquo;s parameters.</li></ul><p>For LLMs, they are traditionally pre-trained on massive offline datasets. In the post-training reinforcement learning phase, the model is trained with feedback from humans or other models to produce high-quality text that better aligns with human preferences or task requirements.</p><h3 id=comparison>Comparison<a hidden class=anchor aria-hidden=true href=#comparison>#</a></h3><p>The table below shows the differences between the two:</p><table><thead><tr><th><strong>Comparison Dimension</strong></th><th><strong>LLM Agent</strong></th><th><strong>RL Agent</strong></th></tr></thead><tbody><tr><td><strong>Core Principle</strong></td><td>Automates complex tasks through planning, memory, and tools.</td><td>Continuously optimizes its policy to maximize long-term rewards through a trial-and-error feedback loop with the environment.</td></tr><tr><td><strong>Optimization Method</strong></td><td><strong>Does not directly update model parameters.</strong> Performance is improved mainly through context extension, external memory, and tools.</td><td><strong>Continuously and frequently updates policy model parameters,</strong> relying on reward signals from the environment for optimization.</td></tr><tr><td><strong>Interaction Method</strong></td><td>Interacts with users or external systems using natural language, flexibly calling various tools to obtain external information.</td><td>Interacts with a real or simulated environment, which provides rewards or penalties, forming a closed feedback loop.</td></tr><tr><td><strong>Objective</strong></td><td>Decomposes complex tasks and completes them with the help of external resources, focusing on the quality and accuracy of the task outcome.</td><td>Maximizes long-term rewards, seeking an optimal balance between short-term and long-term returns.</td></tr></tbody></table><p>As research progresses, the combination of LLM and RL agents presents more possibilities, such as:</p><ul><li>Using reinforcement learning methods to train Reasoning LLMs (like o1/o3), making them more suitable as base models for LLM agents.</li><li>Simultaneously, recording the data and feedback from LLM agents executing tasks to provide rich training data for Reasoning LLMs, thereby enhancing model performance.</li></ul><h2 id=prompt-engineering>Prompt Engineering<a hidden class=anchor aria-hidden=true href=#prompt-engineering>#</a></h2><p><strong>Prompt Engineering</strong>, also known as <strong>In-Context Prompting</strong>, is the technique of optimizing input prompts to guide an LLM to produce the desired output. Its core objective is to control the model&rsquo;s behavior through effective communication <strong>without updating the model&rsquo;s weights</strong>.</p><h3 id=zero-shot-prompting>Zero-Shot Prompting<a hidden class=anchor aria-hidden=true href=#zero-shot-prompting>#</a></h3><p><strong>Zero-Shot Prompting</strong> directly provides the model with task instructions without any examples. This method relies entirely on the knowledge and instruction-following capabilities the model learned during its pre-training phase. For example, for sentiment analysis:</p><figure class=align-center><img loading=lazy src=zero_shot.png#center alt="Fig. 3. Zero-Shot Prompting." width=100%><figcaption><p>Fig. 3. Zero-Shot Prompting.</p></figcaption></figure><p>For models that have undergone instruction fine-tuning, such as GPT-5 or Claude 4, they can understand and execute these direct instructions very well.</p><h3 id=few-shot-prompting>Few-Shot Prompting<a hidden class=anchor aria-hidden=true href=#few-shot-prompting>#</a></h3><p><strong>Few-Shot Prompting</strong> provides a set of high-quality examples in the prompt, with each example containing an input and the desired output. Through these examples, the model can better understand the user&rsquo;s intent and the specific requirements of the task, leading to better performance than zero-shot prompting. However, a drawback of this method is that it consumes more of the context window length. For example, providing a few sentiment analysis examples:</p><figure class=align-center><img loading=lazy src=few_shot.png#center alt="Fig. 4. Few-Shot Prompting." width=100%><figcaption><p>Fig. 4. Few-Shot Prompting.</p></figcaption></figure><h3 id=automatic-prompt-construction>Automatic Prompt Construction<a hidden class=anchor aria-hidden=true href=#automatic-prompt-construction>#</a></h3><p><strong>Automatic Prompt Engineer (APE)</strong> (<a href=https://arxiv.org/abs/2211.01910>Zhou et al. 2022</a>) is a method that searches through a pool of candidate instructions generated by the model. It filters the candidate set and ultimately selects the highest-scoring instruction based on a chosen scoring function.</p><figure class=align-center><img loading=lazy src=ape.png#center alt="Fig. 5. Automatic Prompt Engineer (APE) workflow. (Image source: Zhou et al. 2022)" width=100%><figcaption><p>Fig. 5. Automatic Prompt Engineer (APE) workflow. (Image source: <a href=https://arxiv.org/abs/2211.01910>Zhou et al. 2022</a>)</p></figcaption></figure><p><strong>Automatic Chain-of-Thought (Auto-CoT)</strong> (<a href=https://arxiv.org/abs/2210.03493>Zhang et al. 2022</a>) proposes a method for automatically constructing Chain-of-Thought examples, aiming to solve the problem of manual prompt design being time-consuming and potentially suboptimal. Its core idea is to sample questions through <strong>clustering techniques</strong> and then <strong>leverage the LLM&rsquo;s own zero-shot reasoning capabilities to automatically generate reasoning chains</strong>, thereby constructing diverse, high-quality examples.</p><figure class=align-center><img loading=lazy src=auto_cot.png#center alt="Fig. 6. Overview of the Auto-CoT method. (Image source: Zhang et al. 2022)" width=100%><figcaption><p>Fig. 6. Overview of the Auto-CoT method. (Image source: <a href=https://arxiv.org/abs/2210.03493>Zhang et al. 2022</a>)</p></figcaption></figure><p><strong>Auto-CoT consists of two main stages:</strong></p><ol><li><strong>Question Clustering</strong>: Embeds the questions in the dataset and runs an algorithm like $k$-means for clustering. This step aims to group similar questions into the same cluster to ensure the diversity of subsequently sampled questions.</li><li><strong>Demonstration Selection & Rationale Generation</strong>: Selects one or more representative questions from each cluster (e.g., choosing the question closest to the cluster centroid). Then, it uses a <strong>Zero-Shot CoT</strong> prompt to have the LLM generate reasoning chains for these selected questions. These automatically generated &ldquo;question-rationale&rdquo; pairs form the final few-shot prompt used for task execution.</li></ol><h2 id=knowledge-enhancement>Knowledge Enhancement<a hidden class=anchor aria-hidden=true href=#knowledge-enhancement>#</a></h2><p>When dealing with knowledge-intensive or commonsense reasoning tasks, relying solely on the LLM&rsquo;s parametric knowledge is often insufficient and can lead to incorrect or outdated answers. To address this issue, researchers have proposed two main approaches:</p><p><strong>Generated Knowledge Prompting</strong> (<a href=https://arxiv.org/abs/2110.08387>Liu et al. 2022</a>) is a method where the model is first prompted to <strong>generate relevant knowledge</strong> before making a prediction. The core idea is that when a task requires commonsense or external information, the model may make mistakes due to a lack of context. If the model is first guided to generate knowledge related to the input and then answers based on that knowledge, the accuracy of its reasoning can be improved.</p><figure class=align-center><img loading=lazy src=generated_knowledge_prompting.png#center alt="Fig. 7. Overview of the Generated Knowledge Prompting. (Image source: Liu et al. 2022)" width=100%><figcaption><p>Fig. 7. Overview of the Generated Knowledge Prompting. (Image source: <a href=https://arxiv.org/abs/2110.08387>Liu et al. 2022</a>)</p></figcaption></figure><ol><li><strong>Knowledge Generation</strong>: Based on the input, the model first generates relevant factual knowledge.</li><li><strong>Knowledge Integration</strong>: The generated knowledge is combined with the original question to form a new prompt.</li><li><strong>Answer Prediction</strong>: The model provides an answer based on the enhanced input.</li></ol><p><strong>Retrieval Augmented Generation (RAG)</strong> (<a href=https://arxiv.org/abs/2005.11401>Lewis et al. 2021</a>) is a method that combines <strong>information retrieval with text generation</strong> to tackle knowledge-intensive tasks. Its core idea is that relying solely on an LLM&rsquo;s parametric (static) knowledge can easily lead to factual errors. By introducing retrieval from external knowledge bases, the <strong>factual consistency and timeliness</strong> of the generated results can be improved.</p><figure class=align-center><img loading=lazy src=rag.png#center alt="Fig. 8. Overview of the Retrieval Augmented Generation. (Image source: Lewis et al. 2021)" width=100%><figcaption><p>Fig. 8. Overview of the Retrieval Augmented Generation. (Image source: <a href=https://arxiv.org/abs/2005.11401>Lewis et al. 2021</a>)</p></figcaption></figure><ol><li><strong>Retrieval</strong>: Retrieves relevant documents from an external knowledge source (e.g., Wikipedia or a private knowledge base).</li><li><strong>Augmentation</strong>: Concatenates the retrieved documents with the original input to serve as the prompt context.</li><li><strong>Generation</strong>: A generation model (the original paper used a pre-trained seq2seq model, but today LLMs are mainstream) outputs the answer based on the augmented prompt.</li></ol><h3 id=multimodal-chain-of-thought-prompting>Multimodal Chain-of-Thought Prompting<a hidden class=anchor aria-hidden=true href=#multimodal-chain-of-thought-prompting>#</a></h3><p><strong>Multimodal CoT Prompting (MCoT)</strong> (<a href=https://arxiv.org/abs/2302.00923>Zhang et al. 2023</a>) integrates <strong>text and visual information</strong> into the reasoning process, breaking the limitation of traditional CoT, which relies solely on the language modality. Its framework is divided into two stages:</p><ol><li><strong>Rationale Generation</strong>: Generates an explanatory reasoning chain based on multimodal information (text + image).</li><li><strong>Answer Inference</strong>: Uses the generated rationale as an aid to infer the final answer.</li></ol><figure class=align-center><img loading=lazy src=MCoT.png#center alt="Fig. 9. Overview of our Multimodal-CoT framework. (Image source: Zhang et al. 2023)" width=100%><figcaption><p>Fig. 9. Overview of our Multimodal-CoT framework. (Image source: <a href=https://arxiv.org/abs/2302.00923>Zhang et al. 2023</a>)</p></figcaption></figure><h3 id=active-prompt>Active Prompt<a hidden class=anchor aria-hidden=true href=#active-prompt>#</a></h3><p><strong>Active Prompt</strong> (<a href=https://arxiv.org/abs/2302.12246>Diao et al. 2023</a>) addresses the limitation of traditional CoT methods that rely on a fixed set of manually annotated examples. The problem is that <strong>fixed examples are not necessarily optimal for all tasks, which can lead to poor generalization</strong>. Active Prompt introduces an active learning strategy to adaptively select and update the best task-relevant examples, thereby improving the model&rsquo;s reasoning performance.</p><figure class=align-center><img loading=lazy src=active_prompt.png#center alt="Fig. 10. Illustrations of active prompting framework. (Image source: Diao et al. 2023)" width=100%><figcaption><p>Fig. 10. Illustrations of active prompting framework. (Image source: <a href=https://arxiv.org/abs/2302.12246>Diao et al. 2023</a>)</p></figcaption></figure><ol><li><strong>Uncertainty Estimation</strong>: With or without a small number of manual CoT examples, the LLM generates <em>k</em> answers for training questions (in the paper, <em>k=5</em>), and an uncertainty metric is calculated based on the variance of these answers.</li><li><strong>Selection</strong>: Based on the uncertainty level, the most uncertain questions are selected.</li><li><strong>Annotation</strong>: The selected questions are manually annotated to provide new, high-quality CoT examples.</li><li><strong>Inference</strong>: The newly annotated examples are used for inference, improving the model&rsquo;s performance on the target task.</li></ol><h2 id=planning-task-decomposition>Planning: Task Decomposition<a hidden class=anchor aria-hidden=true href=#planning-task-decomposition>#</a></h2><p>The core components of an LLM Agent include <strong>planning</strong>, <strong>memory</strong>, and <strong>tool use</strong>. These components work together to enable the agent to autonomously execute complex tasks.</p><figure class=align-center><img loading=lazy src=llm_agent_overview.png#center alt="Fig. 11. Overview of a LLM-powered autonomous agent system. (Image source: Weng, 2017)" width=100%><figcaption><p>Fig. 11. Overview of a LLM-powered autonomous agent system. (Image source: <a href=https://lilianweng.github.io/posts/2023-06-23-agent/>Weng, 2017</a>)</p></figcaption></figure><p>Planning is crucial for the successful execution of complex tasks. It can be approached in different ways depending on the complexity and the need for iterative refinement. In simple scenarios, the planning module can use an LLM to outline a detailed plan in advance, including all necessary sub-tasks. This step ensures that the agent systematically performs <strong>task decomposition</strong> and follows a clear logical flow from the outset.</p><h3 id=chain-of-thought>Chain of Thought<a hidden class=anchor aria-hidden=true href=#chain-of-thought>#</a></h3><p><strong>Chain of Thought (CoT)</strong> (<a href=https://arxiv.org/abs/2201.11903>Wei et al. 2022</a>) works by generating a series of short sentences step-by-step to describe the reasoning process; these sentences are called reasoning steps. Its purpose is to explicitly show the model&rsquo;s reasoning path, helping it better handle <strong>complex reasoning tasks</strong>. The figure below shows the difference between few-shot prompting (left) and CoT prompting (right). The few-shot prompt leads to an incorrect answer, while the CoT method guides the model to state its reasoning process step-by-step, more clearly reflecting the model&rsquo;s logical process and thus improving the answer&rsquo;s accuracy and interpretability.</p><figure class=align-center><img loading=lazy src=cot.png#center alt="Fig. 12. The comparison example of few-shot prompting and CoT prompting. (Image source: Wei et al. 2022)" width=100%><figcaption><p>Fig. 12. The comparison example of few-shot prompting and CoT prompting. (Image source: <a href=https://arxiv.org/abs/2201.11903>Wei et al. 2022</a>)</p></figcaption></figure><p><strong>Zero-Shot CoT</strong> (<a href=https://arxiv.org/abs/2205.11916>Kojima et al. 2022</a>) is a follow-up to CoT that proposes an extremely simple zero-shot prompting method. They found that by simply appending the phrase <code>Let's think step by step</code> to the end of the question, the LLM can produce a chain of thought, leading to more accurate answers.</p><figure class=align-center><img loading=lazy src=zero_shot_cot.png#center alt="Fig. 13. The comparison example of few-shot prompting and CoT prompting. (Image source: Kojima et al. 2022)" width=100%><figcaption><p>Fig. 13. The comparison example of few-shot prompting and CoT prompting. (Image source: <a href=https://arxiv.org/abs/2205.11916>Kojima et al. 2022</a>)</p></figcaption></figure><h3 id=self-consistency-sampling>Self-Consistency Sampling<a hidden class=anchor aria-hidden=true href=#self-consistency-sampling>#</a></h3><p><strong>Self-consistency sampling</strong> (<a href=https://arxiv.org/abs/2203.11171>Wang et al. 2022a</a>) generates <strong>multiple diverse answers</strong> by sampling multiple times from the same prompt with a <code>temperature > 0</code> and then selecting the best answer from the set. The core idea is to improve the final answer&rsquo;s accuracy and robustness by sampling multiple reasoning paths and then taking a majority vote. The criteria for selecting the best answer can vary for different tasks, but <strong>majority voting</strong> is generally used as a universal solution. For tasks that are easy to verify, such as programming problems, the answers can be validated by running them through an interpreter and using unit tests. This is an optimization of CoT, and when used in combination, it can significantly improve the model&rsquo;s performance on complex reasoning tasks.</p><figure class=align-center><img loading=lazy src=self_consistency.png#center alt="Fig. 14. Overview of the Self-Consistency Method for Chain-of-Thought Reasoning. (Image source: Wang et al. 2022a)" width=100%><figcaption><p>Fig. 14. Overview of the Self-Consistency Method for Chain-of-Thought Reasoning. (Image source: <a href=https://arxiv.org/abs/2203.11171>Wang et al. 2022a</a>)</p></figcaption></figure><p>Here are some subsequent optimization works:</p><ul><li>(<a href=https://arxiv.org/abs/2207.00747>Wang et al. 2022b</a>) later used another ensemble learning method for optimization, increasing randomness by <strong>changing the order of examples</strong> or <strong>replacing human-written reasoning with model-generated ones</strong>, followed by majority voting.</li></ul><figure class=align-center><img loading=lazy src=rationale_augmented.png#center alt="Fig. 15. An overview of different ways of composing rationale-augmented ensembles, depending on how the randomness of rationales is introduced. (Image source: Wang et al. 2022b)" width=100%><figcaption><p>Fig. 15. An overview of different ways of composing rationale-augmented ensembles, depending on how the randomness of rationales is introduced. (Image source: <a href=https://arxiv.org/abs/2207.00747>Wang et al. 2022b</a>)</p></figcaption></figure><ul><li>If training samples only provide the correct answer without the reasoning, the <strong>STaR (Self-Taught Reasoner)</strong> (<a href=https://arxiv.org/abs/2203.14465>Zelikman et al. 2022</a>) method can be used: (1) Have the LLM generate reasoning chains, and keep only the reasoning for answers that are correct. (2) Fine-tune the model with the generated reasoning, iterating until convergence. Note that a high <code>temperature</code> can easily lead to results with the correct answer but incorrect reasoning. If there is no ground truth, majority voting can be considered the &ldquo;correct answer.&rdquo;</li></ul><figure class=align-center><img loading=lazy src=STaR.png#center alt="Fig. 16. An overview of STaR and a STaR-generated rationale on CommonsenseQA. (Image source: Zelikman et al. 2022)" width=100%><figcaption><p>Fig. 16. An overview of STaR and a STaR-generated rationale on CommonsenseQA. (Image source: <a href=https://arxiv.org/abs/2203.14465>Zelikman et al. 2022</a>)</p></figcaption></figure><ul><li>(<a href=https://arxiv.org/abs/2210.00720>Fu et al. 2023</a>) found that more complex examples (with more reasoning steps) can improve model performance. When separating reasoning steps, a newline character <code>\n</code> works better than <code>step i</code>, <code>.</code>, or <code>;</code>. Additionally, using a complexity-based consistency strategy, which only performs majority voting on the top-$k$ most complex generated reasoning chains, can further optimize the model&rsquo;s output. It was also shown that replacing <code>Q:</code> with <code>Question:</code> in the prompt provides an additional performance boost.</li></ul><figure class=align-center><img loading=lazy src=linebreak.png#center alt="Fig. 17. Sensitivity analysis on step formatting. Complex prompts consistently lead to better performance with regard to different step formatting. (Image source: Fu et al. 2023)" width=100%><figcaption><p>Fig. 17. Sensitivity analysis on step formatting. Complex prompts consistently lead to better performance with regard to different step formatting. (Image source: <a href=https://arxiv.org/abs/2210.00720>Fu et al. 2023</a>)</p></figcaption></figure><h3 id=tree-of-thoughts>Tree of Thoughts<a hidden class=anchor aria-hidden=true href=#tree-of-thoughts>#</a></h3><p><strong>Tree of Thoughts (ToT)</strong> (<a href=https://arxiv.org/abs/2305.10601>Yao et al. 2023</a>) expands on CoT by exploring multiple reasoning possibilities at each step. It first decomposes a problem into multiple thought steps and generates several different ideas at each step, forming a tree structure. The search process can use Breadth-First Search (BFS) or Depth-First Search (DFS), and each state is evaluated by a classifier (or by having the LLM score it) or through majority voting. It involves three main steps:</p><ul><li><strong>Expand</strong>: Generate one or more candidate solutions.</li><li><strong>Score</strong>: Measure the quality of the candidate solutions.</li><li><strong>Prune</strong>: Keep the top-$k$ best candidate solutions.</li></ul><p>If no solution is found (or if the quality of the candidates is not high enough), the process backtracks to the expansion step.</p><figure class=align-center><img loading=lazy src=tot.png#center alt="Fig. 18. Schematic illustrating various approaches to problem solving with LLMs. (Image source: Yao et al. 2023)" width=100%><figcaption><p>Fig. 18. Schematic illustrating various approaches to problem solving with LLMs. (Image source: <a href=https://arxiv.org/abs/2305.10601>Yao et al. 2023</a>)</p></figcaption></figure><h2 id=planning-self-reflexion>Planning: Self-Reflexion<a hidden class=anchor aria-hidden=true href=#planning-self-reflexion>#</a></h2><p><strong>Self-Reflexion</strong> is a key factor that enables an agent to achieve iterative improvement by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.</p><h3 id=react>ReAct<a hidden class=anchor aria-hidden=true href=#react>#</a></h3><p>The <strong>ReAct (Reason + Act)</strong> (<a href=https://arxiv.org/abs/2210.03629>Yao et al. 2023</a>) framework achieves seamless integration of reasoning and acting in LLMs by combining task-specific discrete actions with the language space. This design not only allows the model to interact with the environment by calling external interfaces like the Wikipedia search API but also to generate detailed reasoning trajectories in natural language to solve complex problems.</p><p>The ReAct prompt template includes explicit thought steps, with the basic format as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>Thought: ...
</span></span><span class=line><span class=cl>Action: ...
</span></span><span class=line><span class=cl>Observation: ...
</span></span><span class=line><span class=cl>...(Repeated many times)
</span></span></code></pre></div><figure class=align-center><img loading=lazy src=ReAct.png#center alt="Fig. 19. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023)" width=100%><figcaption><p>Fig. 19. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: <a href=https://arxiv.org/abs/2210.03629>Yao et al. 2023</a>)</p></figcaption></figure><p>As seen in the figure below, ReAct significantly outperforms the baseline <code>Action</code>-only method in both knowledge-intensive and decision-making tasks, demonstrating its advantages in enhancing reasoning effectiveness and interactive performance.</p><figure class=align-center><img loading=lazy src=ReAct_res.png#center alt="Fig. 20. PaLM-540B prompting results on HotpotQA and Fever. (Image source: Yao et al. 2023)" width=50%><figcaption><p>Fig. 20. PaLM-540B prompting results on HotpotQA and Fever. (Image source: <a href=https://arxiv.org/abs/2210.03629>Yao et al. 2023</a>)</p></figcaption></figure><h3 id=reflexion>Reflexion<a hidden class=anchor aria-hidden=true href=#reflexion>#</a></h3><p><strong>Reflexion</strong> (<a href=https://arxiv.org/abs/2303.11366>Shinn et al. 2023</a>) enables an LLM to iteratively improve and optimize its decisions through self-feedback and dynamic memory.</p><p>This method essentially borrows ideas from reinforcement learning. In the traditional Actor-Critic model, the Actor selects an action $a_t$ based on the current state $s_t$, while the Critic provides an evaluation (e.g., a value function $V(s_t)$ or an action-value function $Q(s_t, a_t)$) and gives feedback to the Actor for policy optimization. Correspondingly, in Reflexion&rsquo;s three main components:</p><ul><li><p><strong>Actor</strong>: Played by the LLM, it outputs text and corresponding actions based on the environment&rsquo;s state (including context and historical information). This can be denoted as:</p>$$
a_t = \pi_\theta(s_t),
$$<p>where $\pi_\theta$ represents the policy based on parameters $\theta$ (i.e., the LLM&rsquo;s weights or prompt). The Actor interacts with the environment and produces a trajectory $\tau = {(s_1,a_1,r_1), \dots, (s_T,a_T,r_T)}$.</p></li><li><p><strong>Evaluator</strong>: Similar to the Critic, the Evaluator receives the trajectory generated by the Actor and outputs a reward signal $r_t$. In the Reflexion framework, the Evaluator can analyze the trajectory using pre-designed heuristic rules or an additional LLM to generate rewards. For example:</p>$$
r_t = R(\tau_t),
$$<p>where $R(\cdot)$ is the reward function based on the current trajectory $\tau_t$.</p></li><li><p><strong>Self-Reflection</strong>: This module adds a self-regulating feedback mechanism on top of the Actor-Critic model. It integrates the current trajectory $\tau$, reward signals ${r_t}$, and historical experience from long-term memory, using its language generation capabilities to produce self-improvement suggestions for the next decision. This feedback is then written to external memory, providing richer context for the Actor&rsquo;s subsequent decisions, thus achieving iterative optimization similar to updating policy parameters $\theta$ through dynamic adjustment of the prompt, without updating the LLM&rsquo;s internal parameters.</p></li></ul><figure class=align-center><img loading=lazy src=Reflexion.png#center alt="Fig. 21. (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm. (Image source: Shinn et al. 2023)" width=100%><figcaption><p>Fig. 21. (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm. (Image source: <a href=https://arxiv.org/abs/2303.11366>Shinn et al. 2023</a>)</p></figcaption></figure><p>The core loop and algorithm of Reflexion are described as follows:</p><ul><li><p><strong>Initialization</strong></p><ul><li>Instantiate the Actor, Evaluator, and Self-Reflection models simultaneously (all can be implemented by LLMs), denoted as $M_a, M_e, M_{sr}$ respectively.</li><li>Initialize the policy $\pi_\theta$ (including the Actor&rsquo;s model parameters or prompt, and initial memory).</li><li>The Actor first generates an initial trajectory $\tau_0$ according to the current policy $\pi_\theta$. After evaluation by $M_e$, $M_{sr}$ generates the first self-reflection text and stores it in long-term memory.</li></ul></li><li><p><strong>Generate Trajectory</strong></p><ul><li>In each iteration, $M_a$ reads the current long-term memory and environmental observations, sequentially outputting actions ${a_1, a_2, \ldots}$, interacting with the environment, and receiving corresponding feedback to form a new trajectory $\tau_t$. $\tau_t$ can be considered the short-term memory for this task, used only in the current iteration.</li></ul></li><li><p><strong>Evaluation</strong></p><ul><li>$M_e$ outputs rewards or scores ${r_1, r_2, \ldots}$ based on the trajectory $\tau_t$ (i.e., the sequence of the Actor&rsquo;s behaviors and environmental feedback). This step corresponds to internal feedback from $M_e$ or results directly provided by the external environment.</li></ul></li><li><p><strong>Self-Reflection</strong></p><ul><li>The $M_{sr}$ module synthesizes the trajectory $\tau_t$ and reward signals ${r_t}$ to generate self-correction or improvement suggestions $\mathrm{sr}_t$ at the language level.</li><li>The reflection text can be seen as an analysis of errors or as providing new inspirational ideas, and it is stored in long-term memory. In practice, we can vectorize the feedback information and store it in a vector database.</li></ul></li><li><p><strong>Update and Repeat</strong></p><ul><li>After appending the latest self-reflection text $\mathrm{sr}_t$ to long-term memory, the Actor can use RAG to retrieve relevant historical information in the next iteration to adjust its policy.</li><li>Repeat the above steps until $M_e$ determines the task is completed or the maximum number of rounds is reached. In this loop, Reflexion relies on the continuous accumulation of <strong>self-reflection + long-term memory</strong> to improve decision-making, rather than directly modifying model parameters.</li></ul></li></ul><p>Below are examples of Reflexion applied to decision-making, programming, and reasoning tasks:</p><figure class=align-center><img loading=lazy src=reflextion_examples.png#center alt="Fig. 22. Reflexion works on decision-making, programming, and reasoning tasks. (Image source: Shinn et al. 2023)" width=100%><figcaption><p>Fig. 22. Reflexion works on decision-making, programming, and reasoning tasks. (Image source: <a href=https://arxiv.org/abs/2303.11366>Shinn et al. 2023</a>)</p></figcaption></figure><p>In an experiment with 100 HotPotQA questions, a comparison between the CoT method and a method with episodic memory showed that the Reflexion method, with an added self-reflection step at the end, significantly improved its search, information retrieval, and reasoning capabilities.</p><figure class=align-center><img loading=lazy src=reflextion_result.png#center alt="Fig. 23. Comparative Analysis of Chain-of-Thought (CoT) and ReAct on the HotPotQA Benchmark. (Image source: Shinn et al. 2023)" width=100%><figcaption><p>Fig. 23. Comparative Analysis of Chain-of-Thought (CoT) and ReAct on the HotPotQA Benchmark. (Image source: <a href=https://arxiv.org/abs/2303.11366>Shinn et al. 2023</a>)</p></figcaption></figure><h3 id=deepseek-r1>DeepSeek R1<a hidden class=anchor aria-hidden=true href=#deepseek-r1>#</a></h3><p><strong>DeepSeek-R1</strong> (<a href=https://arxiv.org/abs/2501.12948>DeepSeek-AI, 2025</a>) represents a major breakthrough in the open-source community&rsquo;s efforts to replicate OpenAI&rsquo;s o1 (<a href=https://openai.com/o1/>OpenAI, 2024</a>), successfully training an advanced reasoning model with deep reflection capabilities through reinforcement learning techniques.</p><blockquote><p>For a detailed training process and technical implementation of DeepSeek R1, please refer to my previous blog post: <a href=https://syhya.github.io/en/posts/2025-01-27-deepseek-r1/>Progress in Replicating OpenAI o1: DeepSeek-R1</a>.</p></blockquote><p>A key transformation during the training of DeepSeek-R1-Zero is that as training progresses, the model gradually <strong>emerges</strong> with an outstanding <strong>self-evolution</strong> capability. This capability is manifested in three core aspects:</p><ul><li><strong>Self-Reflection</strong>: The model can look back and critically evaluate previous reasoning steps.</li><li><strong>Proactive Exploration</strong>: When it finds the current problem-solving path to be suboptimal, it can autonomously search for and try alternative solutions.</li><li><strong>Dynamic Thought Adjustment</strong>: It adaptively adjusts the number of generated tokens based on the complexity of the problem, achieving a deeper thought process.</li></ul><p>This dynamic and spontaneous reasoning behavior significantly enhances the model&rsquo;s ability to solve complex problems, enabling it to tackle challenging tasks more efficiently and accurately.</p><figure class=align-center><img loading=lazy src=deepseek_r1_zero_response_time.png#center alt="Fig. 24. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)" width=90%><figcaption><p>Fig. 24. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: <a href=https://arxiv.org/abs/2501.12948>DeepSeek-AI, 2025</a>)</p></figcaption></figure><p>A typical &ldquo;aha moment&rdquo; also emerged during the training of DeepSeek-R1-Zero. At this critical stage, the model suddenly realized during the reasoning process that its previous line of thought was flawed, and it quickly adjusted its thinking direction, ultimately leading to the correct answer. This phenomenon strongly demonstrates that the model has developed powerful <strong>self-correction</strong> and <strong>reflection capabilities</strong> in its reasoning process, similar to the &ldquo;aha&rdquo; experience in human thinking.</p><figure class=align-center><img loading=lazy src=aha_moment.png#center alt="Fig. 25. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: DeepSeek-AI, 2025)" width=90%><figcaption><p>Fig. 25. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: <a href=https://arxiv.org/abs/2501.12948>DeepSeek-AI, 2025</a>)</p></figcaption></figure><h2 id=memory>Memory<a hidden class=anchor aria-hidden=true href=#memory>#</a></h2><h3 id=human-memory>Human Memory<a hidden class=anchor aria-hidden=true href=#human-memory>#</a></h3><p><strong>Memory</strong> refers to the process of acquiring, storing, retaining, and retrieving information. Human memory is primarily divided into the following three categories:</p><figure class=align-center><img loading=lazy src=category_human_memory.png#center alt="Fig. 26. Categorization of human memory. (Image source: Weng, 2017)" width=100%><figcaption><p>Fig. 26. Categorization of human memory. (Image source: <a href=https://lilianweng.github.io/posts/2023-06-23-agent/>Weng, 2017</a>)</p></figcaption></figure><ul><li><p><strong>Sensory Memory:</strong> Used to briefly retain sensory information after the original stimulus (visual, auditory, tactile, etc.) has disappeared, typically lasting for milliseconds or seconds. Sensory memory is further divided into:</p><ul><li>Iconic Memory: The transient image or visual impression retained by the visual channel, generally lasting 0.25–0.5 seconds, used to form visual continuity in video or animation scenes.</li><li>Echoic Memory: The brief storage of auditory information, which can last for several seconds, allowing a person to replay recently heard sentences or sound clips.</li><li>Haptic Memory: Used to retain brief tactile or force information, generally lasting from milliseconds to seconds, such as the brief finger sensations when typing on a keyboard or reading Braille.</li></ul></li><li><p><strong>Short-Term Memory:</strong> Stores the information we are currently conscious of.</p><ul><li>Lasts for about 20–30 seconds, with a capacity typically of 7±2 items.</li><li>Responsible for the temporary processing and maintenance of information during complex cognitive tasks like learning and reasoning.</li></ul></li><li><p><strong>Long-Term Memory:</strong> Can store information for days to decades, with a virtually unlimited capacity. Long-term memory is divided into:</p><ul><li>Explicit Memory: Can be consciously recalled, including episodic memory (personal experiences, event details) and semantic memory (facts and concepts).</li><li>Implicit Memory: Unconscious memory, primarily related to skills and habits, such as riding a bike or touch typing.</li></ul></li></ul><p>These three types of human memory are intertwined and together form our cognition and understanding of the world. When building LLM Agents, we can draw inspiration from this classification of human memory:</p><ul><li><strong>Sensory Memory</strong> corresponds to the LLM&rsquo;s embedding representations of raw input data (such as text, images, and videos).</li><li><strong>Short-Term Memory</strong> corresponds to the LLM&rsquo;s in-context learning, limited by the model&rsquo;s context window <code>max_tokens</code>. When the conversation length exceeds the window, earlier information is truncated.</li><li><strong>Long-Term Memory</strong> corresponds to an external vector store or database, where the Agent can retrieve historical information on demand using <strong>RAG</strong> technology.</li></ul><h3 id=llm-agent-memory>LLM Agent Memory<a hidden class=anchor aria-hidden=true href=#llm-agent-memory>#</a></h3><p>When an Agent engages in multi-turn interactions with a user or executes multi-step tasks, it can utilize different forms of memory and environmental information to complete its workflow.</p><figure class=align-center><img loading=lazy src=llm_memory_overview.png#center alt="Fig. 27. An overview of the sources, forms, and operations of the memory in LLM-based agents. (Image source: Zhang et al. 2024)" width=100%><figcaption><p>Fig. 27. An overview of the sources, forms, and operations of the memory in LLM-based agents. (Image source: <a href=https://arxiv.org/abs/2404.13501>Zhang et al. 2024</a>)</p></figcaption></figure><ul><li><p><strong>Textual Memory</strong></p><ul><li>Full Interaction: Records all dialogue and action trajectories, helping the Agent trace back the context.</li><li>Recent Interaction: Retains only the dialogue content highly relevant to the current task, reducing unnecessary context usage.</li><li>Retrieved Interaction: The Agent can retrieve historical dialogues or records related to the current task from an external knowledge base and integrate them into the current context.</li><li>External Knowledge: When the Agent encounters a knowledge gap, it can retrieve and acquire additional information through APIs or external storage.</li></ul></li><li><p><strong>Parametric Memory</strong></p><ul><li>Fine-tuning: Expands the model&rsquo;s internal knowledge by injecting new information or knowledge into the LLM.</li><li>Knowledge Editing: Modifies or updates existing knowledge at the model level, achieving dynamic adjustment of the model&rsquo;s internal parametric memory.</li></ul></li><li><p><strong>Environment</strong></p><ul><li>Represents the entities and context involved when the Agent interacts with users and external systems, such as the user Alice, or accessible tools or interfaces (e.g., a ticket booking system, a streaming platform).</li></ul></li><li><p><strong>Agent</strong></p><ul><li>The LLM Agent is responsible for read and write operations, i.e., reading information from the external environment or knowledge base and writing new actions or content.</li><li>It also includes a series of management functions, such as merging, reflecting, and forgetting, to dynamically maintain short-term and long-term memory.</li></ul></li></ul><p>Another example is an Agent completing two different but related tasks, requiring the use of both short-term and long-term memory:</p><ul><li><strong>Task A: Play a video</strong>: The Agent records the current plan, actions, and environmental state (e.g., searching, clicking, playing the video) in its short-term memory. This information is stored in memory and the LLM&rsquo;s context window.</li><li><strong>Task B: Download a game</strong>: The Agent utilizes its long-term memory related to Arcane and League of Legends to quickly find a way to download the game. The figure shows the Agent searching on Google; we can consider Google&rsquo;s knowledge base as an external knowledge source. All new search, click, and download actions are also updated in the short-term memory.</li></ul><figure class=align-center><img loading=lazy src=gui_agent_memory_illustration.png#center alt="Fig. 28: Illustration of short-term memory and long-term memory in an LLM-brained GUI agent. (Image source: Zhang et al. 2024)" width=100%><figcaption><p>Fig. 28: Illustration of short-term memory and long-term memory in an LLM-brained GUI agent. (Image source: <a href=https://arxiv.org/abs/2411.18279>Zhang et al. 2024</a>)</p></figcaption></figure><p>Common memory elements and their corresponding storage methods can be summarized in the following table:</p><table><thead><tr><th><strong>Memory Element</strong></th><th><strong>Memory Type</strong></th><th><strong>Description</strong></th><th><strong>Storage Medium / Method</strong></th></tr></thead><tbody><tr><td>Action</td><td>Short-Term Memory</td><td>Historical action trajectory (e.g., clicking buttons, entering text)</td><td>Memory, LLM Context Window</td></tr><tr><td>Plan</td><td>Short-Term Memory</td><td>The plan for the next operation generated in the previous or current step</td><td>Memory, LLM Context Window</td></tr><tr><td>Execution Result</td><td>Short-Term Memory</td><td>The result returned after an action, error messages, and environmental feedback</td><td>Memory, LLM Context Window</td></tr><tr><td>Environment State</td><td>Short-Term Memory</td><td>Available buttons, page titles, system status, etc., in the current UI environment</td><td>Memory, LLM Context Window</td></tr><tr><td>Self-Experience</td><td>Long-Term Memory</td><td>Historical task trajectories and execution steps</td><td>Database, Disk</td></tr><tr><td>Self-Guidance</td><td>Long-Term Memory</td><td>Guiding rules and best practices summarized from historical successful trajectories</td><td>Database, Disk</td></tr><tr><td>External Knowledge</td><td>Long-Term Memory</td><td>External knowledge bases, documents, or other data sources to assist in task completion</td><td>External Database, Vector Retrieval</td></tr><tr><td>Task Success Metrics</td><td>Long-Term Memory</td><td>Records of task success rates, failure rates, etc., for improvement and analysis</td><td>Database, Disk</td></tr></tbody></table><p>Furthermore, researchers have proposed new training and storage methods to enhance the memory capabilities of LLMs:</p><p><strong>LongMem (Language Models Augmented with Long-Term Memory)</strong> (<a href=https://arxiv.org/abs/2306.07174>Wang, et al. 2023</a>) enables LLMs to remember long historical information. It adopts a decoupled network architecture, freezing the original LLM parameters as a memory encoder, while using an Adaptive Residual Side-Network (SideNet) as a memory retriever for memory checking and reading.</p><figure class=align-center><img loading=lazy src=LongMem.png#center alt="Fig. 29. Overview of the memory caching and retrieval flow of LongMem. (Image source: Wang, et al. 2023)" width=100%><figcaption><p>Fig. 29. Overview of the memory caching and retrieval flow of LongMem. (Image source: <a href=https://arxiv.org/abs/2306.07174>Wang, et al. 2023</a>)</p></figcaption></figure><p>It mainly consists of three parts: <strong>Frozen LLM</strong>, <strong>Residual SideNet</strong>, and <strong>Cached Memory Bank</strong>. Its workflow is as follows:</p><ul><li>First, a long text sequence is split into fixed-length segments. Each segment is encoded layer by layer in the Frozen LLM, and at the $m$-th layer, the attention&rsquo;s $K, V \in \mathbb{R}^{H \times M \times d}$ vector pairs are extracted and cached in the Cached Memory Bank.</li><li>When a new input sequence is encountered, the model retrieves the top-$k$ most relevant key-value pairs from the long-term memory bank based on the current input&rsquo;s query-key. These are then integrated into the subsequent language generation process. Meanwhile, the memory bank removes the oldest content to ensure the availability of the latest contextual information.</li><li>The Residual SideNet fuses the hidden layer outputs of the frozen LLM with the retrieved historical key-values during inference, enabling effective modeling and utilization of context from ultra-long texts.</li></ul><p>Through this decoupled design, LongMem can flexibly schedule massive amounts of historical information without expanding its native context window, balancing both speed and long-term memory capabilities.</p><h2 id=tool-use>Tool Use<a hidden class=anchor aria-hidden=true href=#tool-use>#</a></h2><p><strong>Tool use</strong> is an important component of LLM Agents. By empowering LLMs with the ability to call external tools, their capabilities are significantly expanded: they can not only generate natural language but also obtain real-time information, perform complex calculations, and interact with various systems (such as databases, APIs, etc.), effectively breaking through the limitations of their pre-trained knowledge and avoiding the inefficiency of reinventing the wheel.</p><p>Traditional LLMs primarily rely on pre-trained data for text generation, which makes them deficient in areas like mathematical operations, data retrieval, and real-time information updates. Through tool calling, models can:</p><ul><li><strong>Enhance computational ability:</strong> For example, by calling a specialized calculator tool like <a href=https://gpt.wolfram.com/index.php.en>Wolfram</a>, the model can perform more precise mathematical calculations, compensating for its own arithmetic shortcomings.</li><li><strong>Obtain real-time information:</strong> Using search engines like Google, Bing, or database APIs, the model can access the latest information, ensuring the timeliness and accuracy of the generated content.</li><li><strong>Increase information credibility:</strong> With the support of external tools, the model can cite real data sources, reducing the risk of fabricating information and improving overall credibility.</li><li><strong>Improve system transparency:</strong> Tracking API call records can help users understand the model&rsquo;s decision-making process, providing a degree of interpretability.</li></ul><p>Currently, various LLM applications based on tool calling have emerged, utilizing different strategies and architectures to cover everything from simple tasks to complex multi-step reasoning.</p><h3 id=toolformer>Toolformer<a hidden class=anchor aria-hidden=true href=#toolformer>#</a></h3><p><strong>Toolformer</strong> (<a href=https://arxiv.org/abs/2302.04761>Schick, et al. 2023</a>) is an LLM that can use external tools through simple APIs. It is trained by fine-tuning the GPT-J model, requiring only a few examples for each API. The tools Toolformer learns to call include a question-answering system, Wikipedia search, a calculator, a calendar, and a translation system:</p><figure class=align-center><img loading=lazy src=Toolformer_api.png#center alt="Fig. 30. Examples of inputs and outputs for all APIs used. (Image source: Schick, et al. 2023)" width=100%><figcaption><p>Fig. 30. Examples of inputs and outputs for all APIs used. (Image source: <a href=https://arxiv.org/abs/2302.04761>Schick, et al. 2023</a>)</p></figcaption></figure><h3 id=hugginggpt>HuggingGPT<a hidden class=anchor aria-hidden=true href=#hugginggpt>#</a></h3><p><strong>HuggingGPT</strong> (<a href=https://arxiv.org/abs/2303.17580>Shen, et al. 2023</a>) is a framework that uses ChatGPT as a task planner. It selects available models from <a href=https://huggingface.co/>HuggingFace</a> by reading their descriptions to complete user tasks and summarizes the results based on their execution.</p><figure class=align-center><img loading=lazy src=HuggingGPT.png#center alt="Fig. 31. Illustration of how HuggingGPT works. (Image source: Shen, et al. 2023)" width=100%><figcaption><p>Fig. 31. Illustration of how HuggingGPT works. (Image source: <a href=https://arxiv.org/abs/2303.17580>Shen, et al. 2023</a>)</p></figcaption></figure><p>The system consists of the following four stages:</p><ul><li><strong>Task Planning</strong>: Parses the user&rsquo;s request into multiple sub-tasks. Each task has four attributes: task type, ID, dependencies, and parameters. The paper uses few-shot prompting to guide the model in task decomposition and planning.</li><li><strong>Model Selection</strong>: Assigns each sub-task to different expert models, using a multiple-choice format to determine the most suitable model. Due to the limited context length, models need to be initially filtered based on the task type.</li><li><strong>Task Execution</strong>: The expert models execute their assigned specific tasks and record the results, which are then passed to the LLM for further processing.</li><li><strong>Response Generation</strong>: Receives the execution results from each expert model and finally outputs a summary answer to the user.</li></ul><h2 id=llm-agent-applications>LLM Agent Applications<a hidden class=anchor aria-hidden=true href=#llm-agent-applications>#</a></h2><h3 id=generative-agent>Generative Agent<a hidden class=anchor aria-hidden=true href=#generative-agent>#</a></h3><p>The <strong>Generative Agent</strong> (<a href=https://arxiv.org/abs/2304.03442>Park, et al. 2023</a>) experiment simulates realistic human behavior with 25 virtual characters driven by large language models in a sandbox environment. Its core design integrates memory, retrieval, reflection, and planning/reaction mechanisms, allowing agents to record and review their experiences and extract key information to guide future actions and interactions.</p><figure class=align-center><img loading=lazy src=generative_agent_sandbox.png#center alt="Fig. 32. The screenshot of generative agent sandbox. (Image source: Park, et al. 2023)" width=100%><figcaption><p>Fig. 32. The screenshot of generative agent sandbox. (Image source: <a href=https://arxiv.org/abs/2304.03442>Park, et al. 2023</a>)</p></figcaption></figure><p>The entire system uses a long-term memory module to record all observed events, a retrieval model to extract information based on recency, importance, and relevance, and a reflection mechanism to generate high-level inferences, ultimately translating these outcomes into concrete actions. This simulation demonstrates emergent behaviors such as information diffusion, relationship memory, and social event coordination, providing a realistic simulation of human behavior for interactive applications.</p><figure class=align-center><img loading=lazy src=generative_agent_architecture.png#center alt="Fig. 25. The generative agent architecture. (Park, et al. 2023)" width=100%><figcaption><p>Fig. 25. The generative agent architecture. (<a href=https://arxiv.org/abs/2304.03442>Park, et al. 2023</a>)</p></figcaption></figure><h3 id=webvoyager>WebVoyager<a hidden class=anchor aria-hidden=true href=#webvoyager>#</a></h3><p><strong>WebVoyager</strong> (<a href=https://arxiv.org/abs/2401.13919>He et al. 2024</a>) is an autonomous web interaction agent based on large multimodal models, capable of controlling the mouse and keyboard for web browsing. WebVoyager uses the classic ReAct loop. In each interaction step, it views a browser screenshot annotated with a method similar to <strong>SoM (Set-of-Marks)</strong> (<a href=https://arxiv.org/abs/2310.11441>Yang, et al. 2023</a>), which provides interaction cues by placing numerical labels on web elements, and then decides on the next action. This combination of visual annotation and the ReAct loop allows users to interact with web pages using natural language. For a concrete example, you can refer to the <a href=https://langchain-ai.github.io/langgraph/tutorials/web-navigation/web_voyager/>WebVoyager code</a> using the LangGraph framework.</p><figure class=align-center><img loading=lazy src=WebVoyager.png#center alt="Fig. 33. The overall workflow of WebVoyager. (Image source: He et al. 2024)" width=100%><figcaption><p>Fig. 33. The overall workflow of WebVoyager. (Image source: <a href=https://arxiv.org/abs/2401.13919>He et al. 2024</a>)</p></figcaption></figure><h3 id=openai-operator>OpenAI Operator<a hidden class=anchor aria-hidden=true href=#openai-operator>#</a></h3><p><strong>Operator</strong> (<a href=https://openai.com/index/introducing-operator/>OpenAI, 2025</a>) is an AI agent recently released by OpenAI, designed to autonomously execute web tasks. Operator can interact with web pages like a human user, completing specified tasks by typing, clicking, and scrolling. The core technology behind Operator is the <strong>Computer-Using Agent (CUA)</strong> (<a href=https://openai.com/index/computer-using-agent/>OpenAI, 2025</a>). CUA combines the visual capabilities of GPT-4o with enhanced reasoning abilities gained through reinforcement learning, and it has been specially trained to interact with graphical user interfaces (GUIs), including buttons, menus, and text boxes that users see on the screen.</p><figure class=align-center><img loading=lazy src=cua_overview.png#center alt="Fig. 34. Overview of OpenAI CUA. (Image source: OpenAI, 2025)" width=100%><figcaption><p>Fig. 34. Overview of OpenAI CUA. (Image source: <a href=https://openai.com/index/computer-using-agent/>OpenAI, 2025</a>)</p></figcaption></figure><p>CUA operates in an iterative loop consisting of three stages:</p><ul><li><strong>Perception</strong>: CUA &ldquo;observes&rdquo; the web page content by capturing browser screenshots. This vision-based input allows it to understand the page&rsquo;s layout and elements.</li><li><strong>Reasoning</strong>: Using a chain-of-thought reasoning process, CUA evaluates the next action based on the current and previous screenshots and the actions already taken. This reasoning ability enables it to track task progress, review intermediate steps, and make adjustments as needed.</li><li><strong>Action</strong>: CUA interacts with the browser by simulating mouse and keyboard operations (such as clicking, typing, and scrolling). This allows it to perform a wide range of web tasks without needing specific API integrations.</li></ul><p>The difference between CUA and the pre-existing WebVoyager is that CUA is an agent specifically trained with reinforcement learning, rather than a fixed-flow workflow built by directly calling GPT-4o. Although CUA is still in its early stages and has certain limitations, it has achieved state-of-the-art results on the following benchmarks.</p><figure class=align-center><img loading=lazy src=cua_benchmark.png#center alt="Fig. 35. OpenAI CUA Benchmark Results. (Image source: OpenAI, 2025)" width=100%><figcaption><p>Fig. 35. OpenAI CUA Benchmark Results. (Image source: <a href=https://openai.com/index/computer-using-agent/>OpenAI, 2025</a>)</p></figcaption></figure><h2 id=deep-research>Deep Research<a hidden class=anchor aria-hidden=true href=#deep-research>#</a></h2><p>Deep Research is essentially a report generation system: given a user&rsquo;s query, the system uses an LLM as its core agent to generate a structured and detailed report through multiple rounds of iterative information retrieval and analysis. Currently, the implementation logic of various Deep Research systems can be mainly divided into two approaches: <strong>Workflow Agent</strong> and <strong>RL Agent</strong>.</p><h3 id=workflow-agent-vs-rl-agent>Workflow Agent vs RL Agent<a hidden class=anchor aria-hidden=true href=#workflow-agent-vs-rl-agent>#</a></h3><p>The Workflow Agent approach relies on developers to pre-design workflows and manually craft prompts to organize the entire report generation process. Its main features include:</p><ul><li><strong>Task Decomposition and Flow Orchestration</strong>: The system breaks down the user&rsquo;s query into several sub-tasks, such as generating an outline, information retrieval, and content summarization, and then executes them in a predetermined sequence.</li><li><strong>Fixed Process</strong>: The calls and interactions between different stages are pre-defined, similar to building a static flowchart or a directed acyclic graph (DAG), ensuring that each step has a clear responsibility.</li><li><strong>Dependence on Manual Design</strong>: This method heavily relies on the experience of engineers, who improve output quality through repeated prompt tuning. It is highly applicable but has limited flexibility.</li></ul><p>The <a href=https://langchain-ai.github.io/langgraph/>LangGraph</a> framework can be used to build and orchestrate workflows in the form of a graph.</p><figure class=align-center><img loading=lazy src=langgraph_workflow.png#center alt="Fig. 36. A workflow of the LangGraph. (Image source: LangGraph, 2025)" width=100%><figcaption><p>Fig. 36. A workflow of the LangGraph. (Image source: <a href="https://langchain-ai.github.io/langgraph/tutorials/workflows/?h=workflow">LangGraph, 2025</a>)</p></figcaption></figure><p>The following table compares 5 common workflow and agent patterns:</p><table><thead><tr><th>Pattern</th><th>Core Mechanism</th><th>Advantages</th><th>Limitations</th><th>Use Cases</th></tr></thead><tbody><tr><td><strong>Prompt Chaining</strong></td><td>Sequentially calls LLMs, passing results step-by-step</td><td>Suitable for phased reasoning, more accurate results</td><td>Fixed process, high latency</td><td>Document generation (outline → content), translation polishing</td></tr><tr><td><strong>Parallelization</strong></td><td>Splits sub-tasks for parallel processing, or multi-model voting</td><td>Increases speed, more robust results</td><td>Sub-tasks must be independent, high resource consumption</td><td>Parallel content moderation, multi-model code detection</td></tr><tr><td><strong>Routing</strong></td><td>First classifies, then assigns to different models/processes</td><td>Highly targeted, improves efficiency</td><td>Effectiveness depends on classification accuracy</td><td>Customer service query routing, dynamic model size selection</td></tr><tr><td><strong>Evaluator-Optimizer</strong></td><td>Generate → Evaluate → Optimize iteratively</td><td>Improves result quality, suitable for tasks with standards</td><td>High cost, multiple iterations increase latency</td><td>Translation optimization, multi-round retrieval refinement</td></tr><tr><td><strong>Orchestrator-Worker</strong></td><td>Central orchestration, dynamically decomposes and schedules sub-tasks</td><td>Flexible, can handle complex tasks</td><td>Complex architecture, high scheduling cost</td><td>Multi-file code modification, real-time research integration</td></tr><tr><td><strong>Agent</strong></td><td>LLM makes autonomous decisions, calls tools based on environmental feedback</td><td>Highly flexible, adapts to dynamic environments</td><td>Unpredictable, cost and security need control</td><td>Autonomous research agents, interactive problem-solving</td></tr></tbody></table><p>Currently, there are several open-source projects on GitHub that have implemented workflow-based Deep Research Agents, such as <a href=https://github.com/assafelovic/gpt-researcher>GPT Researcher</a> and <a href=https://github.com/langchain-ai/open_deep_research>open deep research</a>.</p><figure class=align-center><img loading=lazy src=open_deep_research.png#center alt="Fig. 37. An overview of the open deep research. (Image source: LangChain, 2025)" width=100%><figcaption><p>Fig. 37. An overview of the open deep research. (Image source: <a href=https://github.com/langchain-ai/open_deep_research>LangChain, 2025</a>)</p></figcaption></figure><p>The RL Agent is an alternative approach that uses RL to train a reasoning model to optimize the agent&rsquo;s multi-round search, analysis, and report writing process. Its main features include:</p><ul><li><strong>Autonomous Decision-Making Capability</strong>: The system is trained through reinforcement learning, allowing the agent to autonomously judge, make decisions, and adjust its strategy when facing complex search and content integration tasks, thereby generating reports more efficiently.</li><li><strong>Continuous Optimization</strong>: Using a reward mechanism to score and provide feedback on the generation process, the agent can continuously iterate and optimize its own policy, improving the overall quality from task decomposition to the final report.</li><li><strong>Reduced Manual Intervention</strong>: Compared to fixed processes that rely on manually crafted prompts, the reinforcement learning training approach reduces dependence on manual design, making it more suitable for handling variable and complex real-world application scenarios.</li></ul><p>The table below summarizes the main differences between these two approaches:</p><table><thead><tr><th>Feature</th><th>Workflow Agent</th><th>RL Agent</th></tr></thead><tbody><tr><td><strong>Process Design</strong></td><td>Pre-designed fixed workflow with clear task decomposition and flow orchestration</td><td>End-to-end learning, with the agent making autonomous decisions and dynamically adjusting the process</td></tr><tr><td><strong>Autonomous Decision-Making</strong></td><td>Relies on manually designed prompts; the decision process is fixed and immutable</td><td>Through reinforcement learning, the agent can autonomously judge, decide, and optimize its strategy</td></tr><tr><td><strong>Manual Intervention</strong></td><td>Requires extensive manual design and tuning of prompts; significant manual intervention</td><td>Reduces manual intervention, achieving automatic feedback and continuous optimization through a reward mechanism</td></tr><tr><td><strong>Flexibility & Adaptability</strong></td><td>Weaker adaptability to complex or changing scenarios; limited extensibility</td><td>Better suited for variable and complex real-world scenarios, with high flexibility</td></tr><tr><td><strong>Optimization Mechanism</strong></td><td>Optimization mainly relies on engineers&rsquo; experience and adjustments; lacks an end-to-end feedback mechanism</td><td>Utilizes reward feedback from reinforcement learning for continuous, automated performance improvement</td></tr><tr><td><strong>Implementation Difficulty</strong></td><td>Relatively straightforward to implement, but requires tedious process design and maintenance</td><td>Requires training data and computational resources; higher initial development investment, but better long-term results</td></tr><tr><td><strong>Training Required</strong></td><td>No additional training needed; relies solely on manually constructed processes and prompts</td><td>Requires training the agent through reinforcement learning to achieve autonomous decision-making</td></tr></tbody></table><h3 id=openai-deep-research>OpenAI Deep Research<a hidden class=anchor aria-hidden=true href=#openai-deep-research>#</a></h3><p><strong>OpenAI Deep Research</strong> (<a href=https://openai.com/index/introducing-deep-research/>OpenAI, 2025</a>) is an intelligent agent officially released by OpenAI in February 2025. Designed for complex scenarios, it can automatically search, filter, analyze, and integrate multi-source information to ultimately generate high-quality comprehensive reports. The system is built on <a href=https://openai.com/index/openai-o3-mini/>o3</a> as its core base model and incorporates reinforcement learning methods, significantly improving the accuracy and robustness of its multi-round iterative search and reasoning processes.</p><p>Compared to traditional ChatGPT plugin-based search or conventional RAG techniques, OpenAI Deep Research has the following outstanding advantages:</p><ol><li><p><strong>Reinforcement Learning-Driven Iterative Reasoning</strong>
Leveraging the <strong>o3 reasoning model</strong> and reinforcement learning training strategies, the agent can continuously optimize its reasoning path during multi-round search and summarization, effectively reducing the risk of distortion caused by error accumulation.</p></li><li><p><strong>Multi-Source Information Integration and Cross-Validation</strong>
Breaking the limitations of a single search engine, it can simultaneously call upon various authoritative data sources such as specific databases and professional knowledge bases, forming more reliable research conclusions through cross-validation.</p></li><li><p><strong>High-Quality Report Generation</strong>
The training phase introduces an LLM-as-a-judge scoring mechanism and strict evaluation criteria, enabling the system to self-evaluate when outputting reports, thereby generating more clearly structured and rigorously argued professional texts.</p></li></ol><h4 id=training-process>Training Process<a hidden class=anchor aria-hidden=true href=#training-process>#</a></h4><p>The training process for OpenAI Deep Research utilized a <strong>browser interaction dataset</strong> specifically tailored for research scenarios. Through these datasets, the model mastered core browsing functions—including searching, clicking, scrolling, and file parsing—and also learned to use Python tools in a sandboxed environment for computation, data analysis, and visualization. Furthermore, with reinforcement learning training on these browsing tasks, the model can efficiently perform information retrieval, integration, and reasoning across a vast number of websites, quickly locating key information or generating comprehensive research reports.</p><p>These training datasets include both objective tasks with ground-truth answers that can be automatically scored, as well as open-ended tasks equipped with detailed scoring rubrics. During training, the model&rsquo;s responses are rigorously compared against the ground truth or scoring criteria, and the model generates CoT thought processes to allow an evaluation model to provide feedback.</p><p>Additionally, the training process reused the safety datasets accumulated during the o1 model&rsquo;s training phase and was specifically supplemented with safety training data for Deep Research scenarios, ensuring that the model strictly adheres to relevant compliance and safety requirements during automated search and browsing.</p><h4 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h4><p>The model achieved state-of-the-art results on the <strong>Humanity&rsquo;s Last Exam</strong> benchmark (<a href=https://arxiv.org/abs/2501.14249>Phan, et al. 2025</a>), which evaluates AI&rsquo;s ability to answer expert-level questions across various professional domains.</p><figure class=align-center><img loading=lazy src=human_last_exam.png#center alt="Fig. 38. Humanity&rsquo;s Last Exam Benchmark Results. (Image source: OpenAI, 2025)" width=80%><figcaption><p>Fig. 38. Humanity&rsquo;s Last Exam Benchmark Results. (Image source: <a href=https://openai.com/index/introducing-deep-research/>OpenAI, 2025</a>)</p></figcaption></figure><h2 id=future-directions>Future Directions<a hidden class=anchor aria-hidden=true href=#future-directions>#</a></h2><p>Intelligent agents show vast promise, but to achieve reliable and widespread application, the following key challenges still need to be addressed:</p><ul><li><p><strong>Context Window Limitations</strong>: The limited context window of LLMs restricts the amount of information they can process, affecting long-term planning and memory capabilities and reducing task coherence. Current research is exploring external memory mechanisms and context compression techniques to enhance long-term memory and complex information processing abilities. Currently, OpenAI&rsquo;s latest model, <strong>GPT-4.5</strong> (<a href=https://openai.com/index/introducing-gpt-4-5/>OpenAI, 2025</a>), has a maximum context window of 128k tokens.</p></li><li><p><strong>Interface Standardization and Interoperability</strong>: The current natural language-based interaction with tools suffers from inconsistent formatting. The <strong>Model Context Protocol (MCP)</strong> (<a href=https://www.anthropic.com/news/model-context-protocol>Anthropic, 2024</a>) aims to unify the interaction between LLMs and applications through an open standard, reducing development complexity and improving system stability and cross-platform compatibility.</p></li><li><p><strong>Task Planning and Decomposition Capabilities</strong>: Agents struggle to formulate coherent plans for complex tasks, effectively decompose sub-tasks, and lack the ability to dynamically adjust in unexpected situations. More powerful planning algorithms, self-reflection mechanisms, and dynamic policy adjustment methods are needed to flexibly respond to uncertain environments.</p></li><li><p><strong>Computational Resources and Economic Viability</strong>: Deploying large model agents is costly due to multiple API calls and intensive computation, limiting their use in some practical scenarios. Optimization directions include more efficient model architectures, quantization techniques, inference optimization, caching strategies, and intelligent scheduling mechanisms. With the development of specialized GPU hardware like the <a href=https://www.nvidia.com/en-sg/data-center/dgx-b200/>NVIDIA DGX B200</a> and distributed technologies, computational efficiency is expected to improve significantly.</p></li><li><p><strong>Security and Privacy Protection</strong>: Agents face security risks such as prompt injection and need robust authentication, permission control, input validation, and sandboxed environments. For multimodal inputs and external tools, data anonymization, the principle of least privilege, and audit logs must be strengthened to meet security and privacy compliance requirements.</p></li><li><p><strong>Decision Transparency and Explainability</strong>: The difficulty in explaining agent decisions limits their application in high-stakes domains. Enhancing explainability requires the development of visualization tools, chain-of-thought tracking, and decision rationale generation mechanisms to improve decision transparency, build user trust, and meet regulatory requirements.</p></li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] DAIR.AI. <a href=https://www.promptingguide.ai/research/llm-agents>&ldquo;LLM Agents.&rdquo;</a> Prompt Engineering Guide, 2024.</p><p>[2] Sutton, Richard S., and Andrew G. Barto. <a href=http://incompleteideas.net/book/the-book.html>&ldquo;Reinforcement Learning: An Introduction.&rdquo;</a> MIT Press, 2018.</p><p>[3] Weng, Lilian. <a href=https://lilianweng.github.io/posts/2023-06-23-agent/>&ldquo;LLM-powered Autonomous Agents.&rdquo;</a> Lil’Log, 2023.</p><p>[4] Zhou, Yongchao, et al. <a href=https://arxiv.org/abs/2211.01910>&ldquo;Large language models are human-level prompt engineers.&rdquo;</a> The eleventh international conference on learning representations. 2022.</p><p>[5] Zhang, Zhuosheng, et al. <a href=https://arxiv.org/abs/2210.03493>&ldquo;Automatic chain of thought prompting in large language models.&rdquo;</a> arXiv preprint arXiv:2210.03493 (2022).</p><p>[6] Liu, Jiacheng, et al. <a href=https://arxiv.org/abs/2110.08387>&ldquo;Generated knowledge prompting for commonsense reasoning.&rdquo;</a> arXiv preprint arXiv:2110.08387 (2021).</p><p>[7] Lewis, Patrick, et al. <a href=https://arxiv.org/abs/2005.11401>&ldquo;Retrieval-augmented generation for knowledge-intensive nlp tasks.&rdquo;</a> Advances in neural information processing systems 33 (2020): 9459-9474.</p><p>[8] Zhang, Zhuosheng, et al. <a href=https://arxiv.org/abs/2302.00923>&ldquo;Multimodal chain-of-thought reasoning in language models.&rdquo;</a> arXiv preprint arXiv:2302.00923 (2023).</p><p>[9] Diao, Shizhe, et al. <a href=https://arxiv.org/abs/2302.12246>&ldquo;Active prompting with chain-of-thought for large language models.&rdquo;</a> arXiv preprint arXiv:2302.12246 (2023).</p><p>[10] Wei, Jason, et al. <a href=https://arxiv.org/abs/2201.11903>&ldquo;Chain-of-thought prompting elicits reasoning in large language models.&rdquo;</a> Advances in neural information processing systems 35 (2022): 24824-24837.</p><p>[11] Kojima, Takeshi, et al. <a href=https://arxiv.org/abs/2205.11916>&ldquo;Large language models are zero-shot reasoners.&rdquo;</a> Advances in neural information processing systems 35 (2022): 22199-22213.</p><p>[12] Wang, Xuezhi, et al. <a href=https://arxiv.org/abs/2203.11171>&ldquo;Self-consistency improves chain of thought reasoning in language models.&rdquo;</a> arXiv preprint arXiv:2203.11171 (2022).</p><p>[13] Wang, Xuezhi, et al. <a href=https://arxiv.org/abs/2207.00747>&ldquo;Rationale-augmented ensembles in language models.&rdquo;</a> arXiv preprint arXiv:2207.00747 (2022).</p><p>[14] Zelikman, Eric, et al. <a href=https://arxiv.org/abs/2203.14465>&ldquo;Star: Bootstrapping reasoning with reasoning.&rdquo;</a> Advances in Neural Information Processing Systems 35 (2022): 15476-15488.</p><p>[15] Fu, Yao, et al. <a href=https://arxiv.org/abs/2210.00720>&ldquo;Complexity-based prompting for multi-step reasoning.&rdquo;</a> arXiv preprint arXiv:2210.00720 (2022).</p><p>[16] Yao, Shunyu, et al. <a href=https://arxiv.org/abs/2305.10601>&ldquo;Tree of thoughts: Deliberate problem solving with large language models.&rdquo;</a> Advances in neural information processing systems 36 (2023): 11809-11822.</p><p>[17] Yao, Shunyu, et al. <a href=https://arxiv.org/abs/2210.03629>&ldquo;React: Synergizing reasoning and acting in language models.&rdquo;</a> International Conference on Learning Representations (ICLR). 2023.</p><p>[18] Shinn, Noah, et al. <a href=https://arxiv.org/abs/2303.11366>&ldquo;Reflexion: Language agents with verbal reinforcement learning.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 8634-8652.</p><p>[19] Guo, Daya, et al. <a href=https://arxiv.org/abs/2501.12948>&ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.&rdquo;</a> arXiv preprint arXiv:2501.12948 (2025).</p><p>[20] OpenAI. <a href=https://openai.com/o1/>&ldquo;Introducing OpenAI o1&rdquo;</a> OpenAI, 2024.</p><p>[21] Zhang, Zeyu, et al. <a href=https://arxiv.org/abs/2404.13501>&ldquo;A survey on the memory mechanism of large language model based agents.&rdquo;</a> arXiv preprint arXiv:2404.13501 (2024).</p><p>[22] Zhang, Chaoyun, et al. <a href=https://arxiv.org/abs/2411.18279>&ldquo;Large language model-brained gui agents: A survey.&rdquo;</a> arXiv preprint arXiv:2411.18279 (2024).</p><p>[23] Wang, Weizhi, et al. <a href=https://arxiv.org/abs/2306.07174>&ldquo;Augmenting language models with long-term memory.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 74530-74543.</p><p>[24] Schick, Timo, et al. <a href=https://arxiv.org/abs/2302.04761>&ldquo;Toolformer: Language models can teach themselves to use tools.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 68539-68551.</p><p>[25] Shen, Yongliang, et al. <a href=https://arxiv.org/abs/2303.17580>&ldquo;Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 38154-38180.</p><p>[26] Park, Joon Sung, et al. <a href=https://arxiv.org/abs/2304.03442>&ldquo;Generative agents: Interactive simulacra of human behavior.&rdquo;</a> Proceedings of the 36th annual acm symposium on user interface software and technology. 2023.</p><p>[27] He, Hongliang, et al. <a href=https://arxiv.org/abs/2401.13919>&ldquo;WebVoyager: Building an end-to-end web agent with large multimodal models.&rdquo;</a> arXiv preprint arXiv:2401.13919 (2024).</p><p>[28] Yang, Jianwei, et al. <a href=https://arxiv.org/abs/2310.11441>&ldquo;Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v.&rdquo;</a> arXiv preprint arXiv:2310.11441 (2023).</p><p>[29] OpenAI. <a href=https://openai.com/index/introducing-operator/>&ldquo;Introducing Operator.&rdquo;</a> OpenAI, 2025.</p><p>[30] OpenAI. <a href=https://openai.com/index/computer-using-agent/>&ldquo;Computer-Using Agent.&rdquo;</a> OpenAI, 2025.</p><p>[31] OpenAI. <a href=https://openai.com/index/introducing-deep-research/>&ldquo;Introducing Deep Research.&rdquo;</a> OpenAI, 2025.</p><p>[32] Phan, Long, et al. <a href=https://arxiv.org/abs/2501.14249>&ldquo;Humanity&rsquo;s Last Exam.&rdquo;</a> arXiv preprint arXiv:2501.14249 (2025).</p><p>[33] OpenAI. <a href=https://openai.com/index/introducing-gpt-4-5/>&ldquo;Introducing GPT-4.5.&rdquo;</a> OpenAI, 2025.</p><p>[34] Anthropic. <a href=https://www.anthropic.com/news/model-context-protocol>&ldquo;Introducing the Model Context Protocol.&rdquo;</a> Anthropic, 2024.</p><p>[35] LangGraph. <a href="https://langchain-ai.github.io/langgraph/tutorials/workflows/?h=workflow">&ldquo;A workflow of the LangGraph.&rdquo;</a> LangGraph Tutorials, 2025.</p><p>[36] Assaf Elovic. <a href=https://github.com/assafelovic/gpt-researcher>&ldquo;GPT Researcher&rdquo;</a> GitHub Repository, 2025.</p><p>[37] LangChain. <a href=https://github.com/langchain-ai/open_deep_research>&ldquo;Open Deep Research&rdquo;</a> GitHub Repository, 2025.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reprinting or citing the content of this article, please indicate the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui.(Mar 2025). Large Language Model Agents.
<a href=https://syhya.github.io/posts/2025-03-27-llm-agent>https://syhya.github.io/posts/2025-03-27-llm-agent</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025llm-agent</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;Large Language Model Agents&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Mar&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-03-27-llm-agent&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/large-language-model/>Large Language Model</a></li><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/agent/>Agent</a></li><li><a href=https://syhya.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://syhya.github.io/tags/planning/>Planning</a></li><li><a href=https://syhya.github.io/tags/memory/>Memory</a></li><li><a href=https://syhya.github.io/tags/tool-use/>Tool Use</a></li><li><a href=https://syhya.github.io/tags/deep-research/>Deep Research</a></li><li><a href=https://syhya.github.io/tags/react/>ReAct</a></li><li><a href=https://syhya.github.io/tags/reflexion/>Reflexion</a></li><li><a href=https://syhya.github.io/tags/webvoyager/>WebVoyager</a></li><li><a href=https://syhya.github.io/tags/openai-operator/>OpenAI Operator</a></li><li><a href=https://syhya.github.io/tags/cot/>CoT</a></li><li><a href=https://syhya.github.io/tags/tot/>ToT</a></li><li><a href=https://syhya.github.io/tags/workflow/>Workflow</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-04-06-llama/><span class=title>« Prev</span><br><span>The LLaMA Herd</span>
</a><a class=next href=https://syhya.github.io/posts/2025-03-01-train-llm/><span class=title>Next »</span><br><span>Parallelism and Memory Optimization Techniques for Training Large Models</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on x" href="https://x.com/intent/tweet/?text=Large%20Language%20Model%20Agents&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f&amp;hashtags=LargeLanguageModel%2cAI%2cAgent%2cReinforcementLearning%2cPlanning%2cMemory%2cToolUse%2cDeepResearch%2cReAct%2cReflexion%2cWebVoyager%2cOpenAIOperator%2cCoT%2cToT%2cWorkflow"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f&amp;title=Large%20Language%20Model%20Agents&amp;summary=Large%20Language%20Model%20Agents&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f&title=Large%20Language%20Model%20Agents"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on whatsapp" href="https://api.whatsapp.com/send?text=Large%20Language%20Model%20Agents%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on telegram" href="https://telegram.me/share/url?text=Large%20Language%20Model%20Agents&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on ycombinator" href="https://news.ycombinator.com/submitlink?t=Large%20Language%20Model%20Agents&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>