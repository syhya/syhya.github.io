<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>基座大语言模型（长期更新中） | Yue Shui 博客</title>
<meta name=keywords content="AI,NLP,LLM,Pre-training,Post-training,Llama,DeepSeek,Qwen,MoE,MLA,RoPE,GQA"><meta name=description content="
注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。
引言
本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 DeepSeek 和 Qwen 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2025-01-22-base_llm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-01-22-base_llm/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2025-01-22-base_llm/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="基座大语言模型（长期更新中）"><meta property="og:description" content=" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。
引言 本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 DeepSeek 和 Qwen 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-22T12:00:00+08:00"><meta property="article:modified_time" content="2025-01-22T12:00:00+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Pre-Training"><meta property="article:tag" content="Post-Training"><meta property="article:tag" content="LLaMA"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="基座大语言模型（长期更新中）"><meta name=twitter:description content="
注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。
引言
本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 DeepSeek 和 Qwen 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"基座大语言模型（长期更新中）","item":"https://syhya.github.io/zh/posts/2025-01-22-base_llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"基座大语言模型（长期更新中）","name":"基座大语言模型（长期更新中）","description":" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。\n引言 本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 DeepSeek 和 Qwen 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。\n","keywords":["AI","NLP","LLM","Pre-training","Post-training","Llama","DeepSeek","Qwen","MoE","MLA","RoPE","GQA"],"articleBody":" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。\n引言 本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 DeepSeek 和 Qwen 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。\nDeepSeek 系列模型 DeepSeek AI 专注于通用人工智能研究，推出了一系列高性能开源模型，尤其在代码和数学领域表现突出，并积极探索 MoE 等高效架构。\nDeepSeek LLM (Base Models) DeepSeek LLM (DeepSeek-AI, 2023) 是该系列的 foundational work，发布于 2023 年底，提供了强大的开源基础模型。\n核心特点:\n参数规模: 提供 7B 和 67B 两种规模的基础 (Base) 和对话 (Chat) 模型。 训练数据: 在 2 万亿 (2T) tokens 的高质量中英文语料上从头训练。 性能: 67B 模型在代码、数学、推理上优于 LLaMA-2 70B；Chat 版本优于 GPT-3.5。 Scaling Laws 研究: 强调高质量数据的重要性，发现高质量数据下扩展模型比扩展数据更有效。 关键技术:\n架构: 类 LLaMA 架构（RMSNorm, SwiGLU, RoPE），调整了学习率调度器；67B 模型采用 GQA 提升推理效率。上下文长度 4096 tokens。 数据处理: 严格的去重、过滤、重混流程保证数据质量。 对齐: Chat 版本使用 SFT 和 DPO 进行对齐。 意义： DeepSeek-LLM 作为系列开篇，证明了自研的开源模型可以比肩甚至超越国外同级模型（LLaMA2），为后续的 MoE 模型提供了强大的密集模型基线。\nDeepSeekMoE DeepSeekMoE (Dai et al., 2024) 是在混合专家 (MoE) 架构上的重要创新，旨在提升模型效率和专家特化程度。\n核心特点:\n核心创新: 细粒度专家分割: 将 FFN 专家进一步拆分为多个更小的专家，路由时激活多个细粒度专家组合，提高灵活性和表达能力。 共享专家隔离: 引入一部分始终被激活的共享专家处理通用知识，减少路由专家之间的知识冗余，促进路由专家的特化。 效率: 在相似计算成本下性能优于传统 MoE（如 GShard Top-2），以更少计算量（约 40%）即可达到同等规模稠密模型（如 LLaMA2-7B）的性能。 关键技术:\n路由机制: Top-K 路由结合细粒度分割和共享专家。 负载均衡: 采用专家级和设备级平衡损失（后续版本中可能优化为无辅助损失）。 参数效率: 通过专家特化提高参数利用率。 验证模型： 团队发布了 DeepSeekMoE-16B（激活约 2.4B 参数）模型进行验证，证明了该架构的有效性。\n意义： DeepSeekMoE 验证了 MoE 架构的可行性和优越性，为后续更大规模的 MoE 模型（V2, V3）打下基础，并为 MoE 领域贡献了新的架构思路。\nDeepSeek-V2 DeepSeek-V2 (DeepSeek-AI, 2024a) 是一款强大的开源 MoE 模型，平衡了模型强度、训练成本和推理效率。\n核心特点:\n参数规模: 236B 总参数，每个 token 激活 21B 参数 (稀疏激活)。 上下文长度: 支持高达 128K tokens。 核心架构: 结合 DeepSeekMoE 和创新的 多头潜在注意力 (Multi-head Latent Attention, MLA)。 效率提升: 训练成本比 DeepSeek 67B 降低 42.5%。 KV 缓存大小减少 93.3% (通过 MLA)。 最大生成吞吐量提升 5.76 倍。 性能: 发布时成为最强开源 MoE 之一，性能优于 Llama2-70B 等。 关键技术:\nMLA (Multi-head Latent Attention): 通过将 Key-Value 缓存压缩到低秩潜在空间，显著降低长上下文推理时的显存占用，并提升吞吐量。MLA 是实现 128k 上下文的关键。 DeepSeekMoE 应用: 应用于 FFN 层，实现稀疏计算和专家特化。可能采用了无需辅助损失的负载均衡策略。 训练数据: 在 8.1T tokens 高质量多源语料上训练。 训练流程: 预训练 + SFT + RLHF 三阶段。 上下文扩展: 可能使用了如 YaRN 等技术辅助 RoPE 扩展到 128k。 意义： DeepSeek-V2 首次在开源社区大规模验证了 MLA 技术，并成功将 MoE 模型扩展到 200B+ 级别，同时实现了超长上下文处理能力，是开源 MoE 模型发展的重要里程碑。\nDeepSeek-V3 DeepSeek-V3 (DeepSeek-AI, 2024c) 是 DeepSeek 最新的旗舰 MoE 模型，性能接近顶尖闭源模型。\n核心特点:\n参数规模: 671B 总参数，每个 token 激活 37B 参数。 核心架构: 沿用 MLA 和 DeepSeekMoE，并引入新创新。 关键创新: 无辅助损失的负载均衡: 通过改进路由策略（可能涉及动态调整专家偏置）实现专家负载均衡，避免辅助损失对性能的潜在影响。 多 Token 预测 (MTP): 训练时引入预测多个未来 token 的目标，增加训练信号，提升模型性能，并可用于加速推理（推测式解码）。 训练效率: 采用 FP8 混合精度训练和优化的训练框架 (DualPipe)，以极低的成本（约 2.78M H800 GPU·h）完成了超大规模模型的训练。 性能: 在知识、代码、数学、推理、长上下文等基准上达到 SOTA 开源水平，可媲美 GPT-4o、Claude 3.5 Sonnet 等。 关键技术:\n架构: MLA + DeepSeekMoE + 无辅助损失均衡 + MTP。 训练数据: 在 14.8T tokens 高质量、多样化语料上训练，增加了数学、编程、多语言比例。 知识蒸馏: 受益于 DeepSeek-R1 系列模型的推理能力蒸馏（可能在微调阶段）。 Tokenizer: 扩展并优化词汇表（具体大小未明确，但应大于 DeepSeek-LLM）。 意义： DeepSeek-V3 将开源 MoE 模型推向了 600B+ 参数的新高度，并在性能上实现了对顶级闭源模型的有力挑战。它展示了通过架构创新和高效训练技术，开源社区也能在有限资源下训练出超大规模、高性能的模型。\nDeepSeek-Coder DeepSeek-Coder (Guo et al., 2024; DeepSeek-AI, 2024b) 是专为代码智能设计的系列模型。\n核心特点:\n版本: 包括初版 DeepSeek-Coder 和升级版 DeepSeek-Coder-V2。 参数规模 (Coder-V2): 提供 16B (激活 2.4B) 和 236B (激活 21B) 两种 MoE 规模。 训练数据 (Coder-V2): 基于 DeepSeek-V2 checkpoint，额外使用 6T tokens 的代码和数学相关语料进行继续预训练。 核心技术: 仓库级预训练: 增强跨文件理解能力。 填充中间 (FIM): 提升代码补全能力。 多语言支持 (Coder-V2): 支持多达 338 种编程语言。 上下文长度 (Coder-V2): 继承 V2 的 128K tokens。 性能 (Coder-V2): 在代码生成、补全、数学推理等方面表现 SOTA，超越 GPT-4 Turbo 等闭源模型。 关键技术:\n数据构成: 大量高质量项目级代码，覆盖广泛的编程语言。 训练目标: Next Token Prediction + FIM。 架构 (Coder-V2): 基于 DeepSeek-V2 (MoE + MLA)。 意义： DeepSeek-Coder 系列，特别是 V2，填补了开源社区在顶级代码生成模型上的空白，为开发者提供了强大的、可本地部署的 AI 编程助手。\nDeepSeek-R1 DeepSeek-R1 (DeepSeek-AI, 2025) 是利用强化学习 (RL) 显著增强 LLM 推理能力的第一代模型。\n核心特点:\n核心方法: 广泛使用 RL（特别是 GRPO 算法）直接培养模型的复杂推理能力，减少对 SFT 数据的依赖。 关键模型: DeepSeek-R1-Zero: 基于 DeepSeek-V3，通过纯 RL 训练涌现出复杂推理行为（如长链思考、自我反思）。 DeepSeek-R1: 在 R1-Zero 基础上，结合冷启动策略、面向推理的 RL 和拒绝采样/SFT 进行多阶段训练，提升实用性。 性能: 在 AIME、MATH、Codeforces 等高难度推理基准上取得 SOTA 性能，可媲美 OpenAI 的 o1 系列推理模型。 能力蒸馏: 成功将 R1 的强大推理能力蒸馏到一系列更小的密集模型（基于 Qwen 和 Llama 架构，1.5B 到 70B）。 关键技术:\nRL 激励: 主要依赖基于规则的奖励系统或模型自我评估。 训练流程: 精心设计的多阶段流程结合 SFT 和 RL。 涌现能力: RL 驱动模型发展出复杂推理行为。 意义： DeepSeek-R1 探索了通过强化学习提升 LLM 深度推理能力的新范式，证明了模型可以通过自我探索变得更“聪明”，并为社区提供了强大的开源推理模型（及其蒸馏版本）。\nDeepSeek 系列模型特性对比 特性 DeepSeek LLM (2023/11) DeepSeek-V2 (2024/05) DeepSeek-V3 (2024/12) DeepSeek-Coder-V2 (2024/11) DeepSeek-R1 (2025/01) 基础模型 - - - DeepSeek-V2 DeepSeek-V3 模型最大规模 67B (Dense) 236B (MoE) 671B (MoE) 236B (MoE) 671B (MoE, Base for RL) 激活参数量 67B 21B 37B 2.4B / 21B 37B 训练数据量 2T tokens 8.1T tokens 14.8T tokens V2 + 6T Code/Math tokens V3 + RL Data 上下文长度 4K / 32K (扩展) 128K tokens 128K tokens 128K tokens 128K tokens Tokenizer Custom BPE Custom BPE (Optimized) Custom BPE (Expanded) Inherited from V2 Inherited from V3 位置编码 RoPE RoPE RoPE RoPE RoPE 注意力/推理优化 GQA + KV Cache MoE + MLA + KV Cache MoE + MLA + KV Cache MoE + MLA + KV Cache MoE + MLA + KV Cache 归一化 RMSNorm RMSNorm RMSNorm RMSNorm RMSNorm 激活函数 SwiGLU SwiGLU SwiGLU SwiGLU SwiGLU 关键创新 - MLA, DeepSeekMoE MTP, FP8 Training Code/Math Specialization Pure RL Reasoning 模型类别 基座/对话模型 基座/对话模型 (MoE) 基座/对话模型 (MoE) 代码/数学模型 (MoE) 推理增强模型 (MoE) Qwen 系列模型 阿里巴巴达摩院的通义千问（Qwen）系列模型是大型语言模型（LLM）及多模态模型领域的重要贡献者。自 2023 年发布以来，Qwen 系列不断迭代，涵盖从小型到超大规模的基础模型、对话模型（Qwen-Chat），以及针对代码（Qwen-Coder）、数学（Qwen-Math）、视觉（Qwen-VL）、音频（Qwen-Audio）乃至全模态（Qwen-Omni）的专用模型。Qwen 系列以其强大的性能、广泛的多语言支持和持续的开源贡献，在社区中产生了广泛影响。下面我们按照发布时间顺序，依次介绍 Qwen 系列的主要模型。\nQwen (Qwen 1.0) 发布时间： 约 2023 年 8-9 月\n关键特性: 作为 Qwen 系列的开山之作，Qwen 旨在提供强大的中英双语基础模型 (Bai et al., 2023).\n模型规模: 发布了 7B 和 14B 两个尺寸。 训练数据: 在约 2.2 万亿 tokens 的高质量数据上训练，以中英文为主，包含代码数据。 上下文长度: 基础模型支持 2048 tokens，通过增量训练使 7B 支持 32K，14B 支持 8K。 架构: 基于标准 Transformer 解码器，采用 RoPE 位置编码、RMSNorm 和 SwiGLU 激活函数。 对齐: 发布了 Qwen-Chat 模型，通过 SFT 和 RLHF 进行对齐。 性能: Qwen-14B 在 MMLU 上得分约 70，在当时开源模型中表现突出，尤其是在中文任务上 (Bai et al., 2023).\n总结: Qwen 1.0 以其在中英双语上的优异表现和扎实的架构设计，成功打响了 Qwen 系列的名号，为后续快速迭代奠定了基础。\nQwen1.5 发布时间： 约 2024 年 2 月\n关键特性： Qwen1.5 是对 Qwen1.0 的改进版本，重点提升了多语言能力和易用性 (Qwen Team, 2024g).\n模型规模: 开源了 0.5B, 1.5B, 7B, 14B, 32B, 70B (后改为 72B?) 以及一个 110B MoE 模型。 多语言: 显著增强了非英语语言的处理能力。 上下文长度: 全系支持 32K tokens 上下文。 架构: 引入 untied embeddings 和去 Bias 化等微调。与 HuggingFace Transformers 深度集成，使用更方便。 对齐: 改进了 Chat 模型的对话能力和指令跟随。 性能: Qwen1.5-72B 在 MMLU 上得分约 77，相比 Qwen1.0 有显著提升。Chat 模型在 MT-Bench 上表现良好 (Qwen Team, 2024g).\n总结: Qwen1.5 承接 Qwen1.0，在多语言、长上下文和易用性上做了关键改进，并扩展了模型规模选项，为 Qwen2 的大规模升级铺平了道路。\nQwen2 发布时间： 约 2024 年 6 月\n关键特性： Qwen2 是 Qwen1.5 的重要迭代，显著提升了多语言能力、上下文长度和模型规模 (Bai et al., 2024b).\n模型规模: 开源了 0.5B, 1.5B, 7B, 14B, 72B 以及一个 57B-A14B 的 MoE 变体。 训练数据: 扩展到 7 万亿 tokens，覆盖 27 种语言，并增加了代码和数学数据比例。 上下文长度: 全系原生支持 128K tokens 的长上下文。 架构: 延续 Transformer，使用 GQA（Grouped Query Attention）优化大模型推理效率。 对齐: 使用数十万指令数据进行 SFT 和 RLHF，引入了工具使用能力。 多模态扩展: 同期发布了 Qwen2-VL 和 Qwen2-Audio 模型。 性能: Qwen2-72B 在 MMLU 上达到 84.2，HumanEval 64.6，GSM8K 88.7 (Bai et al., 2024b). 在多语言基准（如 Flores）上表现优异。\n总结: Qwen2 通过扩大数据、增长上下文、优化架构（GQA）和引入工具使用，实现了性能的全面飞跃，并为后续 Qwen2.5 奠定了基础。\nQwen2.5 (Base Models) 发布时间： 约 2024 年 9 月\n设计动机: 作为 Qwen2.0 的重要升级，Qwen2.5 旨在全面提升模型的通用能力（知识、推理、语言理解）、指令跟随效果和易用性 (Qwen Team, 2024f). 主要通过大幅增加预训练数据（从 7T 到 18T tokens）、改进对齐策略（百万级指令数据、多阶段 RLHF）和提供更丰富的模型规模（0.5B 到 72B）来实现。\n模型架构: 沿用 Transformer 解码器架构，提供 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B 密集模型。内部训练了 MoE 变体（Turbo/Plus）用于云服务。继续使用 RoPE、RMSNorm、SwiGLU 等优化组件。支持 128K 上下文，并可能利用位置插值技术增强超长上下文外推能力。\n预训练与对齐策略: 预训练数据量达 18 万亿 tokens，包含更多多语种文本、更新的知识、专业领域数据（法律、医疗等）以及代码和数学数据 (Qwen Team, 2024f). 指令微调使用超过 100 万条高质量指令数据，并采用多阶段 RLHF 进行精细对齐，提升人类偏好评分和安全性。\n性能评估: Qwen2.5 系列在各项基准上表现优异。\nMMLU: 72B 模型达到 85+，接近 Llama3-405B (Qwen Team, 2024f). HumanEval: 72B 模型约 65 分 (Bai et al., 2024b). GSM8K: 72B 模型达 89.5 分 (Bai et al., 2024b). MT-Bench / Arena Hard: 72B Chat 模型评分 9.1 / 48.1，居开源模型前列 (Bai et al., 2024b). Qwen2.5-72B 已成为最强的开源通用模型之一。 开源情况: 开源了 0.5B 到 72B 共 7 个尺寸的基础和指令模型（Apache 2.0），可在 Hugging Face 获取 (Qwen Team, 2024f). 提供量化模型、微调脚本和示例。Qwen2.5-Turbo/Plus (MoE) 通过 API 提供。\n特色能力:\n广博的知识储备和多语言能力（支持 30+ 种语言）。 生成结构良好的超长内容。 支持 JSON、表格等结构化输出。 通过系统提示进行角色扮演和语气控制。 强大的工具使用（函数调用）能力。 长对话一致性和较低的幻觉率。 应用场景: 智能客服、内容创作、教育辅导、科研助手、数据分析（结合工具）、医疗与法律辅助等通用 NLP 任务。\n未来展望: 更大规模模型（Qwen 3.0）、更深度多模态融合、自动化持续学习、模型压缩与终端部署、AI 安全与伦理强化。\nQwen2.5-Coder 发布时间： 约 2024 年 9 月\n设计动机: Qwen2.5-Coder 是 Qwen 系列新一代代码专用模型，旨在打造开源顶尖代码模型，缩小与最强闭源代码模型（如 GPT-4 Codex 能力）的差距 (Qwen Team, 2024e). 目标是提升多语言代码生成质量、增强复杂编程任务理解，并结合 Qwen2.5 的通用知识进行代码解释和调试。更名 Coder 也寓意其作为更智能、人性化的“编码助手”。\n模型架构: 基于 Qwen2.5 LLM 主干，针对代码任务进行调整：\n优化的 Tokenizer: 更好地处理代码缩进、符号和关键字。 插入位置支持: 使用特殊提示或标记支持在代码中间插入内容（代码补全）。 长上下文: 支持至少 8K-32K 上下文，便于处理长文件和多文件项目。 依赖 Transformer 自身能力，未引入 AST 解析器等外部模块。 预训练与对齐策略: 在 Qwen2.5 基础上，使用海量高质量、经清洗的多语言代码语料（GitHub、竞赛题解等）进行继续预训练。指令微调阶段使用大规模指令数据（人工编写、用户反馈、自我改进数据），训练模型遵循编程任务描述、生成高质量代码、注释和文档。可能使用 RLHF 优化代码风格和优雅性。\n性能评估: Qwen2.5-Coder 在多项代码基准上取得开源 SOTA 成绩 (Qwen Team, 2024e).\nHumanEval (Python): 32B 模型通过率接近 65%，与 GPT-4o 相当或更高 (Bai et al., 2024b). LeetCode: 在中等难度问题上通过率远超 Code Llama 等。 MultiPL-E: 在 C++, Java 等多语言上表现均衡且领先。 在代码审查、重构和调试任务上表现优于同类开源模型。 开源情况: 全面开源 1.5B, 7B, 32B 三个尺寸的模型权重（Apache 2.0），可在 Hugging Face 和 ModelScope 获取 (Qwen Team, 2024e). 提供 Transformers 支持、量化模型和微调示例。\n特色能力:\n精通多种编程语言。 代码解释、文档生成和错误调试。 智能代码补全和重构。 结合自然语言生成技术文档。 理解常用框架和工具上下文。 应用场景: IDE 插件（补全、调试）、Code Review 自动化、技术支持问答、编程学习与培训、企业内部代码生成服务。\n未来展望: 支持更多语言和领域、与 IDE 环境深度结合、持续学习新框架、更精细的多目标对齐、结合语义搜索处理超大代码库。\nQwQ (Qwen with Questions) 发布时间： 约 2024 年 11 月 (预览版)\n设计动机: QwQ-32B 是一款实验性预览模型，旨在探索提升大模型推理和自我反思能力 (Qwen Team, 2024d). 其名称寓意“提问”和“深思”，希望模型能像哲学家一样“知其无知”，通过主动质疑和反复校正来提高复杂推理（数学、逻辑、常识）的可靠性。\n模型架构: 基于 Qwen2.5-32B-Instruct，核心仍是 Transformer 解码器。通过 Prompt 设计和微调实现增强推理流程：\n显式思考链: 输出答案前会以特殊格式（如“思考：”）输出中间推理步骤和子问题。 不确定性标记: 可能引入机制让模型在不确定时标记或主动寻求澄清。 长上下文增强: 支持 32K 上下文以容纳推理链。 沿用 RoPE、SwiGLU、RMSNorm 等优化组件 (Bai et al., 2023). 预训练与对齐策略: 基于 Qwen2.5-Chat 进行微调。微调数据包含大量需要多步推理的问题及其详细解析过程（类似 CoT 数据）。特别训练模型在回答前先提出澄清性子问题。可能使用 RLHF 奖励详尽、逻辑自洽且正确的推理链。\n性能评估: QwQ-32B 在 GPQA（通用解难问答）基准上得分超过 GPT-4o 和 Claude 3.5 Sonnet (Qwen Team, 2024d). 在 MATH 数据集和 AQUA-RAT（代数问答）上表现优异，推理步骤更完整合理。在代码推理（如 APE）上也取得领先。社区测试表明其在抗幻觉和自我纠错方面有优势。\n开源情况: QwQ-32B-Preview 模型权重已开源（Apache 2.0），可在 Hugging Face 获取 (Qwen Team, 2024d).\n特色能力:\n强烈的质疑意识和审慎态度。 自我检查和纠错能力。 条理清晰的论证过程。 减少幻觉产生。 潜在的教学能力（通过展示解题思路）。 应用场景: 科研辅助、教育答疑、代码审查调试、法律分析、高可靠场景 AI 助手。\n未来展望: 将 QwQ 技术融入更大模型、更精细的反思机制、自动化思维链数据生成、用户可控的思维输出、与外部工具结合、改善效率。\nQwen2.5-1M 发布时间： 约 2024 年 11-12 月\n设计动机： 为了满足处理超长文本（如书籍、大型代码库、长篇报告）的需求，Qwen2.5-1M 将模型上下文长度扩展到 1,000,000 tokens (Qwen Team, 2024c). 主要动机是突破 Transformer 的长度瓶颈，在百万级上下文下实现高效且性能不衰减的推理，同时降低长上下文训练和推理的成本。\n模型架构： 基于 Qwen2.5-7B/14B 模型，针对长序列进行了架构和推理优化：\n稀疏局部注意力: 采用滑动窗口或分层注意力等稀疏模式，降低 $O(n^2)$ 复杂度，实现 3-7 倍推理加速 (Qwen Team, 2024c). 分块预填充 (Chunked Prefill): 将长序列分块输入，利用并行和流水线优化显存和计算峰值 (Qwen Team, 2024c). 长度外推技术: 利用 RoPE 的可外推性或位置插值等方法，无需完全重训即可将 128K 模型扩展到 1M 上下文 (Qwen Team, 2024c). 优化的推理引擎: 定制 CUDA 核、流水线并行、调度优化等工程手段保障端到端速度。 模型采用渐进式训练，逐步增加上下文长度。 预训练与对齐策略： 预训练数据强调长文本，可能使用拼接文档构造超长样本。加入特殊任务（如长文中间摘要、远距离问答）训练长程依赖。指令微调使用百万级长上下文指令数据，训练模型在长文档中搜索、关联信息和遵循指令。RLHF 用于优化长文本生成的稳定性和质量。\n性能评估： Qwen2.5-1M 在长文档问答任务上显著优于同类模型（如 GPT-4o-mini），支持的上下文长度是其 8 倍 (Qwen Team, 2024c). 在长文本摘要、代码跨文件理解、多轮长程对话中表现出色，展现了惊人的记忆力和全局一致性。推理效率通过优化大幅提升，处理 1M token 的速度接近处理 128K 的量级。\n开源情况： 开源了 Qwen2.5-7B-Instruct-1M 和 Qwen2.5-14B-Instruct-1M 模型（Apache 2.0），以及配套的长文本推理框架 (Qwen Team, 2024c). Qwen2.5-Turbo-1M (MoE) 通过 API 提供。\n特色能力：\n超长上下文记忆（百万 token 级别）。 生成全局一致的超长文稿。 即时跨文档查询与精确引用。 耐久对话与持续学习能力。 复杂的多文档、跨篇章推理。 应用场景： 企业知识库问答、法律分析、学术研究、金融分析、大型代码库理解、超长文学创作、长对话陪伴。\n未来展望： 更长甚至无限上下文、长程交互算法改进、片段版权与来源追踪、稀疏 MoE 结合、长上下文评测标准、训练成本进一步降低。\nQwen2.5-VL 发布时间： 约 2024 年 11-12 月\n设计动机： 作为 Qwen 系列新一代旗舰视觉语言模型，Qwen2.5-VL 旨在提升对任意分辨率图像的适应性、增强长视频理解能力，并在多模态推理和多语言图像理解上达到业界领先水平 (Qwen Team, 2024b). 它希望解决传统视觉模型缩放图像导致细节丢失的问题，并满足对长时序视频分析的需求。\n模型架构： Qwen2.5-VL 采用 ViT 视觉编码器与 Qwen2.5 语言模型结合的架构。关键改进包括：\n原生动态分辨率 (Naive Dynamic Resolution): 允许输入任意尺寸图像，ViT 在训练时处理不同数量的 patch，保留图像原始细节和比例 (Bai et al., 2024). 增强的多模态 RoPE (M-ROPE): 优化了对 2D 图像和 3D 时空（视频）位置的编码，通过将位置拆分为时间、高度、宽度维度并应用旋转变换，显式建模视觉信号的空间/时间对应关系 (Bai et al., 2024). 视觉编码器本身参数量较大（约 6 亿），采用先预训练 ViT 再与 LLM 联合微调的策略。 预训练与对齐策略： 使用了包含亿级图文对（不同分辨率）、百万级视频文本对、多语言 OCR 数据在内的超大规模多模态数据集 (Qwen Team, 2024b). 预训练目标是根据视觉输入生成文本。训练中随机改变图像尺寸和视频长度以适应动态分辨率和 M-ROPE。对齐阶段进行指令微调和基于人类反馈的 RLHF，优化答案质量、格式遵循和视觉相关的安全性。\n性能评估： Qwen2.5-VL 在 DocVQA、RealWorldQA、MathVista 等图像理解基准上取得 SOTA 或接近 SOTA 的成绩，超越了许多开源和闭源模型 (Qwen Team, 2024b). 能够处理长达 20 分钟的视频问答，并在多语言 OCR 和图像文本理解上表现突出。Qwen2.5-VL-72B 在 OpenCompass 等综合评测中位居前列 (Qwen Team, 2024b).\n开源情况： 开源了 Qwen2.5-VL-7B 和 Qwen2.5-VL-2B 模型权重（Apache 2.0），可在 Hugging Face 获取 (Qwen Team, 2024b). Qwen2.5-VL-72B 通过阿里云 API 提供服务。官方提供了推理代码、优化工具（vLLM 适配）和评测数据集示例。\n特色能力：\n处理任意分辨率和长图文档（如小票、扫描件）。 复杂场景的多模态推理。 同时处理多张图像或图像+视频。 具备一定的视觉代理（UI 操作规划）能力。 结合 Qwen2.5 的知识库进行视觉问答。 应用场景： 文档信息抽取、安防监控、媒体内容审核、多媒体搜索推荐、辅助创作、机器人视觉。\n未来展望： 融合更丰富模态（3D）、引入推理链、结合知识库、模型压缩部署、更复杂的视觉指令交互、多模态安全提升。\nQVQ (Qwen with Vision \u0026 Questions) 发布时间： 约 2024 年 12 月 (预览版)\n设计动机： QVQ-72B 是 Qwen 团队推出的实验性多模态推理模型，旨在提升模型在复杂视觉推理任务上的能力，使其不仅能描述图像，还能进行涉及常识和多步推理的深度思考 (Qwen Team, 2024d). 其名称寓意“质疑”，希望模型能像哲学家一样“看”世界并提出问题，探索模型的“自我反思”能力。\n模型架构： 基于 Qwen2.5-VL-72B，保留了 ViT 编码器、M-ROPE 和 72B 解码器。针对推理进行了优化：\n插入式思维链 (Chain-of-Thought): 模型在解码时可输出隐藏的推理步骤，再给出最终答案，训练时可能使用了 CoT 格式的数据。 更大的上下文窗口: 可能支持 128K 或更长上下文以容纳复杂推理链。 多模态融合: 深度融合视觉感知与语言推理能力。 预训练与对齐策略: 基于 Qwen2.5-VL 进行微调，加入大量需要多步视觉推理的指令数据（如图片逻辑题、看图推断社会现象等）。可能引入了让模型在不确定时主动提问或标记不确定性的训练策略。\n性能评估: QVQ-72B 在高难度的多模态基准 MMMU 上取得了 70.3% 的准确率，达到 SOTA 水平，超越了 GPT-4o 等模型 (Qwen Team, 2024d). 这证明了其在复杂视觉推理上的强大实力。\n开源情况: 作为研究预览版，QVQ-72B-Chat 模型权重已开源 (Qwen Team, 2024d).\n特色能力:\n深度视觉推理（超越表面描述）。 潜在的自我反思和提问能力。 结合视觉信息进行多步逻辑推断。 应用场景: 复杂图像分析、视觉常识问答、需要深度理解的图像场景（如医学影像初步分析、科学图表解读）。\n未来展望: 将 QVQ 的推理能力融入主流多模态模型，提升可靠性和可解释性。\nQwen2.5-Omni 发布时间： 约 2024 年 12 月 / 2025 年初\n设计动机： Qwen2.5-Omni 旨在打造一个能够处理文本、图像、音频、视频多种模态输入的“全能”（omni-modal）AI 助手，实现端到端的感知和实时交互 (Qwen Team, 2024a). 其目标是模拟人类多感官交互，让 AI 能够同时“看”、“听”、“说”，提供自然流畅的人机对话体验。\n模型架构： Qwen2.5-Omni 采用了创新的 “思考者-说话人”（Thinker-Talker）架构 (Qwen Team, 2024a).\nThinker: 基于 Transformer 解码器，整合了视觉和音频编码器，负责理解多模态输入并生成高层语义表示和文本输出。引入了时间对齐多模态 RoPE (TMRoPE) 来同步视频帧和音频序列的时间轴 (Qwen Team, 2024a). Talker: 采用双轨自回归 Transformer 解码器，接收 Thinker 的表示，以流式方式合成语音输出。使用了滑动窗口离散表示 Transformer (sliding-window DiT) 来实现低延迟的实时语音合成 (Qwen Team, 2024a). 该架构实现了端到端训练，无需独立的 TTS 模块。 预训练与对齐策略： 模型在包含文本、图文对、音文对、视频文本对的大规模多模态语料上进行预训练。损失函数结合了文本生成的交叉熵损失和语音 codec token 生成的损失。对齐阶段采用监督微调 (SFT) 和人类反馈对齐（包括文本 RLHF 和语音质量优化），确保模型遵循指令并生成高质量、安全的多模态响应 (Qwen Team, 2024a).\n性能评估： Qwen2.5-Omni 在多模态综合基准 Omni-Bench 上达到 SOTA 水平。其单项能力（语音理解、视觉问答）可媲美同规模的专用模型（如 Qwen2-Audio, Qwen2.5-VL）。在端到端语音指令跟随任务上表现出色，语音合成质量也超过了许多实时 TTS 系统 (Qwen Team, 2024a).\n开源情况： 已开源 Qwen2.5-Omni-7B 模型权重（基础版和聊天版），采用 Apache 2.0 许可证 (Qwen Team, 2024a). 更大模型通过阿里云 API 提供服务。相关的推理代码和语音解码库也在 GitHub 提供。\n特色能力：\n全模态输入输出（文本、图像、音频、视频输入；文本+语音输出）。 实时对话与连续理解（流式处理）。 复杂的跨模态推理。 支持情感和多语种的语音合成。 继承 Qwen 系列的工具使用和行动规划能力。 应用场景： 智能客服、教育科普、无障碍辅助、内容创作与审核、机器人控制等。\n未来展望： 更大规模模型、更长时空理解、更自然的语音合成、多模态知识推理、与外部工具交互、推理性能优化。\nQwen 系列模型特性对比 特性 Qwen (1.0) (2023/09) Qwen1.5 (2024/02) Qwen2 (2024/06) Qwen2.5 (2024/09+) Qwen2.5-Specialized (Coder/VL/Omni/1M/QwQ/QVQ) 模型规模 (开源) 7B, 14B 0.5B-14B, 32B, 70B? 0.5B-14B, 72B (+MoE) 0.5B-14B, 32B, 72B 1.5B-72B (Depends on model) 训练数据量 ~2.2T tokens \u003e 2.2T (多语言增强) 7T+ tokens 18T+ tokens Base + Domain Specific Data 语言覆盖 中英为主 多语言增强 27+ 种语言 30+ 种语言 Base + Domain Specific (e.g., Code languages) 上下文长度 2K (可扩展 8K/32K) 32K 128K 128K (可扩展 1M) 32K - 1M (Depends on model) Tokenizer BPE BPE BPE BPE BPE (可能针对代码等优化) 位置编码 RoPE RoPE RoPE RoPE (M-ROPE for VL/Omni) RoPE / M-ROPE 注意力/推理优化 MHA MHA (+Untied Emb) MHA / GQA MHA / GQA GQA / Sparse Attention (1M) 归一化 RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) 激活函数 SwiGLU SwiGLU SwiGLU SwiGLU SwiGLU 模型类别 基座/对话模型 基座/对话模型 基座/对话/多模态/MoE 基座/对话/MoE 代码/视觉/音频/全模态/长上下文/推理增强 开源许可 Apache 2.0 / Tongyi Lic. Apache 2.0 Apache 2.0 Apache 2.0 Apache 2.0 关键技术解析 以下是目前基座大模型所采用的关键技术的详细解析，包括数学公式和相关说明。\nMulti-Head Latent Attention (MLA) 原理简介： Multi-Head Latent Attention (MLA) 是 DeepSeek-V2/V3 中引入的一种创新的注意力机制，旨在解决超长上下文（如 128k tokens）带来的 KV 缓存爆炸和计算瓶颈问题 (DeepSeek-AI, 2024a)。\n机制： MLA 的核心思想是将长序列的 Key 和 Value 信息压缩到一个固定大小（远小于序列长度）的潜在表示 (latent representation) 中。当前的 Query 不再直接关注原始的长序列 K/V，而是关注这个压缩后的潜在 K/V。这可以通过学习一个低秩投影或引入可学习的潜在 token 来实现。\n优势：\n极大压缩 KV 缓存： KV 缓存大小从与序列长度 \\(L\\) 相关降低到与潜在表示大小 \\(M\\) 相关（\\(M \\ll L\\)），DeepSeek-V2 报告减少了 93.3%。 提升推理吞吐量： 减少了内存访问和通信开销，显著提高了长序列生成的吞吐量（DeepSeek-V2 报告提升 5.76 倍）。 支持超长上下文： 使得处理 128k 甚至更长的上下文成为可能。 挑战： 压缩可能导致信息损失，需要精心设计潜在表示和训练策略（如 DeepSeek-V3 中的解耦 RoPE）来保持性能。\n应用： DeepSeek-V2 和 DeepSeek-V3 使用 MLA 成功实现了 128k 的上下文窗口。MLA 被认为是 GQA/MQA 之后处理长上下文的更先进方案。\nMixture-of-Experts (MoE) 原理简介： Mixture-of-Experts (MoE) 是一种稀疏激活的神经网络架构。它将网络中的某些层（通常是 FFN 层）替换为多个并行的“专家”子网络，并通过一个门控网络 (gating network) 为每个输入 token 选择性地激活其中少数几个专家进行计算。\n机制：\n专家 (Experts): \\(N\\) 个并行的子网络（如 FFN）。 门控网络 (Gating Network): 根据输入 \\(x\\) 计算每个专家的权重或选择概率 \\(G(x)\\)。 稀疏激活: 通常只选择 Top-K (K 通常为 1 或 2) 个专家进行计算。 输出: 选中专家的输出根据门控权重加权求和：\\(y = \\sum_{i \\in \\text{TopK}(G(x))} G(x)_i \\cdot E_i(x)\\)。 优势：\n参数规模与计算解耦： 允许模型拥有巨大的总参数量（通过增加专家数量），但每次前向传播的计算量只取决于激活的少数专家，远低于同等总参数的密集模型。 专家特化： 理论上，不同专家可以学习处理不同类型的数据或任务，提高模型能力和泛化性。 挑战：\n负载均衡： 需要确保专家被均匀利用，避免部分专家过载或闲置。通常需要引入辅助损失。 通信开销： 在分布式训练和推理中，需要在不同设备间进行 All-to-All 通信以路由 token 到正确的专家。 训练稳定性： MoE 训练可能比密集模型更不稳定。 DeepSeek MoE 创新： DeepSeekMoE 架构通过细粒度专家分割和共享专家隔离来提升专家特化程度和负载均衡效果 (Dai et al., 2024)。DeepSeek-V3 进一步实现了无辅助损失的负载均衡。\n应用： Google 的 Switch Transformer、GLaM 以及 DeepSeek 系列的 MoE、V2、V3、Coder-V2、R1 都采用了 MoE 架构。MoE 被认为是扩展模型规模、突破密集模型瓶颈的关键技术之一。\n参考文献 Ainslie, J., et al. (2023). GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv preprint arXiv:2305.13245. (Link) AI at Meta. (2024a). Introducing Meta Llama 3: The most capable openly available LLM to date. (Link) AI at Meta. (2024b). Introducing Llama 3.1: Our most capable models to date. (Link) Chi, Z., et al. (2024). Llama Guard 3 Vision: Advancing Multimodal Safety. arXiv preprint arXiv:2411.10414. (Link) Dai, W., et al. (2024). DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. arXiv preprint arXiv:2401.06066. (Link) Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv preprint arXiv:2205.14135. (Link) DeepSeek-AI. (2023). DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. GitHub Repository. (Link) (Corresponds to arXiv:2401.02954) DeepSeek-AI. (2024a). DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model. arXiv preprint arXiv:2405.04434. (Link) DeepSeek-AI. (2024b). DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. GitHub Repository. (Link) DeepSeek-AI. (2024c). DeepSeek-V3 Technical Report. arXiv preprint arXiv:2412.19437. (Link) DeepSeek-AI. (2025). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948. (Link) Grattafiori, D., et al. (2024). The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783. (Link) Guo, D., et al. (2024). DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence. arXiv preprint arXiv:2401.14196. (Link) Inan, H., et al. (2023). Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. arXiv preprint arXiv:2312.06674. (Link) Rozière, B., et al. (2023). Code Llama: Open Foundation Models for Code. arXiv preprint arXiv:2308.12950. (Link) Shazeer, N. (2020). GLU Variants Improve Transformer. arXiv preprint arXiv:2002.05202. (Link) Su, J., et al. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint arXiv:2104.09864. (Link) Touvron, H., et al. (2023a). LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971. (Link) Touvron, H., et al. (2023b). Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288. (Link) Vidgen, B., et al. (2024). Building a Taxonomy and Datasets for Safe AI Dialogue. arXiv preprint arXiv:2404.12241. (Link) Zhang, B., \u0026 Sennrich, R. (2019). Root Mean Square Layer Normalization. Advances in Neural Information Processing Systems 32 (NeurIPS 2019). (Link) Zhu, Y., et al. (2024). TransMLA: Multi-Head Latent Attention Is All You Need. arXiv preprint arXiv:2502.07864. (Link) ","wordCount":"11023","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2025-01-22T12:00:00+08:00","dateModified":"2025-01-22T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2025-01-22-base_llm/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">基座大语言模型（长期更新中）</h1><div class=post-meta><span title='2025-01-22 12:00:00 +0800 +0800'>2025-01-22</span>&nbsp;·&nbsp;23 分钟&nbsp;·&nbsp;11023 字&nbsp;·&nbsp;Yue Shui</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#引言>引言</a></li><li><a href=#deepseek-系列模型>DeepSeek 系列模型</a><ul><li><a href=#deepseek-llm-base-models>DeepSeek LLM (Base Models)</a></li><li><a href=#deepseekmoe>DeepSeekMoE</a></li><li><a href=#deepseek-v2>DeepSeek-V2</a></li><li><a href=#deepseek-v3>DeepSeek-V3</a></li><li><a href=#deepseek-coder>DeepSeek-Coder</a></li><li><a href=#deepseek-r1>DeepSeek-R1</a></li><li><a href=#deepseek-系列模型特性对比>DeepSeek 系列模型特性对比</a></li></ul></li><li><a href=#qwen-系列模型>Qwen 系列模型</a><ul><li><a href=#qwen-qwen-10>Qwen (Qwen 1.0)</a></li><li><a href=#qwen15>Qwen1.5</a></li><li><a href=#qwen2>Qwen2</a></li><li><a href=#qwen25-base-models>Qwen2.5 (Base Models)</a></li><li><a href=#qwen25-coder>Qwen2.5-Coder</a></li><li><a href=#qwq-qwen-with-questions>QwQ (Qwen with Questions)</a></li><li><a href=#qwen25-1m>Qwen2.5-1M</a></li><li><a href=#qwen25-vl>Qwen2.5-VL</a></li><li><a href=#qvq-qwen-with-vision--questions>QVQ (Qwen with Vision & Questions)</a></li><li><a href=#qwen25-omni>Qwen2.5-Omni</a></li></ul></li><li><a href=#qwen-系列模型特性对比>Qwen 系列模型特性对比</a></li><li><a href=#关键技术解析>关键技术解析</a><ul><li><a href=#multi-head-latent-attention-mla>Multi-Head Latent Attention (MLA)</a></li><li><a href=#mixture-of-experts-moe>Mixture-of-Experts (MoE)</a></li></ul></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p><strong>注意</strong>: 本文<strong>正在更新中</strong>，内容只是<strong>草稿版本</strong>，并不完善，后续会有变动。请随时关注最新版本。</p></blockquote><h2 id=引言>引言<a hidden class=anchor aria-hidden=true href=#引言>#</a></h2><p>本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 <strong>DeepSeek</strong> 和 <strong>Qwen</strong> 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。</p><h2 id=deepseek-系列模型>DeepSeek 系列模型<a hidden class=anchor aria-hidden=true href=#deepseek-系列模型>#</a></h2><p><strong>DeepSeek AI</strong> 专注于通用人工智能研究，推出了一系列高性能开源模型，尤其在代码和数学领域表现突出，并积极探索 MoE 等高效架构。</p><h3 id=deepseek-llm-base-models>DeepSeek LLM (Base Models)<a hidden class=anchor aria-hidden=true href=#deepseek-llm-base-models>#</a></h3><p><strong>DeepSeek LLM</strong> (<a href=https://github.com/deepseek-ai/deepseek-llm>DeepSeek-AI, 2023</a>) 是该系列的 foundational work，发布于 2023 年底，提供了强大的开源基础模型。</p><p><strong>核心特点:</strong></p><ul><li><strong>参数规模</strong>: 提供 <strong>7B</strong> 和 <strong>67B</strong> 两种规模的基础 (Base) 和对话 (Chat) 模型。</li><li><strong>训练数据</strong>: 在 <strong>2 万亿 (2T) tokens</strong> 的高质量中英文语料上从头训练。</li><li><strong>性能</strong>: <strong>67B 模型</strong>在代码、数学、推理上优于 LLaMA-2 70B；Chat 版本优于 GPT-3.5。</li><li><strong>Scaling Laws 研究</strong>: 强调高质量数据的重要性，发现高质量数据下扩展模型比扩展数据更有效。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>架构</strong>: 类 LLaMA 架构（RMSNorm, SwiGLU, RoPE），调整了学习率调度器；67B 模型采用 <strong>GQA</strong> 提升推理效率。上下文长度 4096 tokens。</li><li><strong>数据处理</strong>: 严格的去重、过滤、重混流程保证数据质量。</li><li><strong>对齐</strong>: Chat 版本使用 <strong>SFT</strong> 和 <strong>DPO</strong> 进行对齐。</li></ul><p><strong>意义：</strong> DeepSeek-LLM 作为系列开篇，证明了自研的开源模型可以比肩甚至超越国外同级模型（LLaMA2），为后续的 MoE 模型提供了强大的密集模型基线。</p><h3 id=deepseekmoe>DeepSeekMoE<a hidden class=anchor aria-hidden=true href=#deepseekmoe>#</a></h3><p><strong>DeepSeekMoE</strong> (<a href=https://arxiv.org/abs/2401.06066>Dai et al., 2024</a>) 是在<strong>混合专家 (MoE)</strong> 架构上的重要创新，旨在提升模型效率和专家特化程度。</p><p><strong>核心特点:</strong></p><ul><li><strong>核心创新</strong>:<ol><li><strong>细粒度专家分割</strong>: 将 FFN 专家进一步拆分为多个更小的专家，路由时激活多个细粒度专家组合，提高灵活性和表达能力。</li><li><strong>共享专家隔离</strong>: 引入一部分始终被激活的共享专家处理通用知识，减少路由专家之间的知识冗余，促进路由专家的特化。</li></ol></li><li><strong>效率</strong>: 在相似计算成本下性能优于传统 MoE（如 GShard Top-2），以更少计算量（约 40%）即可达到同等规模稠密模型（如 LLaMA2-7B）的性能。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>路由机制</strong>: Top-K 路由结合细粒度分割和共享专家。</li><li><strong>负载均衡</strong>: 采用专家级和设备级平衡损失（后续版本中可能优化为无辅助损失）。</li><li><strong>参数效率</strong>: 通过专家特化提高参数利用率。</li></ul><p><strong>验证模型：</strong> 团队发布了 <strong>DeepSeekMoE-16B</strong>（激活约 2.4B 参数）模型进行验证，证明了该架构的有效性。</p><p><strong>意义：</strong> DeepSeekMoE 验证了 MoE 架构的可行性和优越性，为后续更大规模的 MoE 模型（V2, V3）打下基础，并为 MoE 领域贡献了新的架构思路。</p><h3 id=deepseek-v2>DeepSeek-V2<a hidden class=anchor aria-hidden=true href=#deepseek-v2>#</a></h3><p><strong>DeepSeek-V2</strong> (<a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024a</a>) 是一款强大的开源 <strong>MoE</strong> 模型，平衡了模型强度、训练成本和推理效率。</p><p><img alt="alt text" loading=lazy src=/zh/posts/2025-01-22-base_llm/image-2.png></p><p><strong>核心特点:</strong></p><ul><li><strong>参数规模</strong>: <strong>236B</strong> 总参数，每个 token 激活 <strong>21B</strong> 参数 (稀疏激活)。</li><li><strong>上下文长度</strong>: 支持高达 <strong>128K tokens</strong>。</li><li><strong>核心架构</strong>: 结合 <strong>DeepSeekMoE</strong> 和创新的 <strong>多头潜在注意力 (Multi-head Latent Attention, MLA)</strong>。</li><li><strong>效率提升</strong>:<ul><li>训练成本比 DeepSeek 67B 降低 42.5%。</li><li><strong>KV 缓存大小减少 93.3%</strong> (通过 MLA)。</li><li>最大生成吞吐量提升 5.76 倍。</li></ul></li><li><strong>性能</strong>: 发布时成为最强开源 MoE 之一，性能优于 Llama2-70B 等。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>MLA (Multi-head Latent Attention)</strong>: 通过将 Key-Value 缓存压缩到低秩潜在空间，<strong>显著降低长上下文推理时的显存占用</strong>，并提升吞吐量。MLA 是实现 128k 上下文的关键。</li><li><strong>DeepSeekMoE 应用</strong>: 应用于 FFN 层，实现稀疏计算和专家特化。可能采用了无需辅助损失的负载均衡策略。</li><li><strong>训练数据</strong>: 在 <strong>8.1T tokens</strong> 高质量多源语料上训练。</li><li><strong>训练流程</strong>: 预训练 + SFT + RLHF 三阶段。</li><li><strong>上下文扩展</strong>: 可能使用了如 YaRN 等技术辅助 RoPE 扩展到 128k。</li></ul><p><strong>意义：</strong> DeepSeek-V2 首次在开源社区大规模验证了 MLA 技术，并成功将 MoE 模型扩展到 200B+ 级别，同时实现了超长上下文处理能力，是开源 MoE 模型发展的重要里程碑。</p><h3 id=deepseek-v3>DeepSeek-V3<a hidden class=anchor aria-hidden=true href=#deepseek-v3>#</a></h3><p><strong>DeepSeek-V3</strong> (<a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024c</a>) 是 DeepSeek 最新的旗舰 <strong>MoE</strong> 模型，性能接近顶尖闭源模型。</p><p><img alt="alt text" loading=lazy src=/zh/posts/2025-01-22-base_llm/image-1.png></p><p><strong>核心特点:</strong></p><ul><li><strong>参数规模</strong>: <strong>671B</strong> 总参数，每个 token 激活 <strong>37B</strong> 参数。</li><li><strong>核心架构</strong>: 沿用 <strong>MLA</strong> 和 <strong>DeepSeekMoE</strong>，并引入新创新。</li><li><strong>关键创新</strong>:<ol><li><strong>无辅助损失的负载均衡</strong>: 通过改进路由策略（可能涉及动态调整专家偏置）实现专家负载均衡，避免辅助损失对性能的潜在影响。</li><li><strong>多 Token 预测 (MTP)</strong>: 训练时引入预测多个未来 token 的目标，增加训练信号，提升模型性能，并可用于加速推理（推测式解码）。</li></ol></li><li><strong>训练效率</strong>: 采用 <strong>FP8</strong> 混合精度训练和优化的训练框架 (DualPipe)，以极低的成本（约 2.78M H800 GPU·h）完成了超大规模模型的训练。</li><li><strong>性能</strong>: 在知识、代码、数学、推理、长上下文等基准上达到 <strong>SOTA 开源水平</strong>，可媲美 GPT-4o、Claude 3.5 Sonnet 等。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>架构</strong>: MLA + DeepSeekMoE + 无辅助损失均衡 + MTP。</li><li><strong>训练数据</strong>: 在 <strong>14.8T tokens</strong> 高质量、多样化语料上训练，增加了数学、编程、多语言比例。</li><li><strong>知识蒸馏</strong>: 受益于 <strong>DeepSeek-R1</strong> 系列模型的推理能力蒸馏（可能在微调阶段）。</li><li><strong>Tokenizer</strong>: 扩展并优化词汇表（具体大小未明确，但应大于 DeepSeek-LLM）。</li></ul><p><strong>意义：</strong> DeepSeek-V3 将开源 MoE 模型推向了 600B+ 参数的新高度，并在性能上实现了对顶级闭源模型的有力挑战。它展示了通过架构创新和高效训练技术，开源社区也能在有限资源下训练出超大规模、高性能的模型。</p><h3 id=deepseek-coder>DeepSeek-Coder<a hidden class=anchor aria-hidden=true href=#deepseek-coder>#</a></h3><p><strong>DeepSeek-Coder</strong> (<a href=https://arxiv.org/abs/2401.14196>Guo et al., 2024</a>; <a href=https://github.com/deepseek-ai/DeepSeek-Coder-V2>DeepSeek-AI, 2024b</a>) 是专为<strong>代码智能</strong>设计的系列模型。</p><p><strong>核心特点:</strong></p><ul><li><strong>版本</strong>: 包括初版 DeepSeek-Coder 和升级版 DeepSeek-Coder-V2。</li><li><strong>参数规模 (Coder-V2)</strong>: 提供 <strong>16B (激活 2.4B)</strong> 和 <strong>236B (激活 21B)</strong> 两种 MoE 规模。</li><li><strong>训练数据 (Coder-V2)</strong>: 基于 DeepSeek-V2 checkpoint，额外使用 <strong>6T tokens</strong> 的代码和数学相关语料进行继续预训练。</li><li><strong>核心技术</strong>:<ol><li><strong>仓库级预训练</strong>: 增强跨文件理解能力。</li><li><strong>填充中间 (FIM)</strong>: 提升代码补全能力。</li><li><strong>多语言支持 (Coder-V2)</strong>: 支持多达 <strong>338 种</strong>编程语言。</li></ol></li><li><strong>上下文长度 (Coder-V2)</strong>: 继承 V2 的 <strong>128K tokens</strong>。</li><li><strong>性能 (Coder-V2)</strong>: 在代码生成、补全、数学推理等方面表现 <strong>SOTA</strong>，超越 GPT-4 Turbo 等闭源模型。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>数据构成</strong>: 大量高质量项目级代码，覆盖广泛的编程语言。</li><li><strong>训练目标</strong>: Next Token Prediction + FIM。</li><li><strong>架构 (Coder-V2)</strong>: 基于 DeepSeek-V2 (MoE + MLA)。</li></ul><p><strong>意义：</strong> DeepSeek-Coder 系列，特别是 V2，填补了开源社区在顶级代码生成模型上的空白，为开发者提供了强大的、可本地部署的 AI 编程助手。</p><h3 id=deepseek-r1>DeepSeek-R1<a hidden class=anchor aria-hidden=true href=#deepseek-r1>#</a></h3><p><strong>DeepSeek-R1</strong> (<a href=https://arxiv.org/abs/2501.12948>DeepSeek-AI, 2025</a>) 是利用<strong>强化学习 (RL)</strong> 显著增强 LLM <strong>推理能力</strong>的第一代模型。</p><p><strong>核心特点:</strong></p><ul><li><strong>核心方法</strong>: 广泛使用 <strong>RL</strong>（特别是 GRPO 算法）直接培养模型的复杂推理能力，减少对 SFT 数据的依赖。</li><li><strong>关键模型</strong>:<ul><li><strong>DeepSeek-R1-Zero</strong>: 基于 DeepSeek-V3，通过纯 RL 训练涌现出复杂推理行为（如长链思考、自我反思）。</li><li><strong>DeepSeek-R1</strong>: 在 R1-Zero 基础上，结合冷启动策略、面向推理的 RL 和拒绝采样/SFT 进行多阶段训练，提升实用性。</li></ul></li><li><strong>性能</strong>: 在 <strong>AIME、MATH、Codeforces</strong> 等高难度推理基准上取得 <strong>SOTA</strong> 性能，可媲美 OpenAI 的 o1 系列推理模型。</li><li><strong>能力蒸馏</strong>: 成功将 R1 的强大推理能力蒸馏到一系列更小的密集模型（基于 Qwen 和 Llama 架构，1.5B 到 70B）。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>RL 激励</strong>: 主要依赖基于规则的奖励系统或模型自我评估。</li><li><strong>训练流程</strong>: 精心设计的多阶段流程结合 SFT 和 RL。</li><li><strong>涌现能力</strong>: RL 驱动模型发展出复杂推理行为。</li></ul><p><strong>意义：</strong> DeepSeek-R1 探索了通过强化学习提升 LLM 深度推理能力的新范式，证明了模型可以通过自我探索变得更“聪明”，并为社区提供了强大的开源推理模型（及其蒸馏版本）。</p><h3 id=deepseek-系列模型特性对比>DeepSeek 系列模型特性对比<a hidden class=anchor aria-hidden=true href=#deepseek-系列模型特性对比>#</a></h3><table><thead><tr><th>特性</th><th>DeepSeek LLM (2023/11)</th><th>DeepSeek-V2 (2024/05)</th><th>DeepSeek-V3 (2024/12)</th><th>DeepSeek-Coder-V2 (2024/11)</th><th>DeepSeek-R1 (2025/01)</th></tr></thead><tbody><tr><td><strong>基础模型</strong></td><td>-</td><td>-</td><td>-</td><td>DeepSeek-V2</td><td>DeepSeek-V3</td></tr><tr><td><strong>模型最大规模</strong></td><td>67B (Dense)</td><td>236B (MoE)</td><td><strong>671B (MoE)</strong></td><td>236B (MoE)</td><td>671B (MoE, Base for RL)</td></tr><tr><td><strong>激活参数量</strong></td><td>67B</td><td>21B</td><td>37B</td><td>2.4B / 21B</td><td>37B</td></tr><tr><td><strong>训练数据量</strong></td><td>2T tokens</td><td>8.1T tokens</td><td><strong>14.8T tokens</strong></td><td>V2 + 6T Code/Math tokens</td><td>V3 + RL Data</td></tr><tr><td><strong>上下文长度</strong></td><td>4K / 32K (扩展)</td><td><strong>128K tokens</strong></td><td><strong>128K tokens</strong></td><td><strong>128K tokens</strong></td><td><strong>128K tokens</strong></td></tr><tr><td><strong>Tokenizer</strong></td><td>Custom BPE</td><td>Custom BPE (Optimized)</td><td>Custom BPE (Expanded)</td><td>Inherited from V2</td><td>Inherited from V3</td></tr><tr><td><strong>位置编码</strong></td><td>RoPE</td><td>RoPE</td><td>RoPE</td><td>RoPE</td><td>RoPE</td></tr><tr><td><strong>注意力/推理优化</strong></td><td>GQA + KV Cache</td><td><strong>MoE + MLA</strong> + KV Cache</td><td><strong>MoE + MLA</strong> + KV Cache</td><td><strong>MoE + MLA</strong> + KV Cache</td><td><strong>MoE + MLA</strong> + KV Cache</td></tr><tr><td><strong>归一化</strong></td><td>RMSNorm</td><td>RMSNorm</td><td>RMSNorm</td><td>RMSNorm</td><td>RMSNorm</td></tr><tr><td><strong>激活函数</strong></td><td>SwiGLU</td><td>SwiGLU</td><td>SwiGLU</td><td>SwiGLU</td><td>SwiGLU</td></tr><tr><td><strong>关键创新</strong></td><td>-</td><td>MLA, DeepSeekMoE</td><td>MTP, FP8 Training</td><td>Code/Math Specialization</td><td>Pure RL Reasoning</td></tr><tr><td><strong>模型类别</strong></td><td>基座/对话模型</td><td>基座/对话模型 (MoE)</td><td>基座/对话模型 (MoE)</td><td>代码/数学模型 (MoE)</td><td>推理增强模型 (MoE)</td></tr></tbody></table><h2 id=qwen-系列模型>Qwen 系列模型<a hidden class=anchor aria-hidden=true href=#qwen-系列模型>#</a></h2><p>阿里巴巴达摩院的通义千问（<strong>Qwen</strong>）系列模型是大型语言模型（LLM）及多模态模型领域的重要贡献者。自 2023 年发布以来，Qwen 系列不断迭代，涵盖从小型到超大规模的基础模型、对话模型（Qwen-Chat），以及针对代码（Qwen-Coder）、数学（Qwen-Math）、视觉（Qwen-VL）、音频（Qwen-Audio）乃至全模态（Qwen-Omni）的专用模型。Qwen 系列以其强大的性能、广泛的多语言支持和持续的开源贡献，在社区中产生了广泛影响。下面我们按照发布时间顺序，依次介绍 Qwen 系列的主要模型。</p><h3 id=qwen-qwen-10>Qwen (Qwen 1.0)<a hidden class=anchor aria-hidden=true href=#qwen-qwen-10>#</a></h3><p><strong>发布时间：</strong> 约 2023 年 8-9 月</p><p><strong>关键特性:</strong> 作为 Qwen 系列的开山之作，Qwen 旨在提供强大的中英双语基础模型 (<a href=https://arxiv.org/abs/2309.16609>Bai et al., 2023</a>).</p><ul><li><strong>模型规模:</strong> 发布了 7B 和 14B 两个尺寸。</li><li><strong>训练数据:</strong> 在约 <strong>2.2 万亿 tokens</strong> 的高质量数据上训练，以中英文为主，包含代码数据。</li><li><strong>上下文长度:</strong> 基础模型支持 2048 tokens，通过增量训练使 7B 支持 32K，14B 支持 8K。</li><li><strong>架构:</strong> 基于标准 Transformer 解码器，采用 RoPE 位置编码、RMSNorm 和 SwiGLU 激活函数。</li><li><strong>对齐:</strong> 发布了 Qwen-Chat 模型，通过 SFT 和 RLHF 进行对齐。</li></ul><p><strong>性能:</strong> Qwen-14B 在 MMLU 上得分约 70，在当时开源模型中表现突出，尤其是在中文任务上 (<a href=https://arxiv.org/abs/2309.16609>Bai et al., 2023</a>).</p><p><strong>总结:</strong> Qwen 1.0 以其在中英双语上的优异表现和扎实的架构设计，成功打响了 Qwen 系列的名号，为后续快速迭代奠定了基础。</p><h3 id=qwen15>Qwen1.5<a hidden class=anchor aria-hidden=true href=#qwen15>#</a></h3><p><strong>发布时间：</strong> 约 2024 年 2 月</p><p><strong>关键特性：</strong> Qwen1.5 是对 Qwen1.0 的改进版本，重点提升了多语言能力和易用性 (<a href=https://qwenlm.github.io/blog/qwen1.5/>Qwen Team, 2024g</a>).</p><ul><li><strong>模型规模:</strong> 开源了 0.5B, 1.5B, 7B, 14B, 32B, 70B (后改为 72B?) 以及一个 110B MoE 模型。</li><li><strong>多语言:</strong> 显著增强了非英语语言的处理能力。</li><li><strong>上下文长度:</strong> 全系支持 <strong>32K tokens</strong> 上下文。</li><li><strong>架构:</strong> 引入 untied embeddings 和去 Bias 化等微调。与 HuggingFace Transformers 深度集成，使用更方便。</li><li><strong>对齐:</strong> 改进了 Chat 模型的对话能力和指令跟随。</li></ul><p><strong>性能:</strong> Qwen1.5-72B 在 MMLU 上得分约 77，相比 Qwen1.0 有显著提升。Chat 模型在 MT-Bench 上表现良好 (<a href=https://qwenlm.github.io/blog/qwen1.5/>Qwen Team, 2024g</a>).</p><p><strong>总结:</strong> Qwen1.5 承接 Qwen1.0，在多语言、长上下文和易用性上做了关键改进，并扩展了模型规模选项，为 Qwen2 的大规模升级铺平了道路。</p><h3 id=qwen2>Qwen2<a hidden class=anchor aria-hidden=true href=#qwen2>#</a></h3><p><strong>发布时间：</strong> 约 2024 年 6 月</p><p><strong>关键特性：</strong> Qwen2 是 Qwen1.5 的重要迭代，显著提升了多语言能力、上下文长度和模型规模 (<a href=https://arxiv.org/abs/2406.17849>Bai et al., 2024b</a>).</p><ul><li><strong>模型规模:</strong> 开源了 0.5B, 1.5B, 7B, 14B, 72B 以及一个 57B-A14B 的 MoE 变体。</li><li><strong>训练数据:</strong> 扩展到 <strong>7 万亿 tokens</strong>，覆盖 <strong>27 种语言</strong>，并增加了代码和数学数据比例。</li><li><strong>上下文长度:</strong> 全系原生支持 <strong>128K tokens</strong> 的长上下文。</li><li><strong>架构:</strong> 延续 Transformer，使用 GQA（Grouped Query Attention）优化大模型推理效率。</li><li><strong>对齐:</strong> 使用数十万指令数据进行 SFT 和 RLHF，引入了工具使用能力。</li><li><strong>多模态扩展:</strong> 同期发布了 Qwen2-VL 和 Qwen2-Audio 模型。</li></ul><p><strong>性能:</strong> Qwen2-72B 在 MMLU 上达到 84.2，HumanEval 64.6，GSM8K 88.7 (<a href=https://arxiv.org/abs/2406.17849>Bai et al., 2024b</a>). 在多语言基准（如 Flores）上表现优异。</p><p><strong>总结:</strong> Qwen2 通过扩大数据、增长上下文、优化架构（GQA）和引入工具使用，实现了性能的全面飞跃，并为后续 Qwen2.5 奠定了基础。</p><h3 id=qwen25-base-models>Qwen2.5 (Base Models)<a hidden class=anchor aria-hidden=true href=#qwen25-base-models>#</a></h3><p><strong>发布时间：</strong> 约 2024 年 9 月</p><p><strong>设计动机:</strong> 作为 Qwen2.0 的重要升级，Qwen2.5 旨在全面提升模型的通用能力（知识、推理、语言理解）、指令跟随效果和易用性 (<a href=https://qwenlm.github.io/blog/qwen2.5/>Qwen Team, 2024f</a>). 主要通过<strong>大幅增加预训练数据</strong>（从 7T 到 18T tokens）、<strong>改进对齐策略</strong>（百万级指令数据、多阶段 RLHF）和<strong>提供更丰富的模型规模</strong>（0.5B 到 72B）来实现。</p><p><strong>模型架构:</strong> 沿用 Transformer 解码器架构，提供 0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B 密集模型。内部训练了 MoE 变体（Turbo/Plus）用于云服务。继续使用 RoPE、RMSNorm、SwiGLU 等优化组件。支持 128K 上下文，并可能利用位置插值技术增强超长上下文外推能力。</p><p><strong>预训练与对齐策略:</strong> 预训练数据量达 <strong>18 万亿 tokens</strong>，包含更多多语种文本、更新的知识、专业领域数据（法律、医疗等）以及代码和数学数据 (<a href=https://qwenlm.github.io/blog/qwen2.5/>Qwen Team, 2024f</a>). 指令微调使用超过 <strong>100 万条</strong>高质量指令数据，并采用<strong>多阶段 RLHF</strong> 进行精细对齐，提升人类偏好评分和安全性。</p><p><strong>性能评估:</strong> Qwen2.5 系列在各项基准上表现优异。</p><ul><li><strong>MMLU:</strong> 72B 模型达到 85+，接近 Llama3-405B (<a href=https://qwenlm.github.io/blog/qwen2.5/>Qwen Team, 2024f</a>).</li><li><strong>HumanEval:</strong> 72B 模型约 65 分 (<a href=https://arxiv.org/abs/2406.17849>Bai et al., 2024b</a>).</li><li><strong>GSM8K:</strong> 72B 模型达 89.5 分 (<a href=https://arxiv.org/abs/2406.17849>Bai et al., 2024b</a>).</li><li><strong>MT-Bench / Arena Hard:</strong> 72B Chat 模型评分 9.1 / 48.1，居开源模型前列 (<a href=https://arxiv.org/abs/2406.17849>Bai et al., 2024b</a>).
Qwen2.5-72B 已成为最强的开源通用模型之一。</li></ul><p><strong>开源情况:</strong> 开源了 <strong>0.5B 到 72B</strong> 共 7 个尺寸的基础和指令模型（Apache 2.0），可在 Hugging Face 获取 (<a href=https://huggingface.co/collections/Qwen/qwen25-670f0f0ce9bffc6a01e4bebf>Qwen Team, 2024f</a>). 提供量化模型、微调脚本和示例。<strong>Qwen2.5-Turbo/Plus</strong> (MoE) 通过 API 提供。</p><p><strong>特色能力:</strong></p><ul><li>广博的知识储备和多语言能力（支持 30+ 种语言）。</li><li>生成结构良好的超长内容。</li><li>支持 JSON、表格等结构化输出。</li><li>通过系统提示进行角色扮演和语气控制。</li><li>强大的工具使用（函数调用）能力。</li><li>长对话一致性和较低的幻觉率。</li></ul><p><strong>应用场景:</strong> 智能客服、内容创作、教育辅导、科研助手、数据分析（结合工具）、医疗与法律辅助等通用 NLP 任务。</p><p><strong>未来展望:</strong> 更大规模模型（Qwen 3.0）、更深度多模态融合、自动化持续学习、模型压缩与终端部署、AI 安全与伦理强化。</p><h3 id=qwen25-coder>Qwen2.5-Coder<a hidden class=anchor aria-hidden=true href=#qwen25-coder>#</a></h3><p><strong>发布时间：</strong> 约 2024 年 9 月</p><p><strong>设计动机:</strong> Qwen2.5-Coder 是 Qwen 系列新一代代码专用模型，旨在打造开源顶尖代码模型，缩小与最强闭源代码模型（如 GPT-4 Codex 能力）的差距 (<a href=https://qwenlm.github.io/blog/qwen-coder/>Qwen Team, 2024e</a>). 目标是提升多语言代码生成质量、增强复杂编程任务理解，并结合 Qwen2.5 的通用知识进行代码解释和调试。更名 Coder 也寓意其作为更智能、人性化的“编码助手”。</p><p><strong>模型架构:</strong> 基于 Qwen2.5 LLM 主干，针对代码任务进行调整：</p><ul><li><strong>优化的 Tokenizer:</strong> 更好地处理代码缩进、符号和关键字。</li><li><strong>插入位置支持:</strong> 使用特殊提示或标记支持在代码中间插入内容（代码补全）。</li><li><strong>长上下文:</strong> 支持至少 8K-32K 上下文，便于处理长文件和多文件项目。</li><li>依赖 Transformer 自身能力，未引入 AST 解析器等外部模块。</li></ul><p><strong>预训练与对齐策略:</strong> 在 Qwen2.5 基础上，使用海量高质量、经清洗的多语言代码语料（GitHub、竞赛题解等）进行继续预训练。指令微调阶段使用大规模指令数据（人工编写、用户反馈、自我改进数据），训练模型遵循编程任务描述、生成高质量代码、注释和文档。可能使用 RLHF 优化代码风格和优雅性。</p><p><strong>性能评估:</strong> Qwen2.5-Coder 在多项代码基准上取得开源 SOTA 成绩 (<a href=https://qwenlm.github.io/blog/qwen-coder/>Qwen Team, 2024e</a>).</p><ul><li><strong>HumanEval (Python):</strong> 32B 模型通过率接近 65%，与 GPT-4o 相当或更高 (<a href=https://arxiv.org/abs/2406.17849>Bai et al., 2024b</a>).</li><li><strong>LeetCode:</strong> 在中等难度问题上通过率远超 Code Llama 等。</li><li><strong>MultiPL-E:</strong> 在 C++, Java 等多语言上表现均衡且领先。</li><li>在代码审查、重构和调试任务上表现优于同类开源模型。</li></ul><p><strong>开源情况:</strong> 全面开源 <strong>1.5B, 7B, 32B</strong> 三个尺寸的模型权重（Apache 2.0），可在 Hugging Face 和 ModelScope 获取 (<a href=https://huggingface.co/collections/Qwen/qwen25-coder-66e932484f95e00f01164a9f>Qwen Team, 2024e</a>). 提供 Transformers 支持、量化模型和微调示例。</p><p><strong>特色能力:</strong></p><ul><li>精通多种编程语言。</li><li>代码解释、文档生成和错误调试。</li><li>智能代码补全和重构。</li><li>结合自然语言生成技术文档。</li><li>理解常用框架和工具上下文。</li></ul><p><strong>应用场景:</strong> IDE 插件（补全、调试）、Code Review 自动化、技术支持问答、编程学习与培训、企业内部代码生成服务。</p><p><strong>未来展望:</strong> 支持更多语言和领域、与 IDE 环境深度结合、持续学习新框架、更精细的多目标对齐、结合语义搜索处理超大代码库。</p><h3 id=qwq-qwen-with-questions>QwQ (Qwen with Questions)<a hidden class=anchor aria-hidden=true href=#qwq-qwen-with-questions>#</a></h3><p><strong>发布时间：</strong> 约 2024 年 11 月 (预览版)</p><p><strong>设计动机:</strong> QwQ-32B 是一款<strong>实验性预览模型</strong>，旨在探索提升大模型<strong>推理和自我反思</strong>能力 (<a href=https://qwenlm.github.io/blog/qwen-qxq/>Qwen Team, 2024d</a>). 其名称寓意“提问”和“深思”，希望模型能像哲学家一样“知其无知”，通过主动质疑和反复校正来提高复杂推理（数学、逻辑、常识）的可靠性。</p><p><strong>模型架构:</strong> 基于 Qwen2.5-32B-Instruct，核心仍是 Transformer 解码器。通过 Prompt 设计和微调实现增强推理流程：</p><ul><li><strong>显式思考链:</strong> 输出答案前会以特殊格式（如“思考：”）输出中间推理步骤和子问题。</li><li><strong>不确定性标记:</strong> 可能引入机制让模型在不确定时标记或主动寻求澄清。</li><li><strong>长上下文增强:</strong> 支持 32K 上下文以容纳推理链。</li><li>沿用 RoPE、SwiGLU、RMSNorm 等优化组件 (<a href=https://arxiv.org/abs/2309.16609>Bai et al., 2023</a>).</li></ul><p><strong>预训练与对齐策略:</strong> 基于 Qwen2.5-Chat 进行微调。微调数据包含大量需要多步推理的问题及其详细解析过程（类似 CoT 数据）。特别训练模型在回答前先提出澄清性子问题。可能使用 RLHF 奖励详尽、逻辑自洽且正确的推理链。</p><p><strong>性能评估:</strong> QwQ-32B 在 <strong>GPQA</strong>（通用解难问答）基准上得分超过 GPT-4o 和 Claude 3.5 Sonnet (<a href=https://huggingface.co/Qwen/QwQ-32B-Chat>Qwen Team, 2024d</a>). 在 <strong>MATH</strong> 数据集和 <strong>AQUA-RAT</strong>（代数问答）上表现优异，推理步骤更完整合理。在代码推理（如 APE）上也取得领先。社区测试表明其在抗幻觉和自我纠错方面有优势。</p><p><strong>开源情况:</strong> <strong>QwQ-32B-Preview</strong> 模型权重已开源（Apache 2.0），可在 Hugging Face 获取 (<a href=https://huggingface.co/Qwen/QwQ-32B-Chat>Qwen Team, 2024d</a>).</p><p><strong>特色能力:</strong></p><ul><li>强烈的质疑意识和审慎态度。</li><li>自我检查和纠错能力。</li><li>条理清晰的论证过程。</li><li>减少幻觉产生。</li><li>潜在的教学能力（通过展示解题思路）。</li></ul><p><strong>应用场景:</strong> 科研辅助、教育答疑、代码审查调试、法律分析、高可靠场景 AI 助手。</p><p><strong>未来展望:</strong> 将 QwQ 技术融入更大模型、更精细的反思机制、自动化思维链数据生成、用户可控的思维输出、与外部工具结合、改善效率。</p><h3 id=qwen25-1m>Qwen2.5-1M<a hidden class=anchor aria-hidden=true href=#qwen25-1m>#</a></h3><p><strong>发布时间：</strong> 约 2024 年 11-12 月</p><p><strong>设计动机：</strong> 为了满足处理超长文本（如书籍、大型代码库、长篇报告）的需求，Qwen2.5-1M 将模型上下文长度扩展到 <strong>1,000,000 tokens</strong> (<a href=https://qwenlm.github.io/blog/qwen2.5-1m/>Qwen Team, 2024c</a>). 主要动机是突破 Transformer 的长度瓶颈，在百万级上下文下实现高效且性能不衰减的推理，同时降低长上下文训练和推理的成本。</p><p><strong>模型架构：</strong> 基于 Qwen2.5-7B/14B 模型，针对长序列进行了架构和推理优化：</p><ul><li><strong>稀疏局部注意力:</strong> 采用滑动窗口或分层注意力等稀疏模式，降低 $O(n^2)$ 复杂度，实现 3-7 倍推理加速 (<a href=https://qwenlm.github.io/blog/qwen2.5-1m/>Qwen Team, 2024c</a>).</li><li><strong>分块预填充 (Chunked Prefill):</strong> 将长序列分块输入，利用并行和流水线优化显存和计算峰值 (<a href=https://qwenlm.github.io/blog/qwen2.5-1m/>Qwen Team, 2024c</a>).</li><li><strong>长度外推技术:</strong> 利用 RoPE 的可外推性或位置插值等方法，无需完全重训即可将 128K 模型扩展到 1M 上下文 (<a href=https://qwenlm.github.io/blog/qwen2.5-1m/>Qwen Team, 2024c</a>).</li><li><strong>优化的推理引擎:</strong> 定制 CUDA 核、流水线并行、调度优化等工程手段保障端到端速度。
模型采用渐进式训练，逐步增加上下文长度。</li></ul><p><strong>预训练与对齐策略：</strong> 预训练数据强调长文本，可能使用拼接文档构造超长样本。加入特殊任务（如长文中间摘要、远距离问答）训练长程依赖。指令微调使用百万级长上下文指令数据，训练模型在长文档中搜索、关联信息和遵循指令。RLHF 用于优化长文本生成的稳定性和质量。</p><p><strong>性能评估：</strong> Qwen2.5-1M 在长文档问答任务上显著优于同类模型（如 GPT-4o-mini），支持的上下文长度是其 8 倍 (<a href=https://qwenlm.github.io/blog/qwen2.5-1m/>Qwen Team, 2024c</a>). 在长文本摘要、代码跨文件理解、多轮长程对话中表现出色，展现了惊人的记忆力和全局一致性。推理效率通过优化大幅提升，处理 1M token 的速度接近处理 128K 的量级。</p><p><strong>开源情况：</strong> 开源了 <strong>Qwen2.5-7B-Instruct-1M</strong> 和 <strong>Qwen2.5-14B-Instruct-1M</strong> 模型（Apache 2.0），以及配套的长文本推理框架 (<a href=https://huggingface.co/collections/Qwen/qwen25-1m-670f1127384613c1a138a431>Qwen Team, 2024c</a>). <strong>Qwen2.5-Turbo-1M</strong> (MoE) 通过 API 提供。</p><p><strong>特色能力：</strong></p><ul><li>超长上下文记忆（百万 token 级别）。</li><li>生成全局一致的超长文稿。</li><li>即时跨文档查询与精确引用。</li><li>耐久对话与持续学习能力。</li><li>复杂的多文档、跨篇章推理。</li></ul><p><strong>应用场景：</strong> 企业知识库问答、法律分析、学术研究、金融分析、大型代码库理解、超长文学创作、长对话陪伴。</p><p><strong>未来展望：</strong> 更长甚至无限上下文、长程交互算法改进、片段版权与来源追踪、稀疏 MoE 结合、长上下文评测标准、训练成本进一步降低。</p><h3 id=qwen25-vl>Qwen2.5-VL<a hidden class=anchor aria-hidden=true href=#qwen25-vl>#</a></h3><p><strong>发布时间：</strong> 约 2024 年 11-12 月</p><p><strong>设计动机：</strong> 作为 Qwen 系列新一代旗舰视觉语言模型，Qwen2.5-VL 旨在提升对<strong>任意分辨率图像</strong>的适应性、增强<strong>长视频理解</strong>能力，并在多模态推理和多语言图像理解上达到业界领先水平 (<a href=https://qwenlm.github.io/blog/qwen2.5-vl/>Qwen Team, 2024b</a>). 它希望解决传统视觉模型缩放图像导致细节丢失的问题，并满足对长时序视频分析的需求。</p><p><strong>模型架构：</strong> Qwen2.5-VL 采用 <strong>ViT 视觉编码器</strong>与 <strong>Qwen2.5 语言模型</strong>结合的架构。关键改进包括：</p><ul><li><strong>原生动态分辨率 (Naive Dynamic Resolution):</strong> 允许输入任意尺寸图像，ViT 在训练时处理不同数量的 patch，保留图像原始细节和比例 (<a href=https://arxiv.org/abs/2408.11701>Bai et al., 2024</a>).</li><li><strong>增强的多模态 RoPE (M-ROPE):</strong> 优化了对 2D 图像和 3D 时空（视频）位置的编码，通过将位置拆分为时间、高度、宽度维度并应用旋转变换，显式建模视觉信号的空间/时间对应关系 (<a href=https://arxiv.org/abs/2408.11701>Bai et al., 2024</a>).
视觉编码器本身参数量较大（约 6 亿），采用先预训练 ViT 再与 LLM 联合微调的策略。</li></ul><p><strong>预训练与对齐策略：</strong> 使用了包含亿级图文对（不同分辨率）、百万级视频文本对、多语言 OCR 数据在内的超大规模多模态数据集 (<a href=https://qwenlm.github.io/blog/qwen2.5-vl/>Qwen Team, 2024b</a>). 预训练目标是根据视觉输入生成文本。训练中随机改变图像尺寸和视频长度以适应动态分辨率和 M-ROPE。对齐阶段进行指令微调和基于人类反馈的 RLHF，优化答案质量、格式遵循和视觉相关的安全性。</p><p><strong>性能评估：</strong> Qwen2.5-VL 在 DocVQA、RealWorldQA、MathVista 等图像理解基准上取得 SOTA 或接近 SOTA 的成绩，超越了许多开源和闭源模型 (<a href=https://qwenlm.github.io/blog/qwen2.5-vl/>Qwen Team, 2024b</a>). 能够处理长达 20 分钟的视频问答，并在多语言 OCR 和图像文本理解上表现突出。Qwen2.5-VL-72B 在 OpenCompass 等综合评测中位居前列 (<a href=https://huggingface.co/collections/Qwen/qwen25-vl-67394505f55508a7797a021a>Qwen Team, 2024b</a>).</p><p><strong>开源情况：</strong> 开源了 <strong>Qwen2.5-VL-7B</strong> 和 <strong>Qwen2.5-VL-2B</strong> 模型权重（Apache 2.0），可在 Hugging Face 获取 (<a href=https://huggingface.co/collections/Qwen/qwen25-vl-67394505f55508a7797a021a>Qwen Team, 2024b</a>). <strong>Qwen2.5-VL-72B</strong> 通过阿里云 API 提供服务。官方提供了推理代码、优化工具（vLLM 适配）和评测数据集示例。</p><p><strong>特色能力：</strong></p><ul><li>处理任意分辨率和长图文档（如小票、扫描件）。</li><li>复杂场景的多模态推理。</li><li>同时处理多张图像或图像+视频。</li><li>具备一定的视觉代理（UI 操作规划）能力。</li><li>结合 Qwen2.5 的知识库进行视觉问答。</li></ul><p><strong>应用场景：</strong> 文档信息抽取、安防监控、媒体内容审核、多媒体搜索推荐、辅助创作、机器人视觉。</p><p><strong>未来展望：</strong> 融合更丰富模态（3D）、引入推理链、结合知识库、模型压缩部署、更复杂的视觉指令交互、多模态安全提升。</p><h3 id=qvq-qwen-with-vision--questions>QVQ (Qwen with Vision & Questions)<a hidden class=anchor aria-hidden=true href=#qvq-qwen-with-vision--questions>#</a></h3><p><strong>发布时间：</strong> 约 2024 年 12 月 (预览版)</p><p><strong>设计动机：</strong> QVQ-72B 是 Qwen 团队推出的<strong>实验性多模态推理模型</strong>，旨在提升模型在复杂视觉推理任务上的能力，使其不仅能描述图像，还能进行涉及常识和多步推理的深度思考 (<a href=https://qwenlm.github.io/blog/qwen-qxq/>Qwen Team, 2024d</a>). 其名称寓意“质疑”，希望模型能像哲学家一样“看”世界并提出问题，探索模型的“自我反思”能力。</p><p><strong>模型架构：</strong> 基于 Qwen2.5-VL-72B，保留了 ViT 编码器、M-ROPE 和 72B 解码器。针对推理进行了优化：</p><ul><li><strong>插入式思维链 (Chain-of-Thought):</strong> 模型在解码时可输出隐藏的推理步骤，再给出最终答案，训练时可能使用了 CoT 格式的数据。</li><li><strong>更大的上下文窗口:</strong> 可能支持 128K 或更长上下文以容纳复杂推理链。</li><li><strong>多模态融合:</strong> 深度融合视觉感知与语言推理能力。</li></ul><p><strong>预训练与对齐策略:</strong> 基于 Qwen2.5-VL 进行微调，加入大量需要多步视觉推理的指令数据（如图片逻辑题、看图推断社会现象等）。可能引入了让模型在不确定时主动提问或标记不确定性的训练策略。</p><p><strong>性能评估:</strong> QVQ-72B 在高难度的多模态基准 <strong>MMMU</strong> 上取得了 <strong>70.3%</strong> 的准确率，达到 SOTA 水平，超越了 GPT-4o 等模型 (<a href=https://qwenlm.github.io/blog/qwen-qxq/>Qwen Team, 2024d</a>). 这证明了其在复杂视觉推理上的强大实力。</p><p><strong>开源情况:</strong> 作为研究预览版，<strong>QVQ-72B-Chat</strong> 模型权重已开源 (<a href=https://huggingface.co/Qwen/QVQ-72B-Chat>Qwen Team, 2024d</a>).</p><p><strong>特色能力:</strong></p><ul><li>深度视觉推理（超越表面描述）。</li><li>潜在的自我反思和提问能力。</li><li>结合视觉信息进行多步逻辑推断。</li></ul><p><strong>应用场景:</strong> 复杂图像分析、视觉常识问答、需要深度理解的图像场景（如医学影像初步分析、科学图表解读）。</p><p><strong>未来展望:</strong> 将 QVQ 的推理能力融入主流多模态模型，提升可靠性和可解释性。</p><h3 id=qwen25-omni>Qwen2.5-Omni<a hidden class=anchor aria-hidden=true href=#qwen25-omni>#</a></h3><p><strong>发布时间：</strong> 约 2024 年 12 月 / 2025 年初</p><p><strong>设计动机：</strong> Qwen2.5-Omni 旨在打造一个能够处理文本、图像、音频、视频多种模态输入的“全能”（omni-modal）AI 助手，实现端到端的感知和实时交互 (<a href=https://qwenlm.github.io/blog/qwen2.5/>Qwen Team, 2024a</a>). 其目标是模拟人类多感官交互，让 AI 能够同时“看”、“听”、“说”，提供自然流畅的人机对话体验。</p><p><strong>模型架构：</strong> Qwen2.5-Omni 采用了创新的 <strong>“思考者-说话人”（Thinker-Talker）架构</strong> (<a href=https://qwenlm.github.io/blog/qwen2.5/>Qwen Team, 2024a</a>).</p><ul><li><strong>Thinker:</strong> 基于 Transformer 解码器，整合了视觉和音频编码器，负责理解多模态输入并生成高层语义表示和文本输出。引入了<strong>时间对齐多模态 RoPE (TMRoPE)</strong> 来同步视频帧和音频序列的时间轴 (<a href=https://qwenlm.github.io/blog/qwen2.5/>Qwen Team, 2024a</a>).</li><li><strong>Talker:</strong> 采用<strong>双轨自回归 Transformer 解码器</strong>，接收 Thinker 的表示，以流式方式合成语音输出。使用了<strong>滑动窗口离散表示 Transformer (sliding-window DiT)</strong> 来实现低延迟的实时语音合成 (<a href=https://qwenlm.github.io/blog/qwen2.5/>Qwen Team, 2024a</a>).
该架构实现了端到端训练，无需独立的 TTS 模块。</li></ul><p><strong>预训练与对齐策略：</strong> 模型在包含文本、图文对、音文对、视频文本对的大规模多模态语料上进行预训练。损失函数结合了文本生成的交叉熵损失和语音 codec token 生成的损失。对齐阶段采用<strong>监督微调 (SFT)</strong> 和<strong>人类反馈对齐</strong>（包括文本 RLHF 和语音质量优化），确保模型遵循指令并生成高质量、安全的多模态响应 (<a href=https://qwenlm.github.io/blog/qwen2.5/>Qwen Team, 2024a</a>).</p><p><strong>性能评估：</strong> Qwen2.5-Omni 在多模态综合基准 Omni-Bench 上达到 SOTA 水平。其单项能力（语音理解、视觉问答）可媲美同规模的专用模型（如 Qwen2-Audio, Qwen2.5-VL）。在端到端语音指令跟随任务上表现出色，语音合成质量也超过了许多实时 TTS 系统 (<a href=https://qwenlm.github.io/blog/qwen2.5/>Qwen Team, 2024a</a>).</p><p><strong>开源情况：</strong> 已开源 <strong>Qwen2.5-Omni-7B</strong> 模型权重（基础版和聊天版），采用 Apache 2.0 许可证 (<a href=https://huggingface.co/Qwen/Qwen2.5-Omni-7B>Qwen Team, 2024a</a>). 更大模型通过阿里云 API 提供服务。相关的推理代码和语音解码库也在 GitHub 提供。</p><p><strong>特色能力：</strong></p><ul><li>全模态输入输出（文本、图像、音频、视频输入；文本+语音输出）。</li><li>实时对话与连续理解（流式处理）。</li><li>复杂的跨模态推理。</li><li>支持情感和多语种的语音合成。</li><li>继承 Qwen 系列的工具使用和行动规划能力。</li></ul><p><strong>应用场景：</strong> 智能客服、教育科普、无障碍辅助、内容创作与审核、机器人控制等。</p><p><strong>未来展望：</strong> 更大规模模型、更长时空理解、更自然的语音合成、多模态知识推理、与外部工具交互、推理性能优化。</p><hr><h2 id=qwen-系列模型特性对比>Qwen 系列模型特性对比<a hidden class=anchor aria-hidden=true href=#qwen-系列模型特性对比>#</a></h2><table><thead><tr><th>特性</th><th>Qwen (1.0) (2023/09)</th><th>Qwen1.5 (2024/02)</th><th>Qwen2 (2024/06)</th><th>Qwen2.5 (2024/09+)</th><th>Qwen2.5-Specialized (Coder/VL/Omni/1M/QwQ/QVQ)</th></tr></thead><tbody><tr><td><strong>模型规模 (开源)</strong></td><td>7B, 14B</td><td>0.5B-14B, 32B, 70B?</td><td>0.5B-14B, 72B (+MoE)</td><td><strong>0.5B-14B, 32B, 72B</strong></td><td>1.5B-72B (Depends on model)</td></tr><tr><td><strong>训练数据量</strong></td><td>~2.2T tokens</td><td>> 2.2T (多语言增强)</td><td><strong>7T+ tokens</strong></td><td><strong>18T+ tokens</strong></td><td>Base + Domain Specific Data</td></tr><tr><td><strong>语言覆盖</strong></td><td>中英为主</td><td>多语言增强</td><td><strong>27+ 种语言</strong></td><td><strong>30+ 种语言</strong></td><td>Base + Domain Specific (e.g., Code languages)</td></tr><tr><td><strong>上下文长度</strong></td><td>2K (可扩展 8K/32K)</td><td><strong>32K</strong></td><td><strong>128K</strong></td><td><strong>128K</strong> (可扩展 <strong>1M</strong>)</td><td>32K - <strong>1M</strong> (Depends on model)</td></tr><tr><td><strong>Tokenizer</strong></td><td>BPE</td><td>BPE</td><td>BPE</td><td>BPE</td><td>BPE (可能针对代码等优化)</td></tr><tr><td><strong>位置编码</strong></td><td>RoPE</td><td>RoPE</td><td>RoPE</td><td>RoPE (M-ROPE for VL/Omni)</td><td>RoPE / M-ROPE</td></tr><tr><td><strong>注意力/推理优化</strong></td><td>MHA</td><td>MHA (+Untied Emb)</td><td>MHA / <strong>GQA</strong></td><td>MHA / <strong>GQA</strong></td><td>GQA / Sparse Attention (1M)</td></tr><tr><td><strong>归一化</strong></td><td>RMSNorm (PreNorm)</td><td>RMSNorm (PreNorm)</td><td>RMSNorm (PreNorm)</td><td>RMSNorm (PreNorm)</td><td>RMSNorm (PreNorm)</td></tr><tr><td><strong>激活函数</strong></td><td>SwiGLU</td><td>SwiGLU</td><td>SwiGLU</td><td>SwiGLU</td><td>SwiGLU</td></tr><tr><td><strong>模型类别</strong></td><td>基座/对话模型</td><td>基座/对话模型</td><td>基座/对话/多模态/MoE</td><td>基座/对话/MoE</td><td><strong>代码/视觉/音频/全模态/长上下文/推理增强</strong></td></tr><tr><td><strong>开源许可</strong></td><td>Apache 2.0 / Tongyi Lic.</td><td>Apache 2.0</td><td>Apache 2.0</td><td>Apache 2.0</td><td>Apache 2.0</td></tr></tbody></table><h2 id=关键技术解析>关键技术解析<a hidden class=anchor aria-hidden=true href=#关键技术解析>#</a></h2><p>以下是目前基座大模型所采用的关键技术的详细解析，包括数学公式和相关说明。</p><h3 id=multi-head-latent-attention-mla>Multi-Head Latent Attention (MLA)<a hidden class=anchor aria-hidden=true href=#multi-head-latent-attention-mla>#</a></h3><p><strong>原理简介：</strong> Multi-Head Latent Attention (MLA) 是 DeepSeek-V2/V3 中引入的一种创新的注意力机制，旨在解决超长上下文（如 128k tokens）带来的 KV 缓存爆炸和计算瓶颈问题 (<a href=#ref-13>DeepSeek-AI, 2024a</a>)。</p><p><strong>机制：</strong> MLA 的核心思想是将长序列的 Key 和 Value 信息压缩到一个固定大小（远小于序列长度）的<strong>潜在表示 (latent representation)</strong> 中。当前的 Query 不再直接关注原始的长序列 K/V，而是关注这个压缩后的潜在 K/V。这可以通过学习一个低秩投影或引入可学习的潜在 token 来实现。</p><p><strong>优势：</strong></p><ul><li><strong>极大压缩 KV 缓存：</strong> KV 缓存大小从与序列长度 \(L\) 相关降低到与潜在表示大小 \(M\) 相关（\(M \ll L\)），DeepSeek-V2 报告减少了 93.3%。</li><li><strong>提升推理吞吐量：</strong> 减少了内存访问和通信开销，显著提高了长序列生成的吞吐量（DeepSeek-V2 报告提升 5.76 倍）。</li><li><strong>支持超长上下文：</strong> 使得处理 128k 甚至更长的上下文成为可能。</li></ul><p><strong>挑战：</strong> 压缩可能导致信息损失，需要精心设计潜在表示和训练策略（如 DeepSeek-V3 中的解耦 RoPE）来保持性能。</p><p><strong>应用：</strong> DeepSeek-V2 和 DeepSeek-V3 使用 MLA 成功实现了 128k 的上下文窗口。MLA 被认为是 GQA/MQA 之后处理长上下文的更先进方案。</p><h3 id=mixture-of-experts-moe>Mixture-of-Experts (MoE)<a hidden class=anchor aria-hidden=true href=#mixture-of-experts-moe>#</a></h3><p><strong>原理简介：</strong> Mixture-of-Experts (MoE) 是一种稀疏激活的神经网络架构。它将网络中的某些层（通常是 FFN 层）替换为多个并行的“专家”子网络，并通过一个门控网络 (gating network) 为每个输入 token 选择性地激活其中少数几个专家进行计算。</p><p><strong>机制：</strong></p><ul><li><strong>专家 (Experts):</strong> \(N\) 个并行的子网络（如 FFN）。</li><li><strong>门控网络 (Gating Network):</strong> 根据输入 \(x\) 计算每个专家的权重或选择概率 \(G(x)\)。</li><li><strong>稀疏激活:</strong> 通常只选择 Top-K (K 通常为 1 或 2) 个专家进行计算。</li><li><strong>输出:</strong> 选中专家的输出根据门控权重加权求和：\(y = \sum_{i \in \text{TopK}(G(x))} G(x)_i \cdot E_i(x)\)。</li></ul><p><strong>优势：</strong></p><ul><li><strong>参数规模与计算解耦：</strong> 允许模型拥有巨大的总参数量（通过增加专家数量），但每次前向传播的计算量只取决于激活的少数专家，远低于同等总参数的密集模型。</li><li><strong>专家特化：</strong> 理论上，不同专家可以学习处理不同类型的数据或任务，提高模型能力和泛化性。</li></ul><p><strong>挑战：</strong></p><ul><li><strong>负载均衡：</strong> 需要确保专家被均匀利用，避免部分专家过载或闲置。通常需要引入辅助损失。</li><li><strong>通信开销：</strong> 在分布式训练和推理中，需要在不同设备间进行 All-to-All 通信以路由 token 到正确的专家。</li><li><strong>训练稳定性：</strong> MoE 训练可能比密集模型更不稳定。</li></ul><p><strong>DeepSeek MoE 创新：</strong> DeepSeekMoE 架构通过<strong>细粒度专家分割</strong>和<strong>共享专家隔离</strong>来提升专家特化程度和负载均衡效果 (<a href=#ref-10>Dai et al., 2024</a>)。DeepSeek-V3 进一步实现了<strong>无辅助损失的负载均衡</strong>。</p><p><strong>应用：</strong> Google 的 Switch Transformer、GLaM 以及 DeepSeek 系列的 MoE、V2、V3、Coder-V2、R1 都采用了 MoE 架构。MoE 被认为是扩展模型规模、突破密集模型瓶颈的关键技术之一。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><ul><li>Ainslie, J., et al. (2023). GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. <em>arXiv preprint arXiv:2305.13245</em>. (<a href=https://arxiv.org/abs/2305.13245>Link</a>)</li><li>AI at Meta. (2024a). Introducing Meta Llama 3: The most capable openly available LLM to date. (<a href=https://ai.meta.com/blog/meta-llama-3/>Link</a>)</li><li>AI at Meta. (2024b). Introducing Llama 3.1: Our most capable models to date. (<a href=https://ai.meta.com/blog/meta-llama-3-1/>Link</a>)</li><li>Chi, Z., et al. (2024). Llama Guard 3 Vision: Advancing Multimodal Safety. <em>arXiv preprint arXiv:2411.10414</em>. (<a href=https://arxiv.org/abs/2411.10414>Link</a>)</li><li>Dai, W., et al. (2024). DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. <em>arXiv preprint arXiv:2401.06066</em>. (<a href=https://arxiv.org/abs/2401.06066>Link</a>)</li><li>Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. <em>arXiv preprint arXiv:2205.14135</em>. (<a href=https://arxiv.org/abs/2205.14135>Link</a>)</li><li>DeepSeek-AI. (2023). DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. <em>GitHub Repository</em>. (<a href=https://github.com/deepseek-ai/deepseek-llm>Link</a>) (Corresponds to arXiv:2401.02954)</li><li>DeepSeek-AI. (2024a). DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model. <em>arXiv preprint arXiv:2405.04434</em>. (<a href=https://arxiv.org/abs/2405.04434>Link</a>)</li><li>DeepSeek-AI. (2024b). DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. <em>GitHub Repository</em>. (<a href=https://github.com/deepseek-ai/DeepSeek-Coder-V2>Link</a>)</li><li>DeepSeek-AI. (2024c). DeepSeek-V3 Technical Report. <em>arXiv preprint arXiv:2412.19437</em>. (<a href=https://arxiv.org/abs/2412.19437>Link</a>)</li><li>DeepSeek-AI. (2025). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. <em>arXiv preprint arXiv:2501.12948</em>. (<a href=https://arxiv.org/abs/2501.12948>Link</a>)</li><li>Grattafiori, D., et al. (2024). The Llama 3 Herd of Models. <em>arXiv preprint arXiv:2407.21783</em>. (<a href=https://arxiv.org/abs/2407.21783>Link</a>)</li><li>Guo, D., et al. (2024). DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence. <em>arXiv preprint arXiv:2401.14196</em>. (<a href=https://arxiv.org/abs/2401.14196>Link</a>)</li><li>Inan, H., et al. (2023). Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations. <em>arXiv preprint arXiv:2312.06674</em>. (<a href=https://arxiv.org/abs/2312.06674>Link</a>)</li><li>Rozière, B., et al. (2023). Code Llama: Open Foundation Models for Code. <em>arXiv preprint arXiv:2308.12950</em>. (<a href=https://arxiv.org/abs/2308.12950>Link</a>)</li><li>Shazeer, N. (2020). GLU Variants Improve Transformer. <em>arXiv preprint arXiv:2002.05202</em>. (<a href=https://arxiv.org/abs/2002.05202v1>Link</a>)</li><li>Su, J., et al. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. <em>arXiv preprint arXiv:2104.09864</em>. (<a href=https://arxiv.org/abs/2104.09864>Link</a>)</li><li>Touvron, H., et al. (2023a). LLaMA: Open and Efficient Foundation Language Models. <em>arXiv preprint arXiv:2302.13971</em>. (<a href=https://arxiv.org/abs/2302.13971>Link</a>)</li><li>Touvron, H., et al. (2023b). Llama 2: Open Foundation and Fine-Tuned Chat Models. <em>arXiv preprint arXiv:2307.09288</em>. (<a href=https://arxiv.org/abs/2307.09288>Link</a>)</li><li>Vidgen, B., et al. (2024). Building a Taxonomy and Datasets for Safe AI Dialogue. <em>arXiv preprint arXiv:2404.12241</em>. (<a href=https://arxiv.org/abs/2404.12241>Link</a>)</li><li>Zhang, B., & Sennrich, R. (2019). Root Mean Square Layer Normalization. <em>Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</em>. (<a href=https://arxiv.org/abs/1910.07467>Link</a>)</li><li>Zhu, Y., et al. (2024). TransMLA: Multi-Head Latent Attention Is All You Need. <em>arXiv preprint arXiv:2502.07864</em>. (<a href=https://arxiv.org/abs/2502.07864>Link</a>)</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/zh/tags/nlp/>NLP</a></li><li><a href=https://syhya.github.io/zh/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/zh/tags/pre-training/>Pre-Training</a></li><li><a href=https://syhya.github.io/zh/tags/post-training/>Post-Training</a></li><li><a href=https://syhya.github.io/zh/tags/llama/>LLaMA</a></li><li><a href=https://syhya.github.io/zh/tags/deepseek/>DeepSeek</a></li><li><a href=https://syhya.github.io/zh/tags/qwen/>Qwen</a></li><li><a href=https://syhya.github.io/zh/tags/moe/>MoE</a></li><li><a href=https://syhya.github.io/zh/tags/mla/>MLA</a></li><li><a href=https://syhya.github.io/zh/tags/rope/>RoPE</a></li><li><a href=https://syhya.github.io/zh/tags/gqa/>GQA</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/zh/posts/2025-01-27-deepseek-r1/><span class=title>« 上一页</span><br><span>OpenAI o1复现进展：DeepSeek-R1</span>
</a><a class=next href=https://syhya.github.io/zh/posts/2025-01-16-group-query-attention/><span class=title>下一页 »</span><br><span>Transformer注意力机制：MHA、MQA与GQA的对比</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型（长期更新中） on x" href="https://x.com/intent/tweet/?text=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-base_llm%2f&amp;hashtags=AI%2cNLP%2cLLM%2cPre-training%2cPost-training%2cLlama%2cDeepSeek%2cQwen%2cMoE%2cMLA%2cRoPE%2cGQA"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型（长期更新中） on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-base_llm%2f&amp;title=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89&amp;summary=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-base_llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型（长期更新中） on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-base_llm%2f&title=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型（长期更新中） on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-base_llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型（长期更新中） on whatsapp" href="https://api.whatsapp.com/send?text=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-base_llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型（长期更新中） on telegram" href="https://telegram.me/share/url?text=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-base_llm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型（长期更新中） on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-base_llm%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>