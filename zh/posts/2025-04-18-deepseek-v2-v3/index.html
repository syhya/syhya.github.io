<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DeepSeek-V2 vs V3 | Yue Shui 博客</title><meta name=keywords content="Deep Learning,AI,LLM,DeepSeek-V2,DeepSeek-V3,MoE,Transformer,MLA,DeepSeekMoE,MTP,FP8 Training,GRPO,SFT,RL,KV Cache"><meta name=description content="DeepSeek AI 先后发布了 DeepSeek-V2 (DeepSeek-AI, 2024) 和 DeepSeek-V3 (DeepSeek-AI, 2024)，这两款强大的混合专家（Mixture-of-Experts, MoE）语言模型在保持顶尖性能的同时，显著优化了训练成本和推理效率。DeepSeek-V2 拥有 236B 总参数，每次激活 21B；而 DeepSeek-V3 则进一步扩展至 671B 总参数，每次激活 37B。两者均支持 128K 上下文长度。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="DeepSeek-V2 vs V3"><meta property="og:description" content="DeepSeek AI 先后发布了 DeepSeek-V2 (DeepSeek-AI, 2024) 和 DeepSeek-V3 (DeepSeek-AI, 2024)，这两款强大的混合专家（Mixture-of-Experts, MoE）语言模型在保持顶尖性能的同时，显著优化了训练成本和推理效率。DeepSeek-V2 拥有 236B 总参数，每次激活 21B；而 DeepSeek-V3 则进一步扩展至 671B 总参数，每次激活 37B。两者均支持 128K 上下文长度。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-18T12:00:00+08:00"><meta property="article:modified_time" content="2025-06-16T18:31:58+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="article:tag" content="DeepSeek-V2"><meta property="article:tag" content="DeepSeek-V3"><meta property="article:tag" content="MoE"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="DeepSeek-V2 vs V3"><meta name=twitter:description content="DeepSeek AI 先后发布了 DeepSeek-V2 (DeepSeek-AI, 2024) 和 DeepSeek-V3 (DeepSeek-AI, 2024)，这两款强大的混合专家（Mixture-of-Experts, MoE）语言模型在保持顶尖性能的同时，显著优化了训练成本和推理效率。DeepSeek-V2 拥有 236B 总参数，每次激活 21B；而 DeepSeek-V3 则进一步扩展至 671B 总参数，每次激活 37B。两者均支持 128K 上下文长度。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"DeepSeek-V2 vs V3","item":"https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DeepSeek-V2 vs V3","name":"DeepSeek-V2 vs V3","description":"DeepSeek AI 先后发布了 DeepSeek-V2 (DeepSeek-AI, 2024) 和 DeepSeek-V3 (DeepSeek-AI, 2024)，这两款强大的混合专家（Mixture-of-Experts, MoE）语言模型在保持顶尖性能的同时，显著优化了训练成本和推理效率。DeepSeek-V2 拥有 236B 总参数，每次激活 21B；而 DeepSeek-V3 则进一步扩展至 671B 总参数，每次激活 37B。两者均支持 128K 上下文长度。\n","keywords":["Deep Learning","AI","LLM","DeepSeek-V2","DeepSeek-V3","MoE","Transformer","MLA","DeepSeekMoE","MTP","FP8 Training","GRPO","SFT","RL","KV Cache"],"articleBody":"DeepSeek AI 先后发布了 DeepSeek-V2 (DeepSeek-AI, 2024) 和 DeepSeek-V3 (DeepSeek-AI, 2024)，这两款强大的混合专家（Mixture-of-Experts, MoE）语言模型在保持顶尖性能的同时，显著优化了训练成本和推理效率。DeepSeek-V2 拥有 236B 总参数，每次激活 21B；而 DeepSeek-V3 则进一步扩展至 671B 总参数，每次激活 37B。两者均支持 128K 上下文长度。\n这两个模型的核心创新在于采用了 多头隐注意力 (Multi-head Latent Attention, MLA) 和 DeepSeekMoE 架构 (Dai et al., 2024)。MLA 通过将键值（KV）缓存压缩到低维隐向量中，大幅降低了推理时的显存占用，提高了效率。DeepSeekMoE 则通过细粒度专家切分和共享专家隔离，实现了更强的专家特化能力和更经济的训练成本。DeepSeek-V3 在 V2 的基础上，进一步引入了无辅助损失的负载均衡策略 (Auxiliary-Loss-Free Load Balancing) (Wang et al., 2024) 和多 token 预测 (Multi-Token Prediction, MTP) (Gloeckle et al., 2024)训练目标 ，进一步提升了模型性能和训练效率。\nDeepSeek-V2 在 8.1T tokens 上进行预训练，而 DeepSeek-V3 则在更大规模的 14.8T tokens 上训练。两者都经过了监督微调（Supervised Fine-Tuning, SFT）和强化学习（Reinforcement Learning, RL）阶段以充分释放潜力。评估结果显示，DeepSeek-V2 和 V3 在众多基准测试中均达到了开源模型的顶尖水平，DeepSeek-V3 更是成为了目前最强的开源基础模型之一，性能可与顶尖闭源模型媲美。\nFig. 1. (a) MMLU accuracy vs. activated parameters, among different open-source models. (b) Training costs and inference efficiency of DeepSeek 67B (Dense) and DeepSeek-V2. (Image source: DeepSeek-AI, 2024)\nFig. 2. Benchmark performance of DeepSeek-V3 and its counterparts. (Image source: DeepSeek-AI, 2024)\n本文将深入探讨 DeepSeek-V2 和 DeepSeek-V3 的关键技术，包括其创新的模型架构、高效的训练基础设施、预训练和对齐过程。\n符号表 下面列举了文章所使用的数学公式，可以帮你更轻松阅读。\n符号 含义 \\( d \\) 嵌入维度 \\( n_h \\) 注意力头数量 \\( d_h \\) 每个注意力头的维度 \\( \\mathbf{h}_t \\in \\mathbb{R}^d \\) 第 \\( t \\) 个 token 在注意力层的输入 \\( \\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t \\) 查询, 键, 值向量 \\( W^Q, W^K, W^V, W^O \\) 查询、键、值、输出的投影矩阵 \\( \\mathbf{q}_{t,i}, \\mathbf{k}_{t,i}, \\mathbf{v}_{t,i} \\) 第 \\( i \\) 个注意力头的查询、键、值向量 \\( \\mathbf{o}_{t,i} \\) 第 \\( i \\) 个注意力头的输出 \\( \\mathbf{u}_t \\) 注意力层的最终输出 \\( l \\) 模型层数 \\( \\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c} \\) 键值的压缩隐向量 \\( d_c \\) KV 压缩维度 \\( W^{DKV}, W^{UK}, W^{UV} \\) KV 的下投影、键的上投影、值的上投影矩阵 \\( \\mathbf{k}_t^C, \\mathbf{v}_t^C \\) 通过上投影从隐向量恢复的键和值 \\( \\mathbf{c}_t^Q \\in \\mathbb{R}^{d_c'} \\) 查询的压缩隐向量 \\( d_c' \\) 查询压缩维度 \\( W^{DQ}, W^{UQ} \\) 查询的下投影、上投影矩阵 \\( \\mathbf{q}_t^C \\) 通过上投影从隐向量恢复的查询 \\( \\mathbf{q}_{t,i}^R, \\mathbf{k}_t^R \\) 解耦的 RoPE 查询和键 \\( d_h^R \\) 解耦 RoPE 查询/键的头维度 \\( W^{QR}, W^{KR} \\) 解耦 RoPE 查询/键的生成矩阵 \\( \\operatorname{RoPE}(\\cdot) \\) 应用旋转位置编码的操作 \\( [\\cdot ; \\cdot] \\) 拼接操作 \\( n_g \\) GQA 中的组数 \\( n \\) MoE 中的专家总数 \\( E_i \\) 第 \\( i \\) 个专家网络 \\( G(\\cdot) \\) 门控网络函数 \\( p_i \\) 门控网络输出的第 \\( i \\) 个概率 \\( H^{(i)}(x) \\) Noisy Top-k Gating 中专家 \\( i \\) 的门控分数 \\( W_g, W_{\\text{noise}} \\) MoE 门控网络和噪声网络的权重矩阵 \\( \\epsilon \\) 标准高斯噪声 \\( \\text{softplus}(\\cdot) \\) Softplus 激活函数 \\( k \\) MoE 中每个 token 选择的专家数量 \\( \\text{topk}(\\cdot, k) \\) 选择前 k 个最大值的函数 \\( \\mathcal{L}_{\\text{aux}} \\) MoE 辅助损失 \\( w_{\\text{aux}} \\) 辅助损失权重 \\( \\text{CV}(\\cdot) \\) 变异系数 \\( N_s, N_r \\) DeepSeekMoE 中共享专家和路由专家的数量 \\( \\operatorname{FFN}_i^{(s)}(\\cdot), \\operatorname{FFN}_i^{(r)}(\\cdot) \\) 第 \\( i \\) 个共享专家和路由专家函数 \\( K_r \\) DeepSeekMoE 中激活的路由专家数量 \\( g_{i,t} \\) 第 \\( i \\) 个专家对第 \\( t \\) 个 token 的门控值 \\( g_{i,t}' \\) TopK 选择后的原始门控值（V3） \\( s_{i,t} \\) 第 \\( t \\) 个 token 对第 \\( i \\) 个专家的亲和度分数 \\( \\mathbf{e}_i \\) 第 \\( i \\) 个路由专家的中心向量 \\( M \\) 设备/节点限制路由中的设备/节点数上限 \\( \\mathcal{L}_{\\text{ExpBal}}, \\mathcal{L}_{\\text{DevBal}}, \\mathcal{L}_{\\text{CommBal}} \\) 专家级、设备级、通信级负载均衡损失 \\( f_i, P_i \\) 专家 \\( i \\) 的负载分数和平均亲和度 \\( \\alpha_1, \\alpha_2, \\alpha_3 \\) 负载均衡损失的超参数 \\( T \\) 序列中的 token 数量 \\( D \\) 设备/节点组的数量 \\( \\mathcal{E}_i \\) 第 \\( i \\) 个设备/节点上的专家组 \\( f_i', P_i' \\) 设备组 \\( i \\) 的平均负载分数和总亲和度 \\( f_i'', P_i'' \\) 发送到设备 \\( i \\) 的 token 比例和设备组 \\( i \\) 的总亲和度 \\( b_i \\) 第 \\( i \\) 个专家的偏置项 (aux-loss-free balancing) \\( \\gamma \\) 偏置项更新速度 \\( \\mathcal{L}_{\\text{Bal}} \\) 序列级负载均衡损失 \\( \\alpha \\) 序列级负载均衡损失的超参数 \\( D_{MTP} \\) MTP 预测深度 \\( \\operatorname{Emb}(\\cdot), \\operatorname{OutHead}(\\cdot) \\) 共享嵌入层和输出头 (MTP) \\( \\operatorname{TRM}_k(\\cdot) \\) 第 \\( k \\) 个 MTP 模块的 Transformer 块 \\( M_k \\) 第 \\( k \\) 个 MTP 模块的投影矩阵 \\( \\mathbf{h}_i^k \\) 第 \\( i \\) 个 token 在第 \\( k \\) 个 MTP 深度的表示 \\( \\mathbf{h}_i^{\\prime k} \\) 第 \\( k \\) 个 MTP 模块的 Transformer 块的输入 \\( P_{i+k+1}^k \\) 第 \\( k \\) 个 MTP 模块对第 \\( i+k+1 \\) 个 token 的预测概率分布 \\( V \\) 词汇表大小 \\( \\mathcal{L}_{\\text{MTP}}^k \\) 第 \\( k \\) 个 MTP 深度的交叉熵损失 \\( \\mathcal{L}_{\\text{MTP}} \\) 总 MTP 损失 \\( \\lambda \\) MTP 损失的权重因子 \\( \\mathcal{J}_{GRPO}(\\theta) \\) GRPO 目标函数 \\( A_i \\) 相对优势值 (GRPO) \\( \\varepsilon \\) PPO/GRPO 中的裁剪超参数 \\( \\beta \\) KL 散度惩罚项系数 \\( \\mathbb{D}_{KL}(\\pi_\\theta \\| \\pi_{ref}) \\) KL 散度 \\( \\pi_\\theta, \\pi_{\\theta_{old}}, \\pi_{ref} \\) 当前策略、旧策略、参考策略模型 \\( r_i \\) 第 \\( i \\) 个输出的奖励值 \\( \\mathbb{1}(\\cdot) \\) 指示函数 核心架构 DeepSeek-V2 和 V3 均基于 Transformer 架构，但在注意力和前馈网络（FFN）部分采用了创新设计比如 MLA 和 DeepseekMoE，以平衡性能、训练成本和推理效率。下图展示了 DeepSeek-V2 和 V3 的架构。\nFig. 3. Illustration of the architecture of DeepSeek-V2 and DeepSeek-V3. MLA ensures efficient inference by significantly reducing the KV cache for generation, and DeepSeekMoE enables training strong models at an economical cost through the sparse architecture. (Image source: DeepSeek-AI, 2024)\n多头隐注意力 (MLA) 传统的 Transformer 模型通常采用多头注意力（Multi-Head Attention, MHA）(Vaswani et al., 2017)，但在生成过程中，其庞大的 KV cache 成为限制推理效率的瓶颈。为了解决这个问题，研究者提出了多查询注意力（Multi-Query Attention, MQA）(Shazeer, 2019) 和分组查询注意力（Grouped-Query Attention, GQA）(Ainslie et al., 2023)。这些方法虽然减少了 KV 缓存，但往往以牺牲模型性能为代价。\nDeepSeek-V2 和 V3 采用了创新的 多头隐注意力（Multi-head Latent Attention, MLA） 机制。MLA 的核心思想是 低秩键值联合压缩 (Low-Rank Key-Value Joint Compression)。\nFig. 4. Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. (Image source: DeepSeek-AI, 2024)\nMHA 回顾 标准 MHA 首先通过三个投影矩阵 \\(W^Q, W^K, W^V \\in \\mathbb{R}^{d_h n_h \\times d}\\) 将输入 \\(\\mathbf{h}_t \\in \\mathbb{R}^d\\) 转换为查询 \\(\\mathbf{q}_t\\)、键 \\(\\mathbf{k}_t\\) 和值 \\(\\mathbf{v}_t \\in \\mathbb{R}^{d_h n_h}\\)： \\[ \\begin{aligned} \\mathbf{q}_{t} \u0026= W^{Q} \\mathbf{h}_{t}, \\\\ \\mathbf{k}_{t} \u0026= W^{K} \\mathbf{h}_{t}, \\\\ \\mathbf{v}_{t} \u0026= W^{V} \\mathbf{h}_{t}. \\end{aligned} \\] 然后将 \\(\\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t\\) 切分为 \\(n_h\\) 个头，进行多头注意力计算： \\[ \\begin{aligned} \u0026 [\\mathbf{q}_{t, 1} ; \\mathbf{q}_{t, 2} ; \\ldots ; \\mathbf{q}_{t, n_{h}}] = \\mathbf{q}_{t}, \\\\ \u0026 [\\mathbf{k}_{t, 1} ; \\mathbf{k}_{t, 2} ; \\ldots ; \\mathbf{k}_{t, n_{h}}] = \\mathbf{k}_{t}, \\\\ \u0026 [\\mathbf{v}_{t, 1} ; \\mathbf{v}_{t, 2} ; \\ldots ; \\mathbf{v}_{t, n_{h}}] = \\mathbf{v}_{t}, \\\\ \u0026 \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} \\operatorname{Softmax}_{j}\\left(\\frac{\\mathbf{q}_{t, i}^{T} \\mathbf{k}_{j, i}}{\\sqrt{d_{h}}}\\right) \\mathbf{v}_{j, i}, \\\\ \u0026 \\mathbf{u}_{t} = W^{O}\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right], \\end{aligned} \\] 其中 \\(\\mathbf{q}_{t, i}, \\mathbf{k}_{t, i}, \\mathbf{v}_{t, i} \\in \\mathbb{R}^{d_h}\\) 分别是第 \\(i\\) 个头的查询、键、值，\\(W^O \\in \\mathbb{R}^{d \\times d_h n_h}\\) 是输出投影矩阵。推理时，需要缓存所有 \\(t\\) 步的键和值，每个 token 需要缓存 \\(2 n_h d_h l\\) 个元素（\\(l\\) 为层数），这构成了巨大的 KV 缓存开销。\n低秩键值联合压缩 MLA 通过引入一个低维的隐向量 \\(\\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c}\\) 来联合压缩键和值，其中 \\(d_c \\ll d_h n_h\\)： \\[ \\begin{aligned} \\boxed{\\mathbf{c}_{t}^{K V}} \u0026= W^{D K V} \\mathbf{h}_{t}, \\\\ \\mathbf{k}_{t}^{C} \u0026= W^{U K} \\mathbf{c}_{t}^{K V}, \\\\ \\mathbf{v}_{t}^{C} \u0026= W^{U V} \\mathbf{c}_{t}^{K V}. \\end{aligned} \\] 这里 \\(W^{DKV} \\in \\mathbb{R}^{d_c \\times d}\\) 是下投影矩阵，\\(W^{UK}, W^{UV} \\in \\mathbb{R}^{d_h n_h \\times d_c}\\) 分别是键和值的上投影矩阵。推理时，MLA 只需要缓存压缩后的隐向量 \\(\\mathbf{c}_t^{KV}\\)（以及后面提到的解耦 RoPE 键 \\(\\mathbf{k}_t^R\\)），极大地减少了 KV 缓存量。\n为了减少训练时的激活内存，MLA 也对查询进行了类似的低秩压缩： \\[ \\begin{aligned} \\mathbf{c}_{t}^{Q} \u0026= W^{D Q} \\mathbf{h}_{t}, \\\\ \\mathbf{q}_{t}^{C} \u0026= W^{U Q} \\mathbf{c}_{t}^{Q}, \\end{aligned} \\] 其中 \\(\\mathbf{c}_t^Q \\in \\mathbb{R}^{d_c'}\\) 是查询的压缩隐向量，\\(d_c' \\ll d_h n_h\\)，\\(W^{DQ} \\in \\mathbb{R}^{d_c' \\times d}\\) 和 \\(W^{UQ} \\in \\mathbb{R}^{d_h n_h \\times d_c'}\\) 分别是查询的下投影和上投影矩阵。\n解耦旋转位置编码 标准的旋转位置编码（RoPE）(Su et al., 2024) 直接应用于键和查询，但这与 MLA 的低秩 KV 压缩不兼容。如果在压缩后的键 \\(\\mathbf{k}_t^C\\) 上应用 RoPE，那么上投影矩阵 \\(W^{UK}\\) 会与位置相关的 RoPE 矩阵耦合，导致推理时无法将其吸收到 \\(W^Q\\) 中，必须重新计算所有前缀 token 的键，严重影响效率。\n为了解决这个问题，MLA 提出了解耦 RoPE (Decoupled RoPE) 策略。它引入了额外的多头查询 \\(\\mathbf{q}_{t, i}^R \\in \\mathbb{R}^{d_h^R}\\) 和一个共享的键 \\(\\mathbf{k}_t^R \\in \\mathbb{R}^{d_h^R}\\) 来专门承载 RoPE 信息：\n\\[ \\begin{aligned} \\left[\\mathbf{q}_{t,1}^R;\\,\\mathbf{q}_{t,2}^R;\\,\\dots;\\,\\mathbf{q}_{t,n_h}^R\\right] = \\mathbf{q}_t^R \u0026= \\operatorname{RoPE}\\bigl(W^{Q R}\\,\\mathbf{c}_t^Q\\bigr),\\\\ \\boxed{\\mathbf{k}_t^R} \u0026= \\operatorname{RoPE}\\bigl(W^{K R}\\,\\mathbf{h}_t\\bigr). \\end{aligned} \\] 这里 \\(W^{QR} \\in \\mathbb{R}^{d_h^R n_h \\times d_c'}\\) 和 \\(W^{KR} \\in \\mathbb{R}^{d_h^R \\times d}\\) 是生成解耦查询和键的矩阵。 然后将压缩得到的键/查询部分 (\\(C\\)) 与解耦的 RoPE 部分 (\\(R\\)) 拼接起来形成最终的键和查询： \\[ \\begin{aligned} \\mathbf{q}_{t, i} \u0026= [\\mathbf{q}_{t, i}^{C} ; \\mathbf{q}_{t, i}^{R}], \\\\ \\mathbf{k}_{t, i} \u0026= [\\mathbf{k}_{t, i}^{C} ; \\mathbf{k}_{t}^{R}]. \\end{aligned} \\] 最终的注意力计算变为： \\[ \\begin{aligned} \\mathbf{o}_{t, i} \u0026= \\sum_{j=1}^{t} \\operatorname{Softmax}_{j}\\left(\\frac{\\mathbf{q}_{t, i}^{T} \\mathbf{k}_{j, i}}{\\sqrt{d_{h}+d_{h}^{R}}}\\right) \\mathbf{v}_{j, i}^{C}, \\\\ \\mathbf{u}_{t} \u0026= W^{O}\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right]. \\end{aligned} \\] 推理时，除了缓存 \\(\\mathbf{c}_t^{KV}\\)，还需要缓存解耦的 RoPE 键 \\(\\mathbf{k}_t^R\\)。因此，DeepSeek-V2/V3 每个 token 总共需要缓存 \\((d_c + d_h^R)l\\) 个元素。\nMLA 推理中的矩阵吸收 MLA 的一个关键优势在于推理效率的提升，这部分得益于矩阵乘法结合律允许将上投影矩阵 \\(W^{UK}\\) 和 \\(W^{UV}\\) “吸收”掉，避免显式计算完整的键 \\(\\mathbf{k}_t^C\\) 和值 \\(\\mathbf{v}_t^C\\)。\n1. 吸收 \\(W^{UK}\\) (优化注意力分数计算):\n注意力分数计算的核心是查询和键的点积 \\(\\mathbf{q}_{t,i}^T \\mathbf{k}_{j,i}\\)。关注其中由压缩向量生成的 \\(C\\) 部分的点积： \\[ (\\mathbf{q}_{t,i}^C)^T \\mathbf{k}_{j,i}^C \\] 将 \\(\\mathbf{k}_{j,i}^C = W^{UK} \\mathbf{c}_j^{KV}\\) 代入： \\[ (\\mathbf{q}_{t,i}^C)^T (W^{UK} \\mathbf{c}_j^{KV}) \\] 根据矩阵乘法结合律 \\((AB)C = A(BC)\\) 和转置性质 \\((AB)^T = B^T A^T\\)，可以将上式改写为： \\[ (\\mathbf{q}_{t,i}^C)^T (W^{UK} \\mathbf{c}_j^{KV}) = ((W^{UK})^T \\mathbf{q}_{t,i}^C)^T \\mathbf{c}_j^{KV} \\] 这个变换的意义在于：不再需要用 \\(W^{UK}\\) 作用于缓存的 \\(\\mathbf{c}_j^{KV}\\) 来得到 \\(\\mathbf{k}_{j,i}^C\\)。相反，可以先计算一个“有效查询” \\(\\tilde{\\mathbf{q}}_{t,i}^C = (W^{UK})^T \\mathbf{q}_{t,i}^C\\)，然后直接用这个有效查询与缓存的隐向量 \\(\\mathbf{c}_j^{KV}\\) 进行点积。\n原始查询 \\(\\mathbf{q}_{t,i}^C\\) 是通过 \\(W^{UQ}\\) 和 \\(W^{DQ}\\) 从 \\(\\mathbf{h}_t\\) 计算得到的 (\\(\\mathbf{q}_{t,i}^C = (W^{UQ} W^{DQ} \\mathbf{h}_t)_i\\))。因此，从 \\(\\mathbf{h}_t\\) 到有效查询 \\(\\tilde{\\mathbf{q}}_{t,i}^C\\) 的整个计算过程可以看作是一个新的、合并了 \\(W^{UK}\\) 的有效查询投影操作。在实际实现中，这意味着计算 \\(\\mathbf{q}_{t,i}^C\\) 后，可以再左乘 \\((W^{UK})^T\\)，或者更高效地，将 \\((W^{UK})^T\\) 合并到生成查询的原始矩阵 \\(W^Q\\)（或 \\(W^{UQ}W^{DQ}\\)）中，形成一个新的查询投影矩阵 \\(\\tilde{W}^Q = (W^{UK})^T W^{UQ} W^{DQ}\\)。\n关键在于，涉及 \\(W^{UK}\\) 的计算被移到了查询侧，在计算注意力分数之前一次性完成，而无需在每次查询时都用 \\(W^{UK}\\) 从缓存的 \\(\\mathbf{c}_j^{KV}\\) 中恢复 \\(\\mathbf{k}_{j,i}^C\\)。\n2. 吸收 \\(W^{UV}\\) (优化加权求和):\n注意力头的输出 \\(\\mathbf{o}_{t,i}\\) 是注意力权重 (记作 \\(w_{ij}\\)) 与值 \\(\\mathbf{v}_{j,i}^C\\) 的加权和： \\[ \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} w_{ij} \\cdot \\mathbf{v}_{j, i}^{C} \\] 将 \\(\\mathbf{v}_{j,i}^C = (W^{UV} \\mathbf{c}_j^{KV})_i\\) 代入（这里 \\(( \\cdot )_i\\) 表示属于第 \\(i\\) 个头的部分）： \\[ \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} w_{ij} \\cdot (W^{UV} \\mathbf{c}_j^{KV})_i \\] 最终的注意力层输出 \\(\\mathbf{u}_t\\) 是所有头的输出 \\(\\mathbf{o}_{t,i}\\) 拼接后通过输出矩阵 \\(W^O\\) 投影得到的： \\[ \\mathbf{u}_{t} = W^{O}\\left[\\mathbf{o}_{t, 1} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right] = W^{O} \\begin{bmatrix} \\sum_{j} w_{1j} (W^{UV} \\mathbf{c}_j^{KV})_1 \\\\ \\vdots \\\\ \\sum_{j} w_{n_h j} (W^{UV} \\mathbf{c}_j^{KV})_{n_h} \\end{bmatrix} \\] 由于矩阵乘法的线性性质（\\(A(B+C) = AB + AC\\) 以及 \\(A(cB) = c(AB)\\)），可以将 \\(W^{UV}\\) 从求和中“提出”（这里是为了直观理解，实际操作是矩阵层面的）： \\[ \\mathbf{u}_{t} \\approx W^{O} W^{UV} \\left( \\sum_{j=1}^{t} \\begin{bmatrix} w_{1j} (\\mathbf{c}_j^{KV})_1 \\\\ \\vdots \\\\ w_{n_h j} (\\mathbf{c}_j^{KV})_{n_h} \\end{bmatrix} \\right) \\] （注意：这里的 \\((\\mathbf{c}_j^{KV})_i\\) 只是示意，实际计算中是直接对完整的 \\(\\mathbf{c}_j^{KV}\\) 操作，但原理相同，即先对 \\(\\mathbf{c}_j^{KV}\\) 进行加权求和，再应用 \\(W^{UV}\\) 和 \\(W^O\\)）。\n令有效输出矩阵 \\(\\tilde{W}^O = W^O W^{UV}\\)。这意味着可以先计算注意力权重与隐向量 \\(\\mathbf{c}_j^{KV}\\) 的加权和（得到一个维度为 \\(d_c\\) 的中间结果 \\(\\tilde{\\mathbf{o}}_t = \\sum_j w_{ij} \\mathbf{c}_j^{KV}\\)），然后直接用这个合并后的有效输出矩阵 \\(\\tilde{W}^O\\) 进行最终投影得到 \\(\\mathbf{u}_t\\)。同样，涉及 \\(W^{UV}\\) 的计算被合并到了最后的输出投影步骤，无需在计算加权和时从 \\(\\mathbf{c}_j^{KV}\\) 恢复 \\(\\mathbf{v}_{j,i}^C\\)。\n总结: 通过矩阵吸收，MLA 在推理时避免了从缓存的低维隐向量 \\(\\mathbf{c}_j^{KV}\\) 重复计算高维的键 \\(\\mathbf{k}_{j,i}^C\\) 和值 \\(\\mathbf{v}_{j,i}^C\\)，显著提高了计算效率。实际缓存的只有 \\(\\mathbf{c}_t^{KV}\\) 和 \\(\\mathbf{k}_t^R\\)。\nKV 缓存对比 下表比较了不同注意力机制的每 Token KV 缓存对比。\\(n_h\\) 是注意力头数，\\(d_h\\) 是每头维度，\\(l\\) 是层数，\\(n_g\\) 是 GQA 组数，\\(d_c\\) 和 \\(d_h^R\\) 是 MLA 的 KV 压缩维度和解耦 RoPE 维度。对于 DeepSeek-V2，\\(d_c = 4d_h\\)，\\(d_h^R = d_h/2\\)，其 KV 缓存相当于 \\(n_g=2.25\\) 的 GQA，但性能优于 MHA。DeepSeek-V3 沿用了类似配置。\n注意力机制 每 Token KV 缓存大小 (# 元素) 能力 Multi-Head Attention (MHA) \\(2 n_{h} d_{h} l\\) 强 Grouped-Query Attention (GQA) \\(2 n_{g} d_{h} l\\) 中等 Multi-Query Attention (MQA) \\(2 d_{h} l\\) 弱 Multi-head Latent Attention (MLA) \\(\\bigl(d_{c} + d_{h}^{R}\\bigr) l \\approx \\tfrac{9}{2} \\, d_{h} \\, l\\) 更强 下图表明，MLA 不仅显著减少了 KV 缓存，其性能甚至优于标准的 MHA。\nFig. 5. Comparison between MLA and MHA on hard benchmarks. DeepSeek-V2 shows better performance than MHA, but requires a significantly smaller amount of KV cache. (Image source: DeepSeek-AI, 2024)\n混合专家模型 在深入 DeepSeekMoE 之前，先回顾一下混合专家模型（MoE）的基础知识。\n混合专家模型(Mixture-of-Experts, MoE)(Shazeer et al. 2017) 是一种稀疏激活模型，它通过结合多个独立的“专家”网络和一个门控网络(Gating Network)，在不显著增加计算成本的前提下，大幅提升了模型的参数量和性能。MoE 的核心思想是稀疏激活(Sparse Activation)，即对于每个输入样本，仅激活部分专家网络，而不是整个模型。这种方法既提高了计算效率，又增强了模型的表达能力，使其在 LLMs 中表现出色。\nMoE 设计灵感来源于集成学习(Ensemble learning), 一种将复杂任务分解为多个子任务并由不同模型协作完成的技术。在 MoE 中，这些“子任务”由多个独立的专家网络处理，而门控网络则负责根据输入样本的特征动态选择最适合的专家。这种分工合作的机制类似于人类社会中的专家团队：不同领域的专家针对特定问题提供专业意见，最终综合得出结果。\nFig. 6. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)\nMoE 核心组件 一个典型的 MoE 包含以下组件：\n专家网络(Experts): 一组独立的神经网络 ${E_1, E_2, …, E_n}$，每个专家网络 $E_i$ 可以是任意类型的神经网络，例如 FFN, CNN, RNN 等。专家网络的数量 $n$ 可以很大，例如几十个、几百个甚至几千个。 门控网络(Gating Network): 一个可训练的神经网络 $G$，用于根据输入样本 $x$ 学习一个概率分布，决定激活哪些专家。门控网络的输入是输入样本 $x$，输出是一个 $n$ 维的概率向量 $p = G(x) = [p_1, p_2, …, p_n]$，其中 $p_i$ 表示激活专家 $E_i$ 的概率。 专家输出聚合(Expert Output Aggregation): 根据门控网络的输出概率分布，将激活的专家网络的输出进行加权求和，得到 MoE 层的最终输出 $y$。 Noisy Top-k Gating 为了实现稀疏激活并确保专家使用均衡，MoE 通常采用 Noisy Top-k Gating 作为门控机制。这种方法通过引入噪声和 top-k 选择，既保证了计算效率，又避免了专家负载不均的问题。以下是其详细工作流程：\n门控分数计算: 对于输入样本 $x$，门控网络首先计算每个专家的门控分数 $H^{(i)}(x)$。这一分数包含两部分：线性变换和噪声项，公式如下：\n$$ H^{(i)}(x) =(x W_g)^{(i)} + \\epsilon \\cdot \\text{softplus}\\left((x W_{\\text{noise}})^{(i)} \\right), \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$ 参数说明： $W_g \\in \\mathbb{R}^{d \\times n}$：门控网络的可训练权重矩阵，$d$ 是输入特征维度，$n$ 是专家数量。 $W_{\\text{noise}} \\in \\mathbb{R}^{d \\times n}$：用于生成噪声的权重矩阵。 $\\epsilon \\sim \\mathcal{N}(0, 1)$：标准高斯噪声，增加门控随机性。 $\\text{softplus}(x) = \\log(1 + e^x)$：平滑激活函数，确保噪声非负。 噪声的引入避免了门控网络总是选择固定的专家，增强了模型的鲁棒性和多样性。\nTop-k 选择: 计算出门控分数向量 $H(x) = [H^{(1)}(x), H^{(2)}(x), \\dots, H^{(n)}(x)]$ 后，门控网络选择其中值最大的前 $k$ 个专家(通常 $k \\ll n$)。这一步骤通过 $\\text{topk}(v, k)$ 函数实现：\n$$ \\text{topk}^{(i)}(v, k) = \\begin{cases} v^{(i)} \u0026 \\text{if } v^{(i)} \\text{ is in the top } k \\text{ elements of } v \\\\ -\\infty \u0026 \\text{otherwise} \\end{cases} $$将非 Top-k 专家的分数设为 $-\\infty$，确保后续 softmax 操作中这些专家的概率为 0，实现稀疏性。\nSoftmax 归一化: 对 Top-k 专家的门控分数进行 softmax 归一化，得到稀疏的概率分布 $G(x)$：\n$$ G(x) = \\text{softmax}\\left( \\text{topk}(H(x), k) \\right) $$只有 Top-k 个专家的概率非零，其余为 0。例如，若 $n=100, k=2$，则 98 个专家的概率为 0。\n加权求和: 将 Top-k 个专家的输出按概率加权求和，得到 MoE 层的输出：\n$$ y = \\sum_{i=1}^{n} G^{(i)}(x) E_i(x) $$由于只有 $k$ 个专家被激活，计算量远低于激活所有 $n$ 个专家。\n辅助损失 为了避免门控网络过度偏向少数专家，MoE 引入了辅助损失(Auxiliary Loss)(Shazeer et al. 2017)，鼓励所有专家被均匀使用。一种常用方法是基于专家使用率的变异系数(Coefficient of Variation, CV)的平方：\n$$ \\mathcal{L}_{\\text{aux}} = w_{\\text{aux}} \\cdot \\text{CV}\\left( \\sum_{x \\in X} G(x) \\right)^2 $$ 参数说明：\n$X$：一个 mini-batch 的输入样本。 $\\sum_{x \\in X} G(x)$：统计每个专家在 mini-batch 中的激活次数。 $\\text{CV}$：标准差与均值的比值，衡量专家使用分布的均匀性。 $w_{\\text{aux}}$：辅助损失的权重，需手动调整。 作用：通过最小化 $\\mathcal{L}_{\\text{aux}}$，模型优化专家选择的均衡性，避免某些专家被过度使用而其他专家闲置。\nGShard GShard(Lepikhin et al. 2020)主要对 MoE 层进行分片，将 MoE 层中的专家网络 ${E_1, E_2, …, E_n}$ 分散到多个 TPU 设备上。例如，如果有 $P$ 个 TPU 设备，可以将专家网络划分为 $P$ 组，每组专家网络分配到一个 TPU 设备上。Transformer 模型的其他层(例如自注意力层、LayerNorm 层) 则在所有 TPU 设备上复制。\nGShard 的改进门控机制:\nGShard 在 Noisy Top-k Gating 的基础上，进行了一些改进，以提高门控机制的性能和稳定性：\n专家容量(Expert Capacity):\n为了避免专家过载，GShard 引入了专家容量限制。每个专家网络都有一个容量上限，表示它最多可以处理的 token 数量。如果一个 token 被路由到一个已经达到容量上限的专家网络，则该 token 会被标记为 “overflowed”，门控输出会被设置为零向量，表示该 token 不会被路由到任何专家网络。\n局部组分发(Local Group Dispatching):\n为了提高门控效率，GShard 将 token 分组，在组级别强制执行专家容量限制。例如，将 mini-batch 中的 token 划分为多个局部组，每个局部组包含一定数量的 token。门控网络为每个局部组选择 top-k 个专家网络，并确保每个专家网络在一个局部组内处理的 token 数量不超过其容量上限。\n辅助损失(Auxiliary Loss):\nGShard 也使用了辅助损失函数来平衡专家负载。与原始 MoE 模型的辅助损失不同，GShard 的辅助损失旨在最小化每个专家网络路由到的数据比例的均方误差，更加直接地衡量专家负载平衡程度。\n随机路由(Random Routing):\n为了增加路由的随机性，GShard 在选择 top-k 个专家网络时，引入了随机路由机制。除了选择最佳的 top-k 个专家网络外，GShard 还会以一定的概率随机选择次优的专家网络，增加专家网络的多样性，提高模型的泛化能力。\n下面是 GShard 的核心算法流程:\nFig. 7. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)\nSwitch Transformer Switch Transformer(Fedus et al. 2021) 是 Google 提出的一个参数量达到万亿级别的 MoE 模型。其核心创新是将 Transformer 模型中的密集前馈网络(FFN) 层替换为稀疏的 Switch FFN 层。与 GShard 的 Top-2 Gating 不同，Switch Transformer 每个输入 token 只路由到一个专家网络，具有更高的稀疏性，进一步降低了计算成本，使得训练万亿参数模型成为可能。鼓励 token 路由在 $N$ 个专家之间更加均衡。Switch Transformer 的辅助损失基于实际路由比例与预测路由概率的乘积累加，具体公式如下：\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ 参数说明： $N$：专家的总数。\n$f_i$：路由到第 $i$ 个专家的 token 比例，定义为：\n$$ f_i = \\frac{1}{T} \\sum_{x \\in B} 1\\{\\text{argmax } p(x) = i\\} $$ $P_i$：gating 网络预测的第 $i$ 个专家的路由概率，定义为：\n$$ P_i = \\frac{1}{T} \\sum_{x \\in B} p_i(x) $$ $T$：批次 $B$ 中的 token 总数。\n$\\alpha$：辅助损失的权重超参数，通常设为 $10^{-2}$。\n通过最小化 $\\text{loss}$，模型使实际路由比例 $f_i$ 与预测概率 $P_i$ 趋于一致，从而间接促进专家间的负载平衡，避免部分专家闲置。\nFig. 8. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)\nSwitch Router 机制:\n路由预测:\n对于输入 token $x$，Switch Router 预测每个专家网络的路由概率 $p_i = G^{(i)}(x)$，其中 $i = 1, 2, …, n$，n 是专家网络数量。\n专家选择:\n选择路由概率最高的专家网络作为最佳专家网络。Switch Transformer 采用 Top-1 路由策略，即每个 token 只路由到路由概率最高的专家网络。\ntoken 路由:\n将输入 token $x$ 路由到选择的最佳专家网络进行处理。\nSwitch Transformer 的训练稳定性优化:\n为提升 Switch Transformer 的训练稳定性，论文提出了如下优化策略：\n选择性精度(Selective Precision)\n在路由函数内部采用 FP32 精度既能提高训练稳定性，又能避免因 FP32 张量通信而产生的额外开销。具体来说，Switch Router 的计算过程全程使用 FP32，最终结果再转换为 FP16 以兼顾效率与精度。\n更小初始化(Smaller Initialization)\n建议将 Transformer 的权重初始化尺度参数 $s$ 从 1 调整至 0.1。较小的初始化尺度有助于缓解训练初期的梯度爆炸风险，从而提升整体训练稳定性。具体实现为：从均值为 0、标准差为 $\\sqrt{s/n}$(其中 $n$ 为输入单元数) 的截断正态分布中采样。\n更高专家 Dropout(Higher Expert Dropout)\n在专家 FFN 层中采用较高的 dropout 率(例如 0.4)，而在非专家层则保持较低的 dropout 率(例如 0.1)，这种设置能有效防止过拟合，进而增强模型的泛化能力。下图实验结果显示，在 GLUE、CNNDM、SQuAD 和 SuperGLUE 等任务上，当专家层 dropout 率设为 0.4 时，模型表现最佳。\nFig. 9. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: Fedus et al. 2021)\nSwitch Transformers 论文中使用下图直观的展示了使用不同的并行技术如何分割模型权重和数据:\nFig. 10. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)\n专家选择 专家选择(Expert Choice, EC)(Zhou et al. 2022) 是一种与 token 选择路由(如 GShard 的 top-2 或 Switch Transformer 的 top-1)相反的路由策略。在 token 选择路由中，每个 token 从所有专家中选择 top-k 个进行路由；而在专家选择路由中，每个专家从所有 token 中挑选 top-k 个进行处理。这种方法旨在解决 token 选择路由中的负载不均和 token 浪费问题，同时显著提高训练效率。下面是具体的计算过程：\n计算 token-to-expert 亲和度分数\n对于输入矩阵 $X \\in \\mathbb{R}^{n \\times d}$，计算 token-to-expert 亲和度分数矩阵 $S \\in \\mathbb{R}^{n \\times e}$ 的过程为：\n$$ S = \\text{softmax}(X \\cdot W_g), \\quad \\text{where } W_g \\in \\mathbb{R}^{d \\times e}. $$ 这里，$W_g$ 为门控权重矩阵，$e$ 为专家数量。\n专家选择 token\n每个专家从所有 token 中选择 top-k 个进行处理。通过对 $S^T$ 进行 top-k 选择：\n$$ G, I = \\text{top-}k(S^T, k), $$得到：\n门控矩阵 $G \\in \\mathbb{R}^{e \\times k}$： 记录专家选择的 token 对应的路由权重，其中 $G[i, j]$ 表示专家 $i$ 选择的第 $j$ 个 token 的权重； token 索引矩阵 $I \\in \\mathbb{R}^{e \\times k}$： 表示每个专家选择的 token 在输入中的索引。 One-hot 编码\n将 token 索引矩阵 $I$ 转换为 one-hot 编码矩阵 $P \\in \\mathbb{R}^{e \\times k \\times n}$，用于后续计算：\n$$ P = \\operatorname{one}-\\operatorname{hot}(I) $$ 构造 Gated FFN 层输入\n对于每个专家 $i$，其 gated FFN 层的输入为：\n$$ (P \\cdot X) \\in \\mathbb{R}^{e \\times k \\times d}. $$ EC 通过正则化限制每个 token 被路由到的专家数量，从而控制模型的稀疏性。一个常见的正则化目标如下：\n$$ \\begin{aligned} \u0026 \\max_{A} \\langle S^{\\top}, A \\rangle + \\lambda H(A) \\\\ \u0026 \\text{s.t. } \\forall i: \\sum_{j'} A[i, j'] = k, \\quad \\forall j: \\sum_{i'} A[i', j] \\leq b, \\quad \\forall i,j: 0 \\leq A[i, j] \\leq 1, \\end{aligned} $$考虑的优化问题中定义了一个矩阵 $A$，其第 $i$ 行第 $j$ 列的元素表示第 $i$ 个专家是否选择了第 $j$ 个 token(取值 0 或 1)。由于该优化问题求解较为复杂，论文中采用 Dijkstra 算法(通过多次迭代获得近似解)来解决。\n参数 $b$ 通常由批量中 token 总数 $n$ 与容量因子决定，其中容量因子表示每个 token 平均使用的专家数量。大多数实验采用较高的容量因子，实验结果表明，即使在容量降低的情况下，EC 整体表现仍优于传统的 top-1 token 选择路由，尽管 capped expert choice 略微降低了微调性能。\nEC 的优势主要体现在以下两方面：\n完美负载均衡： 每个专家固定处理 $k$ 个 token，从而避免了部分专家过载而其他专家闲置的问题，实现了理想的负载均衡。 更高训练效率： 实验表明，EC 能将训练收敛速度提升约 2 倍，相较于传统 token 选择路由具有更高的效率。 但 EC 也存在以下局限性：\n批量大小要求： 由于 EC 对 batch size 有较高要求，因此不适用于较小 batch size 的场景。 自回归生成限制： 在自回归文本生成任务中，由于无法预知未来 token，EC 的 top-k 选择无法实现，因此不适用于此类任务。 DeepSeekMoE 混合专家（MoE）模型通过将计算路由到特定的“专家”子网络来提高效率和性能。DeepSeek-V2 和 V3 在其 FFN（前馈网络）层中采用了名为 DeepSeekMoE (Dai et al., 2024)架构 。与 GShard 等传统 MoE 架构相比，DeepSeekMoE 的核心思想是：\n细粒度专家切分 (Fine-grained Expert Segmentation): 将专家网络切分得更小。这旨在实现更高的专家特化能力和更精确的知识获取，因为每个专家可以专注于更窄的领域。 共享专家隔离 (Shared Expert Isolation): 架构中包含一部分“共享专家”，这些专家由所有 token 处理，旨在处理通用知识。这减少了需要路由的“路由专家”之间的知识冗余，让路由专家可以更专注于特定知识。 基本架构 对于 FFN 层的输入 token 表示 \\(\\mathbf{u}_t\\)，DeepSeekMoE 的输出 \\(\\mathbf{h}_t'\\) 通过结合共享专家和选定的路由专家的输出来计算： \\[ \\mathbf{h}_{t}^{\\prime} = \\mathbf{u}_{t} + \\sum_{i=1}^{N_{s}} \\operatorname{FFN}_{i}^{(s)}(\\mathbf{u}_{t}) + \\sum_{i=1}^{N_{r}} g_{i, t} \\operatorname{FFN}_{i}^{(r)}(\\mathbf{u}_{t}), \\] 其中：\n\\(N_s\\) 是共享专家的数量。 \\(N_r\\) 是路由专家的数量。 \\(\\operatorname{FFN}_i^{(s)}\\) 是第 \\(i\\) 个共享专家网络。 \\(\\operatorname{FFN}_i^{(r)}\\) 是第 \\(i\\) 个路由专家网络。 \\(g_{i, t}\\) 是第 \\(t\\) 个 token 分配给第 \\(i\\) 个路由专家的门控值（权重）。 门控值 \\(g_{i,t}\\) 的计算方式是 DeepSeek-V2 和 V3 之间的关键区别之一，它基于 token-to-expert 的亲和度分数 \\(s_{i,t}\\)，并通过 Top-K 路由机制选择激活哪些专家。\nV2 与 V3 的门控机制和负载均衡对比 MoE 模型的一个核心挑战是负载均衡：确保所有专家都能得到有效利用，避免某些专家过载而其他专家空闲，这会影响训练稳定性和计算效率。DeepSeek-V2 和 V3 在门控机制和负载均衡策略上采取了不同的方法。\n1. 亲和度计算 (\\(s_{i,t}\\)) 与 Top-K 选择:\nDeepSeek-V2: 使用 Softmax 函数计算每个 token 对每个路由专家的亲和度分数。Top-K 选择直接基于这些亲和度分数 \\(s_{i,t}\\)。 \\[ s_{i, t} = \\operatorname{Softmax}_{i}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i}) \\] 其中 \\(\\mathbf{e}_i\\) 是第 \\(i\\) 个路由专家的可学习中心向量。选择 \\(s_{i,t}\\) 最高的 \\(K_r\\) 个专家。\nDeepSeek-V3: 使用 Sigmoid 函数计算亲和度分数。更重要的是，它引入了一个可学习的偏置项 \\(b_i\\) 用于每个路由专家。Top-K 选择是基于 加偏置后的亲和度 \\(s_{i,t} + b_i\\)。 \\[ s_{i, t} = \\operatorname{Sigmoid}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i}) \\] 选择依据是 \\(s_{i,t} + b_i\\) 值最高的 \\(K_r\\) 个专家。\n2. 门控值计算 (\\(g_{i,t}\\)):\nDeepSeek-V2: 对于被 Top-K 选中的专家，其门控值 \\(g_{i,t}\\) 直接等于其原始亲和度分数 \\(s_{i,t}\\)。对于未被选中的专家，\\(g_{i,t} = 0\\)。 \\[ g_{i, t}^{\\prime} = \\begin{cases} s_{i, t}, \u0026 s_{i, t} \\in \\operatorname{Topk}(\\{s_{j, t}\\}, K_{r}), \\\\ 0, \u0026 \\text{otherwise}, \\end{cases} \\] \\[ g_{i, t} = g_{i, t}^{\\prime} \\quad (\\text{V2 中不进行额外归一化}) \\] DeepSeek-V3: 对于基于 \\(s_{i,t} + b_i\\) 被 Top-K 选中的专家，其门控值 \\(g_{i,t}\\) 是通过对这些被选中专家的 原始亲和度 \\(s_{i,t}\\) 进行归一化得到的。偏置 \\(b_i\\) 仅用于路由选择，不影响最终的加权求和。 \\[ g_{i, t}^{\\prime}= \\begin{cases} s_{i, t}, \u0026 s_{i, t}+b_{i} \\in \\operatorname{Topk}\\left(\\left\\{s_{j, t}+b_{j} \\mid 1 \\leqslant j \\leqslant N_{r}\\right\\}, K_{r}\\right) \\\\ 0, \u0026 \\text{otherwise.} \\end{cases} \\] \\[ g_{i, t} = \\frac{g_{i, t}^{\\prime}}{\\sum_{j=1}^{N_{r}} g_{j, t}^{\\prime}} \\quad (\\text{对选中的专家亲和度进行归一化}) \\] 3. 负载均衡策略:\nDeepSeek-V2:\n主要策略：辅助损失 V2 引入了多种辅助损失项来显式地鼓励负载均衡: 专家级平衡损失 (\\(\\mathcal{L}_{\\text{ExpBal}}\\)): 鼓励每个专家处理大致相等数量的 token。 \\[ \\begin{aligned} \\mathcal{L}_{\\text{ExpBal}} \u0026= \\alpha_{1} \\sum_{i=1}^{N_{r}} f_{i} P_{i} \\\\ f_{i} \u0026= \\frac{N_{r}}{K_{r} T} \\sum_{t=1}^{T} \\mathbb{1}(\\text{Token } t \\text{ selects Expert } i) \\\\ P_{i} \u0026= \\frac{1}{T} \\sum_{t=1}^{T} s_{i, t} \\end{aligned} \\] 其中 \\(T\\) 是 batch 中的 token 总数，\\(f_i\\) 是路由到专家 \\(i\\) 的 token 比例（相对于理想均衡状态），\\(P_i\\) 是专家 \\(i\\) 的平均亲和度分数，\\(\\alpha_1\\) 是超参数。 设备级平衡损失 (\\(\\mathcal{L}_{\\text{DevBal}}\\)): 鼓励将计算负载均匀分布到不同的设备组上（假设专家分布在 \\(D\\) 个设备组 \\(\\{\\mathcal{E}_1, \\dots, \\mathcal{E}_D\\}\\)）。 \\[ \\begin{aligned} \\mathcal{L}_{\\text{DevBal}} \u0026= \\alpha_{2} \\sum_{i=1}^{D} f_{i}^{\\prime} P_{i}^{\\prime} \\\\ f_{i}^{\\prime} \u0026= \\frac{1}{|\\mathcal{E}_{i}|} \\sum_{j \\in \\mathcal{E}_{i}} f_{j} \\\\ P_{i}^{\\prime} \u0026= \\sum_{j \\in \\mathcal{E}_{i}} P_{j} \\end{aligned} \\] 其中 \\(f_i'\\) 是设备组 \\(i\\) 的平均负载分数，\\(P_i'\\) 是设备组 \\(i\\) 的总亲和度，\\(\\alpha_2\\) 是超参数。 通信平衡损失 (\\(\\mathcal{L}_{\\text{CommBal}}\\)): 鼓励发送到每个设备的 token 数量大致相等，以平衡 All-to-All 通信负载。 \\[ \\begin{aligned} \\mathcal{L}_{\\text{CommBal}} \u0026= \\alpha_{3} \\sum_{i=1}^{D} f_{i}^{\\prime \\prime} P_{i}^{\\prime \\prime} \\\\ f_{i}^{\\prime \\prime} \u0026= \\frac{D}{M T} \\sum_{t=1}^{T} \\mathbb{1}(\\text{Token } t \\text{ is sent to Device } i) \\\\ P_{i}^{\\prime \\prime} \u0026= \\sum_{j \\in \\mathcal{E}_{i}} P_{j} \\end{aligned} \\] 其中 \\(f_i''\\) 是发送到设备 \\(i\\) 的 token 比例（相对于理想均衡状态），\\(P_i''\\) 是设备组 \\(i\\) 的总亲和度，\\(\\alpha_3\\) 是超参数。 路由限制：设备限制路由 限制每个 token 最多只能路由到分布在 \\(M\\) 个不同设备上的专家。V2 中设 \\(M=3\\)。 Token 丢弃: 在训练期间，如果某个设备接收到的 token 数量超过了预设的容量因子（通常略大于平均值），则会丢弃一部分具有最低路由权重（亲和度）的 token，以避免计算资源的浪费。但会保留约 10% 序列的 token 不被丢弃。 DeepSeek-V3:\n主要策略：无辅助损失的负载均衡 (Auxiliary-Loss-Free Load Balancing) V3 认为辅助损失会损害模型性能，因此采用了一种创新的无辅助损失的负载均衡(Wang et al., 2024)。它通过动态调整前面提到的可学习偏置项 \\(b_i\\) 来实现负载均衡： 偏置更新: 在每个训练步骤之后，监控每个专家 \\(i\\) 在当前 batch 中处理的 token 数量。 如果专家 \\(i\\) 过载（处理的 token 数 \u003e Batch 总 token 数 / \\(N_r\\)），则降低其偏置：\\(b_i \\leftarrow b_i - \\gamma\\)。 如果专家 \\(i\\) 欠载（处理的 token 数 \u003c Batch 总 token 数 / \\(N_r\\)），则增加其偏置：\\(b_i \\leftarrow b_i + \\gamma\\)。 \\(\\gamma\\) 是一个小的正步长（偏置更新速率超参数）。通过这种方式，负载高的专家在后续路由中被选中的概率会降低，负载低的专家被选中的概率会增加，从而在批处理级别上动态平衡负载。 补充策略：序列级辅助损失 (\\(\\mathcal{L}_{\\text{Bal}}\\)) V3 仍然保留了一个权重极小 (\\(\\alpha=0.0001\\)) 的辅助损失，但它作用于单个序列内部的专家选择平衡，而不是整个 batch。这主要是为了防止在单个序列中出现极端不平衡的情况。 \\[ \\begin{gathered} \\mathcal{L}_{\\text{Bal}} = \\alpha \\sum_{i=1}^{N_{r}} f_{i} P_{i}, \\\\ f_{i} = \\frac{N_{r}}{K_{r} T_{seq}} \\sum_{t=1}^{T_{seq}} \\mathbb{1}\\left(s_{i, t} \\in \\operatorname{Topk}\\left(\\left\\{s_{j, t} \\mid 1 \\leqslant j \\leqslant N_{r}\\right\\}, K_{r}\\right)\\right), \\\\ s_{i, t}^{\\prime} = \\frac{s_{i, t}}{\\sum_{j=1}^{N_{r}} s_{j, t}}, \\quad P_{i} = \\frac{1}{T_{seq}} \\sum_{t=1}^{T_{seq}} s_{i, t}^{\\prime} \\end{gathered} \\] 注意这里的 \\(f_i, P_i\\) 是在单个序列（长度为 \\(T_{seq}\\)）上计算的，并且 \\(s_{i,t}'\\) 是在序列内对原始 \\(s_{i,t}\\) 归一化后的值。 路由限制：节点限制路由 类似于 V2 的设备限制，但应用于节点级别。V3 中设 \\(M=4\\)。 无 Token 丢弃: 由于基于偏置调整的负载均衡效果良好，V3 在训练和推理过程中均不丢弃任何 token。 V3 策略的优势: V3 的无辅助损失策略旨在最小化负载均衡机制对模型最终性能的负面影响。通过动态调整偏置项进行批处理级别的负载均衡，相比 V2 中基于辅助损失的序列级均衡，约束更宽松。这允许专家在不同领域展现出更强的特化模式，因为路由决策不必在每个序列内部都严格遵循均衡分布。下图实验表明该策略在多个基准测试上优于基于辅助损失的方法。\nFig. 11. Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. (Image source: DeepSeek-AI, 2024)\n辅助损失无关的负载均衡与序列级辅助损失的关键区别在于它们的均衡范围：批次级与序列级。相比序列级辅助损失，批次级均衡施加了更灵活的约束，因为它不强制每个序列内的领域平衡。这种灵活性使得专家能够更好地在不同领域中进行专门化。为了验证这一点，图中记录并分析了在 Pile 测试集不同领域上，基于辅助损失的 16B 基线模型和无辅助损失的 16B 模型的专家负载，可以观察到无辅助损失模型表现出更明显的专家专门化模式，符合预期。\nFig. 12. Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. (Image source: DeepSeek-AI, 2024)\nDeepSeekMoE V2 vs V3 对比总结表 特性 DeepSeek-V2 DeepSeek-V3 亲和度计算 \\(s_{i,t}\\) \\(\\operatorname{Softmax}_{i}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i})\\) \\(\\operatorname{Sigmoid}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i})\\) TopK 选择依据 原始亲和度 \\(s_{i,t}\\) 加偏置后的亲和度 \\(s_{i,t} + b_i\\) 门控值计算 \\(g_{i,t}\\) 对选中的专家，\\(g_{i,t} = s_{i,t}\\) (通常无额外归一化) 对选中的专家，基于原始亲和度 \\(s_{i,t}\\) 进行归一化: \\(g_{i, t} = \\frac{s_{i, t}}{\\sum_{j \\in \\text{Selected}} s_{j, t}}\\) 主要负载均衡策略 辅助损失: - \\(\\mathcal{L}_{\\text{ExpBal}}\\) (专家级) - \\(\\mathcal{L}_{\\text{DevBal}}\\) (设备级) - \\(\\mathcal{L}_{\\text{CommBal}}\\) (通信级) 无辅助损失: - 通过动态调整可学习偏置项 \\(b_i\\) (步长 \\(\\gamma\\)) 实现批处理级均衡 补充负载均衡 无明确的补充策略 序列级辅助损失 \\(\\mathcal{L}_{\\text{Bal}}\\) (权重 \\(\\alpha\\) 极小, e.g., 0.0001)，防止单序列内极端不平衡 路由限制 设备限制: 每个 token 最多路由到 \\(M=3\\) 个设备上的专家 节点限制: 每个 token 最多路由到 \\(M=4\\) 个节点上的专家 Token 丢弃 是: 训练时，为缓解计算瓶颈，会丢弃超出设备容量的 token 中亲和度最低的部分 (保留约10%序列不丢弃) 否: 训练和推理中均不丢弃 token 均衡粒度 主要通过辅助损失在序列/Batch 级别强制均衡 主要通过偏置调整在 Batch 级别动态均衡，约束更宽松 对模型性能影响 辅助损失可能对模型性能产生负面影响 设计上旨在最小化均衡策略对性能的负面影响，允许更好的专家特化 多 token 预测 (MTP) 为了进一步提升模型性能和数据效率，DeepSeek-V3 引入了多 token 预测 (Multi-Token Prediction, MTP) 训练目标 (Gloeckle et al., 2024)。标准的语言模型只预测下一个 token，而 MTP 让模型在每个位置预测未来多个（V3 中是 \\(D_{MTP}=1\\)，即预测下下个 token）token。\nMTP 实现 MTP 通过 \\(D_{MTP}\\) 个顺序模块实现。第 \\(k\\) 个 MTP 模块 (\\(k=1, \\dots, D_{MTP}\\)) 包含：\n共享的嵌入层 \\(\\operatorname{Emb}(\\cdot)\\) 共享的输出头 \\(\\operatorname{OutHead}(\\cdot)\\) 独立的 Transformer 块 \\(\\operatorname{TRM}_k(\\cdot)\\) 独立的投影矩阵 \\(M_k \\in \\mathbb{R}^{d \\times 2d}\\) 对于输入序列中的第 \\(i\\) 个 token \\(t_i\\)，在第 \\(k\\) 个预测深度：\n将第 \\(i\\) 个 token 在第 \\(k-1\\) 深度的表示 \\(\\mathbf{h}_i^{k-1}\\)（\\(k=1\\) 时为主模型输出）与第 \\(i+k\\) 个 token 的嵌入 \\(\\operatorname{Emb}(t_{i+k})\\) 拼接，并通过投影矩阵 \\(M_k\\) 得到组合表示 \\(\\mathbf{h}_i^{\\prime k}\\)： \\[ \\mathbf{h}_{i}^{\\prime k} = M_{k}[\\operatorname{RMSNorm}(\\mathbf{h}_{i}^{k-1}) ; \\operatorname{RMSNorm}(\\operatorname{Emb}(t_{i+k}))] \\] 将组合表示输入到第 \\(k\\) 个 Transformer 块，得到当前深度的输出表示 \\(\\mathbf{h}_i^k\\)： \\[ \\mathbf{h}_{1: T-k}^{k} = \\operatorname{TRM}_{k}(\\mathbf{h}_{1: T-k}^{\\prime k}) \\] 使用共享输出头预测第 \\(i+k+1\\) 个 token 的概率分布 \\(P_{i+k+1}^k \\in \\mathbb{R}^V\\)： \\[ P_{i+k+1}^{k} = \\operatorname{OutHead}(\\mathbf{h}_{i}^{k}) \\] 关键在于，这种实现方式保持了每个预测深度的完整因果链，与并行预测多个 token 的方法不同。\nFig. 13. Illustration of our Multi-Token Prediction (MTP) implementation. They keep the complete causal chain for the prediction of each token at each depth. (Image source: DeepSeek-AI, 2024)\nMTP 训练目标 计算每个预测深度 \\(k\\) 的交叉熵损失 \\(\\mathcal{L}_{\\text{MTP}}^k\\)： \\[ \\mathcal{L}_{\\text{MTP}}^{k} = \\operatorname{CrossEntropy}(P_{2+k: T+1}^{k}, t_{2+k: T+1}) = -\\frac{1}{T} \\sum_{i=2+k}^{T+1} \\log P_{i}^{k}[t_{i}] \\] 总的 MTP 损失是所有深度损失的加权平均： \\[ \\mathcal{L}_{\\text{MTP}} = \\frac{\\lambda}{D_{MTP}} \\sum_{k=1}^{D_{MTP}} \\mathcal{L}_{\\text{MTP}}^{k} \\] 其中 \\(\\lambda\\) 是权重因子（V3 中前期为 0.3，后期为 0.1）。这个损失会加到主模型的标准 next-token prediction 损失上。\nMTP 推理 MTP 主要用于提升主模型性能。推理时，可以直接丢弃 MTP 模块，主模型可以独立工作。或者也可以利用 MTP 模块进行投机解码 (Leviathan et al., 2023; Xia et al., 2023)，以加速生成过程。V3 的实验表明，第二个 token 的接受率在 85%-90% 之间，可将解码速度提升约 1.8 倍。\n基础设施与训练效率 DeepSeek-V3 的高效训练与部署得益于算法、框架和硬件的协同设计。\n计算集群 DeepSeek-V3 在一个配备了 2048 块 NVIDIA H800 GPU 的集群上进行训练。\n节点内部: 每个节点包含 8 块 H800 GPU，通过 NVLink 和 NVSwitch 高速互联。 节点之间: 不同节点间使用 InfiniBand (IB) 网络进行通信。 训练框架 DeepSeek-V3 的训练基于自研的高效轻量级框架 HAI-LLM。整体上采用了：\n16 路流水线并行 (Pipeline Parallelism, PP) (Qi et al., 2023) 64 路专家并行 (Expert Parallelism, EP) (跨 8 节点) (Lepikhin et al., 2021) ZeRO-1 数据并行 (Data Parallelism, DP) (Rajbhandari et al., 2020) 为了实现高效训练，Deepseek 进行了细致的工程优化：\n设计了 DualPipe 算法以实现高效流水线并行，减少气泡并重叠计算与通信，解决了跨节点专家并行带来的重通信开销问题。 开发了高效的跨节点 All-to-all 通信 Kernel，充分利用 IB 和 NVLink 带宽，并节省用于通信的 SM 资源。 细致优化了训练过程中的内存占用，使得可以在不使用张量并行 (Tensor Parallelism, TP) 的情况下训练 DeepSeek-V3。 DualPipe 与计算通信重叠 挑战: 跨节点专家并行导致计算通信比接近 1:1，效率不高。 Fig. 17. Overlapping strategy for a pair of forward and backward chunks with misaligned transformer block boundaries. Orange: forward, green: backward for input, blue: backward for weights, purple: PP communication, red: barriers. Both all-to-all and PP communications are fully hidden. (Image source: DeepSeek-AI, 2024)\n核心思想: 重叠一对独立的前向和反向 chunk 内的计算和通信。将每个 chunk 分解为 Attention、All-to-all Dispatch、MLP、All-to-all Combine 四个组件（反向的 Attention 和 MLP 进一步细分为 backward for input 和 backward for weights，类似 ZeroBubble (Qi et al., 2023）。通过重排这些组件并手动调整用于通信与计算的 GPU SM 比例，实现 All-to-all 和 PP 通信的完全隐藏。 调度: 采用双向流水线调度，同时从流水线的两端输入微批次，大部分通信可以被完全重叠。 Fig. 18. Example DualPipe scheduling with 8 PP ranks and 20 micro-batches in both directions. The reverse-direction micro-batches mirror the forward ones, so their batch IDs are omitted for simplicity. Two cells within a shared black border represent mutually overlapped computation and communication. (Image source: DeepSeek-AI, 2024)\n优势: 即使在没有重通信负担的一般场景下也具有效率优势。 相比 ZB1P (Qi et al., 2023) 和 1F1B (Harlap et al., 2018)，显著减少流水线气泡，仅增加 \\(\\frac{1}{PP}\\) 倍的峰值激活内存。 虽然需要两份模型参数副本，但由于训练中使用了大的 EP size，内存增加不显著。 相比 Chimera (Li and Hoefler, 2021)，对微批次数量的要求更宽松（只需能被 2 整除），且气泡和激活内存不随微批次数量增加而增长。 方法 (Method) 气泡 (Bubble) 参数 (Parameter) 激活 (Activation) 1F1B \\((PP-1)(F+B)\\) \\(1 \\times\\) \\(PP\\) ZB1P \\((PP-1)(F+B-2W)\\) \\(1 \\times\\) \\(PP\\) DualPipe (Deepseek V3) \\(\\left(\\frac{PP}{2}-1\\right)(F\\\u0026B+B-3W)\\) \\(2 \\times\\) \\(PP+1\\) 上表对不同流水线并行方法的流水线气泡和内存使用比较。\\(F\\)：前向 chunk 执行时间；\\(B\\)：完整反向 chunk 执行时间；\\(W\\)：“权重反向” chunk 执行时间；\\(F\\\u0026B\\)：两个相互重叠的前向和反向 chunk 的执行时间。\n高效的跨节点 All-to-All 通信实现 目标: 为 DualPipe 提供足够的计算性能，定制高效的跨节点 All-to-all 通信 Kernel (dispatching \u0026 combining)，节省通信专用 SM。 策略: 结合 MoE 门控算法和集群网络拓扑（节点间 IB 全互联，节点内 NVLink）。 带宽利用: NVLink 带宽 (\\(160 \\mathrm{~GB} / \\mathrm{s}\\)) 约是 IB (\\(50 \\mathrm{~GB} / \\mathrm{s}\\)) 的 3.2 倍。限制每个 token 最多分发到 4 个节点以减少 IB 流量。 传输路径: Token 确定路由后，先通过 IB 传输到目标节点上具有相同节点内索引的 GPU。到达目标节点后，立即通过 NVLink 转发给托管目标专家的特定 GPU，避免被后续到达的 token 阻塞。 效果: IB 和 NVLink 通信完全重叠。每个 token 可高效选择平均 3.2 个专家/节点，无需额外 NVLink 开销。这意味着 V3 实际选择 8 个路由专家，但理论上可扩展到 13 个专家 (4 nodes × 3.2 experts/node) 而通信成本不变。 实现: 使用 Warp Specialization (Bauer et al., 2014)技术 ，将 20 个 SM 划分为 10 个通信通道。 Dispatch 过程：IB 发送、IB-to-NVLink 转发、NVLink 接收由各自的 warp 处理，warp 数量根据负载动态调整。 Combine 过程：NVLink 发送、NVLink-to-IB 转发与累加、IB 接收与累加也由动态调整的 warp 处理。 优化: Dispatch 和 Combine Kernel 与计算流重叠。使用定制 PTX 指令并自动调整通信 chunk 大小，显著减少 L2 缓存使用和对其他 SM 计算 Kernel 的干扰。 结果: 仅需 20 个 SM 即可充分利用 IB 和 NVLink 带宽。 极致内存优化与最小开销 为减少训练内存占用，采用了以下技术：\n重计算: 在反向传播中重计算所有 RMSNorm 操作和 MLA 的上投影，避免存储它们的输出激活值。以微小开销显著减少激活内存需求。 CPU 存储 EMA: 将模型参数的指数移动平均 (EMA) 保存在 CPU 内存中，并在每个训练步骤后异步更新。无需额外 GPU 内存或时间开销即可维护 EMA 参数。 共享 Embedding 和输出头: 利用 DualPipe 策略，将模型最浅层（含 Embedding 层）和最深层（含输出头）部署在同一个 PP rank 上。这使得 MTP 模块和主模型可以物理共享 Embedding 和输出头的参数及梯度，进一步提升内存效率。 效果: 这些优化使得 DeepSeek-V3 可以在不使用昂贵的张量并行 (TP) 的情况下进行训练。 FP8 训练 为了加速训练并减少显存占用，DeepSeek-V3 采用了 FP8 混合精度训练框架 (Dettmers et al., 2022; Noune et al., 2022; Peng et al., 2023)，并在超大规模模型上首次验证了其有效性。\n混合精度框架 核心计算 (GEMM): 大部分 GEMM 操作（前向、激活梯度反向、权重梯度反向）使用 FP8 输入，输出 BF16 或 FP32，理论上计算速度翻倍。 高精度保留: 对精度敏感或计算开销小的部分（如 Embedding、输出头、MoE 门控、Normalization、Attention）保留 BF16/FP32 精度。 高精度存储: 主权重、权重梯度、优化器状态（部分 BF16）使用更高精度存储，通过 ZeRO-1 分片降低显存压力。 Fig. 14. The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. (Image source: DeepSeek-AI, 2024)\n精度提升策略 细粒度量化: 为了解决 FP8 动态范围有限和对离群值敏感的问题 (Fishman et al., 2024; He et al., 2024; Sun et al., 2024)，采用更细粒度的量化： 激活: 按 \\(1 \\times 128\\) 的 tile 分组缩放。 权重: 按 \\(128 \\times 128\\) 的 block 分组缩放。 这种方法让缩放因子更适应局部数据的范围，减少量化误差。 提升累加精度: H800 的 Tensor Core 进行 FP8 GEMM 时累加精度有限（约 14 位）。为解决此问题，采用 Promotion to CUDA Cores 策略 (Thakkar et al., 2023)：Tensor Core 计算部分累加和（例如每 \\(N_C=128\\) 个元素），然后将结果传输到 CUDA Core 的 FP32 寄存器中进行全精度累加。细粒度量化的缩放因子也可以在 CUDA Core 上高效应用。通过 WGMMA 操作的并发执行，这种方法在提升精度的同时，对计算效率影响较小。 E4M3 格式: V3 在所有张量上统一使用 E4M3 格式（4 位指数，3 位尾数），而非混合使用 E5M2 (NVIDIA, 2024; Peng et al., 2023; Sun et al., 2019b)。细粒度量化策略有效缓解了 E4M3 动态范围较小的问题。 在线量化: 实时计算每个 tile/block 的最大绝对值来确定缩放因子，而非依赖历史值 (NVIDIA, 2024; Peng et al., 2023)，确保量化精度。 Fig. 15. (a) Fine-grained quantization method to mitigate quantization errors. (b) Improved FP8 GEMM precision by promoting to CUDA Cores for high-precision accumulation. (Image source: DeepSeek-AI, 2024)\n低精度存储与通信 优化器状态: AdamW (Loshchilov and Hutter, 2017) 的一阶和二阶矩使用 BF16 存储。主权重和梯度累加仍用 FP32。 激活缓存: Wgrad 操作使用 FP8 输入，因此激活值可以缓存为 FP8。对特定敏感操作（如 Attention 后 Linear 的输入）使用定制的 E5M6 格式，并进行 round scaling。MoE 中 SwiGLU 的输入也缓存为 FP8。 通信: MoE up-projection 前的激活量化为 FP8 进行分发（dispatch），MoE down-projection 前的激活梯度也量化为 FP8。Combine 操作保留 BF16 精度。 下图实验证明，FP8 训练的损失与 BF16 相比，相对误差低于0.25%，在可接受范围内。\nFig. 16. Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9 DeepSeek-AI, 2024)\n推理与部署 DeepSeek-V3 部署在 H800 集群上（节点内 NVLink，节点间 IB 全互联）。为同时保证在线服务的 SLO (服务等级目标) 和高吞吐量，采用分离 Prefilling (预填充) 和 Decoding (解码) 阶段的部署策略。\n预填充阶段 最小部署单元: 4 节点 32 GPU。 并行策略: Attention 部分: TP4 (张量并行) + SP (序列并行) 结合 DP8 (数据并行)。较小的 TP size (4) 限制了 TP 通信开销。 MoE 部分: EP32 (专家并行)，确保每个专家处理足够大的批次以提高计算效率。 浅层 Dense MLP: 使用 TP1 节省 TP 通信。 MoE All-to-All 通信: 采用与训练类似的方法：先通过 IB 跨节点传输 token，然后在节点内 GPU 间通过 NVLink 转发。 负载均衡: 采用 冗余专家 的部署策略。 根据在线部署收集的统计数据，周期性（例如每 10 分钟）检测高负载专家并进行复制部署。 确定冗余专家集后，根据观察到的负载仔细地在节点内 GPU 间重排专家，在不增加跨节点 All-to-all 通信开销的前提下尽可能平衡 GPU 负载。 DeepSeek-V3 部署中，Prefilling 阶段设置 32 个冗余专家。每个 GPU 除了托管原有的 8 个专家外，还会额外托管 1 个冗余专家。 效率优化: 为提高吞吐并隐藏 All-to-all 和 TP 通信开销，同时处理两个计算负载相似的微批次，将一个微批次的 Attention 和 MoE 与另一个微批次的 Dispatch 和 Combine 重叠。 探索方向: 动态冗余 策略，每个 GPU 托管更多专家（如 16 个），但每次推理步骤只激活 9 个。在每层 All-to-all 操作开始前动态计算全局最优路由方案。由于 Prefilling 计算量大，计算路由方案的开销几乎可忽略。 解码阶段 专家视角: 将共享专家 视为一个路由目标。从这个角度看，每个 token 在路由时会选择 9 个专家（共享专家被视为一个总会被选中的高负载专家）。 最小部署单元: 40 节点 320 GPU。 并行策略: Attention 部分: TP4 + SP 结合 DP80。 MoE 部分: EP320。每个 GPU 只托管一个专家，其中 64 个 GPU 负责托管冗余专家和共享专家。 All-to-All 通信: Dispatch 和 Combine 部分通过 IB 直接点对点传输以实现低延迟。利用 IBGDA (NVIDIA, 2022) 技术进一步最小化延迟和提升通信效率。 负载均衡: 类似 Prefilling，根据在线服务的统计专家负载，周期性确定冗余专家集。但由于每个 GPU 只托管一个专家，无需重排。 探索方向: 动态冗余策略: 但需要更仔细地优化计算全局最优路由方案的算法，并与 Dispatch Kernel 融合以减少开销。 同时处理两个微批次: 与 Prefilling 不同，解码阶段 Attention 耗时占比更大。因此，将一个微批次的 Attention 与另一个微批次的 Dispatch+MoE+Combine 重叠。解码阶段每个专家的批次大小相对较小（通常小于 256 token），瓶颈是内存访问而非计算。由于 MoE 部分只需加载一个专家的参数，访存开销小，使用较少 SM 不会显著影响整体性能。因此，可以只分配一小部分 SM 给 Dispatch+MoE+Combine，避免影响 Attention 部分的计算速度。 对硬件设计的建议 DeepSeek 团队基于 All-to-all 通信和 FP8 训练方案的实现，向 AI 硬件供应商提出以下芯片设计建议。\n通信硬件 现状: 通过计算/通信重叠隐藏了通信延迟，显著降低了对通信带宽的依赖。但当前通信实现依赖昂贵的 SM（例如 H800 上分配了 132 个 SM 中的 20 个用于此目的），限制了计算吞吐量，且 SM 用于通信导致 Tensor Core 完全闲置，效率低下。 SM 主要任务: 在 IB 和 NVLink 域之间转发数据，同时将发往同一节点内多个 GPU 的 IB 流量从单个 GPU 聚合。 在 RDMA 缓冲区和输入/输出缓冲区之间传输数据。 为 All-to-all Combine 执行 Reduce 操作。 在跨 IB 和 NVLink 域向多个专家分块传输数据时管理细粒度内存布局。 期望: 未来供应商开发硬件，将这些通信任务从宝贵的计算单元 SM 卸载，作为 GPU 协处理器或网络协处理器其类似于 NVIDIA SHARP(Graham et al., 2016)。 为降低应用编程复杂性，期望该硬件能从计算单元的角度统一 IB (scale-out) 和 NVLink (scale-up) 网络。通过这个统一接口，计算单元可以通过提交基于简单原语的通信请求，轻松完成在整个 IB-NVLink 统一域内的读、写、多播和 Reduce 等操作。 计算硬件 Tensor Core 中更高精度的 FP8 GEMM 累加: 问题: 当前 NVIDIA Hopper 架构 Tensor Core 实现中，FP8 GEMM 使用定点累加，通过右移对齐尾数乘积再相加。实验表明，符号填充右移后仅使用每个尾数乘积的最高 14 位，超出范围的位被截断。然而，例如要从 32 个 FP8×FP8 乘积累加得到精确的 FP32 结果，至少需要 34 位精度。 建议: 未来芯片设计应增加 Tensor Core 中的累加精度以支持全精度累加，或根据训练和推理算法的精度要求选择合适的累加位宽。这种方法能在保持计算效率的同时确保误差在可接受范围内。 支持 Tile 和 Block 级量化: 问题: 当前 GPU 仅支持 per-tensor 量化，缺乏对 tile-wise 和 block-wise 等细粒度量化的原生支持。当前实现中，达到 \\(N_C\\) 间隔时，部分结果需从 Tensor Core 复制到 CUDA Core，乘以缩放因子，再加到 CUDA Core 的 FP32 寄存器上。虽然结合精确 FP32 累加策略显著缓解了反量化开销，但 Tensor Core 和 CUDA Core 间的频繁数据移动仍限制计算效率。 建议: 未来芯片应通过让 Tensor Core 能够接收缩放因子并实现带分组缩放的 MMA 来支持细粒度量化。这样，整个部分和累加与反量化可以在 Tensor Core 内部直接完成，直到产生最终结果，避免频繁数据移动。 支持在线量化: 问题: 当前实现难以有效支持在线量化，尽管其有效性已在研究中得到证明。现有流程中，需要从 HBM 读取 128 个 BF16 激活值（前一计算的输出）进行量化，量化后的 FP8 值写回 HBM，然后再次读取用于 MMA。 建议: 未来芯片将 FP8 类型转换和 TMA (Tensor Memory Accelerator) 访问融合成单一操作，使得量化可以在激活从全局内存传输到共享内存的过程中完成，避免频繁的内存读写。 推荐支持 warp 级 cast 指令以加速，进一步促进 Layer Normalization 和 FP8 cast 的更好融合。 或者，采用近存计算方法，在 HBM 附近放置计算逻辑。这样，BF16 元素可以在从 HBM 读入 GPU 时直接转换为 FP8，减少约 50% 的片外内存访问。 支持转置 GEMM 操作: 问题: 当前架构将矩阵转置与 GEMM 操作融合起来很麻烦。在工作流中，前向传播的激活被量化为 \\(1 \\times 128\\) 的 FP8 tile 并存储。反向传播时，需要读出矩阵，反量化，转置，重新量化为 \\(128 \\times 1\\) tile，再存入 HBM。 建议: 未来芯片应支持在 MMA 操作前直接从共享内存读取转置后的矩阵（针对训练和推理所需的那些精度）。结合 FP8 格式转换和 TMA 访问的融合，此增强将显著简化量化工作流。 训练成本与效率 DeepSeek-V2: 相比 DeepSeek 67B (Dense)，节省 42.5% 训练成本，KV 缓存减少 93.3%，最大吞吐量提升 5.76 倍。 DeepSeek-V3: 训练效率极高，每训练 1T token 仅需 180K H800 GPU 小时。总训练成本（预训练+上下文扩展+后训练）仅 2.788M H800 GPU 小时（约 558 万美元，按 2 美元/小时计）。预训练在 2048 卡 H800 集群上耗时不到 2 个月。 训练阶段 H800 GPU 小时 预估成本 (美元) 预训练 2664 K $5.328 M 上下文扩展 119 K $0.238 M 后训练 5 K $0.01 M 总计 2788 K $5.576 M 预训练 数据构建 相较于 DeepSeek‑V2（基于 67B 模型，使用 100K 词表 Byte‑level BPE Tokenizer，8.1T tokens），DeepSeek‑V3 在预训练阶段通过以下策略，实现了更大规模和更高质量的数据构建：\n语料库扩展与精炼\n专注领域：显著增加数学与编程相关文本占比，强化模型在技术领域的理解与生成能力。 多语言覆盖：在英语、中文之外，新增多种语种语料，提升跨语言泛化性能。 去重与多样性：采用高效的数据去重和过滤流程，既最大限度减少冗余，又保证内容多样性。 规模提升：最终构建了约 14.8T 高质量 tokens，比 V2 增长近 83%。 训练策略与技术创新\n文档打包 结合 Document Packing(Ding et al., 2024)方法，将连贯文本打包为更长片段，以提升 GPU 利用率和上下文完整性；未采用跨样本注意力掩码，保持实现简洁。 Fill‑in‑Middle（FIM）策略 动机：借鉴 DeepSeekCoder‑V2(DeepSeek‑AI, 2024)的方法，旨在提升模型对中间缺失信息的填充能力。 框架：引入 Prefix‑Suffix‑Middle (PSM) 结构 ，样例如下： \u003c|fim_begin|\u003e f_pre \u003c|fim_hole|\u003e f_suf \u003c|fim_end|\u003e f_middle \u003c|eos_token|\u003e 应用比例：文档级预打包前插入 FIM，占比 10%，平衡生成与预测任务。 Tokenizer 优化\nBBPE 词表扩容：采用 Byte‑level BPE，词表由 100K 扩至 128K，提升罕见词与专有名词覆盖。 预分词器改进：针对多语言场景，调整分词规则，提升压缩效率与编码一致性。 边界偏见缓解：参考 Lundberg, 2023 的方法为减少标点符号+换行组合 token 在 few‑shot 场景下的偏倚，引入随机拆分机制，让模型接触更多边界变体。 超参数 参数 DeepSeek-V2 DeepSeek-V3 Transformer 层数 60 61 隐藏层维度 \\(d\\) 5120 7168 初始化标准差 0.006 0.006 MLA 参数 注意力头数 \\(n_h\\) 128 128 每头维度 \\(d_h\\) 128 128 KV 压缩维度 \\(d_c\\) 512 (\\(4d_h\\)) 512 (\\(4d_h\\)) 查询压缩维度 \\(d_c'\\) 1536 1536 解耦 RoPE 维度 \\(d_h^R\\) 64 (\\(d_h/2\\)) 64 (\\(d_h/2\\)) DeepSeekMoE 参数 MoE 层位置 除第 1 层外 除前 3 层外 共享专家数 \\(N_s\\) 2 1 路由专家数 \\(N_r\\) 160 256 专家中间维度 1536 2048 激活专家数 \\(K_r\\) 6 8 设备/节点限制路由 \\(M\\) 3 (设备) 4 (节点) 负载均衡策略 辅助损失 (\\(\\alpha_1=0.003, \\alpha_2=0.05, \\alpha_3=0.02\\)) + Token Dropping 无辅助损失 (\\(\\gamma=0.001\\)) + 序列级损失 (\\(\\alpha=0.0001\\)) MTP 参数 (V3 only) 预测深度 \\(D_{MTP}\\) N/A 1 MTP 损失权重 \\(\\lambda\\) N/A 0.3 (前 10T) / 0.1 (后 4.8T) 训练参数 优化器 AdamW (\\(\\beta_1=0.9, \\beta_2=0.95, wd=0.1\\)) AdamW (\\(\\beta_1=0.9, \\beta_2=0.95, wd=0.1\\)) 最大序列长度 4K 4K 训练 Tokens 8.1T 14.8T 学习率 Warmup + Step Decay (Max \\(2.4 \\times 10^{-4}\\)) Warmup + Cosine Decay + Constant (Max \\(2.2 \\times 10^{-4}\\)) Batch Size 2304 -\u003e 9216 3072 -\u003e 15360 梯度裁剪 1.0 1.0 精度 BF16 FP8 混合精度 长上下文扩展 两者均使用 YaRN (Peng et al., 2023) 技术扩展上下文窗口。\nDeepSeek-V2: 从 4K 扩展到 128K。使用 YaRN (scale \\(s=40, \\alpha=1, \\beta=32\\))，在 32K 序列长度上训练 1000 步。调整了长度缩放因子 \\(\\sqrt{t}=0.0707 \\ln s+1\\)。 DeepSeek-V3: 分两阶段从 4K 扩展到 32K，再到 128K。每阶段训练 1000 步。YaRN 参数与 V2 相同，长度缩放因子 \\(\\sqrt{t}=0.1 \\ln s+1\\)。第一阶段序列长度 32K，第二阶段 128K。 两模型在 NIAH 测试中均表现出良好的长上下文能力。\nFig. 19. Evaluation results on the ‘Needle In A Haystack’ (NIAH) tests for DeepSeek-V2. (Image source: DeepSeek-AI, 2024)\nFig. 20. Evaluation results on the ‘Needle In A Haystack’ (NIAH) tests for DeepSeek-V3. (Image source: DeepSeek-AI, 2024)\n评估 DeepSeek-V2 评估结果:\nDeepSeek-V2 与代表性开源模型对比 (部分结果)。DeepSeek-V2 以 21B 激活参数达到当时顶尖水平。\nBenchmark (Metric) # Shots DeepSeek 67B Qwen1.5 72B Mixtral 8x22B LLaMA 3 70B DeepSeek-V2 # Activated Params - 67B 72B 39B 70B 21B English MMLU (Hendrycks et al., 2020) (Acc.) 5-shot 71.3 77.2 77.6 78.9 78.5 Code HumanEval (Chen et al., 2021) (Pass@1) 0-shot 45.1 43.9 53.1 48.2 48.8 Math GSM8K (Cobbe et al., 2021) (EM) 8-shot 63.4 77.9 80.3 83.0 79.2 Chinese C-Eval (Huang et al., 2023) (Acc.) 5-shot 66.1 83.7 59.6 67.5 81.7 DeepSeek-V3 评估结果:\nDeepSeek-V3-Base 与代表性开源模型对比 (部分结果)。DeepSeek-V3-Base 在多数基准上成为最强开源模型，尤其在代码和数学方面。\nBenchmark (Metric) # Shots DeepSeek-V2 Base Qwen2.5 72B Base LLaMA-3.1 405B Base DeepSeek-V3 Base # Activated Params - 21B 72B 405B 37B English MMLU (Hendrycks et al., 2020) (EM) 5-shot 78.4 85.0 84.4 87.1 MMLU-Pro (Wang et al., 2024) (em) 5-shot 51.4 58.3 52.8 64.4 Code HumanEval (Chen et al., 2021) (Pass@1) 0-shot 43.3 53.0 54.9 65.2 LiveCodeBench-Base (Jain et al., 2024) (Pass@1) 3-shot 11.6 12.9 15.5 19.4 Math GSM8K (Cobbe et al., 2021) (Em) 8-shot 81.6 88.3 83.5 89.3 MATH (Hendrycks et al., 2021) (EM) 4-shot 43.4 54.4 49.0 61.6 Chinese C-Eval (Huang et al., 2023) (EM) 5-shot 81.4 89.2 72.5 90.1 Multilingual MMMLU-non-English (OpenAI, 2024) (em) 5-shot 64.0 74.8 73.8 79.4 总结: DeepSeek-V3-Base 凭借其架构创新、更大规模的训练数据和高效的训练方法，全面超越了 DeepSeek-V2-Base 和其他顶尖开源模型（包括参数量远超其激活参数的 LLaMA-3.1 405B）。\n对齐 为了使模型更好地理解指令、遵循人类偏好并提升特定能力（如推理），DeepSeek-V2 和 V3 都进行了监督微调 (SFT) 和强化学习 (RL)。\n监督微调 DeepSeek-V2: 使用了约 1.5M 条指令数据，包含 1.2M 帮助性数据和 0.3M 安全性数据，注重提升数据质量以减少幻觉、增强写作能力。 DeepSeek-V3: 推理数据: 利用内部的 DeepSeek-R1 模型 (Guo et al., 2025) 生成推理过程（数学、代码、逻辑等）。由于 R1 输出可能过长或格式不佳，V3 采用了知识蒸馏的思路： 训练领域专家模型（如代码专家）：结合原始 SFT 数据和 R1 生成的长 CoT 数据（带有引导反思/验证的系统提示）进行 SFT+RL 训练。 使用专家模型生成 SFT 数据：专家模型在 RL 过程中学会融合 R1 的推理模式和常规 SFT 数据的简洁性。 拒绝采样：筛选高质量 SFT 数据用于最终 V3 的 SFT。 非推理数据: 使用 DeepSeek-V2.5 生成，并由人工标注员验证。 SFT 设置: 微调 2 个 epoch，学习率从 \\(5 \\times 10^{-6}\\) 余弦衰减到 \\(1 \\times 10^{-6}\\)。采用样本打包和掩码隔离。 强化学习 两者均采用 组相对策略优化 (Group Relative Policy Optimization, GRPO) 算法 (Shao et al., 2024) 进行 RL。GRPO 是一种 Actor-Only 的方法，它通过比较一组（\\(G\\) 个）候选输出的相对好坏来估计优势 \\(A_i\\)，从而避免了训练与策略模型同样大小的 Critic 模型，节省了成本。\nGRPO 目标函数： \\[ \\begin{gathered} \\mathcal{J}_{G R P O}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_{i}\\right\\}_{i=1}^{G} \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\ \\frac{1}{G} \\sum_{i=1}^{G}\\left(\\min \\left(\\frac{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i} \\mid q\\right)} A_{i}, \\operatorname{clip}\\left(\\frac{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i} \\mid q\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A_{i}\\right)-\\beta \\mathbb{D}_{K L}\\left(\\pi_{\\theta}| | \\pi_{r e f}\\right)\\right), \\end{gathered} \\] 其中优势 \\(A_i\\) 通过组内奖励 \\(r_i\\) 标准化得到： \\[ A_{i}=\\frac{r_{i}-\\operatorname{mean}\\left(\\left\\{r_{1}, r_{2}, \\cdots, r_{G}\\right\\}\\right)}{\\operatorname{std}\\left(\\left\\{r_{1}, r_{2}, \\cdots, r_{G}\\right\\}\\right)}. \\] KL 散度惩罚项使用 Schulman 无偏估计器： \\[ \\mathbb{D}_{K L}\\left(\\pi_{\\theta}| | \\pi_{r e f}\\right)=\\frac{\\pi_{r e f}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}-1. \\]奖励模型 (RM):\nDeepSeek-V2: 采用两阶段 RL 策略。 推理对齐: 使用专门训练的 \\(RM_{\\text{reasoning}}\\) 对代码和数学推理任务进行优化。 人类偏好对齐: 使用多奖励框架，结合 \\(RM_{\\text{helpful}}\\)、\\(RM_{\\text{safety}}\\) 和基于规则的 \\(RM_{\\text{rule}}\\)。 DeepSeek-V3: 基于规则的 RM: 对于可验证的任务（如数学答案格式、LeetCode 测试用例），使用规则来提供可靠奖励。 基于模型的 RM: 对于自由格式答案或无标准答案的任务（如创意写作），使用从 V3 SFT Checkpoint 初始化的 RM。该 RM 通过学习带有 CoT 的偏好数据来提升可靠性，减少奖励 Hacking 风险。 自奖励: V3 探索使用模型自身的判断能力（通过投票增强）作为反馈来源，特别是在通用场景下，结合 Constitutional AI (Bai et al., 2022) 的思想进行优化。 RL 训练优化 (V2/V3): 针对大模型 RL 的高资源需求，进行了工程优化，如混合引擎（训练/推理并行策略不同）、使用 vLLM (Kwon et al., 2023) 加速采样、CPU Offloading 调度等。\n评估 DeepSeek-V2 Chat 评估:\nDeepSeek-V2 Chat (SFT/RL) 与代表性开源 Chat 模型在开放式生成任务上的对比。V2 Chat (RL) 在 AlpacaEval 2.0 和 AlignBench 上表现突出。\nModel MT-Bench (Zheng et al., 2023) AlpacaEval 2.0 (Dubois et al., 2024) (LC Win Rate) AlignBench (Liu et al., 2023) (中文) DeepSeek 67B Chat 8.35 16.6 6.43 Mistral 8x22B Instruct 8.66 30.9 - Qwen1.5 72B Chat 8.61 36.6 7.19 LLaMA3 70B Instruct 8.95 34.4 - DeepSeek-V2 Chat (SFT) 8.62 30.0 7.74 DeepSeek-V2 Chat (RL) 8.97 38.9 7.91 DeepSeek-V3 Chat 评估:\nDeepSeek-V3 Chat 与代表性开源及闭源 Chat 模型对比 (部分结果)。V3 在多数基准上领先开源模型，并在代码、数学、中文及开放式生成任务上与顶尖闭源模型相当。\nBenchmark (Metric) DeepSeek V2.5-0905 Qwen2.5 72B-Inst. LLaMA-3.1 405B-Inst. Claude-3.5- Sonnet-1022 GPT-4o 0513 DeepSeek V3 English MMLU (Hendrycks et al., 2020) (EM) 80.6 85.3 88.6 88.3 87.2 88.5 MMLU-Pro (Wang et al., 2024) (EM) 66.2 71.6 73.3 78.0 72.6 75.9 GPQA-Diamond (Rein et al., 2023) (Pass@1) 41.3 49.0 51.1 65.0 49.9 59.1 SimpleQA (OpenAI, 2024c) (Correct) 10.2 9.1 17.1 28.4 38.2 24.9 Code HumanEval-Mul (Pass@1) 77.4 77.3 77.2 81.7 80.5 82.6 LiveCodeBench (Jain et al., 2024) (Pass@1-COT) 29.2 31.1 28.4 36.3 33.4 40.5 SWE Verified (OpenAI, 2024d) (Resolved) 22.6 23.8 24.5 50.8 38.8 42.0 Math AIME 2024 (MAA, 2024 (Pass@1) 16.7 23.3 23.3 16.0 9.3 39.2 MATH-500 (Hendrycks et al., 2021) (ЕМ) 74.7 80.0 73.8 78.3 74.6 90.2 Chinese C-Eval (Huang et al., 2023) (EM) 79.5 86.1 61.5 76.7 76.0 86.5 C-SimpleQA (He et al., 2024) (Correct) 54.1 48.4 50.4 51.3 59.3 64.8 Open-Ended Arena-Hard (Li et al., 2024) 76.2 81.2 69.3 85.2 80.4 85.5 AlpacaEval 2.0 (Dubois et al., 2024) (LC Win Rate) 50.5 49.1 40.5 52.0 51.1 70.0 总结:\nDeepSeek-V2 Chat (RL) 在发布时已是顶尖的开源聊天模型，尤其在 AlpacaEval 和中文 AlignBench 上表现优异。 DeepSeek-V3 Chat 进一步提升了性能，成为目前最强的开源聊天模型，在代码、数学、中文知识以及 Arena-Hard (Li et al., 2024)、AlpacaEval 等开放式评估中表现极其亮眼，达到了与 GPT-4o、Claude-3.5-Sonnet 相媲美的水平。 V3 的 R1 蒸馏显著提升了推理能力，但也可能增加响应长度，需要在准确性和效率间权衡。 V3 的自奖励能力（在 RewardBench (Lambert et al., 2024) 上表现优异）为其持续对齐提供了有效途径。 讨论 负载均衡策略演进: 从 V2 的辅助损失到 V3 的无辅助损失+偏置调整，体现了在保证负载均衡的同时，尽量减少对模型性能本身干扰的趋势。批处理级别的均衡相比序列级均衡，更能促进专家特化。 MTP 的有效性: V3 的实验证明，多 token 预测作为辅助训练目标，确实能提升模型在标准评估任务上的性能，同时为推理加速（推测解码）提供了可能。 R1 蒸馏: V3 成功地将 DeepSeek-R1 的长链推理能力蒸馏到标准 LLM 中，显著提升了数学和代码能力。这是一个重要的技术方向，但也需要注意控制生成长度。 自奖励: V3 强大的判断能力（RewardBench(Lambert et al., 2024)）结果使其能有效进行自反馈和自对齐，这对于减少对人类标注的依赖、实现模型持续自我提升至关重要。 SFT 数据量: 虽然在LIMA(Zhou et al., 2024）认为少量高质量 SFT 数据即可达到不错的效果，但对于特定技能（如指令遵循 IFEval），仍需足够数据量的高质量数据才能达到满意效果。 对齐税: OpenAI 在InstructGPT (Ouyang et al., 2022)中指出 RL 对齐在提升开放式生成能力的同时，可能牺牲部分标准基准的性能。V2 和 V3 都努力在数据处理和训练策略上缓解此问题，以达到可接受的平衡。 结论、局限性与未来方向 结论 DeepSeek-V2 和 DeepSeek-V3 是两款强大、经济且高效的 MoE 语言模型。它们通过 MLA 和 DeepSeekMoE 架构创新，以及 V3 引入的无辅助损失负载均衡、MTP、FP8 训练和 R1 蒸馏等技术，在性能、训练成本和推理效率上取得了突破。DeepSeek-V3 已成为当前最强的开源模型之一，性能可与顶尖闭源模型竞争。\n局限性 通用 LLM 局限: 如知识截止、幻觉、事实性错误等。 语言覆盖: 主要针对中英文，其他语言能力有限 (V2)。V3 扩展了多语言，但仍以中英文为主。 部署门槛 (V3): 高效推理需要较大的部署单元（多节点），对小型团队可能有挑战。 推理效率: 虽然 V3 推理效率相比 V2 有提升，但仍有优化空间。 未来方向 架构创新: 持续优化 MoE 架构，探索支持无限上下文、突破 Transformer 限制的新架构。 数据扩展: 提升训练数据的数量、质量和维度（多模态等）。 深度思考: 增强模型的推理长度和深度，提升智能水平。 评估方法: 发展更全面、多维度的评估方法，避免过拟合特定基准。 对齐与安全: 持续改进对齐技术（如自奖励），确保模型有用、诚实、无害，与人类价值观对齐。 参考文献 [1] Liu, Aixin, et al. “Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.” arXiv preprint arXiv:2405.04434 (2024).\n[2] Liu, Aixin, et al. “Deepseek-v3 technical report.” arXiv preprint arXiv:2412.19437 (2024).\n[3] Dai, Damai, et al. “Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.” arXiv preprint arXiv:2401.06066 (2024).\n[4] Wang, Lean, et al. “Auxiliary-loss-free load balancing strategy for mixture-of-experts.” arXiv preprint arXiv:2408.15664 (2024).\n[5] Gloeckle, Fabian, et al. “Better \u0026 faster large language models via multi-token prediction.” Proceedings of the 41st International Conference on Machine Learning. PMLR 235:16821-16841 (2024).\n[6] Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems 30 (2017).\n[7] Shazeer, Noam. “Fast transformer decoding: One write-head is all you need.” arXiv preprint arXiv:1911.02150 (2019).\n[8] Ainslie, Joshua, et al. “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.” Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4895-4901 (2023).\n[9] Su, Jianlin, et al. “Roformer: Enhanced transformer with rotary position embedding.” Neurocomputing 568 (2024): 127063.\n[10] Shazeer, Noam, et al. “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.” arXiv preprint arXiv:1701.06538 (2017).\n[11] Lepikhin, Dmitry, et al. “Gshard: Scaling giant models with conditional computation and automatic sharding.” arXiv preprint arXiv:2006.16668 (2020).\n[12] Fedus, William, Barret Zoph, and Noam Shazeer. “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.” The Journal of Machine Learning Research 23.1: 5232-5270 (2022).\n[13] Zhou, Zexuan, et al. “Mixture-of-experts with expert choice routing.” Advances in Neural Information Processing Systems 35: 7103-7114 (2022).\n[14] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. “Fast inference from transformers via speculative decoding.” Proceedings of the 40th International Conference on Machine Learning. PMLR 202:19274-19286 (2023).\n[15] Xia, Yichao, et al. “Accelerating large language model decoding with speculative sampling.” arXiv preprint arXiv:2302.01318 (2023).\n[16] Qi, Hai, et al. “ZeroBubble: A High-Performance Framework for Training Mixture-of-Experts Models.” arXiv preprint arXiv:2401.10241 (2024).\n[17] Rajbhandari, Samyam, et al. “Zero: Memory optimizations toward training trillion parameter models.” SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE (2020).\n[18] Harlap, Aaron, et al. “Pipedream: Fast and efficient pipeline parallel dnn training.” arXiv preprint arXiv:1806.03377 (2018).\n[19] Li, Shigang, and Torsten Hoefler. “Chimera: Efficiently training large-scale neural networks with bidirectional pipelines.” Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.\n[20] Bauer, Michael, Sean Treichler, and Alex Aiken. “Singe: Leveraging warp specialization for high performance on gpus.” Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming. 2014.\n[21] Dettmers, Tim, et al. “Llm. int8 (): 8-bit matrix multiplication for transformers at scale.” Advances in Neural Information Processing Systems 35: 34138-34151 (2022).\n[22] Noune, Badreddine, et al. “8-bit numerical formats for deep neural networks.” arXiv preprint arXiv:2206.02915 (2022).\n[23] Peng, Houwen, et al. “FP8-LM: Training FP8 Large Language Models.” arXiv preprint arXiv:2310.18313 (2023).\n[24] Fishman, Maxim, et al. “Scaling FP8 training to trillion-token LLMs.”) arXiv preprint arXiv:2409.12517 (2024).\n[25] He, Bobby, et al. “Understanding and minimising outlier features in neural network training.” arXiv preprint arXiv:2405.19279 (2024).\n[26] Sun, Xiao, et al. “Massive activations in large language models.” arXiv preprint arXiv:2402.17762 (2024).\n[27] NVIDIA. “Transformer Engine.” GitHub Repository (Accessed 2024).\n[28] Sun, Xiao, et al. “Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks.” Advances in neural information processing systems 32 (2019).\n[29] Loshchilov, Ilya, and Frank Hutter. “Decoupled weight decay regularization.” arXiv preprint arXiv:1711.05101 (2017).\n[30] NVIDIA. “GPUDirect Storage: A Direct Path Between Storage and GPU Memory.” NVIDIA Developer Blog (2022).\n[31] Graham, Richard L., et al. “Scalable hierarchical aggregation protocol (SHArP): A hardware architecture for efficient data reduction.” 2016 First International Workshop on Communication Optimizations in HPC (COMHPC). IEEE, 2016.\n[32] Ding, Yiran, et al. “Longrope: Extending llm context window beyond 2 million tokens.” arXiv preprint arXiv:2402.13753 (2024).\n[33] Zhu, Qihao, et al. “DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence.” arXiv preprint arXiv:2406.11931 (2024).\n[34] Lundberg, Scott M. “Guidance: Prompt Boundaries and Token Healing.” GitHub Notebook (2023).\n[35] Peng, Bowen, et al. “YaRN: Efficient Context Window Extension of Large Language Models.” arXiv preprint arXiv:2309.00071 (2023).\n[36] Hendrycks, Dan, et al. “Measuring massive multitask language understanding.” arXiv preprint arXiv:2009.03300 (2020).\n[37] Chen, Mark, et al. “Evaluating large language models trained on code.” arXiv preprint arXiv:2107.03374 (2021).\n[38] Cobbe, Karl, et al. “Training verifiers to solve math word problems.” arXiv preprint arXiv:2110.14168 (2021).\n[39] Huang, Yuzhen, et al. “C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models.” Advances in Neural Information Processing Systems 36 (2023): 62991-63010.\n[40] Wang, Yubo, et al. “Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.” The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024.\n[41] Jain, Naman, et al. “LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code.” arXiv preprint arXiv:2403.07974 (2024).\n[42] Hendrycks, Dan, et al. “Measuring mathematical problem solving with the math dataset.” arXiv preprint arXiv:2103.03874 (2021).\n[43] OpenAI. “MMMLU Dataset.” Hugging Face Datasets (Accessed 2024).\n[44] Guo, Daya, et al. “Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.” arXiv preprint arXiv:2501.12948 (2025).\n[45] Shao, Zhihong, et al. “Deepseekmath: Pushing the limits of mathematical reasoning in open language models.” arXiv preprint arXiv:2402.03300 (2024).\n[46] Bai, Yuntao, et al. “Constitutional ai: Harmlessness from ai feedback.” arXiv preprint arXiv:2212.08073 (2022).\n[47] Kwon, Woosuk, et al. “Efficient memory management for large language model serving with pagedattention.” Proceedings of the 29th Symposium on Operating Systems Principles. 2023.\n[48] Zheng, Lianmin, et al. “Judging llm-as-a-judge with mt-bench and chatbot arena.” Advances in Neural Information Processing Systems 36 (2023): 46595-46623.\n[49] Dubois, Yann, et al. “Length-controlled alpacaeval: A simple way to debias automatic evaluators.” arXiv preprint arXiv:2404.04475 (2024).\n[50] Liu, Xiao, et al. “Alignbench: Benchmarking chinese alignment of large language models.” arXiv preprint arXiv:2311.18743 (2023).\n[51] Rein, David, et al. “GPQA: A Graduate-Level Google-Proof Q\u0026A Benchmark.” First Conference on Language Modeling. 2024.\n[52] OpenAI. “Introducing SimpleQA” OpenAI Blog (2024).\n[53] OpenAI. “Introducing SWE-bench Verified “ OpenAI Blog (2024).\n[54] Mathematical Association of America (MAA). “2024 AIME I Problems.” Art of Problem Solving Wiki (2024).\n[55] Li, Tianle, et al. “From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.” arXiv preprint arXiv:2406.11939 (2024).\n[56] Lambert, Nathan, et al. “RewardBench: Evaluating Reward Models for Language Modeling.” arXiv preprint arXiv:2403.13787 (2024).\n[57] Zhou, Chunting, et al. “Lima: Less is more for alignment.” Advances in Neural Information Processing Systems 36 (2023): 55006-55021.\n[58] Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” Advances in neural information processing systems 35 (2022): 27730-27744.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Apr 2025). DeepSeek-V2 vs V3. https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3\nOr\n@article{syhya2025deepseekv2v3, title = \"DeepSeek-V2 vs V3\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Apr\", url = \"https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3\" } ","wordCount":"23038","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2025-04-18T12:00:00+08:00","dateModified":"2025-06-16T18:31:58+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">DeepSeek-V2 vs V3</h1><div class=post-meta><span title='2025-04-18 12:00:00 +0800 +0800'>2025-04-18</span>&nbsp;·&nbsp;46 分钟&nbsp;·&nbsp;23038 字&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;翻译:<ul class=i18n_list><li><a href=https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/>En</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#符号表>符号表</a></li><li><a href=#核心架构>核心架构</a><ul><li><a href=#多头隐注意力-mla>多头隐注意力 (MLA)</a><ul><li><a href=#mha-回顾>MHA 回顾</a></li><li><a href=#低秩键值联合压缩>低秩键值联合压缩</a></li><li><a href=#解耦旋转位置编码>解耦旋转位置编码</a></li><li><a href=#mla-推理中的矩阵吸收>MLA 推理中的矩阵吸收</a></li><li><a href=#kv-缓存对比>KV 缓存对比</a></li></ul></li><li><a href=#混合专家模型>混合专家模型</a></li><li><a href=#moe-核心组件>MoE 核心组件</a></li><li><a href=#noisy-top-k-gating>Noisy Top-k Gating</a></li><li><a href=#辅助损失>辅助损失</a></li><li><a href=#gshard>GShard</a></li><li><a href=#switch-transformer>Switch Transformer</a></li><li><a href=#专家选择>专家选择</a></li><li><a href=#deepseekmoe>DeepSeekMoE</a><ul><li><a href=#基本架构>基本架构</a></li><li><a href=#v2-与-v3-的门控机制和负载均衡对比>V2 与 V3 的门控机制和负载均衡对比</a></li><li><a href=#deepseekmoe-v2-vs-v3-对比总结表>DeepSeekMoE V2 vs V3 对比总结表</a></li></ul></li><li><a href=#多-token-预测-mtp>多 token 预测 (MTP)</a><ul><li><a href=#mtp-实现>MTP 实现</a></li><li><a href=#mtp-训练目标>MTP 训练目标</a></li><li><a href=#mtp-推理>MTP 推理</a></li></ul></li></ul></li><li><a href=#基础设施与训练效率>基础设施与训练效率</a><ul><li><a href=#计算集群>计算集群</a></li><li><a href=#训练框架>训练框架</a><ul><li><a href=#dualpipe-与计算通信重叠>DualPipe 与计算通信重叠</a></li><li><a href=#高效的跨节点-all-to-all-通信实现>高效的跨节点 All-to-All 通信实现</a></li><li><a href=#极致内存优化与最小开销>极致内存优化与最小开销</a></li></ul></li><li><a href=#fp8-训练>FP8 训练</a><ul><li><a href=#混合精度框架>混合精度框架</a></li><li><a href=#精度提升策略>精度提升策略</a></li><li><a href=#低精度存储与通信>低精度存储与通信</a></li></ul></li><li><a href=#推理与部署>推理与部署</a><ul><li><a href=#预填充阶段>预填充阶段</a></li><li><a href=#解码阶段>解码阶段</a></li></ul></li><li><a href=#对硬件设计的建议>对硬件设计的建议</a><ul><li><a href=#通信硬件>通信硬件</a></li><li><a href=#计算硬件>计算硬件</a></li></ul></li><li><a href=#训练成本与效率>训练成本与效率</a></li></ul></li><li><a href=#预训练>预训练</a><ul><li><a href=#数据构建>数据构建</a></li><li><a href=#超参数>超参数</a></li><li><a href=#长上下文扩展>长上下文扩展</a></li><li><a href=#评估>评估</a></li></ul></li><li><a href=#对齐>对齐</a><ul><li><a href=#监督微调>监督微调</a></li><li><a href=#强化学习>强化学习</a></li><li><a href=#评估-1>评估</a></li></ul></li><li><a href=#讨论>讨论</a></li><li><a href=#结论局限性与未来方向>结论、局限性与未来方向</a><ul><li><a href=#结论>结论</a></li><li><a href=#局限性>局限性</a></li><li><a href=#未来方向>未来方向</a></li></ul></li><li><a href=#参考文献>参考文献</a></li><li><a href=#引用>引用</a></li></ul></nav></div></details></div><div class=post-content><p>DeepSeek AI 先后发布了 <strong>DeepSeek-V2</strong> (<a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>) 和 <strong>DeepSeek-V3</strong> (<a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)，这两款强大的混合专家（Mixture-of-Experts, MoE）语言模型在保持顶尖性能的同时，显著优化了训练成本和推理效率。DeepSeek-V2 拥有 236B 总参数，每次激活 21B；而 DeepSeek-V3 则进一步扩展至 671B 总参数，每次激活 37B。两者均支持 128K 上下文长度。</p><p>这两个模型的核心创新在于采用了 <strong>多头隐注意力 (Multi-head Latent Attention, MLA)</strong> 和 <strong>DeepSeekMoE</strong> 架构 (<a href=https://arxiv.org/abs/2401.06066>Dai et al., 2024</a>)。MLA 通过将键值（KV）缓存压缩到低维隐向量中，大幅降低了推理时的显存占用，提高了效率。DeepSeekMoE 则通过细粒度专家切分和共享专家隔离，实现了更强的专家特化能力和更经济的训练成本。DeepSeek-V3 在 V2 的基础上，进一步引入了<strong>无辅助损失的负载均衡策略 (Auxiliary-Loss-Free Load Balancing)</strong> (<a href=https://arxiv.org/abs/2408.15664>Wang et al., 2024</a>) 和<strong>多 token 预测 (Multi-Token Prediction, MTP)</strong> (<a href=https://arxiv.org/abs/2404.19737>Gloeckle et al., 2024</a>)训练目标 ，进一步提升了模型性能和训练效率。</p><p>DeepSeek-V2 在 8.1T tokens 上进行预训练，而 DeepSeek-V3 则在更大规模的 14.8T tokens 上训练。两者都经过了监督微调（Supervised Fine-Tuning, SFT）和强化学习（Reinforcement Learning, RL）阶段以充分释放潜力。评估结果显示，DeepSeek-V2 和 V3 在众多基准测试中均达到了开源模型的顶尖水平，DeepSeek-V3 更是成为了目前最强的开源基础模型之一，性能可与顶尖闭源模型媲美。</p><figure class=align-center><img loading=lazy src=deepseek_v2_benchmark.png#center alt="Fig. 1. (a) MMLU accuracy vs. activated parameters, among different open-source models. (b) Training costs and inference efficiency of DeepSeek 67B (Dense) and DeepSeek-V2. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 1. (a) MMLU accuracy vs. activated parameters, among different open-source models. (b) Training costs and inference efficiency of DeepSeek 67B (Dense) and DeepSeek-V2. (Image source: <a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>)</p></figcaption></figure><figure class=align-center><img loading=lazy src=deepseek_v3_benchmark.png#center alt="Fig. 2. Benchmark performance of DeepSeek-V3 and its counterparts. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 2. Benchmark performance of DeepSeek-V3 and its counterparts. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><p>本文将深入探讨 DeepSeek-V2 和 DeepSeek-V3 的关键技术，包括其创新的模型架构、高效的训练基础设施、预训练和对齐过程。</p><h2 id=符号表>符号表<a hidden class=anchor aria-hidden=true href=#符号表>#</a></h2><p>下面列举了文章所使用的数学公式，可以帮你更轻松阅读。</p><table><thead><tr><th style=text-align:left>符号</th><th style=text-align:left>含义</th></tr></thead><tbody><tr><td style=text-align:left>\( d \)</td><td style=text-align:left>嵌入维度</td></tr><tr><td style=text-align:left>\( n_h \)</td><td style=text-align:left>注意力头数量</td></tr><tr><td style=text-align:left>\( d_h \)</td><td style=text-align:left>每个注意力头的维度</td></tr><tr><td style=text-align:left>\( \mathbf{h}_t \in \mathbb{R}^d \)</td><td style=text-align:left>第 \( t \) 个 token 在注意力层的输入</td></tr><tr><td style=text-align:left>\( \mathbf{q}_t, \mathbf{k}_t, \mathbf{v}_t \)</td><td style=text-align:left>查询, 键, 值向量</td></tr><tr><td style=text-align:left>\( W^Q, W^K, W^V, W^O \)</td><td style=text-align:left>查询、键、值、输出的投影矩阵</td></tr><tr><td style=text-align:left>\( \mathbf{q}_{t,i}, \mathbf{k}_{t,i}, \mathbf{v}_{t,i} \)</td><td style=text-align:left>第 \( i \) 个注意力头的查询、键、值向量</td></tr><tr><td style=text-align:left>\( \mathbf{o}_{t,i} \)</td><td style=text-align:left>第 \( i \) 个注意力头的输出</td></tr><tr><td style=text-align:left>\( \mathbf{u}_t \)</td><td style=text-align:left>注意力层的最终输出</td></tr><tr><td style=text-align:left>\( l \)</td><td style=text-align:left>模型层数</td></tr><tr><td style=text-align:left>\( \mathbf{c}_t^{KV} \in \mathbb{R}^{d_c} \)</td><td style=text-align:left>键值的压缩隐向量</td></tr><tr><td style=text-align:left>\( d_c \)</td><td style=text-align:left>KV 压缩维度</td></tr><tr><td style=text-align:left>\( W^{DKV}, W^{UK}, W^{UV} \)</td><td style=text-align:left>KV 的下投影、键的上投影、值的上投影矩阵</td></tr><tr><td style=text-align:left>\( \mathbf{k}_t^C, \mathbf{v}_t^C \)</td><td style=text-align:left>通过上投影从隐向量恢复的键和值</td></tr><tr><td style=text-align:left>\( \mathbf{c}_t^Q \in \mathbb{R}^{d_c'} \)</td><td style=text-align:left>查询的压缩隐向量</td></tr><tr><td style=text-align:left>\( d_c' \)</td><td style=text-align:left>查询压缩维度</td></tr><tr><td style=text-align:left>\( W^{DQ}, W^{UQ} \)</td><td style=text-align:left>查询的下投影、上投影矩阵</td></tr><tr><td style=text-align:left>\( \mathbf{q}_t^C \)</td><td style=text-align:left>通过上投影从隐向量恢复的查询</td></tr><tr><td style=text-align:left>\( \mathbf{q}_{t,i}^R, \mathbf{k}_t^R \)</td><td style=text-align:left>解耦的 RoPE 查询和键</td></tr><tr><td style=text-align:left>\( d_h^R \)</td><td style=text-align:left>解耦 RoPE 查询/键的头维度</td></tr><tr><td style=text-align:left>\( W^{QR}, W^{KR} \)</td><td style=text-align:left>解耦 RoPE 查询/键的生成矩阵</td></tr><tr><td style=text-align:left>\( \operatorname{RoPE}(\cdot) \)</td><td style=text-align:left>应用旋转位置编码的操作</td></tr><tr><td style=text-align:left>\( [\cdot ; \cdot] \)</td><td style=text-align:left>拼接操作</td></tr><tr><td style=text-align:left>\( n_g \)</td><td style=text-align:left>GQA 中的组数</td></tr><tr><td style=text-align:left>\( n \)</td><td style=text-align:left>MoE 中的专家总数</td></tr><tr><td style=text-align:left>\( E_i \)</td><td style=text-align:left>第 \( i \) 个专家网络</td></tr><tr><td style=text-align:left>\( G(\cdot) \)</td><td style=text-align:left>门控网络函数</td></tr><tr><td style=text-align:left>\( p_i \)</td><td style=text-align:left>门控网络输出的第 \( i \) 个概率</td></tr><tr><td style=text-align:left>\( H^{(i)}(x) \)</td><td style=text-align:left>Noisy Top-k Gating 中专家 \( i \) 的门控分数</td></tr><tr><td style=text-align:left>\( W_g, W_{\text{noise}} \)</td><td style=text-align:left>MoE 门控网络和噪声网络的权重矩阵</td></tr><tr><td style=text-align:left>\( \epsilon \)</td><td style=text-align:left>标准高斯噪声</td></tr><tr><td style=text-align:left>\( \text{softplus}(\cdot) \)</td><td style=text-align:left>Softplus 激活函数</td></tr><tr><td style=text-align:left>\( k \)</td><td style=text-align:left>MoE 中每个 token 选择的专家数量</td></tr><tr><td style=text-align:left>\( \text{topk}(\cdot, k) \)</td><td style=text-align:left>选择前 k 个最大值的函数</td></tr><tr><td style=text-align:left>\( \mathcal{L}_{\text{aux}} \)</td><td style=text-align:left>MoE 辅助损失</td></tr><tr><td style=text-align:left>\( w_{\text{aux}} \)</td><td style=text-align:left>辅助损失权重</td></tr><tr><td style=text-align:left>\( \text{CV}(\cdot) \)</td><td style=text-align:left>变异系数</td></tr><tr><td style=text-align:left>\( N_s, N_r \)</td><td style=text-align:left>DeepSeekMoE 中共享专家和路由专家的数量</td></tr><tr><td style=text-align:left>\( \operatorname{FFN}_i^{(s)}(\cdot), \operatorname{FFN}_i^{(r)}(\cdot) \)</td><td style=text-align:left>第 \( i \) 个共享专家和路由专家函数</td></tr><tr><td style=text-align:left>\( K_r \)</td><td style=text-align:left>DeepSeekMoE 中激活的路由专家数量</td></tr><tr><td style=text-align:left>\( g_{i,t} \)</td><td style=text-align:left>第 \( i \) 个专家对第 \( t \) 个 token 的门控值</td></tr><tr><td style=text-align:left>\( g_{i,t}' \)</td><td style=text-align:left>TopK 选择后的原始门控值（V3）</td></tr><tr><td style=text-align:left>\( s_{i,t} \)</td><td style=text-align:left>第 \( t \) 个 token 对第 \( i \) 个专家的亲和度分数</td></tr><tr><td style=text-align:left>\( \mathbf{e}_i \)</td><td style=text-align:left>第 \( i \) 个路由专家的中心向量</td></tr><tr><td style=text-align:left>\( M \)</td><td style=text-align:left>设备/节点限制路由中的设备/节点数上限</td></tr><tr><td style=text-align:left>\( \mathcal{L}_{\text{ExpBal}}, \mathcal{L}_{\text{DevBal}}, \mathcal{L}_{\text{CommBal}} \)</td><td style=text-align:left>专家级、设备级、通信级负载均衡损失</td></tr><tr><td style=text-align:left>\( f_i, P_i \)</td><td style=text-align:left>专家 \( i \) 的负载分数和平均亲和度</td></tr><tr><td style=text-align:left>\( \alpha_1, \alpha_2, \alpha_3 \)</td><td style=text-align:left>负载均衡损失的超参数</td></tr><tr><td style=text-align:left>\( T \)</td><td style=text-align:left>序列中的 token 数量</td></tr><tr><td style=text-align:left>\( D \)</td><td style=text-align:left>设备/节点组的数量</td></tr><tr><td style=text-align:left>\( \mathcal{E}_i \)</td><td style=text-align:left>第 \( i \) 个设备/节点上的专家组</td></tr><tr><td style=text-align:left>\( f_i', P_i' \)</td><td style=text-align:left>设备组 \( i \) 的平均负载分数和总亲和度</td></tr><tr><td style=text-align:left>\( f_i'', P_i'' \)</td><td style=text-align:left>发送到设备 \( i \) 的 token 比例和设备组 \( i \) 的总亲和度</td></tr><tr><td style=text-align:left>\( b_i \)</td><td style=text-align:left>第 \( i \) 个专家的偏置项 (aux-loss-free balancing)</td></tr><tr><td style=text-align:left>\( \gamma \)</td><td style=text-align:left>偏置项更新速度</td></tr><tr><td style=text-align:left>\( \mathcal{L}_{\text{Bal}} \)</td><td style=text-align:left>序列级负载均衡损失</td></tr><tr><td style=text-align:left>\( \alpha \)</td><td style=text-align:left>序列级负载均衡损失的超参数</td></tr><tr><td style=text-align:left>\( D_{MTP} \)</td><td style=text-align:left>MTP 预测深度</td></tr><tr><td style=text-align:left>\( \operatorname{Emb}(\cdot), \operatorname{OutHead}(\cdot) \)</td><td style=text-align:left>共享嵌入层和输出头 (MTP)</td></tr><tr><td style=text-align:left>\( \operatorname{TRM}_k(\cdot) \)</td><td style=text-align:left>第 \( k \) 个 MTP 模块的 Transformer 块</td></tr><tr><td style=text-align:left>\( M_k \)</td><td style=text-align:left>第 \( k \) 个 MTP 模块的投影矩阵</td></tr><tr><td style=text-align:left>\( \mathbf{h}_i^k \)</td><td style=text-align:left>第 \( i \) 个 token 在第 \( k \) 个 MTP 深度的表示</td></tr><tr><td style=text-align:left>\( \mathbf{h}_i^{\prime k} \)</td><td style=text-align:left>第 \( k \) 个 MTP 模块的 Transformer 块的输入</td></tr><tr><td style=text-align:left>\( P_{i+k+1}^k \)</td><td style=text-align:left>第 \( k \) 个 MTP 模块对第 \( i+k+1 \) 个 token 的预测概率分布</td></tr><tr><td style=text-align:left>\( V \)</td><td style=text-align:left>词汇表大小</td></tr><tr><td style=text-align:left>\( \mathcal{L}_{\text{MTP}}^k \)</td><td style=text-align:left>第 \( k \) 个 MTP 深度的交叉熵损失</td></tr><tr><td style=text-align:left>\( \mathcal{L}_{\text{MTP}} \)</td><td style=text-align:left>总 MTP 损失</td></tr><tr><td style=text-align:left>\( \lambda \)</td><td style=text-align:left>MTP 损失的权重因子</td></tr><tr><td style=text-align:left>\( \mathcal{J}_{GRPO}(\theta) \)</td><td style=text-align:left>GRPO 目标函数</td></tr><tr><td style=text-align:left>\( A_i \)</td><td style=text-align:left>相对优势值 (GRPO)</td></tr><tr><td style=text-align:left>\( \varepsilon \)</td><td style=text-align:left>PPO/GRPO 中的裁剪超参数</td></tr><tr><td style=text-align:left>\( \beta \)</td><td style=text-align:left>KL 散度惩罚项系数</td></tr><tr><td style=text-align:left>\( \mathbb{D}_{KL}(\pi_\theta \| \pi_{ref}) \)</td><td style=text-align:left>KL 散度</td></tr><tr><td style=text-align:left>\( \pi_\theta, \pi_{\theta_{old}}, \pi_{ref} \)</td><td style=text-align:left>当前策略、旧策略、参考策略模型</td></tr><tr><td style=text-align:left>\( r_i \)</td><td style=text-align:left>第 \( i \) 个输出的奖励值</td></tr><tr><td style=text-align:left>\( \mathbb{1}(\cdot) \)</td><td style=text-align:left>指示函数</td></tr></tbody></table><h2 id=核心架构>核心架构<a hidden class=anchor aria-hidden=true href=#核心架构>#</a></h2><p>DeepSeek-V2 和 V3 均基于 Transformer 架构，但在注意力和前馈网络（FFN）部分采用了创新设计比如 MLA 和 DeepseekMoE，以平衡性能、训练成本和推理效率。下图展示了 DeepSeek-V2 和 V3 的架构。</p><figure class=align-center><img loading=lazy src=deepseek_architecture.png#center alt="Fig. 3. Illustration of the architecture of DeepSeek-V2 and DeepSeek-V3. MLA ensures efficient inference by significantly reducing the KV cache for generation, and DeepSeekMoE enables training strong models at an economical cost through the sparse architecture. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 3. Illustration of the architecture of DeepSeek-V2 and DeepSeek-V3. MLA ensures efficient inference by significantly reducing the KV cache for generation, and DeepSeekMoE enables training strong models at an economical cost through the sparse architecture. (Image source: <a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h3 id=多头隐注意力-mla>多头隐注意力 (MLA)<a hidden class=anchor aria-hidden=true href=#多头隐注意力-mla>#</a></h3><p>传统的 Transformer 模型通常采用<strong>多头注意力（Multi-Head Attention, MHA）</strong>(<a href=https://arxiv.org/abs/1706.03762>Vaswani et al., 2017</a>)，但在生成过程中，其庞大的 KV cache 成为限制推理效率的瓶颈。为了解决这个问题，研究者提出了<strong>多查询注意力（Multi-Query Attention, MQA）</strong>(<a href=https://arxiv.org/abs/1911.02150>Shazeer, 2019</a>) 和<strong>分组查询注意力（Grouped-Query Attention, GQA）</strong>(<a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>)。这些方法虽然减少了 KV 缓存，但往往以牺牲模型性能为代价。</p><p>DeepSeek-V2 和 V3 采用了创新的 <strong>多头隐注意力（Multi-head Latent Attention, MLA）</strong> 机制。MLA 的核心思想是 <strong>低秩键值联合压缩 (Low-Rank Key-Value Joint Compression)</strong>。</p><figure class=align-center><img loading=lazy src=mla.png#center alt="Fig. 4. Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 4. Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. (Image source: <a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h4 id=mha-回顾>MHA 回顾<a hidden class=anchor aria-hidden=true href=#mha-回顾>#</a></h4><p>标准 MHA 首先通过三个投影矩阵 \(W^Q, W^K, W^V \in \mathbb{R}^{d_h n_h \times d}\) 将输入 \(\mathbf{h}_t \in \mathbb{R}^d\) 转换为查询 \(\mathbf{q}_t\)、键 \(\mathbf{k}_t\) 和值 \(\mathbf{v}_t \in \mathbb{R}^{d_h n_h}\)：</p>\[
\begin{aligned}
\mathbf{q}_{t} &= W^{Q} \mathbf{h}_{t}, \\
\mathbf{k}_{t} &= W^{K} \mathbf{h}_{t}, \\
\mathbf{v}_{t} &= W^{V} \mathbf{h}_{t}.
\end{aligned}
\]<p>然后将 \(\mathbf{q}_t, \mathbf{k}_t, \mathbf{v}_t\) 切分为 \(n_h\) 个头，进行多头注意力计算：</p>\[
\begin{aligned}
& [\mathbf{q}_{t, 1} ; \mathbf{q}_{t, 2} ; \ldots ; \mathbf{q}_{t, n_{h}}] = \mathbf{q}_{t}, \\
& [\mathbf{k}_{t, 1} ; \mathbf{k}_{t, 2} ; \ldots ; \mathbf{k}_{t, n_{h}}] = \mathbf{k}_{t}, \\
& [\mathbf{v}_{t, 1} ; \mathbf{v}_{t, 2} ; \ldots ; \mathbf{v}_{t, n_{h}}] = \mathbf{v}_{t}, \\
& \mathbf{o}_{t, i} = \sum_{j=1}^{t} \operatorname{Softmax}_{j}\left(\frac{\mathbf{q}_{t, i}^{T} \mathbf{k}_{j, i}}{\sqrt{d_{h}}}\right) \mathbf{v}_{j, i}, \\
& \mathbf{u}_{t} = W^{O}\left[\mathbf{o}_{t, 1} ; \mathbf{o}_{t, 2} ; \ldots ; \mathbf{o}_{t, n_{h}}\right],
\end{aligned}
\]<p>其中 \(\mathbf{q}_{t, i}, \mathbf{k}_{t, i}, \mathbf{v}_{t, i} \in \mathbb{R}^{d_h}\) 分别是第 \(i\) 个头的查询、键、值，\(W^O \in \mathbb{R}^{d \times d_h n_h}\) 是输出投影矩阵。推理时，需要缓存所有 \(t\) 步的键和值，每个 token 需要缓存 \(2 n_h d_h l\) 个元素（\(l\) 为层数），这构成了巨大的 KV 缓存开销。</p><h4 id=低秩键值联合压缩>低秩键值联合压缩<a hidden class=anchor aria-hidden=true href=#低秩键值联合压缩>#</a></h4><p>MLA 通过引入一个低维的隐向量 \(\mathbf{c}_t^{KV} \in \mathbb{R}^{d_c}\) 来联合压缩键和值，其中 \(d_c \ll d_h n_h\)：</p>\[
\begin{aligned}
\boxed{\mathbf{c}_{t}^{K V}} &= W^{D K V} \mathbf{h}_{t}, \\
\mathbf{k}_{t}^{C} &= W^{U K} \mathbf{c}_{t}^{K V}, \\
\mathbf{v}_{t}^{C} &= W^{U V} \mathbf{c}_{t}^{K V}.
\end{aligned}
\]<p>这里 \(W^{DKV} \in \mathbb{R}^{d_c \times d}\) 是下投影矩阵，\(W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}\) 分别是键和值的上投影矩阵。推理时，MLA <strong>只需要缓存压缩后的隐向量 \(\mathbf{c}_t^{KV}\)</strong>（以及后面提到的解耦 RoPE 键 \(\mathbf{k}_t^R\)），极大地减少了 KV 缓存量。</p><p>为了减少训练时的激活内存，MLA 也对查询进行了类似的低秩压缩：</p>\[
\begin{aligned}
\mathbf{c}_{t}^{Q} &= W^{D Q} \mathbf{h}_{t}, \\
\mathbf{q}_{t}^{C} &= W^{U Q} \mathbf{c}_{t}^{Q},
\end{aligned}
\]<p>其中 \(\mathbf{c}_t^Q \in \mathbb{R}^{d_c'}\) 是查询的压缩隐向量，\(d_c' \ll d_h n_h\)，\(W^{DQ} \in \mathbb{R}^{d_c' \times d}\) 和 \(W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c'}\) 分别是查询的下投影和上投影矩阵。</p><h4 id=解耦旋转位置编码>解耦旋转位置编码<a hidden class=anchor aria-hidden=true href=#解耦旋转位置编码>#</a></h4><p>标准的<strong>旋转位置编码（RoPE）</strong>(<a href=https://arxiv.org/abs/2104.09864>Su et al., 2024</a>) 直接应用于键和查询，但这与 MLA 的低秩 KV 压缩不兼容。如果在压缩后的键 \(\mathbf{k}_t^C\) 上应用 RoPE，那么上投影矩阵 \(W^{UK}\) 会与位置相关的 RoPE 矩阵耦合，导致推理时无法将其吸收到 \(W^Q\) 中，必须重新计算所有前缀 token 的键，严重影响效率。</p><p>为了解决这个问题，MLA 提出了<strong>解耦 RoPE (Decoupled RoPE)</strong> 策略。它引入了额外的多头查询 \(\mathbf{q}_{t, i}^R \in \mathbb{R}^{d_h^R}\) 和一个共享的键 \(\mathbf{k}_t^R \in \mathbb{R}^{d_h^R}\) 来专门承载 RoPE 信息：</p>\[
\begin{aligned}
\left[\mathbf{q}_{t,1}^R;\,\mathbf{q}_{t,2}^R;\,\dots;\,\mathbf{q}_{t,n_h}^R\right]
= \mathbf{q}_t^R
&= \operatorname{RoPE}\bigl(W^{Q R}\,\mathbf{c}_t^Q\bigr),\\
\boxed{\mathbf{k}_t^R}
&= \operatorname{RoPE}\bigl(W^{K R}\,\mathbf{h}_t\bigr).
\end{aligned}
\]<p>这里 \(W^{QR} \in \mathbb{R}^{d_h^R n_h \times d_c'}\) 和 \(W^{KR} \in \mathbb{R}^{d_h^R \times d}\) 是生成解耦查询和键的矩阵。
然后将压缩得到的键/查询部分 (\(C\)) 与解耦的 RoPE 部分 (\(R\)) 拼接起来形成最终的键和查询：</p>\[
\begin{aligned}
\mathbf{q}_{t, i} &= [\mathbf{q}_{t, i}^{C} ; \mathbf{q}_{t, i}^{R}], \\
\mathbf{k}_{t, i} &= [\mathbf{k}_{t, i}^{C} ; \mathbf{k}_{t}^{R}].
\end{aligned}
\]<p>最终的注意力计算变为：</p>\[
\begin{aligned}
\mathbf{o}_{t, i} &= \sum_{j=1}^{t} \operatorname{Softmax}_{j}\left(\frac{\mathbf{q}_{t, i}^{T} \mathbf{k}_{j, i}}{\sqrt{d_{h}+d_{h}^{R}}}\right) \mathbf{v}_{j, i}^{C}, \\
\mathbf{u}_{t} &= W^{O}\left[\mathbf{o}_{t, 1} ; \mathbf{o}_{t, 2} ; \ldots ; \mathbf{o}_{t, n_{h}}\right].
\end{aligned}
\]<p>推理时，除了缓存 \(\mathbf{c}_t^{KV}\)，还需要缓存解耦的 RoPE 键 \(\mathbf{k}_t^R\)。因此，DeepSeek-V2/V3 每个 token 总共需要缓存 \((d_c + d_h^R)l\) 个元素。</p><h4 id=mla-推理中的矩阵吸收>MLA 推理中的矩阵吸收<a hidden class=anchor aria-hidden=true href=#mla-推理中的矩阵吸收>#</a></h4><p>MLA 的一个关键优势在于推理效率的提升，这部分得益于矩阵乘法结合律允许将上投影矩阵 \(W^{UK}\) 和 \(W^{UV}\) “吸收”掉，避免显式计算完整的键 \(\mathbf{k}_t^C\) 和值 \(\mathbf{v}_t^C\)。</p><p><strong>1. 吸收 \(W^{UK}\) (优化注意力分数计算):</strong></p><p>注意力分数计算的核心是查询和键的点积 \(\mathbf{q}_{t,i}^T \mathbf{k}_{j,i}\)。关注其中由压缩向量生成的 \(C\) 部分的点积：</p>\[
(\mathbf{q}_{t,i}^C)^T \mathbf{k}_{j,i}^C
\]<p>将 \(\mathbf{k}_{j,i}^C = W^{UK} \mathbf{c}_j^{KV}\) 代入：</p>\[
(\mathbf{q}_{t,i}^C)^T (W^{UK} \mathbf{c}_j^{KV})
\]<p>根据矩阵乘法结合律 \((AB)C = A(BC)\) 和转置性质 \((AB)^T = B^T A^T\)，可以将上式改写为：</p>\[
(\mathbf{q}_{t,i}^C)^T (W^{UK} \mathbf{c}_j^{KV}) = ((W^{UK})^T \mathbf{q}_{t,i}^C)^T \mathbf{c}_j^{KV}
\]<p>这个变换的意义在于：不再需要用 \(W^{UK}\) 作用于缓存的 \(\mathbf{c}_j^{KV}\) 来得到 \(\mathbf{k}_{j,i}^C\)。相反，可以先计算一个“有效查询” \(\tilde{\mathbf{q}}_{t,i}^C = (W^{UK})^T \mathbf{q}_{t,i}^C\)，然后直接用这个有效查询与缓存的隐向量 \(\mathbf{c}_j^{KV}\) 进行点积。</p><p>原始查询 \(\mathbf{q}_{t,i}^C\) 是通过 \(W^{UQ}\) 和 \(W^{DQ}\) 从 \(\mathbf{h}_t\) 计算得到的 (\(\mathbf{q}_{t,i}^C = (W^{UQ} W^{DQ} \mathbf{h}_t)_i\))。因此，从 \(\mathbf{h}_t\) 到有效查询 \(\tilde{\mathbf{q}}_{t,i}^C\) 的整个计算过程可以看作是一个新的、合并了 \(W^{UK}\) 的有效查询投影操作。在实际实现中，这意味着计算 \(\mathbf{q}_{t,i}^C\) 后，可以再左乘 \((W^{UK})^T\)，或者更高效地，将 \((W^{UK})^T\) 合并到生成查询的原始矩阵 \(W^Q\)（或 \(W^{UQ}W^{DQ}\)）中，形成一个新的查询投影矩阵 \(\tilde{W}^Q = (W^{UK})^T W^{UQ} W^{DQ}\)。</p><p>关键在于，涉及 \(W^{UK}\) 的计算被移到了查询侧，在计算注意力分数之前一次性完成，而无需在每次查询时都用 \(W^{UK}\) 从缓存的 \(\mathbf{c}_j^{KV}\) 中恢复 \(\mathbf{k}_{j,i}^C\)。</p><p><strong>2. 吸收 \(W^{UV}\) (优化加权求和):</strong></p><p>注意力头的输出 \(\mathbf{o}_{t,i}\) 是注意力权重 (记作 \(w_{ij}\)) 与值 \(\mathbf{v}_{j,i}^C\) 的加权和：</p>\[
\mathbf{o}_{t, i} = \sum_{j=1}^{t} w_{ij} \cdot \mathbf{v}_{j, i}^{C}
\]<p>将 \(\mathbf{v}_{j,i}^C = (W^{UV} \mathbf{c}_j^{KV})_i\) 代入（这里 \(( \cdot )_i\) 表示属于第 \(i\) 个头的部分）：</p>\[
\mathbf{o}_{t, i} = \sum_{j=1}^{t} w_{ij} \cdot (W^{UV} \mathbf{c}_j^{KV})_i
\]<p>最终的注意力层输出 \(\mathbf{u}_t\) 是所有头的输出 \(\mathbf{o}_{t,i}\) 拼接后通过输出矩阵 \(W^O\) 投影得到的：</p>\[
\mathbf{u}_{t} = W^{O}\left[\mathbf{o}_{t, 1} ; \ldots ; \mathbf{o}_{t, n_{h}}\right] = W^{O} \begin{bmatrix} \sum_{j} w_{1j} (W^{UV} \mathbf{c}_j^{KV})_1 \\ \vdots \\ \sum_{j} w_{n_h j} (W^{UV} \mathbf{c}_j^{KV})_{n_h} \end{bmatrix}
\]<p>由于矩阵乘法的线性性质（\(A(B+C) = AB + AC\) 以及 \(A(cB) = c(AB)\)），可以将 \(W^{UV}\) 从求和中“提出”（这里是为了直观理解，实际操作是矩阵层面的）：</p>\[
\mathbf{u}_{t} \approx W^{O} W^{UV} \left( \sum_{j=1}^{t} \begin{bmatrix} w_{1j} (\mathbf{c}_j^{KV})_1 \\ \vdots \\ w_{n_h j} (\mathbf{c}_j^{KV})_{n_h} \end{bmatrix} \right)
\]<p>（注意：这里的 \((\mathbf{c}_j^{KV})_i\) 只是示意，实际计算中是直接对完整的 \(\mathbf{c}_j^{KV}\) 操作，但原理相同，即先对 \(\mathbf{c}_j^{KV}\) 进行加权求和，再应用 \(W^{UV}\) 和 \(W^O\)）。</p><p>令有效输出矩阵 \(\tilde{W}^O = W^O W^{UV}\)。这意味着可以先计算注意力权重与隐向量 \(\mathbf{c}_j^{KV}\) 的加权和（得到一个维度为 \(d_c\) 的中间结果 \(\tilde{\mathbf{o}}_t = \sum_j w_{ij} \mathbf{c}_j^{KV}\)），然后直接用这个合并后的有效输出矩阵 \(\tilde{W}^O\) 进行最终投影得到 \(\mathbf{u}_t\)。同样，涉及 \(W^{UV}\) 的计算被合并到了最后的输出投影步骤，无需在计算加权和时从 \(\mathbf{c}_j^{KV}\) 恢复 \(\mathbf{v}_{j,i}^C\)。</p><p><strong>总结:</strong> 通过矩阵吸收，MLA 在推理时避免了从缓存的低维隐向量 \(\mathbf{c}_j^{KV}\) 重复计算高维的键 \(\mathbf{k}_{j,i}^C\) 和值 \(\mathbf{v}_{j,i}^C\)，显著提高了计算效率。实际缓存的只有 \(\mathbf{c}_t^{KV}\) 和 \(\mathbf{k}_t^R\)。</p><h4 id=kv-缓存对比>KV 缓存对比<a hidden class=anchor aria-hidden=true href=#kv-缓存对比>#</a></h4><p>下表比较了不同注意力机制的每 Token KV 缓存对比。\(n_h\) 是注意力头数，\(d_h\) 是每头维度，\(l\) 是层数，\(n_g\) 是 GQA 组数，\(d_c\) 和 \(d_h^R\) 是 MLA 的 KV 压缩维度和解耦 RoPE 维度。对于 DeepSeek-V2，\(d_c = 4d_h\)，\(d_h^R = d_h/2\)，其 KV 缓存相当于 \(n_g=2.25\) 的 GQA，但性能优于 MHA。DeepSeek-V3 沿用了类似配置。</p><table><thead><tr><th style=text-align:left>注意力机制</th><th style=text-align:center>每 Token KV 缓存大小 (# 元素)</th><th style=text-align:center>能力</th></tr></thead><tbody><tr><td style=text-align:left>Multi-Head Attention (MHA)</td><td style=text-align:center>\(2 n_{h} d_{h} l\)</td><td style=text-align:center>强</td></tr><tr><td style=text-align:left>Grouped-Query Attention (GQA)</td><td style=text-align:center>\(2 n_{g} d_{h} l\)</td><td style=text-align:center>中等</td></tr><tr><td style=text-align:left>Multi-Query Attention (MQA)</td><td style=text-align:center>\(2 d_{h} l\)</td><td style=text-align:center>弱</td></tr><tr><td style=text-align:left>Multi-head Latent Attention (MLA)</td><td style=text-align:center>\(\bigl(d_{c} + d_{h}^{R}\bigr) l \approx \tfrac{9}{2} \, d_{h} \, l\)</td><td style=text-align:center>更强</td></tr></tbody></table><p>下图表明，MLA 不仅显著减少了 KV 缓存，其性能甚至优于标准的 MHA。</p><figure class=align-center><img loading=lazy src=mla_vs_mha.png#center alt="Fig. 5. Comparison between MLA and MHA on hard benchmarks. DeepSeek-V2 shows better performance than MHA, but requires a significantly smaller amount of KV cache. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 5. Comparison between MLA and MHA on hard benchmarks. DeepSeek-V2 shows better performance than MHA, but requires a significantly smaller amount of KV cache. (Image source: <a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h3 id=混合专家模型>混合专家模型<a hidden class=anchor aria-hidden=true href=#混合专家模型>#</a></h3><p>在深入 DeepSeekMoE 之前，先回顾一下混合专家模型（MoE）的基础知识。</p><p><strong>混合专家模型(Mixture-of-Experts, MoE)</strong>(<a href=https://arxiv.org/abs/1701.06538>Shazeer et al. 2017</a>) 是一种稀疏激活模型，它通过结合多个独立的“专家”网络和一个门控网络(Gating Network)，在不显著增加计算成本的前提下，大幅提升了模型的参数量和性能。MoE 的核心思想是<strong>稀疏激活(Sparse Activation)</strong>，即对于每个输入样本，仅激活部分专家网络，而不是整个模型。这种方法既提高了计算效率，又增强了模型的表达能力，使其在 LLMs 中表现出色。</p><p>MoE 设计灵感来源于<a href=https://en.wikipedia.org/wiki/Ensemble_learning>集成学习(Ensemble learning)</a>, 一种将复杂任务分解为多个子任务并由不同模型协作完成的技术。在 MoE 中，这些“子任务”由多个独立的专家网络处理，而门控网络则负责根据输入样本的特征动态选择最适合的专家。这种分工合作的机制类似于人类社会中的专家团队：不同领域的专家针对特定问题提供专业意见，最终综合得出结果。</p><figure class=align-center><img loading=lazy src=moe.png#center alt="Fig. 6. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)" width=100%><figcaption><p>Fig. 6. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: <a href=https://arxiv.org/abs/1701.06538>Shazeer et al. 2017</a>)</p></figcaption></figure><h3 id=moe-核心组件>MoE 核心组件<a hidden class=anchor aria-hidden=true href=#moe-核心组件>#</a></h3><p>一个典型的 MoE 包含以下组件：</p><ul><li><strong>专家网络(Experts):</strong> 一组独立的神经网络 ${E_1, E_2, &mldr;, E_n}$，每个专家网络 $E_i$ 可以是任意类型的神经网络，例如 FFN, CNN, RNN 等。专家网络的数量 $n$ 可以很大，例如几十个、几百个甚至几千个。</li><li><strong>门控网络(Gating Network):</strong> 一个可训练的神经网络 $G$，用于根据输入样本 $x$ 学习一个概率分布，决定激活哪些专家。门控网络的输入是输入样本 $x$，输出是一个 $n$ 维的概率向量 $p = G(x) = [p_1, p_2, &mldr;, p_n]$，其中 $p_i$ 表示激活专家 $E_i$ 的概率。</li><li><strong>专家输出聚合(Expert Output Aggregation):</strong> 根据门控网络的输出概率分布，将激活的专家网络的输出进行加权求和，得到 MoE 层的最终输出 $y$。</li></ul><h3 id=noisy-top-k-gating>Noisy Top-k Gating<a hidden class=anchor aria-hidden=true href=#noisy-top-k-gating>#</a></h3><p>为了实现稀疏激活并确保专家使用均衡，MoE 通常采用 <strong>Noisy Top-k Gating</strong> 作为门控机制。这种方法通过引入噪声和 top-k 选择，既保证了计算效率，又避免了专家负载不均的问题。以下是其详细工作流程：</p><ol><li><strong>门控分数计算:</strong></li></ol><p>对于输入样本 $x$，门控网络首先计算每个专家的门控分数 $H^{(i)}(x)$。这一分数包含两部分：线性变换和噪声项，公式如下：</p>$$
H^{(i)}(x) =(x W_g)^{(i)} + \epsilon \cdot \text{softplus}\left((x W_{\text{noise}})^{(i)} \right), \quad \epsilon \sim \mathcal{N}(0, 1)
$$<ul><li><strong>参数说明</strong>：<ul><li>$W_g \in \mathbb{R}^{d \times n}$：门控网络的可训练权重矩阵，$d$ 是输入特征维度，$n$ 是专家数量。</li><li>$W_{\text{noise}} \in \mathbb{R}^{d \times n}$：用于生成噪声的权重矩阵。</li><li>$\epsilon \sim \mathcal{N}(0, 1)$：标准高斯噪声，增加门控随机性。</li><li>$\text{softplus}(x) = \log(1 + e^x)$：平滑激活函数，确保噪声非负。</li></ul></li></ul><p>噪声的引入避免了门控网络总是选择固定的专家，增强了模型的鲁棒性和多样性。</p><ol start=2><li><strong>Top-k 选择:</strong></li></ol><p>计算出门控分数向量 $H(x) = [H^{(1)}(x), H^{(2)}(x), \dots, H^{(n)}(x)]$ 后，门控网络选择其中值最大的前 $k$ 个专家(通常 $k \ll n$)。这一步骤通过 $\text{topk}(v, k)$ 函数实现：</p>$$
\text{topk}^{(i)}(v, k) =
\begin{cases}
v^{(i)} & \text{if } v^{(i)} \text{ is in the top } k \text{ elements of } v \\
-\infty & \text{otherwise}
\end{cases}
$$<p>将非 Top-k 专家的分数设为 $-\infty$，确保后续 softmax 操作中这些专家的概率为 0，实现稀疏性。</p><ol start=3><li><strong>Softmax 归一化:</strong></li></ol><p>对 Top-k 专家的门控分数进行 softmax 归一化，得到稀疏的概率分布 $G(x)$：</p>$$
G(x) = \text{softmax}\left( \text{topk}(H(x), k) \right)
$$<p>只有 Top-k 个专家的概率非零，其余为 0。例如，若 $n=100, k=2$，则 98 个专家的概率为 0。</p><ol start=4><li><strong>加权求和:</strong></li></ol><p>将 Top-k 个专家的输出按概率加权求和，得到 MoE 层的输出：</p>$$
y = \sum_{i=1}^{n} G^{(i)}(x) E_i(x)
$$<p>由于只有 $k$ 个专家被激活，计算量远低于激活所有 $n$ 个专家。</p><h3 id=辅助损失>辅助损失<a hidden class=anchor aria-hidden=true href=#辅助损失>#</a></h3><p>为了<strong>避免门控网络过度偏向少数专家</strong>，MoE 引入了<strong>辅助损失(Auxiliary Loss)</strong>(<a href=https://arxiv.org/abs/1701.06538>Shazeer et al. 2017</a>)，鼓励所有专家被均匀使用。一种常用方法是基于专家使用率的<a href=https://en.wikipedia.org/wiki/Coefficient_of_variation>变异系数(Coefficient of Variation, CV)</a>的平方：</p>$$
\mathcal{L}_{\text{aux}} = w_{\text{aux}} \cdot \text{CV}\left( \sum_{x \in X} G(x) \right)^2
$$<ul><li><p><strong>参数说明</strong>：</p><ul><li>$X$：一个 mini-batch 的输入样本。</li><li>$\sum_{x \in X} G(x)$：统计每个专家在 mini-batch 中的激活次数。</li><li>$\text{CV}$：标准差与均值的比值，衡量专家使用分布的均匀性。</li><li>$w_{\text{aux}}$：辅助损失的权重，需手动调整。</li></ul></li><li><p><strong>作用</strong>：通过最小化 $\mathcal{L}_{\text{aux}}$，模型优化专家选择的均衡性，避免某些专家被过度使用而其他专家闲置。</p></li></ul><h3 id=gshard>GShard<a hidden class=anchor aria-hidden=true href=#gshard>#</a></h3><p><strong>GShard</strong>(<a href=https://arxiv.org/abs/2006.16668>Lepikhin et al. 2020</a>)主要对 MoE 层进行分片，将 MoE 层中的专家网络 ${E_1, E_2, &mldr;, E_n}$ 分散到多个 TPU 设备上。例如，如果有 $P$ 个 TPU 设备，可以将专家网络划分为 $P$ 组，每组专家网络分配到一个 TPU 设备上。Transformer 模型的其他层(例如自注意力层、LayerNorm 层) 则在所有 TPU 设备上复制。</p><p><strong>GShard 的改进门控机制:</strong></p><p>GShard 在 Noisy Top-k Gating 的基础上，进行了一些改进，以提高门控机制的性能和稳定性：</p><ul><li><p><strong>专家容量(Expert Capacity):</strong><br>为了避免专家过载，GShard 引入了专家容量限制。每个专家网络都有一个容量上限，表示它最多可以处理的 token 数量。如果一个 token 被路由到一个已经达到容量上限的专家网络，则该 token 会被标记为 &ldquo;overflowed&rdquo;，门控输出会被设置为零向量，表示该 token 不会被路由到任何专家网络。</p></li><li><p><strong>局部组分发(Local Group Dispatching):</strong><br>为了提高门控效率，GShard 将 token 分组，在组级别强制执行专家容量限制。例如，将 mini-batch 中的 token 划分为多个局部组，每个局部组包含一定数量的 token。门控网络为每个局部组选择 top-k 个专家网络，并确保每个专家网络在一个局部组内处理的 token 数量不超过其容量上限。</p></li><li><p><strong>辅助损失(Auxiliary Loss):</strong><br>GShard 也使用了辅助损失函数来平衡专家负载。与原始 MoE 模型的辅助损失不同，GShard 的辅助损失旨在最小化每个专家网络路由到的数据比例的均方误差，更加直接地衡量专家负载平衡程度。</p></li><li><p><strong>随机路由(Random Routing):</strong><br>为了增加路由的随机性，GShard 在选择 top-k 个专家网络时，引入了随机路由机制。除了选择最佳的 top-k 个专家网络外，GShard 还会以一定的概率随机选择次优的专家网络，增加专家网络的多样性，提高模型的泛化能力。</p></li></ul><p>下面是 GShard 的核心算法流程:</p><figure class=align-center><img loading=lazy src=gshard.png#center alt="Fig. 7. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)" width=100%><figcaption><p>Fig. 7. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: <a href=https://arxiv.org/abs/2006.16668>Lepikhin et al. 2020</a>)</p></figcaption></figure><h3 id=switch-transformer>Switch Transformer<a hidden class=anchor aria-hidden=true href=#switch-transformer>#</a></h3><p><strong>Switch Transformer</strong>(<a href=https://arxiv.org/pdf/2101.03961>Fedus et al. 2021</a>) 是 Google 提出的一个参数量达到<strong>万亿</strong>级别的 MoE 模型。其核心创新是将 Transformer 模型中的密集前馈网络(FFN) 层替换为稀疏的 Switch FFN 层。与 GShard 的 Top-2 Gating 不同，Switch Transformer 每个输入 token 只路由到一个专家网络，具有更高的稀疏性，进一步降低了计算成本，使得训练万亿参数模型成为可能。鼓励 token 路由在 $N$ 个专家之间更加均衡。Switch Transformer 的辅助损失基于实际路由比例与预测路由概率的乘积累加，具体公式如下：</p>$$
\text{loss} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i
$$<ul><li><strong>参数说明</strong>：<ul><li><p>$N$：专家的总数。</p></li><li><p>$f_i$：路由到第 $i$ 个专家的 token 比例，定义为：</p>$$
f_i = \frac{1}{T} \sum_{x \in B} 1\{\text{argmax } p(x) = i\}
$$</li><li><p>$P_i$：gating 网络预测的第 $i$ 个专家的路由概率，定义为：</p>$$
P_i = \frac{1}{T} \sum_{x \in B} p_i(x)
$$</li><li><p>$T$：批次 $B$ 中的 token 总数。</p></li><li><p>$\alpha$：辅助损失的权重超参数，通常设为 $10^{-2}$。</p></li></ul></li></ul><p>通过最小化 $\text{loss}$，模型使实际路由比例 $f_i$ 与预测概率 $P_i$ 趋于一致，从而间接促进专家间的负载平衡，避免部分专家闲置。</p><figure class=align-center><img loading=lazy src=switch_transformer.png#center alt="Fig. 8. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)" width=100%><figcaption><p>Fig. 8. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: <a href=https://arxiv.org/abs/2101.03961>Fedus et al. 2021</a>)</p></figcaption></figure><p><strong>Switch Router 机制:</strong></p><ol><li><p><strong>路由预测:</strong><br>对于输入 token $x$，Switch Router 预测每个专家网络的路由概率 $p_i = G^{(i)}(x)$，其中 $i = 1, 2, &mldr;, n$，n 是专家网络数量。</p></li><li><p><strong>专家选择:</strong><br>选择路由概率最高的专家网络作为最佳专家网络。Switch Transformer 采用 Top-1 路由策略，即每个 token 只路由到路由概率最高的专家网络。</p></li><li><p><strong>token 路由:</strong><br>将输入 token $x$ 路由到选择的最佳专家网络进行处理。</p></li></ol><p><strong>Switch Transformer 的训练稳定性优化:</strong></p><p>为提升 Switch Transformer 的训练稳定性，论文提出了如下优化策略：</p><ul><li><p><strong>选择性精度(Selective Precision)</strong><br>在路由函数内部采用 FP32 精度既能提高训练稳定性，又能避免因 FP32 张量通信而产生的额外开销。具体来说，Switch Router 的计算过程全程使用 FP32，最终结果再转换为 FP16 以兼顾效率与精度。</p></li><li><p><strong>更小初始化(Smaller Initialization)</strong><br>建议将 Transformer 的权重初始化尺度参数 $s$ 从 1 调整至 0.1。较小的初始化尺度有助于缓解训练初期的梯度爆炸风险，从而提升整体训练稳定性。具体实现为：从均值为 0、标准差为 $\sqrt{s/n}$(其中 $n$ 为输入单元数) 的截断正态分布中采样。</p></li><li><p><strong>更高专家 Dropout(Higher Expert Dropout)</strong><br>在专家 FFN 层中采用较高的 dropout 率(例如 0.4)，而在非专家层则保持较低的 dropout 率(例如 0.1)，这种设置能有效防止过拟合，进而增强模型的泛化能力。下图实验结果显示，在 GLUE、CNNDM、SQuAD 和 SuperGLUE 等任务上，当专家层 dropout 率设为 0.4 时，模型表现最佳。</p></li></ul><figure class=align-center><img loading=lazy src=switch_transformer_fine_tuning_result.png#center alt="Fig. 9. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: Fedus et al. 2021)" width=100%><figcaption><p>Fig. 9. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: <a href=https://arxiv.org/abs/2101.03961>Fedus et al. 2021</a>)</p></figcaption></figure><p>Switch Transformers 论文中使用下图直观的展示了使用不同的并行技术如何分割模型权重和数据:</p><figure class=align-center><img loading=lazy src=switch_transformer_parallelism.png#center alt="Fig. 10. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)" width=100%><figcaption><p>Fig. 10. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: <a href=https://arxiv.org/abs/2101.03961>Fedus et al. 2021</a>)</p></figcaption></figure><h3 id=专家选择>专家选择<a hidden class=anchor aria-hidden=true href=#专家选择>#</a></h3><p><strong>专家选择(Expert Choice, EC)</strong>(<a href=https://arxiv.org/abs/2202.09368>Zhou et al. 2022</a>) 是一种与 token 选择路由(如 GShard 的 top-2 或 Switch Transformer 的 top-1)相反的路由策略。在 token 选择路由中，每个 token 从所有专家中选择 top-k 个进行路由；而在专家选择路由中，每个专家从所有 token 中挑选 top-k 个进行处理。这种方法旨在解决 token 选择路由中的负载不均和 token 浪费问题，同时显著提高训练效率。下面是具体的计算过程：</p><ol><li><p><strong>计算 token-to-expert 亲和度分数</strong></p><p>对于输入矩阵 $X \in \mathbb{R}^{n \times d}$，计算 token-to-expert 亲和度分数矩阵 $S \in \mathbb{R}^{n \times e}$ 的过程为：</p>$$
S = \text{softmax}(X \cdot W_g), \quad \text{where } W_g \in \mathbb{R}^{d \times e}.
$$<p>这里，$W_g$ 为门控权重矩阵，$e$ 为专家数量。</p></li><li><p><strong>专家选择 token</strong></p><p>每个专家从所有 token 中选择 top-k 个进行处理。通过对 $S^T$ 进行 top-k 选择：</p>$$
G, I = \text{top-}k(S^T, k),
$$<p>得到：</p><ul><li><strong>门控矩阵 $G \in \mathbb{R}^{e \times k}$：</strong> 记录专家选择的 token 对应的路由权重，其中 $G[i, j]$ 表示专家 $i$ 选择的第 $j$ 个 token 的权重；</li><li><strong>token 索引矩阵 $I \in \mathbb{R}^{e \times k}$：</strong> 表示每个专家选择的 token 在输入中的索引。</li></ul></li><li><p><strong>One-hot 编码</strong></p><p>将 token 索引矩阵 $I$ 转换为 one-hot 编码矩阵 $P \in \mathbb{R}^{e \times k \times n}$，用于后续计算：</p>$$
P = \operatorname{one}-\operatorname{hot}(I)
$$</li><li><p><strong>构造 Gated FFN 层输入</strong></p><p>对于每个专家 $i$，其 gated FFN 层的输入为：</p>$$
(P \cdot X) \in \mathbb{R}^{e \times k \times d}.
$$</li></ol><p>EC 通过正则化限制每个 token 被路由到的专家数量，从而控制模型的稀疏性。一个常见的正则化目标如下：</p>$$
\begin{aligned}
& \max_{A} \langle S^{\top}, A \rangle + \lambda H(A) \\
& \text{s.t. } \forall i: \sum_{j'} A[i, j'] = k, \quad \forall j: \sum_{i'} A[i', j] \leq b, \quad \forall i,j: 0 \leq A[i, j] \leq 1,
\end{aligned}
$$<p>考虑的优化问题中定义了一个矩阵 $A$，其第 $i$ 行第 $j$ 列的元素表示第 $i$ 个专家是否选择了第 $j$ 个 token(取值 0 或 1)。由于该优化问题求解较为复杂，论文中采用 <a href=https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm>Dijkstra 算法</a>(通过多次迭代获得近似解)来解决。</p><p>参数 $b$ 通常由批量中 token 总数 $n$ 与容量因子决定，其中容量因子表示每个 token 平均使用的专家数量。大多数实验采用较高的容量因子，实验结果表明，即使在容量降低的情况下，EC 整体表现仍优于传统的 top-1 token 选择路由，尽管 capped expert choice 略微降低了微调性能。</p><p>EC 的优势主要体现在以下两方面：</p><ul><li><strong>完美负载均衡：</strong> 每个专家固定处理 $k$ 个 token，从而避免了部分专家过载而其他专家闲置的问题，实现了理想的负载均衡。</li><li><strong>更高训练效率：</strong> 实验表明，EC 能将训练收敛速度提升约 2 倍，相较于传统 token 选择路由具有更高的效率。</li></ul><p>但 EC 也存在以下局限性：</p><ul><li><strong>批量大小要求：</strong> 由于 EC 对 batch size 有较高要求，因此不适用于较小 batch size 的场景。</li><li><strong>自回归生成限制：</strong> 在自回归文本生成任务中，由于无法预知未来 token，EC 的 top-k 选择无法实现，因此不适用于此类任务。</li></ul><h3 id=deepseekmoe>DeepSeekMoE<a hidden class=anchor aria-hidden=true href=#deepseekmoe>#</a></h3><p>混合专家（MoE）模型通过将计算路由到特定的“专家”子网络来提高效率和性能。DeepSeek-V2 和 V3 在其 FFN（前馈网络）层中采用了名为 <strong>DeepSeekMoE</strong> (<a href=https://arxiv.org/abs/2401.06066>Dai et al., 2024</a>)架构 。与 GShard 等传统 MoE 架构相比，DeepSeekMoE 的核心思想是：</p><ol><li><strong>细粒度专家切分 (Fine-grained Expert Segmentation):</strong> 将专家网络切分得更小。这旨在实现更高的专家特化能力和更精确的知识获取，因为每个专家可以专注于更窄的领域。</li><li><strong>共享专家隔离 (Shared Expert Isolation):</strong> 架构中包含一部分“共享专家”，这些专家由所有 token 处理，旨在处理通用知识。这减少了需要路由的“路由专家”之间的知识冗余，让路由专家可以更专注于特定知识。</li></ol><h4 id=基本架构>基本架构<a hidden class=anchor aria-hidden=true href=#基本架构>#</a></h4><p>对于 FFN 层的输入 token 表示 \(\mathbf{u}_t\)，DeepSeekMoE 的输出 \(\mathbf{h}_t'\) 通过结合共享专家和选定的路由专家的输出来计算：</p>\[
\mathbf{h}_{t}^{\prime} = \mathbf{u}_{t} + \sum_{i=1}^{N_{s}} \operatorname{FFN}_{i}^{(s)}(\mathbf{u}_{t}) + \sum_{i=1}^{N_{r}} g_{i, t} \operatorname{FFN}_{i}^{(r)}(\mathbf{u}_{t}),
\]<p>其中：</p><ul><li>\(N_s\) 是共享专家的数量。</li><li>\(N_r\) 是路由专家的数量。</li><li>\(\operatorname{FFN}_i^{(s)}\) 是第 \(i\) 个共享专家网络。</li><li>\(\operatorname{FFN}_i^{(r)}\) 是第 \(i\) 个路由专家网络。</li><li>\(g_{i, t}\) 是第 \(t\) 个 token 分配给第 \(i\) 个路由专家的门控值（权重）。</li></ul><p>门控值 \(g_{i,t}\) 的计算方式是 DeepSeek-V2 和 V3 之间的关键区别之一，它基于 token-to-expert 的亲和度分数 \(s_{i,t}\)，并通过 Top-K 路由机制选择激活哪些专家。</p><h4 id=v2-与-v3-的门控机制和负载均衡对比>V2 与 V3 的门控机制和负载均衡对比<a hidden class=anchor aria-hidden=true href=#v2-与-v3-的门控机制和负载均衡对比>#</a></h4><p>MoE 模型的一个核心挑战是负载均衡：确保所有专家都能得到有效利用，避免某些专家过载而其他专家空闲，这会影响训练稳定性和计算效率。DeepSeek-V2 和 V3 在门控机制和负载均衡策略上采取了不同的方法。</p><p><strong>1. 亲和度计算 (\(s_{i,t}\)) 与 Top-K 选择:</strong></p><ul><li><p><strong>DeepSeek-V2:</strong> 使用 Softmax 函数计算每个 token 对每个路由专家的亲和度分数。Top-K 选择直接基于这些亲和度分数 \(s_{i,t}\)。</p>\[
s_{i, t} = \operatorname{Softmax}_{i}(\mathbf{u}_{t}^{T} \mathbf{e}_{i})
\]<p>其中 \(\mathbf{e}_i\) 是第 \(i\) 个路由专家的可学习中心向量。选择 \(s_{i,t}\) 最高的 \(K_r\) 个专家。</p></li><li><p><strong>DeepSeek-V3:</strong> 使用 Sigmoid 函数计算亲和度分数。更重要的是，它引入了一个可学习的偏置项 \(b_i\) 用于每个路由专家。Top-K 选择是基于 <strong>加偏置后的亲和度</strong> \(s_{i,t} + b_i\)。</p>\[
s_{i, t} = \operatorname{Sigmoid}(\mathbf{u}_{t}^{T} \mathbf{e}_{i})
\]<p>选择依据是 \(s_{i,t} + b_i\) 值最高的 \(K_r\) 个专家。</p></li></ul><p><strong>2. 门控值计算 (\(g_{i,t}\)):</strong></p><ul><li><p><strong>DeepSeek-V2:</strong> 对于被 Top-K 选中的专家，其门控值 \(g_{i,t}\) 直接等于其原始亲和度分数 \(s_{i,t}\)。对于未被选中的专家，\(g_{i,t} = 0\)。</p>\[
g_{i, t}^{\prime} = \begin{cases} s_{i, t}, & s_{i, t} \in \operatorname{Topk}(\{s_{j, t}\}, K_{r}), \\ 0, & \text{otherwise}, \end{cases}
\]<p></p>\[
g_{i, t} = g_{i, t}^{\prime} \quad (\text{V2 中不进行额外归一化})
\]</li><li><p><strong>DeepSeek-V3:</strong> 对于基于 \(s_{i,t} + b_i\) 被 Top-K 选中的专家，其门控值 \(g_{i,t}\) 是通过对这些被选中专家的 <strong>原始亲和度</strong> \(s_{i,t}\) 进行归一化得到的。偏置 \(b_i\) <strong>仅用于路由选择</strong>，不影响最终的加权求和。</p>\[
g_{i, t}^{\prime}= \begin{cases} s_{i, t}, & s_{i, t}+b_{i} \in \operatorname{Topk}\left(\left\{s_{j, t}+b_{j} \mid 1 \leqslant j \leqslant N_{r}\right\}, K_{r}\right) \\ 0, & \text{otherwise.} \end{cases}
\]<p></p>\[
g_{i, t} = \frac{g_{i, t}^{\prime}}{\sum_{j=1}^{N_{r}} g_{j, t}^{\prime}} \quad (\text{对选中的专家亲和度进行归一化})
\]</li></ul><p><strong>3. 负载均衡策略:</strong></p><ul><li><p><strong>DeepSeek-V2:</strong></p><ul><li><strong>主要策略：辅助损失</strong> V2 引入了多种辅助损失项来显式地鼓励负载均衡:<ul><li><strong>专家级平衡损失 (\(\mathcal{L}_{\text{ExpBal}}\)):</strong> 鼓励每个专家处理大致相等数量的 token。
\[
\begin{aligned}
\mathcal{L}_{\text{ExpBal}} &= \alpha_{1} \sum_{i=1}^{N_{r}} f_{i} P_{i} \\
f_{i} &= \frac{N_{r}}{K_{r} T} \sum_{t=1}^{T} \mathbb{1}(\text{Token } t \text{ selects Expert } i) \\
P_{i} &= \frac{1}{T} \sum_{t=1}^{T} s_{i, t}
\end{aligned}
\]
其中 \(T\) 是 batch 中的 token 总数，\(f_i\) 是路由到专家 \(i\) 的 token 比例（相对于理想均衡状态），\(P_i\) 是专家 \(i\) 的平均亲和度分数，\(\alpha_1\) 是超参数。</li><li><strong>设备级平衡损失 (\(\mathcal{L}_{\text{DevBal}}\)):</strong> 鼓励将计算负载均匀分布到不同的设备组上（假设专家分布在 \(D\) 个设备组 \(\{\mathcal{E}_1, \dots, \mathcal{E}_D\}\)）。
\[
\begin{aligned}
\mathcal{L}_{\text{DevBal}} &= \alpha_{2} \sum_{i=1}^{D} f_{i}^{\prime} P_{i}^{\prime} \\
f_{i}^{\prime} &= \frac{1}{|\mathcal{E}_{i}|} \sum_{j \in \mathcal{E}_{i}} f_{j} \\
P_{i}^{\prime} &= \sum_{j \in \mathcal{E}_{i}} P_{j}
\end{aligned}
\]
其中 \(f_i'\) 是设备组 \(i\) 的平均负载分数，\(P_i'\) 是设备组 \(i\) 的总亲和度，\(\alpha_2\) 是超参数。</li><li><strong>通信平衡损失 (\(\mathcal{L}_{\text{CommBal}}\)):</strong> 鼓励发送到每个设备的 token 数量大致相等，以平衡 All-to-All 通信负载。
\[
\begin{aligned}
\mathcal{L}_{\text{CommBal}} &= \alpha_{3} \sum_{i=1}^{D} f_{i}^{\prime \prime} P_{i}^{\prime \prime} \\
f_{i}^{\prime \prime} &= \frac{D}{M T} \sum_{t=1}^{T} \mathbb{1}(\text{Token } t \text{ is sent to Device } i) \\
P_{i}^{\prime \prime} &= \sum_{j \in \mathcal{E}_{i}} P_{j}
\end{aligned}
\]
其中 \(f_i''\) 是发送到设备 \(i\) 的 token 比例（相对于理想均衡状态），\(P_i''\) 是设备组 \(i\) 的总亲和度，\(\alpha_3\) 是超参数。</li></ul></li><li><strong>路由限制：设备限制路由</strong> 限制每个 token 最多只能路由到分布在 \(M\) 个不同设备上的专家。V2 中设 \(M=3\)。</li><li><strong>Token 丢弃:</strong> 在训练期间，如果某个设备接收到的 token 数量超过了预设的容量因子（通常略大于平均值），则会丢弃一部分具有最低路由权重（亲和度）的 token，以避免计算资源的浪费。但会保留约 10% 序列的 token 不被丢弃。</li></ul></li><li><p><strong>DeepSeek-V3:</strong></p><ul><li><strong>主要策略：无辅助损失的负载均衡 (Auxiliary-Loss-Free Load Balancing)</strong> V3 认为辅助损失会损害模型性能，因此采用了一种创新的<strong>无辅助损失的负载均衡</strong>(<a href=https://arxiv.org/abs/2408.15664>Wang et al., 2024</a>)。它通过动态调整前面提到的可学习偏置项 \(b_i\) 来实现负载均衡：<ul><li><strong>偏置更新:</strong> 在每个训练步骤之后，监控每个专家 \(i\) 在当前 batch 中处理的 token 数量。<ul><li>如果专家 \(i\) 过载（处理的 token 数 > Batch 总 token 数 / \(N_r\)），则降低其偏置：\(b_i \leftarrow b_i - \gamma\)。</li><li>如果专家 \(i\) 欠载（处理的 token 数 &lt; Batch 总 token 数 / \(N_r\)），则增加其偏置：\(b_i \leftarrow b_i + \gamma\)。</li></ul></li><li>\(\gamma\) 是一个小的正步长（偏置更新速率超参数）。通过这种方式，负载高的专家在后续路由中被选中的概率会降低，负载低的专家被选中的概率会增加，从而在批处理级别上动态平衡负载。</li></ul></li><li><strong>补充策略：序列级辅助损失 (\(\mathcal{L}_{\text{Bal}}\))</strong> V3 仍然保留了一个<strong>权重极小</strong> (\(\alpha=0.0001\)) 的辅助损失，但它作用于<strong>单个序列内部</strong>的专家选择平衡，而不是整个 batch。这主要是为了防止在单个序列中出现极端不平衡的情况。
\[
\begin{gathered}
\mathcal{L}_{\text{Bal}} = \alpha \sum_{i=1}^{N_{r}} f_{i} P_{i}, \\
f_{i} = \frac{N_{r}}{K_{r} T_{seq}} \sum_{t=1}^{T_{seq}} \mathbb{1}\left(s_{i, t} \in \operatorname{Topk}\left(\left\{s_{j, t} \mid 1 \leqslant j \leqslant N_{r}\right\}, K_{r}\right)\right), \\
s_{i, t}^{\prime} = \frac{s_{i, t}}{\sum_{j=1}^{N_{r}} s_{j, t}}, \quad P_{i} = \frac{1}{T_{seq}} \sum_{t=1}^{T_{seq}} s_{i, t}^{\prime}
\end{gathered}
\]
注意这里的 \(f_i, P_i\) 是在单个序列（长度为 \(T_{seq}\)）上计算的，并且 \(s_{i,t}'\) 是在序列内对原始 \(s_{i,t}\) 归一化后的值。</li><li><strong>路由限制：节点限制路由</strong> 类似于 V2 的设备限制，但应用于节点级别。V3 中设 \(M=4\)。</li><li><strong>无 Token 丢弃:</strong> 由于基于偏置调整的负载均衡效果良好，V3 在训练和推理过程中均不丢弃任何 token。</li></ul></li></ul><p><strong>V3 策略的优势:</strong>
V3 的无辅助损失策略旨在最小化负载均衡机制对模型最终性能的负面影响。通过动态调整偏置项进行批处理级别的负载均衡，相比 V2 中基于辅助损失的序列级均衡，约束更宽松。这允许专家在不同领域展现出更强的特化模式，因为路由决策不必在每个序列内部都严格遵循均衡分布。下图实验表明该策略在多个基准测试上优于基于辅助损失的方法。</p><figure class=align-center><img loading=lazy src=auxiliary_loss_free_result.png#center alt="Fig. 11. Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 11. Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><p>辅助损失无关的负载均衡与序列级辅助损失的关键区别在于它们的均衡范围：批次级与序列级。相比序列级辅助损失，批次级均衡施加了更灵活的约束，因为它不强制每个序列内的领域平衡。这种灵活性使得专家能够更好地在不同领域中进行专门化。为了验证这一点，图中记录并分析了在 Pile 测试集不同领域上，基于辅助损失的 16B 基线模型和无辅助损失的 16B 模型的专家负载，可以观察到无辅助损失模型表现出更明显的专家专门化模式，符合预期。</p><figure class=align-center><img loading=lazy src=expert_load.png#center alt="Fig. 12. Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 12. Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><hr><h4 id=deepseekmoe-v2-vs-v3-对比总结表>DeepSeekMoE V2 vs V3 对比总结表<a hidden class=anchor aria-hidden=true href=#deepseekmoe-v2-vs-v3-对比总结表>#</a></h4><table><thead><tr><th style=text-align:left>特性</th><th style=text-align:left>DeepSeek-V2</th><th style=text-align:left>DeepSeek-V3</th></tr></thead><tbody><tr><td style=text-align:left><strong>亲和度计算 \(s_{i,t}\)</strong></td><td style=text-align:left>\(\operatorname{Softmax}_{i}(\mathbf{u}_{t}^{T} \mathbf{e}_{i})\)</td><td style=text-align:left>\(\operatorname{Sigmoid}(\mathbf{u}_{t}^{T} \mathbf{e}_{i})\)</td></tr><tr><td style=text-align:left><strong>TopK 选择依据</strong></td><td style=text-align:left>原始亲和度 \(s_{i,t}\)</td><td style=text-align:left>加偏置后的亲和度 \(s_{i,t} + b_i\)</td></tr><tr><td style=text-align:left><strong>门控值计算 \(g_{i,t}\)</strong></td><td style=text-align:left>对选中的专家，\(g_{i,t} = s_{i,t}\) (通常无额外归一化)</td><td style=text-align:left>对选中的专家，基于原始亲和度 \(s_{i,t}\) 进行归一化: \(g_{i, t} = \frac{s_{i, t}}{\sum_{j \in \text{Selected}} s_{j, t}}\)</td></tr><tr><td style=text-align:left><strong>主要负载均衡策略</strong></td><td style=text-align:left><strong>辅助损失:</strong> - \(\mathcal{L}_{\text{ExpBal}}\) (专家级) - \(\mathcal{L}_{\text{DevBal}}\) (设备级) - \(\mathcal{L}_{\text{CommBal}}\) (通信级)</td><td style=text-align:left><strong>无辅助损失:</strong> - 通过动态调整可学习偏置项 \(b_i\) (步长 \(\gamma\)) 实现批处理级均衡</td></tr><tr><td style=text-align:left><strong>补充负载均衡</strong></td><td style=text-align:left>无明确的补充策略</td><td style=text-align:left><strong>序列级辅助损失</strong> \(\mathcal{L}_{\text{Bal}}\) (权重 \(\alpha\) 极小, e.g., 0.0001)，防止单序列内极端不平衡</td></tr><tr><td style=text-align:left><strong>路由限制</strong></td><td style=text-align:left><strong>设备限制:</strong> 每个 token 最多路由到 \(M=3\) 个设备上的专家</td><td style=text-align:left><strong>节点限制:</strong> 每个 token 最多路由到 \(M=4\) 个节点上的专家</td></tr><tr><td style=text-align:left><strong>Token 丢弃</strong></td><td style=text-align:left><strong>是:</strong> 训练时，为缓解计算瓶颈，会丢弃超出设备容量的 token 中亲和度最低的部分 (保留约10%序列不丢弃)</td><td style=text-align:left><strong>否:</strong> 训练和推理中均不丢弃 token</td></tr><tr><td style=text-align:left><strong>均衡粒度</strong></td><td style=text-align:left>主要通过辅助损失在序列/Batch 级别强制均衡</td><td style=text-align:left>主要通过偏置调整在 Batch 级别动态均衡，约束更宽松</td></tr><tr><td style=text-align:left><strong>对模型性能影响</strong></td><td style=text-align:left>辅助损失可能对模型性能产生负面影响</td><td style=text-align:left>设计上旨在最小化均衡策略对性能的负面影响，允许更好的专家特化</td></tr></tbody></table><h3 id=多-token-预测-mtp>多 token 预测 (MTP)<a hidden class=anchor aria-hidden=true href=#多-token-预测-mtp>#</a></h3><p>为了进一步提升模型性能和数据效率，DeepSeek-V3 引入了<strong>多 token 预测 (Multi-Token Prediction, MTP)</strong> 训练目标 (<a href=https://arxiv.org/abs/2404.19737>Gloeckle et al., 2024</a>)。标准的语言模型只预测下一个 token，而 MTP 让模型在每个位置预测未来多个（V3 中是 \(D_{MTP}=1\)，即预测下下个 token）token。</p><h4 id=mtp-实现>MTP 实现<a hidden class=anchor aria-hidden=true href=#mtp-实现>#</a></h4><p>MTP 通过 \(D_{MTP}\) 个顺序模块实现。第 \(k\) 个 MTP 模块 (\(k=1, \dots, D_{MTP}\)) 包含：</p><ul><li>共享的嵌入层 \(\operatorname{Emb}(\cdot)\)</li><li>共享的输出头 \(\operatorname{OutHead}(\cdot)\)</li><li>独立的 Transformer 块 \(\operatorname{TRM}_k(\cdot)\)</li><li>独立的投影矩阵 \(M_k \in \mathbb{R}^{d \times 2d}\)</li></ul><p>对于输入序列中的第 \(i\) 个 token \(t_i\)，在第 \(k\) 个预测深度：</p><ol><li>将第 \(i\) 个 token 在第 \(k-1\) 深度的表示 \(\mathbf{h}_i^{k-1}\)（\(k=1\) 时为主模型输出）与第 \(i+k\) 个 token 的嵌入 \(\operatorname{Emb}(t_{i+k})\) 拼接，并通过投影矩阵 \(M_k\) 得到组合表示 \(\mathbf{h}_i^{\prime k}\)：
\[
\mathbf{h}_{i}^{\prime k} = M_{k}[\operatorname{RMSNorm}(\mathbf{h}_{i}^{k-1}) ; \operatorname{RMSNorm}(\operatorname{Emb}(t_{i+k}))]
\]</li><li>将组合表示输入到第 \(k\) 个 Transformer 块，得到当前深度的输出表示 \(\mathbf{h}_i^k\)：
\[
\mathbf{h}_{1: T-k}^{k} = \operatorname{TRM}_{k}(\mathbf{h}_{1: T-k}^{\prime k})
\]</li><li>使用共享输出头预测第 \(i+k+1\) 个 token 的概率分布 \(P_{i+k+1}^k \in \mathbb{R}^V\)：
\[
P_{i+k+1}^{k} = \operatorname{OutHead}(\mathbf{h}_{i}^{k})
\]</li></ol><p>关键在于，这种实现方式<strong>保持了每个预测深度的完整因果链</strong>，与并行预测多个 token 的方法不同。</p><figure class=align-center><img loading=lazy src=mtp.png#center alt="Fig. 13. Illustration of our Multi-Token Prediction (MTP) implementation. They keep the complete causal chain for the prediction of each token at each depth. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 13. Illustration of our Multi-Token Prediction (MTP) implementation. They keep the complete causal chain for the prediction of each token at each depth. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h4 id=mtp-训练目标>MTP 训练目标<a hidden class=anchor aria-hidden=true href=#mtp-训练目标>#</a></h4><p>计算每个预测深度 \(k\) 的交叉熵损失 \(\mathcal{L}_{\text{MTP}}^k\)：</p>\[
\mathcal{L}_{\text{MTP}}^{k} = \operatorname{CrossEntropy}(P_{2+k: T+1}^{k}, t_{2+k: T+1}) = -\frac{1}{T} \sum_{i=2+k}^{T+1} \log P_{i}^{k}[t_{i}]
\]<p>总的 MTP 损失是所有深度损失的加权平均：</p>\[
\mathcal{L}_{\text{MTP}} = \frac{\lambda}{D_{MTP}} \sum_{k=1}^{D_{MTP}} \mathcal{L}_{\text{MTP}}^{k}
\]<p>其中 \(\lambda\) 是权重因子（V3 中前期为 0.3，后期为 0.1）。这个损失会加到主模型的标准 next-token prediction 损失上。</p><h4 id=mtp-推理>MTP 推理<a hidden class=anchor aria-hidden=true href=#mtp-推理>#</a></h4><p>MTP 主要用于提升主模型性能。推理时，可以<strong>直接丢弃 MTP 模块</strong>，主模型可以独立工作。或者也可以利用 MTP 模块进行<strong>投机解码</strong> (<a href=https://arxiv.org/abs/2211.17192>Leviathan et al., 2023</a>; <a href=https://arxiv.org/abs/2203.16487>Xia et al., 2023</a>)，以加速生成过程。V3 的实验表明，第二个 token 的接受率在 85%-90% 之间，可将解码速度提升约 1.8 倍。</p><h2 id=基础设施与训练效率>基础设施与训练效率<a hidden class=anchor aria-hidden=true href=#基础设施与训练效率>#</a></h2><p>DeepSeek-V3 的高效训练与部署得益于算法、框架和硬件的协同设计。</p><h3 id=计算集群>计算集群<a hidden class=anchor aria-hidden=true href=#计算集群>#</a></h3><p>DeepSeek-V3 在一个配备了 <strong>2048 块 NVIDIA H800 GPU</strong> 的集群上进行训练。</p><ul><li><strong>节点内部:</strong> 每个节点包含 8 块 H800 GPU，通过 <strong>NVLink</strong> 和 <strong>NVSwitch</strong> 高速互联。</li><li><strong>节点之间:</strong> 不同节点间使用 <strong>InfiniBand (IB)</strong> 网络进行通信。</li></ul><h3 id=训练框架>训练框架<a hidden class=anchor aria-hidden=true href=#训练框架>#</a></h3><p>DeepSeek-V3 的训练基于自研的高效轻量级框架 <strong>HAI-LLM</strong>。整体上采用了：</p><ul><li><strong>16 路流水线并行 (Pipeline Parallelism, PP)</strong> (<a href=https://arxiv.org/abs/2401.10241>Qi et al., 2023</a>)</li><li><strong>64 路专家并行 (Expert Parallelism, EP)</strong> (跨 8 节点) (<a href=https://arxiv.org/abs/2006.16668>Lepikhin et al., 2021</a>)</li><li><strong>ZeRO-1 数据并行 (Data Parallelism, DP)</strong> (<a href=https://arxiv.org/pdf/1910.02054>Rajbhandari et al., 2020</a>)</li></ul><p>为了实现高效训练，Deepseek 进行了细致的工程优化：</p><ol><li>设计了 <strong>DualPipe</strong> 算法以实现高效流水线并行，减少气泡并重叠计算与通信，解决了跨节点专家并行带来的重通信开销问题。</li><li>开发了高效的<strong>跨节点 All-to-all 通信 Kernel</strong>，充分利用 IB 和 NVLink 带宽，并节省用于通信的 SM 资源。</li><li>细致优化了训练过程中的<strong>内存占用</strong>，使得可以在<strong>不使用张量并行 (Tensor Parallelism, TP)</strong> 的情况下训练 DeepSeek-V3。</li></ol><h4 id=dualpipe-与计算通信重叠>DualPipe 与计算通信重叠<a hidden class=anchor aria-hidden=true href=#dualpipe-与计算通信重叠>#</a></h4><ul><li><strong>挑战:</strong> 跨节点专家并行导致计算通信比接近 1:1，效率不高。</li></ul><figure class=align-center><img loading=lazy src=forward_backward_chucks.png#center alt="Fig. 17. Overlapping strategy for a pair of forward and backward chunks with misaligned transformer block boundaries. Orange: forward, green: backward for input, blue: backward for weights, purple: PP communication, red: barriers. Both all-to-all and PP communications are fully hidden. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 17. Overlapping strategy for a pair of forward and backward chunks with misaligned transformer block boundaries. Orange: forward, green: backward for input, blue: backward for weights, purple: PP communication, red: barriers. Both all-to-all and PP communications are fully hidden. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><ul><li><strong>核心思想:</strong> 重叠一对独立的前向和反向 chunk 内的计算和通信。将每个 chunk 分解为 <strong>Attention</strong>、<strong>All-to-all Dispatch</strong>、<strong>MLP</strong>、<strong>All-to-all Combine</strong> 四个组件（反向的 Attention 和 MLP 进一步细分为 backward for input 和 backward for weights，类似 <strong>ZeroBubble</strong> (<a href=https://arxiv.org/abs/2401.10241>Qi et al., 2023</a>）。通过重排这些组件并手动调整用于通信与计算的 GPU SM 比例，实现 All-to-all 和 PP 通信的完全隐藏。</li><li><strong>调度:</strong> 采用双向流水线调度，同时从流水线的两端输入微批次，大部分通信可以被完全重叠。</li></ul><figure class=align-center><img loading=lazy src=dualpipe.png#center alt="Fig. 18. Example DualPipe scheduling with 8 PP ranks and 20 micro-batches in both directions. The reverse-direction micro-batches mirror the forward ones, so their batch IDs are omitted for simplicity. Two cells within a shared black border represent mutually overlapped computation and communication. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 18. Example DualPipe scheduling with 8 PP ranks and 20 micro-batches in both directions. The reverse-direction micro-batches mirror the forward ones, so their batch IDs are omitted for simplicity. Two cells within a shared black border represent mutually overlapped computation and communication. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><ul><li><strong>优势:</strong><ul><li>即使在没有重通信负担的一般场景下也具有效率优势。</li><li>相比 <strong>ZB1P</strong> (<a href=https://arxiv.org/abs/2401.10241>Qi et al., 2023</a>) 和 <strong>1F1B</strong> (<a href=https://arxiv.org/abs/1806.03377>Harlap et al., 2018</a>)，显著减少流水线气泡，仅增加 \(\frac{1}{PP}\) 倍的峰值激活内存。</li><li>虽然需要两份模型参数副本，但由于训练中使用了大的 EP size，内存增加不显著。</li><li>相比 <strong>Chimera</strong> (<a href=https://dl.acm.org/doi/10.1145/3458817.3476145>Li and Hoefler, 2021</a>)，对微批次数量的要求更宽松（只需能被 2 整除），且气泡和激活内存不随微批次数量增加而增长。</li></ul></li></ul><table><thead><tr><th style=text-align:left>方法 (Method)</th><th style=text-align:left>气泡 (Bubble)</th><th style=text-align:left>参数 (Parameter)</th><th style=text-align:left>激活 (Activation)</th></tr></thead><tbody><tr><td style=text-align:left>1F1B</td><td style=text-align:left>\((PP-1)(F+B)\)</td><td style=text-align:left>\(1 \times\)</td><td style=text-align:left>\(PP\)</td></tr><tr><td style=text-align:left>ZB1P</td><td style=text-align:left>\((PP-1)(F+B-2W)\)</td><td style=text-align:left>\(1 \times\)</td><td style=text-align:left>\(PP\)</td></tr><tr><td style=text-align:left><strong>DualPipe (Deepseek V3)</strong></td><td style=text-align:left>\(\left(\frac{PP}{2}-1\right)(F\&B+B-3W)\)</td><td style=text-align:left>\(2 \times\)</td><td style=text-align:left>\(PP+1\)</td></tr></tbody></table><p>上表对不同流水线并行方法的流水线气泡和内存使用比较。\(F\)：前向 chunk 执行时间；\(B\)：完整反向 chunk 执行时间；\(W\)：“权重反向” chunk 执行时间；\(F\&B\)：两个相互重叠的前向和反向 chunk 的执行时间。</p><h4 id=高效的跨节点-all-to-all-通信实现>高效的跨节点 All-to-All 通信实现<a hidden class=anchor aria-hidden=true href=#高效的跨节点-all-to-all-通信实现>#</a></h4><ul><li><strong>目标:</strong> 为 DualPipe 提供足够的计算性能，定制高效的跨节点 All-to-all 通信 Kernel (dispatching & combining)，节省通信专用 SM。</li><li><strong>策略:</strong> 结合 MoE 门控算法和集群网络拓扑（节点间 IB 全互联，节点内 NVLink）。<ul><li><strong>带宽利用:</strong> NVLink 带宽 (\(160 \mathrm{~GB} / \mathrm{s}\)) 约是 IB (\(50 \mathrm{~GB} / \mathrm{s}\)) 的 3.2 倍。限制每个 token 最多分发到 <strong>4 个节点</strong>以减少 IB 流量。</li><li><strong>传输路径:</strong> Token 确定路由后，先通过 <strong>IB</strong> 传输到目标节点上具有相同节点内索引的 GPU。到达目标节点后，立即通过 <strong>NVLink</strong> 转发给托管目标专家的特定 GPU，避免被后续到达的 token 阻塞。</li><li><strong>效果:</strong> IB 和 NVLink 通信完全重叠。每个 token 可高效选择平均 <strong>3.2 个专家/节点</strong>，无需额外 NVLink 开销。这意味着 V3 实际选择 8 个路由专家，但理论上可扩展到 <strong>13 个专家</strong> (4 nodes × 3.2 experts/node) 而通信成本不变。</li></ul></li><li><strong>实现:</strong><ul><li>使用 <strong>Warp Specialization</strong> (<a href=https://doi.org/10.1145/2555243.2555258>Bauer et al., 2014</a>)技术 ，将 <strong>20 个 SM</strong> 划分为 10 个通信通道。</li><li>Dispatch 过程：IB 发送、IB-to-NVLink 转发、NVLink 接收由各自的 warp 处理，warp 数量根据负载动态调整。</li><li>Combine 过程：NVLink 发送、NVLink-to-IB 转发与累加、IB 接收与累加也由动态调整的 warp 处理。</li><li><strong>优化:</strong> Dispatch 和 Combine Kernel 与计算流重叠。使用定制 <strong>PTX</strong> 指令并自动调整通信 chunk 大小，显著减少 L2 缓存使用和对其他 SM 计算 Kernel 的干扰。</li></ul></li><li><strong>结果:</strong> 仅需 <strong>20 个 SM</strong> 即可充分利用 IB 和 NVLink 带宽。</li></ul><h4 id=极致内存优化与最小开销>极致内存优化与最小开销<a hidden class=anchor aria-hidden=true href=#极致内存优化与最小开销>#</a></h4><p>为减少训练内存占用，采用了以下技术：</p><ul><li><strong>重计算:</strong> 在反向传播中重计算所有 <strong>RMSNorm</strong> 操作和 <strong>MLA 的上投影</strong>，避免存储它们的输出激活值。以微小开销显著减少激活内存需求。</li><li><strong>CPU 存储 EMA:</strong> 将模型参数的<strong>指数移动平均 (EMA)</strong> 保存在 <strong>CPU 内存</strong>中，并在每个训练步骤后异步更新。无需额外 GPU 内存或时间开销即可维护 EMA 参数。</li><li><strong>共享 Embedding 和输出头:</strong> 利用 DualPipe 策略，将模型最浅层（含 Embedding 层）和最深层（含输出头）部署在<strong>同一个 PP rank</strong> 上。这使得 <strong>MTP 模块</strong>和主模型可以<strong>物理共享</strong> Embedding 和输出头的参数及梯度，进一步提升内存效率。</li><li><strong>效果:</strong> 这些优化使得 DeepSeek-V3 可以在<strong>不使用昂贵的张量并行 (TP)</strong> 的情况下进行训练。</li></ul><h3 id=fp8-训练>FP8 训练<a hidden class=anchor aria-hidden=true href=#fp8-训练>#</a></h3><p>为了加速训练并减少显存占用，DeepSeek-V3 采用了 <strong>FP8 混合精度训练框架</strong> (<a href=https://arxiv.org/pdf/2208.07339>Dettmers et al., 2022</a>; <a href=https://arxiv.org/abs/2206.02915>Noune et al., 2022</a>; <a href=https://arxiv.org/abs/2310.18313>Peng et al., 2023</a>)，并在超大规模模型上首次验证了其有效性。</p><h4 id=混合精度框架>混合精度框架<a hidden class=anchor aria-hidden=true href=#混合精度框架>#</a></h4><ul><li><strong>核心计算 (GEMM):</strong> 大部分 GEMM 操作（前向、激活梯度反向、权重梯度反向）使用 FP8 输入，输出 BF16 或 FP32，理论上计算速度翻倍。</li><li><strong>高精度保留:</strong> 对精度敏感或计算开销小的部分（如 Embedding、输出头、MoE 门控、Normalization、Attention）保留 BF16/FP32 精度。</li><li><strong>高精度存储:</strong> 主权重、权重梯度、优化器状态（部分 BF16）使用更高精度存储，通过 ZeRO-1 分片降低显存压力。</li></ul><figure class=align-center><img loading=lazy src=fp8_framework.png#center alt="Fig. 14. The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 14. The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h4 id=精度提升策略>精度提升策略<a hidden class=anchor aria-hidden=true href=#精度提升策略>#</a></h4><ol><li><strong>细粒度量化:</strong> 为了解决 FP8 <strong>动态范围有限和对离群值敏感</strong>的问题 (<a href=https://arxiv.org/abs/2409.12517>Fishman et al., 2024</a>; <a href=https://arxiv.org/abs/2405.19279>He et al., 2024</a>; <a href=https://arxiv.org/abs/2402.17762>Sun et al., 2024</a>)，采用更细粒度的量化：<ul><li><strong>激活:</strong> 按 \(1 \times 128\) 的 tile 分组缩放。</li><li><strong>权重:</strong> 按 \(128 \times 128\) 的 block 分组缩放。
这种方法让缩放因子更适应局部数据的范围，减少量化误差。</li></ul></li><li><strong>提升累加精度:</strong> H800 的 Tensor Core 进行 FP8 GEMM 时累加精度有限（约 14 位）。为解决此问题，采用 <strong>Promotion to CUDA Cores</strong> 策略 (<a href=https://github.com/NVIDIA/cutlass>Thakkar et al., 2023</a>)：Tensor Core 计算部分累加和（例如每 \(N_C=128\) 个元素），然后将结果传输到 CUDA Core 的 FP32 寄存器中进行全精度累加。细粒度量化的缩放因子也可以在 CUDA Core 上高效应用。通过 WGMMA 操作的并发执行，这种方法在提升精度的同时，对计算效率影响较小。</li><li><strong>E4M3 格式:</strong> V3 在所有张量上统一使用 <strong>E4M3 格式</strong>（4 位指数，3 位尾数），而非混合使用 <strong>E5M2</strong> (<a href=https://github.com/NVIDIA/TransformerEngine>NVIDIA, 2024</a>; <a href=https://arxiv.org/abs/2310.18313>Peng et al., 2023</a>; <a href=https://papers.nips.cc/paper_files/paper/2019/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html>Sun et al., 2019b</a>)。细粒度量化策略有效缓解了 E4M3 动态范围较小的问题。</li><li><strong>在线量化:</strong> 实时计算每个 tile/block 的<strong>最大绝对值来确定缩放因子，而非依赖历史值</strong> (<a href=https://github.com/NVIDIA/TransformerEngine>NVIDIA, 2024</a>; <a href=https://arxiv.org/abs/2310.18313>Peng et al., 2023</a>)，确保量化精度。</li></ol><figure class=align-center><img loading=lazy src=fp8_quantization_enhancement.png#center alt="Fig. 15. (a) Fine-grained quantization method to mitigate quantization errors. (b) Improved FP8 GEMM precision by promoting to CUDA Cores for high-precision accumulation. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 15. (a) Fine-grained quantization method to mitigate quantization errors. (b) Improved FP8 GEMM precision by promoting to CUDA Cores for high-precision accumulation. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h4 id=低精度存储与通信>低精度存储与通信<a hidden class=anchor aria-hidden=true href=#低精度存储与通信>#</a></h4><ul><li><strong>优化器状态:</strong> <strong>AdamW</strong> (<a href=https://arxiv.org/abs/1711.05101>Loshchilov and Hutter, 2017</a>) 的一阶和二阶矩使用 BF16 存储。主权重和梯度累加仍用 FP32。</li><li><strong>激活缓存:</strong> Wgrad 操作使用 FP8 输入，因此激活值可以缓存为 FP8。对特定敏感操作（如 Attention 后 Linear 的输入）使用定制的 E5M6 格式，并进行 round scaling。MoE 中 SwiGLU 的输入也缓存为 FP8。</li><li><strong>通信:</strong> MoE up-projection 前的激活量化为 FP8 进行分发（dispatch），MoE down-projection 前的激活梯度也量化为 FP8。Combine 操作保留 BF16 精度。</li></ul><p>下图实验证明，FP8 训练的损失与 BF16 相比，相对误差低于<strong>0.25%</strong>，在可接受范围内。</p><figure class=align-center><img loading=lazy src=fp8_vs_bf16_loss_curves.png#center alt="Fig. 16. Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9 DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 16. Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9 <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h3 id=推理与部署>推理与部署<a hidden class=anchor aria-hidden=true href=#推理与部署>#</a></h3><p>DeepSeek-V3 部署在 H800 集群上（节点内 NVLink，节点间 IB 全互联）。为同时保证在线服务的 <strong>SLO (服务等级目标)</strong> 和高吞吐量，采用分离 <strong>Prefilling (预填充)</strong> 和 <strong>Decoding (解码)</strong> 阶段的部署策略。</p><h4 id=预填充阶段>预填充阶段<a hidden class=anchor aria-hidden=true href=#预填充阶段>#</a></h4><ul><li><strong>最小部署单元:</strong> 4 节点 32 GPU。</li><li><strong>并行策略:</strong><ul><li>Attention 部分: <strong>TP4 (张量并行) + SP (序列并行)</strong> 结合 <strong>DP8 (数据并行)</strong>。较小的 TP size (4) 限制了 TP 通信开销。</li><li>MoE 部分: <strong>EP32 (专家并行)</strong>，确保每个专家处理足够大的批次以提高计算效率。</li><li>浅层 Dense MLP: 使用 TP1 节省 TP 通信。</li></ul></li><li><strong>MoE All-to-All 通信:</strong> 采用与训练类似的方法：先通过 IB 跨节点传输 token，然后在节点内 GPU 间通过 NVLink 转发。</li><li><strong>负载均衡:</strong> 采用 <strong>冗余专家</strong> 的部署策略。<ul><li>根据在线部署收集的统计数据，周期性（例如每 10 分钟）检测<strong>高负载专家</strong>并进行复制部署。</li><li>确定冗余专家集后，根据观察到的负载仔细地在节点内 GPU 间<strong>重排专家</strong>，在不增加跨节点 All-to-all 通信开销的前提下尽可能平衡 GPU 负载。</li><li>DeepSeek-V3 部署中，Prefilling 阶段设置 <strong>32 个冗余专家</strong>。每个 GPU 除了托管原有的 8 个专家外，还会额外托管 1 个冗余专家。</li></ul></li><li><strong>效率优化:</strong> 为提高吞吐并隐藏 All-to-all 和 TP 通信开销，<strong>同时处理两个计算负载相似的微批次</strong>，将一个微批次的 Attention 和 MoE 与另一个微批次的 Dispatch 和 Combine 重叠。</li><li><strong>探索方向:</strong> <strong>动态冗余</strong> 策略，每个 GPU 托管更多专家（如 16 个），但每次推理步骤只激活 9 个。在每层 All-to-all 操作开始前动态计算全局最优路由方案。由于 Prefilling 计算量大，计算路由方案的开销几乎可忽略。</li></ul><h4 id=解码阶段>解码阶段<a hidden class=anchor aria-hidden=true href=#解码阶段>#</a></h4><ul><li><strong>专家视角:</strong> 将<strong>共享专家</strong> 视为一个路由目标。从这个角度看，每个 token 在路由时会选择 <strong>9 个专家</strong>（共享专家被视为一个总会被选中的高负载专家）。</li><li><strong>最小部署单元:</strong> 40 节点 320 GPU。</li><li><strong>并行策略:</strong><ul><li>Attention 部分: <strong>TP4 + SP</strong> 结合 <strong>DP80</strong>。</li><li>MoE 部分: <strong>EP320</strong>。每个 GPU 只托管一个专家，其中 64 个 GPU 负责托管冗余专家和共享专家。</li></ul></li><li><strong>All-to-All 通信:</strong> Dispatch 和 Combine 部分通过 <strong>IB 直接点对点传输</strong>以实现低延迟。利用 <strong>IBGDA</strong> (<a href=https://developer.nvidia.com/blog/gpudirect-storage/>NVIDIA, 2022</a>) 技术进一步最小化延迟和提升通信效率。</li><li><strong>负载均衡:</strong> 类似 Prefilling，根据在线服务的统计专家负载，周期性确定冗余专家集。但由于每个 GPU 只托管一个专家，无需重排。</li><li><strong>探索方向:</strong><ul><li><strong>动态冗余策略:</strong> 但需要更仔细地优化计算全局最优路由方案的算法，并与 Dispatch Kernel 融合以减少开销。</li><li><strong>同时处理两个微批次:</strong> 与 Prefilling 不同，解码阶段 Attention 耗时占比更大。因此，将一个微批次的 <strong>Attention</strong> 与另一个微批次的 <strong>Dispatch+MoE+Combine</strong> 重叠。解码阶段每个专家的批次大小相对较小（通常小于 256 token），瓶颈是内存访问而非计算。由于 MoE 部分只需加载一个专家的参数，访存开销小，使用较少 SM 不会显著影响整体性能。因此，可以只分配一小部分 SM 给 Dispatch+MoE+Combine，避免影响 Attention 部分的计算速度。</li></ul></li></ul><h3 id=对硬件设计的建议>对硬件设计的建议<a hidden class=anchor aria-hidden=true href=#对硬件设计的建议>#</a></h3><p>DeepSeek 团队基于 <strong>All-to-all 通信</strong>和 <strong>FP8 训练方案</strong>的实现，向 AI 硬件供应商提出以下芯片设计建议。</p><h4 id=通信硬件>通信硬件<a hidden class=anchor aria-hidden=true href=#通信硬件>#</a></h4><ul><li><strong>现状:</strong> 通过计算/通信重叠隐藏了通信延迟，显著降低了对通信带宽的依赖。但当前通信实现依赖昂贵的 <strong>SM</strong>（例如 H800 上分配了 132 个 SM 中的 20 个用于此目的），限制了计算吞吐量，且 SM 用于通信导致 Tensor Core 完全闲置，效率低下。</li><li><strong>SM 主要任务:</strong><ul><li>在 IB 和 NVLink 域之间转发数据，同时将发往同一节点内多个 GPU 的 IB 流量从单个 GPU 聚合。</li><li>在 RDMA 缓冲区和输入/输出缓冲区之间传输数据。</li><li>为 All-to-all Combine 执行 Reduce 操作。</li><li>在跨 IB 和 NVLink 域向多个专家分块传输数据时管理细粒度内存布局。</li></ul></li><li><strong>期望:</strong><ul><li>未来供应商开发硬件，将这些通信任务从宝贵的计算单元 SM <strong>卸载</strong>，作为 GPU 协处理器或网络协处理器其类似于 NVIDIA SHARP(<a href=https://network.nvidia.com/pdf/solutions/hpc/paperieee_copyright.pdf>Graham et al., 2016</a>)。</li><li>为降低应用编程复杂性，期望该硬件能从计算单元的角度<strong>统一 IB (scale-out) 和 NVLink (scale-up) 网络</strong>。通过这个统一接口，计算单元可以通过提交基于简单原语的通信请求，轻松完成在整个 IB-NVLink 统一域内的读、写、多播和 Reduce 等操作。</li></ul></li></ul><h4 id=计算硬件>计算硬件<a hidden class=anchor aria-hidden=true href=#计算硬件>#</a></h4><ol><li><strong>Tensor Core 中更高精度的 FP8 GEMM 累加:</strong><ul><li><strong>问题:</strong> 当前 NVIDIA Hopper 架构 Tensor Core 实现中，FP8 GEMM 使用定点累加，通过右移对齐尾数乘积再相加。实验表明，符号填充右移后仅使用每个尾数乘积的最高 14 位，超出范围的位被截断。然而，例如要从 32 个 FP8×FP8 乘积累加得到精确的 FP32 结果，至少需要 34 位精度。</li><li><strong>建议:</strong> 未来芯片设计应<strong>增加 Tensor Core 中的累加精度</strong>以支持全精度累加，或根据训练和推理算法的精度要求选择合适的累加位宽。这种方法能在保持计算效率的同时确保误差在可接受范围内。</li></ul></li><li><strong>支持 Tile 和 Block 级量化:</strong><ul><li><strong>问题:</strong> 当前 GPU 仅支持 per-tensor 量化，缺乏对 tile-wise 和 block-wise 等细粒度量化的原生支持。当前实现中，达到 \(N_C\) 间隔时，部分结果需从 Tensor Core 复制到 CUDA Core，乘以缩放因子，再加到 CUDA Core 的 FP32 寄存器上。虽然结合精确 FP32 累加策略显著缓解了反量化开销，但 Tensor Core 和 CUDA Core 间的频繁数据移动仍限制计算效率。</li><li><strong>建议:</strong> 未来芯片应通过<strong>让 Tensor Core 能够接收缩放因子并实现带分组缩放的 MMA</strong> 来支持细粒度量化。这样，整个部分和累加与反量化可以在 Tensor Core 内部直接完成，直到产生最终结果，避免频繁数据移动。</li></ul></li><li><strong>支持在线量化:</strong><ul><li><strong>问题:</strong> 当前实现难以有效支持在线量化，尽管其有效性已在研究中得到证明。现有流程中，需要从 HBM 读取 128 个 BF16 激活值（前一计算的输出）进行量化，量化后的 FP8 值写回 HBM，然后再次读取用于 MMA。</li><li><strong>建议:</strong><ul><li>未来芯片将 <strong>FP8 类型转换和 TMA (Tensor Memory Accelerator) 访问融合成单一操作</strong>，使得量化可以在激活从全局内存传输到共享内存的过程中完成，避免频繁的内存读写。</li><li>推荐支持 <strong>warp 级 cast 指令</strong>以加速，进一步促进 Layer Normalization 和 FP8 cast 的更好融合。</li><li>或者，采用<strong>近存计算</strong>方法，在 HBM 附近放置计算逻辑。这样，BF16 元素可以在从 HBM 读入 GPU 时直接转换为 FP8，减少约 50% 的片外内存访问。</li></ul></li></ul></li><li><strong>支持转置 GEMM 操作:</strong><ul><li><strong>问题:</strong> 当前架构将矩阵转置与 GEMM 操作融合起来很麻烦。在工作流中，前向传播的激活被量化为 \(1 \times 128\) 的 FP8 tile 并存储。反向传播时，需要读出矩阵，反量化，转置，重新量化为 \(128 \times 1\) tile，再存入 HBM。</li><li><strong>建议:</strong> 未来芯片应支持在 MMA 操作前<strong>直接从共享内存读取转置后的矩阵</strong>（针对训练和推理所需的那些精度）。结合 FP8 格式转换和 TMA 访问的融合，此增强将显著简化量化工作流。</li></ul></li></ol><h3 id=训练成本与效率>训练成本与效率<a hidden class=anchor aria-hidden=true href=#训练成本与效率>#</a></h3><ul><li><strong>DeepSeek-V2:</strong> 相比 DeepSeek 67B (Dense)，节省 42.5% 训练成本，KV 缓存减少 93.3%，最大吞吐量提升 5.76 倍。</li><li><strong>DeepSeek-V3:</strong> 训练效率极高，每训练 1T token 仅需 180K H800 GPU 小时。总训练成本（预训练+上下文扩展+后训练）仅 2.788M H800 GPU 小时（约 558 万美元，按 2 美元/小时计）。预训练在 2048 卡 H800 集群上耗时不到 2 个月。</li></ul><table><thead><tr><th style=text-align:left>训练阶段</th><th style=text-align:center>H800 GPU 小时</th><th style=text-align:center>预估成本 (美元)</th></tr></thead><tbody><tr><td style=text-align:left>预训练</td><td style=text-align:center>2664 K</td><td style=text-align:center>$5.328 M</td></tr><tr><td style=text-align:left>上下文扩展</td><td style=text-align:center>119 K</td><td style=text-align:center>$0.238 M</td></tr><tr><td style=text-align:left>后训练</td><td style=text-align:center>5 K</td><td style=text-align:center>$0.01 M</td></tr><tr><td style=text-align:left><strong>总计</strong></td><td style=text-align:center><strong>2788 K</strong></td><td style=text-align:center><strong>$5.576 M</strong></td></tr></tbody></table><h2 id=预训练>预训练<a hidden class=anchor aria-hidden=true href=#预训练>#</a></h2><h3 id=数据构建>数据构建<a hidden class=anchor aria-hidden=true href=#数据构建>#</a></h3><p>相较于 DeepSeek‑V2（基于 67B 模型，使用 100K 词表 Byte‑level BPE Tokenizer，8.1T tokens），DeepSeek‑V3 在预训练阶段通过以下策略，实现了更大规模和更高质量的数据构建：</p><ol><li><p><strong>语料库扩展与精炼</strong></p><ul><li><strong>专注领域</strong>：显著增加数学与编程相关文本占比，强化模型在技术领域的理解与生成能力。</li><li><strong>多语言覆盖</strong>：在英语、中文之外，新增多种语种语料，提升跨语言泛化性能。</li><li><strong>去重与多样性</strong>：采用高效的数据去重和过滤流程，既最大限度减少冗余，又保证内容多样性。</li><li><strong>规模提升</strong>：最终构建了约 <strong>14.8T</strong> 高质量 tokens，比 V2 增长近 83%。</li></ul></li><li><p><strong>训练策略与技术创新</strong></p><ul><li><strong>文档打包</strong>
结合 <strong>Document Packing</strong>(<a href=https://arxiv.org/abs/2404.10830>Ding et al., 2024</a>)方法，将连贯文本打包为更长片段，以提升 GPU 利用率和上下文完整性；未采用跨样本注意力掩码，保持实现简洁。</li><li><strong>Fill‑in‑Middle（FIM）策略</strong><ul><li><strong>动机</strong>：借鉴 DeepSeekCoder‑V2(<a href=https://arxiv.org/abs/2406.11931>DeepSeek‑AI, 2024</a>)的方法，旨在提升模型对中间缺失信息的填充能力。</li><li><strong>框架</strong>：引入 Prefix‑Suffix‑Middle (PSM) 结构 ，样例如下：<pre tabindex=0><code>&lt;|fim_begin|&gt; f_pre &lt;|fim_hole|&gt; f_suf &lt;|fim_end|&gt; f_middle &lt;|eos_token|&gt;
</code></pre></li><li><strong>应用比例</strong>：文档级预打包前插入 FIM，占比 <strong>10%</strong>，平衡生成与预测任务。</li></ul></li></ul></li><li><p><strong>Tokenizer 优化</strong></p><ul><li><strong>BBPE 词表扩容</strong>：采用 Byte‑level BPE，词表由 100K 扩至 <strong>128K</strong>，提升罕见词与专有名词覆盖。</li><li><strong>预分词器改进</strong>：针对多语言场景，调整分词规则，提升压缩效率与编码一致性。</li><li><strong>边界偏见缓解</strong>：参考 <a href=https://github.com/guidance-ai/guidance/blob/main/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb>Lundberg, 2023</a> 的方法为减少标点符号+换行组合 token 在 few‑shot 场景下的偏倚，引入随机拆分机制，让模型接触更多边界变体。</li></ul></li></ol><h3 id=超参数>超参数<a hidden class=anchor aria-hidden=true href=#超参数>#</a></h3><table><thead><tr><th style=text-align:left>参数</th><th style=text-align:center>DeepSeek-V2</th><th style=text-align:center>DeepSeek-V3</th></tr></thead><tbody><tr><td style=text-align:left>Transformer 层数</td><td style=text-align:center>60</td><td style=text-align:center>61</td></tr><tr><td style=text-align:left>隐藏层维度 \(d\)</td><td style=text-align:center>5120</td><td style=text-align:center>7168</td></tr><tr><td style=text-align:left>初始化标准差</td><td style=text-align:center>0.006</td><td style=text-align:center>0.006</td></tr><tr><td style=text-align:left><strong>MLA 参数</strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>注意力头数 \(n_h\)</td><td style=text-align:center>128</td><td style=text-align:center>128</td></tr><tr><td style=text-align:left>每头维度 \(d_h\)</td><td style=text-align:center>128</td><td style=text-align:center>128</td></tr><tr><td style=text-align:left>KV 压缩维度 \(d_c\)</td><td style=text-align:center>512 (\(4d_h\))</td><td style=text-align:center>512 (\(4d_h\))</td></tr><tr><td style=text-align:left>查询压缩维度 \(d_c'\)</td><td style=text-align:center>1536</td><td style=text-align:center>1536</td></tr><tr><td style=text-align:left>解耦 RoPE 维度 \(d_h^R\)</td><td style=text-align:center>64 (\(d_h/2\))</td><td style=text-align:center>64 (\(d_h/2\))</td></tr><tr><td style=text-align:left><strong>DeepSeekMoE 参数</strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MoE 层位置</td><td style=text-align:center>除第 1 层外</td><td style=text-align:center>除前 3 层外</td></tr><tr><td style=text-align:left>共享专家数 \(N_s\)</td><td style=text-align:center>2</td><td style=text-align:center>1</td></tr><tr><td style=text-align:left>路由专家数 \(N_r\)</td><td style=text-align:center>160</td><td style=text-align:center>256</td></tr><tr><td style=text-align:left>专家中间维度</td><td style=text-align:center>1536</td><td style=text-align:center>2048</td></tr><tr><td style=text-align:left>激活专家数 \(K_r\)</td><td style=text-align:center>6</td><td style=text-align:center>8</td></tr><tr><td style=text-align:left>设备/节点限制路由 \(M\)</td><td style=text-align:center>3 (设备)</td><td style=text-align:center>4 (节点)</td></tr><tr><td style=text-align:left>负载均衡策略</td><td style=text-align:center>辅助损失 (\(\alpha_1=0.003, \alpha_2=0.05, \alpha_3=0.02\)) + Token Dropping</td><td style=text-align:center>无辅助损失 (\(\gamma=0.001\)) + 序列级损失 (\(\alpha=0.0001\))</td></tr><tr><td style=text-align:left><strong>MTP 参数 (V3 only)</strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>预测深度 \(D_{MTP}\)</td><td style=text-align:center>N/A</td><td style=text-align:center>1</td></tr><tr><td style=text-align:left>MTP 损失权重 \(\lambda\)</td><td style=text-align:center>N/A</td><td style=text-align:center>0.3 (前 10T) / 0.1 (后 4.8T)</td></tr><tr><td style=text-align:left><strong>训练参数</strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>优化器</td><td style=text-align:center>AdamW (\(\beta_1=0.9, \beta_2=0.95, wd=0.1\))</td><td style=text-align:center>AdamW (\(\beta_1=0.9, \beta_2=0.95, wd=0.1\))</td></tr><tr><td style=text-align:left>最大序列长度</td><td style=text-align:center>4K</td><td style=text-align:center>4K</td></tr><tr><td style=text-align:left>训练 Tokens</td><td style=text-align:center>8.1T</td><td style=text-align:center>14.8T</td></tr><tr><td style=text-align:left>学习率</td><td style=text-align:center>Warmup + Step Decay (Max \(2.4 \times 10^{-4}\))</td><td style=text-align:center>Warmup + Cosine Decay + Constant (Max \(2.2 \times 10^{-4}\))</td></tr><tr><td style=text-align:left>Batch Size</td><td style=text-align:center>2304 -> 9216</td><td style=text-align:center>3072 -> 15360</td></tr><tr><td style=text-align:left>梯度裁剪</td><td style=text-align:center>1.0</td><td style=text-align:center>1.0</td></tr><tr><td style=text-align:left>精度</td><td style=text-align:center>BF16</td><td style=text-align:center>FP8 混合精度</td></tr></tbody></table><h3 id=长上下文扩展>长上下文扩展<a hidden class=anchor aria-hidden=true href=#长上下文扩展>#</a></h3><p>两者均使用 <strong>YaRN</strong> (<a href=https://arxiv.org/abs/2309.00071>Peng et al., 2023</a>) 技术扩展上下文窗口。</p><ul><li><strong>DeepSeek-V2:</strong> 从 4K 扩展到 128K。使用 YaRN (scale \(s=40, \alpha=1, \beta=32\))，在 32K 序列长度上训练 1000 步。调整了长度缩放因子 \(\sqrt{t}=0.0707 \ln s+1\)。</li><li><strong>DeepSeek-V3:</strong> 分两阶段从 4K 扩展到 32K，再到 128K。每阶段训练 1000 步。YaRN 参数与 V2 相同，长度缩放因子 \(\sqrt{t}=0.1 \ln s+1\)。第一阶段序列长度 32K，第二阶段 128K。</li></ul><p>两模型在 NIAH 测试中均表现出良好的长上下文能力。</p><figure class=align-center><img loading=lazy src=deepseek_v2_niah.png#center alt="Fig. 19. Evaluation results on the &lsquo;Needle In A Haystack&rsquo; (NIAH) tests for DeepSeek-V2. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 19. Evaluation results on the &lsquo;Needle In A Haystack&rsquo; (NIAH) tests for DeepSeek-V2. (Image source: <a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>)</p></figcaption></figure><figure class=align-center><img loading=lazy src=deepseek_v3_niah.png#center alt="Fig. 20. Evaluation results on the &lsquo;Needle In A Haystack&rsquo; (NIAH) tests for DeepSeek-V3. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 20. Evaluation results on the &lsquo;Needle In A Haystack&rsquo; (NIAH) tests for DeepSeek-V3. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h3 id=评估>评估<a hidden class=anchor aria-hidden=true href=#评估>#</a></h3><p><strong>DeepSeek-V2 评估结果:</strong></p><p>DeepSeek-V2 与代表性开源模型对比 (部分结果)。DeepSeek-V2 以 21B 激活参数达到当时顶尖水平。</p><table><thead><tr><th style=text-align:center></th><th style=text-align:center>Benchmark (Metric)</th><th style=text-align:center># Shots</th><th style=text-align:center>DeepSeek 67B</th><th style=text-align:center>Qwen1.5 72B</th><th style=text-align:center>Mixtral 8x22B</th><th style=text-align:center>LLaMA 3 70B</th><th style=text-align:center>DeepSeek-V2</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center># Activated Params</td><td style=text-align:center>-</td><td style=text-align:center>67B</td><td style=text-align:center>72B</td><td style=text-align:center>39B</td><td style=text-align:center>70B</td><td style=text-align:center><strong>21B</strong></td></tr><tr><td style=text-align:center>English</td><td style=text-align:center>MMLU (<a href=https://arxiv.org/abs/2009.03300>Hendrycks et al., 2020</a>) (Acc.)</td><td style=text-align:center>5-shot</td><td style=text-align:center>71.3</td><td style=text-align:center>77.2</td><td style=text-align:center>77.6</td><td style=text-align:center>78.9</td><td style=text-align:center><strong>78.5</strong></td></tr><tr><td style=text-align:center>Code</td><td style=text-align:center>HumanEval (<a href=https://arxiv.org/abs/2107.03374>Chen et al., 2021</a>) (Pass@1)</td><td style=text-align:center>0-shot</td><td style=text-align:center>45.1</td><td style=text-align:center>43.9</td><td style=text-align:center>53.1</td><td style=text-align:center>48.2</td><td style=text-align:center><strong>48.8</strong></td></tr><tr><td style=text-align:center>Math</td><td style=text-align:center>GSM8K (<a href=https://arxiv.org/abs/2110.14168>Cobbe et al., 2021</a>) (EM)</td><td style=text-align:center>8-shot</td><td style=text-align:center>63.4</td><td style=text-align:center>77.9</td><td style=text-align:center>80.3</td><td style=text-align:center>83.0</td><td style=text-align:center><strong>79.2</strong></td></tr><tr><td style=text-align:center>Chinese</td><td style=text-align:center>C-Eval (<a href=https://arxiv.org/abs/2305.08322>Huang et al., 2023</a>) (Acc.)</td><td style=text-align:center>5-shot</td><td style=text-align:center>66.1</td><td style=text-align:center><strong>83.7</strong></td><td style=text-align:center>59.6</td><td style=text-align:center>67.5</td><td style=text-align:center>81.7</td></tr></tbody></table><p><strong>DeepSeek-V3 评估结果:</strong></p><p>DeepSeek-V3-Base 与代表性开源模型对比 (部分结果)。DeepSeek-V3-Base 在多数基准上成为最强开源模型，尤其在代码和数学方面。</p><table><thead><tr><th style=text-align:center></th><th style=text-align:center>Benchmark (Metric)</th><th style=text-align:center># Shots</th><th style=text-align:center>DeepSeek-V2 Base</th><th style=text-align:center>Qwen2.5 72B Base</th><th style=text-align:center>LLaMA-3.1 405B Base</th><th style=text-align:center>DeepSeek-V3 Base</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center># Activated Params</td><td style=text-align:center>-</td><td style=text-align:center>21B</td><td style=text-align:center>72B</td><td style=text-align:center>405B</td><td style=text-align:center><strong>37B</strong></td></tr><tr><td style=text-align:center>English</td><td style=text-align:center>MMLU (<a href=https://arxiv.org/abs/2009.03300>Hendrycks et al., 2020</a>) (EM)</td><td style=text-align:center>5-shot</td><td style=text-align:center>78.4</td><td style=text-align:center>85.0</td><td style=text-align:center>84.4</td><td style=text-align:center><strong>87.1</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>MMLU-Pro (<a href=https://arxiv.org/abs/2406.01574>Wang et al., 2024</a>) (em)</td><td style=text-align:center>5-shot</td><td style=text-align:center>51.4</td><td style=text-align:center>58.3</td><td style=text-align:center>52.8</td><td style=text-align:center><strong>64.4</strong></td></tr><tr><td style=text-align:center>Code</td><td style=text-align:center>HumanEval (<a href=https://arxiv.org/abs/2107.03374>Chen et al., 2021</a>) (Pass@1)</td><td style=text-align:center>0-shot</td><td style=text-align:center>43.3</td><td style=text-align:center>53.0</td><td style=text-align:center>54.9</td><td style=text-align:center><strong>65.2</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>LiveCodeBench-Base (<a href=https://arxiv.org/abs/2403.07974>Jain et al., 2024</a>) (Pass@1)</td><td style=text-align:center>3-shot</td><td style=text-align:center>11.6</td><td style=text-align:center>12.9</td><td style=text-align:center>15.5</td><td style=text-align:center><strong>19.4</strong></td></tr><tr><td style=text-align:center>Math</td><td style=text-align:center>GSM8K (<a href=https://arxiv.org/abs/2110.14168>Cobbe et al., 2021</a>) (Em)</td><td style=text-align:center>8-shot</td><td style=text-align:center>81.6</td><td style=text-align:center>88.3</td><td style=text-align:center>83.5</td><td style=text-align:center><strong>89.3</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>MATH (<a href=https://arxiv.org/abs/2103.03874>Hendrycks et al., 2021</a>) (EM)</td><td style=text-align:center>4-shot</td><td style=text-align:center>43.4</td><td style=text-align:center>54.4</td><td style=text-align:center>49.0</td><td style=text-align:center><strong>61.6</strong></td></tr><tr><td style=text-align:center>Chinese</td><td style=text-align:center>C-Eval (<a href=https://arxiv.org/abs/2305.08322>Huang et al., 2023</a>) (EM)</td><td style=text-align:center>5-shot</td><td style=text-align:center>81.4</td><td style=text-align:center>89.2</td><td style=text-align:center>72.5</td><td style=text-align:center><strong>90.1</strong></td></tr><tr><td style=text-align:center>Multilingual</td><td style=text-align:center>MMMLU-non-English (<a href=https://huggingface.co/datasets/openai/MMMLU>OpenAI, 2024</a>) (em)</td><td style=text-align:center>5-shot</td><td style=text-align:center>64.0</td><td style=text-align:center>74.8</td><td style=text-align:center>73.8</td><td style=text-align:center><strong>79.4</strong></td></tr></tbody></table><p><strong>总结:</strong> DeepSeek-V3-Base 凭借其架构创新、更大规模的训练数据和高效的训练方法，全面超越了 DeepSeek-V2-Base 和其他顶尖开源模型（包括参数量远超其激活参数的 LLaMA-3.1 405B）。</p><h2 id=对齐>对齐<a hidden class=anchor aria-hidden=true href=#对齐>#</a></h2><p>为了使模型更好地理解指令、遵循人类偏好并提升特定能力（如推理），DeepSeek-V2 和 V3 都进行了监督微调 (SFT) 和强化学习 (RL)。</p><h3 id=监督微调>监督微调<a hidden class=anchor aria-hidden=true href=#监督微调>#</a></h3><ul><li><strong>DeepSeek-V2:</strong> 使用了约 1.5M 条指令数据，包含 1.2M 帮助性数据和 0.3M 安全性数据，注重提升数据质量以减少幻觉、增强写作能力。</li><li><strong>DeepSeek-V3:</strong><ul><li><strong>推理数据:</strong> 利用内部的 DeepSeek-R1 模型 (<a href=https://arxiv.org/abs/2501.12948>Guo et al., 2025</a>) 生成推理过程（数学、代码、逻辑等）。由于 R1 输出可能过长或格式不佳，V3 采用了<strong>知识蒸馏</strong>的思路：<ol><li>训练领域专家模型（如代码专家）：结合原始 SFT 数据和 R1 生成的长 CoT 数据（带有引导反思/验证的系统提示）进行 SFT+RL 训练。</li><li>使用专家模型生成 SFT 数据：专家模型在 RL 过程中学会融合 R1 的推理模式和常规 SFT 数据的简洁性。</li><li>拒绝采样：筛选高质量 SFT 数据用于最终 V3 的 SFT。</li></ol></li><li><strong>非推理数据:</strong> 使用 DeepSeek-V2.5 生成，并由人工标注员验证。</li><li><strong>SFT 设置:</strong> 微调 2 个 epoch，学习率从 \(5 \times 10^{-6}\) 余弦衰减到 \(1 \times 10^{-6}\)。采用样本打包和掩码隔离。</li></ul></li></ul><h3 id=强化学习>强化学习<a hidden class=anchor aria-hidden=true href=#强化学习>#</a></h3><p>两者均采用 <strong>组相对策略优化 (Group Relative Policy Optimization, GRPO)</strong> 算法 (<a href=https://arxiv.org/abs/2402.03300>Shao et al., 2024</a>) 进行 RL。GRPO 是一种 Actor-Only 的方法，它通过比较一组（\(G\) 个）候选输出的相对好坏来估计优势 \(A_i\)，从而避免了训练与策略模型同样大小的 Critic 模型，节省了成本。</p><p>GRPO 目标函数：</p>\[
\begin{gathered}
\mathcal{J}_{G R P O}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_{i}\right\}_{i=1}^{G} \sim \pi_{\theta_{o l d}}(O \mid q)\right] \\
\frac{1}{G} \sum_{i=1}^{G}\left(\min \left(\frac{\pi_{\theta}\left(o_{i} \mid q\right)}{\pi_{\theta_{o l d}}\left(o_{i} \mid q\right)} A_{i}, \operatorname{clip}\left(\frac{\pi_{\theta}\left(o_{i} \mid q\right)}{\pi_{\theta_{o l d}}\left(o_{i} \mid q\right)}, 1-\varepsilon, 1+\varepsilon\right) A_{i}\right)-\beta \mathbb{D}_{K L}\left(\pi_{\theta}| | \pi_{r e f}\right)\right),
\end{gathered}
\]<p>其中优势 \(A_i\) 通过组内奖励 \(r_i\) 标准化得到：</p>\[
A_{i}=\frac{r_{i}-\operatorname{mean}\left(\left\{r_{1}, r_{2}, \cdots, r_{G}\right\}\right)}{\operatorname{std}\left(\left\{r_{1}, r_{2}, \cdots, r_{G}\right\}\right)}.
\]<p>KL 散度惩罚项使用 Schulman 无偏估计器：</p>\[
\mathbb{D}_{K L}\left(\pi_{\theta}| | \pi_{r e f}\right)=\frac{\pi_{r e f}\left(o_{i} \mid q\right)}{\pi_{\theta}\left(o_{i} \mid q\right)}-\log \frac{\pi_{r e f}\left(o_{i} \mid q\right)}{\pi_{\theta}\left(o_{i} \mid q\right)}-1.
\]<p><strong>奖励模型 (RM):</strong></p><ul><li><strong>DeepSeek-V2:</strong> 采用两阶段 RL 策略。<ol><li><strong>推理对齐:</strong> 使用专门训练的 \(RM_{\text{reasoning}}\) 对代码和数学推理任务进行优化。</li><li><strong>人类偏好对齐:</strong> 使用多奖励框架，结合 \(RM_{\text{helpful}}\)、\(RM_{\text{safety}}\) 和基于规则的 \(RM_{\text{rule}}\)。</li></ol></li><li><strong>DeepSeek-V3:</strong><ul><li><strong>基于规则的 RM:</strong> 对于可验证的任务（如数学答案格式、LeetCode 测试用例），使用规则来提供可靠奖励。</li><li><strong>基于模型的 RM:</strong> 对于自由格式答案或无标准答案的任务（如创意写作），使用从 V3 SFT Checkpoint 初始化的 RM。该 RM 通过学习带有 CoT 的偏好数据来提升可靠性，减少奖励 Hacking 风险。</li><li><strong>自奖励:</strong> V3 探索使用模型自身的判断能力（通过投票增强）作为反馈来源，特别是在通用场景下，结合 <strong>Constitutional AI</strong> (<a href=https://arxiv.org/abs/2212.08073>Bai et al., 2022</a>) 的思想进行优化。</li></ul></li></ul><p><strong>RL 训练优化 (V2/V3):</strong> 针对大模型 RL 的高资源需求，进行了工程优化，如混合引擎（训练/推理并行策略不同）、使用 <strong>vLLM</strong> (<a href=https://arxiv.org/abs/2309.06180>Kwon et al., 2023</a>) 加速采样、CPU Offloading 调度等。</p><h3 id=评估-1>评估<a hidden class=anchor aria-hidden=true href=#评估-1>#</a></h3><p><strong>DeepSeek-V2 Chat 评估:</strong></p><p>DeepSeek-V2 Chat (SFT/RL) 与代表性开源 Chat 模型在开放式生成任务上的对比。V2 Chat (RL) 在 AlpacaEval 2.0 和 AlignBench 上表现突出。</p><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>MT-Bench (<a href=https://arxiv.org/abs/2306.05685>Zheng et al., 2023</a>)</th><th style=text-align:center>AlpacaEval 2.0 (<a href=https://arxiv.org/abs/2404.04475>Dubois et al., 2024</a>) (LC Win Rate)</th><th style=text-align:center>AlignBench (<a href=https://doi.org/10.48550/arXiv.2311.18743>Liu et al., 2023</a>) (中文)</th></tr></thead><tbody><tr><td style=text-align:center>DeepSeek 67B Chat</td><td style=text-align:center>8.35</td><td style=text-align:center>16.6</td><td style=text-align:center>6.43</td></tr><tr><td style=text-align:center>Mistral 8x22B Instruct</td><td style=text-align:center>8.66</td><td style=text-align:center>30.9</td><td style=text-align:center>-</td></tr><tr><td style=text-align:center>Qwen1.5 72B Chat</td><td style=text-align:center>8.61</td><td style=text-align:center>36.6</td><td style=text-align:center>7.19</td></tr><tr><td style=text-align:center>LLaMA3 70B Instruct</td><td style=text-align:center><strong>8.95</strong></td><td style=text-align:center>34.4</td><td style=text-align:center>-</td></tr><tr><td style=text-align:center>DeepSeek-V2 Chat (SFT)</td><td style=text-align:center>8.62</td><td style=text-align:center>30.0</td><td style=text-align:center>7.74</td></tr><tr><td style=text-align:center>DeepSeek-V2 Chat (RL)</td><td style=text-align:center><strong>8.97</strong></td><td style=text-align:center><strong>38.9</strong></td><td style=text-align:center><strong>7.91</strong></td></tr></tbody></table><p><strong>DeepSeek-V3 Chat 评估:</strong></p><p>DeepSeek-V3 Chat 与代表性开源及闭源 Chat 模型对比 (部分结果)。V3 在多数基准上领先开源模型，并在代码、数学、中文及开放式生成任务上与顶尖闭源模型相当。</p><table><thead><tr><th style=text-align:center></th><th style=text-align:center>Benchmark (Metric)</th><th style=text-align:center>DeepSeek V2.5-0905</th><th style=text-align:center>Qwen2.5 72B-Inst.</th><th style=text-align:center>LLaMA-3.1 405B-Inst.</th><th style=text-align:center>Claude-3.5- Sonnet-1022</th><th style=text-align:center>GPT-4o 0513</th><th style=text-align:center>DeepSeek V3</th></tr></thead><tbody><tr><td style=text-align:center>English</td><td style=text-align:center>MMLU (<a href=https://arxiv.org/abs/2009.03300>Hendrycks et al., 2020</a>) (EM)</td><td style=text-align:center>80.6</td><td style=text-align:center>85.3</td><td style=text-align:center>88.6</td><td style=text-align:center>88.3</td><td style=text-align:center>87.2</td><td style=text-align:center><strong>88.5</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>MMLU-Pro (<a href=https://arxiv.org/abs/2406.01574>Wang et al., 2024</a>) (EM)</td><td style=text-align:center>66.2</td><td style=text-align:center>71.6</td><td style=text-align:center>73.3</td><td style=text-align:center><strong>78.0</strong></td><td style=text-align:center>72.6</td><td style=text-align:center>75.9</td></tr><tr><td style=text-align:center></td><td style=text-align:center>GPQA-Diamond (<a href=https://arxiv.org/abs/2311.12022>Rein et al., 2023</a>) (Pass@1)</td><td style=text-align:center>41.3</td><td style=text-align:center>49.0</td><td style=text-align:center>51.1</td><td style=text-align:center><strong>65.0</strong></td><td style=text-align:center>49.9</td><td style=text-align:center>59.1</td></tr><tr><td style=text-align:center></td><td style=text-align:center>SimpleQA (<a href=https://openai.com/index/introducing-simpleqa/>OpenAI, 2024c</a>) (Correct)</td><td style=text-align:center>10.2</td><td style=text-align:center>9.1</td><td style=text-align:center>17.1</td><td style=text-align:center>28.4</td><td style=text-align:center><strong>38.2</strong></td><td style=text-align:center>24.9</td></tr><tr><td style=text-align:center>Code</td><td style=text-align:center>HumanEval-Mul (Pass@1)</td><td style=text-align:center>77.4</td><td style=text-align:center>77.3</td><td style=text-align:center>77.2</td><td style=text-align:center>81.7</td><td style=text-align:center>80.5</td><td style=text-align:center><strong>82.6</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>LiveCodeBench (<a href=https://arxiv.org/abs/2403.07974>Jain et al., 2024</a>) (Pass@1-COT)</td><td style=text-align:center>29.2</td><td style=text-align:center>31.1</td><td style=text-align:center>28.4</td><td style=text-align:center>36.3</td><td style=text-align:center>33.4</td><td style=text-align:center><strong>40.5</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>SWE Verified (<a href=https://openai.com/index/introducing-swe-bench-verified/>OpenAI, 2024d</a>) (Resolved)</td><td style=text-align:center>22.6</td><td style=text-align:center>23.8</td><td style=text-align:center>24.5</td><td style=text-align:center><strong>50.8</strong></td><td style=text-align:center>38.8</td><td style=text-align:center>42.0</td></tr><tr><td style=text-align:center>Math</td><td style=text-align:center>AIME 2024 (<a href="https://artofproblemsolving.com/wiki/index.php/2024_AIME_I?srsltid=AfmBOooril84-FGuAUnzl8I-zXl8XG7P00X-BAkMG9x9RIzEWcXHlwWm">MAA, 2024</a> (Pass@1)</td><td style=text-align:center>16.7</td><td style=text-align:center>23.3</td><td style=text-align:center>23.3</td><td style=text-align:center>16.0</td><td style=text-align:center>9.3</td><td style=text-align:center><strong>39.2</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>MATH-500 (<a href=https://arxiv.org/abs/2103.03874>Hendrycks et al., 2021</a>) (ЕМ)</td><td style=text-align:center>74.7</td><td style=text-align:center>80.0</td><td style=text-align:center>73.8</td><td style=text-align:center>78.3</td><td style=text-align:center>74.6</td><td style=text-align:center><strong>90.2</strong></td></tr><tr><td style=text-align:center>Chinese</td><td style=text-align:center>C-Eval (<a href=https://arxiv.org/abs/2305.08322>Huang et al., 2023</a>) (EM)</td><td style=text-align:center>79.5</td><td style=text-align:center>86.1</td><td style=text-align:center>61.5</td><td style=text-align:center>76.7</td><td style=text-align:center>76.0</td><td style=text-align:center><strong>86.5</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>C-SimpleQA (<a href=https://arxiv.org/abs/2411.07140>He et al., 2024</a>) (Correct)</td><td style=text-align:center>54.1</td><td style=text-align:center>48.4</td><td style=text-align:center>50.4</td><td style=text-align:center>51.3</td><td style=text-align:center>59.3</td><td style=text-align:center><strong>64.8</strong></td></tr><tr><td style=text-align:center>Open-Ended</td><td style=text-align:center>Arena-Hard (<a href=https://arxiv.org/abs/2406.11939>Li et al., 2024</a>)</td><td style=text-align:center>76.2</td><td style=text-align:center>81.2</td><td style=text-align:center>69.3</td><td style=text-align:center>85.2</td><td style=text-align:center>80.4</td><td style=text-align:center><strong>85.5</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>AlpacaEval 2.0 (<a href=https://arxiv.org/abs/2404.04475>Dubois et al., 2024</a>) (LC Win Rate)</td><td style=text-align:center>50.5</td><td style=text-align:center>49.1</td><td style=text-align:center>40.5</td><td style=text-align:center>52.0</td><td style=text-align:center>51.1</td><td style=text-align:center><strong>70.0</strong></td></tr></tbody></table><p><strong>总结:</strong></p><ul><li>DeepSeek-V2 Chat (RL) 在发布时已是顶尖的开源聊天模型，尤其在 AlpacaEval 和中文 AlignBench 上表现优异。</li><li>DeepSeek-V3 Chat 进一步提升了性能，成为目前最强的开源聊天模型，在代码、数学、中文知识以及 Arena-Hard (<a href=https://arxiv.org/abs/2406.11939>Li et al., 2024</a>)、AlpacaEval 等开放式评估中表现极其亮眼，达到了与 GPT-4o、Claude-3.5-Sonnet 相媲美的水平。</li><li>V3 的 R1 蒸馏显著提升了推理能力，但也可能增加响应长度，需要在准确性和效率间权衡。</li><li>V3 的自奖励能力（在 RewardBench (<a href=https://arxiv.org/abs/2403.13787>Lambert et al., 2024</a>) 上表现优异）为其持续对齐提供了有效途径。</li></ul><h2 id=讨论>讨论<a hidden class=anchor aria-hidden=true href=#讨论>#</a></h2><ul><li><strong>负载均衡策略演进:</strong> 从 V2 的辅助损失到 V3 的无辅助损失+偏置调整，体现了在保证负载均衡的同时，尽量减少对模型性能本身干扰的趋势。批处理级别的均衡相比序列级均衡，更能促进专家特化。</li><li><strong>MTP 的有效性:</strong> V3 的实验证明，多 token 预测作为辅助训练目标，确实能提升模型在标准评估任务上的性能，同时为推理加速（推测解码）提供了可能。</li><li><strong>R1 蒸馏:</strong> V3 成功地将 DeepSeek-R1 的长链推理能力蒸馏到标准 LLM 中，显著提升了数学和代码能力。这是一个重要的技术方向，但也需要注意控制生成长度。</li><li><strong>自奖励:</strong> V3 强大的判断能力（<strong>RewardBench</strong>(<a href=https://arxiv.org/abs/2403.13787>Lambert et al., 2024</a>)）结果使其能有效进行自反馈和自对齐，这对于减少对人类标注的依赖、实现模型持续自我提升至关重要。</li><li><strong>SFT 数据量:</strong> 虽然在<strong>LIMA</strong>(<a href=https://arxiv.org/abs/2305.11206>Zhou et al., 2024</a>）认为少量高质量 SFT 数据即可达到不错的效果，但对于特定技能（如指令遵循 IFEval），仍需足够数据量的高质量数据才能达到满意效果。</li><li><strong>对齐税:</strong> OpenAI 在<strong>InstructGPT</strong> (<a href=https://arxiv.org/pdf/2203.02155>Ouyang et al., 2022</a>)中指出 RL 对齐在提升开放式生成能力的同时，可能牺牲部分标准基准的性能。V2 和 V3 都努力在数据处理和训练策略上缓解此问题，以达到可接受的平衡。</li></ul><h2 id=结论局限性与未来方向>结论、局限性与未来方向<a hidden class=anchor aria-hidden=true href=#结论局限性与未来方向>#</a></h2><h3 id=结论>结论<a hidden class=anchor aria-hidden=true href=#结论>#</a></h3><p>DeepSeek-V2 和 DeepSeek-V3 是两款强大、经济且高效的 MoE 语言模型。它们通过 MLA 和 DeepSeekMoE 架构创新，以及 V3 引入的无辅助损失负载均衡、MTP、FP8 训练和 R1 蒸馏等技术，在性能、训练成本和推理效率上取得了突破。DeepSeek-V3 已成为当前最强的开源模型之一，性能可与顶尖闭源模型竞争。</p><h3 id=局限性>局限性<a hidden class=anchor aria-hidden=true href=#局限性>#</a></h3><ul><li><strong>通用 LLM 局限:</strong> 如知识截止、幻觉、事实性错误等。</li><li><strong>语言覆盖:</strong> 主要针对中英文，其他语言能力有限 (V2)。V3 扩展了多语言，但仍以中英文为主。</li><li><strong>部署门槛 (V3):</strong> 高效推理需要较大的部署单元（多节点），对小型团队可能有挑战。</li><li><strong>推理效率:</strong> 虽然 V3 推理效率相比 V2 有提升，但仍有优化空间。</li></ul><h3 id=未来方向>未来方向<a hidden class=anchor aria-hidden=true href=#未来方向>#</a></h3><ul><li><strong>架构创新:</strong> 持续优化 MoE 架构，探索支持无限上下文、突破 Transformer 限制的新架构。</li><li><strong>数据扩展:</strong> 提升训练数据的数量、质量和维度（多模态等）。</li><li><strong>深度思考:</strong> 增强模型的推理长度和深度，提升智能水平。</li><li><strong>评估方法:</strong> 发展更全面、多维度的评估方法，避免过拟合特定基准。</li><li><strong>对齐与安全:</strong> 持续改进对齐技术（如自奖励），确保模型有用、诚实、无害，与人类价值观对齐。</li></ul><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><p>[1] Liu, Aixin, et al. <a href=https://arxiv.org/abs/2405.04434>&ldquo;Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.&rdquo;</a> arXiv preprint arXiv:2405.04434 (2024).</p><p>[2] Liu, Aixin, et al. <a href=https://arxiv.org/abs/2412.19437>&ldquo;Deepseek-v3 technical report.&rdquo;</a> arXiv preprint arXiv:2412.19437 (2024).</p><p>[3] Dai, Damai, et al. <a href=https://arxiv.org/abs/2401.06066>&ldquo;Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.&rdquo;</a> arXiv preprint arXiv:2401.06066 (2024).</p><p>[4] Wang, Lean, et al. <a href=https://arxiv.org/abs/2408.15664>&ldquo;Auxiliary-loss-free load balancing strategy for mixture-of-experts.&rdquo;</a> arXiv preprint arXiv:2408.15664 (2024).</p><p>[5] Gloeckle, Fabian, et al. <a href=https://arxiv.org/abs/2404.19737>&ldquo;Better & faster large language models via multi-token prediction.&rdquo;</a> Proceedings of the 41st International Conference on Machine Learning. PMLR 235:16821-16841 (2024).</p><p>[6] Vaswani, Ashish, et al. <a href=https://arxiv.org/abs/1706.03762>&ldquo;Attention is all you need.&rdquo;</a> Advances in neural information processing systems 30 (2017).</p><p>[7] Shazeer, Noam. <a href=https://arxiv.org/abs/1911.02150>&ldquo;Fast transformer decoding: One write-head is all you need.&rdquo;</a> arXiv preprint arXiv:1911.02150 (2019).</p><p>[8] Ainslie, Joshua, et al. <a href=https://arxiv.org/abs/2305.13245>&ldquo;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.&rdquo;</a> Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4895-4901 (2023).</p><p>[9] Su, Jianlin, et al. <a href=https://arxiv.org/abs/2104.09864>&ldquo;Roformer: Enhanced transformer with rotary position embedding.&rdquo;</a> Neurocomputing 568 (2024): 127063.</p><p>[10] Shazeer, Noam, et al. <a href=https://arxiv.org/abs/1701.06538>&ldquo;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.&rdquo;</a> arXiv preprint arXiv:1701.06538 (2017).</p><p>[11] Lepikhin, Dmitry, et al. <a href=https://arxiv.org/abs/2006.16668>&ldquo;Gshard: Scaling giant models with conditional computation and automatic sharding.&rdquo;</a> arXiv preprint arXiv:2006.16668 (2020).</p><p>[12] Fedus, William, Barret Zoph, and Noam Shazeer. <a href=https://arxiv.org/abs/2101.03961>&ldquo;Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.&rdquo;</a> The Journal of Machine Learning Research 23.1: 5232-5270 (2022).</p><p>[13] Zhou, Zexuan, et al. <a href=https://arxiv.org/abs/2202.09368>&ldquo;Mixture-of-experts with expert choice routing.&rdquo;</a> Advances in Neural Information Processing Systems 35: 7103-7114 (2022).</p><p>[14] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. <a href=https://arxiv.org/abs/2211.17192>&ldquo;Fast inference from transformers via speculative decoding.&rdquo;</a> Proceedings of the 40th International Conference on Machine Learning. PMLR 202:19274-19286 (2023).</p><p>[15] Xia, Yichao, et al. <a href=https://arxiv.org/abs/2302.01318>&ldquo;Accelerating large language model decoding with speculative sampling.&rdquo;</a> arXiv preprint arXiv:2302.01318 (2023).</p><p>[16] Qi, Hai, et al. <a href=https://arxiv.org/abs/2401.10241>&ldquo;ZeroBubble: A High-Performance Framework for Training Mixture-of-Experts Models.&rdquo;</a> arXiv preprint arXiv:2401.10241 (2024).</p><p>[17] Rajbhandari, Samyam, et al. <a href=https://arxiv.org/abs/1910.02054>&ldquo;Zero: Memory optimizations toward training trillion parameter models.&rdquo;</a> SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE (2020).</p><p>[18] Harlap, Aaron, et al. <a href=https://arxiv.org/abs/1806.03377>&ldquo;Pipedream: Fast and efficient pipeline parallel dnn training.&rdquo;</a> arXiv preprint arXiv:1806.03377 (2018).</p><p>[19] Li, Shigang, and Torsten Hoefler. <a href=https://arxiv.org/abs/2107.06925>&ldquo;Chimera: Efficiently training large-scale neural networks with bidirectional pipelines.&rdquo;</a> Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.</p><p>[20] Bauer, Michael, Sean Treichler, and Alex Aiken. <a href=https://dl.acm.org/doi/10.1145/2692916.2555258>&ldquo;Singe: Leveraging warp specialization for high performance on gpus.&rdquo;</a> Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming. 2014.</p><p>[21] Dettmers, Tim, et al. <a href=https://arxiv.org/abs/2208.07339>&ldquo;Llm. int8 (): 8-bit matrix multiplication for transformers at scale.&rdquo;</a> Advances in Neural Information Processing Systems 35: 34138-34151 (2022).</p><p>[22] Noune, Badreddine, et al. <a href=https://arxiv.org/abs/2206.02915>&ldquo;8-bit numerical formats for deep neural networks.&rdquo;</a> arXiv preprint arXiv:2206.02915 (2022).</p><p>[23] Peng, Houwen, et al. <a href=https://arxiv.org/abs/2310.18313>&ldquo;FP8-LM: Training FP8 Large Language Models.&rdquo;</a> arXiv preprint arXiv:2310.18313 (2023).</p><p>[24] Fishman, Maxim, et al. <a href=https://arxiv.org/abs/2409.12517>&ldquo;Scaling FP8 training to trillion-token LLMs.&rdquo;</a>) arXiv preprint arXiv:2409.12517 (2024).</p><p>[25] He, Bobby, et al. <a href=https://arxiv.org/abs/2405.19279>&ldquo;Understanding and minimising outlier features in neural network training.&rdquo;</a> arXiv preprint arXiv:2405.19279 (2024).</p><p>[26] Sun, Xiao, et al. <a href=https://arxiv.org/abs/2402.17762>&ldquo;Massive activations in large language models.&rdquo;</a> arXiv preprint arXiv:2402.17762 (2024).</p><p>[27] NVIDIA. <a href=https://github.com/NVIDIA/TransformerEngine>&ldquo;Transformer Engine.&rdquo;</a> GitHub Repository (Accessed 2024).</p><p>[28] Sun, Xiao, et al. <a href=https://papers.nips.cc/paper_files/paper/2019/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html>&ldquo;Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks.&rdquo;</a> Advances in neural information processing systems 32 (2019).</p><p>[29] Loshchilov, Ilya, and Frank Hutter. <a href=https://arxiv.org/abs/1711.05101>&ldquo;Decoupled weight decay regularization.&rdquo;</a> arXiv preprint arXiv:1711.05101 (2017).</p><p>[30] NVIDIA. <a href=https://developer.nvidia.com/blog/gpudirect-storage/>&ldquo;GPUDirect Storage: A Direct Path Between Storage and GPU Memory.&rdquo;</a> NVIDIA Developer Blog (2022).</p><p>[31] Graham, Richard L., et al. <a href=https://network.nvidia.com/pdf/solutions/hpc/paperieee_copyright.pdf>&ldquo;Scalable hierarchical aggregation protocol (SHArP): A hardware architecture for efficient data reduction.&rdquo;</a> 2016 First International Workshop on Communication Optimizations in HPC (COMHPC). IEEE, 2016.</p><p>[32] Ding, Yiran, et al. <a href=https://arxiv.org/abs/2402.13753>&ldquo;Longrope: Extending llm context window beyond 2 million tokens.&rdquo;</a> arXiv preprint arXiv:2402.13753 (2024).</p><p>[33] Zhu, Qihao, et al. <a href=https://arxiv.org/abs/2406.11931>&ldquo;DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence.&rdquo;</a> arXiv preprint arXiv:2406.11931 (2024).</p><p>[34] Lundberg, Scott M. <a href=https://github.com/guidance-ai/guidance/blob/main/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb>&ldquo;Guidance: Prompt Boundaries and Token Healing.&rdquo;</a> GitHub Notebook (2023).</p><p>[35] Peng, Bowen, et al. <a href=https://arxiv.org/abs/2309.00071>&ldquo;YaRN: Efficient Context Window Extension of Large Language Models.&rdquo;</a> arXiv preprint arXiv:2309.00071 (2023).</p><p>[36] Hendrycks, Dan, et al. <a href=https://arxiv.org/abs/2009.03300>&ldquo;Measuring massive multitask language understanding.&rdquo;</a> arXiv preprint arXiv:2009.03300 (2020).</p><p>[37] Chen, Mark, et al. <a href=https://arxiv.org/abs/2107.03374>&ldquo;Evaluating large language models trained on code.&rdquo;</a> arXiv preprint arXiv:2107.03374 (2021).</p><p>[38] Cobbe, Karl, et al. <a href=https://arxiv.org/abs/2110.14168>&ldquo;Training verifiers to solve math word problems.&rdquo;</a> arXiv preprint arXiv:2110.14168 (2021).</p><p>[39] Huang, Yuzhen, et al. <a href=https://arxiv.org/abs/2305.08322>&ldquo;C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 62991-63010.</p><p>[40] Wang, Yubo, et al. <a href=https://arxiv.org/abs/2406.01574>&ldquo;Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.&rdquo;</a> The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024.</p><p>[41] Jain, Naman, et al. <a href=https://arxiv.org/abs/2403.07974>&ldquo;LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code.&rdquo;</a> arXiv preprint arXiv:2403.07974 (2024).</p><p>[42] Hendrycks, Dan, et al. <a href=https://arxiv.org/abs/2103.03874>&ldquo;Measuring mathematical problem solving with the math dataset.&rdquo;</a> arXiv preprint arXiv:2103.03874 (2021).</p><p>[43] OpenAI. <a href=https://huggingface.co/datasets/openai/MMMLU>&ldquo;MMMLU Dataset.&rdquo;</a> Hugging Face Datasets (Accessed 2024).</p><p>[44] Guo, Daya, et al. <a href=https://arxiv.org/abs/2501.12948>&ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.&rdquo;</a> arXiv preprint arXiv:2501.12948 (2025).</p><p>[45] Shao, Zhihong, et al. <a href=https://arxiv.org/abs/2402.03300>&ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.&rdquo;</a> arXiv preprint arXiv:2402.03300 (2024).</p><p>[46] Bai, Yuntao, et al. <a href=https://arxiv.org/abs/2212.08073>&ldquo;Constitutional ai: Harmlessness from ai feedback.&rdquo;</a> arXiv preprint arXiv:2212.08073 (2022).</p><p>[47] Kwon, Woosuk, et al. <a href=https://arxiv.org/abs/2309.06180>&ldquo;Efficient memory management for large language model serving with pagedattention.&rdquo;</a> Proceedings of the 29th Symposium on Operating Systems Principles. 2023.</p><p>[48] Zheng, Lianmin, et al. <a href=https://arxiv.org/abs/2306.05685>&ldquo;Judging llm-as-a-judge with mt-bench and chatbot arena.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 46595-46623.</p><p>[49] Dubois, Yann, et al. <a href=https://arxiv.org/abs/2404.04475>&ldquo;Length-controlled alpacaeval: A simple way to debias automatic evaluators.&rdquo;</a> arXiv preprint arXiv:2404.04475 (2024).</p><p>[50] Liu, Xiao, et al. <a href=https://arxiv.org/abs/2311.18743>&ldquo;Alignbench: Benchmarking chinese alignment of large language models.&rdquo;</a> arXiv preprint arXiv:2311.18743 (2023).</p><p>[51] Rein, David, et al. <a href=https://arxiv.org/abs/2311.12022>&ldquo;GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark.&rdquo;</a> First Conference on Language Modeling. 2024.</p><p>[52] OpenAI. <a href=https://openai.com/index/introducing-simpleqa/>&ldquo;Introducing SimpleQA&rdquo;</a> OpenAI Blog (2024).</p><p>[53] OpenAI. <a href=https://openai.com/index/introducing-swe-bench-verified/>&ldquo;Introducing SWE-bench Verified
&ldquo;</a> OpenAI Blog (2024).</p><p>[54] Mathematical Association of America (MAA). <a href=https://artofproblemsolving.com/wiki/index.php/2024_AIME_I>&ldquo;2024 AIME I Problems.&rdquo;</a> Art of Problem Solving Wiki (2024).</p><p>[55] Li, Tianle, et al. <a href=https://arxiv.org/abs/2406.11939>&ldquo;From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.&rdquo;</a> arXiv preprint arXiv:2406.11939 (2024).</p><p>[56] Lambert, Nathan, et al. <a href=https://arxiv.org/abs/2403.13787>&ldquo;RewardBench: Evaluating Reward Models for Language Modeling.&rdquo;</a> arXiv preprint arXiv:2403.13787 (2024).</p><p>[57] Zhou, Chunting, et al. <a href=https://arxiv.org/abs/2305.11206>&ldquo;Lima: Less is more for alignment.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 55006-55021.</p><p>[58] Ouyang, Long, et al. <a href=https://arxiv.org/abs/2203.02155>&ldquo;Training language models to follow instructions with human feedback.&rdquo;</a> Advances in neural information processing systems 35 (2022): 27730-27744.</p><h2 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h2><blockquote><p><strong>引用</strong>：转载或引用本文内容时，请注明原作者和来源。</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Apr 2025). DeepSeek-V2 vs V3.
<a href=https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3>https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025deepseekv2v3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;DeepSeek-V2 vs V3&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Apr&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/zh/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/zh/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/zh/tags/deepseek-v2/>DeepSeek-V2</a></li><li><a href=https://syhya.github.io/zh/tags/deepseek-v3/>DeepSeek-V3</a></li><li><a href=https://syhya.github.io/zh/tags/moe/>MoE</a></li><li><a href=https://syhya.github.io/zh/tags/transformer/>Transformer</a></li><li><a href=https://syhya.github.io/zh/tags/mla/>MLA</a></li><li><a href=https://syhya.github.io/zh/tags/deepseekmoe/>DeepSeekMoE</a></li><li><a href=https://syhya.github.io/zh/tags/mtp/>MTP</a></li><li><a href=https://syhya.github.io/zh/tags/fp8-training/>FP8 Training</a></li><li><a href=https://syhya.github.io/zh/tags/grpo/>GRPO</a></li><li><a href=https://syhya.github.io/zh/tags/sft/>SFT</a></li><li><a href=https://syhya.github.io/zh/tags/rl/>RL</a></li><li><a href=https://syhya.github.io/zh/tags/kv-cache/>KV Cache</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/zh/posts/2025-05-04-multimodal-llm/><span class=title>« 上一页</span><br><span>多模态大语言模型</span>
</a><a class=next href=https://syhya.github.io/zh/posts/2025-04-06-llama/><span class=title>下一页 »</span><br><span>LLaMA 系列模型</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on x" href="https://x.com/intent/tweet/?text=DeepSeek-V2%20vs%20V3&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-04-18-deepseek-v2-v3%2f&amp;hashtags=DeepLearning%2cAI%2cLLM%2cDeepSeek-V2%2cDeepSeek-V3%2cMoE%2cTransformer%2cMLA%2cDeepSeekMoE%2cMTP%2cFP8Training%2cGRPO%2cSFT%2cRL%2cKVCache"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-04-18-deepseek-v2-v3%2f&amp;title=DeepSeek-V2%20vs%20V3&amp;summary=DeepSeek-V2%20vs%20V3&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-04-18-deepseek-v2-v3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-04-18-deepseek-v2-v3%2f&title=DeepSeek-V2%20vs%20V3"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-04-18-deepseek-v2-v3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on whatsapp" href="https://api.whatsapp.com/send?text=DeepSeek-V2%20vs%20V3%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-04-18-deepseek-v2-v3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on telegram" href="https://telegram.me/share/url?text=DeepSeek-V2%20vs%20V3&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-04-18-deepseek-v2-v3%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on ycombinator" href="https://news.ycombinator.com/submitlink?t=DeepSeek-V2%20vs%20V3&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-04-18-deepseek-v2-v3%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>