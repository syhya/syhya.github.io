<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DeepSeek-V3.2 Series | Yue Shui Blog</title><meta name=keywords content="Deep Learning,LLM,DeepSeek,Sparse Attention,Reinforcement Learning,Reasoning,Agent,Theorem Proving"><meta name=description content="By introducing DeepSeek Sparse Attention (DSA), a scalable reinforcement learning framework, and a large-scale agentic task synthesis pipeline, DeepSeek-V3.2 (DeepSeek-AI, 2025) achieves reasoning capabilities and agent performance comparable to GPT-5.

      
          
      
              Fig. 1. Benchmark of DeepSeek-V3.2 and its counterparts. (Image source: DeepSeek-AI, 2025)
          

DeepSeek Sparse Attention

      
          
      
              Fig. 2. Attention architecture of DeepSeek-V3.2, where DSA is instantiated under MLA. (Image source: DeepSeek-AI, 2025)"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-12-31-deepseekv3.2/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-12-31-deepseekv3.2/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-12-31-deepseekv3.2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-12-31-deepseekv3.2/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="DeepSeek-V3.2 Series"><meta property="og:description" content="By introducing DeepSeek Sparse Attention (DSA), a scalable reinforcement learning framework, and a large-scale agentic task synthesis pipeline, DeepSeek-V3.2 (DeepSeek-AI, 2025) achieves reasoning capabilities and agent performance comparable to GPT-5.
Fig. 1. Benchmark of DeepSeek-V3.2 and its counterparts. (Image source: DeepSeek-AI, 2025)
DeepSeek Sparse Attention Fig. 2. Attention architecture of DeepSeek-V3.2, where DSA is instantiated under MLA. (Image source: DeepSeek-AI, 2025)"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-31T12:00:00+08:00"><meta property="article:modified_time" content="2025-12-31T12:00:00+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="LLM"><meta property="article:tag" content="DeepSeek"><meta property="article:tag" content="Sparse Attention"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Reasoning"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="DeepSeek-V3.2 Series"><meta name=twitter:description content="By introducing DeepSeek Sparse Attention (DSA), a scalable reinforcement learning framework, and a large-scale agentic task synthesis pipeline, DeepSeek-V3.2 (DeepSeek-AI, 2025) achieves reasoning capabilities and agent performance comparable to GPT-5.

      
          
      
              Fig. 1. Benchmark of DeepSeek-V3.2 and its counterparts. (Image source: DeepSeek-AI, 2025)
          

DeepSeek Sparse Attention

      
          
      
              Fig. 2. Attention architecture of DeepSeek-V3.2, where DSA is instantiated under MLA. (Image source: DeepSeek-AI, 2025)"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"DeepSeek-V3.2 Series","item":"https://syhya.github.io/posts/2025-12-31-deepseekv3.2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DeepSeek-V3.2 Series","name":"DeepSeek-V3.2 Series","description":"By introducing DeepSeek Sparse Attention (DSA), a scalable reinforcement learning framework, and a large-scale agentic task synthesis pipeline, DeepSeek-V3.2 (DeepSeek-AI, 2025) achieves reasoning capabilities and agent performance comparable to GPT-5.\nFig. 1. Benchmark of DeepSeek-V3.2 and its counterparts. (Image source: DeepSeek-AI, 2025)\nDeepSeek Sparse Attention Fig. 2. Attention architecture of DeepSeek-V3.2, where DSA is instantiated under MLA. (Image source: DeepSeek-AI, 2025)\n","keywords":["Deep Learning","LLM","DeepSeek","Sparse Attention","Reinforcement Learning","Reasoning","Agent","Theorem Proving"],"articleBody":"By introducing DeepSeek Sparse Attention (DSA), a scalable reinforcement learning framework, and a large-scale agentic task synthesis pipeline, DeepSeek-V3.2 (DeepSeek-AI, 2025) achieves reasoning capabilities and agent performance comparable to GPT-5.\nFig. 1. Benchmark of DeepSeek-V3.2 and its counterparts. (Image source: DeepSeek-AI, 2025)\nDeepSeek Sparse Attention Fig. 2. Attention architecture of DeepSeek-V3.2, where DSA is instantiated under MLA. (Image source: DeepSeek-AI, 2025)\nTraditional attention mechanisms assume that every token needs to attend to all historical tokens. However, from an information-theoretic perspective, effective information in text is highly unevenly distributed, with only a minority of historical tokens being truly relevant to the current token. While sliding window attention recognizes this by limiting attention to recent windows to simplify computation, it risks losing critical long-range dependencies. DeepSeek’s core insight is to enable the model to autonomously learn and dynamically select truly important tokens, achieving a better balance between efficiency and information retention.\nLightning Indexer The core of DSA is using a lightweight Lightning Indexer to quickly filter relevant tokens. For each query token $\\mathbf{h}_t$, the indexer computes relevance scores with all preceding tokens $\\mathbf{h}_s$:\n$$ I_{t,s} = \\sum_{j=1}^{H^I} w_{t,j}^I \\cdot \\text{ReLU}\\left(\\mathbf{q}_{t,j}^I \\cdot \\mathbf{k}_s^I\\right) $$Several design aspects are noteworthy:\nMulti-head indexing: Uses $H^I$ indexer heads, each learning different relevance patterns ReLU activation: Zeros out negative correlations, providing sparsity Learnable weights: $w_{t,j}^I$ determines each head’s contribution, allowing dynamic model adjustment The indexer’s computational cost is far lower than the main attention. The paper mentions it can be implemented in FP8, significantly reducing computational overhead while maintaining precision.\nFine-grained Token Selection With relevance scores computed, the model only needs to select the top-k most relevant tokens:\n$$ \\mathbf{u}_t = \\text{Attn}\\left(\\mathbf{h}_t, \\left\\{\\mathbf{c}_s \\mid I_{t,s} \\in \\text{Top-k}(I_{t,:})\\right\\}\\right) $$Here, $k$ is set to 2048 to balance efficiency and effectiveness. This setting reduces attention computation complexity from $O(L^2)$ to $O(Lk)$, significantly reducing computational overhead while still covering most critical dependencies.\nContinued Pre-training Continued pre-training from DeepSeek-V3.1-Terminus consists of two training stages:\nDense Warm-up Stage\nMaintains the original dense attention, training only the indexer. The goal is to align the indexer’s output distribution with the true attention distribution:\n$$ \\mathcal{L}^I = \\sum_t D_{\\text{KL}}\\left(p_{t,:} \\| \\text{Softmax}(I_{t,:})\\right) $$Here, $p_{t,:}$ represents the true attention distribution aggregated from main attention scores. This stage ensures the indexer learns to identify which historical tokens are most important for modeling at the current time step through alignment with the indexer output distribution.\nLearning rate: $10^{-3}$ Training steps: 1000 steps Per step: 16 sequences × 128K tokens Total tokens: 2.1B Sparse Training Stage\nAfter indexer warm-up, introduces fine-grained token selection mechanism and jointly optimizes all model parameters to adapt to DSA’s sparse attention computation pattern. In this stage, the indexer only aligns with the main attention distribution on the selected subset of key tokens, with loss function defined as:\n$$ \\mathcal{L}^I = \\sum_t \\mathbb{D}_{\\mathrm{KL}}\\left(p_{t,\\mathcal{S}_t} \\| \\operatorname{Softmax}\\left(I_{t,\\mathcal{S}_t}\\right)\\right) $$$$ \\mathcal{S}_t=\\left\\{s \\mid I_{t, s} \\in \\operatorname{Top}-\\mathrm{k}\\left(I_{t,:}\\right)\\right\\} $$where $\\mathcal{S}_t$ represents the set of top-k key-value tokens predicted as most important by the indexer at time step $t$.\nLearning rate: $7.3 \\times 10^{-6}$ Selects 2048 key-value tokens per query token Training steps: 15000 steps Per step: 480 sequences × 128K tokens Total tokens: 943.7B Inference Costs DeepSeek-V3.2 requires less computation compared to MLA in DeepSeek-V3.1-Terminus, with benefits becoming more pronounced as context length increases.\nActual cost comparison on H800 GPU clusters (calculated at $2/GPU hour):\nFig. 3. Inference costs of DeepSeek-V3.1-Terminus and DeepSeek-V3.2 on H800 clusters. (Image source: DeepSeek-AI, 2025)\nScaling GRPO DeepSeek-V3.2 merges reasoning, agent, and human alignment training into a single RL stage. This approach effectively balances performance across diverse domains while circumventing catastrophic forgetting issues common in multi-stage training paradigms.\nReward design:\nReasoning and agent tasks: Rule-based outcome reward + length penalty + language consistency reward General tasks: Generative reward model where each prompt has its own evaluation rubrics GRPO GRPO is an efficient RL algorithm proposed by DeepSeek that replaces the value model in traditional PPO with group-relative advantage estimation. GRPO optimizes the policy model $\\pi_\\theta$ by maximizing the following objective:\n$$ \\begin{aligned} \\mathcal{J}_{\\mathrm{GRPO}}(\\theta)= \u0026 \\mathbb{E}_{q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\mathrm{old}}(\\cdot \\mid q)}\\left[\\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|}\\right. \\\\ \u0026 \\left.\\min \\left(r_{i, t}(\\theta) \\hat{A}_{i, t}, \\operatorname{clip}\\left(r_{i, t}(\\theta), 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right)-\\beta \\mathbb{D}_{\\mathrm{KL}}\\left(\\pi_\\theta\\left(o_{i, t}\\right) \\| \\pi_{\\mathrm{ref}}\\left(o_{i, t}\\right)\\right)\\right], \\end{aligned} $$where the importance sampling ratio is: $$ r_{i, t}(\\theta)=\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,","wordCount":"2917","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-12-31T12:00:00+08:00","dateModified":"2025-12-31T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-12-31-deepseekv3.2/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">DeepSeek-V3.2 Series</h1><div class=post-meta><span title='2025-12-31 12:00:00 +0800 +0800'>Created:&nbsp;2025-12-31</span>&nbsp;·&nbsp;Updated:&nbsp;2025-12-31&nbsp;·&nbsp;14 min&nbsp;·&nbsp;2917 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-12-31-deepseekv3.2/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#deepseek-sparse-attention>DeepSeek Sparse Attention</a><ul><li><a href=#lightning-indexer>Lightning Indexer</a></li><li><a href=#fine-grained-token-selection>Fine-grained Token Selection</a></li><li><a href=#continued-pre-training>Continued Pre-training</a></li><li><a href=#inference-costs>Inference Costs</a></li></ul></li><li><a href=#scaling-grpo>Scaling GRPO</a><ul><li><a href=#grpo>GRPO</a></li><li><a href=#key-strategies-for-stable-rl-scaling>Key Strategies for Stable RL Scaling</a></li></ul></li><li><a href=#agentic-task-synthesis-and-training>Agentic Task Synthesis and Training</a><ul><li><a href=#thinking-in-tool-use>Thinking in Tool-Use</a></li><li><a href=#large-scale-agentic-tasks>Large-Scale Agentic Tasks</a></li><li><a href=#context-management-for-search-agents>Context Management for Search Agents</a></li></ul></li><li><a href=#deepseekmath-v2>DeepSeekMath-V2</a><ul><li><a href=#process-reward-models>Process Reward Models</a></li><li><a href=#overall-architecture>Overall Architecture</a><ul><li><a href=#data-construction>Data Construction</a></li><li><a href=#verifier-training>Verifier Training</a></li><li><a href=#meta-verifier>Meta-Verifier</a></li></ul></li><li><a href=#generator-training>Generator Training</a></li><li><a href=#sequential-refinement>Sequential Refinement</a></li></ul></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><p>By introducing DeepSeek Sparse Attention (DSA), a scalable reinforcement learning framework, and a large-scale agentic task synthesis pipeline, <strong>DeepSeek-V3.2</strong> (<a href=https://arxiv.org/abs/2512.02556>DeepSeek-AI, 2025</a>) achieves reasoning capabilities and agent performance comparable to GPT-5.</p><figure class=align-center><a href=deepseek_eval_res.png data-fancybox=gallery><img loading=lazy src=deepseek_eval_res.png#center alt="Fig. 1. Benchmark of DeepSeek-V3.2 and its counterparts. (Image source: DeepSeek-AI, 2025)" width=100%></a><figcaption><p>Fig. 1. Benchmark of DeepSeek-V3.2 and its counterparts. (Image source: <a href=https://arxiv.org/abs/2512.02556>DeepSeek-AI, 2025</a>)</p></figcaption></figure><h2 id=deepseek-sparse-attention>DeepSeek Sparse Attention<a hidden class=anchor aria-hidden=true href=#deepseek-sparse-attention>#</a></h2><figure class=align-center><a href=deepseekv3.2_arch.png data-fancybox=gallery><img loading=lazy src=deepseekv3.2_arch.png#center alt="Fig. 2. Attention architecture of DeepSeek-V3.2, where DSA is instantiated under MLA. (Image source: DeepSeek-AI, 2025)" width=100%></a><figcaption><p>Fig. 2. Attention architecture of DeepSeek-V3.2, where DSA is instantiated under MLA. (Image source: <a href=https://arxiv.org/abs/2512.02556>DeepSeek-AI, 2025</a>)</p></figcaption></figure><p>Traditional attention mechanisms assume that every token needs to attend to all historical tokens. However, from an information-theoretic perspective, effective information in text is highly unevenly distributed, with only a minority of historical tokens being truly relevant to the current token. While <a href=https://syhya.github.io/posts/2025-08-24-gpt5/#efficient-attention-mechanisms>sliding window attention</a> recognizes this by limiting attention to recent windows to simplify computation, it risks losing critical long-range dependencies. DeepSeek&rsquo;s core insight is to <strong>enable the model to autonomously learn and dynamically select truly important tokens</strong>, achieving a better balance between efficiency and information retention.</p><h3 id=lightning-indexer>Lightning Indexer<a hidden class=anchor aria-hidden=true href=#lightning-indexer>#</a></h3><p>The core of DSA is using a lightweight <strong>Lightning Indexer</strong> to quickly filter relevant tokens. For each query token $\mathbf{h}_t$, the indexer computes relevance scores with all preceding tokens $\mathbf{h}_s$:</p>$$
I_{t,s} = \sum_{j=1}^{H^I} w_{t,j}^I \cdot \text{ReLU}\left(\mathbf{q}_{t,j}^I \cdot \mathbf{k}_s^I\right)
$$<p>Several design aspects are noteworthy:</p><ol><li><strong>Multi-head indexing</strong>: Uses $H^I$ indexer heads, each learning different relevance patterns</li><li><strong>ReLU activation</strong>: Zeros out negative correlations, providing sparsity</li><li><strong>Learnable weights</strong>: $w_{t,j}^I$ determines each head&rsquo;s contribution, allowing dynamic model adjustment</li></ol><p>The indexer&rsquo;s computational cost is far lower than the main attention. The paper mentions it can be implemented in FP8, significantly reducing computational overhead while maintaining precision.</p><h3 id=fine-grained-token-selection>Fine-grained Token Selection<a hidden class=anchor aria-hidden=true href=#fine-grained-token-selection>#</a></h3><p>With relevance scores computed, the model only needs to select the top-k most relevant tokens:</p>$$
\mathbf{u}_t = \text{Attn}\left(\mathbf{h}_t, \left\{\mathbf{c}_s \mid I_{t,s} \in \text{Top-k}(I_{t,:})\right\}\right)
$$<p>Here, $k$ is set to 2048 to balance efficiency and effectiveness. This setting reduces attention computation complexity from $O(L^2)$ to $O(Lk)$, significantly reducing computational overhead while still covering most critical dependencies.</p><h3 id=continued-pre-training>Continued Pre-training<a hidden class=anchor aria-hidden=true href=#continued-pre-training>#</a></h3><p>Continued pre-training from <a href=https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus>DeepSeek-V3.1-Terminus</a> consists of two training stages:</p><p><strong>Dense Warm-up Stage</strong></p><p>Maintains the original dense attention, training only the indexer. The goal is to align the indexer&rsquo;s output distribution with the true attention distribution:</p>$$
\mathcal{L}^I = \sum_t D_{\text{KL}}\left(p_{t,:} \| \text{Softmax}(I_{t,:})\right)
$$<p>Here, $p_{t,:}$ represents the true attention distribution aggregated from main attention scores. This stage ensures the indexer learns to identify which historical tokens are most important for modeling at the current time step through alignment with the indexer output distribution.</p><ul><li>Learning rate: $10^{-3}$</li><li>Training steps: 1000 steps</li><li>Per step: 16 sequences × 128K tokens</li><li>Total tokens: 2.1B</li></ul><p><strong>Sparse Training Stage</strong></p><p>After indexer warm-up, introduces fine-grained token selection mechanism and jointly optimizes all model parameters to adapt to DSA&rsquo;s sparse attention computation pattern. In this stage, the indexer only aligns with the main attention distribution on the selected subset of key tokens, with loss function defined as:</p>$$
\mathcal{L}^I = \sum_t \mathbb{D}_{\mathrm{KL}}\left(p_{t,\mathcal{S}_t} \| \operatorname{Softmax}\left(I_{t,\mathcal{S}_t}\right)\right)
$$$$
\mathcal{S}_t=\left\{s \mid I_{t, s} \in \operatorname{Top}-\mathrm{k}\left(I_{t,:}\right)\right\}
$$<p>where $\mathcal{S}_t$ represents the set of top-k key-value tokens predicted as most important by the indexer at time step $t$.</p><ul><li>Learning rate: $7.3 \times 10^{-6}$</li><li>Selects 2048 key-value tokens per query token</li><li>Training steps: 15000 steps</li><li>Per step: 480 sequences × 128K tokens</li><li>Total tokens: 943.7B</li></ul><h3 id=inference-costs>Inference Costs<a hidden class=anchor aria-hidden=true href=#inference-costs>#</a></h3><p>DeepSeek-V3.2 requires less computation compared to MLA in DeepSeek-V3.1-Terminus, with benefits becoming more pronounced as context length increases.</p><p>Actual cost comparison on H800 GPU clusters (calculated at $2/GPU hour):</p><figure class=align-center><a href=inference_cost_compare.png data-fancybox=gallery><img loading=lazy src=inference_cost_compare.png#center alt="Fig. 3. Inference costs of DeepSeek-V3.1-Terminus and DeepSeek-V3.2 on H800 clusters. (Image source: DeepSeek-AI, 2025)" width=100%></a><figcaption><p>Fig. 3. Inference costs of DeepSeek-V3.1-Terminus and DeepSeek-V3.2 on H800 clusters. (Image source: <a href=https://arxiv.org/abs/2512.02556>DeepSeek-AI, 2025</a>)</p></figcaption></figure><h2 id=scaling-grpo>Scaling GRPO<a hidden class=anchor aria-hidden=true href=#scaling-grpo>#</a></h2><p>DeepSeek-V3.2 merges reasoning, agent, and human alignment training into a single RL stage. This approach effectively balances performance across diverse domains while circumventing catastrophic forgetting issues common in multi-stage training paradigms.</p><p>Reward design:</p><ul><li><strong>Reasoning and agent tasks</strong>: Rule-based outcome reward + length penalty + language consistency reward</li><li><strong>General tasks</strong>: Generative reward model where each prompt has its own evaluation rubrics</li></ul><h3 id=grpo>GRPO<a hidden class=anchor aria-hidden=true href=#grpo>#</a></h3><p><a href=https://syhya.github.io/posts/2025-01-27-deepseek-r1/#grpo>GRPO</a> is an efficient RL algorithm proposed by DeepSeek that replaces the value model in traditional PPO with group-relative advantage estimation. GRPO optimizes the policy model $\pi_\theta$ by maximizing the following objective:</p>$$
\begin{aligned}
\mathcal{J}_{\mathrm{GRPO}}(\theta)= & \mathbb{E}_{q \sim P(Q),\left\{o_i\right\}_{i=1}^G \sim \pi_{\mathrm{old}}(\cdot \mid q)}\left[\frac{1}{G} \sum_{i=1}^G \frac{1}{\left|o_i\right|} \sum_{t=1}^{\left|o_i\right|}\right. \\
& \left.\min \left(r_{i, t}(\theta) \hat{A}_{i, t}, \operatorname{clip}\left(r_{i, t}(\theta), 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right)-\beta \mathbb{D}_{\mathrm{KL}}\left(\pi_\theta\left(o_{i, t}\right) \| \pi_{\mathrm{ref}}\left(o_{i, t}\right)\right)\right],
\end{aligned}
$$<p>where the importance sampling ratio is:</p>$$
r_{i, t}(\theta)=\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t }\right)}{\pi_{\mathrm{old}}\left(o_{i, t} \mid q, o_{i,&lt;t }\right)}
$$<p>However, in large-scale training, vanilla GRPO encounters stability issues. DeepSeek-V3.2 proposes four key improvements.</p><h3 id=key-strategies-for-stable-rl-scaling>Key Strategies for Stable RL Scaling<a hidden class=anchor aria-hidden=true href=#key-strategies-for-stable-rl-scaling>#</a></h3><ol><li><strong>Unbiased KL Estimate</strong></li></ol><p>The K3 estimator used in vanilla GRPO can produce systematic bias in certain cases. When sampled tokens have much lower probability under the current policy than the reference policy ($\pi_\theta\left(o_t \mid q, o_{&lt;t }\right) \ll \pi_{\mathrm{ref}}\left(o_t \mid q, o_{&lt;t }\right)$), gradients become abnormally large, causing training instability.</p>$$
\mathbb{D}_{\mathrm{KL}}\left(\pi_\theta\left(o_{i, t}\right) \| \pi_{\mathrm{ref}}\left(o_{i, t}\right)\right)=\frac{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t }\right)}{\pi_{\mathrm{old}}\left(o_{i, t} \mid q, o_{i,&lt;t }\right)}\left(\frac{\pi_{\mathrm{ref}}\left(o_{i, t} \mid q, o_{i,&lt;t }\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t }\right)}-\log \frac{\pi_{\mathrm{ref}}\left(o_{i, t} \mid q, o_{i,&lt;t }\right)}{\pi_\theta\left(o_{i, t} \mid q, o_{i,&lt;t }\right)}-1\right)
$$<p>By incorporating the <strong>importance-sampling ratio between current policy $\pi_\theta$ and old policy $\pi_{\text{old}}$</strong> to correct the K3 estimator, the KL (and its gradient) estimation becomes <strong>unbiased with more stable convergence</strong>, and allows <strong>domain-specific adjustment of KL penalty strength</strong> (weakening or even omitting when necessary).</p><ol start=2><li><strong>Off-Policy Sequence Masking</strong></li></ol>$$
\begin{aligned}
\mathcal{J}_{\mathrm{GRPO}}(\theta)= & \mathbb{E}_{q \sim P(Q),\left\{o_{i}\right\}_{i=1}^{G} \sim \pi_{\mathrm{old}}(\cdot \mid q)}\left[\frac{1}{G} \sum_{i=1}^{G} \frac{1}{\left|o_{i}\right|} \sum_{t=1}^{\left|o_{i}\right|}\right. \\
& \left.\min \left(r_{i, t}(\theta) \hat{A}_{i, t}, \operatorname{clip}\left(r_{i, t}(\theta), 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right) M_{i, t}-\beta \mathbb{D}_{\mathrm{KL}}\left(\pi_{\theta}\left(o_{i, t}\right) \| \pi_{\mathrm{ref}}\left(o_{i, t}\right)\right)\right],
\end{aligned}
$$$$
M_{i,t} = \begin{cases} 0 & \text{if } \hat{A}_{i,t} < 0 \text{ and } \frac{1}{|o_i|}\sum_{t=1}^{|o_i|} \log\frac{\pi_{\text{old}}(o_{i,t}|q, o_{i,&lt;t })}{\pi_\theta(o_{i,t}|q, o_{i,&lt;t })} > \delta \\ 1 & \text{otherwise} \end{cases}
$$<p>To mitigate off-policy issues introduced by multi-step updates and training-inference inconsistency, <a href=https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda>Off-Policy Sequence Masking</a> is introduced in GRPO, masking only sequences with <strong>negative advantage and policy divergence (KL) exceeding a threshold</strong>, thereby suppressing interference from highly off-policy negative samples on optimization. This mechanism significantly improves training stability while retaining effective learning signals.</p><ol start=3><li><strong>Keep Routing</strong></li></ol><p>For MoE models, a subtle issue arises: the combination of experts activated during inference may differ from training. Even with identical inputs, framework differences or policy updates can lead to different routing results.</p><p>DeepSeek&rsquo;s approach is to record routing paths during sampling and enforce the same paths during training. This ensures that gradients update the exact parameters that produced the sampled output. The verl framework has integrated <a href=https://github.com/volcengine/verl/tree/main/examples/router_replay>Router Replay</a> functionality for direct use.</p><ol start=4><li><strong>Keep Sampling Mask</strong></li></ol><p>During the rollout phase of RL training, Top-p/Top-k sampling is commonly used to filter low-probability tokens and improve generation quality. However, training typically optimizes over the full vocabulary, causing action space inconsistency between old policy $\pi_{\text{old}}$ and new policy $\pi_\theta$, violating importance sampling assumptions and causing training instability. To address this, DeepSeek records the truncation mask generated during rollout sampling and applies the same mask to $\pi_\theta$ during training, forcing both policies to optimize within a consistent action subspace, thereby maintaining generation consistency and stability in RL training when combined with Top-p sampling.</p><h2 id=agentic-task-synthesis-and-training>Agentic Task Synthesis and Training<a hidden class=anchor aria-hidden=true href=#agentic-task-synthesis-and-training>#</a></h2><h3 id=thinking-in-tool-use>Thinking in Tool-Use<a hidden class=anchor aria-hidden=true href=#thinking-in-tool-use>#</a></h3><p><strong>Thinking Context Management</strong></p><p>Integrating reasoning capabilities into tool-use scenarios is an interesting challenge. DeepSeek R1&rsquo;s approach discards previous reasoning content at each new message round, but this causes severe token waste in tool-calling scenarios—the model must re-reason through the entire problem after each tool call. <strong>Claude Opus 4.5</strong> (<a href=https://www.anthropic.com/news/claude-opus-4-5>Anthropic, 2025</a>) achieves cross-context information persistence and context reset through <a href=https://platform.claude.com/docs/en/agents-and-tools/tool-use/memory-tool>memory tool</a> and <a href=https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf>new context tool</a>, supporting up to 1M effective tokens accumulated across multiple resets while maintaining a single 200k token context window.</p><figure class=align-center><a href=claude_context_window.png data-fancybox=gallery><img loading=lazy src=claude_context_window.png#center alt="Fig. 4. The context window token management when combining extended thinking with tool use in Claude. (Image source: Claude Docs)" width=100%></a><figcaption><p>Fig. 4. The context window token management when combining extended thinking with tool use in Claude. (Image source: <a href=https://platform.claude.com/docs/en/build-with-claude/context-windows#the-context-window-with-extended-thinking-and-tool-use>Claude Docs</a>)</p></figcaption></figure><p>DeepSeek-V3.2 implements similar fine-grained context management:</p><figure class=align-center><a href=deepseek_thinking_tool_use.png data-fancybox=gallery><img loading=lazy src=deepseek_thinking_tool_use.png#center alt="Fig. 5. Thinking retention mechanism in tool-calling scenarios. (Image source: DeepSeek-AI, 2025)" width=100%></a><figcaption><p>Fig. 5. Thinking retention mechanism in tool-calling scenarios. (Image source: <a href=https://arxiv.org/abs/2512.02556>DeepSeek-AI, 2025</a>)</p></figcaption></figure><ul><li>Reasoning content is discarded only when receiving <strong>new user messages</strong></li><li>Tool outputs and similar messages don&rsquo;t trigger reasoning content deletion</li><li>Tool call history is always preserved in context</li></ul><p>This design maintains reasoning coherence while avoiding redundant computation.</p><h3 id=large-scale-agentic-tasks>Large-Scale Agentic Tasks<a hidden class=anchor aria-hidden=true href=#large-scale-agentic-tasks>#</a></h3><p>A diverse set of RL tasks is crucial for enhancing model robustness. DeepSeek-V3.2 uses the following agentic tasks:</p><table><thead><tr><th>Agent Type</th><th>Number of Tasks</th><th>Environment</th><th>Prompt Type</th></tr></thead><tbody><tr><td>Code Agent</td><td>24,667</td><td>Real</td><td>Extracted</td></tr><tr><td>Search Agent</td><td>50,275</td><td>Real</td><td>Synthesized</td></tr><tr><td>General Agent</td><td>4,417</td><td>Synthesized</td><td>Synthesized</td></tr><tr><td>Code Interpreter</td><td>5,908</td><td>Real</td><td>Extracted</td></tr></tbody></table><figure class=align-center><a href=deepseekv3.2_benchmark.png data-fancybox=gallery><img loading=lazy src=deepseekv3.2_benchmark.png#center alt="Fig. 6. Comparison between DeepSeek-V3.2 and closed/open models. (Image source: DeepSeek-AI, 2025)" width=100%></a><figcaption><p>Fig. 6. Comparison between DeepSeek-V3.2 and closed/open models. (Image source: <a href=https://arxiv.org/abs/2512.02556>DeepSeek-AI, 2025</a>)</p></figcaption></figure><p>Evaluation results show DeepSeek-V3.2 achieves performance comparable to GPT-5 High on reasoning tasks, significantly outperforms open-source models on Code Agent tasks, and effectively narrows the gap with top closed-source models in Tool-Use scenarios, demonstrating strong generalization capabilities.</p><h3 id=context-management-for-search-agents>Context Management for Search Agents<a hidden class=anchor aria-hidden=true href=#context-management-for-search-agents>#</a></h3><p>Despite extended context windows like 128K, agentic workflows (especially search-based scenarios) frequently encounter maximum length limits that prematurely truncate reasoning processes. This bottleneck severely inhibits the full realization of test-time compute potential. To address this, when token usage exceeds <strong>80%</strong> of the context window, the following context management strategies are introduced to dynamically extend token budget at test time:</p><ol><li><strong>Summary</strong>: Summarizes overflowed trajectory content and re-initiates subsequent reasoning.</li><li><strong>Discard-75%</strong>: Discards the first 75% of tool call history to free up space.</li><li><strong>Discard-all</strong>: Resets context by discarding all previous tool call history (similar to Anthropic&rsquo;s new context tool).</li></ol><figure class=align-center><a href=new_context_tool.png data-fancybox=gallery><img loading=lazy src=new_context_tool.png#center alt="Fig. 7. New context tool feature in Claude. (Image source: Anthropic, 2025)" width=90%></a><figcaption><p>Fig. 7. New context tool feature in Claude. (Image source: <a href=https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf>Anthropic, 2025</a>)</p></figcaption></figure><p>These strategies were evaluated on the <strong>BrowseComp</strong> benchmark. Results show that context management significantly improves test-time compute scaling and model performance by allowing more execution steps. Specific performance:</p><figure class=align-center><a href=search_agent_browsecomp.png data-fancybox=gallery><img loading=lazy src=search_agent_browsecomp.png#center alt="Fig. 8. Accuracy of BrowseComp with different test-time compute expansion strategies. (Image source: DeepSeek-AI, 2025)" width=100%></a><figcaption><p>Fig. 8. Accuracy of BrowseComp with different test-time compute expansion strategies. (Image source: <a href=https://arxiv.org/abs/2512.02556>DeepSeek-AI, 2025</a>)</p></figcaption></figure><p>While the Summary strategy successfully extends average steps from 140 to 364, improving scores from 53.4 to 60.2, its overall computational efficiency is relatively low. The Discard-all strategy performs best in both efficiency and scalability, achieving a high score of <strong>67.6</strong>. Notably, it matches the performance of parallel scaling baselines while requiring significantly fewer execution steps.</p><h2 id=deepseekmath-v2>DeepSeekMath-V2<a hidden class=anchor aria-hidden=true href=#deepseekmath-v2>#</a></h2><p>Developed on <a href=https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp-Base>DeepSeek-V3.2-Exp-Base</a>, <strong>DeepSeekMath-V2</strong> (<a href=https://arxiv.org/abs/2511.22570>Shao et al., 2025</a>) focuses on formal mathematical reasoning and theorem proving. Unlike traditional final-answer-based reinforcement learning, DeepSeekMath-V2 introduces a <strong>Self-Verification</strong> mechanism that enables rigorous self-examination of reasoning processes during proof generation through collaborative training of an independent proof verifier and meta-verifier. This approach achieves gold-medal level performance on challenging mathematical competition benchmarks like <a href=https://imobench.github.io/>IMO-Proof Bench</a> and <a href=https://maa.org/putnam/>Putnam</a>.</p><h3 id=process-reward-models>Process Reward Models<a hidden class=anchor aria-hidden=true href=#process-reward-models>#</a></h3><p>Traditional mathematical reasoning RL typically employs <strong>Outcome-based Reward Models (ORM)</strong>, rewarding only based on final answer correctness. While this works for competitions like AIME and HMMT that focus on numerical answers, it exposes fundamental limitations in more complex reasoning tasks:</p><ol><li><strong>Unreliable proxy metric</strong>: A correct final answer doesn&rsquo;t guarantee correct reasoning. Models may arrive at correct results through flawed logic, shortcuts, or coincidence (False Positive).</li><li><strong>Inapplicable to theorem proving</strong>: Higher-order mathematical tasks like theorem proving emphasize rigorous step-by-step logical derivation, where single outcome rewards provide insufficient training signal.</li></ol><p>To address these issues, DeepMind pioneered <strong>Process Reward Models (PRM)</strong> for mathematical problem-solving in 2022, proposing explicit evaluation of intermediate reasoning steps to mitigate the supervision deficiency of outcome rewards (<a href=https://arxiv.org/abs/2211.14275>Uesato et al., 2022</a>). This work systematically revealed that modeling the reasoning process itself is more conducive to learning complex reasoning capabilities than supervising only final answers.</p><figure class=align-center><a href=orm_prm_compare.png data-fancybox=gallery><img loading=lazy src=orm_prm_compare.png#center alt="Fig. 9. Comparison of policy improvement mechanisms for Final-Answer RL, ORM-RL, and PRM-RL. (Image source: Uesato et al., 2022)" width=100%></a><figcaption><p>Fig. 9. Comparison of policy improvement mechanisms for Final-Answer RL, ORM-RL, and PRM-RL. (Image source: <a href=https://arxiv.org/abs/2211.14275>Uesato et al., 2022</a>)</p></figcaption></figure><p>DeepSeekMath-V2 is also trained in this mode, enabling the model to actively identify potential logical gaps during reasoning generation and perform self-correction, simulating human thinking patterns when reading and reviewing proofs.</p><h3 id=overall-architecture>Overall Architecture<a hidden class=anchor aria-hidden=true href=#overall-architecture>#</a></h3><figure class=align-center><a href=deepseek_math_v2_arch.png data-fancybox=gallery><img loading=lazy src=deepseek_math_v2_arch.png#center alt="Fig. 10. Self-verification architecture with proof generation, verifier-based evaluation, and meta-verification." width=100%></a><figcaption><p>Fig. 10. Self-verification architecture with proof generation, verifier-based evaluation, and meta-verification.</p></figcaption></figure><p>DeepSeekMath-V2 constructs a three-tier verification architecture achieving continuous improvement through inter-model supervision:</p><ul><li><strong>Proof Generator ($\pi_\theta$)</strong>: Generates mathematical proofs based on problem $X$.</li><li><strong>Proof Verifier ($\pi_\varphi$)</strong>: Acts as LLM-as-a-judge to evaluate proof quality.</li><li><strong>Meta-Verifier ($\pi_\psi$)</strong>: Supervises the verifier&rsquo;s evaluation process to ensure verification quality.</li></ul><h4 id=data-construction>Data Construction<a hidden class=anchor aria-hidden=true href=#data-construction>#</a></h4><p>The team constructed initial training data through the following process:</p><ol><li><p><strong>Problem Collection</strong>: Crawled problems from <a href="https://artofproblemsolving.com/?srsltid=AfmBOoqcstCRpzZaf7rDkaLdkuHkR_SUAaTVBUHDrPo-nctXiCEuobst">Art of Problem Solving (AoPS)</a>, prioritizing mathematical olympiads, team selection tests, and post-2010 problems explicitly requiring proofs, totaling <strong>17,503 problems</strong>, denoted as $\mathcal{D}_p$</p></li><li><p><strong>Candidate Proof Generation</strong>: Used a variant of DeepSeek-V3.2-Exp-Thinking to generate candidate proofs. Since this model wasn&rsquo;t optimized for theorem proving and tended to produce concise but error-prone outputs, it was prompted to iteratively refine proofs over multiple rounds to improve comprehensiveness and rigor</p></li><li><p><strong>Expert Annotation</strong>: Randomly sampled proofs across different problem types (e.g., algebra, number theory) and had mathematical experts score each proof according to evaluation criteria</p></li></ol><p>This process produced an initial RL dataset $\mathcal{D}_v = {(X_i, Y_i, s_i)}$, where each item contains problem $X_i$, proof $Y_i$, and overall proof score $s_i \in {0, 0.5, 1}$.</p><h4 id=verifier-training>Verifier Training<a hidden class=anchor aria-hidden=true href=#verifier-training>#</a></h4><p>The verifier is initialized from DeepSeek-V3.2-Exp-SFT (supervised fine-tuned on mathematics and code-related reasoning data) and employs a three-level scoring standard:</p><ul><li><strong>1.0 score</strong>: Complete and rigorous proof with all logical steps clearly justified.</li><li><strong>0.5 score</strong>: Generally correct overall logic but with minor errors or omitted details.</li><li><strong>0.0 score</strong>: Fundamentally flawed proof containing fatal logical errors or critical gaps.</li></ul><p>Given problem $X$ and proof $Y$, the verifier $\pi_\varphi(\cdot|X, Y, \mathcal{I}_v)$ is designed to first summarize identified issues, then assign a score based on the criteria.</p><p>The verifier is optimized through reinforcement learning with two reward components:</p><ul><li><p><strong>Format reward $R_{\text{format}}$</strong>: Enforces output of issue summary and proof score in specified format by checking for designated evaluation phrases and scores in <code>\boxed{}</code>.</p></li><li><p><strong>Score reward $R_{\text{score}}$</strong>: Based on proximity between predicted score $s&rsquo;_i$ and annotated score $s_i$:</p>$$R_{\text{score}}(s'_i, s_i) = 1 - |s'_i - s_i|$$</li></ul><p>The verifier&rsquo;s RL objective is:</p>$$
\max_{\pi_\varphi} \mathbb{E}_{(X_i, Y_i, s_i) \sim \mathcal{D}_v, (V'_i, s'_i) \sim \pi_\varphi(\cdot|X_i, Y_i)} \left[ R_{\text{format}}(V'_i) \cdot R_{\text{score}}(s'_i, s_i) \right]
$$<p>where $V&rsquo;_i$ represents the verifier&rsquo;s final response and $s&rsquo;_i$ is the proof score extracted from it.</p><h4 id=meta-verifier>Meta-Verifier<a hidden class=anchor aria-hidden=true href=#meta-verifier>#</a></h4><p>The above approach trains the proof verifier through RL to align predicted proof scores with expert annotations, but <strong>doesn&rsquo;t directly supervise the identified issues themselves</strong>. This creates a critical vulnerability: when evaluating flawed proofs during training ($s_i &lt; 1$), the verifier can receive full reward by predicting correct scores while hallucinating non-existent issues, undermining its trustworthiness.</p><p>To address this problem, <strong>Meta-Verification</strong> is introduced: a secondary evaluation process that assesses whether issues identified by the verifier actually exist and whether these issues logically justify the predicted proof score according to evaluation criteria $\mathcal{I}_v$.</p><p>The meta-verifier is also trained through reinforcement learning with an objective function similar to the verifier&rsquo;s. Using the trained meta-verifier $\pi_\psi$, verifier training is enhanced by integrating meta-verification feedback into the reward function:</p>$$
R_V = R_{\text{format}} \cdot R_{\text{score}} \cdot R_{\text{meta}}
$$<p>where $R_{\text{meta}}$ is the quality score from the meta-verifier.</p><p>Experimental results show that introducing the meta-verifier improves the average quality score of the verifier&rsquo;s proof analyses from 0.85 to 0.96 on a validation split of $\mathcal{D}_v$, while maintaining the same proof score prediction accuracy.</p><p>This design is similar to the idea of <strong>Generative Adversarial Networks (GANs)</strong> (<a href=https://arxiv.org/abs/1406.2661>Goodfellow et al., 2014</a>): the verifier drives the generator to improve, while a stronger generator provides more challenging training samples for the verifier, forming a virtuous cycle. Note that the meta-verification score is <strong>used only during training</strong> and doesn&rsquo;t participate in inference computation.</p><h3 id=generator-training>Generator Training<a hidden class=anchor aria-hidden=true href=#generator-training>#</a></h3><p>With verifier $\pi_\varphi$ as a generative reward model, the proof generator $\pi_\theta$&rsquo;s optimization objective is:</p>$$
\max_{\pi_\theta} \mathbb{E}_{X_i \sim \mathcal{D}_p, Y_i \sim \pi_\theta(\cdot|X_i)} [R_Y]
$$<p>where $R_Y$ is the proof score produced by $\pi_\varphi(\cdot|X_i, Y_i, \mathcal{I}_v)$.</p><p>During training, the generator $\pi_\theta$ is prompted to produce proof $Y$ followed by self-analysis $Z$ following the same format and criteria $\mathcal{I}_v$ as the verifier. The predicted proof score in self-analysis is denoted as $s&rsquo;$. The reward function comprehensively considers these evaluations:</p>$$
R = R_{\text{format}}(Y, Z) \cdot (\alpha \cdot R_Y + \beta \cdot R_Z)
$$$$
R_Z = R_{\text{score}}(s', s) \cdot R_{\text{meta}}(Z)
$$<p>where $\alpha = 0.76$ and $\beta = 0.24$.</p><p>This reward structure creates the following incentives:</p><ul><li><strong>Honesty over falsehood</strong>: Faithfully acknowledging errors receives higher rewards than falsely claiming correctness.</li><li><strong>Self-awareness</strong>: Highest rewards come from producing correct proofs and accurately recognizing their rigor.</li><li><strong>Active improvement</strong>: An effective strategy for the proof generator to obtain high rewards is to identify and resolve as many potential issues as possible before finalizing the response.</li></ul><h3 id=sequential-refinement>Sequential Refinement<a hidden class=anchor aria-hidden=true href=#sequential-refinement>#</a></h3><figure class=align-center><a href=imo_2024_res.png data-fancybox=gallery><img loading=lazy src=imo_2024_res.png#center alt="Fig. 11. Proof Quality Improves with Increasing Sequential Self-Refinement Iterations (1–8). (Image source: Shao et al., 2025)" width=80%></a><figcaption><p>Fig. 11. Proof Quality Improves with Increasing Sequential Self-Refinement Iterations (1–8). (Image source: <a href=https://arxiv.org/abs/2511.22570>Shao et al., 2025</a>)</p></figcaption></figure><p><a href=https://syhya.github.io/posts/2025-11-19-scaling-law/#test-time-scaling>Test-time Scaling</a> improves model performance by increasing computation during inference, especially suitable for high-difficulty mathematical proof tasks like IMO and Putnam. The above figure shows <strong>DeepSeekMath-V2</strong> performance variation under up to <strong>8 sequential iterations</strong>: as maximum sequential iterations increase, <strong>Pass@1</strong> steadily improves from approximately <strong>0.15</strong> to <strong>0.27</strong>, and <strong>Best@32</strong> improves from approximately <strong>0.26</strong> to <strong>0.42</strong>. Sequential refinement combined with self-verification mechanisms significantly improves proof success rates through multiple rounds of generation and error correction under controllable computational cost, showing stable performance gains that increase with iteration count.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Liu, Aixin, et al. <a href=https://arxiv.org/abs/2512.02556>&ldquo;DeepSeek-V3.2: Pushing the frontier of open large language models.&rdquo;</a> arXiv preprint arXiv:2512.02556 (2025).</p><p>[2] Shao, Zhihong, et al. <a href=https://arxiv.org/abs/2511.22570>&ldquo;DeepSeekMath-V2: Towards self-verifiable mathematical reasoning.&rdquo;</a> arXiv preprint arXiv:2511.22570 (2025).</p><p>[3] Uesato, Jonathan, et al. <a href=https://arxiv.org/abs/2211.14275>&ldquo;Solving math word problems with process-and outcome-based feedback.&rdquo;</a> arXiv preprint arXiv:2211.14275 (2022).</p><p>[4] Goodfellow, Ian J., et al. <a href=https://arxiv.org/abs/1406.2661>&ldquo;Generative adversarial nets.&rdquo;</a> Advances in neural information processing systems 27 (2014).</p><p>[5] Anthropic. <a href=https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf>&ldquo;Claude Opus 4.5 — System Card.&rdquo;</a> 2025. (PDF)</p><p>[6] Luong, Minh-Thang, et al. <a href=https://arxiv.org/abs/2511.01846>&ldquo;Towards robust mathematical reasoning.&rdquo;</a> Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing. 2025.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reposting or citing content from this article, please credit the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Dec 2025). DeepSeek-V3.2 Series.
<a href=https://syhya.github.io/posts/2025-12-31-deepseekv3.2>https://syhya.github.io/posts/2025-12-31-deepseekv3.2</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025deepseekv32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;DeepSeek-V3.2 Series&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Dec&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-12-31-deepseekv3.2&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/tags/deepseek/>DeepSeek</a></li><li><a href=https://syhya.github.io/tags/sparse-attention/>Sparse Attention</a></li><li><a href=https://syhya.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://syhya.github.io/tags/reasoning/>Reasoning</a></li><li><a href=https://syhya.github.io/tags/agent/>Agent</a></li><li><a href=https://syhya.github.io/tags/theorem-proving/>Theorem Proving</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2026-02-20-self-evolving-agents/><span class=title>« Prev</span><br><span>Self-Evolving Agents</span>
</a><a class=next href=https://syhya.github.io/posts/2025-11-19-scaling-law/><span class=title>Next »</span><br><span>Scaling Laws</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V3.2 Series on x" href="https://x.com/intent/tweet/?text=DeepSeek-V3.2%20Series&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-12-31-deepseekv3.2%2f&amp;hashtags=DeepLearning%2cLLM%2cDeepSeek%2cSparseAttention%2cReinforcementLearning%2cReasoning%2cAgent%2cTheoremProving"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V3.2 Series on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-12-31-deepseekv3.2%2f&amp;title=DeepSeek-V3.2%20Series&amp;summary=DeepSeek-V3.2%20Series&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-12-31-deepseekv3.2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V3.2 Series on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-12-31-deepseekv3.2%2f&title=DeepSeek-V3.2%20Series"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V3.2 Series on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-12-31-deepseekv3.2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V3.2 Series on whatsapp" href="https://api.whatsapp.com/send?text=DeepSeek-V3.2%20Series%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-12-31-deepseekv3.2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V3.2 Series on telegram" href="https://telegram.me/share/url?text=DeepSeek-V3.2%20Series&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-12-31-deepseekv3.2%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V3.2 Series on ycombinator" href="https://news.ycombinator.com/submitlink?t=DeepSeek-V3.2%20Series&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-12-31-deepseekv3.2%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>