[{"content":"Background The Transformer (Vaswani et al., 2017) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (Devlin et al., 2018) which uses only the encoder, GPT (Radford et al., 2018) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (Touvron et al., 2023) and GPT-4 (OpenAI et al., 2024), most of which adopt a decoder-only architecture.\nNotations Symbol Meaning \\(B\\) Batch Size \\(S\\) Sequence Length \\(d\\) Hidden Dimension / Model Size \\(H\\) Number of Heads in Multi-Head Attention \\(G\\) Number of Groups, used for Grouped-Query Attention (GQA) \\(d_{\\text{head}} = \\frac{d}{H}\\) Dimension of each attention head \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) Input sequence, with batch size \\(B\\), sequence length \\(S\\), and hidden dimension \\(d\\) \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{B \\times S \\times d}\\) Query, Key, and Value matrices after linear transformation \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) Trainable linear projection matrices for generating \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) respectively \\(W_O \\in \\mathbb{R}^{d \\times d}\\) Trainable linear projection matrix for mapping multi-head/grouped attention outputs back to dimension \\(d\\) \\(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h \\in \\mathbb{R}^{B \\times S \\times d_{\\text{head}}}\\) Query, Key, and Value sub-matrices for the \\(h\\)-th attention head \\(\\mathbf{K}^*, \\mathbf{V}^*\\) Shared \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) obtained by averaging or merging all heads\u0026rsquo; \\(\\mathbf{K}_h, \\mathbf{V}_h\\) in Multi-Query Attention (MQA) \\(\\mathbf{q}, \\mathbf{k}\\in \\mathbb{R}^{d_{\\text{head}}}\\) Single query and key vectors used in mathematical derivations (Central Limit Theorem) in Scaled Dot-Product Attention Attention Mechanism in Transformers The core of the Transformer model is the Self-Attention Mechanism, which allows the model to dynamically focus on different parts of the sequence when processing sequential data. Specifically, given an input sequence \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) (batch size \\(B\\), sequence length \\(S\\), hidden dimension \\(d\\)), the Transformer projects it into queries (\\(\\mathbf{Q}\\)), keys (\\(\\mathbf{K}\\)), and values (\\(\\mathbf{V}\\)) through three linear layers:\n\\[ \\mathbf{Q} = \\mathbf{X} W_Q, \\quad \\mathbf{K} = \\mathbf{X} W_K, \\quad \\mathbf{V} = \\mathbf{X} W_V \\]where \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) are trainable weight matrices. MHA enhances the model\u0026rsquo;s representational capacity by splitting these projections into multiple heads, each responsible for different subspace representations.\nThere are various forms of attention mechanisms, and the Transformer relies on Scaled Dot-Product Attention: given query matrix \\(\\mathbf{Q}\\), key matrix \\(\\mathbf{K}\\), and value matrix \\(\\mathbf{V}\\), the output is a weighted sum of the value vectors, where each weight is determined by the dot product of the query with the corresponding key:\n\\[ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V} \\] Fig. 1. Scaled Dot-Product Attention. (Image source: Vaswani et al., 2017)\nMulti-Head Attention (MHA) Multi-Head Attention (MHA) splits \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) into multiple heads, each with independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\), thereby increasing the model\u0026rsquo;s capacity and flexibility:\n\\[ \\text{MHA}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]where each head is computed as:\n\\[ \\text{head}_h = \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q}_h \\mathbf{K}_h^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V}_h \\] Fig. 2. Multi-Head Attention. (Image source: Vaswani et al., 2017)\nBenefits of Using Multi-Head Attention Capturing Diverse Features: A single-head attention mechanism can only focus on one type of feature or pattern in the input sequence, whereas MHA can simultaneously focus on different features or patterns across multiple attention heads, enabling the model to understand the input data more comprehensively. Enhanced Expressive Power: Each attention head can learn different representations, enhancing the model\u0026rsquo;s expressive power. Different heads can focus on different parts or relationships within the input sequence, helping the model capture complex dependencies more effectively. Improved Stability and Performance: MHA reduces noise and instability from individual attention heads by averaging or combining multiple heads, thereby improving the model\u0026rsquo;s stability and performance. Parallel Computation: MHA allows for parallel computation since each attention head\u0026rsquo;s calculations are independent. This boosts computational efficiency, especially when using hardware accelerators like GPUs or TPUs. Softmax in Scaled Dot-Product Attention The softmax function transforms a vector \\(\\mathbf{z} = [z_1, z_2, \\dots, z_n]\\) into a probability distribution \\(\\mathbf{y} = [y_1, y_2, \\dots, y_n]\\) defined as:\n\\[ y_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)} \\quad \\text{for} \\quad i = 1, 2, \\dots, n \\]In the attention mechanism, the softmax function is used to convert the scaled dot product \\(\\tfrac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\) into attention weights:\n\\[ \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr) = \\Bigl[ \\frac{\\exp\\Bigl(\\frac{Q_1 \\cdot K_1}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_1 \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)}, \\dots, \\frac{\\exp\\Bigl(\\frac{Q_S \\cdot K_S}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_S \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)} \\Bigr] \\]In the Transformer\u0026rsquo;s attention mechanism, the scaling factor \\(\\sqrt{d_{\\text{head}}}\\) in the scaled dot-product attention formula ensures that the dot product results do not become excessively large as the vector dimension increases before applying softmax. This is primarily for the following reasons:\nPreventing Gradient Vanishing: Scaling the attention scores avoids overly large inputs to the softmax function, preventing gradients from vanishing during backpropagation.\nNumerical Stability: Scaling ensures that the input range to the softmax function remains reasonable, avoiding extreme values that could lead to numerical instability and overflow issues, especially when the vector dimensions are large. Without scaling, the dot product results could cause the softmax\u0026rsquo;s exponential function to produce excessively large values.\nMathematical Explanation: Suppose vectors \\(\\mathbf{q}\\) and \\(\\mathbf{k}\\) have independent and identically distributed components with mean 0 and variance 1. Their dot product \\(\\mathbf{q} \\cdot \\mathbf{k}\\) has a mean of 0 and a variance of \\(d_{\\text{head}}\\). To prevent the dot product\u0026rsquo;s variance from increasing with the dimension \\(d_{\\text{head}}\\), it is scaled by \\(\\frac{1}{\\sqrt{d_{\\text{head}}}}\\). This scaling ensures that the variance of the scaled dot product remains 1, independent of \\(d_{\\text{head}}\\).\nAccording to statistical principles, dividing a random variable by a constant scales its variance by the inverse square of that constant. Therefore, the scaling factor \\(\\tfrac{1}{\\sqrt{d_{\\text{head}}}}\\) effectively controls the magnitude of the attention scores, enhancing numerical stability. The detailed derivation is as follows:\nAssume \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^{d_{\\text{head}}}\\) with independent and identically distributed components, mean 0, and variance 1. Their dot product is:\n\\[ \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d_{\\text{head}}} q_i k_i \\]By the Central Limit Theorem, for large \\(d_{\\text{head}}\\), the dot product \\(\\mathbf{q} \\cdot \\mathbf{k}\\) approximately follows a normal distribution with mean 0 and variance \\(d_{\\text{head}}\\):\n\\[ \\mathbf{q} \\cdot \\mathbf{k} \\sim \\mathcal{N}(0, d_{\\text{head}}) \\]To achieve unit variance in the scaled dot product, we divide by \\(\\sqrt{d_{\\text{head}}}\\):\n\\[ \\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}} \\;\\sim\\; \\mathcal{N}\\!\\Bigl(0, \\frac{d_{\\text{head}}}{d_{\\text{head}}}\\Bigr) = \\mathcal{N}(0, 1) \\]Thus, the scaled dot product \\(\\tfrac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}}\\) maintains a variance of 1, independent of \\(d_{\\text{head}}\\). This scaling operation keeps the dot product within a stable range, preventing the softmax function from encountering numerical instability due to excessively large or small input values.\nMulti-Query Attention (MQA) Multi-Query Attention (MQA) (Shazeer, 2019) significantly reduces memory bandwidth requirements by allowing all query heads to share the same set of keys (\\(\\mathbf{K}\\)) and values (\\(\\mathbf{V}\\)). Specifically, if we average all \\(\\mathbf{K}_h\\) and \\(\\mathbf{V}_h\\) from traditional MHA as follows:\n\\[ \\mathbf{K}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{K}_h, \\quad \\mathbf{V}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{V}_h, \\]where \\(H\\) is the number of query heads, and \\(\\mathbf{K}_h\\) and \\(\\mathbf{V}_h\\) are the keys and values for the \\(h\\)-th head, respectively. During inference, each head only needs to use the same \\(\\mathbf{K}^*\\) and \\(\\mathbf{V}^*\\), significantly reducing memory bandwidth usage. Finally, all head outputs are concatenated and projected back to the output space:\n\\[ \\text{MQA}(\\mathbf{Q}, \\mathbf{K}^*, \\mathbf{V}^*) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]Since keys and values are consolidated into a single set, MQA inference is faster but may limit the model\u0026rsquo;s expressive capacity and performance in certain scenarios.\nGrouped-Query Attention (GQA) Grouped-Query Attention (GQA) (Ainslie, 2023) serves as a compromise between MHA and MQA. It divides query heads into multiple groups, allowing each group to share a set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads, thereby balancing inference speed and model performance. Each group contains \\(\\frac{H}{G}\\) query heads and shares one set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads. The specific process is as follows:\nProjection: Project the input \\(\\mathbf{X}\\) into \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) via linear transformations. Grouped Query: After splitting \\(\\mathbf{Q}\\) into \\(H\\) heads, further divide these heads into \\(G\\) groups. Grouped Key/Value: Split \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) into \\(G\\) groups, with each group sharing a set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Within-Group Attention: Perform attention calculations for each group\u0026rsquo;s \\(\\mathbf{Q}\\) with the shared \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Concatenate Outputs: Concatenate the attention results from all groups along the channel dimension and project them through a linear layer to obtain the final output. Relationship Between the Three Attention Methods Fig. 3. Overview of grouped-query method. (Image source: Ainslie et al., 2023)\nFigure 3 intuitively illustrates the relationship between the three attention mechanisms: MHA maintains independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) for each query head; MQA allows all query heads to share the same set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\); GQA strikes a balance by sharing \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) within groups.\nWhen \\(G=1\\): All query heads share the same set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). In this case, GQA degenerates into MQA.\nNumber of \\(\\mathbf{K}/\\mathbf{V}\\) Heads: 1 Model Behavior: All heads use the same \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) for attention calculations, significantly reducing memory bandwidth requirements. When \\(G=H\\): Each query head has its own independent set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). In this case, GQA degenerates into MHA.\nNumber of \\(\\mathbf{K}/\\mathbf{V}\\) Heads: \\(H\\) Model Behavior: Each head uses completely independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\), maintaining the high model capacity and performance of MHA. By adjusting the number of groups \\(G\\), GQA allows flexible switching between MHA and MQA, achieving a balance between maintaining high model performance and improving inference speed.\nImplementation Code Snippet Below is a simple PyTorch implementation of MHA 、MQA和 GQA. For GQA, two approaches are demonstrated: broadcasting and repeating.\nAdditionally, note that in the actual implementation of LLaMA3, GQA incorporates KV Cache for optimization. To keep the example concise, this part is omitted in the code below. For more comprehensive details, you can refer to the official source code in model.py.\nMHA Code Snippet multi_head_attention.py\nimport math import torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # (nums_head * head_dim = hidden_dim) assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(dropout_rate) # Define linear projection layers self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): # x has shape: (batch_size, seq_len, hidden_dim) batch_size, seq_len, _ = x.size() # Q, K, V each has shape: (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) K = self.k_proj(x) V = self.v_proj(x) # Reshaping from (batch_size, seq_len, hidden_dim) to (batch_size, seq_len, nums_head, head_dim) # Then transpose to (batch_size, nums_head, seq_len, head_dim) # q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3) # [Another approach to do it] q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) k = K.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Matrix multiplication: (batch_size, nums_head, seq_len, head_dim) * (batch_size, nums_head, head_dim, seq_len) # Resulting shape: (batch_size, nums_head, seq_len, seq_len) # Note that the scaling factor uses head_dim, not hidden_dim. attention_val = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) print(f\u0026#34;attention_mask shape is {attention_mask.size()}\u0026#34;) if attention_mask is not None: # If attention_mask is provided, it should have shape (batch_size, nums_head, seq_len, seq_len). assert attention_val.size() == attention_mask.size() attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # Apply softmax along the last dimension to get attention weights. attention_weight = torch.softmax(attention_val, dim=-1) # Dropout on attention weights attention_weight = self.dropout(attention_weight) # Multiply attention weights with V: # (batch_size, nums_head, seq_len, seq_len) * (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v # Transpose back: (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) # # Note: The transpose operation changes the dimension ordering but does not change the memory layout, # resulting in a non-contiguous tensor. The contiguous() method makes the tensor contiguous in memory, # allowing subsequent view or reshape operations without error. output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) # output = output_mid.permute(0, 2, 1, 3).reshpae(batch_size, seq_len, self.hidden_dim) # # [Another approach to do it] output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 # attention_mask has shape: (batch_size, nums_head, seq_len, seq_len). # Here we use a lower-triangular mask to simulate causal masking. attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_head_attention = MultiHeadAttention(hidden_dim=hidden_dim, nums_head=nums_head) x_forward = multi_head_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) MQA Code Snippet multi_query_attention.py\nimport torch import torch.nn as nn import math class MultiQueryAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(p=dropout) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # For kv, project: hidden_dim -\u0026gt; head_dim self.k_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.v_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() Q, K, V = self.q_proj(x), self.k_proj(x), self.v_proj(x) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Broadcast k and v to match q\u0026#39;s dimensions for attention computation # k -\u0026gt; (batch_size, 1, seq_len, head_dim) # v -\u0026gt; (batch_size, 1, seq_len, head_dim) k = K.unsqueeze(1) v = V.unsqueeze(1) # (batch_size, head_num, seq_len, head_dim) * (batch_size, 1, head_dim, seq_len) # -\u0026gt; (batch_size, head_num, seq_len, seq_len) attention_val = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) attention_weight = torch.softmax(attention_val, dim=-1) print(f\u0026#34;attention_weight is {attention_weight}\u0026#34;) attention_weight = self.dropout(attention_weight) # (batch_size, head_num, seq_len, seq_len) * (batch_size, 1, seq_len, head_dim) # -\u0026gt; (batch_size, head_num, seq_len, head_dim) output_tmp = attention_weight @ v # -\u0026gt; (batch_size, seq_len, head_num, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_query_attention = MultiQueryAttention(hidden_dim=hidden_dim, nums_head=nums_head, dropout=0.2) x_forward = multi_query_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) GQA Code Snippet group_query_attention.py\nimport torch import torch.nn as nn class GQABroadcast(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # Total number of Q heads (H) self.nums_kv_head = nums_kv_head # Number of K, V heads (G, groups) assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head # Number of Q heads per group self.q_heads_per_group = nums_head // nums_kv_head self.dropout = nn.Dropout(dropout_rate) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # Projection output dimensions for K, V = nums_kv_head * head_dim self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask= None): batch_size, seq_len, _ = x.size() Q = self.q_proj(x) # (batch_size, seq_len, hidden_dim) K = self.k_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) V = self.v_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) # Q: (batch_size, seq_len, hidden_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2).contiguous() q = q.view(batch_size, self.nums_kv_head, self.q_heads_per_group, seq_len, self.head_dim) # K, V: (batch_size, seq_len, nums_kv_head * head_dim) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim # -\u0026gt; (batch_size, nums_kv_head, 1, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) # q: (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) * (batch_size, nums_kv_head, 1, head_dim, seq_len) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_val = q @ k.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) * (batch_size, nums_kv_head, 1, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) output_tmp = attention_weight @ v # (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = output_tmp.view(batch_size, self.nums_head, seq_len, self.head_dim) # (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) -\u0026gt; (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output class GQARepeat(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head self.nums_kv_head = nums_kv_head assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head self.q_head_per_group = nums_head // nums_kv_head self.q_proj = nn.Linear(hidden_dim, nums_head * self.head_dim) self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) self.dropout = nn.Dropout(dropout_rate) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() # (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) # (batch_size, seq_len, nums_kv_head * self.head_dim) K = self.k_proj(x) V = self.v_proj(x) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) # (batch_size, nums_head, seq_len, head_dim) k_repeat = k.repeat_interleave(self.q_head_per_group, dim=1) v_repeat = v.repeat_interleave(self.q_head_per_group, dim=1) # (batch_size, nums_head, seq_len, seq_len) attention_val = q @ k_repeat.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#39;-inf\u0026#39;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v_repeat # (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 16) batch_size, seq_len, hidden_dim = x.size() nums_head = 8 head_dim = hidden_dim // nums_head nums_kv_head = 4 q_heads_per_group = nums_head // nums_kv_head # v1: Boardcast # attention_mask_v1 has shape: (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_mask_v1 = torch.tril(torch.ones(batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len)) gqa_boradcast = GQABroadcast(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v1 = gqa_boradcast.forward(x, attention_mask=attention_mask_v1) # print(x_forward_v1) print(x_forward_v1.size()) # v2: Repeat # attention_mask_v2 has shape: (batch_size, nums_head, seq_len, seq_len) attention_mask_v2 = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) gqa_repeat = GQARepeat(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v2 = gqa_repeat.forward(x, attention_mask=attention_mask_v2) # print(x_forward_v2) print(x_forward_v2.size()) Time and Space Complexity Analysis Note: The following discussion focuses on the computational complexity of a single forward pass. In training, one must also account for backward pass and parameter updates, which rely on the intermediate activations stored during the forward pass. The additional computation to calculate gradients and maintain partial derivatives usually makes the total training cost (both computation and memory usage) significantly higher—often multiple times the forward-pass cost.\nWhen analyzing different attention mechanisms (MHA, MQA, GQA), our main focus is on their time complexity and space complexity during the forward pass of either self-attention or cross-attention. Even though their implementation details (e.g., whether \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) are shared) can differ, the overall computational cost and memory usage are roughly on the same order of magnitude.\nAssume that each position in the sequence produces its own representations of query \\(\\mathbf{Q}\\), key \\(\\mathbf{K}\\), and value \\(\\mathbf{V}\\). After splitting by batch size and number of heads, their shapes can be written as:\n\\[ \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\;\\in\\; \\mathbb{R}^{B \\times H \\times S \\times d_{\\text{head}}} \\]Time Complexity Analysis General Time Complexity of Matrix Multiplication For two matrices \\(\\mathbf{A}\\) of shape \\(m \\times n\\) and \\(\\mathbf{B}\\) of shape \\(n \\times p\\), the complexity of computing the product \\(\\mathbf{A}\\mathbf{B}\\) is typically expressed as:\n\\[ \\mathcal{O}(m \\times n \\times p) \\]In attention-related computations, this formula is frequently used to analyze \\(\\mathbf{Q}\\mathbf{K}^\\top\\) and the multiplication of attention scores by \\(\\mathbf{V}\\).\nMain Steps and Complexity in Self-Attention Dot Product (\\(\\mathbf{Q}\\mathbf{K}^\\top\\))\nShape of \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nShape of \\(\\mathbf{K}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nConsequently, the result of \\(\\mathbf{Q}\\mathbf{K}^\\top\\) has shape \\(B \\times H \\times S \\times S\\)\nThe calculation can be viewed as \\(S \\times S\\) dot products for each head in each batch. Each dot product involves \\(d_{\\text{head}}\\) multiply-add operations.\nHence, its time complexity is:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S \\times S \\times d_{\\text{head}}\\bigr) \\;=\\; \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] Softmax Operation\nApplied element-wise to the attention score matrix of shape \\(B \\times H \\times S \\times S\\)\nSoftmax entails computing exponentials and performing normalization on each element. The complexity is approximately:\n\\[ \\mathcal{O}(\\text{number of elements}) = \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) \\] Compared with the matrix multiplication above, this step’s dependency on \\(d_{\\text{head}}\\) is negligible and is thus often considered a smaller overhead.\nWeighted Averaging (Multiplying Attention Scores with \\(\\mathbf{V}\\))\nShape of \\(\\mathbf{V}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nShape of the attention score matrix: \\(B \\times H \\times S \\times S\\)\nMultiplying each position’s attention scores by the corresponding \\(\\mathbf{V}\\) vector yields an output of shape \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nIts time complexity is analogous to that of \\(\\mathbf{Q}\\mathbf{K}^\\top\\):\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] Combining these three steps, the dominant costs come from the two matrix multiplications, each of complexity \\(\\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}})\\). Therefore, for a single full forward pass, the total complexity can be denoted as:\n\\[ \\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}}) = \\mathcal{O}(B \\times S^2 \\times d) \\]Here, we use \\(d_{\\text{head}} = \\frac{d}{H}\\).\nTime Complexity in Incremental Decoding/Inference with KV Cache Fig. 4. KV cache example. (Image source: Efficient NLP YouTube Channel)\nAs depicted in Figure 4, incremental decoding (especially in autoregressive generation) often employs a KV Cache to store previously computed \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Thus, one does not have to recalculate keys and values at each new time step. With every new token generated (i.e., a new time step), the following operations are performed:\nCompute \\(\\mathbf{Q}\\) for the New Token (and corresponding \\(\\mathbf{K}, \\mathbf{V}\\))\nIf only the projection weights are retained, then generating the new \\(\\mathbf{Q}\\) vector and the local \\(\\mathbf{K}, \\mathbf{V}\\) involves \\(\\mathcal{O}(d^2)\\) parameters, but this overhead is small as it is only for a single token. Perform Attention with the Existing KV Cache\nThe KV Cache stores all previous \\(\\mathbf{K}, \\mathbf{V}\\) vectors, with shape:\n\\[ B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} \\]Here, \\(S_{\\text{past}}\\) is the length of the already-generated sequence.\nThe new \\(\\mathbf{Q}\\) has shape \\(B \\times H \\times 1 \\times d_{\\text{head}}\\). Hence, computing the attention scores for the new token:\n\\[ \\mathbf{Q}\\mathbf{K}^\\top : \\; \\mathcal{O}\\bigl(B \\times H \\times 1 \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] Similarly, multiplying these scores by \\(\\mathbf{V}\\) has the same order:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] Update the KV Cache\nAppend the newly generated \\(\\mathbf{K}, \\mathbf{V}\\) to the cache, so they can be used at the subsequent time step. This merely requires a concatenation or append operation, which primarily grows the memory usage rather than incurring high compute. Thus, for incremental decoding, each new token involves:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] in computation, instead of the \\(S \\times S\\) scale for each forward pass. If one aims to generate \\(S\\) tokens in total, the cumulative complexity (under ideal conditions) becomes:\n\\[ \\sum_{k=1}^{S} \\mathcal{O}\\bigl(B \\times H \\times k \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] which is the same order as the one-shot computation. The difference is that incremental decoding computes one token at a time, thus requiring lower temporary memory usage per step and avoiding a full \\(S \\times S\\) attention score matrix at once.\nSummary of Time Complexity MHA (Multi-Head Attention): Multiple heads, each computing its own \\(\\mathbf{K}, \\mathbf{V}\\). MQA (Multi-Query Attention): Multiple heads share \\(\\mathbf{K}, \\mathbf{V}\\). GQA (Grouped Query Attention): The \\(H\\) heads are divided into \\(G\\) groups, each group sharing a single \\(\\mathbf{K}, \\mathbf{V}\\). Regardless of whether we use MHA, MQA, or GQA, in a full forward pass (or the forward portion during training), the main matrix multiplications have roughly the same complexity:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) \\]On the other hand, in incremental inference with KV Cache, the per-token complexity decreases to\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] but one must maintain and update the KV Cache over multiple decoding steps.\nSpace Complexity Analysis Space complexity encompasses both model parameters (weights) and intermediate activations needed during the forward pass—particularly the attention score matrices, weighted outputs, and potential KV Cache.\nModel Parameter Scale Parameters for the Linear Projection Layers\nProjecting the input vector of dimension \\(d\\) into \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\):\n\\[ \\underbrace{d \\times d}_{\\text{Q projection}} + \\underbrace{d \\times d}_{\\text{K projection}} + \\underbrace{d \\times d}_{\\text{V projection}} = 3d^2 \\] These parameters may be split among heads, but the total remains \\(\\mathcal{O}(d^2)\\), independent of the number of heads \\(H\\).\nOutput Merging Layer\nAfter concatenating multiple heads, there is typically another \\(d \\times d\\) linear layer to project the concatenated outputs back into dimension \\(d\\). This is also \\(\\mathcal{O}(d^2)\\).\nTherefore, combining these yields:\n\\[ 3d^2 + d^2 = 4d^2 \\] which remains \\(\\mathcal{O}(d^2)\\).\nIntermediate Activations for the Forward Pass During training or a full forward pass, the following key tensors often need to be stored:\nAttention Score Matrix\nShape: \\(B \\times H \\times S \\times S\\). Regardless of MHA, MQA, or GQA, each head (or group) computes \\(\\mathbf{Q}\\mathbf{K}^\\top\\) for the attention scores, yielding:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) \\] Weighted Output\nShape: \\(B \\times H \\times S \\times d_{\\text{head}}\\), corresponding to the contextual vectors after weighting \\(\\mathbf{V}\\). Its size is:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S \\times d\\bigr) \\] Storage of \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) for Backprop\nIn backward propagation, we need the forward outputs (or intermediate gradients). If explicitly stored, their shapes and scales are usually:\nMHA (Multi-Head Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) MQA (Multi-Query Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\) (shared): \\(B \\times S \\times d\\) GQA (Grouped Query Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\) (shared by group): \\(B \\times G \\times S \\times d_{\\text{head}}\\), where \\(G \\times d_{\\text{head}} = d\\). Space Usage in Incremental Decoding (KV Cache) In inference with incremental decoding, a KV Cache is typically used to store all previously computed keys and values, thus avoiding repeated computation for past tokens. The structure is generally as follows:\nKV Cache Dimensions (MHA example):\n\\[ \\mathbf{K}, \\mathbf{V} : B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} \\] As the generated sequence length \\(S_{\\text{past}}\\) grows, the cache usage increases linearly.\nPer-Step Attention Score Matrix:\nEach new token only requires a score matrix of shape:\n\\[ B \\times H \\times 1 \\times S_{\\text{past}} \\]which is much smaller than the \\(B \\times H \\times S \\times S\\) matrix used during training.\nTherefore, in incremental decoding, large temporary activations—such as the \\(S \\times S\\) score matrix—are not needed; however, the KV Cache itself (size \\(\\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}})\\)) must be maintained and grows along with the sequence length.\nCombined Space Complexity Training / Full Forward\nThe main activations (attention scores + weighted outputs + explicit storage of Q,K,V) add up to:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) \\]For large \\(S\\), the \\(\\mathcal{O}(B \\times H \\times S^2)\\) term tends to dominate.\nInference / Incremental Decoding (KV Cache)\nThere is no need for the full \\(S^2\\) attention matrix, but a KV Cache of size\n\\[ \\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}) \\] must be stored. This grows linearly with the decoding steps \\(S_{\\text{past}}\\).\nMeanwhile, the per-step attention matrix is only \\(B \\times H \\times 1 \\times S_{\\text{past}}\\), significantly smaller than the \\(\\mathcal{O}(S^2)\\) scenario in training.\nConclusions and Comparisons Time Complexity\nFor self-attention—whether using MHA, MQA, or GQA—in a full forward pass (which also applies to the forward portion during training), the principal matrix multiplications remain:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) \\] In incremental inference (KV Cache), each new token only requires\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\]but the KV Cache must be updated and maintained throughout the decoding sequence.\nSpace Complexity\nModel Parameters: All three attention mechanisms (MHA, MQA, GQA) reside in \\(\\mathcal{O}(d^2)\\) parameter space.\nIntermediate Activations (Training / Full Forward): Dominated by the attention score matrix and weighted outputs:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) \\] Incremental Decoding (KV Cache): Saves on the \\(\\mathcal{O}(S^2)\\) score matrix cost but requires\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] of storage for the KV Cache, which increases linearly with \\(S_{\\text{past}}\\).\nBenefits of MQA / GQA\nAlthough from a high-level perspective, MHA, MQA, and GQA share similar asymptotic complexities when \\(S\\) is large, MQA and GQA can achieve improved efficiency in practice due to key/value sharing (or partial sharing) which can reduce memory bandwidth demands and improve cache locality. Consequently, in real-world systems, they often deliver better speed and memory performance. The table below summarizes the main differences among MHA, MQA, and GQA attention mechanisms:\nFeature MHA MQA GQA Number of K/V Heads Same as number of heads (\\(H\\)) Single K/V head Number of groups (\\(G\\)), one K/V head per group Inference Time Slower Fastest Faster, but slightly slower than MQA Memory Bandwidth Requirement Highest, \\(H\\) times K/V loading Lowest, only one K/V head Between MHA and MQA, \\(G\\) times K/V loading Model Capacity Highest Lowest Moderate, depending on the number of groups \\(G\\) Performance Best Slightly lower than MHA Close to MHA, significantly better than MQA Uptraining Requirement None High, requires more stability and tuning Lower, GQA models stabilize after minimal uptraining Applicable Scenarios Applications with high performance requirements but insensitive to inference speed Scenarios requiring extremely fast inference with lower model performance demands Applications needing a balance between inference speed and model performance In summary, from a theoretical standpoint, all three attention mechanisms (MHA, MQA, GQA) share \\(\\mathcal{O}(B \\times S^2 \\times d)\\) complexity in a full pass and \\(\\mathcal{O}(B \\times S_{\\text{past}} \\times d)\\) per-step complexity in incremental decoding.\nExperimental Results Performance Testing This experiment was conducted on an environment equipped with dual NVIDIA RTX 4090 GPUs using data parallelism (DP), evenly splitting the batch size across both GPUs. The experiment only tested the performance of the forward pass, including average latency time (Time_mean, unit: ms) and peak memory usage (Peak_Mem_mean, unit: MB), to evaluate the resource requirements and efficiency of different attention mechanisms (MHA, MQA, and GQA) during the inference phase. You can get the source code in benchmark_attention.py.\nThe tests were based on Llama3 8B hyperparameters. Fig. 5. Overview of the key hyperparameters of Llama 3. (Image source: Grattafiori et al., 2024)\nThe main configuration parameters are as follows:\nTotal Layers: 32 layers. Hidden Layer Dimension: 4096. Total Number of Multi-Head Attention Heads: 32. Different Group Configurations (nums_kv_head): 32 (MHA), 1 (MQA), 8 (GQA-8). Experimental Results This section primarily introduces the experimental performance of MHA, MQA, and GQA-8 under different sequence lengths (512, 1024, and 1536), including data on latency and memory usage. For ease of comparison, the table below presents the specific test results for the three attention mechanisms.\nModel Size Method nums_kv_head Seq Length Time_mean (ms) Peak_Mem_mean (MB) Llama3 8B GQA-8 8 512 40.8777 2322.328 Llama3 8B MHA 32 512 53.0167 2706.375 Llama3 8B MQA 1 512 37.3592 2210.314 Llama3 8B GQA-8 8 1024 85.5850 6738.328 Llama3 8B MQA 1 1024 80.8002 6570.314 Llama3 8B MHA 32 1024 102.0514 7314.375 Llama3 8B GQA-8 8 1536 147.5949 13586.328 Llama3 8B MHA 32 1536 168.8142 14354.375 Llama3 8B MQA 1 1536 141.5059 13362.314 Fig. 6. Average Time Benchmark.\nFig. 7. Average Peak Memory Benchmark.\nIn scenarios sensitive to memory and time overheads, MQA and GQA-8 are more efficient choices, with MQA performing the best but potentially lacking in model performance capabilities; GQA-8 achieves a good balance between efficiency and performance.\nGQA Paper Experimental Results Inference Performance Fig. 8. Inference time and performance comparison. (Image source: Ainslie et al., 2023)\nFig. 9. Additional Experimental Results. (Image source: Ainslie et al., 2023)\nThe experimental results show that:\nInference Speed:\nMHA-XXL\u0026rsquo;s inference time is significantly higher than MHA-Large, primarily due to its larger number of heads and model size. Compared to MHA-XXL, MQA-XXL and GQA-8-XXL reduce inference time to approximately 1/6 and 1/5, respectively. Performance:\nMHA-XXL performs best across all tasks but has longer inference times. MQA-XXL has an advantage in inference speed, with average scores only slightly lower than MHA-XXL. GQA-8-XXL has inference speed close to MQA-XXL but nearly matches MHA-XXL in performance, demonstrating the efficiency and superiority of GQA. Checkpoint Conversion Fig. 10. Ablation Study on Checkpoint Conversion Methods. (Image source: Ainslie et al., 2023)\nFigure 10 compares the performance of different methods for checkpoint conversion. The mean pooling method performs best in retaining model information, followed by selecting the first head, while random initialization performs the worst. Mean pooling effectively integrates information from multiple \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads, maintaining model performance.\nUptraining Ratio Fig. 11. Ablation Study on Uptraining Ratios. (Image source: Ainslie et al., 2023)\nFigure 11 shows how performance varies with uptraining proportion for T5 XXL with MQA and GQA.\nGQA: Even with only conversion (no uptraining), GQA already has certain performance. As the uptraining ratio increases, performance continues to improve. MQA: Requires at least a 5% uptraining ratio to achieve practical performance, and as the ratio increases, performance gains tend to plateau. Effect of Number of GQA Groups on Inference Speed Fig. 12. Effect of the Number of GQA Groups on Inference Speed. (Image source: Ainslie et al., 2023)\nFigure 12 demonstrates that as the number of groups increases, inference time slightly rises, but it still maintains a significant speed advantage over MHA. Choosing an appropriate number of groups, such as 8, can achieve a good balance between speed and performance. Figure 3 also shows that models ranging from 7B to 405B parameters in Llama3 adopt 8 as the number of groups (key/value heads = 8).\nOther Optimization Methods In addition to optimizing the attention mechanism, researchers have proposed various methods to enhance the inference and training efficiency of Transformer models:\nLoRA (Hu et al., 2021): Achieves efficient parameter fine-tuning by adding low-rank matrices to the pretrained model\u0026rsquo;s weight matrices. Flash Attention (Dao et al., 2022): Reduces memory and computational overhead by optimizing attention calculations. Quantization Techniques: LLM.int8 (Dettmers et al., 2022) and GPTQ (Frantar et al., 2022) reduce memory usage and computational costs by lowering the precision of model weights and activations. Model Distillation (Hinton et al., 2015): Reduces model size by training smaller models to mimic the behavior of larger models. Speculative Sampling (Chen et al., 2023): Enhances generation efficiency through parallel generation and filtering. Key Takeaways Uptraining methods can effectively utilize existing MHA model checkpoints. By performing a small amount of additional training, they can transform these into more efficient MQA or GQA models, significantly reducing training costs. Grouped-Query Attention (GQA) strikes a good balance between inference efficiency and model performance, making it especially suitable for applications requiring both high-efficiency inference and high performance. Experimental results demonstrate that GQA can significantly improve inference speed while maintaining performance comparable to MHA models, making it suitable for large-scale model deployment and real-time applications. References [1] Vaswani A. Attention is all you need [J]. Advances in Neural Information Processing Systems, 2017.\n[2] Devlin J. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] Radford A. Improving Language Understanding by Generative Pre-Training [J]. 2018.\n[4] Touvron H, Lavril T, Izacard G, et al. LLaMA: Open and Efficient Foundation Language Models [J]. arXiv preprint arXiv:2302.13971, 2023.\n[5] Achiam J, Adler S, Agarwal S, et al. GPT-4 Technical Report [J]. arXiv preprint arXiv:2303.08774, 2023.\n[6] Shazeer N. Fast Transformer Decoding: One Write-Head is All You Need [J]. arXiv preprint arXiv:1911.02150, 2019.\n[7] Ainslie J, Lee-Thorp J, de Jong M, et al. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints [J]. arXiv preprint arXiv:2305.13245, 2023.\n[8] Hu E J, Shen Y, Wallis P, et al. LoRA: Low-Rank Adaptation of Large Language Models [J]. arXiv preprint arXiv:2106.09685, 2021.\n[9] Dettmers T, Lewis M, Belkada Y, et al. GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale [J]. Advances in Neural Information Processing Systems, 2022, 35: 30318-30332.\n[10] Frantar E, Ashkboos S, Hoefler T, et al. GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers [J]. arXiv preprint arXiv:2210.17323, 2022.\n[11] Hinton G. Distilling the Knowledge in a Neural Network [J]. arXiv preprint arXiv:1503.02531, 2015.\n[12] Chen C, Borgeaud S, Irving G, et al. Accelerating Large Language Model Decoding with Speculative Sampling [J]. arXiv preprint arXiv:2302.01318, 2023.\nCitation Citation: To reproduce or cite the content of this article, please acknowledge the original author and source.\nCited as:\nYue Shui. (Jan 2025). Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA.\nhttps://syhya.github.io/posts/2025-01-16-gqa-attention/\nOr\n@article{syhya2025gqa, title = \u0026#34;Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-16-gqa-attention/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-01-16-gqa-attention/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eThe Transformer (\u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eVaswani et al., 2017\u003c/a\u003e) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (\u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eDevlin et al., 2018\u003c/a\u003e) which uses only the encoder, GPT (\u003ca href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\"\u003eRadford et al., 2018\u003c/a\u003e) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (\u003ca href=\"https://arxiv.org/abs/2302.13971\"\u003eTouvron et al., 2023\u003c/a\u003e) and GPT-4 (\u003ca href=\"https://arxiv.org/abs/2303.08774\"\u003eOpenAI et al., 2024\u003c/a\u003e), most of which adopt a decoder-only architecture.\u003c/p\u003e","title":"Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA"},{"content":"Background With the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.\nBased on my work experience, this article summarizes how to build LLMs equipped with specific domain knowledge by leveraging data preparation, model training, deployment, evaluation, and continuous iteration on top of existing general models.\nWhy Inject Domain Knowledge into the Foundational LLMs? Challenge 1: Limited Domain Knowledge Existing pre-trained models (such as GPT-4 and Llama 3) are primarily trained on general-purpose corpora, lacking in-depth understanding of niche languages or proprietary domains. This deficiency leads to subpar performance when the models handle programming code.\nChallenge 2: Data Security and Compliance When enterprises handle sensitive data, they must adhere to strict data sovereignty and compliance requirements. Uploading proprietary business data to third-party cloud services poses security risks, necessitating data processing and model training within local environments.\nChallenge 3: Limitations of OpenAI Fine-Tuning Mainstream commercial APIs for fine-tuning are typically basic and struggle to achieve deep alignment and optimization. For highly customized domain models, such approaches often fail to meet the required specifications.\nTwo Approaches of Injecting Knowledge In practical projects, the common methods for injecting domain knowledge into base models include Fine-Tuning and Retrieval-Augmented Generation (RAG). The following sections provide a detailed comparison of these methods to aid in selecting the most suitable strategy.\nMethod Comparison Fine-Tuning Core Concept\nThrough continued pre-training, supervised fine-tuning, and preference alignment, directly update the model parameters to enable it to master domain-specific knowledge and task patterns.\nTechnical Details\nContinued Pre-Training (CPT): Continue pre-training the base model on a large volume of domain-specific unsupervised data. Supervised Fine-Tuning (SFT): Perform supervised fine-tuning using high-quality labeled data. Preference Alignment (DPO): Optimize model outputs based on user feedback. Parameter Tuning Methods: Utilize full-parameter fine-tuning or combine with PEFT methods like LoRA to freeze some parameters and add adapters. Advantages\nDeep Customization: Updating the internal weights of the model enables a profound understanding of domain knowledge. No External Retrieval Dependency: Inference does not require additional knowledge bases, reducing latency and total token consumption. Enhanced Overall Performance: Significantly outperforms general models in domain-specific tasks. Disadvantages\nHigh Computational Cost: Requires substantial computational resources for training, especially during the CPT phase. Long Training Cycles: From data preparation to model training and optimization, the process is time-consuming. Catastrophic Forgetting: The model may forget its original general capabilities while learning new knowledge. Retrieval-Augmented Generation (RAG) Core Concept\nBuild a domain-specific knowledge base and retrieve relevant documents during inference to assist the model in generating more accurate responses without directly altering model parameters.\nTechnical Details\nData Processing: Preprocess domain documents by chunking them based on size and overlap. Vectorization: Embedding text chunks as vectors using embedding models and storing them in a Vector Store for retrieval. Retrieval: During inference, retrieve relevant documents through similarity search to provide contextual information or few-shot examples to the base model. Advantages\nPreserves General Capabilities: Model parameters remain unchanged, retaining general language abilities. Quick Updates: The knowledge base can be dynamically updated without retraining the model. Computational Efficiency: Avoids large-scale training, saving computational resources. Disadvantages\nDependence on Knowledge Base Quality: The quality of retrieved documents directly impacts response quality. Inference Speed: The retrieval process may increase inference latency and require more tokens. Limited Knowledge Coverage: The model’s internal knowledge is still restricted by the base model’s pre-training data. Models and Training Resources Base Models Taking the Llama 3 series as an example, it features the following characteristics:\nParameter Scale\nThe Llama 3 series includes models ranging from 1B to 405B parameters, widely supporting multilingual processing, code generation, reasoning, as well as visual and textual tasks. Smaller models (1B and 3B) are specially optimized for edge and mobile devices, supporting up to 128K context windows, efficiently handling local tasks such as summary generation, instruction execution, and text rewriting.\nMultimodal Capabilities\nLlama 3\u0026rsquo;s visual models (11B and 90B parameters) outperform many closed models in image understanding tasks and support multimodal processing of images, videos, and audio. All models support fine-tuning, facilitating customized development for specific domains.\nOpen Source and Community Support\nLlama 3 series models and their weights are released in open-source form and can be accessed via llama.com and the Hugging Face platform, providing convenient access and application support for developers.\nDataset Restrictions\nAlthough the Llama 3 models are released as open-source, the datasets used for their training are not open-sourced. Therefore, strictly speaking, Llama 3 is not entirely open-source. This limitation may pose challenges in addressing catastrophic forgetting, as obtaining data sets identical to the original training data is difficult.\nTraining Resources Training large language models requires robust computational resources and efficient distributed training frameworks.\nHardware Resources\nGPU Clusters: NVIDIA A100 or H100 GPUs are recommended, with configurations of 4 or 8 GPUs connected via NVLink or InfiniBand to enhance communication bandwidth. Storage Resources: High-performance SSDs (e.g., NVMe) to support fast data read and write operations. Software Frameworks\nDistributed Training Frameworks: DeepSpeed, Megatron-LM, among others, support large-scale model training. Inference Frameworks: vLLM, ollama, etc., optimize inference speed and resource utilization. Parallel Strategies\nData Parallelism (DP): Suitable when the model fits on a single GPU, implemented via DeepSpeed\u0026rsquo;s ZeRO Stage 0. Model Parallelism (MP), Pipeline Parallelism (PP), and Tensor Parallelism (TP): When the model cannot fit on a single GPU, optimize using ZeRO Stage 1, 2, or 3, or employ ZeRO-Infinity to offload parts of parameters and optimizer states to CPU or NVMe. DeepSpeed ZeRO Sharding Strategies Comparison ZeRO Stage Sharding Strategies ZeRO Stage Description GPU Memory Usage Training Speed ZeRO-0 Pure data parallelism without any sharding. All optimizer states, gradients, and parameters are fully replicated on each GPU. Highest Fastest ZeRO-1 Shards optimizer states (e.g., momentum and second moments), reducing GPU memory usage, but gradients and parameters remain data parallel. High Slightly slower than ZeRO-0 ZeRO-2 Shards optimizer states and gradients, further reducing GPU memory usage based on ZeRO-1. Medium Slower than ZeRO-1 ZeRO-3 Shards optimizer states, gradients, and model parameters, achieving the lowest GPU memory usage, suitable for extremely large models. Requires parameter broadcasting (All-Gather/All-Reduce) during forward/backward passes, significantly increasing communication overhead. Low Significantly slower than ZeRO-2, depends on model size and network bandwidth Offload Strategies Offload Type Description GPU Memory Usage Training Speed ZeRO-1 + CPU Offload Extends ZeRO-1 by offloading optimizer states to CPU memory, further reducing GPU memory usage but necessitating CPU-GPU data transfer, relying on PCIe bandwidth, and occupying CPU memory. Medium-low Slower than ZeRO-1, affected by CPU performance and PCIe bandwidth ZeRO-2 + CPU Offload Extends ZeRO-2 by offloading optimizer states to CPU memory, further reducing GPU memory usage for larger models but increasing CPU-GPU data transfer overhead. Lower Slower than ZeRO-2, affected by CPU performance and PCIe bandwidth ZeRO-3 + CPU Offload Extends ZeRO-3 by offloading optimizer states and model parameters to CPU, achieving minimal GPU memory usage but with extremely high CPU-GPU communication volume and CPU bandwidth significantly lower than GPU-GPU communication. Extremely Low Very Slow ZeRO-Infinity (NVMe Offload) Based on ZeRO-3, offloads optimizer states, gradients, and parameters to NVMe, breaking CPU memory limits and suitable for ultra-large-scale models; performance highly depends on NVMe parallel read/write speeds. Extremely LowRequires NVMe support Slower than ZeRO-3 but generally faster than ZeRO-3 + CPU Offload, can achieve better throughput if NVMe bandwidth is sufficient Communication Volume and Performance Impact ZeRO-0/1/2:\nCommunication is primarily gradient synchronization using All-Reduce operations, resulting in relatively low communication volume.\nZeRO-3:\nRequires All-Gather/All-Reduce operations for model parameters, significantly increasing communication volume. Network bandwidth becomes a critical bottleneck, and parameter broadcasting during forward/backward passes further exacerbates communication load.\nCPU Offload (ZeRO-1/2/3 + CPU):\nOffloads optimizer states or parameters to CPU, reducing GPU memory usage. Communication volume mainly arises from CPU \u0026lt;-\u0026gt; GPU data transfers, which have much lower bandwidth compared to GPU-GPU communication, easily causing performance bottlenecks, especially in ZeRO-3 scenarios. NVMe Offload (ZeRO-Infinity):\nFurther offloads to NVMe based on ZeRO-3, overcoming CPU memory limitations to support ultra-large-scale models. Performance heavily relies on NVMe I/O bandwidth and parallelism. If NVMe speed is sufficiently high, it typically outperforms CPU Offload; however, performance may suffer in scenarios with weak I/O performance or high latency. Hardware and Configuration Impact Hardware Constraints:\nPCIe Bandwidth, Network Bandwidth, NVMe I/O, etc., significantly impact Offload performance. Optimal strategies should be selected based on the hardware environment. Additional Notes:\nCPU Offload utilizes CPU memory and transfers data via PCIe; NVMe Offload saves states on NVMe devices. NVMe Offload generally outperforms CPU Offload when NVMe I/O performance is adequate, but care must be taken to avoid performance bottlenecks caused by insufficient I/O performance. Reference to Official Documentation:\nIt is recommended to consult the DeepSpeed official documentation for the latest and most accurate configuration parameters and performance tuning advice. Data Preparation: The Core of Training Success Data quality directly determines model performance. Data preparation includes data collection, cleaning, deduplication, categorization and balancing, anonymization, and other steps.\nPre-Training Data Data Sources Public Datasets: Such as the-stack-v2, Common Crawl, etc. Enterprise Proprietary Data: Internal documents, code repositories, business logs, etc. Web Crawlers: Collect domain-relevant web content using crawling technologies. Data Scale It is recommended to use at least hundreds of millions to billions of tokens to ensure the model can thoroughly learn domain knowledge. When data volume is insufficient, model performance may be limited. Data augmentation methods are suggested to supplement the data. Data Processing Data Preprocessing\nUniform Formatting: Process large volumes of unlabeled corpora from multiple data sources to ensure consistent formatting. It is recommended to use efficient storage formats like Parquet to improve data reading and processing efficiency. Data Deduplication\nDetection Methods: Use algorithms such as MinHash, SimHash, or cosine similarity for approximate duplicate detection. Granularity of Processing: Choose to deduplicate at the sentence, paragraph, or document level, adjusting flexibly based on task requirements. Similarity Threshold: Set a reasonable similarity threshold (e.g., 0.9) to remove texts with duplication above the threshold, ensuring data diversity. Data Cleaning\nText Filtering: Remove garbled text, spelling errors, and low-quality text by combining rule-based methods and model scorers (e.g., BERT/RoBERTa). Formatting Processing: Prefer using JSON format to handle data, ensuring the accuracy of special formats like code, Markdown, and LaTeX. Data Anonymization\nPrivacy Protection: Anonymize or remove sensitive information such as names, phone numbers, emails, passwords, etc., to ensure data compliance. Filtering Non-Compliant Content: Remove data blocks containing illegal, pornographic, or racially discriminatory content. Data Mixing and Balancing\nProportion Control: For example, combine 70% domain-specific data with 30% general data to prevent the model from forgetting general capabilities. Task Types: Ensure the data includes various task types such as code generation, Q\u0026amp;A dialogue, document summarization, multi-turn conversations, and mathematical reasoning. Data Sequencing\nProgressive Guidance: Use Curriculum Learning to start training with simple, clean data and gradually introduce more complex or noisy data, optimizing the model\u0026rsquo;s learning efficiency and convergence path. Semantic Coherence: Utilize In-Context Pretraining techniques to concatenate semantically similar documents, enhancing contextual consistency and improving the model\u0026rsquo;s depth of semantic understanding and generalization ability. Supervised Fine-Tuning Data Data Format Adopt Alpaca or Vicuna styles, such as single-turn and multi-turn dialogues structured as [instruction, input, output].\nScale: From thousands to hundreds of thousands, depending on project requirements and computational resources. Quality: Ensure high-quality and diverse data to prevent the model from learning errors or biases. Data Construction During the data construction process, we first collect daily business data and collaboratively build foundational questions with business experts. Subsequently, we use large language models for data augmentation to enhance data diversity and robustness. The specific data augmentation strategies are as follows:\nData Augmentation Strategies Diverse Expressions\nRewrite existing data using large language models through synonym replacement and syntactic transformations to increase data diversity.\nRobustness Enhancement\nCreate prompts containing spelling errors, mixed languages, and other input variations to simulate real-world scenarios while ensuring high-quality generated answers.\nKnowledge Distillation\nUtilize large language models like GPT-4 and Claude for knowledge distillation to generate Q\u0026amp;A pairs that meet requirements.\nComplex Task Design\nManually design high-quality data for complex scenarios (e.g., multi-turn dialogues, logical reasoning) to cover the model\u0026rsquo;s capability boundaries.\nData Generation Pipeline\nBuild an automated data generation pipeline that integrates data generation, filtering, formatting, and validation to improve overall efficiency.\nKey Points Task Type Annotation: Clearly annotate each data entry with its task type to facilitate subsequent fine-grained analysis and tuning. Multi-Turn Dialogues and Topic Switching: Construct data that captures contextual coherence and topic transitions in multi-turn dialogues to ensure the model learns the ability to handle topic switching and maintain contextual relevance. Chain of Thought (CoT) Strategy: For classification and reasoning tasks, generate procedural answers using CoT to improve accuracy. Data Flywheel: Continuously collect real user queries after deployment, iterating data based on real needs; regularly clean the data to ensure quality and diversity. Preference Data Data Format Triple Structure: [prompt, chosen answer, rejected answer] Annotation Details: Multi-Model Sampling: Generate answers using multiple models at different training stages or with different data ratios to increase data diversity. Editing and Optimization: Annotators can make slight modifications to the chosen answers to ensure answer quality. Sampling Strategies Multi-Model Sampling: Deploy multiple versions of the model to generate diverse answers for the same prompt. Comparative Annotation: Use manual or automated systems to compare generated answers and select superior answer pairs. Key Points Data Diversity and Coverage: Ensure preference data covers various scenarios and tasks to prevent the model from underperforming in specific contexts. High-Quality Annotation: The quality of preference data directly affects the model\u0026rsquo;s alignment, requiring accurate and consistent annotations. Training Process A complete training process for a domain-specific large language model typically includes Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO) as the three main steps, ultimately achieving model deployment and continuous optimization.\nComparison of Three Methods Training Method Overview Training Method Main Objective Data Requirements Typical Application Scenarios Continued Pre-Training (CPT) Continue pre-training on large-scale unsupervised corpora to inject new domain knowledge Large amounts of unlabeled text (at least hundreds of millions to billions of tokens) Supplementing domain knowledge, such as specialized texts in law, medicine, finance, etc. Supervised Fine-Tuning (SFT) Fine-tune on supervised labeled data to strengthen specific tasks and instruction execution capabilities Customized labeled data (instruction/dialog pairs), ranging from thousands to hundreds of thousands Various specific tasks, such as code generation, Q\u0026amp;A, text rewriting, complex instruction execution, etc. Direct Preference Optimization (DPO) Optimize model outputs to align with human preferences using preference data (chosen vs. rejected) Preference data: [prompt, chosen, rejected](relatively smaller scale) Aligning with human feedback, such as response style, compliance, safety, etc. Advantages and Challenges Continued Pre-Training (CPT) Advantages:\nBetter domain coverage, comprehensively enhancing the model\u0026rsquo;s understanding and generation capabilities in specific domains. No need for additional manual annotation. Challenges/Limitations:\nRequires a large volume of high-quality domain data. High training costs, necessitating massive computational power and time. May introduce domain biases, necessitating careful handling of data quality and distribution. Supervised Fine-Tuning (SFT) Advantages:\nQuickly acquires task execution capabilities. Significantly improves accuracy in specific scenarios. Challenges/Limitations:\nHigh data annotation costs. Requires careful selection of labeled data to avoid overfitting. Fine-tuning may weaken the model\u0026rsquo;s generality. Direct Preference Optimization (DPO) Advantages:\nNo need to train a separate Reward Model. Requires less data and computational resources to achieve similar or better results compared to PPO. Challenges/Limitations:\nRequires reliable preference annotations. Continues to need more preference data for complex and diverse scenarios. Easily constrained by the distribution of preference data. General Training Tips and Technical Details When performing CPT, SFT, and DPO, there are numerous general training tips and technical details. The following sections uniformly describe these general aspects for better understanding and application.\nData Processing and Preparation Data Quality: Regardless of CPT, SFT, or DPO, data quality is crucial. Ensure data accuracy, unambiguity, and diversity. Data Formatting: Consistent data formats simplify the training process. For example, using JSON or other structured formats to store training data. Data Augmentation: Increase data diversity and improve the model\u0026rsquo;s generalization ability through methods like LLM rewriting and optimization. Learning Rate and Optimization Learning Rate Settings: Typically use a smaller learning rate than during pre-training, such as reducing from 3e-4 to 3e-5, depending on the task and data volume. Learning Rate Scheduling: Use warm-up strategies (e.g., linearly increasing for the first 10% of steps), followed by linear decay or cosine annealing to ensure a smooth training process. Optimizer Selection: Choose suitable optimizers based on model size and hardware resources, such as AdamW. Training Strategies Full-Parameter Fine-Tuning: When resources permit, prioritize full-parameter fine-tuning to ensure the model fully captures new knowledge. Parameter-Efficient Fine-Tuning (PEFT): Methods like LoRA are suitable for scenarios with limited computational resources by freezing some parameters and adding adapters for efficient fine-tuning. Mixed Precision Training: Use bf16 or fp16 on supported GPUs to reduce memory usage and increase training speed. Training Stability: Employ techniques such as gradient clipping, regularization, dropout, and weight decay to prevent gradient explosion and model overfitting. Flash Attention: Utilize Flash Attention to optimize the computation efficiency of the attention mechanism, enhancing training speed and reducing memory usage. Monitoring and Tuning Convergence Monitoring: Continuously monitor loss curves on training and validation sets to ensure the model is converging properly. Adjust learning rates and other hyperparameters as needed. Checkpoint: Regularly save checkpoints to prevent loss of all training progress due to unexpected interruptions. Early Stopping: Prevent model overfitting by stopping training at an appropriate time and saving the best model state. Model Evaluation: Conduct periodic evaluations during training to ensure model performance meets expectations. Continued Pre-Training (CPT) Objective Inject new domain knowledge into the base model by continuing pre-training on a large volume of domain-specific unsupervised data, enhancing the model\u0026rsquo;s understanding and generation capabilities in the specific domain.\nTraining Tips Streaming Data Loading\nImplement streaming data loading to dynamically read data during training, preventing memory overflows and training interruptions. Full-Parameter Fine-Tuning\nTypically, update all model parameters during training to ensure comprehensive knowledge acquisition. Compared to parameter-efficient fine-tuning methods (e.g., LoRA), full-parameter fine-tuning offers better domain knowledge injection, especially when computational resources are abundant. It is recommended to prioritize full-parameter fine-tuning under such conditions. Supervised Fine-Tuning (SFT) Objective Enhance the model\u0026rsquo;s practicality and accuracy by training it on high-quality labeled data to perform specific tasks such as code generation, code repair, and complex instruction execution.\nTraining Tips Number of Epochs\nTypically, 1 to 4 epochs are sufficient to observe significant effects when data volume is adequate. If data volume is insufficient, consider increasing the number of epochs while being mindful of overfitting risks. Data augmentation is recommended in such cases. Data Augmentation and Diversity\nEnsure training data covers a variety of task types and instruction expressions to improve the model\u0026rsquo;s generalization ability. Include multi-turn dialogues and robustness data to enhance the model\u0026rsquo;s capability to handle real user scenarios. Direct Preference Optimization (DPO) Objective Optimize model outputs to better align with human expectations and needs, including response style, safety, and readability, by leveraging user feedback and preference data.\nCharacteristics of DPO Direct Optimization\nDoes not require training a separate Reward Model. Instead, directly performs contrastive learning on (chosen, rejected) data pairs.\nEfficiency\nCompared to PPO, DPO requires less data and computational resources to achieve similar or better results.\nDynamic Adaptation\nThe model can immediately adapt whenever new data is available without the need to retrain a Reward Model.\nTraining Tips Collecting Preference Data\nDeploy multiple models at different training stages or with different data ratios to generate diverse responses. Annotate chosen and rejected answer pairs through manual or automated means to ensure data diversity and quality. Contrastive Learning\nOptimize model parameters by maximizing the probability of chosen answers and minimizing the probability of rejected answers. Iterative Optimization\nContinuously collect user feedback, generate new preference data, and perform iterative training to gradually enhance model performance. Implement a data flywheel mechanism to achieve ongoing model evolution and optimization. Common Issues and Solutions Repetitive Outputs\nIssue: The model generates repetitive content, continuously printing without stopping.\nSolutions:\nData Deduplication and Cleaning: Ensure training data does not contain a large amount of repetitive content. Check EOT (End-of-Token) Settings: Prevent the model from continuously generating without stopping. Align via SFT/DPO: Optimize model output quality. Adjust Decoding Strategies: Increase parameters like top_k, repetition penalty, and temperature. Catastrophic Forgetting\nIssue: The model forgets its original general capabilities during fine-tuning, effectively overfitting to the new dataset and causing excessive changes to the original model parameter space.\nSolutions:\nMix in Some General Data: Maintain the model’s general capabilities. Lower the Learning Rate: Reduce the impact on existing knowledge. Increase Dropout Rate and Weight Decay: Prevent overfitting. Use Parameter-Efficient Fine-Tuning Methods like LoRA: Avoid large-scale parameter updates. Utilize RAG Assistance: Combine with external knowledge bases to enhance model performance. Chat Vector: Quickly inject conversational and general capabilities into the model through simple arithmetic operations on model weights. Insufficient Understanding of Entity Relationships and Reasoning Paths\nIssue: The model struggles to correctly understand complex entity relationships and reasoning paths.\nSolutions:\nIntroduce Chain-of-Thought (CoT) Data and Enhanced Reasoning Training: Improve the model\u0026rsquo;s capabilities through step-by-step reasoning training, combined with Reinforcement Fine-Tuning and o1/o3 training methods. Expand Training Data Coverage: Incorporate more diverse scenarios containing complex entity relationships and reasoning paths. Combine with Knowledge Graph Modeling: Use GraphRAG to strengthen the model\u0026rsquo;s understanding and reasoning abilities regarding entity relationships. Model Deployment and Evaluation Deployment Inference Frameworks\nollama: Local inference deployment based on llama.cpp, enabling quick startups. vLLM: Optimized for high concurrency and inference throughput in multi-user scenarios. Quantization: Quantize the model to 8-bit or 4-bit to further reduce inference costs and improve deployment efficiency. Integrate RAG \u0026amp; Agents\nRAG: Combine with a vector knowledge base to retrieve relevant documents or code snippets in real-time, assisting the model in generating more accurate responses. Agents: Utilize Function Calls or multi-turn dialogue mechanisms to enable the model to invoke external tools or perform complex reasoning, enhancing interactivity and practicality. Langgraph: Encapsulate RAG and multi-agent workflows to build customized dialogue systems or automated code generation platforms. Evaluation Evaluation Metrics\nCPT Phase: Use domain-specific test sets to evaluate perplexity (PPL) or cross-entropy, measuring the model\u0026rsquo;s mastery of new knowledge. SFT/DPO Phase: Human or Model Evaluation: Assess the accuracy, coherence, readability, and safety of responses through human ratings or automated tools. Code Generation: Build a large-scale unit test set to evaluate the pass@k metric, measuring the correctness rate of code generation. General Capabilities: Test the model on common benchmarks (e.g., MMLU, CMMLU) to ensure minimal performance degradation on general tasks. Decoding Hyperparameters\nConsistency: Maintain consistent decoding parameters such as top_k, top_p, temperature, and max_new_tokens during evaluation to ensure comparability of results. Grid Search: When computational resources permit, evaluate different combinations of decoding parameters to select the optimal configuration. Data Flywheel and Continuous Iteration Data Flywheel Mechanism\nReal-Time Collection of User Logs\nCollect real user prompts and generated responses online, covering diverse usage scenarios and task types. Automated or Manual Annotation\nAnnotate collected user prompts and responses with preferences, generating new (chosen, rejected) data pairs. Iterative Training\nIncorporate newly generated preference data into the next round of SFT/DPO training to continuously optimize response quality and user experience. Robustness Data\nInclude data with spelling errors, mixed languages, vague instructions, etc., to enhance the model’s robustness and ability to handle real-world scenarios. Continuous Optimization\nFeedback Loop: Utilize user feedback to continuously improve training data and model performance, achieving self-optimization and evolution of the model. Multi-Model Collaboration: Deploy multiple versions of the model to generate diverse responses, enhancing the model\u0026rsquo;s comprehensive capabilities through contrastive learning. Integrating Intent Recognition and Multi-Agent Reasoning Use an intent classification model to allow the large model to determine the category of user input intent. Based on the mapping between intent categories and context types, supervise the reasoning path, and then perform multi-way retrieval based on the reasoning path. Provide this information to the trained model to generate the final result.\nConclusion Through the combination of Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO), it is possible to effectively inject domain-specific knowledge into base large models, constructing closed-source LLMs capable of efficiently solving business problems. The key steps are as follows:\nData Preparation\nHigh-quality data collection, cleaning, deduplication, and categorization to ensure data diversity and accuracy. Implement data anonymization strategies to protect privacy and ensure compliance. Model Training\nUse CPT to inject domain knowledge, SFT to learn specific task patterns, and DPO to optimize model outputs to align with human preferences and safety. Leverage efficient parallel training frameworks and hyperparameter tuning techniques to enhance training efficiency and resource utilization. Deployment and Evaluation\nEmploy efficient inference frameworks, integrating RAG and Agents for knowledge enhancement and functional extension. Conduct multi-dimensional evaluations to ensure the model performs as expected at each stage. Continuous Iteration\nBuild a data flywheel to continuously collect user feedback and optimize training data and model performance. Integrate RAG and Agents to achieve ongoing improvement and expansion of model capabilities. Ultimately, through a systematic process and technical measures, it is possible to construct an AI system with not only profound domain knowledge but also the flexibility to handle complex business requirements over its lifecycle.\nReferences DeepSpeed Megatron-LM ollama vLLM GraphRAG The Llama 3 Herd of Models ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages Evaluating Large Language Models Trained on Code Direct Preference Optimization: Your Language Model is Secretly a Reward Model Citation Citation: To reproduce or cite the content of this article, please acknowledge the original author and source.\nCited as:\nYue Shui. (Jan 2025). Building Domain-Specific LLMs.\nhttps://syhya.github.io/posts/2025-01-05-build-domain-llm\nOr\n@article{syhya2024domainllm, title = \u0026#34;Building Domain-Specific LLMs\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-05-build-domain-llm/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-01-05-domain-llm-training/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eWith the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.\u003c/p\u003e","title":"Building Domain-Specific LLMs"},{"content":"Rent a GPU or Buy Your Own? Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\nAdvantages of Renting GPUs:\nNo high upfront hardware costs Elastic scalability according to project needs Cloud vendors typically provide data compliance and security assurances, reducing hardware maintenance concerns Advantages of Buying GPUs:\nLower total cost if used extensively over the long term Higher privacy and control for in-house data and models Hardware can be upgraded or adjusted at any time, offering more flexible deployment Personal Suggestions\nIf you have a limited budget or are just starting out, use Colab, Kaggle, or cloud-based GPU solutions first. Once computing needs and privacy requirements increase, consider building your own multi-GPU server or renting a multi-node, multi-GPU cluster. Background In September 2023, to continue my exploration and research on Large Language Models (LLMs) outside of work, I assembled a dual RTX 4090 personal AI server. It has been running for nearly a year, and here are some observations:\nNoise: Placed under my desk, the fans can get quite loud under full load. However, noise levels are acceptable during inference or moderate loads. Inference Performance: With a total of 48GB of VRAM, and by using 4-bit quantization, it can handle 70B-level models (e.g., Llama 70B, Qwen 72B). Training Performance: By using DeepSpeed with distributed and offload techniques (ZeRO-3 + CPU offload), I can finetune around 34B parameter models (e.g., CodeLlama 34B). Cost-Effectiveness: For personal or small-team experiments and small-to-medium scale model training, this configuration is quite practical. However, full-scale training of very large models (hundreds of billions of parameters) still requires more professional GPUs (e.g., multi-GPU A100 or H100 clusters). Below is an illustration of VRAM requirements for various model sizes and training approaches :\nFig. 1. Hardware Requirement. (Image source: LLaMA-Factory)\nAssembly Strategy \u0026amp; Configuration Details The total budget is around 40,000 RMB (~6,000 USD). The final build list is as follows (for reference only):\nComponent Model Price (RMB) GPU RTX 4090 * 2 25098 Motherboard + CPU AMD R9 7900X + MSI MPG X670E CARBON 5157.55 Memory Corsair 48GB * 2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + Samsung 990PRO 4TB 4587 PSU Corsair AX1600i 2699 Fans Phanteks T30 120mm P * 6 1066.76 CPU Cooler Thermalright FC140 BLACK 419 Chassis Phanteks 620PC Full Tower 897.99 GPU Riser Cable Phanteks FL60 PCI-E4.0 *16 399 Total: ~ 42,723.3 RMB\nGPU Selection For large-scale model research, floating-point performance (TFLOPS) and VRAM capacity are the most critical metrics. Professional GPUs (A100, H100, etc.) boast higher VRAM and NVLink support but can easily cost hundreds of thousands of RMB, which is prohibitive for most individual users. According to Tim Dettmers, the RTX 4090 offers excellent performance-per-dollar and supports features like BF16 and Flash Attention, making it a high-value option.\nCooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling Cooling Method Advantages Disadvantages Best For Blower Fan Compact form factor; good for multi-GPU setups Loud noise, generally weaker cooling Server racks, dense multi-GPU deployments Air-Cooling Good balance of performance and noise; easy upkeep Cards are often large, require space Home or personal research (with enough space) Liquid-Cooling Excellent cooling, quieter under full load Risk of leaks, higher cost Extreme quiet needs or heavy overclocking Home Setup Recommendation: Air-cooled GPUs are typically the best balance of cooling efficiency, noise, and maintenance. They are generally more user-friendly compared to blower or liquid-cooled alternatives.\nCPU \u0026amp; Motherboard In deep learning scenarios, the CPU handles data preprocessing, pipeline scheduling, and multi-process/thread management, ensuring high-throughput, low-latency data delivery to the GPUs. Thus, the CPU’s main requirements include sufficient PCIe lanes and robust multi-threaded performance.\nIntel: 13th/14th Generation i9 (e.g., 13900K) offers 20 primary PCIe lanes, enough for dual GPUs at x8 + x8. AMD: The Ryzen 7000/9000 series (e.g., 7950X) provides 28 (24 usable) PCIe lanes and also supports dual GPUs at x8 + x8, with enough bandwidth for multiple M.2 SSDs. MSI MPG X670E CARBON Motherboard Expandability: Supports PCIe 5.0 and DDR5 memory, offering ample future-proofing. Stability: High-quality power delivery, ensuring stable operation for both CPU and multiple GPUs. Rich I/O: Supports multiple M.2 SSDs and USB4 for various usage scenarios. AMD Ryzen 9 7900X Highlights Cores \u0026amp; Threads: 12 cores, 24 threads, excellent for data preprocessing and multitasking in deep learning workloads. PCIe Bandwidth: Provides 28 (24 usable) PCIe 5.0 lanes to support dual GPUs at x8 + x8 while also powering high-speed M.2 SSDs. Power Efficiency: Built on the Zen 4 architecture, delivering outstanding performance-to-power ratio for high-performance computing tasks. Key Motherboard Considerations Physical Layout RTX 4090 cards are huge, often occupying multiple slots. Confirm the board can hold two such cards simultaneously. If space or thermal conflicts arise, use a riser cable for vertical placement of the second GPU. PCIe Lane Splitting Ensure the motherboard can run two PCIe 4.0 x8 + x8 slots. Avoid a setup ending up as x16 + x2, which severely limits the second GPU’s bandwidth and can lead to a significant performance bottleneck, especially critical in large model training. Expandability With two GPUs installed, you still need enough M.2 slots and external ports for additional storage or peripherals. After evaluating expandability, performance, and cost-effectiveness, I chose the AMD Ryzen 9 7900X paired with the MSI MPG X670E CARBON. A GPU riser cable resolved the slot conflicts caused by the thickness of dual RTX 4090s.\nBIOS Setup Recommendations Memory Optimization Enable XMP/EXPO (Intel/AMD) to boost memory clock speeds and bandwidth. Overclocking If additional performance is needed, enable PBO (Precision Boost Overdrive) or Intel Performance Tuning and monitor system stability. Thermals \u0026amp; Stability Avoid extreme overclocking and keep temperatures under control to maintain system stability. Memory During deep learning training, large amounts of system memory are used for data loading and optimizer states (especially in multi-GPU scenarios with Zero-stage parallelism). Aim for at least 2× the total GPU VRAM capacity. This build uses 48GB * 2 (96GB in total), sufficient for everyday multitasking and distributed training without frequent swapping.\nStorage Prefer M.2 NVMe SSDs: They offer superior read/write performance, critical for loading large model weights, caching intermediate files, and storing training logs. Capacity ≥ 2TB: Model files continue to grow in size. 2TB can fill up quickly; consider 4TB or more depending on your needs. SSD Brands: Samsung, SK Hynix, and Western Digital have reliable high-end product lines. Power Supply Dual RTX 4090s can draw 900W–1000W under full load, and you also need overhead for the CPU, motherboard, and storage. Generally, 1,500W+ Platinum or Titanium PSUs are recommended to ensure stable power delivery and mitigate voltage fluctuations.\nI opted for a Corsair AX1600i (digital PSU), which provides real-time power monitoring and sufficient headroom.\nCooling \u0026amp; Fans I chose an air-cooling setup:\nCPU Cooler: Thermalright FC140, a dual-tower air cooler offering solid thermal performance with relatively low noise. Case Fans: Phanteks T30 120mm * 6, maintaining slightly positive internal pressure to ensure proper airflow across the GPUs and power delivery modules. For prolonged high-load training (such as distributed training of large models), chassis airflow management and adequate fans are crucial. Monitor CPU, GPU, and VRM temperatures regularly and adjust fan curves as needed.\nAdvanced Cooling\nFor even quieter operation, consider a Hybrid or partial liquid-cooling solution, along with finely tuned fan curves. Regularly cleaning dust filters, using dust guards, and refreshing thermal compound can also help improve thermals and stability. Chassis Because the RTX 4090 is massive, and two of them stacked requires ample internal space and airflow, a full-tower chassis is recommended for better cable routing and thermal design. I went with the Phanteks 620PC, which offers ample space and built-in cable management.\nBelow is a picture of the built computer:\nFig. 2. Computer\nSystem \u0026amp; Software Environment Operating System: Linux (e.g., Ubuntu 22.04 LTS) is highly recommended due to better compatibility and support for CUDA, NVIDIA drivers, and popular deep learning frameworks. The general setup flow:\nOS Installation: Ubuntu or another Linux distribution. NVIDIA Driver Installation: Make sure nvidia-smi detects both 4090 GPUs correctly:\nFig. 3. nvidia-smi Output\nCUDA Toolkit: Verify via nvcc -V:\nFig. 4. nvcc -V Output\ncuDNN: Ensure your deep learning frameworks can leverage GPU-accelerated kernels for convolution, RNN, etc. Framework Testing: Use PyTorch, TensorFlow, or JAX to confirm basic inference and training functionality. Docker Containerization: With nvidia-container-toolkit, containers can directly access GPU resources, eliminating host-environment conflicts. For multi-node, multi-GPU setups, consider Kubernetes, Ray, or Slurm for cluster scheduling and resource management. Recommended Tools \u0026amp; Frameworks Training Frameworks\nLLaMA-Factory: Offers user-friendly packaging for large language model training and inference. Great for beginners. DeepSpeed: Provides distributed training for large models, with multiple parallelization strategies and optimizations. Megatron-LM: NVIDIA’s official large-scale language model training framework, ideal for multi-node, multi-GPU scenarios. Monitoring \u0026amp; Visualization\nWeights \u0026amp; Biases or TensorBoard: Real-time monitoring of loss, learning rate, etc., with remote-friendly UI. Inference Tools\nollama: Based on llama.cpp, easy local inference setup. vLLM: Optimized for high-concurrency, multi-user scenarios, delivering superior throughput. Framework ollama vLLM Function Simple local LLM deployment High-concurrency / high-throughput LLM Concurrent Performance drops as concurrency increases Handles higher concurrency with better TPS 16 Threads ~17s/req ~9s/req Throughput Slower token generation speeds ~2× faster token generation Max Concur. Performance deteriorates over 32 threads Remains stable under large concurrency Use Cases Personal or low-traffic apps Enterprise or multi-user high concurrency WebUI\nOpen-WebUI: A user-friendly, web-based solution that integrates multiple AI backends (ollama, OpenAI API, etc.), handy for rapid prototyping and visualization. Advanced Suggestions Development \u0026amp; Debugging Efficiency\nUse SSH for remote development, and create custom Docker images to reduce setup overhead. Quantization \u0026amp; Pruning\nTechniques like 4-bit or 8-bit quantization and pruning can reduce model size and VRAM usage, thereby improving inference performance. Mixed-Precision Training\nSwitch to BF16 or FP16 training to accelerate training speed, combined with gradient scaling (GradScaler) to maintain numerical stability. CPU Coordination\nEnhance data loading using multi-threading, multi-processing, or RAM disks for streaming large pretraining datasets. Multi-Node Cluster Deployment\nConnect nodes via InfiniBand or high-speed Ethernet and use Kubernetes for efficient resource scheduling. Summary With the above configuration and methodology, I successfully built a dual RTX 4090 deep learning workstation. It excels at inference and small to medium scale fine-tuning scenarios—ideal for individuals or small teams focusing on LLM research or application development. This setup strikes a balance between cost-effectiveness and flexibility. However, if you plan to train massive models (hundreds of billions of parameters) in a full-parameter regime, you will still need more GPUs (e.g., multi-GPU A100 or H100 clusters).\nFrom personal experience, dual 4090 GPUs provide sufficient performance within a reasonable budget, meeting the majority of small-to-medium-scale R\u0026amp;D needs—a solid option for qualified individuals or teams to consider.\nReferences Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe lane specs AMD R5 7600X PCIe lane specs MSI MPG X670E CARBON Specifications nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI Copyright \u0026amp; Citation Disclaimer: The configurations, prices, and recommendations in this article are for technical discussion and research purposes only. Actual purchases and deployments should be carefully evaluated based on individual budgets and requirements. The author assumes no liability for any direct or indirect consequences resulting from following or adapting the information provided herein.\nCitation: When reposting or referencing this content, please credit the original author and source.\nCited as:\nYue Shui. (Dec 2024). Building a Home Deep Learning Rig with Dual RTX 4090 GPUs. https://syhya.github.io/posts/2024-12-21-build-gpu-server\nOr\n@article{syhya2024build, title = \u0026#34;Building a Home Deep Learning Rig with Dual RTX 4090 GPUs\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Dec\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2024-12-21-build-gpu-server/\u0026#34; ","permalink":"https://syhya.github.io/posts/2024-12-21-build-gpu-server/","summary":"\u003ch2 id=\"rent-a-gpu-or-buy-your-own\"\u003eRent a GPU or Buy Your Own?\u003c/h2\u003e\n\u003cp\u003eBefore setting up a deep learning environment, consider \u003cstrong\u003eusage duration\u003c/strong\u003e, \u003cstrong\u003ebudget\u003c/strong\u003e, \u003cstrong\u003edata privacy\u003c/strong\u003e, and \u003cstrong\u003emaintenance overhead\u003c/strong\u003e. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\u003c/p\u003e","title":"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs"},{"content":"Abstract The stock market is a crucial component of the financial market. In recent years, the stock market has flourished, attracting researchers from various fields to study stock price prediction and quantitative investment strategies. With the development of artificial intelligence and machine learning in recent years, scholars have transitioned from traditional statistical models to AI algorithms. Particularly, after the surge of deep learning, neural networks have achieved commendable results in stock price prediction and quantitative investment strategy research. The goal of deep learning is to learn multi-level features by combining low-level features to construct abstract high-level features, thereby mining the distributed feature representations of data and performing complex nonlinear modeling based on this to accomplish prediction tasks. Among these, RNNs are widely applied to sequential data such as natural language and speech. Daily stock prices and trading information are sequential data; therefore, many researchers have previously used RNNs to predict stock prices. However, basic recurrent neural networks suffer from the vanishing gradient problem when the number of layers is too large, a problem that the emergence of LSTM solved. Subsequently, variants of LSTM such as GRU, Peephole LSTM, and BiLSTM appeared. Traditional stock prediction models often overlook the temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a theoretical perspective, the BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions in time series, avoiding the vanishing and exploding gradient problems in long sequences, and better learning information with long-term dependencies.\nThe first part of this paper\u0026rsquo;s experiment establishes stock prediction models using LSTM, GRU, and BiLSTM based on stock data from China’s Shanghai Pudong Development Bank and the foreign company IBM, respectively. By comparing the prediction results of these three deep learning models, it was found that the BiLSTM model outperforms the other models on both datasets, achieving higher prediction accuracy. The second part uses stock data from the entire A-share market and first applies the LightGBM model to screen 50 factors, selecting the top 10 most important factors. Subsequently, the BiLSTM model is used to select and combine these factors to establish a quantitative investment strategy. Finally, empirical tests and backtesting demonstrate that this strategy outperforms the market benchmark index, illustrating the practical application value of the BiLSTM model in stock price prediction and quantitative investment.\nKeywords: Quantitative Investment; Deep Learning; Neural Network Models; Multi-Factor Stock Selection; BiLSTM\nChapter 1: Introduction 1.1 Research Background and Significance 1.1.1 Research Background Since its gradual emergence in the 1970s, quantitative investment has entered the view of various investors, initiating a new revolution that changed the previously dominated landscape of passive management and the Efficient Market Hypothesis (EMH) in portfolio management. The EMH posits that under the premise of market efficiency, stock prices reflect all available information, and no excess returns exist. Passive investment management is based on the belief that markets are efficient, focusing more on asset classes. The most common method is purchasing index funds and tracking the performance of published indices. In contrast, active investment management primarily relies on investors\u0026rsquo; subjective judgments about the market and individual stocks, using publicly available data and applying mathematical models in the financial field to evaluate stocks and construct investment portfolios to achieve returns. Quantitative investment, which involves statistical processing of large amounts of historical data to uncover investment opportunities and eliminate subjective factors, has been increasingly favored by investors. Since the rise of quantitative investment, various technologies have been employed to predict stock prices better and establish quantitative investment strategies. Early scholars domestically and internationally used statistical methods for modeling stock price prediction, such as the moving average method, multiple regression, and autoregressive integrated moving average (ARIMA) models. However, due to the numerous factors influencing the stock market and the large volume of data, stock prediction is highly challenging, and the predictive performance of these statistical models is often unsatisfactory.\nIn recent years, related technologies like machine learning, deep learning, and neural networks have continuously developed, providing technical support for stock price prediction and the construction of quantitative strategies. Many scholars have achieved breakthroughs using methods such as random forests, neural networks, support vector machines, and convolutional neural networks. The ample historical data in the stock market, combined with diverse technical support, provides favorable conditions for stock price prediction and the construction of quantitative strategies.\n1.1.2 Research Significance From the perspective of the national economic system and the long-term development of financial markets, research on stock price prediction models and quantitative investment strategies is indispensable. China started relatively late, and its financial markets are not yet mature, with insufficient financial instruments and lower market efficiency. However, in recent years, the government has gradually relaxed policies and vigorously developed financial markets, providing a \u0026ldquo;fertile ground\u0026rdquo; for the development of quantitative investment and emerging financial technologies. Developing quantitative investment and new financial technologies can offer opportunities for China\u0026rsquo;s financial markets to achieve leapfrog growth. Moreover, stock price indices, as important economic indicators, act as barometers for China\u0026rsquo;s economic development.\nFrom the perspective of individual and institutional investors, constructing stock price prediction models and quantitative investment strategy models enhances market efficiency. Individual investors often lack sufficient professional knowledge, leading to somewhat blind investment behaviors. Constructing relevant models to provide references can reduce the probability of judgment errors and change the relatively disadvantaged position of individual investors in the capital market. For institutional investors, rational and objective models combined with personal judgment improve decision accuracy and provide new directions for investment activities.\nIn summary, considering China\u0026rsquo;s current development status, this paper selects individual stocks for stock price prediction models and the entire A-share market for quantitative strategy research, which holds significant practical research significance.\n1.2 Literature Review White (1988)$^{[1]}$ used BP neural networks to predict IBM’s daily returns. However, due to the BP neural network model being susceptible to gradient explosion, the model could not converge to the global minimum, resulting in inaccurate predictions.\nKimoto (1990)$^{[2]}$ developed a system for forecasting the Tokyo Stock Exchange Prices Indexes (TOPIX) using modular neural network technology. The system not only successfully predicted TOPIX but also achieved a certain level of profitability through simulated stock trading based on the prediction results.\nG. Peter Zhang (2003)$^{[3]}$ conducted a comparative study on the performance of the Autoregressive Integrated Moving Average (ARIMA) model and the Artificial Neural Network (ANN) model in time series forecasting. The results showed that the ANN model significantly outperformed the ARIMA model in time series prediction accuracy.\nRyo Akita (2016)$^{[4]}$ selected consumer price indices, price-earnings ratios, and various events from newspapers as features. Using paragraph vectors and LSTM networks, a financial time series prediction model was constructed. Empirical data from fifty listed companies on the Tokyo Stock Exchange validated the model\u0026rsquo;s effectiveness in predicting stock opening prices.\nKunihiro Miyazaki (2017)$^{[5]}$ constructed a model for predicting the rise and fall of the Topix Core 30 index and its constituent stocks by extracting daily stock chart images and 30-minute stock price data. The study compared various models, including Logistic Regression (LR), Random Forest (RF), Multilayer Perceptron (MLP), LSTM, CNN, PCA-CNN, and CNN-LSTM. The results indicated that LSTM had the best predictive performance, CNN performed moderately, and hybrid models combining CNN and LSTM could enhance prediction accuracy.\nTaewook Kim (2019)$^{[6]}$ proposed a hybrid LSTM-CNN model that combines features from stock price time series and stock price images to predict the S\u0026amp;P 500 index. The study demonstrated that the LSTM-CNN model outperformed single models in stock price prediction and that such predictions hold practical significance for constructing quantitative investment strategies.\n1.3 Innovations of the Paper This paper presents the following innovations in stock prediction:\nDiverse Market Data Usage: It separately uses data from China’s A-share Shanghai Pudong Development Bank and the foreign stock IBM, avoiding the limitations of studying a single market. Traditional BP models have never considered temporal factors, and models like LSTM only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. Theoretically, the BiLSTM model fully utilizes contextual relationships in both forward and backward temporal directions in time series, avoiding the vanishing and exploding gradient problems in long sequences, and better learning information with long-term dependencies. Empirical comparisons with LSTM and GRU models demonstrate its ability to enhance prediction accuracy.\nMulti-Feature Stock Price Prediction: The stock price prediction model trains on multiple features, including opening price, closing price, highest price, and trading volume. Compared to single-feature prediction of closing prices, this approach is theoretically more accurate and better facilitates the comparison of prediction effects among LSTM, GRU, and BiLSTM models.\nIn terms of quantitative strategy research, this paper presents the following innovations:\nCustom Factor Selection: Instead of using commonly available market factors, it employs multiple price and volume factors derived through Genetic Programming (GP) and manual data mining. The LightGBM model is then used to select the top 10 most important factors from an initial set of 50 factors.\nBiLSTM-Based Factor Combination: Traditional quantitative investment models generally use LSTM and CNN models to build investment strategies. This paper utilizes data from the entire A-share market and employs the BiLSTM model for factor combination to establish a quantitative investment strategy. Empirical tests and backtesting show that this strategy outperforms the market benchmark index (CSI All Share Index), indicating the practical application value of the BiLSTM model in stock price prediction and quantitative investment.\n1.4 Research Framework This paper conducts stock price prediction and quantitative strategy research based on deep learning algorithms. The specific research framework is shown in Fig. 1:\nFig. 1. Research Framework.\nThe specific research framework content is as follows:\nChapter 1: Introduction. This chapter first introduces the research significance and background of stock price prediction and quantitative strategy research. It then reviews the current research status, outlines the innovations of this paper compared to existing research, and briefly describes the research framework.\nChapter 2: Theoretical Foundations. This chapter introduces the deep learning models and the fundamental theories of quantitative stock selection involved in this research. The subsection on deep learning models sequentially introduces RNN, LSTM, GRU, and BiLSTM, with a focus on the internal structure of the LSTM model. The subsection on quantitative stock selection theory sequentially introduces the Mean-Variance Model, Capital Asset Pricing Model (CAPM), Arbitrage Pricing Theory (APT), Multi-Factor Models, and the Fama-French Three-Factor and Five-Factor Models. This subsection outlines the development of multi-factor quantitative stock selection from various financial theories and model development trajectories.\nChapter 3: Comparative Study of LSTM, GRU, and BiLSTM in Stock Price Prediction. This chapter first introduces the domestic and foreign stock datasets used in the experiments, followed by data normalization and preprocessing steps. It then describes the network structures, model compilation, and hyperparameter settings of the LSTM, GRU, and BiLSTM models used in this chapter, and presents experimental results. Finally, it analyzes the experimental results and concludes the chapter.\nChapter 4: Research on LightGBM-BiLSTM-Based Quantitative Investment Models. This chapter first provides an overview of the experimental steps, followed by a detailed introduction of the stock and factor data used in the experiments. It then performs factor cleaning steps, including handling missing values, outlier removal, factor normalization, and factor neutralization. Subsequently, it uses the LightGBM model for factor selection and the BiLSTM model for factor combination. Finally, it constructs a quantitative investment strategy based on the obtained models and conducts backtesting.\nChapter 5: Conclusion and Outlook. This chapter summarizes the main research content regarding stock price prediction and quantitative investment strategies. It then discusses the current research limitations and provides prospects for future research directions.\nChapter 2: Theoretical Foundations 2.1 Deep Learning Models 2.1.1 RNN Recurrent Neural Networks (RNNs) are widely applied to sequential data such as natural language and speech. Daily stock prices and trading information are sequential data; therefore, many previous studies have used RNNs to predict stock prices. RNNs employ a simple repetitive module in a chain-like structure, such as a single tanh layer. However, basic RNNs encounter the vanishing gradient problem when the number of layers becomes too large, a problem that the emergence of LSTM resolved. Fig. 2 is the RNN structure diagram.\nFig. 2. RNN Structure Diagram. (Image source: Understanding LSTM Networks)\n2.1.2 LSTM Long Short-Term Memory (LSTM) networks are a special type of RNN capable of learning long-term dependencies. They were proposed by Hochreiter \u0026amp; Schmidhuber (1997)$^{[7]}$ and have been improved and popularized by many subsequent works. Due to their unique design structure, LSTMs are relatively insensitive to sequence length and solve the vanishing and exploding gradient problems inherent in traditional RNNs. Compared to traditional RNNs and other time series models like Hidden Markov Models (HMMs), LSTMs can better handle and predict significant events in time series with long intervals and delays. Consequently, LSTMs are widely used in machine translation and speech recognition fields.\nLSTMs are explicitly designed to avoid the long-term dependency problem. All recurrent neural networks have a chain-like structure of repeated network modules, whereas LSTMs modify the RNN structure. Instead of using a single neural network layer, LSTMs use a special four-layer structure that interacts in a unique way.\nFig. 3. LSTM Structure Diagram 1. (Image source: Understanding LSTM Networks)\nFig. 4. LSTM Structure Diagram 2. (Image source: Understanding LSTM Networks)\nAs shown in Fig. 3, the black lines represent the transmission of a node\u0026rsquo;s output vector to another node\u0026rsquo;s input vector. The Neural Network Layer is a processing module with a $\\sigma$ activation function or tanh activation function; Pointwise Operation represents element-wise multiplication between vectors; Vector Transfer indicates the direction of information transmission; Concatenate and Copy are represented by two black lines merging together and separating, respectively, indicating the merging and copying of information.\nBelow, we detail the structure of an LSTM using the following components:\nForget Gate Fig. 5. Forget Gate Calculation (Image source: Understanding LSTM Networks)\n$$ f_{t} = \\sigma\\left(W_{f} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{f}\\right) $$Parameter Description:\n$h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $\\sigma$: Sigmoid activation function $W_{f}$: Weight matrix for the forget gate $b_{f}$: Bias vector parameter for the forget gate The first step, as shown in Fig. 5, is the process of deciding which information to discard from the cell state. This is determined by computing $f_{t}$ using the sigmoid function (where $f_{t}$ ranges between 0 and 1, with 0 meaning completely blocked and 1 meaning completely passed) to decide whether to allow the previous cell state $C_{t-1}$ to pass through or be partially allowed.\nInput Gate Fig. 6. Input Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} i_{t} \u0026= \\sigma\\left(W_{i} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{i}\\right) \\\\ \\tilde{C}_{t} \u0026= \\tanh\\left(W_{C} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{C}\\right) \\end{aligned} $$Parameter Description:\n$h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $\\sigma$: Sigmoid activation function $W_{i}$: Weight matrix for the input gate $b_{i}$: Bias vector parameter for the input gate $W_{C}$: Weight matrix for the cell state $b_{C}$: Bias vector parameter for the cell state $\\tanh$: Tanh activation function The second step, as shown in Fig. 6, involves using the sigmoid function to determine what information to store in the cell state. Subsequently, a $\\tanh$ layer creates the candidate vector $\\tilde{C}_{t}$, which will be added to the cell state.\nFig. 7. Current Cell State Calculation (Image source: Understanding LSTM Networks)\n$$ C_{t} = f_{t} * C_{t-1} + i_{t} * \\tilde{C}_{t} $$Parameter Description:\n$C_{t-1}$: Cell state from the previous time step $\\tilde{C}_{t}$: Candidate cell state $i_{t}$: Value of the input gate $f_{t}$: Value of the forget gate The third step, as shown in Fig. 7, calculates the current cell state $C_t$ by combining the effects of the forget gate and the input gate.\nThe forget gate $f_t$ weights the previous cell state $C_{t-1}$ to discard unnecessary information. The input gate $i_t$ weights the candidate cell state $\\tilde{C}_t$ to determine how much new information to introduce. Finally, the two parts are added together to update the current cell state $C_t$. Output Gate Fig. 8. Output Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} o_{t} \u0026= \\sigma\\left(W_{o} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{o}\\right) \\\\ h_{t} \u0026= o_{t} * \\tanh\\left(C_{t}\\right) \\end{aligned} $$Parameter Description:\n$o_{t}$: Value of the output gate $\\sigma$: Sigmoid activation function $W_{o}$: Weight matrix for the output gate $h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $b_{o}$: Bias vector parameter for the output gate $h_{t}$: Output at the current time step $\\tanh$: Tanh activation function $C_{t}$: Current cell state The fourth step, as shown in Fig. 8, uses the sigmoid function to calculate the output gate\u0026rsquo;s value. Finally, the cell state $C_t$ at this time step is processed through the tanh activation function and multiplied by the output gate\u0026rsquo;s value $o_t$ to obtain the current output $h_t$.\n2.1.3 GRU K. Cho (2014)$^{[8]}$ proposed the Gated Recurrent Unit (GRU). GRU primarily simplifies and adjusts LSTM by merging LSTM\u0026rsquo;s original forget gate, input gate, and output gate into an update gate and a reset gate. Additionally, GRU combines the cell state with the hidden state, reducing model complexity while still achieving performance comparable to LSTM on certain tasks.\nThis model can save considerable time when training on larger datasets and demonstrates better performance on some smaller and less frequent datasets$^{[9][10]}$.\nFig. 9. GRU Structure Diagram (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} z_{t} \u0026= \\sigma\\left(W_{z} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ r_{t} \u0026= \\sigma\\left(W_{r} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ \\tilde{h}_{t} \u0026= \\tanh\\left(W \\cdot \\left[r_{t} * h_{t-1}, x_{t}\\right]\\right) \\\\ h_{t} \u0026= \\left(1 - z_{t}\\right) * h_{t-1} + z_{t} * \\tilde{h}_{t} \\end{aligned} $$Parameter Description:\n$z_{t}$: Value of the update gate $r_{t}$: Value of the reset gate $W_{z}$: Weight matrix for the update gate $W_{r}$: Weight matrix for the reset gate $\\tilde{h}_{t}$: Candidate hidden state 2.1.4 BiLSTM Bidirectional Long Short-Term Memory (BiLSTM) networks combine forward and backward LSTMs. The BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions in time series, enabling the learning of information with long-term dependencies. Compared to unidirectional LSTM, BiLSTM can better consider the reverse influence of future data. Fig. 10 is the BiLSTM structure diagram.\nFig. 10. BiLSTM Structure Diagram. (Image source: Baeldung)\n2.2 Quantitative Stock Selection Theory 2.2.1 Mean-Variance Model The quantitative stock selection strategy originated in the 1950s when Markowitz (1952)$^{[11]}$ proposed the Mean-Variance Model. This model not only laid the foundation for modern portfolio theory by quantifying investment risk but also established a specific model describing risk and expected return. It broke the previous situation of only qualitative analysis of portfolios without quantitative analysis, successfully introducing mathematical models into the financial investment field.\n$$ \\begin{aligned} \\mathrm{E}\\left(R_{p}\\right) \u0026= \\sum_{i=1}^{n} w_{i} \\mathrm{E}\\left(R_{i}\\right) \\\\ \\sigma_{p}^{2} \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\operatorname{Cov}\\left(R_{i}, R_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\sigma_{i} \\sigma_{j} \\rho_{ij} \\\\ \\sigma_{i} \u0026= \\sqrt{\\operatorname{Var}\\left(R_{i}\\right)}, \\quad \\rho_{ij} = \\operatorname{Corr}\\left(R_{i}, R_{j}\\right) \\end{aligned} $$$$ \\min \\sigma_{p}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} \\mathrm{E}\\left(R_{i}\\right) w_{i} = \\mu_{p}, \\quad \\sum_{i=1}^{n} w_{i} = 1 $$$$ \\begin{aligned} \\Omega \u0026= \\begin{pmatrix} \\sigma_{11} \u0026 \\cdots \u0026 \\sigma_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\sigma_{n1} \u0026 \\cdots \u0026 \\sigma_{nn} \\end{pmatrix} = \\begin{pmatrix} \\operatorname{Var}\\left(R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Cov}\\left(R_{1}, R_{n}\\right) \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\operatorname{Cov}\\left(R_{n}, R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Var}\\left(R_{n}\\right) \\end{pmatrix} \\\\ \\Omega^{-1} \u0026= \\begin{pmatrix} v_{11} \u0026 \\cdots \u0026 v_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ v_{n1} \u0026 \\cdots \u0026 v_{nn} \\end{pmatrix} \\\\ w_{i} \u0026= \\frac{1}{D}\\left(\\mu_{p} \\sum_{j=1}^{n} v_{ij}\\left(C \\mathrm{E}\\left(R_{j}\\right) - A\\right) + \\sum_{j=1}^{n} v_{ij}\\left(B - A \\mathrm{E}\\left(R_{j}\\right)\\right)\\right), \\quad i = 1, \\ldots, n \\end{aligned} $$$$ \\begin{aligned} A \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{j}\\right), \\quad B = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{i}\\right) \\mathrm{E}\\left(R_{j}\\right), \\quad C = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij}, \\quad D = BC - A^{2} \u003e 0 \\\\ \\sigma_{p}^{2} \u0026= \\frac{C \\mu_{p}^{2} - 2A \\mu_{p} + B}{D} \\end{aligned} $$Where:\n$\\mathrm{E}\\left(R_{p}\\right)$ and $\\mu_{p}$ are the expected return of portfolio $p$ $\\mathrm{E}\\left(R_{i}\\right)$ is the expected return of asset $i$ $\\sigma_{i}$ is the standard deviation of asset $i$ $\\sigma_{j}$ is the standard deviation of asset $j$ $w_{i}$ is the weight of asset $i$ in the portfolio $\\sigma_{p}^{2}$ is the variance of portfolio $p$ $\\rho_{ij}$ is the correlation coefficient between assets $i$ and $j$ Using the above formulas$^{[12]}$, we can construct an investment portfolio that minimizes non-systematic risk under a given expected return condition.\n2.2.2 Capital Asset Pricing Model William Sharpe (1964)$^{[13]}$, John Lintner (1965)$^{[14]}$, and Jan Mossin (1966)$^{[15]}$ proposed the Capital Asset Pricing Model (CAPM). This model posits that the expected return of an asset is related to its risk measure, the $\\beta$ value. The model connects the expected return rate of an asset with market risk through a simple linear relationship, making Markowitz\u0026rsquo;s (1952) portfolio selection theory more applicable to the real world and laying the theoretical foundation for multi-factor stock selection models.\nAccording to the CAPM, for a given asset $i$, the relationship between its expected return and the expected return of the market portfolio can be expressed as:\n$$ E\\left(r_{i}\\right) = r_{f} + \\beta_{im}\\left[E\\left(r_{m}\\right) - r_{f}\\right] $$Where:\n$E\\left(r_{i}\\right)$ is the expected return of asset $i$ $r_{f}$ is the risk-free rate $\\beta_{im}$ (Beta) is the systematic risk coefficient of asset $i$, $\\beta_{im} = \\frac{\\operatorname{Cov}\\left(r_{i}, r_{m}\\right)}{\\operatorname{Var}\\left(r_{m}\\right)}$ $E\\left(r_{m}\\right)$ is the expected return of the market portfolio $m$ $E\\left(r_{m}\\right) - r_{f}$ is the market risk premium factor 2.2.3 Arbitrage Pricing Theory and Multi-Factor Models Ross (1976)$^{[16]}$ proposed the Arbitrage Pricing Theory (APT). This theory asserts that arbitrage activities are the decisive factor in forming market equilibrium prices. By introducing a series of factors into the process of return formation and constructing linear relationships, APT overcomes the limitations of the Capital Asset Pricing Model (CAPM) and provides important theoretical guidance for subsequent researchers.\nAPT is considered the theoretical foundation of Multi-Factor Models (MFM), an essential component of asset pricing models and one of the cornerstones of asset pricing theory. The general expression of a multi-factor model is:\n$$ r_{j} = a_{j} + \\lambda_{j1} f_{1} + \\lambda_{j2} f_{2} + \\cdots + \\lambda_{jn} f_{n} + \\delta_{j} $$Where:\n$r_{j}$ is the return of asset $j$ $a_{j}$ is the constant term for asset $j$ $f_{n}$ are the systematic factors $\\lambda_{jn}$ are the factor loadings $\\delta_{j}$ is the random error 2.2.4 Fama-French Three-Factor and Five-Factor Models Fama (1992) and French (1992)$^{[17]}$ discovered through cross-sectional regressions combined with time series analysis that a stock’s $\\beta$ value cannot explain the differences in returns among different stocks. However, factors such as a company’s market capitalization, book-to-market ratio, and price-earnings ratio can significantly explain the differences in stock returns. They concluded that excess returns compensate for risk factors not captured by the $\\beta$ in CAPM, leading to the Fama-French Three-Factor Model. These three factors are:\nMarket Risk Premium Factor\nRepresents the overall systematic risk of the market, calculated as the expected return of the market portfolio minus the risk-free rate. Measures the excess return investors expect for taking on systematic risk (risk that cannot be eliminated through diversification). Calculation formula:\n$$ \\text{Market Risk Premium} = E(R_m) - R_f $$ where $E(R_m)$ is the expected return of the market, and $R_f$ is the risk-free rate. Size Factor (SMB: Small Minus Big)\nRepresents the return difference between small-cap and large-cap stocks. Small-cap stocks generally carry higher risk but historically show higher expected returns than large-cap stocks. Calculation formula:\n$$ SMB = R_{\\text{Small}} - R_{\\text{Big}} $$ Reflects the market’s compensation for the additional risk premium of small-cap stocks. Book-to-Market Ratio Factor (HML: High Minus Low)\nReflects the return difference between high book-to-market (value) stocks and low book-to-market (growth) stocks. High book-to-market stocks are typically undervalued by the market but may offer higher returns in the long run. Calculation formula:\n$$ HML = R_{\\text{High}} - R_{\\text{Low}} $$ Low book-to-market stocks may be overvalued due to the market’s overly optimistic expectations. This model specifies the factors in the APT model and concludes that investing in small-cap and high-growth stocks exhibits high-risk and high-return characteristics. The Fama-French Three-Factor Model is widely applied in modern investment behavior analysis and practice.\nSubsequently, Fama (2015) and French (2015)$^{[18]}$ expanded the three-factor model by adding two more factors:\nProfitability Factor (RMW: Robust Minus Weak)\nReflects the return difference between high-profitability and low-profitability companies. Companies with strong profitability (high ROE, net profit margin) are more likely to provide stable and higher returns. Calculation formula:\n$$ RMW = R_{\\text{Robust}} - R_{\\text{Weak}} $$ Investment Factor (CMA: Conservative Minus Aggressive)\nReflects the return difference between conservative and aggressive investment companies. Aggressive companies (rapid expansion, high capital expenditure) usually carry greater operational risks, while conservative companies (steady capital expenditure) exhibit higher stability and returns. Calculation formula:\n$$ CMA = R_{\\text{Conservative}} - R_{\\text{Aggressive}} $$ The Fama-French Three-Factor Model formula is:\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\epsilon_i $$The Fama-French Five-Factor Model formula is:\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\beta_{i,RMW} \\cdot RMW + \\beta_{i,CMA} \\cdot CMA + \\epsilon_i $$Where:\n$R_i$: Expected return of stock $i$ $R_f$: Risk-free rate $R_m$: Expected return of the market portfolio $R_m - R_f$: Market risk premium factor $SMB$: Size factor (Small Minus Big) $HML$: Book-to-market ratio factor (High Minus Low) $RMW$: Profitability factor (Robust Minus Weak) $CMA$: Investment factor (Conservative Minus Aggressive) $\\beta_{i,*}$: Sensitivity of stock $i$ to the corresponding factor $\\epsilon_i$: Regression residual 2.2.5 Model Comparison Tables Table 2.1: Comparison of Models The following table summarizes the core content and factor sources of the Mean-Variance Model, Capital Asset Pricing Model (CAPM), Arbitrage Pricing Theory (APT), and Fama-French Models:\nModel Core Content Factor Sources Mean-Variance Model Foundation of portfolio theory, optimizing portfolios through expected returns and covariance matrices. Expected returns and covariance matrices of portfolio assets Capital Asset Pricing Model (CAPM) Explains asset returns through market risk factor ($\\beta$), laying the theoretical foundation for multi-factor models. Market factor $\\beta$ Arbitrage Pricing Theory (APT) Multi-factor framework allowing multiple economic variables (e.g., inflation rate, interest rate) to explain asset returns. Multiple factors (macroeconomic variables like inflation rate, interest rate) Fama-French Three-Factor Model Enhances asset return explanation by adding size and book-to-market ratio factors. Market factor, SMB (Size factor), HML (Book-to-Market ratio factor) Fama-French Five-Factor Model Further improves asset pricing by adding profitability and investment factors on top of the three-factor model. Market factor, SMB, HML, RMW (Profitability factor), CMA (Investment factor) The following table summarizes the advantages and disadvantages of these models:\nTable 2.2: Comparison of Model Advantages and Disadvantages Model Advantages Disadvantages Mean-Variance Model Provides a systematic method for portfolio optimization, laying the foundation for modern investment theory. Optimizes only for returns and variance, without explicitly identifying sources of risk compensation. Capital Asset Pricing Model (CAPM) Simple to use, connects return differences to market risk, providing a theoretical foundation for multi-factor models. Assumes a single factor (market risk) determines returns, ignoring other systematic risk factors. Arbitrage Pricing Theory (APT) Allows multiple factors to explain asset returns, reducing reliance on single-factor assumptions and offering greater flexibility. Does not specify specific factors, making it less practical and only providing a framework. Fama-French Three-Factor Model Significantly improves the explanatory power of asset returns by adding size and book-to-market ratio factors. Ignores other factors such as profitability and investment behaviors. Fama-French Five-Factor Model Further enhances asset pricing by incorporating profitability and investment factors, capturing key variables influencing asset returns more comprehensively. Higher model complexity, greater data requirements, and potential omission of certain latent factors. Chapter 3: Comparative Study of LSTM, GRU, and BiLSTM in Stock Price Prediction 3.1 Introduction of Experimental Data Many domestic and foreign studies focus primarily on their own stock indices, with relatively few studies examining individual stocks from different markets. Moreover, few studies have compared the performance of LSTM, GRU, and BiLSTM models. Therefore, this paper selects China\u0026rsquo;s A-share Shanghai Pudong Development Bank (Pudong Bank, code 600000) and the US stock International Business Machines Corporation (IBM) for research, enabling a more accurate comparison of the three models used. For Pudong Bank, stock data from January 1, 2008, to December 31, 2020, comprising 3,114 valid data points, were sourced from the Tushare Financial Data Platform. The features selected for this dataset include date, opening price, closing price, highest price, lowest price, and trading volume. Except for the date, which serves as the time series index, the other five features are used as independent variables. For IBM, stock data from January 2, 1990, to November 15, 2018, comprising 7,278 valid data points, were sourced from Yahoo Finance. The features selected for this dataset include date, opening price, highest price, lowest price, closing price, adjusted closing price, and trading volume. Except for the date, which serves as the time series index, the other six features are used as independent variables. In this experiment, the closing price is chosen as the target variable. Tables 3.1 and 3.2 display partial data from the two datasets.\nTable 3.1: Partial Display of Pudong Bank Dataset Date Open Close High Low Volume Code 2008-01-02 9.007 9.101 9.356 8.805 131,583.90 600000 2008-01-03 9.007 8.645 9.101 8.426 211,346.56 600000 2008-01-04 8659 9.009 9.111 8.501 139,249.67 600000 2008-01-07 8.970 9.515 9.593 8.953 228,043.01 600000 2008-01-08 9.551 9.583 9.719 9.517 161,255.31 600000 2008-01-09 9.583 9.663 9.772 9.432 102,510.92 600000 2008-01-10 9.701 9.680 9.836 9.602 217,966.25 600000 2008-01-11 9.670 10.467 10.532 9.670 231,544.21 600000 2008-01-14 10.367 10.059 10.433 10.027 142,918.39 600000 2008-01-15 10.142 10.051 10.389 10.006 161,221.52 600000 Data Source: Tushare\nTable 3.2: Partial Display of IBM Dataset Date Open High Low Close Adj Close Volume 1990-01-02 23.6875 24.5313 23.6250 24.5000 6.590755 7,041,600 1990-01-03 24.6875 24.8750 24.5938 24.7188 6.649599 9,464,000 1990-01-04 24.7500 25.0938 24.7188 25.0000 6.725261 9,674,800 1990-01-05 24.9688 25.4063 24.8750 24.9375 6.708448 7,570,000 1990-01-08 24.8125 25.2188 24.8125 25.0938 6.750481 4,625,200 1990-01-09 25.1250 25.3125 24.8438 24.8438 6.683229 7,048,000 1990-01-10 24.8750 25.0000 24.6563 24.7500 6.658009 5,945,600 1990-01-11 24.8750 25.0938 24.8438 24.9688 6.716855 5,905,600 1990-01-12 24.6563 24.8125 24.4063 24.4688 6.582347 5,390,800 1990-01-15 24.4063 24.5938 24.3125 24.5313 6.599163 4,035,600 Data Source: Yahoo Finance\n3.2 Data Preprocessing 3.2.1 Data Normalization In the experiment, different features have varying units and magnitudes. For example, there is a significant magnitude difference between stock prices and trading volumes, which can impact the final prediction results. Therefore, the MinMaxScaler method from the sklearn.preprocessing library is used to scale the data features to the range of 0 to 1. This not only improves model accuracy but also enhances the model’s convergence speed. The normalization formula is:\n$$ x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)} $$Where $x^{\\prime}$ is the normalized data, $x$ is the original data, $\\min (x)$ is the minimum value of the original dataset, and $\\max (x)$ is the maximum value of the original dataset. After obtaining the prediction results during the experiment, the data will be denormalized for stock price prediction and model evaluation.\n3.2.2 Data Splitting Here, the entire experimental datasets of Pudong Bank and IBM are input with a timestep of 60 for the recurrent kernels, and the number of input features per timestep is 5 and 6, respectively. This allows the model to input data from the previous 60 trading days to predict the closing price on the 61st day. This ensures that our dataset meets the input requirements of the three neural network models to be compared, namely the number of samples, the timestep length, and the number of input features per timestep. Subsequently, the Pudong Bank dataset is split into training, validation, and testing sets in a ratio of 2,488:311:255 after normalization. The IBM dataset is split into training, validation, and testing sets in a ratio of 6,550:364:304 after normalization. The purpose of the validation set here is to facilitate the adjustment of the model’s hyperparameters to optimize each model before comparison.\n3.3 Model Network Structures Through extensive experimentation, the network structures set for each model are as shown in the table below. Between layers, the default tanh and linear activation functions for recurrent neural networks are used, and Dropout with a dropout rate of 0.2 is added to prevent overfitting. Each recurrent layer in LSTM and GRU models has 50 neurons, while the BiLSTM recurrent layers have 100 neurons. Each LSTM, GRU, and BiLSTM model comprises four recurrent layers and one fully connected layer, with a Dropout layer between each network layer.\nTable 3.3: LSTM Network Structure for IBM Layer(type) Output Shape Param# lstm_1 (LSTM) (None, 60, 50) 11,400 dropout_1 (Dropout) (None, 60, 50) 0 lstm_2 (LSTM) (None, 60, 50) 20,200 dropout_2 (Dropout) (None, 60, 50) 0 lstm_3 (LSTM) (None, 60, 50) 20,200 dropout_3 (Dropout) (None, 60, 50) 0 lstm_4 (LSTM) (None, 50) 20,200 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params: 72,051\nTrainable params: 72,051\nNon-trainable params: 0\nTable 3.4: GRU Network Structure for IBM Layer(type) Output Shape Param# gru_1 (GRU) (None, 60, 50) 8,550 dropout_1 (Dropout) (None, 60, 50) 0 gru_2 (GRU) (None, 60, 50) 15,150 dropout_2 (Dropout) (None, 60, 50) 0 gru_3 (GRU) (None, 60, 50) 15,150 dropout_3 (Dropout) (None, 60, 50) 0 gru_4 (GRU) (None, 50) 15,150 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params: 54,051\nTrainable params: 54,051\nNon-trainable params: 0\nTable 3.5: BiLSTM Network Structure for IBM Layer(type) Output Shape Param# bidirectional_1 (Bidirectional) (None, 100) 22,400 dropout_1 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 256) 25,856 dropout_2 (Dropout) (None, 256) 0 dense_2 (Dense) (None, 64) 16,448 dense_3 (Dense) (None, 1) 0 Total params: 66,769\nTrainable params: 66,769\nNon-trainable params: 0\n3.4 Model Compilation and Hyperparameter Settings After iterative tuning of hyperparameters aiming to minimize the loss function on the validation set, the following settings are chosen for the three models for Pudong Bank and IBM respectively: epochs=100, batch_size=32 for Pudong Bank models and epochs=50, batch_size=32 for IBM models. The optimizer used is Adaptive Moment Estimation (Adam)$^{[19]}$ with default settings in the keras package: lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, and decay=0.0. The loss function is Mean Squared Error (MSE).\nParameter Descriptions:\nlr: Learning rate beta_1: Exponential decay rate for the first moment estimates beta_2: Exponential decay rate for the second moment estimates epsilon: Small constant for numerical stability decay: Learning rate decay per update 3.5 Experimental Results and Analysis First, a brief introduction to the evaluation metrics used for model assessment is provided. The calculation formulas are as follows:\nMean Squared Error (MSE): $$ MSE=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2} $$ Root Mean Squared Error (RMSE): $$ RMSE=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}} $$ Mean Absolute Error (MAE): $$ MAE=\\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-\\hat{Y}_{i}\\right| $$ \\( R^2 \\) (R Squared): $$ \\begin{gathered} \\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} \\\\ R^{2}=1-\\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\end{gathered} $$Where $n$ is the number of samples, $Y$ is the actual closing price, $\\hat{Y}_{i}$ is the predicted closing price, and $\\bar{Y}$ is the average closing price. Lower values of MSE, RMSE, and MAE indicate more accurate models. A higher $R^{2}$ indicates a better fit of the model coefficients.\n3.5.1 Experimental Results for Pudong Bank Table 3.6: Experimental Results for Pudong Bank LSTM GRU BiLSTM MSE 0.059781 0.069323 0.056454 RMSE 0.244501 0.263292 0.237601 MAE 0.186541 0.202665 0.154289 R-squared 0.91788 0.896214 0.929643 Comparing the evaluation metrics of the three models, we observe that on the Pudong Bank test set, the BiLSTM model has lower MSE, RMSE, and MAE than both the LSTM and GRU models, while its $R^2$ is higher than those of LSTM and GRU models. Specifically, the RMSE comparison shows that BiLSTM has a 2.90% performance improvement over LSTM and a 10.81% performance improvement over GRU on the validation set.\n3.5.2 Experimental Results for IBM Table 3.7: Experimental Results for IBM LSTM GRU BiLSTM MSE 18.01311 12.938584 11.057501 RMSE 4.244186 3.597024 3.325282 MAE 3.793223 3.069033 2.732075 R-squared 0.789453 0.851939 0.883334 Comparing the evaluation metrics of the three models, we find that on the IBM test set, the BiLSTM model has lower MSE, RMSE, and MAE than both the LSTM and GRU models, while its $R^2$ is higher than those of LSTM and GRU models. Specifically, the RMSE comparison shows that BiLSTM has a 27.63% performance improvement over LSTM and an 8.17% performance improvement over GRU on the validation set.\n3.6 Chapter Summary This chapter introduced the datasets selected for the experiments—Pudong Bank and IBM—and the chosen features. It then performed data normalization and data splitting as preprocessing steps. The chapter also detailed the network structures and hyperparameter settings of the LSTM, GRU, and BiLSTM models used in the experiments. Finally, it presented the loss function curves and a series of fitting graphs for each model. By comparing multiple evaluation metrics and fitting graphs, it was concluded that the BiLSTM model can better predict stock prices, laying the foundation for the subsequent research on the LightGBM-BiLSTM quantitative investment strategy.\nChapter 4: Research on LightGBM-BiLSTM-Based Quantitative Investment Models 4.1 Experimental Steps Fig. 11. LightGBM-BiLSTM Diagram.\nAs shown in Fig. 11, this experiment first selects 50 factors from the factor library. Subsequently, each factor undergoes outlier removal, normalization, and missing value imputation steps for factor cleaning. The LightGBM model is then used for factor selection, ranking factors based on importance and selecting the top ten most important factors for cross-sectional analysis. Next, the BiLSTM model is used to combine these factors to build a multi-factor model. Finally, the quantitative investment strategy is constructed based on the obtained models and subjected to backtesting analysis.\n4.2 Experimental Data 4.2.1 Stock Data The market data used in this paper are sourced from Tushare. The specific features of the dataset are shown in Table 4.1.\nTable 4.1: Features Included in the Dataset Name Type Description ts_code str Stock code trade_date str Trading date open float Opening price high float Highest price low float Lowest price close float Closing price pre_close float Previous day\u0026rsquo;s closing price change float Price change pct_chg float Percentage change (unadjusted) vol float Trading volume (in hands) amount float Trading amount (in thousands of yuan) The entire A-share daily market dataset contains 5,872,309 rows of data, representing 5,872,309 samples. As shown in Table 4.2, the A-share daily market dataset has the following 11 features: stock code (ts_code), trading date (trade_date), opening price (open), closing price (close), highest price (high), lowest price (low), previous day\u0026rsquo;s closing price (pre_close), price change (change), turnover rate (turnover_rate), trading amount (amount), total market value (total_mv), and adjustment factor (adj_factor).\nTable 4.2: Partial Display of A-share Daily Market Dataset ts_code trade_date open high low close pre_close change vol amount 600613.SH 20120104 8.20 8.20 7.84 7.86 8.16 -0.30 4,762.98 3,854.1000 600690.SH 20120104 9.00 9.17 8.78 8.78 8.93 -0.15 142,288.41 127,992.6050 300277.SZ 20120104 22.90 22.98 20.81 20.88 22.68 -1.80 12,212.39 26,797.1370 002403.SZ 20120104 8.87 8.90 8.40 8.40 8.84 -0.441 10,331.97 9,013.4317 300179.SZ 20120104 19.99 20.32 19.20 19.50 19.96 -0.46 1,532.31 3,008.0594 600000.SH 20120104 8.54 8.56 8.39 8.41 8.49 -0.08 342,013.79 290,229.5510 300282.SZ 20120104 22.90 23.33 21.02 21.02 23.35 -2.33 38,408.60 86,216.2356 002319.SZ 20120104 9.74 9.95 9.38 9.41 9.73 -0.32 4,809.74 4,671.4803 601991.SH 20120104 5.17 5.39 5.12 5.25 5.16 0.09 145,268.38 76,547.7490 000780.SZ 20120104 10.42 10.49 10.00 10.00 10.30 -0.30 20,362.30 20,830.1761 [5,872,309 rows x 11 columns]\nThe CSI All Share Index daily dataset contains 5,057 rows of data, representing 5,057 samples. As shown in Table 4.3, the CSI All Share daily dataset has the following 7 features: trading date (trade_date), opening price (open), highest price (high), lowest price (low), closing price (close), trading volume (volume), and previous day\u0026rsquo;s closing price (pre_close).\nTable 4.3: Partial Display of CSI All Share Daily Dataset trade_date open high low close volume pre_close 2006-11-24 1564.3560 1579.3470 1549.9790 1576.1530 7.521819e+09 1567.0910 2006-11-27 1574.1130 1598.7440 1574.1130 1598.7440 7.212786e+09 1581.1530 2006-11-28 1597.7200 1604.7190 1585.3620 1596.8400 7.025637e+09 1598.7440 2006-11-29 1575.3030 1620.2870 1575.3030 1617.9880 7.250354e+09 1596.8400 2006-11-30 1621.4280 1657.3230 1621.4280 1657.3230 9.656888e+09 1617.9880 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 2020-11-11 5477.8870 5493.5867 5422.9110 5425.8017 5.604086e+10 5494.1042 2020-11-12 5439.2296 5454.3452 5413.9659 5435.1379 4.594251e+10 5425.8017 2020-11-13 5418.2953 5418.3523 5364.2031 5402.7702 4.688916e+10 5435.1379 2020-11-16 5422.3565 5456.7264 5391.9232 5456.7264 5.593672e+10 5402.7702 2020-11-17 5454.0696 5454.0696 5395.6052 5428.0765 5.857009e+10 5456.7264 [5,057 rows x 7 columns]\nBelow is partial data of the original factors. After undergoing the four factor cleaning steps—missing value imputation, outlier removal, factor normalization, and factor neutralization—the cleaned factor data is displayed in the table below.\nTable 4.4: Original Factor Data trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; Table 4.5: Cleaned Factor Data sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.3 Factor Cleaning 4.3.1 Handling Missing Values and Outlier Removal Methods for handling missing values in factors include case elimination, mean substitution, regression imputation, etc. This paper adopts the relatively simple mean substitution method to handle missing values, replacing missing data with the factor’s average value. For outlier removal, methods include median trimming, percentile-based trimming, and $3\\sigma$ trimming. This paper uses the $3\\sigma$ trimming method, which applies the statistical $3\\sigma$ principle to transform extreme factor values more than three standard deviations away from the mean to exactly three standard deviations away from the mean. The specific calculation formula is as follows:\n$$ X_i^{\\prime}= \\begin{cases} \\bar{X}+3 \\sigma \u0026 \\text{if } X_i \u003e \\bar{X} + 3 \\sigma \\\\ \\bar{X}-3 \\sigma \u0026 \\text{if } X_i \u003c \\bar{X} - 3 \\sigma \\\\ X_i \u0026 \\text{if } \\bar{X} - 3 \\sigma \u003c X_i \u003c \\bar{X} + 3 \\sigma \\end{cases} $$Where:\n$X_{i}$: Original value of the factor before processing $\\bar{X}$: Mean of the factor series $\\sigma$: Standard deviation of the factor series $X_{i}^{\\prime}$: Value of the factor after outlier removal 4.3.2 Factor Normalization In this experiment, multiple factors with different dimensions are selected, and their units are not entirely consistent. To facilitate comparison and regression, normalization of factors is necessary. Common normalization methods include Min-Max normalization, Z-score normalization, and Decimal scaling normalization. This paper adopts the Z-score normalization method, standardizing data using the mean and standard deviation of the original data. The processed data follow a standard normal distribution with a mean of 0 and a standard deviation of 1, resulting in normalized values that are both positive and negative, forming a standard normal distribution curve.\nThe Z-score normalization formula used in this paper is as follows:\n$$ \\tilde{x}=\\frac{x_{i}-u}{\\sigma} $$Where:\n$x_{i}$: Original value of the factor $u$: Mean of the factor series $\\sigma$: Standard deviation of the factor series $\\tilde{x}$: Normalized factor value 4.3.3 Factor Neutralization Factor neutralization aims to eliminate the influence of other factors on the selected factor, ensuring that the stocks chosen for constructing the quantitative investment strategy are more diversified rather than concentrated in specific industries or market capitalizations. This better distributes investment portfolio risk and addresses multicollinearity issues among factors. Market capitalization and industry are the two main independent variables influencing stock returns; therefore, they must be considered during factor cleaning. In this paper\u0026rsquo;s empirical study, only industry factors are included, with the market factor incorporated into the industry factors. The single-factor regression model for neutralizing factors is shown in formula (31). The residuals from the following regression model are used as the new, neutralized factor values.\n$$ \\tilde{r}_{j}^{t}=\\sum_{s=1}^{S} X_{j s}^{t} \\tilde{f}_{s}^{t} + X_{j k}^{t} \\tilde{f}_{k}^{t} + \\tilde{u}_{j}^{t} $$Where:\n$\\tilde{r}_{j}^{t}$: Return of stock $j$ in period $t$ $X_{j s}^{t}$: Exposure of stock $j$ to industry $s$ in period $t$ $\\tilde{f}_{s}^{t}$: Return of industry $s$ in period $t$ $X_{j k}^{t}$: Exposure of stock $j$ to factor $k$ in period $t$ $\\tilde{f}_{k}^{t}$: Return of factor $k$ in period $t$ $\\tilde{u}_j^t$: A $0-1$ dummy variable, indicating whether stock $j$ belongs to industry $s$ (1) or not (0) In this paper, the industry assignment is not proportionally split; that is, stock $j$ can belong to only one specific industry $s$, with an exposure of 1 to industry $s$ and 0 to all other industries. This paper uses the Shenwan Hongyuan industry classification standard, categorizing companies into 28 primary industries: Agriculture, Forestry, Animal Husbandry, and Fishery; Mining; Chemicals; Steel; Non-Ferrous Metals; Electronic Components; Home Appliances; Food and Beverage; Textile and Apparel; Light Manufacturing; Pharmaceutical and Biotechnology; Public Utilities; Transportation; Real Estate; Commerce and Trade; Catering and Tourism; Comprehensive; Building Materials; Architectural Decoration; Electrical Equipment; National Defense and Military Industry; Computers; Media; Telecommunications; Banking; Non-Banking Finance; Automobiles; and Machinery Equipment. The table below shows the historical market data of the Shenwan Hongyuan primary industry index as of February 5, 2021.\nTable 4.9: Historical Market Data of Shenwan Hongyuan Primary Industry Index as of February 5, 2021 Index Code Index Name Release Date Opening Index Highest Index Lowest Index Closing Index Trading Volume (Billion) Trading Amount (Hundred Million Yuan) Percentage Change (%) 801010 Agriculture, Forestry, Animal Husbandry, and Fishery 2021/2/5 0:00 4,111.43 4,271.09 4,072.53 4,081.81 1.581 307.82 -0.3 801020 Mining 2021/2/5 0:00 2,344.62 2,357.33 2,288.97 2,289.41 1.806 115.6 -2.25 801030 Chemicals 2021/2/5 0:00 4,087.77 4,097.59 3,910.67 3,910.67 5.578 778.85 -3.95 801040 Steel 2021/2/5 0:00 2,253.78 2,268.17 2,243.48 2,250.81 1.161 48.39 -1.02 801050 Non-Ferrous Metals 2021/2/5 0:00 4,212.10 4,250.59 4,035.99 4,036.74 4.541 593.92 -4.43 801080 Electronic Components 2021/2/5 0:00 4,694.80 4,694.80 4,561.95 4,561.95 5.267 850.79 -2.78 801110 Home Appliances 2021/2/5 0:00 10,033.82 10,171.26 9,968.93 10,096.83 0.855 149.18 0.83 801120 Food and Beverage 2021/2/5 0:00 30,876.33 31,545.02 30,649.57 30,931.69 1.132 657.11 0.47 801130 Textile and Apparel 2021/2/5 0:00 1,614.48 1,633.89 1,604.68 1,607.63 0.628 57.47 -0.39 801140 Light Manufacturing 2021/2/5 0:00 2,782.07 2,791.88 2,735.48 2,737.24 1.528 176.16 -1.35 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Data Source: Shenwan Hongyuan\nBelow is partial data of the original factors. After undergoing the four factor cleaning steps—missing value imputation, outlier removal, factor normalization, and factor neutralization—the cleaned factor data is displayed in the table below.\nTable 4.10: Original Factor Data trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; Table 4.11: Cleaned Factor Data sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.4 Factor Selection Based on LightGBM Freeman (2001)$^{[20]}$ proposed Gradient Boosting Decision Trees (GBDT), an iterative regression decision tree method. The main idea is to iteratively add weak classifiers (usually decision trees) to optimize the model, minimizing the loss function. GBDT can be expressed as:\n$$ \\hat{y} = \\sum_{m=1}^{M} \\gamma_m h_m(\\mathbf{x}) $$Where:\n\\( M \\) is the number of iterations \\( \\gamma_m \\) is the weight of the \\( m \\)-th weak classifier \\( h_m(\\mathbf{x}) \\) is the \\( m \\)-th decision tree model The training process of GBDT minimizes the loss function by sequentially fitting in the direction of the negative gradient, with the specific update formula as:\n$$ \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{N} L\\left(y_i, \\hat{y}_{i}^{(m-1)} + \\gamma h_m(\\mathbf{x}_i)\\right) $$Where \\( L \\) is the loss function, \\( y_i \\) is the true value, and \\( \\hat{y}_{i}^{(m-1)} \\) is the prediction after the \\( m-1 \\)-th iteration.\nLight Gradient Boosting Machine (LightGBM)$^{[21]}$ is an efficient implementation framework of the GBDT algorithm, initially developed by Microsoft as a free and open-source distributed gradient boosting framework. LightGBM is based on decision tree algorithms and is widely used in ranking, classification, and other machine learning tasks. Its development focuses on performance and scalability, with major advantages including highly efficient parallel training, faster training speed, lower memory consumption, better accuracy, support for distributed computing, and fast processing of massive data$^{[22]}$.\nThe core algorithm of LightGBM is based on the following optimization objective:\n$$ L = \\sum_{i=1}^{N} l(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(h_m) $$Where \\( l \\) is the loss function, \\( \\Omega \\) is the regularization term controlling model complexity, typically expressed as:\n$$ \\Omega(h_m) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 $$Here, \\( T \\) is the number of leaves in the tree, \\( w_j \\) is the weight of the \\( j \\)-th leaf, and \\( \\gamma \\) and \\( \\lambda \\) are regularization parameters.\nLightGBM employs techniques such as Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) to significantly enhance training efficiency and model performance.\nIn this study, the loss function used during training is Mean Squared Error (MSE), defined as:\n$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$Where \\( y \\) is the true return, \\( \\hat{y} \\) is the predicted return, and \\( N \\) is the number of samples.\nThe specific algorithm process of this subsection is as follows:\nData Preparation: Use one year\u0026rsquo;s worth of data for each stock in the A-share market, consisting of 50 factor data points and the historical return for the next month as features.\nModel Training: Utilize Grid Search to optimize the LightGBM model’s hyperparameters and train the model to predict the next month\u0026rsquo;s returns. The model training process is shown in Fig. 12.\n$$ \\text{Parameter Optimization:} \\quad \\theta^* = \\arg\\min_\\theta \\sum_{i=1}^{N} L(y_i, \\hat{y}_i(\\theta)) $$Where \\( \\theta \\) represents the set of model hyperparameters, and \\( \\theta^* \\) is the optimal set.\nFactor Importance Calculation: Use LightGBM\u0026rsquo;s feature_importances_ method to calculate the feature importance of each factor. Feature importance is primarily measured using two indicators:\nSplit: The number of times the feature is used for splitting across all trees. Gain: The total gain brought by the feature across all splits (i.e., the reduction in the loss function). The feature importance of factors can be expressed as:\n$$ \\text{Importance}_{\\text{split}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\mathbb{I}(f \\text{ used in the split of the } j\\text{-th leaf node of the } m\\text{-th tree}) $$$$ \\text{Importance}_{\\text{gain}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\Delta L_{m,j} \\cdot \\mathbb{I}(f \\text{ used in the split of the } j\\text{-th leaf node of the } m\\text{-th tree}) $$Where \\( \\mathbb{I} \\) is the indicator function, and \\( \\Delta L_{m,j} \\) is the loss reduction brought by factor \\( f \\) in the split of the \\( j \\)-th leaf node of the \\( m \\)-th tree.\nFactor Selection: Rank the factors based on their calculated importance and select the top ten most important factors for cross-sectional analysis, as shown in Table 4.9.\nTable 4.9: Partial Factor Importance Ranking Importance Feature Name Trade Date 35 factor_35 2010-08-11 27 factor_27 2010-08-11 33 factor_33 2010-08-11 20 factor_20 2010-08-11 24 factor_24 2010-08-11 45 factor_45 2010-08-11 37 factor_37 2010-08-11 49 factor_49 2010-08-11 19 factor_19 2010-08-11 47 factor_47 2010-08-11 22 factor_22 2010-09-09 20 factor_20 2010-09-09 30 factor_30 2010-09-09 24 factor_24 2010-09-09 Code Implementation Snippet: The following code snippet demonstrates part of the training process used for factor selection. def feature_choice( self, days=21, is_local=False ): if is_local: feature_info = pd.read_hdf(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;)) else: factors = self.get_env().query_data(Factors_Data) factors = factors[ factors[COM_DATE] \u0026gt;= \u0026#39;2010-01-01\u0026#39; ] trade_list = list(set(factors[COM_DATE])) trade_list.sort() if len(trade_list) % days == 0: n = int(len(trade_list) / days) - 7 else: n = int(len(trade_list) / days) - 6 feature_info = pd.DataFrame() begin_index = 147 feature = list(factors.columns) feature.remove(COM_SEC) feature.remove(COM_DATE) feature.remove(Ret) for i in range(n): end_date = days * i + begin_index - 21 begin_date = days * i trade_date = days * i + begin_index print(trade_list[trade_date]) train_data = factors[ (factors[COM_DATE] \u0026lt;= trade_list[end_date]) \u0026amp; (factors[COM_DATE] \u0026gt;= trade_list[begin_date]) ] model = lgb.LGBMRegressor() model.fit(train_data[feature], train_data[Ret]) feature_info_cell = pd.DataFrame(columns=Info_Fields) feature_info_cell[Importance] = model.feature_importances_ feature_info_cell[Feature_Name] = model.feature_name_ feature_info_cell = feature_info_cell.sort_values(by=Importance).tail(10) feature_info_cell[COM_DATE] = trade_list[trade_date] feature_info = pd.concat( [feature_info, feature_info_cell], axis=0 ) h = pd.HDFStore(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;), \u0026#39;w\u0026#39;) h[\u0026#39;data\u0026#39;] = feature_info h.close() self.get_env().add_data(feature_info, Feature_Info) pass Through the above process, LightGBM efficiently screens out factors that have the most significant impact on predicting future returns, thereby enhancing the model’s predictive capability and interpretability.\n4.5 Factor Combination Based on BiLSTM This subsection uses BiLSTM for factor combination. The specific principles of BiLSTM have been introduced in Chapter 2, so they will not be reiterated here. Below, we describe the specific network structure used. After extensive experimentation, the BiLSTM network structure is set as shown in Table 4.10 and Fig. 12. Between layers, the default tanh and linear activation functions for recurrent neural networks are used. To prevent overfitting, Dropout with a dropout rate of 0.01 is added. However, if the dropout rate is too high, it can lead to underfitting. The BiLSTM recurrent layer has 100 neurons, and the network comprises one BiLSTM layer and three fully connected layers, with a Dropout layer between the BiLSTM layer and the first fully connected layer.\nTable 4.10: BiLSTM Network Structure Layer(type) Output Shape Param# bidirectional_1 (Bidirectional) (None, 100) 22,400 dropout_1 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 256) 25,856 dropout_2 (Dropout) (None, 256) 0 dense_2 (Dense) (None, 64) 16,448 dense_3 (Dense) (None, 1) 0 Total params: 66,769\nTrainable params: 66,769\nNon-trainable params: 0\nDue to the large volume of training data in this experiment, epochs=400 and batch_size=1024 are selected for the BiLSTM model. The loss function used is Mean Squared Error (MSE), and the optimizer chosen is Stochastic Gradient Descent (SGD). Compared to Gradient Descent (GD), SGD has the advantages of more effectively utilizing information in redundant data, excellent performance in early iterations, and suitability for handling large sample datasets$^{[23]}$. Given the large training data volume, using SGD allows for faster training as it updates with one sample at a time, significantly reducing training time. The default settings in the keras package are used: lr=0.01, momentum=0.0, decay=0.0, and nesterov=False.\nParameter Descriptions:\nlr: Learning rate momentum: Momentum parameter decay: Learning rate decay after each update nesterov: Whether to use Nesterov momentum The specific algorithm process of this subsection is as follows:\nData Usage: Use one year\u0026rsquo;s worth of data for each stock in the A-share market, consisting of 10 factors selected by LightGBM and the historical return for the next month as features. Model Training: Use the BiLSTM model to train with the selected factors to predict the next month\u0026rsquo;s returns, as shown in Fig. 12. Fig. 12. Rolling Window\nPrediction: Apply the trained BiLSTM model to the out-of-sample data of one month to obtain the expected returns for each stock in the next month. The predicted returns are shown in Table 4.11. Table 4.11: Partial Display of Predicted Returns for Stocks sec_code trade_date y_hat 000001.SZ 2011/5/26 0.0424621 000002.SZ 2011/5/26 -0.1632174 000004.SZ 2011/5/26 -0.0642319 000005.SZ 2011/5/26 0.08154649 000006.SZ 2011/5/26 0.00093213 000007.SZ 2011/5/26 -0.073218 000008.SZ 2011/5/26 -0.0464256 000009.SZ 2011/5/26 -0.091549 000010.SZ 2011/5/26 0.08154649 000011.SZ 2011/5/26 -0.1219943 000012.SZ 2011/5/26 -0.1448984 000014.SZ 2011/5/26 0.09038845 000016.SZ 2011/5/26 -0.11225 Code Implementation Snippet: The following code snippet demonstrates part of the training process used to construct a BiLSTM network. def build_net_blstm(self): model = ks.Sequential() model.add( ks.layers.Bidirectional(ks.layers.LSTM( 50 ),input_shape=(11,10)) ) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(256)) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(64)) model.add(ks.layers.Dense(1)) model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;mse\u0026#39;) model.summary() self.set_model(model) 4.6 Quantitative Strategy and Strategy Backtesting 4.6.1 Backtesting Metrics Below are common backtesting metrics used to evaluate the strategy. The metrics include Total Rate of Return, Annualized Rate of Return, Annualized Volatility, Sharpe Ratio, Maximum Drawdown (MDD), Annualized Turnover Rate, and Annualized Transaction Cost Rate, assuming 252 trading days in a year and a risk-free rate of 3.5%. The transaction fee is assumed to be 0.2% per trade.\nTotal Rate of Return: Indicates the cumulative return of the strategy. The higher the cumulative return, the better the strategy\u0026rsquo;s performance. The formula is as follows: $$ \\text{Total Rate of Return} = r_{p} = \\frac{P_{1} - P_{0}}{P_{0}} $$Where:\n$P_{1}$: Total value of stocks and cash at the end $P_{0}$: Total value of stocks and cash at the beginning Annualized Rate of Return: Converts the total cumulative return into a geometric average return on an annual basis. The higher the annualized return, the better the strategy. The formula is as follows: $$ \\text{Annualized Rate of Return} = R_{p} = \\left(1 + r_{p}\\right)^{\\frac{252}{t}} - 1 $$Where:\n$r_{p}$: Total rate of return $t$: Number of trading days the strategy is executed Annualized Volatility: Defined as the standard deviation of the logarithm of the asset’s annual return rates. It measures the risk of the strategy, with higher volatility indicating higher risk. The formula is as follows: $$ \\begin{aligned} \\text{Annualized volatility} = \\sigma_{p} \u0026= \\sqrt{\\frac{252}{t-1} \\sum_{i=1}^{t}\\left(r_{d} - \\bar{r}_{d}\\right)^{2}} \\\\ \\bar{r}_{d} \u0026= \\frac{1}{t} \\sum_{i=1}^{t} r_{d_{i}} \\end{aligned} $$Where:\n$r_{d_{i}}$: Daily return on the $i$-th day $\\bar{r}_{d}$: Daily average return $t$: Number of trading days the strategy is executed Sharpe Ratio: Proposed by Sharpe (1966)$^{[24]}$, it represents the excess return per unit of risk. Here, the annualized Sharpe ratio is calculated as follows: $$ S = \\frac{R_{p} - R_{f}}{\\sigma_{p}} $$Where:\n$R_{p}$: Annualized rate of return $R_{f}$: Risk-free rate $\\sigma_{p}$: Annualized volatility Maximum Drawdown (MDD): Represents the maximum decline in the total value of stocks and cash during the strategy’s execution period. It measures the most extreme potential loss scenario of the strategy. $$ MDD = \\frac{\\max \\left(V_{x} - V_{y}\\right)}{V_{x}} $$Where:\n$V_{x}$ and $V_{y}$ are the total values of stocks and cash on day $x$ and day $y$, respectively, with $x \u0026lt; y$. Annualized Turnover Rate: Measures the frequency of buying and selling stocks within the investment portfolio. A higher turnover rate indicates more frequent rebalancing and higher transaction costs. $$ \\text{Change} = \\frac{N \\times 252}{t} $$Where:\n$t$: Number of trading days the strategy is executed $N$: Total number of buy and sell operations Annualized Transaction Cost Rate: Measures the transaction costs of the investment portfolio strategy, with higher values indicating higher transaction costs. $$ c = \\left(1 + \\text{commission}\\right)^{\\text{change}} - 1 $$Where:\nChange: Annualized turnover rate Commission: Transaction fee rate 4.6.2 Strategy and Backtesting Results This paper adopts a quantitative trading strategy that rebalances the portfolio every month (i.e., a rebalancing period of 28 trading days). Each rebalance involves equally weighting the top 25 stocks with the highest expected returns predicted by BiLSTM and selling the previously held stocks. The backtesting period and rules are as follows:\nBacktesting Period: From January 2012 to October 2020. Backtesting Stock Pool: All A-share stocks, excluding Special Treatment (ST) stocks. Transaction Fees: A trading commission of 0.2% is paid to brokers upon buying and selling, with a minimum commission of 5 yuan per trade if the calculated fee is less than 5 yuan. Trading Rules: Stocks hitting the daily limit-up cannot be bought, and stocks hitting the daily limit-down cannot be sold on the same day. Table 4.12: Strategy Backtesting Results Total Rate of Return Annualized Rate of Return Annualized Volatility Sharpe Ratio Maximum Drawdown Annualized Turnover Rate Annualized Transaction Cost Rate Strategy 701.00% 29.18% 33.44% 0.77 51.10% 51.10% 11.35% Benchmark 110.40% 9.70% 26.01% 0.24 58.49% 58.49% 0.00% Fig. 22. Net Profit Curve\nThe backtesting results are shown in Table 4.12 and Fig. 22. The strategy employed is the LightGBM-BiLSTM quantitative strategy introduced in this chapter. The benchmark used is the CSI All Share Index (000985). From the results, the strategy achieves a cumulative return of 701.00%, significantly higher than the benchmark’s 110.40%. The annualized return is 29.18%, far exceeding the benchmark’s 9.70%. The Sharpe ratio is 0.77, which is higher than the benchmark’s 0.24. These three backtesting metrics indicate that the LightGBM-BiLSTM quantitative strategy indeed provides greater returns to investors, demonstrating the effectiveness of using deep learning to construct quantitative investment strategies. The strategy’s annualized volatility is 33.44%, higher than the benchmark’s 26.01%, and the maximum drawdown is 51.10%, lower than the benchmark’s 58.49%. These two metrics indicate that the LightGBM-BiLSTM quantitative strategy carries certain risks, particularly in resisting systemic risk shocks. The annualized turnover rate is 11.35%, and the annualized transaction cost rate is 2.29%, indicating that the strategy is not a high-frequency trading strategy with relatively low transaction costs. The return curve shows that the LightGBM-BiLSTM quantitative strategy’s returns were similar to the benchmark in the first two years without notable advantages. However, starting around April 2015, the strategy’s returns clearly outperformed the benchmark. Overall, the LightGBM-BiLSTM quantitative strategy delivers substantial returns but still carries certain risks.\nChapter 5: Conclusion and Outlook 5.1 Conclusion This paper first introduced the research background and significance of deep learning-based stock price prediction and quantitative investment strategy research. It then reviewed the current domestic and international research status of stock price prediction and quantitative investment strategies, outlined the innovations of this paper, and presented the research framework. Next, the theoretical foundations chapter briefly introduced the deep learning models and the development history of quantitative investment used in this paper, with a focus on the basic structures, principles, and characteristics of LSTM, GRU, and BiLSTM models.\nSubsequently, using daily data from Pudong Bank and IBM, the paper conducted a series of data processing steps and feature extraction for data preprocessing. It then detailed the network structures and hyperparameter settings of the LSTM, GRU, and BiLSTM models used in the experiments. The study used LSTM, GRU, and BiLSTM to predict the closing prices of the two stocks and compared the models’ performance. The experimental results showed that the BiLSTM model achieved higher prediction accuracy for both stocks.\nFinally, to further demonstrate BiLSTM’s application value in finance, this paper constructed a LightGBM-BiLSTM-based quantitative investment model. It selected and cleaned multiple factors from the entire A-share market data, employed LightGBM for factor selection, and used BiLSTM for factor combination. The paper then constructed a quantitative investment strategy based on the model and compared it against the CSI All Share Index using metrics such as cumulative return, annualized return, annualized volatility, and Sharpe ratio. The comparison showed that the LightGBM-BiLSTM quantitative investment model achieved better returns, indicating the effectiveness of using deep learning to build quantitative investment strategies.\n5.2 Outlook Although this paper compared the performance of LSTM, GRU, and BiLSTM models in predicting stock closing prices and achieved certain results with the LightGBM-BiLSTM quantitative investment strategy, there are still some shortcomings. Based on the research outcomes of this paper, the following areas can be further explored and improved:\nDiverse Prediction Targets: This paper focuses on predicting stock closing prices, which, while intuitive, is challenging and less interpretable under the Random Walk Hypothesis (RWH) proposed by Bachelier (1900)$^{[26]}$, which posits that stock prices follow a random walk and are unpredictable. Although behavioral economists have shown that this hypothesis is not entirely correct, it indicates that simply predicting stock closing prices is difficult and less interpretable$^{[27][28]}$. Therefore, future research can explore predicting stock volatility, stock price movements (up or down), and stock returns.\nDiverse Model Comparisons: This paper compared LSTM, GRU, and BiLSTM models in predicting stock prices, demonstrating that BiLSTM achieves better accuracy. However, it lacks comparisons with other diverse models. Future research can delve into comparing single or hybrid models like ARIMA, Convolutional Neural Networks (CNN), Deep Neural Networks (DNN), CNN-LSTM, Transformer, and TimeGPT.\nDiverse Factor Selection: The factors used in constructing the quantitative investment strategy in this paper are primarily technical price and volume factors, with a limited variety. Future research can incorporate financial factors, sentiment factors, growth factors, and other types of factors to enhance the strategy’s performance. Additionally, incorporating timing strategies—such as increasing positions when the market is predicted to rise and decreasing positions when the market is predicted to fall—can capture beta ($\\beta$) returns.\nInvestment Portfolio Optimization: The factor combination process in this paper remains imperfect. Future research can utilize quadratic programming methods to optimize the investment portfolio.\nHigh-Frequency Trading Strategy Research: The quantitative investment strategy in this paper adopts a low-frequency trading approach. Future research can leverage tick data to explore high-frequency and ultra-high-frequency trading strategies.\nReferences [1] White, H. “Economic prediction using neural networks: The case of IBM daily stock returns.” Proc. of ICNN. 1988, 2: 451-458.\n[2] Kimoto, T., Asakawa, K., Yoda, M., et al. “Stock market prediction system with modular neural networks.” Proc. of 1990 IJCNN International Joint Conference on Neural Networks. IEEE, 1990: 1-6.\n[3] Zhang, G. P. “Time series forecasting using a hybrid ARIMA and neural network model.” Neurocomputing. 2003, 50: 159-175.\n[4] Akita, R., Yoshihara, A., Matsubara, T., et al. “Deep learning for stock prediction using numerical and textual information.” Proc. of 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS). IEEE, 2016: 1-6.\n[5] 宮崎邦洋, 松尾豊. “Deep Learning を用いた株価予測の分析.” 人工知能学会全国大会論文集 第31回全国大会. 一般社団法人 人工知能学会, 2017: 2D3OS19a3-2D3OS19a3.\n[6] Kim, T., Kim, H. Y. “Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data.” PLoS ONE. 2019, 14(2): e0212320.\n[7] Hochreiter, S., Schmidhuber, J. “Long short-term memory.” Neural Computation. 1997, 9(8): 1735-1780.\n[8] Cho, K., Van Merriënboer, B., Gulcehre, C., et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078. 2014.\n[9] Chung, J., Gulcehre, C., Cho, K. H., et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” arXiv preprint arXiv:1412.3555. 2014.\n[10] Gruber, N., Jockisch, A. “Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?” Frontiers in Artificial Intelligence. 2020, 3(40): 1-6.\n[11] Markowitz, H. “Portfolio Selection.” The Journal of Finance. 1952, 7(1): 77-91. doi:10.2307/2975974.\n[12] Merton, R. C. “An analytic derivation of the efficient portfolio frontier.” Journal of Financial and Quantitative Analysis. 1972: 1851-1872.\n[13] Sharpe, W. F. “Capital asset prices: A theory of market equilibrium under conditions of risk.” The Journal of Finance. 1964, 19(3): 425-442.\n[14] Lintner, J. “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.” Review of Economics and Statistics. 1965, 47(1): 13-37.\n[15] Mossin, J. “Equilibrium in a capital asset market.” Econometrica: Journal of the Econometric Society. 1966: 768-783.\n[16] Ross, S. A. “The arbitrage theory of capital asset pricing.” Journal of Economic Theory. 1976, 13(3): 341-60.\n[17] Fama, E. F., French, K. R. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics. 1993, 33(1): 3-56.\n[18] Fama, E. F., French, K. R. “A five-factor asset pricing model.” Journal of Financial Economics. 2015, 116(1): 1-22.\n[19] Kingma, D. P., Ba, J. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980. 2014.\n[20] Friedman, J. H. “Greedy function approximation: A gradient boosting machine.” Annals of Statistics. 2001: 1189-1232.\n[21] Kopitar, L., Kocbek, P., Cilar, L., et al. “Early detection of type 2 diabetes mellitus using machine learning-based prediction models.” Scientific Reports. 2020, 10(1): 1-12.\n[22] Ke, G., Meng, Q., Finley, T., et al. “Lightgbm: A highly efficient gradient boosting decision tree.” Advances in Neural Information Processing Systems. 2017, 30: 3146-3154.\n[23] Bottou, L., Curtis, F. E., Nocedal, J. “Optimization methods for large-scale machine learning.” SIAM Review. 2018, 60(2): 223-311.\n[24] Sharpe, W. F. “Mutual fund performance.” The Journal of Business. 1966, 39(1): 119-138.\n[25] Sharpe, W. F. “The sharpe ratio.” Journal of Portfolio Management. 1994, 21(1): 49-58.\n[26] Bachelier, L. “Théorie de la spéculation.” Annales Scientifiques de l\u0026rsquo;École Normale Supérieure. 1900, 17: 21-86.\n[27] Fromlet, H. “Behavioral finance-theory and practical application: Systematic analysis of departures from the homo oeconomicus paradigm are essential for realistic financial research and analysis.” Business Economics. 2001: 63-69.\n[28] Lo, A. W. “The adaptive markets hypothesis.” The Journal of Portfolio Management. 2004, 30(5): 15-29.\nReference Blogs Colah\u0026rsquo;s Blog. (2015, August 27). Understanding LSTM Networks. Citation Citation: To reproduce or cite the content of this article, please acknowledge the original author and source.\nCited as:\nYue Shui. (Apr 2021). Research on Stock Price Prediction and Quantitative Strategies Based on Deep Learning.\nhttps://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\nOr\n@article{syhya2021stockprediction, title = \u0026#34;Research on Stock Price Prediction and Quantitative Strategies Based on Deep Learning\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2021\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/","summary":"\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eThe stock market is a crucial component of the financial market. In recent years, the stock market has flourished, attracting researchers from various fields to study stock price prediction and quantitative investment strategies. With the development of artificial intelligence and machine learning in recent years, scholars have transitioned from traditional statistical models to AI algorithms. Particularly, after the surge of deep learning, neural networks have achieved commendable results in stock price prediction and quantitative investment strategy research. The goal of deep learning is to learn multi-level features by combining low-level features to construct abstract high-level features, thereby mining the distributed feature representations of data and performing complex nonlinear modeling based on this to accomplish prediction tasks. Among these, RNNs are widely applied to sequential data such as natural language and speech. Daily stock prices and trading information are sequential data; therefore, many researchers have previously used RNNs to predict stock prices. However, basic recurrent neural networks suffer from the vanishing gradient problem when the number of layers is too large, a problem that the emergence of LSTM solved. Subsequently, variants of LSTM such as GRU, Peephole LSTM, and BiLSTM appeared. Traditional stock prediction models often overlook the temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a theoretical perspective, the BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions in time series, avoiding the vanishing and exploding gradient problems in long sequences, and better learning information with long-term dependencies.\u003c/p\u003e","title":"Research on Stock Price Prediction and Quantitative Strategies Based on Deep Learning"}]