<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>大语言模型对齐: 直接偏好优化(DPO) | Yue Shui 博客</title><meta name=keywords content="AI,NLP,LLM,Post-training,DPO,RLHF,Alignment,Bradley–Terry Model"><meta name=description content="这篇博客主要介绍一种比 RLHF 更精简的替代算法 DPO。与 RLHF 一样，DPO 目的是使模型输出与人类偏好保持一致，但它在实现上更加简单，并且对资源的需求更低。在项目资源受限的情况下，DPO 是一个实用解决方案。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2025-02-08-dpo/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-02-08-dpo/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-02-08-dpo/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2025-02-08-dpo/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="大语言模型对齐: 直接偏好优化(DPO)"><meta property="og:description" content="这篇博客主要介绍一种比 RLHF 更精简的替代算法 DPO。与 RLHF 一样，DPO 目的是使模型输出与人类偏好保持一致，但它在实现上更加简单，并且对资源的需求更低。在项目资源受限的情况下，DPO 是一个实用解决方案。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-08T12:00:00+08:00"><meta property="article:modified_time" content="2025-09-01T18:02:26+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Post-Training"><meta property="article:tag" content="DPO"><meta property="article:tag" content="RLHF"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="大语言模型对齐: 直接偏好优化(DPO)"><meta name=twitter:description content="这篇博客主要介绍一种比 RLHF 更精简的替代算法 DPO。与 RLHF 一样，DPO 目的是使模型输出与人类偏好保持一致，但它在实现上更加简单，并且对资源的需求更低。在项目资源受限的情况下，DPO 是一个实用解决方案。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"大语言模型对齐: 直接偏好优化(DPO)","item":"https://syhya.github.io/zh/posts/2025-02-08-dpo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"大语言模型对齐: 直接偏好优化(DPO)","name":"大语言模型对齐: 直接偏好优化(DPO)","description":"这篇博客主要介绍一种比 RLHF 更精简的替代算法 DPO。与 RLHF 一样，DPO 目的是使模型输出与人类偏好保持一致，但它在实现上更加简单，并且对资源的需求更低。在项目资源受限的情况下，DPO 是一个实用解决方案。\n","keywords":["AI","NLP","LLM","Post-training","DPO","RLHF","Alignment","Bradley–Terry Model"],"articleBody":"这篇博客主要介绍一种比 RLHF 更精简的替代算法 DPO。与 RLHF 一样，DPO 目的是使模型输出与人类偏好保持一致，但它在实现上更加简单，并且对资源的需求更低。在项目资源受限的情况下，DPO 是一个实用解决方案。\n符号 符号 含义 \\( x \\) 用户输入（Prompt），即模型需要回答的问题 \\( y \\) 模型生成的回答（Response / Completion），即模型输出的文本 \\( \\pi_\\theta(y \\mid x) \\) Actor 模型：待训练策略，用于生成回答 \\(y\\)；参数为 \\(\\theta\\) \\( \\pi_{\\mathrm{ref}}(y \\mid x) \\) 参考模型：冻结的 SFT 模型，作为对齐基准 \\( r_\\phi(x,y) \\) 奖励模型：用于评估回答 \\(y\\) 质量的奖励函数；参数为 \\(\\phi\\) \\( V_\\psi(x) \\) critic 模型：用于估计给定输入 \\(x\\) 下未来累计奖励的值函数；参数为 \\(\\psi\\) \\( \\pi^*(y \\mid x) \\) 最优策略分布，通过参考模型与奖励函数确定 \\( r_\\theta(x,y) \\) 基于 Actor 模型导出的奖励函数，通过 \\(\\pi_\\theta\\) 与 \\(\\pi_{\\mathrm{ref}}\\) 构造 \\(\\beta\\) 超参数，控制 KL 惩罚项或对数比差异项的权重 \\(\\mathbb{D}_{\\mathrm{KL}}[P \\| Q]\\) KL 散度，衡量概率分布 \\(P\\) 与 \\(Q\\) 之间的差异 \\(\\sigma(z)\\) Sigmoid 函数，定义为：\\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\) \\(\\log\\) 对数函数 \\(\\mathbb{E}\\) 期望算子，用于求随机变量的平均值 \\( (y_w, y_l) \\) 一对偏好数据，其中 \\( y_w \\) 表示被偏好（质量更好）的回答，\\( y_l \\) 表示质量较差的回答 \\( P\\left(y_w \\succ y_l \\mid x\\right) \\) 在输入 \\(x\\) 下，回答 \\( y_w \\) 优于 \\( y_l \\) 的概率 \\( Z(x) \\) 配分函数，对所有回答 \\(y\\) 归一化概率分布 \\( \\mathcal{L}_{\\mathrm{DPO}} \\) DPO 的损失函数 从 RLHF 到 DPO RLHF OpenAI 主要利用人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)(Christiano et al., 2017) 来训练 InstructGPT(Ouyang et al., 2022)，而其构成了大语言模型（如 ChatGPT, Llama 等）的基础。整个训练过程通常包括以下三个主要步骤：\nFig. 1. A diagram illustrating the three steps of InstructGPT. (Image source: Ouyang et al., 2022)\n监督微调（SFT）\n利用大量人工示例数据对预训练模型进行微调，得到一个初步能理解指令并生成合理回答的模型，即参考模型 \\( \\pi_{\\mathrm{ref}}(y \\mid x) \\) 。\n奖励模型训练\n这里我们简化只考虑生成两个不同的结果，实际可以生成多个结果进行排序。针对同一输入 \\(x\\) 生成两个回答 \\(y_w\\)（较优）和 \\(y_l\\)（较劣），由人工排序后收集偏好数据。基于这些数据训练奖励模型 \\(r_\\phi(x, y)\\)，使其能预测哪种回答更符合人类偏好。\n基于 PPO 的强化学习\n利用奖励模型 \\(r_\\phi\\) 提供的反馈，通过 PPO 算法优化 Actor 模型 \\(\\pi_\\theta\\) 以提升回答质量。为防止模型偏离 \\(\\pi_{\\mathrm{ref}}\\)，在优化过程中引入 KL 惩罚项。该阶段通常涉及以下 4 个模型：\n\\(\\pi_\\theta\\)：经过 SFT 后待更新的模型。 \\(\\pi_{\\mathrm{ref}}\\)：冻结的 SFT 模型，作为对齐基准。 \\(r_\\phi\\)：用于评估回答质量，参数固定。 \\(V_\\psi\\)：用于估计未来奖励，辅助 Actor 模型更新。 RLHF 的局限性 尽管 RLHF 能充分利用人类偏好信息提升模型对齐效果，但其固有局限性包括：\n多模型训练：除 Actor 模型 \\(\\pi_\\theta\\) 外，还需额外训练奖励模型 \\(r_\\phi\\) 和 Critic 模型 \\(V_\\psi\\)，整体训练过程复杂且资源消耗大。 高采样成本：LLM 生成文本计算量大，强化学习过程中的大量在线采样进一步推高了计算开销；采样不足可能导致错误的优化方向。 训练不稳定与超参数敏感：PPO 涉及众多超参数（如学习率、采样量等），调参复杂且训练过程易受不稳定因素影响。 对齐税效应：在提高模型对齐性的同时，可能会降低模型在其他任务上的表现。 DPO 简介 Fig. 2. DPO optimizes for human preferences while avoiding reinforcement learning.（Image source: Rafailov et al., 2023）\n直接偏好优化(Direct Preference Optimization, DPO) (Rafailov et al., 2023)为了解决 RLHF 的上述问题，其核心思路是将 RLHF 的目标转化为类似于监督微调的对比学习任务，从而实现：\n省略奖励模型训练：直接利用人类偏好数据优化 Actor 模型 \\(\\pi_\\theta\\)，无须单独训练 \\(r_\\phi\\)。 消除强化学习采样：采用对比损失函数替代 PPO，降低采样和计算资源消耗。 提升训练稳定性：基于监督学习的方法对超参数不敏感，训练过程更加平稳。 虽然 DPO 在 LLM 性能提升的上限上可能不及 RLHF，但在资源利用、实现复杂度和训练稳定性方面具有优势。\n方法对比 方法 训练步骤 模型 训练方式 优点 缺点 RLHF 先训练奖励模型，再使用 PPO 优化策略 \\(\\pi_\\theta\\)、\\(\\pi_{\\mathrm{ref}}\\)、\\(r_\\phi\\)、\\(V_\\psi\\) 强化学习和在线采样 充分利用人类偏好，上限潜力较高 资源消耗大、训练不稳定、超参数敏感 DPO 直接利用偏好数据训练 Actor 模型 \\(\\pi_\\theta\\)、\\(\\pi_{\\mathrm{ref}}\\) 类似 SFT 监督学习 流程简化、训练稳定、资源消耗低 性能提升上限可能低于 RLHF DPO 数学推导 RLHF 目标与最优策略分布 在大规模语言模型对齐中，我们希望利用人类反馈强化学习（RLHF）来优化模型输出。设输入 \\( x \\) 来自数据集 \\(\\mathcal{D}\\)，模型生成回答 \\( y \\)；待训练的 模型记为 \\(\\pi_\\theta(y \\mid x)\\)，而参考模型记为 \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\)（通常为SFT模型），同时引入奖励函数 \\( r(x,y) \\) 衡量回答质量。RLHF 的目标可写为\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\Big[ r(x,y) \\Big] \\;-\\; \\beta\\, \\mathbb{D}_{\\mathrm{KL}}\\Big[ \\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x) \\Big], \\tag{1} \\]其中 \\(\\beta\\) 为调节奖励与参考模型偏差的超参数。利用 KL 散度的定义\n\\[ \\mathbb{D}_{\\mathrm{KL}} \\Big[\\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\Big] = \\mathbb{E}_{y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right], \\tag{2} \\]式 (1) 可重写为\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ r(x,y) - \\beta \\, \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right]. \\tag{3} \\]将 (3) 式转换为最小化问题并除以 \\(\\beta\\) 得\n\\[ \\min_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} - \\frac{1}{\\beta} r(x,y) \\right]. \\tag{4} \\]假设存在一个最优策略分布 \\(\\pi^*(y \\mid x)\\) 使 (4) 式全局最优，则令\n\\[ \\pi^*(y \\mid x) \\;=\\; \\frac{1}{Z(x)} \\,\\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big), \\tag{5} \\]其中配分函数 \\( Z(x) \\) 定义为\n\\[ Z(x) = \\sum_{y}\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big). \\tag{6} \\] \\(Z(x)\\) 对所有可能的 \\(y\\) 求和，实现归一化，使得 \\(\\pi^*(y \\mid x)\\) 构成合法概率分布。 \\(Z(x)\\) 是 \\(x\\) 的函数，与待优化的 Actor 模型 \\(\\pi_\\theta\\) 无关。 对 (5) 式取对数得到\n\\[ \\log \\pi^*(y \\mid x) = \\log \\pi_{\\mathrm{ref}}(y \\mid x) + \\frac{1}{\\beta}\\, r(x,y) - \\log Z(x), \\tag{7} \\]从而解得\n\\[ r(x,y) = \\beta \\left[\\log \\frac{\\pi^*(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{8} \\]Bradley–Terry 模型 为了利用成对偏好数据 \\((x, y_w, y_l)\\) 训练模型，我们希望在相同输入 \\( x \\) 下，模型输出更偏好于高质量回答 \\( y_w \\) 而不是低质量回答 \\( y_l \\)。\nBradley–Terry 模型用于预测成对比较的结果。对于任意两个对象 \\( i \\) 和 \\( j \\)，若为每个对象分配正实数得分 \\( p_i \\) 和 \\( p_j \\)，则对象 \\( i \\) 被认为比对象 \\( j \\) 强的概率为\n\\[ \\Pr(i \u003e j) = \\frac{p_i}{p_i + p_j}. \\tag{9} \\]在我们的场景中，将每个回答 \\( y \\) 的强度参数设为 \\( p_{y} = \\exp\\big(r(x,y)\\big) \\)（保证为正实数）。因此，给定输入 \\( x \\) 下，回答 \\( y_w \\) 好于 \\( y_l \\) 的概率为\n\\[ P\\left(y_w \\succ y_l \\mid x\\right)=\\frac{\\exp \\big[r(x,y_w)\\big]}{\\exp \\big[r(x,y_w)\\big]+\\exp \\big[r(x,y_l)\\big]}. \\tag{10} \\]为了使得数据集中每个成对偏好数据 \\((x, y_w, y_l)\\) 中，高质量回答 \\( y_w \\) 的胜出概率尽可能大，我们将奖励模型训练目标设计为最大化 \\( y_w \\) 被偏好的概率，或等价地最小化负对数似然损失：\n\\[ L_{R}\\left(r_{\\phi}, D\\right) = -\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\left[\\log P\\left(y_w \\succ y_l \\mid x\\right)\\right], \\tag{11} \\] 其中数据集定义为 \\[ D=\\{(x^i, y_w^i, y_l^i)\\}_{i=1}^{N}. \\tag{12} \\]利用公式 (10)、(11) 以及下面的恒等式\n\\[ \\log \\frac{e^a}{e^a+e^b} = \\log\\frac{1}{1+e^{b-a}} = \\log \\sigma(a-b), \\tag{13} \\]其中 Sigmoid 函数定义为\n\\[ \\sigma(z)=\\frac{1}{1+e^{-z}}, \\tag{14} \\]可得\n\\[ \\log P\\left(y_w \\succ y_l \\mid x\\right) = \\log \\sigma\\Big(r(x,y_w)-r(x,y_l)\\Big). \\tag{15} \\]直接偏好优化 注意到 (8) 式中，奖励 \\( r(x,y) \\) 与最优策略的对数比有关。为避免显式训练一个单独的奖励模型 \\(r_\\phi\\)，我们采用 DPO的思想，即直接用待训练 Actor 模型 \\(\\pi_\\theta\\) 替换最优策略 \\(\\pi^*\\) 的位置，将 (8) 式中的奖励表示为\n\\[ r_\\theta(x,y) \\;=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{16} \\]在成对比较中，对于相同输入 \\( x \\)，两个回答 \\( y_w \\) 和 \\( y_l \\) 均包含相同的 \\(\\log Z(x)\\) 项，因此在计算奖励差值时，该项会被消去，即\n\\[ \\begin{aligned} r_\\theta(x,y_w)-r_\\theta(x,y_l) \u0026=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} + \\log Z(x)\\right] - \\beta \\left[\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} + \\log Z(x)\\right] \\\\ \u0026=\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)}. \\end{aligned} \\tag{17} \\]将 (17) 式代入 (15) 式，并结合 (11) 式，我们最终得到 DPO 的损失函数\n\\[ \\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}}) = - \\mathbb{E}_{(x,y_w,y_l) \\sim D} \\left[ \\log \\sigma\\Big( \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} -\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big) \\right]. \\tag{18} \\]该损失函数针对待训练 Actor 模型 \\(\\pi_\\theta\\) 而设，通过比较其在高质量回答 \\(y_w\\) 与低质量回答 \\(y_l\\) 上、相对于参考模型 \\(\\pi_{\\mathrm{ref}}\\) 的对数概率比来区分好坏回答：当 \\(\\pi_\\theta\\) 在 \\(y_w\\) 上的相对概率比远大于 \\(y_l\\) 时，Sigmoid 输出更接近 1，损失更小；反之，若 \\(\\pi_\\theta\\) 对 \\(y_l\\) 的相对概率过大，则损失增加。参数 \\(\\beta\\) 用于放大或缩小这些对数比的差值，从而调节模型对好坏答案的区分强度。\n训练流程 数据准备 偏好数据采集\n收集大量偏好数据，每个样本包含输入 \\( x \\) 及一对回答 \\((y_w, y_l)\\)，其中 \\( y_w \\) 为被偏好回答(chosen)，\\( y_l \\) 为质量较差的回答(rejected)。\n数据集例子: dpo_en_demo.json\n{ \"conversations\": [ { \"from\": \"human\", \"value\": \"what does abstraction mean on oop\" } ], \"chosen\": { \"from\": \"gpt\", \"value\": \"Abstraction in Object-Oriented Programming (OOP) is the process of exposing only the necessary information to the outside world while hiding the implementation details. It's a way to simplify complex systems by focusing only on the essential aspects and encapsulating the implementation details.\\n\\nIn other words, abstraction is about hiding the internal workings of an object or system and only exposing the necessary interfaces or APIs to interact with it. This helps to reduce complexity, improve modularity, and make the system easier to understand and maintain.\\n\\nFor example, consider a car. A car has many components like engine, transmission, brakes, etc. However, when you drive a car, you don't need to know how the engine works or how the transmission changes gears. All you need to know is how to operate the car's controls like accelerator, brakes, and steering wheel. The car's controls are an abstraction of the underlying complex systems, allowing you to interact with the car without needing to know the details of how it works.\\n\\nIn programming, abstraction is achieved by defining interfaces or abstract classes that define the essential functions or behaviors of an object without specifying how they are implemented. This allows developers to create different implementations of the same interface or abstract class, which can be swapped out without affecting the rest of the system.\\n\\nAbstraction is a fundamental concept in OOP and is used to create modular, maintainable, and flexible software systems. It helps to reduce code duplication, improve code reuse, and make software systems more understandable and easier to evolve over time.\" }, \"rejected\": { \"from\": \"gpt\", \"value\": \"Abstraction is a principle in object-oriented programming that refers to the process of focusing on the essential features of an object or concept without emphasizing the details that are not important for its functionality. It enables programmers to create classes that represent the objects they want to work with, and specifies only the behavior of those objects, leaving out unnecessary details. Abstraction helps to make the code more maintainable, modular, and scalable. It also improves the productivity of developers by reducing the amount of code they need to write.\" } } 输出预计算\n为降低计算开销，预先计算并缓存参考模型 \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\) 的输出。\n模型训练 训练目标\n通过最小化 DPO 损失 \\(\\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}})\\) 直接优化 Actor 模型 \\(\\pi_\\theta\\)，使其生成的回答更符合人类偏好。\n训练步骤\n从数据集中采样一批 \\((x, y_w, y_l)\\) 数据。\n计算 Actor 模型 \\(\\pi_\\theta(y \\mid x)\\) 的输出概率。\n利用下式计算损失：\n\\[ \\mathcal{L}_{\\mathrm{DPO}} = - \\log \\sigma\\Big( \\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big). \\] 通过反向传播更新 Actor 模型参数 \\(\\theta\\)。\n模型推理 训练完成后，得到的 Actor 模型 \\(\\pi_\\theta\\) 可直接用于推理。给定输入 \\( x \\) 后，模型基于学到的概率分布生成回答。由于训练过程中参考了人类偏好，同时受到参考模型 \\(\\pi_{\\mathrm{ref}}\\) 的约束，生成的回答既符合预期，又能保持生成文本的稳定性。\n总结 DPO 将 RLHF 过程简化为直接的监督学习任务，不仅节省了资源、提高了训练稳定性，同时降低了实现复杂度，是 LLM 对齐训练的一种高效替代方法。在实际应用中，我们可以根据业务场景选择 RLHF 或 DPO 方法，以达到最佳的训练效果。\n参考文献 [1] Christiano, Paul F., et al. “Deep reinforcement learning from human preferences.” Advances in neural information processing systems 30 (2017).\n[2] Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” Advances in neural information processing systems 35 (2022): 27730-27744.\n[3] Rafailov, Rafael, et al. “Direct preference optimization: Your language model is secretly a reward model.” Advances in Neural Information Processing Systems 36 (2024).\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Feb 2025). 大语言模型对齐: 直接偏好优化(DPO). https://syhya.github.io/posts/2025-02-08-dpo\nOr\n@article{syhya2025dpo, title = \"大语言模型对齐: 直接偏好优化(DPO)\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Feb\", url = \"https://syhya.github.io/posts/2025-02-08-dpo\" } ","wordCount":"4127","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2025-02-08T12:00:00+08:00","dateModified":"2025-09-01T18:02:26+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2025-02-08-dpo/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">大语言模型对齐: 直接偏好优化(DPO)</h1><div class=post-meta><span title='2025-02-08 12:00:00 +0800 +0800'>2025-02-08</span>&nbsp;·&nbsp;9 分钟&nbsp;·&nbsp;4127 字&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;翻译:<ul class=i18n_list><li><a href=https://syhya.github.io/posts/2025-02-08-dpo/>En</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#符号>符号</a></li><li><a href=#从-rlhf-到-dpo>从 RLHF 到 DPO</a><ul><li><a href=#rlhf>RLHF</a></li><li><a href=#rlhf-的局限性>RLHF 的局限性</a></li><li><a href=#dpo-简介>DPO 简介</a></li><li><a href=#方法对比>方法对比</a></li></ul></li><li><a href=#dpo-数学推导>DPO 数学推导</a><ul><li><a href=#rlhf-目标与最优策略分布>RLHF 目标与最优策略分布</a></li><li><a href=#bradleyterry-模型>Bradley–Terry 模型</a></li><li><a href=#直接偏好优化>直接偏好优化</a></li></ul></li><li><a href=#训练流程>训练流程</a><ul><li><a href=#数据准备>数据准备</a></li><li><a href=#模型训练>模型训练</a></li><li><a href=#模型推理>模型推理</a></li></ul></li><li><a href=#总结>总结</a></li><li><a href=#参考文献>参考文献</a></li><li><a href=#引用>引用</a></li></ul></nav></div></details></div><div class=post-content><p>这篇博客主要介绍一种比 RLHF 更精简的替代算法 DPO。与 RLHF 一样，DPO 目的是使模型输出与人类偏好保持一致，但它在实现上更加简单，并且对资源的需求更低。在项目资源受限的情况下，DPO 是一个实用解决方案。</p><h2 id=符号>符号<a hidden class=anchor aria-hidden=true href=#符号>#</a></h2><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td>\( x \)</td><td>用户输入（Prompt），即模型需要回答的问题</td></tr><tr><td>\( y \)</td><td>模型生成的回答（Response / Completion），即模型输出的文本</td></tr><tr><td>\( \pi_\theta(y \mid x) \)</td><td>Actor 模型：待训练策略，用于生成回答 \(y\)；参数为 \(\theta\)</td></tr><tr><td>\( \pi_{\mathrm{ref}}(y \mid x) \)</td><td>参考模型：冻结的 SFT 模型，作为对齐基准</td></tr><tr><td>\( r_\phi(x,y) \)</td><td>奖励模型：用于评估回答 \(y\) 质量的奖励函数；参数为 \(\phi\)</td></tr><tr><td>\( V_\psi(x) \)</td><td>critic 模型：用于估计给定输入 \(x\) 下未来累计奖励的值函数；参数为 \(\psi\)</td></tr><tr><td>\( \pi^*(y \mid x) \)</td><td>最优策略分布，通过参考模型与奖励函数确定</td></tr><tr><td>\( r_\theta(x,y) \)</td><td>基于 Actor 模型导出的奖励函数，通过 \(\pi_\theta\) 与 \(\pi_{\mathrm{ref}}\) 构造</td></tr><tr><td>\(\beta\)</td><td>超参数，控制 KL 惩罚项或对数比差异项的权重</td></tr><tr><td>\(\mathbb{D}_{\mathrm{KL}}[P \| Q]\)</td><td>KL 散度，衡量概率分布 \(P\) 与 \(Q\) 之间的差异</td></tr><tr><td>\(\sigma(z)\)</td><td>Sigmoid 函数，定义为：\(\sigma(z)=\frac{1}{1+e^{-z}}\)</td></tr><tr><td>\(\log\)</td><td>对数函数</td></tr><tr><td>\(\mathbb{E}\)</td><td>期望算子，用于求随机变量的平均值</td></tr><tr><td>\( (y_w, y_l) \)</td><td>一对偏好数据，其中 \( y_w \) 表示被偏好（质量更好）的回答，\( y_l \) 表示质量较差的回答</td></tr><tr><td>\( P\left(y_w \succ y_l \mid x\right) \)</td><td>在输入 \(x\) 下，回答 \( y_w \) 优于 \( y_l \) 的概率</td></tr><tr><td>\( Z(x) \)</td><td>配分函数，对所有回答 \(y\) 归一化概率分布</td></tr><tr><td>\( \mathcal{L}_{\mathrm{DPO}} \)</td><td>DPO 的损失函数</td></tr></tbody></table><h2 id=从-rlhf-到-dpo>从 RLHF 到 DPO<a hidden class=anchor aria-hidden=true href=#从-rlhf-到-dpo>#</a></h2><h3 id=rlhf>RLHF<a hidden class=anchor aria-hidden=true href=#rlhf>#</a></h3><p>OpenAI 主要利用<strong>人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)</strong>(<a href=https://arxiv.org/abs/1706.03741>Christiano et al., 2017</a>) 来训练 InstructGPT(<a href=https://arxiv.org/abs/2203.02155>Ouyang et al., 2022</a>)，而其构成了大语言模型（如 ChatGPT, Llama 等）的基础。整个训练过程通常包括以下三个主要步骤：</p><figure class=align-center><img loading=lazy src=InstructGPT.png#center alt="Fig. 1. A diagram illustrating the three steps of InstructGPT. (Image source: Ouyang et al., 2022)" width=100%><figcaption><p>Fig. 1. A diagram illustrating the three steps of InstructGPT. (Image source: <a href=https://arxiv.org/abs/2203.02155>Ouyang et al., 2022</a>)</p></figcaption></figure><ol><li><p><strong>监督微调（SFT）</strong><br>利用大量人工示例数据对预训练模型进行微调，得到一个初步能理解指令并生成合理回答的模型，即参考模型 \( \pi_{\mathrm{ref}}(y \mid x) \) 。</p></li><li><p><strong>奖励模型训练</strong><br>这里我们简化只考虑生成两个不同的结果，实际可以生成多个结果进行排序。针对同一输入 \(x\) 生成两个回答 \(y_w\)（较优）和 \(y_l\)（较劣），由人工排序后收集偏好数据。基于这些数据训练奖励模型 \(r_\phi(x, y)\)，使其能预测哪种回答更符合人类偏好。</p></li><li><p><strong>基于 PPO 的强化学习</strong><br>利用奖励模型 \(r_\phi\) 提供的反馈，通过 PPO 算法优化 Actor 模型 \(\pi_\theta\) 以提升回答质量。为防止模型偏离 \(\pi_{\mathrm{ref}}\)，在优化过程中引入 KL 惩罚项。该阶段通常涉及以下 4 个模型：</p><ul><li>\(\pi_\theta\)：经过 SFT 后待更新的模型。</li><li>\(\pi_{\mathrm{ref}}\)：冻结的 SFT 模型，作为对齐基准。</li><li>\(r_\phi\)：用于评估回答质量，参数固定。</li><li>\(V_\psi\)：用于估计未来奖励，辅助 Actor 模型更新。</li></ul></li></ol><h3 id=rlhf-的局限性>RLHF 的局限性<a hidden class=anchor aria-hidden=true href=#rlhf-的局限性>#</a></h3><p>尽管 RLHF 能充分利用人类偏好信息提升模型对齐效果，但其固有局限性包括：</p><ul><li><strong>多模型训练</strong>：除 Actor 模型 \(\pi_\theta\) 外，还需额外训练奖励模型 \(r_\phi\) 和 Critic 模型 \(V_\psi\)，整体训练过程复杂且资源消耗大。</li><li><strong>高采样成本</strong>：LLM 生成文本计算量大，强化学习过程中的大量在线采样进一步推高了计算开销；采样不足可能导致错误的优化方向。</li><li><strong>训练不稳定与超参数敏感</strong>：PPO 涉及众多超参数（如学习率、采样量等），调参复杂且训练过程易受不稳定因素影响。</li><li><strong>对齐税效应</strong>：在提高模型对齐性的同时，可能会降低模型在其他任务上的表现。</li></ul><h3 id=dpo-简介>DPO 简介<a hidden class=anchor aria-hidden=true href=#dpo-简介>#</a></h3><figure class=align-center><img loading=lazy src=rlhf_dpo.png#center alt="Fig. 2. DPO optimizes for human preferences while avoiding reinforcement learning.（Image source: Rafailov et al., 2023）" width=100%><figcaption><p>Fig. 2. DPO optimizes for human preferences while avoiding reinforcement learning.（Image source: <a href=https://arxiv.org/abs/2305.18290>Rafailov et al., 2023</a>）</p></figcaption></figure><p><strong>直接偏好优化(Direct Preference Optimization, DPO)</strong> (<a href=https://arxiv.org/abs/2305.18290>Rafailov et al., 2023</a>)为了解决 RLHF 的上述问题，其核心思路是将 RLHF 的目标转化为类似于监督微调的对比学习任务，从而实现：</p><ul><li><strong>省略奖励模型训练</strong>：直接利用人类偏好数据优化 Actor 模型 \(\pi_\theta\)，无须单独训练 \(r_\phi\)。</li><li><strong>消除强化学习采样</strong>：采用对比损失函数替代 PPO，降低采样和计算资源消耗。</li><li><strong>提升训练稳定性</strong>：基于监督学习的方法对超参数不敏感，训练过程更加平稳。</li></ul><p>虽然 DPO 在 LLM 性能提升的上限上可能不及 RLHF，但在资源利用、实现复杂度和训练稳定性方面具有优势。</p><h3 id=方法对比>方法对比<a hidden class=anchor aria-hidden=true href=#方法对比>#</a></h3><table><thead><tr><th>方法</th><th>训练步骤</th><th>模型</th><th>训练方式</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>RLHF</strong></td><td>先训练奖励模型，再使用 PPO 优化策略</td><td>\(\pi_\theta\)、\(\pi_{\mathrm{ref}}\)、\(r_\phi\)、\(V_\psi\)</td><td>强化学习和在线采样</td><td>充分利用人类偏好，上限潜力较高</td><td>资源消耗大、训练不稳定、超参数敏感</td></tr><tr><td><strong>DPO</strong></td><td>直接利用偏好数据训练 Actor 模型</td><td>\(\pi_\theta\)、\(\pi_{\mathrm{ref}}\)</td><td>类似 SFT 监督学习</td><td>流程简化、训练稳定、资源消耗低</td><td>性能提升上限可能低于 RLHF</td></tr></tbody></table><h2 id=dpo-数学推导>DPO 数学推导<a hidden class=anchor aria-hidden=true href=#dpo-数学推导>#</a></h2><h3 id=rlhf-目标与最优策略分布>RLHF 目标与最优策略分布<a hidden class=anchor aria-hidden=true href=#rlhf-目标与最优策略分布>#</a></h3><p>在大规模语言模型对齐中，我们希望利用人类反馈强化学习（RLHF）来优化模型输出。设输入 \( x \) 来自数据集 \(\mathcal{D}\)，模型生成回答 \( y \)；待训练的 模型记为 \(\pi_\theta(y \mid x)\)，而参考模型记为 \(\pi_{\mathrm{ref}}(y \mid x)\)（通常为SFT模型），同时引入奖励函数 \( r(x,y) \) 衡量回答质量。RLHF 的目标可写为</p>\[
\max_{\pi} \; \mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi(y \mid x)} \Big[ r(x,y) \Big] \;-\; \beta\, \mathbb{D}_{\mathrm{KL}}\Big[ \pi(y \mid x) \,\|\, \pi_{\mathrm{ref}}(y \mid x) \Big],
\tag{1}
\]<p>其中 \(\beta\) 为调节奖励与参考模型偏差的超参数。利用 KL 散度的定义</p>\[
\mathbb{D}_{\mathrm{KL}} \Big[\pi(y \mid x) \,\|\, \pi_{\mathrm{ref}}(y \mid x)\Big] = \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \right],
\tag{2}
\]<p>式 (1) 可重写为</p>\[
\max_{\pi} \; \mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi(y \mid x)} \left[ r(x,y) - \beta \, \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \right].
\tag{3}
\]<p>将 (3) 式转换为最小化问题并除以 \(\beta\) 得</p>\[
\min_{\pi} \; \mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi(y \mid x)} \left[ \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} - \frac{1}{\beta} r(x,y) \right].
\tag{4}
\]<p>假设存在一个最优策略分布 \(\pi^*(y \mid x)\) 使 (4) 式全局最优，则令</p>\[
\pi^*(y \mid x) \;=\; \frac{1}{Z(x)} \,\pi_{\mathrm{ref}}(y \mid x)\, \exp\!\Big(\frac{1}{\beta} \, r(x,y)\Big),
\tag{5}
\]<p>其中配分函数 \( Z(x) \) 定义为</p>\[
Z(x) = \sum_{y}\, \pi_{\mathrm{ref}}(y \mid x)\, \exp\!\Big(\frac{1}{\beta} \, r(x,y)\Big).
\tag{6}
\]<ul><li>\(Z(x)\) 对所有可能的 \(y\) 求和，实现归一化，使得 \(\pi^*(y \mid x)\) 构成合法概率分布。</li><li>\(Z(x)\) 是 \(x\) 的函数，与待优化的 Actor 模型 \(\pi_\theta\) 无关。</li></ul><p>对 (5) 式取对数得到</p>\[
\log \pi^*(y \mid x) = \log \pi_{\mathrm{ref}}(y \mid x) + \frac{1}{\beta}\, r(x,y) - \log Z(x),
\tag{7}
\]<p>从而解得</p>\[
r(x,y) = \beta \left[\log \frac{\pi^*(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} + \log Z(x)\right].
\tag{8}
\]<h3 id=bradleyterry-模型>Bradley–Terry 模型<a hidden class=anchor aria-hidden=true href=#bradleyterry-模型>#</a></h3><p>为了利用成对偏好数据 \((x, y_w, y_l)\) 训练模型，我们希望在相同输入 \( x \) 下，模型输出更偏好于高质量回答 \( y_w \) 而不是低质量回答 \( y_l \)。</p><p><a href=https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model>Bradley–Terry 模型</a>用于预测成对比较的结果。对于任意两个对象 \( i \) 和 \( j \)，若为每个对象分配正实数得分 \( p_i \) 和 \( p_j \)，则对象 \( i \) 被认为比对象 \( j \) 强的概率为</p>\[
\Pr(i > j) = \frac{p_i}{p_i + p_j}.
\tag{9}
\]<p>在我们的场景中，将每个回答 \( y \) 的强度参数设为 \( p_{y} = \exp\big(r(x,y)\big) \)（保证为正实数）。因此，给定输入 \( x \) 下，回答 \( y_w \) 好于 \( y_l \) 的概率为</p>\[
P\left(y_w \succ y_l \mid x\right)=\frac{\exp \big[r(x,y_w)\big]}{\exp \big[r(x,y_w)\big]+\exp \big[r(x,y_l)\big]}.
\tag{10}
\]<p>为了使得数据集中每个成对偏好数据 \((x, y_w, y_l)\) 中，高质量回答 \( y_w \) 的胜出概率尽可能大，我们将奖励模型训练目标设计为最大化 \( y_w \) 被偏好的概率，或等价地最小化负对数似然损失：</p>\[
L_{R}\left(r_{\phi}, D\right) = -\mathbb{E}_{(x,y_w,y_l) \sim D}\left[\log P\left(y_w \succ y_l \mid x\right)\right],
\tag{11}
\]<p>其中数据集定义为</p>\[
D=\{(x^i, y_w^i, y_l^i)\}_{i=1}^{N}.
\tag{12}
\]<p>利用公式 (10)、(11) 以及下面的恒等式</p>\[
\log \frac{e^a}{e^a+e^b} = \log\frac{1}{1+e^{b-a}} = \log \sigma(a-b),
\tag{13}
\]<p>其中 Sigmoid 函数定义为</p>\[
\sigma(z)=\frac{1}{1+e^{-z}},
\tag{14}
\]<p>可得</p>\[
\log P\left(y_w \succ y_l \mid x\right) = \log \sigma\Big(r(x,y_w)-r(x,y_l)\Big).
\tag{15}
\]<h3 id=直接偏好优化>直接偏好优化<a hidden class=anchor aria-hidden=true href=#直接偏好优化>#</a></h3><p>注意到 (8) 式中，奖励 \( r(x,y) \) 与最优策略的对数比有关。为避免显式训练一个单独的奖励模型 \(r_\phi\)，我们采用 DPO的思想，即<strong>直接用待训练 Actor 模型 \(\pi_\theta\) 替换最优策略 \(\pi^*\) 的位置</strong>，将 (8) 式中的奖励表示为</p>\[
r_\theta(x,y) \;=\; \beta \left[\log \frac{\pi_\theta(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} + \log Z(x)\right].
\tag{16}
\]<p>在成对比较中，对于相同输入 \( x \)，两个回答 \( y_w \) 和 \( y_l \) 均包含相同的 \(\log Z(x)\) 项，因此在计算奖励差值时，该项会被消去，即</p>\[
\begin{aligned}
r_\theta(x,y_w)-r_\theta(x,y_l)
&=\; \beta \left[\log \frac{\pi_\theta(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)} + \log Z(x)\right] - \beta \left[\log \frac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)} + \log Z(x)\right] \\
&=\; \beta \,\log \frac{\pi_\theta(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)} - \beta \,\log \frac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)}.
\end{aligned}
\tag{17}
\]<p>将 (17) 式代入 (15) 式，并结合 (11) 式，我们最终得到 DPO 的损失函数</p>\[
\mathcal{L}_{\mathrm{DPO}}(\pi_\theta; \pi_{\mathrm{ref}})
= - \mathbb{E}_{(x,y_w,y_l) \sim D} \left[ \log \sigma\Big(
\beta \,\log \frac{\pi_\theta(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)}
-\; \beta \,\log \frac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)}
\Big) \right].
\tag{18}
\]<p>该损失函数针对待训练 Actor 模型 \(\pi_\theta\) 而设，通过比较其在高质量回答 \(y_w\) 与低质量回答 \(y_l\) 上、相对于参考模型 \(\pi_{\mathrm{ref}}\) 的对数概率比来区分好坏回答：当 \(\pi_\theta\) 在 \(y_w\) 上的相对概率比远大于 \(y_l\) 时，Sigmoid 输出更接近 1，损失更小；反之，若 \(\pi_\theta\) 对 \(y_l\) 的相对概率过大，则损失增加。参数 \(\beta\) 用于放大或缩小这些对数比的差值，从而调节模型对好坏答案的区分强度。</p><h2 id=训练流程>训练流程<a hidden class=anchor aria-hidden=true href=#训练流程>#</a></h2><h3 id=数据准备>数据准备<a hidden class=anchor aria-hidden=true href=#数据准备>#</a></h3><ul><li><p><strong>偏好数据采集</strong><br>收集大量偏好数据，每个样本包含输入 \( x \) 及一对回答 \((y_w, y_l)\)，其中 \( y_w \) 为被偏好回答(chosen)，\( y_l \) 为质量较差的回答(rejected)。</p><p><strong>数据集例子</strong>: <a href=https://github.com/hiyouga/LLaMA-Factory/blob/main/data/dpo_en_demo.json>dpo_en_demo.json</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl> <span class=nt>&#34;conversations&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>   <span class=p>{</span>
</span></span><span class=line><span class=cl>     <span class=nt>&#34;from&#34;</span><span class=p>:</span> <span class=s2>&#34;human&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>     <span class=nt>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;what does abstraction mean on oop&#34;</span>
</span></span><span class=line><span class=cl>   <span class=p>}</span>
</span></span><span class=line><span class=cl> <span class=p>],</span>
</span></span><span class=line><span class=cl> <span class=nt>&#34;chosen&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;from&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;Abstraction in Object-Oriented Programming (OOP) is the process of exposing only the necessary information to the outside world while hiding the implementation details. It&#39;s a way to simplify complex systems by focusing only on the essential aspects and encapsulating the implementation details.\n\nIn other words, abstraction is about hiding the internal workings of an object or system and only exposing the necessary interfaces or APIs to interact with it. This helps to reduce complexity, improve modularity, and make the system easier to understand and maintain.\n\nFor example, consider a car. A car has many components like engine, transmission, brakes, etc. However, when you drive a car, you don&#39;t need to know how the engine works or how the transmission changes gears. All you need to know is how to operate the car&#39;s controls like accelerator, brakes, and steering wheel. The car&#39;s controls are an abstraction of the underlying complex systems, allowing you to interact with the car without needing to know the details of how it works.\n\nIn programming, abstraction is achieved by defining interfaces or abstract classes that define the essential functions or behaviors of an object without specifying how they are implemented. This allows developers to create different implementations of the same interface or abstract class, which can be swapped out without affecting the rest of the system.\n\nAbstraction is a fundamental concept in OOP and is used to create modular, maintainable, and flexible software systems. It helps to reduce code duplication, improve code reuse, and make software systems more understandable and easier to evolve over time.&#34;</span>
</span></span><span class=line><span class=cl> <span class=p>},</span>
</span></span><span class=line><span class=cl> <span class=nt>&#34;rejected&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;from&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=nt>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;Abstraction is a principle in object-oriented programming that refers to the process of focusing on the essential features of an object or concept without emphasizing the details that are not important for its functionality. It enables programmers to create classes that represent the objects they want to work with, and specifies only the behavior of those objects, leaving out unnecessary details. Abstraction helps to make the code more maintainable, modular, and scalable. It also improves the productivity of developers by reducing the amount of code they need to write.&#34;</span>
</span></span><span class=line><span class=cl> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></li><li><p><strong>输出预计算</strong><br>为降低计算开销，预先计算并缓存参考模型 \(\pi_{\mathrm{ref}}(y \mid x)\) 的输出。</p></li></ul><h3 id=模型训练>模型训练<a hidden class=anchor aria-hidden=true href=#模型训练>#</a></h3><ul><li><p><strong>训练目标</strong><br>通过最小化 DPO 损失 \(\mathcal{L}_{\mathrm{DPO}}(\pi_\theta; \pi_{\mathrm{ref}})\) 直接优化 Actor 模型 \(\pi_\theta\)，使其生成的回答更符合人类偏好。</p></li><li><p><strong>训练步骤</strong></p><ol><li><p>从数据集中采样一批 \((x, y_w, y_l)\) 数据。</p></li><li><p>计算 Actor 模型 \(\pi_\theta(y \mid x)\) 的输出概率。</p></li><li><p>利用下式计算损失：</p>\[
\mathcal{L}_{\mathrm{DPO}} = - \log \sigma\Big( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)} \Big).
\]</li><li><p>通过反向传播更新 Actor 模型参数 \(\theta\)。</p></li></ol></li></ul><h3 id=模型推理>模型推理<a hidden class=anchor aria-hidden=true href=#模型推理>#</a></h3><p>训练完成后，得到的 Actor 模型 \(\pi_\theta\) 可直接用于推理。给定输入 \( x \) 后，模型基于学到的概率分布生成回答。由于训练过程中参考了人类偏好，同时受到参考模型 \(\pi_{\mathrm{ref}}\) 的约束，生成的回答既符合预期，又能保持生成文本的稳定性。</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>DPO 将 RLHF 过程简化为直接的监督学习任务，不仅节省了资源、提高了训练稳定性，同时降低了实现复杂度，是 LLM 对齐训练的一种高效替代方法。在实际应用中，我们可以根据业务场景选择 RLHF 或 DPO 方法，以达到最佳的训练效果。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><p>[1] Christiano, Paul F., et al. <a href=https://arxiv.org/abs/1706.03741>&ldquo;Deep reinforcement learning from human preferences.&rdquo;</a> Advances in neural information processing systems 30 (2017).</p><p>[2] Ouyang, Long, et al. <a href=https://arxiv.org/abs/2203.02155>&ldquo;Training language models to follow instructions with human feedback.&rdquo;</a> Advances in neural information processing systems 35 (2022): 27730-27744.</p><p>[3] Rafailov, Rafael, et al. <a href=https://arxiv.org/abs/1706.03741>&ldquo;Direct preference optimization: Your language model is secretly a reward model.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2024).</p><h2 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h2><blockquote><p><strong>引用</strong>：转载或引用本文内容时，请注明原作者和来源。</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Feb 2025). 大语言模型对齐: 直接偏好优化(DPO).
<a href=https://syhya.github.io/posts/2025-02-08-dpo>https://syhya.github.io/posts/2025-02-08-dpo</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025dpo</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;大语言模型对齐: 直接偏好优化(DPO)&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Feb&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-02-08-dpo&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/zh/tags/nlp/>NLP</a></li><li><a href=https://syhya.github.io/zh/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/zh/tags/post-training/>Post-Training</a></li><li><a href=https://syhya.github.io/zh/tags/dpo/>DPO</a></li><li><a href=https://syhya.github.io/zh/tags/rlhf/>RLHF</a></li><li><a href=https://syhya.github.io/zh/tags/alignment/>Alignment</a></li><li><a href=https://syhya.github.io/zh/tags/bradleyterry-model/>Bradley–Terry Model</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/zh/posts/2025-02-15-deep-research-tech-report/><span class=title>« 上一页</span><br><span>OpenAI Deep Research 案例分享 - 深度研究技术综合研究报告</span>
</a><a class=next href=https://syhya.github.io/zh/posts/2025-02-03-rag/><span class=title>下一页 »</span><br><span>检索增强生成 (RAG) 技术综述（长期更新中）</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 大语言模型对齐: 直接偏好优化(DPO) on x" href="https://x.com/intent/tweet/?text=%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%af%b9%e9%bd%90%3a%20%e7%9b%b4%e6%8e%a5%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96%28DPO%29&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-08-dpo%2f&amp;hashtags=AI%2cNLP%2cLLM%2cPost-training%2cDPO%2cRLHF%2cAlignment%2cBradley%e2%80%93TerryModel"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 大语言模型对齐: 直接偏好优化(DPO) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-08-dpo%2f&amp;title=%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%af%b9%e9%bd%90%3a%20%e7%9b%b4%e6%8e%a5%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96%28DPO%29&amp;summary=%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%af%b9%e9%bd%90%3a%20%e7%9b%b4%e6%8e%a5%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96%28DPO%29&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-08-dpo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 大语言模型对齐: 直接偏好优化(DPO) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-08-dpo%2f&title=%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%af%b9%e9%bd%90%3a%20%e7%9b%b4%e6%8e%a5%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96%28DPO%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 大语言模型对齐: 直接偏好优化(DPO) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-08-dpo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 大语言模型对齐: 直接偏好优化(DPO) on whatsapp" href="https://api.whatsapp.com/send?text=%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%af%b9%e9%bd%90%3a%20%e7%9b%b4%e6%8e%a5%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96%28DPO%29%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-08-dpo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 大语言模型对齐: 直接偏好优化(DPO) on telegram" href="https://telegram.me/share/url?text=%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%af%b9%e9%bd%90%3a%20%e7%9b%b4%e6%8e%a5%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96%28DPO%29&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-08-dpo%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 大语言模型对齐: 直接偏好优化(DPO) on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e5%af%b9%e9%bd%90%3a%20%e7%9b%b4%e6%8e%a5%e5%81%8f%e5%a5%bd%e4%bc%98%e5%8c%96%28DPO%29&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-08-dpo%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>