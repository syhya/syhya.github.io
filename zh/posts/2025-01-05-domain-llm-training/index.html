<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>构建特定领域的大语言模型 | Yue Shui 博客</title><meta name=keywords content="AI,NLP,LLM,Pre-training,Post-training,DPO,领域模型,DeepSpeed"><meta name=description content="背景
随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-01-05-domain-llm-training/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="构建特定领域的大语言模型"><meta property="og:description" content="背景 随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-05T12:00:00+08:00"><meta property="article:modified_time" content="2025-07-03T09:46:11+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Pre-Training"><meta property="article:tag" content="Post-Training"><meta property="article:tag" content="DPO"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="构建特定领域的大语言模型"><meta name=twitter:description content="背景
随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"构建特定领域的大语言模型","item":"https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"构建特定领域的大语言模型","name":"构建特定领域的大语言模型","description":"背景 随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。\n","keywords":["AI","NLP","LLM","Pre-training","Post-training","DPO","领域模型","DeepSpeed"],"articleBody":"背景 随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。\n本文基于作者的工作经验，总结了如何在现有通用模型的基础上，通过数据准备、模型训练、部署、评估及持续迭代，构建具备特定领域知识的大语言模型。\n为什么要向基座模型注入领域知识 挑战一：有限的领域知识 现有的预训练模型（如 GPT-4、Llama 3）主要基于通用语料库进行训练，缺乏对小众语言或专有领域的深入理解，导致模型在处理编程代码时表现不佳。\n挑战二：数据安全与合规 企业在处理敏感数据时，必须遵循严格的数据主权和合规性要求。将私有业务数据上传至第三方云服务存在安全隐患，因此需要在本地环境中完成数据处理与模型训练。\n挑战三：OpenAI微调的局限 主流商用 API 的微调方案通常较为基础，难以实现深层次的对齐和优化。对于需要高度定制化的领域模型，这种方法难以满足需求。\n注入知识两种方法 在实际项目中，常见的将领域知识注入基座模型的方法主要包括 微调 (Fine-Tuning) 和 检索增强生成 (RAG)。下文将详细对比这些方法，以帮助选择最适合的策略。\n方法对比 微调 (Fine-Tuning) 核心思路\n通过持续预训练、监督微调和偏好对齐，直接更新模型参数，使其掌握特定领域知识和任务模式。\n技术细节\n继续预训练 (CPT)：在大量领域特定的无监督数据上继续预训练基座模型。 监督微调 (SFT)：使用高质量的标注数据进行有监督微调。 偏好对齐 (DPO)：通过用户反馈优化模型输出。 参数微调方法：使用全参数微调或者结合 LoRA 等 PEFT 方法冻结部分参数并添加 adapter。 优势\n深度定制：模型内部权重更新，能够深入理解领域知识。 无需依赖外部检索：推理时不需额外的知识库支持，减少延迟和总的 token 消耗。 提升整体性能：在特定领域任务上表现显著优于通用模型。 劣势\n高计算成本：需要大量计算资源进行训练，尤其是 CPT 阶段。 训练周期长：从数据准备到模型训练再到优化，需要较长时间。 灾难性遗忘：模型可能在学习新知识的同时，遗忘原有的通用能力。 检索增强生成 (RAG) 核心思路\n构建领域知识库，在推理阶段检索相关文档，辅助模型生成更准确的回答，无需直接改变模型参数。\n技术细节\n数据处理：对领域文档进行预处理，按块大小和重叠量切分。 向量化：使用文本嵌入模型将文本块转换为向量，存储在向量数据库中。 召回：推理时通过相似度搜索召回相关文档，作为上下文信息或 few-shot 示例提供给基座模型。 优势\n保持通用能力：模型参数不变，仍保留通用语言能力。 快速更新：知识库可动态更新，无需重新训练模型。 计算效率：避免大规模训练，节省计算资源。 劣势\n依赖知识库质量：检索到的文档质量直接影响回答质量。 推理速度：检索过程可能增加推理延迟，并且需要更多的 token。 知识覆盖有限：模型内部知识仍受限于基座模型的预训练数据。 模型与训练资源 基座模型 以 Llama 3 系列 为例，其具有以下特点：\n参数规模\nLlama 3 系列涵盖从 1B 到 405B 参数的模型，广泛支持多语言处理、代码生成、推理，以及视觉和文本任务。小型模型（1B 和 3B）经过专门优化，适合边缘和移动设备，支持最大 128K 的上下文窗口，可高效处理本地任务，例如摘要生成、指令执行和文本重写。\n多模态能力\nLlama 3 的视觉模型（11B 和 90B 参数）在图像理解任务上的表现优于许多封闭模型，同时支持图像、视频和语音的多模态处理。所有模型均支持微调，便于针对特定领域进行定制化开发。\n开源与社区支持\nLlama 3 系列模型及其权重以开源形式发布，可通过 llama.com 和 Hugging Face 平台 获取，为开发者提供便捷的访问和应用支持。\n数据集限制\n虽然 Llama 3 模型本身以开源形式发布，但其训练所使用的数据集并未开源。因此，严格来说，Llama 3 并非完全开源的模型。这一限制可能会在解决灾难性遗忘问题时带来挑战，因为难以获得与原始训练完全一致的数据集。\n训练资源 训练大型语言模型需要强大的计算资源和高效的分布式训练框架。\n硬件资源\nGPU 集群：建议使用 NVIDIA A100 或 H100 GPU，4卡或8卡配置，通过 NVLink 或 InfiniBand 提升通信带宽。 存储资源：高性能 SSD（如 NVMe）以支持快速的数据读写。 软件框架\n并行框架：DeepSpeed、Megatron-LM 等分布式训练框架，支持大规模模型训练。 推理框架：vLLM、ollama 等，优化推理速度和资源利用。 并行策略\n数据并行：适用于单卡可容纳模型的情况，通过 DeepSpeed 的 ZeRO Stage 0 实现。 模型并行、流水线并行和张量并行：单卡无法容纳时，采用 ZeRO Stage 1、2、3 进行优化，或使用 ZeRO-Infinity 将参数和优化器状态部分卸载到 CPU 或 NVMe。 DeepSpeed ZeRO 分片策略对比 为了更好地理解 DeepSpeed 的 ZeRO 分片策略，以下将分为不同的部分进行详细说明。\nZeRO Stage 分片策略 ZeRO Stage 描述 显存占用 训练速度 ZeRO-0 纯数据并行，不进行任何分片。所有优化器状态、梯度和参数在每张 GPU 上完全复制。 最高 最快 ZeRO-1 分片优化器状态（例如动量和二阶矩），减少显存占用，但梯度和参数仍为数据并行。 高 略慢于 ZeRO-0 ZeRO-2 分片优化器状态和梯度，在 ZeRO-1 的基础上进一步减少显存占用。 中 慢于 ZeRO-1 ZeRO-3 分片优化器状态、梯度和模型参数，显存占用最低，适合大规模模型。但需要在前向和后向时进行参数广播（All-Gather/All-Reduce），通信量显著增加。 低 明显慢于 ZeRO-2，取决于模型大小和网络带宽 Offload 策略 Offload 类型 描述 显存占用 训练速度 ZeRO-1 + CPU Offload 在 ZeRO-1 的基础上，将优化器状态卸载到 CPU 内存；可进一步降低 GPU 显存占用，但需要 CPU-GPU 数据传输，依赖 PCIe 带宽，且占用 CPU 内存。 中偏低 慢于 ZeRO-1，受 CPU 性能和 PCIe 带宽影响 ZeRO-2 + CPU Offload 在 ZeRO-2 的基础上，将优化器状态卸载到 CPU 内存；对较大模型进一步降低 GPU 显存占用，但会增加 CPU-GPU 数据传输开销。 较低 慢于 ZeRO-2，受 CPU 性能和 PCIe 带宽影响 ZeRO-3 + CPU Offload 在 ZeRO-3 的基础上，将优化器状态和模型参数卸载到 CPU；GPU 显存占用最小，但 CPU-GPU 通信量极大，且 CPU 带宽远小于 GPU-GPU 通信。 极低 非常慢 ZeRO-Infinity (NVMe Offload) 基于 ZeRO-3，将优化器状态、梯度和参数卸载到 NVMe，突破 CPU 内存限制，适合超大规模模型；性能高度依赖 NVMe 并行读写速度。 极低需 NVMe 支持 慢于 ZeRO-3，但通常优于 ZeRO-3 + CPU Offload 通信量与性能影响 ZeRO-0/1/2\n通信以 梯度同步 为主，使用 All-Reduce 操作，通信量相对较低。 ZeRO-3\n需要对模型参数进行 All-Gather/All-Reduce 操作，通信量显著增大，网络带宽成为关键瓶颈，前后传播时的参数广播进一步加剧通信负担。 CPU Offload（ZeRO-1/2/3 + CPU）\n卸载优化器状态或参数到 CPU，减少 GPU 显存占用。 通信量主要来自 CPU \u003c-\u003e GPU 数据传输，带宽远低于 GPU-GPU 通信，极易造成性能瓶颈，尤其在 ZeRO-3 场景下。 NVMe Offload（ZeRO-Infinity）\n在 ZeRO-3 的基础上进一步卸载至 NVMe，突破 CPU 内存限制以支持超大规模模型。 性能强烈依赖 NVMe I/O 带宽 和并行度，若 NVMe 速度足够高，通常优于 CPU Offload；但在 I/O 性能较弱或高延迟场景下，效果可能不佳。 硬件与配置影响 硬件限制\nPCIe 带宽、网络带宽、NVMe I/O 等对 Offload 性能有显著影响，需根据硬件环境选择最佳策略。 补充说明\nCPU Offload 利用 CPU 内存并通过 PCIe 传输数据；NVMe Offload 则将状态保存于 NVMe 设备。 NVMe Offload 在 NVMe I/O 性能充足 时通常优于 CPU Offload，但需避免因 I/O 性能不足导致的性能瓶颈。 与官方文档对照\n建议结合 DeepSpeed 官方文档 获取最新、最准确的配置参数和性能调优建议。 数据准备：决定训练成败的核心 数据质量直接决定了模型的性能。数据准备包括数据收集、清洗、去重、分类与配比、脱敏等步骤。\n预训练数据 数据来源 公开数据集：如：the-stack-v2、Common Crawl 等。 企业自有数据：内部文档、代码库、业务日志等。 网络爬虫：通过爬虫技术采集领域相关的网页内容。 数据规模 建议使用至少数亿到数十亿个 token，以确保模型能够充分学习领域知识。 当数据量不足时，模型效果可能受限，建议采用数据增强的方法来补充数据。 数据处理 数据预处理\n格式统一：对来自多个数据源的无标注大量语料进行处理，确保其格式一致。推荐使用高效的存储格式，如 Parquet，以提高数据读取和处理的效率。 数据去重\n检测方法：使用 MinHash、SimHash 或余弦相似度等算法进行近似重复检测。 处理粒度：可选择按句子、段落或文档级别去重，根据任务需求灵活调整。 相似度阈值：设定合理的相似度阈值（如 0.9），删除重复度高于阈值的文本，确保数据多样性。 数据清洗\n文本过滤：结合规则和模型评分器（如 BERT/RoBERTa）去除乱码、拼写错误和低质量文本。 格式化处理：优先使用 JSON 格式处理数据，确保代码、Markdown 和 LaTeX 等特殊格式的准确性。 数据脱敏\n隐私保护：匿名化或去除人名、电话号码、邮箱、密码等敏感信息，确保数据合规。 不合规内容过滤：剔除含有违法、色情或种族歧视等内容的数据块。 数据混合与配比\n比例控制：例如，将 70% 的领域特定数据与 30% 的通用数据相结合，避免模型遗忘通用能力。 任务类型：确保数据包含代码生成、问答对话、文档摘要、多轮对话和数学推理等多种任务类型。 数据顺序\n逐步引导：采用课程学习（Curriculum Learning）方法，从简单、干净的数据开始训练，逐步引入更复杂或噪声较高的数据，优化模型的学习效率和收敛路径。 语义连贯性：利用上下文预训练（In-context Pretraining）技术，将语义相似的文档拼接在一起，增强上下文一致性，提升模型的语义理解深度与泛化能力。 监督微调数据 数据格式 可采用 Alpaca 或 Vicuna 风格，比如结构化为 [instruction, input, output] 的单轮和多轮对话。\n规模：几千条到几十万条，具体根据项目需求和计算资源决定。 质量：确保数据的高质量和多样性，避免模型学习到错误或偏见。 数据构建 在数据构建过程中，我们首先收集日常业务数据，并与业务专家共同构建基础问题。随后，利用大语言模型进行数据增强，以提升数据的多样性和鲁棒性。以下是具体的数据增强策略：\n数据增强策略 表达多样化\n通过大语言模型对现有数据进行改写，采用同义词替换和语法变换等方法，增加数据的多样性。\n鲁棒性增强\n构建包含拼写错误、混合语言等输入的提示（Prompt），以模拟真实场景，同时确保生成答案的高质量。\n知识蒸馏\n利用 GPT-4、Claude 等大型语言模型进行知识蒸馏，生成符合需求的问答数据对。\n复杂任务设计\n针对复杂场景（如多轮对话、逻辑推理等），手动设计高质量数据，以覆盖模型的能力边界。\n数据生成管道\n构建自动化数据生成流水线，将数据生成、筛选、格式化和校验等环节集成，提高整体效率。\n关键要点 任务类型标注：每条数据标注明确的任务类型，便于后续精细化分析和调优。 多轮对话与话题切换：构建多轮对话中上下文关联与话题转换的数据，确保模型能够学习话题切换与上下文关联的能力。 思维链（Chain of Thought）策略：分类、推理等任务可先用 COT 生成过程性答案，提高准确率。 数据飞轮：上线后持续收集用户真实问题，结合真实需求迭代数据；定期清洗，确保质量与多样性。 偏好数据 数据格式 三元组结构：[prompt, chosen answer, rejected answer] 标注细节： 多模型采样：使用多个不同训练阶段或不同数据配比的模型生成回答，增加数据多样性。 编辑与优化：标注人员可对选择的回答进行小幅修改，确保回答质量。 采样策略 多模型采样：部署多个不同版本的模型，对同一 prompt 生成不同回答。 对比标注：由人工或自动化系统对生成的回答进行对比，选择更优的回答对。 关键要点 数据多样性与覆盖：确保偏好数据涵盖各种场景和任务，避免模型在特定情境下表现不佳。 高质量标注：偏好数据的质量直接影响模型的对齐效果，需确保标注准确且一致。 训练流程 一个完整的特定领域大语言模型训练流程通常包括 继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO) 三个主要步骤，最终实现模型的部署与持续优化。\n三种方法对比 训练方法概览 训练方法 主要目标 数据需求 典型应用场景 继续预训练 (CPT) 继续在大规模无监督语料上进行预训练，注入新领域知识 大量无标签文本（至少数亿到数十亿 Token） 补足领域知识，如法律、医疗、金融等专业文本 监督微调 (SFT) 在有监督标注的数据上进行微调，强化特定任务和指令执行能力 定制化标注数据（指令/对话对等），从几千到几十万条 各类特定任务，如代码生成、问答、文本改写、复杂指令执行等 直接偏好对齐 (DPO) 利用偏好数据（正例 chosen vs. 负例 rejected）直接对齐人类偏好 偏好数据：[prompt, chosen, rejected](规模相对较小) 对齐人类反馈，如回答风格、合规性、安全性等 优势与挑战 继续预训练 (CPT) 优势:\n更好领域覆盖，全面提升模型在特定领域的理解和生成能力。 无需额外手动标注。 挑战/局限:\n需要大量高质量领域数据。 训练成本高，需大规模算力与时间。 可能引入领域偏见，需谨慎处理数据质量与分布。 监督微调 (SFT) 优势:\n快速获取可用的任务执行能力。 显著提升模型对特定场景的准确性。 挑战/局限:\n数据标注成本较高。 需谨慎选择标注数据以避免过拟合。 微调后可能削弱模型的通用性。 直接偏好对齐 (DPO) 优势:\n无需单独训练 Reward Model。 数据量需求较小，调优效率高。 挑战/局限:\n需要可靠的偏好标注。 对复杂、多样化场景仍需持续迭代收集更多偏好数据。 易受偏好数据分布的限制。 通用训练要点与技术细节 在进行 CPT、SFT、DPO 三种训练方法时，存在许多通用的训练要点和技术细节。以下部分将这些通用内容进行统一描述，以便于更好地理解和应用。\n数据处理与准备 数据质量：无论是 CPT、SFT 还是 DPO，数据的质量都是至关重要的。需要确保数据的准确性、无歧义性和多样性。 数据格式：统一的数据格式有助于简化训练流程。例如，使用 JSON 或其他结构化格式来存储训练数据。 数据增强：通过 LLM 重写和优化等方式增加数据多样性，提升模型的泛化能力。 学习率与优化 学习率设置：通常采用比预训练阶段更小的学习率，如从 3e-4 降低到 3e-5，具体数值视任务和数据量而定。 学习率调度：使用 warm-up 策略（如前 10% 步骤线性递增），随后采用线性衰减或余弦退火策略，确保训练过程平稳。 优化器选择：根据模型规模和硬件资源选择合适的优化器，比如 AdamW。 训练策略 全参数微调：在资源允许的情况下，优先进行全参数微调，以确保模型能够全面掌握新知识。 参数高效微调（PEFT）：如 LoRA，适用于计算资源有限的场景，通过冻结部分参数并添加 adapter 实现高效微调。 混合精度训练：在支持的 GPU 上使用 bf16 或 fp16，降低显存占用，提高训练速度。 训练稳定性：采用梯度裁剪、正则化、dropout、权重衰减等技术，防止梯度爆炸和模型过拟合。 Flash Attention：利用 Flash Attention 技术优化注意力机制的计算效率，提升训练速度和降低显存占用。 监控与调优 收敛监控：实时监控训练集和验证集的 loss 曲线，确保模型逐步收敛，必要时调整学习率和其他超参数。 Checkpoint：定期保留 Checkpoint，防止意外中断导致全部训练进度丢失。 早停机制：防止模型过拟合，适时停止训练，保存最佳模型状态。 模型评估：在训练过程中定期进行评估，确保模型性能符合预期。 继续预训练 (CPT) 目标 通过在大量领域特定的无监督数据上继续预训练基座模型，注入新的领域知识，使模型更好地理解和生成特定领域的内容。\n训练要点 流式加载\n实施流式数据加载，以便在训练过程中动态读取数据，防止超过最大内存，训练中断。\n全参数微调\n在进行模型训练时，通常需要更新模型的全部参数，以确保模型能够全面掌握新知识。\n全量微调相较于参数高效微调（如 LoRA）在领域知识注入方面效果更佳，尤其在运算资源充足的情况下，建议优先选择全参数微调。\n监督微调 (SFT) 目标 通过高质量的标注数据，训练模型执行特定任务，如代码生成、代码修复、复杂指令执行等，提升模型的实用性和准确性。\n训练要点 Epoch 数量\n在数据量充足的情况下通常 1 ~ 4 个 epoch 即可见到显著效果。 如果数据量不够，可以考虑加大 epoch 数量，但要注意过拟合的风险，建议进行数据增强。 数据增强与多样性\n确保训练数据涵盖多种任务类型和指令表达方式，提升模型的泛化能力。 包含多轮对话和鲁棒性数据，增强模型应对真实用户场景的能力。 直接偏好对齐 (DPO) 目标 通过用户反馈和偏好数据，优化模型输出，使其更符合人类的期望和需求，包括回答风格、安全性和可读性等方面。\nDPO 的特点 直接优化\n不需要单独训练 Reward Model，直接通过 (chosen, rejected) 数据对模型进行对比学习。\n高效性\n相较于 PPO，DPO 需要更少的数据和计算资源即可达到相似甚至更好的效果。\n动态适应\n每次有新数据时，模型能立即适应，无需重新训练 Reward Model。\n训练要点 偏好数据的收集\n部署多个不同训练阶段或不同数据配比的模型，生成多样化的回答。 通过人工或自动化方式标注 chosen 和 rejected 回答对，确保数据的多样性和质量。 对比学习\n通过最大化 chosen 回答的概率，最小化 rejected 回答的概率，优化模型参数。\n迭代优化\n持续收集用户反馈，生成新的偏好数据，进行循环迭代，逐步提升模型性能。 结合数据飞轮机制，实现模型的持续进化与优化。 常见问题与解决方案 重复输出 (Repetitive Outputs)\n问题：模型生成内容重复，连续打印停不下来。\n解决方案：\n数据去重与清洗：确保训练数据不含大量重复内容。 检查 EOT（End-of-Token）设置：防止模型连接打印无法停止。 通过 SFT/DPO 进行对齐：优化模型输出质量。 调整解码策略：如增加 top_k、repetition penalty 和 temperature 参数。 灾难性遗忘 (Catastrophic Forgetting)\n问题：模型在微调过程中遗忘原有的通用能力，可以看作是在新的数据集上过拟合，原本模型参数空间变化过大。\n解决方案：\n混合一部分通用数据：保持模型的通用能力。 调低学习率：减少对原有知识的冲击。 增加 Dropout Rate 和 Weight Decay：避免过拟合。 采用 LoRA 等参数高效微调方法：避免大规模参数更新。 使用 RAG 辅助：结合外部知识库提升模型表现。 Chat Vector: 通过模型权重的简单算术操作，快速为模型注入对话和通用能力。 实体关系与推理路径理解不足\n问题：模型难以正确理解复杂的实体关系和推理路径。\n解决方案：\n引入 Chain-of-Thought (CoT) 数据与强化推理训练：\n通过分步推理训练提升模型的能力，结合 强化微调 和 o1/o3 的训练方法。 扩展训练数据覆盖面：\n引入更多包含复杂实体关系和推理路径的多样化场景数据。 结合知识图谱建模：\n利用 GraphRAG 强化模型对实体关系的理解与推理能力。 模型部署与评估 部署 推理框架\nollama：基于 llama.cpp 的本地推理部署，可快速启动 vLLM：主打高并发、多用户场景下的推理吞吐量优化 量化：将模型量化为 8-bit 或 4-bit，进一步降低推理成本，提高部署效率。 集成 RAG \u0026 智能体\nRAG：结合向量知识库，实时检索相关文档或代码片段，辅助模型生成更精准的回答。 智能体：利用 Function Call 或多轮对话机制，让模型调用外部工具或进行复杂推理，提升互动性和实用性。 Langgraph：封装 RAG 和 多智能体工作流，构建定制化的对话系统或代码自动生成平台。 评估 评估指标\nCPT 阶段：使用领域内测试集，评估困惑度（Perplexity，PPL）或者交叉熵（Cross Entropy），衡量模型对新知识的掌握程度。 SFT / DPO 阶段： Human 或模型评测：通过人工评分或自动化工具，评估回答的准确性、连贯性、可读性和安全性。 代码生成：构建大规模单元测试集，评估 pass@k 指标，衡量代码生成的正确率。 通用能力：在常见 benchmark（如 MMLU、CMMLU）对模型进行测试，确保模型在通用任务上的表现下降不大。 解码超参数\n一致性：在评估过程中保持 top_k、top_p、temperature、max_new_tokens 等解码参数一致，确保评估结果的可比性。 网格搜索：在算力允许的情况下，对不同解码参数组合进行评估，选择最优的参数配置。 数据飞轮与持续迭代 数据飞轮机制\n实时收集用户日志\n收集线上用户的真实 prompt 和生成的回答，覆盖多样化的使用场景和任务类型。\n自动或人工标注\n对收集到的用户 prompt 和回答进行偏好标注，生成新的 (chosen, rejected) 数据对。\n迭代训练\n将新生成的偏好数据加入到下一轮的 SFT/DPO 训练中，不断优化模型的回答质量和用户体验。\n鲁棒性数据\n包含拼写错误、混合语言、模糊指令等数据，提升模型在真实场景下的鲁棒性和应对能力。\n持续优化\n反馈循环：利用用户反馈，持续改进训练数据和模型表现，实现模型的自我优化和进化。 多模型协同：部署多个版本的模型，生成多样化的回答，通过对比学习提升模型的综合能力。 结合意图识别和多智能体推理 使用意图分类模型让大模型判断用户输入意图类别。基于意图类别与上下文类型的映射，监督推理路径，然后根据推理路径进行多路召回。将这些信息提供给训练好的模型，生成最终结果。\n总结 通过 继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO) 的组合方法，能够有效地在基座大模型上注入特定领域的知识，构建出具备高效解决业务问题能力的闭源大语言模型。关键步骤如下：\n数据准备\n高质量的数据收集、清洗、去重和分类，确保训练数据的多样性与准确性。 结合数据脱敏策略，保护隐私与合规。 模型训练\n通过 CPT 注入领域知识，SFT 学习特定任务模式，DPO 优化模型输出符合人类偏好和安全。 利用高效的并行训练框架和调参技巧，提升训练效率和资源利用率。 部署与评估\n采用高效的推理框架，结合 RAG 和 Agent 实现知识增强和功能扩展。 通过多维度评估，确保模型在各个阶段的表现符合预期。 持续迭代\n构建数据飞轮，实时收集用户反馈，不断优化训练数据和模型表现。 集成 RAG 和 Agent，实现模型能力的持续提升与扩展。 最终，通过系统化的流程和技术手段，能够构建一个不仅具备深厚领域知识，还能灵活应对复杂业务需求的长生命周期 AI 系统。\n参考资料 DeepSpeed Megatron-LM ollama vLLM GraphRAG The Llama 3 Herd of Models ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages Evaluating Large Language Models Trained on Code Direct Preference Optimization: Your Language Model is Secretly a Reward Model 引用 引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Jan 2025). 构建特定领域的大语言模型.\nhttps://syhya.github.io/posts/2025-01-05-domain-llm-training\nOr\n@article{syhya2024domainllm, title = \"构建特定领域的大语言模型\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Jan\", url = \"https://syhya.github.io/posts/2025-01-05-domain-llm-training/\" } ","wordCount":"8675","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2025-01-05T12:00:00+08:00","dateModified":"2025-07-03T09:46:11+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">构建特定领域的大语言模型</h1><div class=post-meta><span title='2025-01-05 12:00:00 +0800 +0800'>2025-01-05</span>&nbsp;·&nbsp;18 分钟&nbsp;·&nbsp;8675 字&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;翻译:<ul class=i18n_list><li><a href=https://syhya.github.io/posts/2025-01-05-domain-llm-training/>En</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#背景>背景</a></li><li><a href=#为什么要向基座模型注入领域知识>为什么要向基座模型注入领域知识</a><ul><li><a href=#挑战一有限的领域知识>挑战一：有限的领域知识</a></li><li><a href=#挑战二数据安全与合规>挑战二：数据安全与合规</a></li><li><a href=#挑战三openai微调的局限>挑战三：OpenAI微调的局限</a></li></ul></li><li><a href=#注入知识两种方法>注入知识两种方法</a><ul><li><a href=#方法对比>方法对比</a><ul><li><a href=#微调-fine-tuning>微调 (Fine-Tuning)</a></li><li><a href=#检索增强生成-rag>检索增强生成 (RAG)</a></li></ul></li></ul></li><li><a href=#模型与训练资源>模型与训练资源</a><ul><li><a href=#基座模型>基座模型</a></li><li><a href=#训练资源>训练资源</a></li></ul></li><li><a href=#deepspeed-zero-分片策略对比>DeepSpeed ZeRO 分片策略对比</a><ul><li><a href=#zero-stage-分片策略>ZeRO Stage 分片策略</a></li><li><a href=#offload-策略>Offload 策略</a></li></ul></li><li><a href=#通信量与性能影响>通信量与性能影响</a></li><li><a href=#硬件与配置影响>硬件与配置影响</a></li><li><a href=#数据准备决定训练成败的核心>数据准备：决定训练成败的核心</a><ul><li><a href=#预训练数据>预训练数据</a><ul><li><a href=#数据来源>数据来源</a></li><li><a href=#数据规模>数据规模</a></li><li><a href=#数据处理>数据处理</a></li></ul></li><li><a href=#监督微调数据>监督微调数据</a><ul><li><a href=#数据格式>数据格式</a></li><li><a href=#数据构建>数据构建</a></li><li><a href=#数据增强策略>数据增强策略</a></li><li><a href=#关键要点>关键要点</a></li></ul></li><li><a href=#偏好数据>偏好数据</a><ul><li><a href=#数据格式-1>数据格式</a></li><li><a href=#采样策略>采样策略</a></li><li><a href=#关键要点-1>关键要点</a></li></ul></li></ul></li><li><a href=#训练流程>训练流程</a><ul><li><a href=#三种方法对比>三种方法对比</a><ul><li><a href=#训练方法概览>训练方法概览</a></li><li><a href=#优势与挑战>优势与挑战</a></li></ul></li><li><a href=#通用训练要点与技术细节>通用训练要点与技术细节</a><ul><li><a href=#数据处理与准备>数据处理与准备</a></li><li><a href=#学习率与优化>学习率与优化</a></li><li><a href=#训练策略>训练策略</a></li><li><a href=#监控与调优>监控与调优</a></li></ul></li><li><a href=#继续预训练-cpt-1>继续预训练 (CPT)</a><ul><li><a href=#目标>目标</a></li><li><a href=#训练要点>训练要点</a></li></ul></li><li><a href=#监督微调-sft-1>监督微调 (SFT)</a><ul><li><a href=#目标-1>目标</a></li><li><a href=#训练要点-1>训练要点</a></li></ul></li><li><a href=#直接偏好对齐-dpo-1>直接偏好对齐 (DPO)</a><ul><li><a href=#目标-2>目标</a></li><li><a href=#dpo-的特点>DPO 的特点</a></li><li><a href=#训练要点-2>训练要点</a></li></ul></li><li><a href=#常见问题与解决方案>常见问题与解决方案</a></li></ul></li><li><a href=#模型部署与评估>模型部署与评估</a><ul><li><a href=#部署>部署</a></li><li><a href=#评估>评估</a></li></ul></li><li><a href=#数据飞轮与持续迭代>数据飞轮与持续迭代</a></li><li><a href=#结合意图识别和多智能体推理>结合意图识别和多智能体推理</a></li><li><a href=#总结>总结</a></li><li><a href=#参考资料>参考资料</a></li><li><a href=#引用>引用</a></li></ul></nav></div></details></div><div class=post-content><h2 id=背景>背景<a hidden class=anchor aria-hidden=true href=#背景>#</a></h2><p>随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。</p><p>本文基于作者的工作经验，总结了如何在现有通用模型的基础上，通过数据准备、模型训练、部署、评估及持续迭代，构建具备特定领域知识的大语言模型。</p><h2 id=为什么要向基座模型注入领域知识>为什么要向基座模型注入领域知识<a hidden class=anchor aria-hidden=true href=#为什么要向基座模型注入领域知识>#</a></h2><h3 id=挑战一有限的领域知识>挑战一：有限的领域知识<a hidden class=anchor aria-hidden=true href=#挑战一有限的领域知识>#</a></h3><p>现有的预训练模型（如 GPT-4、Llama 3）主要基于通用语料库进行训练，缺乏对小众语言或专有领域的深入理解，导致模型在处理编程代码时表现不佳。</p><h3 id=挑战二数据安全与合规>挑战二：数据安全与合规<a hidden class=anchor aria-hidden=true href=#挑战二数据安全与合规>#</a></h3><p>企业在处理敏感数据时，必须遵循严格的数据主权和合规性要求。将私有业务数据上传至第三方云服务存在安全隐患，因此需要在本地环境中完成数据处理与模型训练。</p><h3 id=挑战三openai微调的局限>挑战三：OpenAI微调的局限<a hidden class=anchor aria-hidden=true href=#挑战三openai微调的局限>#</a></h3><p>主流商用 API 的微调方案通常较为基础，难以实现深层次的对齐和优化。对于需要高度定制化的领域模型，这种方法难以满足需求。</p><hr><h2 id=注入知识两种方法>注入知识两种方法<a hidden class=anchor aria-hidden=true href=#注入知识两种方法>#</a></h2><p>在实际项目中，常见的将领域知识注入基座模型的方法主要包括 <strong>微调 (Fine-Tuning)</strong> 和 <strong>检索增强生成 (RAG)</strong>。下文将详细对比这些方法，以帮助选择最适合的策略。</p><h3 id=方法对比>方法对比<a hidden class=anchor aria-hidden=true href=#方法对比>#</a></h3><h4 id=微调-fine-tuning>微调 (Fine-Tuning)<a hidden class=anchor aria-hidden=true href=#微调-fine-tuning>#</a></h4><p><strong>核心思路</strong><br>通过持续预训练、监督微调和偏好对齐，直接更新模型参数，使其掌握特定领域知识和任务模式。</p><p><strong>技术细节</strong></p><ul><li><strong>继续预训练 (CPT)</strong>：在大量领域特定的无监督数据上继续预训练基座模型。</li><li><strong>监督微调 (SFT)</strong>：使用高质量的标注数据进行有监督微调。</li><li><strong>偏好对齐 (DPO)</strong>：通过用户反馈优化模型输出。</li><li><strong>参数微调方法</strong>：使用全参数微调或者结合 LoRA 等 PEFT 方法冻结部分参数并添加 adapter。</li></ul><p><strong>优势</strong></p><ul><li><strong>深度定制</strong>：模型内部权重更新，能够深入理解领域知识。</li><li><strong>无需依赖外部检索</strong>：推理时不需额外的知识库支持，减少延迟和总的 token 消耗。</li><li><strong>提升整体性能</strong>：在特定领域任务上表现显著优于通用模型。</li></ul><p><strong>劣势</strong></p><ul><li><strong>高计算成本</strong>：需要大量计算资源进行训练，尤其是 CPT 阶段。</li><li><strong>训练周期长</strong>：从数据准备到模型训练再到优化，需要较长时间。</li><li><strong>灾难性遗忘</strong>：模型可能在学习新知识的同时，遗忘原有的通用能力。</li></ul><h4 id=检索增强生成-rag>检索增强生成 (RAG)<a hidden class=anchor aria-hidden=true href=#检索增强生成-rag>#</a></h4><p><strong>核心思路</strong><br>构建领域知识库，在推理阶段检索相关文档，辅助模型生成更准确的回答，无需直接改变模型参数。</p><p><strong>技术细节</strong></p><ul><li><strong>数据处理</strong>：对领域文档进行预处理，按块大小和重叠量切分。</li><li><strong>向量化</strong>：使用文本嵌入模型将文本块转换为向量，存储在向量数据库中。</li><li><strong>召回</strong>：推理时通过相似度搜索召回相关文档，作为上下文信息或 few-shot 示例提供给基座模型。</li></ul><p><strong>优势</strong></p><ul><li><strong>保持通用能力</strong>：模型参数不变，仍保留通用语言能力。</li><li><strong>快速更新</strong>：知识库可动态更新，无需重新训练模型。</li><li><strong>计算效率</strong>：避免大规模训练，节省计算资源。</li></ul><p><strong>劣势</strong></p><ul><li><strong>依赖知识库质量</strong>：检索到的文档质量直接影响回答质量。</li><li><strong>推理速度</strong>：检索过程可能增加推理延迟，并且需要更多的 token。</li><li><strong>知识覆盖有限</strong>：模型内部知识仍受限于基座模型的预训练数据。</li></ul><hr><h2 id=模型与训练资源>模型与训练资源<a hidden class=anchor aria-hidden=true href=#模型与训练资源>#</a></h2><h3 id=基座模型>基座模型<a hidden class=anchor aria-hidden=true href=#基座模型>#</a></h3><p>以 <a href=https://arxiv.org/pdf/2407.21783>Llama 3 系列</a> 为例，其具有以下特点：</p><ul><li><p><strong>参数规模</strong><br>Llama 3 系列涵盖从 1B 到 405B 参数的模型，广泛支持多语言处理、代码生成、推理，以及视觉和文本任务。小型模型（1B 和 3B）经过专门优化，适合边缘和移动设备，支持最大 128K 的上下文窗口，可高效处理本地任务，例如摘要生成、指令执行和文本重写。</p></li><li><p><strong>多模态能力</strong><br>Llama 3 的视觉模型（11B 和 90B 参数）在图像理解任务上的表现优于许多封闭模型，同时支持图像、视频和语音的多模态处理。所有模型均支持微调，便于针对特定领域进行定制化开发。</p></li><li><p><strong>开源与社区支持</strong><br>Llama 3 系列模型及其权重以开源形式发布，可通过 <a href=https://llama.com>llama.com</a> 和 <a href=https://huggingface.co/meta-llama>Hugging Face 平台</a> 获取，为开发者提供便捷的访问和应用支持。</p></li><li><p><strong>数据集限制</strong><br>虽然 Llama 3 模型本身以开源形式发布，但其训练所使用的数据集并未开源。因此，严格来说，Llama 3 并非完全开源的模型。这一限制可能会在解决灾难性遗忘问题时带来挑战，因为难以获得与原始训练完全一致的数据集。</p></li></ul><h3 id=训练资源>训练资源<a hidden class=anchor aria-hidden=true href=#训练资源>#</a></h3><p>训练大型语言模型需要强大的计算资源和高效的分布式训练框架。</p><ul><li><p><strong>硬件资源</strong></p><ul><li><strong>GPU 集群</strong>：建议使用 NVIDIA A100 或 H100 GPU，4卡或8卡配置，通过 NVLink 或 InfiniBand 提升通信带宽。</li><li><strong>存储资源</strong>：高性能 SSD（如 NVMe）以支持快速的数据读写。</li></ul></li><li><p><strong>软件框架</strong></p><ul><li><strong>并行框架</strong>：<a href=https://github.com/microsoft/DeepSpeed>DeepSpeed</a>、<a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a> 等分布式训练框架，支持大规模模型训练。</li><li><strong>推理框架</strong>：<a href=https://github.com/vllm-project/vllm>vLLM</a>、<a href=https://github.com/jmorganca/ollama>ollama</a> 等，优化推理速度和资源利用。</li></ul></li><li><p><strong>并行策略</strong></p><ul><li><strong>数据并行</strong>：适用于单卡可容纳模型的情况，通过 DeepSpeed 的 ZeRO Stage 0 实现。</li><li><strong>模型并行、流水线并行和张量并行</strong>：单卡无法容纳时，采用 ZeRO Stage 1、2、3 进行优化，或使用 ZeRO-Infinity 将参数和优化器状态部分卸载到 CPU 或 NVMe。</li></ul></li></ul><h2 id=deepspeed-zero-分片策略对比>DeepSpeed ZeRO 分片策略对比<a hidden class=anchor aria-hidden=true href=#deepspeed-zero-分片策略对比>#</a></h2><p>为了更好地理解 DeepSpeed 的 ZeRO 分片策略，以下将分为不同的部分进行详细说明。</p><h3 id=zero-stage-分片策略>ZeRO Stage 分片策略<a hidden class=anchor aria-hidden=true href=#zero-stage-分片策略>#</a></h3><table><thead><tr><th><strong>ZeRO Stage</strong></th><th><strong>描述</strong></th><th><strong>显存占用</strong></th><th><strong>训练速度</strong></th></tr></thead><tbody><tr><td><strong>ZeRO-0</strong></td><td>纯数据并行，不进行任何分片。所有优化器状态、梯度和参数在每张 GPU 上完全复制。</td><td>最高</td><td><strong>最快</strong></td></tr><tr><td><strong>ZeRO-1</strong></td><td>分片优化器状态（例如动量和二阶矩），减少显存占用，但梯度和参数仍为数据并行。</td><td>高</td><td>略慢于 ZeRO-0</td></tr><tr><td><strong>ZeRO-2</strong></td><td>分片优化器状态和梯度，在 ZeRO-1 的基础上进一步减少显存占用。</td><td>中</td><td>慢于 ZeRO-1</td></tr><tr><td><strong>ZeRO-3</strong></td><td>分片优化器状态、梯度和模型参数，显存占用最低，适合大规模模型。但需要在前向和后向时进行参数广播（All-Gather/All-Reduce），通信量显著增加。</td><td>低</td><td>明显慢于 ZeRO-2，取决于模型大小和网络带宽</td></tr></tbody></table><h3 id=offload-策略>Offload 策略<a hidden class=anchor aria-hidden=true href=#offload-策略>#</a></h3><table><thead><tr><th><strong>Offload 类型</strong></th><th><strong>描述</strong></th><th><strong>显存占用</strong></th><th><strong>训练速度</strong></th></tr></thead><tbody><tr><td><strong>ZeRO-1 + CPU Offload</strong></td><td>在 ZeRO-1 的基础上，将优化器状态卸载到 CPU 内存；可进一步降低 GPU 显存占用，但需要 CPU-GPU 数据传输，依赖 PCIe 带宽，且占用 CPU 内存。</td><td>中偏低</td><td>慢于 ZeRO-1，受 CPU 性能和 PCIe 带宽影响</td></tr><tr><td><strong>ZeRO-2 + CPU Offload</strong></td><td>在 ZeRO-2 的基础上，将优化器状态卸载到 CPU 内存；对较大模型进一步降低 GPU 显存占用，但会增加 CPU-GPU 数据传输开销。</td><td>较低</td><td>慢于 ZeRO-2，受 CPU 性能和 PCIe 带宽影响</td></tr><tr><td><strong>ZeRO-3 + CPU Offload</strong></td><td>在 ZeRO-3 的基础上，将优化器状态和模型参数卸载到 CPU；GPU 显存占用最小，但 CPU-GPU 通信量极大，且 CPU 带宽远小于 GPU-GPU 通信。</td><td><strong>极低</strong></td><td><strong>非常慢</strong></td></tr><tr><td><strong>ZeRO-Infinity (NVMe Offload)</strong></td><td>基于 ZeRO-3，将优化器状态、梯度和参数卸载到 NVMe，突破 CPU 内存限制，适合超大规模模型；性能高度依赖 NVMe 并行读写速度。</td><td><strong>极低</strong>需 NVMe 支持</td><td>慢于 ZeRO-3，但通常优于 ZeRO-3 + CPU Offload</td></tr></tbody></table><h2 id=通信量与性能影响>通信量与性能影响<a hidden class=anchor aria-hidden=true href=#通信量与性能影响>#</a></h2><ul><li><p><strong>ZeRO-0/1/2</strong></p><ul><li>通信以 <strong>梯度同步</strong> 为主，使用 All-Reduce 操作，通信量相对较低。</li></ul></li><li><p><strong>ZeRO-3</strong></p><ul><li>需要对模型参数进行 <strong>All-Gather/All-Reduce</strong> 操作，通信量显著增大，网络带宽成为关键瓶颈，前后传播时的参数广播进一步加剧通信负担。</li></ul></li><li><p><strong>CPU Offload</strong>（ZeRO-1/2/3 + CPU）</p><ul><li>卸载优化器状态或参数到 CPU，减少 GPU 显存占用。</li><li>通信量主要来自 <strong>CPU &lt;-> GPU</strong> 数据传输，带宽远低于 GPU-GPU 通信，极易造成性能瓶颈，尤其在 <strong>ZeRO-3</strong> 场景下。</li></ul></li><li><p><strong>NVMe Offload</strong>（ZeRO-Infinity）</p><ul><li>在 <strong>ZeRO-3</strong> 的基础上进一步卸载至 NVMe，突破 CPU 内存限制以支持超大规模模型。</li><li>性能强烈依赖 <strong>NVMe I/O 带宽</strong> 和并行度，若 NVMe 速度足够高，通常优于 CPU Offload；但在 I/O 性能较弱或高延迟场景下，效果可能不佳。</li></ul></li></ul><h2 id=硬件与配置影响>硬件与配置影响<a hidden class=anchor aria-hidden=true href=#硬件与配置影响>#</a></h2><ul><li><p><strong>硬件限制</strong></p><ul><li><strong>PCIe 带宽</strong>、<strong>网络带宽</strong>、<strong>NVMe I/O</strong> 等对 Offload 性能有显著影响，需根据硬件环境选择最佳策略。</li></ul></li><li><p><strong>补充说明</strong></p><ul><li><strong>CPU Offload</strong> 利用 CPU 内存并通过 PCIe 传输数据；<strong>NVMe Offload</strong> 则将状态保存于 NVMe 设备。</li><li>NVMe Offload 在 <strong>NVMe I/O 性能充足</strong> 时通常优于 CPU Offload，但需避免因 I/O 性能不足导致的性能瓶颈。</li></ul></li><li><p><strong>与官方文档对照</strong></p><ul><li>建议结合 <a href=https://www.deepspeed.ai/>DeepSpeed 官方文档</a> 获取最新、最准确的配置参数和性能调优建议。</li></ul></li></ul><hr><h2 id=数据准备决定训练成败的核心>数据准备：决定训练成败的核心<a hidden class=anchor aria-hidden=true href=#数据准备决定训练成败的核心>#</a></h2><p>数据质量直接决定了模型的性能。数据准备包括数据收集、清洗、去重、分类与配比、脱敏等步骤。</p><h3 id=预训练数据>预训练数据<a hidden class=anchor aria-hidden=true href=#预训练数据>#</a></h3><h4 id=数据来源>数据来源<a hidden class=anchor aria-hidden=true href=#数据来源>#</a></h4><ul><li><strong>公开数据集</strong>：如：<a href=https://huggingface.co/datasets/bigcode/the-stack-v2>the-stack-v2</a>、Common Crawl 等。</li><li><strong>企业自有数据</strong>：内部文档、代码库、业务日志等。</li><li><strong>网络爬虫</strong>：通过爬虫技术采集领域相关的网页内容。</li></ul><h4 id=数据规模>数据规模<a hidden class=anchor aria-hidden=true href=#数据规模>#</a></h4><ul><li>建议使用至少数亿到数十亿个 token，以确保模型能够充分学习领域知识。</li><li>当数据量不足时，模型效果可能受限，建议采用数据增强的方法来补充数据。</li></ul><h4 id=数据处理>数据处理<a hidden class=anchor aria-hidden=true href=#数据处理>#</a></h4><ol><li><p><strong>数据预处理</strong></p><ul><li><strong>格式统一</strong>：对来自多个数据源的无标注大量语料进行处理，确保其格式一致。推荐使用高效的存储格式，如 Parquet，以提高数据读取和处理的效率。</li></ul></li><li><p><strong>数据去重</strong></p><ul><li><strong>检测方法</strong>：使用 MinHash、SimHash 或余弦相似度等算法进行近似重复检测。</li><li><strong>处理粒度</strong>：可选择按句子、段落或文档级别去重，根据任务需求灵活调整。</li><li><strong>相似度阈值</strong>：设定合理的相似度阈值（如 0.9），删除重复度高于阈值的文本，确保数据多样性。</li></ul></li><li><p><strong>数据清洗</strong></p><ul><li><strong>文本过滤</strong>：结合规则和模型评分器（如 BERT/RoBERTa）去除乱码、拼写错误和低质量文本。</li><li><strong>格式化处理</strong>：优先使用 JSON 格式处理数据，确保代码、Markdown 和 LaTeX 等特殊格式的准确性。</li></ul></li><li><p><strong>数据脱敏</strong></p><ul><li><strong>隐私保护</strong>：匿名化或去除人名、电话号码、邮箱、密码等敏感信息，确保数据合规。</li><li><strong>不合规内容过滤</strong>：剔除含有违法、色情或种族歧视等内容的数据块。</li></ul></li><li><p><strong>数据混合与配比</strong></p><ul><li><strong>比例控制</strong>：例如，将 70% 的领域特定数据与 30% 的通用数据相结合，避免模型遗忘通用能力。</li><li><strong>任务类型</strong>：确保数据包含代码生成、问答对话、文档摘要、多轮对话和数学推理等多种任务类型。</li></ul></li><li><p><strong>数据顺序</strong></p><ul><li><strong>逐步引导</strong>：采用课程学习（Curriculum Learning）方法，从简单、干净的数据开始训练，逐步引入更复杂或噪声较高的数据，优化模型的学习效率和收敛路径。</li><li><strong>语义连贯性</strong>：利用上下文预训练（In-context Pretraining）技术，将语义相似的文档拼接在一起，增强上下文一致性，提升模型的语义理解深度与泛化能力。</li></ul></li></ol><h3 id=监督微调数据>监督微调数据<a hidden class=anchor aria-hidden=true href=#监督微调数据>#</a></h3><h4 id=数据格式>数据格式<a hidden class=anchor aria-hidden=true href=#数据格式>#</a></h4><p>可采用 Alpaca 或 Vicuna 风格，比如结构化为 [instruction, input, output] 的单轮和多轮对话。</p><ul><li><strong>规模</strong>：几千条到几十万条，具体根据项目需求和计算资源决定。</li><li><strong>质量</strong>：确保数据的高质量和多样性，避免模型学习到错误或偏见。</li></ul><h4 id=数据构建>数据构建<a hidden class=anchor aria-hidden=true href=#数据构建>#</a></h4><p>在数据构建过程中，我们首先收集日常业务数据，并与业务专家共同构建基础问题。随后，利用大语言模型进行数据增强，以提升数据的多样性和鲁棒性。以下是具体的数据增强策略：</p><h4 id=数据增强策略>数据增强策略<a hidden class=anchor aria-hidden=true href=#数据增强策略>#</a></h4><ul><li><p><strong>表达多样化</strong><br>通过大语言模型对现有数据进行改写，采用同义词替换和语法变换等方法，增加数据的多样性。</p></li><li><p><strong>鲁棒性增强</strong><br>构建包含拼写错误、混合语言等输入的提示（Prompt），以模拟真实场景，同时确保生成答案的高质量。</p></li><li><p><strong>知识蒸馏</strong><br>利用 GPT-4、Claude 等大型语言模型进行知识蒸馏，生成符合需求的问答数据对。</p></li><li><p><strong>复杂任务设计</strong><br>针对复杂场景（如多轮对话、逻辑推理等），手动设计高质量数据，以覆盖模型的能力边界。</p></li><li><p><strong>数据生成管道</strong><br>构建自动化数据生成流水线，将数据生成、筛选、格式化和校验等环节集成，提高整体效率。</p></li></ul><h4 id=关键要点>关键要点<a hidden class=anchor aria-hidden=true href=#关键要点>#</a></h4><ul><li><strong>任务类型标注</strong>：每条数据标注明确的任务类型，便于后续精细化分析和调优。</li><li><strong>多轮对话与话题切换</strong>：构建多轮对话中上下文关联与话题转换的数据，确保模型能够学习话题切换与上下文关联的能力。</li><li><strong>思维链（Chain of Thought）策略</strong>：分类、推理等任务可先用 COT 生成过程性答案，提高准确率。</li><li><strong>数据飞轮</strong>：上线后持续收集用户真实问题，结合真实需求迭代数据；定期清洗，确保质量与多样性。</li></ul><h3 id=偏好数据>偏好数据<a hidden class=anchor aria-hidden=true href=#偏好数据>#</a></h3><h4 id=数据格式-1>数据格式<a hidden class=anchor aria-hidden=true href=#数据格式-1>#</a></h4><ul><li><strong>三元组结构</strong>：[prompt, chosen answer, rejected answer]</li><li><strong>标注细节</strong>：<ol><li><strong>多模型采样</strong>：使用多个不同训练阶段或不同数据配比的模型生成回答，增加数据多样性。</li><li><strong>编辑与优化</strong>：标注人员可对选择的回答进行小幅修改，确保回答质量。</li></ol></li></ul><h4 id=采样策略>采样策略<a hidden class=anchor aria-hidden=true href=#采样策略>#</a></h4><ol><li><strong>多模型采样</strong>：部署多个不同版本的模型，对同一 prompt 生成不同回答。</li><li><strong>对比标注</strong>：由人工或自动化系统对生成的回答进行对比，选择更优的回答对。</li></ol><h4 id=关键要点-1>关键要点<a hidden class=anchor aria-hidden=true href=#关键要点-1>#</a></h4><ul><li><strong>数据多样性与覆盖</strong>：确保偏好数据涵盖各种场景和任务，避免模型在特定情境下表现不佳。</li><li><strong>高质量标注</strong>：偏好数据的质量直接影响模型的对齐效果，需确保标注准确且一致。</li></ul><hr><h2 id=训练流程>训练流程<a hidden class=anchor aria-hidden=true href=#训练流程>#</a></h2><p>一个完整的特定领域大语言模型训练流程通常包括 <strong>继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO)</strong> 三个主要步骤，最终实现模型的部署与持续优化。</p><h3 id=三种方法对比>三种方法对比<a hidden class=anchor aria-hidden=true href=#三种方法对比>#</a></h3><h4 id=训练方法概览>训练方法概览<a hidden class=anchor aria-hidden=true href=#训练方法概览>#</a></h4><table><thead><tr><th><strong>训练方法</strong></th><th><strong>主要目标</strong></th><th><strong>数据需求</strong></th><th><strong>典型应用场景</strong></th></tr></thead><tbody><tr><td>继续预训练 (CPT)</td><td>继续在大规模无监督语料上进行预训练，注入新领域知识</td><td>大量无标签文本（至少数亿到数十亿 Token）</td><td>补足领域知识，如法律、医疗、金融等专业文本</td></tr><tr><td>监督微调 (SFT)</td><td>在有监督标注的数据上进行微调，强化特定任务和指令执行能力</td><td>定制化标注数据（指令/对话对等），从几千到几十万条</td><td>各类特定任务，如代码生成、问答、文本改写、复杂指令执行等</td></tr><tr><td>直接偏好对齐 (DPO)</td><td>利用偏好数据（正例 chosen vs. 负例 rejected）直接对齐人类偏好</td><td>偏好数据：[prompt, chosen, rejected](规模相对较小)</td><td>对齐人类反馈，如回答风格、合规性、安全性等</td></tr></tbody></table><h4 id=优势与挑战>优势与挑战<a hidden class=anchor aria-hidden=true href=#优势与挑战>#</a></h4><h5 id=继续预训练-cpt>继续预训练 (CPT)<a hidden class=anchor aria-hidden=true href=#继续预训练-cpt>#</a></h5><p><strong>优势</strong>:</p><ul><li>更好领域覆盖，全面提升模型在特定领域的理解和生成能力。</li><li>无需额外手动标注。</li></ul><p><strong>挑战/局限</strong>:</p><ul><li>需要大量高质量领域数据。</li><li>训练成本高，需大规模算力与时间。</li><li>可能引入领域偏见，需谨慎处理数据质量与分布。</li></ul><h5 id=监督微调-sft>监督微调 (SFT)<a hidden class=anchor aria-hidden=true href=#监督微调-sft>#</a></h5><p><strong>优势</strong>:</p><ul><li>快速获取可用的任务执行能力。</li><li>显著提升模型对特定场景的准确性。</li></ul><p><strong>挑战/局限</strong>:</p><ul><li>数据标注成本较高。</li><li>需谨慎选择标注数据以避免过拟合。</li><li>微调后可能削弱模型的通用性。</li></ul><h5 id=直接偏好对齐-dpo>直接偏好对齐 (DPO)<a hidden class=anchor aria-hidden=true href=#直接偏好对齐-dpo>#</a></h5><p><strong>优势</strong>:</p><ul><li>无需单独训练 Reward Model。</li><li>数据量需求较小，调优效率高。</li></ul><p><strong>挑战/局限</strong>:</p><ul><li>需要可靠的偏好标注。</li><li>对复杂、多样化场景仍需持续迭代收集更多偏好数据。</li><li>易受偏好数据分布的限制。</li></ul><hr><h3 id=通用训练要点与技术细节>通用训练要点与技术细节<a hidden class=anchor aria-hidden=true href=#通用训练要点与技术细节>#</a></h3><p>在进行 <strong>CPT、SFT、DPO</strong> 三种训练方法时，存在许多通用的训练要点和技术细节。以下部分将这些通用内容进行统一描述，以便于更好地理解和应用。</p><h4 id=数据处理与准备>数据处理与准备<a hidden class=anchor aria-hidden=true href=#数据处理与准备>#</a></h4><ul><li><strong>数据质量</strong>：无论是 CPT、SFT 还是 DPO，数据的质量都是至关重要的。需要确保数据的准确性、无歧义性和多样性。</li><li><strong>数据格式</strong>：统一的数据格式有助于简化训练流程。例如，使用 JSON 或其他结构化格式来存储训练数据。</li><li><strong>数据增强</strong>：通过 LLM 重写和优化等方式增加数据多样性，提升模型的泛化能力。</li></ul><h4 id=学习率与优化>学习率与优化<a hidden class=anchor aria-hidden=true href=#学习率与优化>#</a></h4><ul><li><strong>学习率设置</strong>：通常采用比预训练阶段更小的学习率，如从 3e-4 降低到 3e-5，具体数值视任务和数据量而定。</li><li><strong>学习率调度</strong>：使用 warm-up 策略（如前 10% 步骤线性递增），随后采用线性衰减或余弦退火策略，确保训练过程平稳。</li><li><strong>优化器选择</strong>：根据模型规模和硬件资源选择合适的优化器，比如 AdamW。</li></ul><h4 id=训练策略>训练策略<a hidden class=anchor aria-hidden=true href=#训练策略>#</a></h4><ul><li><strong>全参数微调</strong>：在资源允许的情况下，优先进行全参数微调，以确保模型能够全面掌握新知识。</li><li><strong>参数高效微调（PEFT）</strong>：如 LoRA，适用于计算资源有限的场景，通过冻结部分参数并添加 adapter 实现高效微调。</li><li><strong>混合精度训练</strong>：在支持的 GPU 上使用 bf16 或 fp16，降低显存占用，提高训练速度。</li><li><strong>训练稳定性</strong>：采用梯度裁剪、正则化、dropout、权重衰减等技术，防止梯度爆炸和模型过拟合。</li><li><strong>Flash Attention</strong>：利用 <a href=https://github.com/Dao-AILab/flash-attention>Flash Attention</a> 技术优化注意力机制的计算效率，提升训练速度和降低显存占用。</li></ul><h4 id=监控与调优>监控与调优<a hidden class=anchor aria-hidden=true href=#监控与调优>#</a></h4><ul><li><strong>收敛监控</strong>：实时监控训练集和验证集的 loss 曲线，确保模型逐步收敛，必要时调整学习率和其他超参数。</li><li><strong>Checkpoint</strong>：定期保留 Checkpoint，防止意外中断导致全部训练进度丢失。</li><li><strong>早停机制</strong>：防止模型过拟合，适时停止训练，保存最佳模型状态。</li><li><strong>模型评估</strong>：在训练过程中定期进行评估，确保模型性能符合预期。</li></ul><h3 id=继续预训练-cpt-1>继续预训练 (CPT)<a hidden class=anchor aria-hidden=true href=#继续预训练-cpt-1>#</a></h3><h4 id=目标>目标<a hidden class=anchor aria-hidden=true href=#目标>#</a></h4><p>通过在大量领域特定的无监督数据上继续预训练基座模型，注入新的领域知识，使模型更好地理解和生成特定领域的内容。</p><h4 id=训练要点>训练要点<a hidden class=anchor aria-hidden=true href=#训练要点>#</a></h4><ol><li><p><strong>流式加载</strong><br>实施流式数据加载，以便在训练过程中动态读取数据，防止超过最大内存，训练中断。</p></li><li><p><strong>全参数微调</strong><br>在进行模型训练时，通常需要更新模型的全部参数，以确保模型能够全面掌握新知识。<br>全量微调相较于参数高效微调（如 LoRA）在领域知识注入方面效果更佳，尤其在运算资源充足的情况下，建议优先选择全参数微调。</p></li></ol><h3 id=监督微调-sft-1>监督微调 (SFT)<a hidden class=anchor aria-hidden=true href=#监督微调-sft-1>#</a></h3><h4 id=目标-1>目标<a hidden class=anchor aria-hidden=true href=#目标-1>#</a></h4><p>通过高质量的标注数据，训练模型执行特定任务，如代码生成、代码修复、复杂指令执行等，提升模型的实用性和准确性。</p><h4 id=训练要点-1>训练要点<a hidden class=anchor aria-hidden=true href=#训练要点-1>#</a></h4><ol><li><p><strong>Epoch 数量</strong></p><ul><li>在数据量充足的情况下通常 1 ~ 4 个 epoch 即可见到显著效果。</li><li>如果数据量不够，可以考虑加大 epoch 数量，但要注意过拟合的风险，建议进行数据增强。</li></ul></li><li><p><strong>数据增强与多样性</strong></p><ul><li>确保训练数据涵盖多种任务类型和指令表达方式，提升模型的泛化能力。</li><li>包含多轮对话和鲁棒性数据，增强模型应对真实用户场景的能力。</li></ul></li></ol><h3 id=直接偏好对齐-dpo-1>直接偏好对齐 (DPO)<a hidden class=anchor aria-hidden=true href=#直接偏好对齐-dpo-1>#</a></h3><h4 id=目标-2>目标<a hidden class=anchor aria-hidden=true href=#目标-2>#</a></h4><p>通过用户反馈和偏好数据，优化模型输出，使其更符合人类的期望和需求，包括回答风格、安全性和可读性等方面。</p><h4 id=dpo-的特点>DPO 的特点<a hidden class=anchor aria-hidden=true href=#dpo-的特点>#</a></h4><ul><li><p><strong>直接优化</strong><br>不需要单独训练 Reward Model，直接通过 (chosen, rejected) 数据对模型进行对比学习。</p></li><li><p><strong>高效性</strong><br>相较于 PPO，DPO 需要更少的数据和计算资源即可达到相似甚至更好的效果。</p></li><li><p><strong>动态适应</strong><br>每次有新数据时，模型能立即适应，无需重新训练 Reward Model。</p></li></ul><h4 id=训练要点-2>训练要点<a hidden class=anchor aria-hidden=true href=#训练要点-2>#</a></h4><ol><li><p><strong>偏好数据的收集</strong></p><ul><li>部署多个不同训练阶段或不同数据配比的模型，生成多样化的回答。</li><li>通过人工或自动化方式标注 chosen 和 rejected 回答对，确保数据的多样性和质量。</li></ul></li><li><p><strong>对比学习</strong><br>通过最大化 chosen 回答的概率，最小化 rejected 回答的概率，优化模型参数。</p></li><li><p><strong>迭代优化</strong></p><ul><li>持续收集用户反馈，生成新的偏好数据，进行循环迭代，逐步提升模型性能。</li><li>结合数据飞轮机制，实现模型的持续进化与优化。</li></ul></li></ol><hr><h3 id=常见问题与解决方案>常见问题与解决方案<a hidden class=anchor aria-hidden=true href=#常见问题与解决方案>#</a></h3><ol><li><p><strong>重复输出 (Repetitive Outputs)</strong><br><strong>问题</strong>：模型生成内容重复，连续打印停不下来。<br><strong>解决方案</strong>：</p><ol><li><strong>数据去重与清洗</strong>：确保训练数据不含大量重复内容。</li><li><strong>检查 EOT（End-of-Token）设置</strong>：防止模型连接打印无法停止。</li><li><strong>通过 SFT/DPO 进行对齐</strong>：优化模型输出质量。</li><li><strong>调整解码策略</strong>：如增加 top_k、repetition penalty 和 temperature 参数。</li></ol></li><li><p><strong>灾难性遗忘 (Catastrophic Forgetting)</strong><br><strong>问题</strong>：模型在微调过程中遗忘原有的通用能力，可以看作是在新的数据集上过拟合，原本模型参数空间变化过大。<br><strong>解决方案</strong>：</p><ol><li><strong>混合一部分通用数据</strong>：保持模型的通用能力。</li><li><strong>调低学习率</strong>：减少对原有知识的冲击。</li><li><strong>增加 Dropout Rate 和 Weight Decay</strong>：避免过拟合。</li><li><strong>采用 LoRA 等参数高效微调方法</strong>：避免大规模参数更新。</li><li><strong>使用 RAG 辅助</strong>：结合外部知识库提升模型表现。</li><li><strong><a href=https://arxiv.org/pdf/2310.04799>Chat Vector</a></strong>: 通过模型权重的简单算术操作，快速为模型注入对话和通用能力。</li></ol></li><li><p><strong>实体关系与推理路径理解不足</strong><br><strong>问题</strong>：模型难以正确理解复杂的实体关系和推理路径。<br><strong>解决方案</strong>：</p><ol><li><strong>引入 Chain-of-Thought (CoT) 数据与强化推理训练</strong>：<br>通过分步推理训练提升模型的能力，结合 <a href=https://openai.com/form/rft-research-program/>强化微调</a> 和 <a href=https://openai.com/o1/>o1/o3</a> 的训练方法。</li><li><strong>扩展训练数据覆盖面</strong>：<br>引入更多包含复杂实体关系和推理路径的多样化场景数据。</li><li><strong>结合知识图谱建模</strong>：<br>利用 <a href=https://github.com/microsoft/graphrag>GraphRAG</a> 强化模型对实体关系的理解与推理能力。</li></ol></li></ol><hr><h2 id=模型部署与评估>模型部署与评估<a hidden class=anchor aria-hidden=true href=#模型部署与评估>#</a></h2><h3 id=部署>部署<a hidden class=anchor aria-hidden=true href=#部署>#</a></h3><p><strong>推理框架</strong></p><ul><li><a href=https://github.com/jmorganca/ollama><strong>ollama</strong></a>：基于 <a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a> 的本地推理部署，可快速启动</li><li><a href=https://github.com/vllm-project/vllm><strong>vLLM</strong></a>：主打高并发、多用户场景下的推理吞吐量优化</li><li><strong>量化</strong>：将模型量化为 8-bit 或 4-bit，进一步降低推理成本，提高部署效率。</li></ul><p><strong>集成 RAG & 智能体</strong></p><ul><li><strong>RAG</strong>：结合向量知识库，实时检索相关文档或代码片段，辅助模型生成更精准的回答。</li><li><strong>智能体</strong>：利用 Function Call 或多轮对话机制，让模型调用外部工具或进行复杂推理，提升互动性和实用性。</li><li><strong>Langgraph</strong>：封装 RAG 和 多智能体工作流，构建定制化的对话系统或代码自动生成平台。</li></ul><h3 id=评估>评估<a hidden class=anchor aria-hidden=true href=#评估>#</a></h3><p><strong>评估指标</strong></p><ul><li><strong>CPT 阶段</strong>：使用领域内测试集，评估困惑度（Perplexity，PPL）或者交叉熵（Cross Entropy），衡量模型对新知识的掌握程度。</li><li><strong>SFT / DPO 阶段</strong>：<ul><li><strong>Human 或模型评测</strong>：通过人工评分或自动化工具，评估回答的准确性、连贯性、可读性和安全性。</li><li><strong>代码生成</strong>：构建大规模单元测试集，评估 pass@k 指标，衡量代码生成的正确率。</li><li><strong>通用能力</strong>：在常见 benchmark（如 MMLU、CMMLU）对模型进行测试，确保模型在通用任务上的表现下降不大。</li></ul></li></ul><p><strong>解码超参数</strong></p><ul><li><strong>一致性</strong>：在评估过程中保持 top_k、top_p、temperature、max_new_tokens 等解码参数一致，确保评估结果的可比性。</li><li><strong>网格搜索</strong>：在算力允许的情况下，对不同解码参数组合进行评估，选择最优的参数配置。</li></ul><hr><h2 id=数据飞轮与持续迭代>数据飞轮与持续迭代<a hidden class=anchor aria-hidden=true href=#数据飞轮与持续迭代>#</a></h2><p><strong>数据飞轮机制</strong></p><ol><li><p><strong>实时收集用户日志</strong><br>收集线上用户的真实 prompt 和生成的回答，覆盖多样化的使用场景和任务类型。</p></li><li><p><strong>自动或人工标注</strong><br>对收集到的用户 prompt 和回答进行偏好标注，生成新的 (chosen, rejected) 数据对。</p></li><li><p><strong>迭代训练</strong><br>将新生成的偏好数据加入到下一轮的 SFT/DPO 训练中，不断优化模型的回答质量和用户体验。</p></li><li><p><strong>鲁棒性数据</strong><br>包含拼写错误、混合语言、模糊指令等数据，提升模型在真实场景下的鲁棒性和应对能力。</p></li></ol><p><strong>持续优化</strong></p><ul><li><strong>反馈循环</strong>：利用用户反馈，持续改进训练数据和模型表现，实现模型的自我优化和进化。</li><li><strong>多模型协同</strong>：部署多个版本的模型，生成多样化的回答，通过对比学习提升模型的综合能力。</li></ul><hr><h2 id=结合意图识别和多智能体推理>结合意图识别和多智能体推理<a hidden class=anchor aria-hidden=true href=#结合意图识别和多智能体推理>#</a></h2><p>使用意图分类模型让大模型判断用户输入意图类别。基于意图类别与上下文类型的映射，监督推理路径，然后根据推理路径进行多路召回。将这些信息提供给训练好的模型，生成最终结果。</p><hr><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>通过 <strong>继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO)</strong> 的组合方法，能够有效地在基座大模型上注入特定领域的知识，构建出具备高效解决业务问题能力的闭源大语言模型。关键步骤如下：</p><ol><li><p><strong>数据准备</strong></p><ul><li>高质量的数据收集、清洗、去重和分类，确保训练数据的多样性与准确性。</li><li>结合数据脱敏策略，保护隐私与合规。</li></ul></li><li><p><strong>模型训练</strong></p><ul><li>通过 CPT 注入领域知识，SFT 学习特定任务模式，DPO 优化模型输出符合人类偏好和安全。</li><li>利用高效的并行训练框架和调参技巧，提升训练效率和资源利用率。</li></ul></li><li><p><strong>部署与评估</strong></p><ul><li>采用高效的推理框架，结合 RAG 和 Agent 实现知识增强和功能扩展。</li><li>通过多维度评估，确保模型在各个阶段的表现符合预期。</li></ul></li><li><p><strong>持续迭代</strong></p><ul><li>构建数据飞轮，实时收集用户反馈，不断优化训练数据和模型表现。</li><li>集成 RAG 和 Agent，实现模型能力的持续提升与扩展。</li></ul></li></ol><p>最终，通过系统化的流程和技术手段，能够构建一个不仅具备深厚领域知识，还能灵活应对复杂业务需求的长生命周期 AI 系统。</p><hr><h2 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h2><ol><li><a href=https://github.com/microsoft/DeepSpeed>DeepSpeed</a></li><li><a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a></li><li><a href=https://github.com/jmorganca/ollama>ollama</a></li><li><a href=https://github.com/vllm-project/vllm>vLLM</a></li><li><a href=https://microsoft.github.io/graphrag/>GraphRAG</a></li><li><a href=https://arxiv.org/pdf/2407.21783>The Llama 3 Herd of Models</a></li><li><a href=https://arxiv.org/abs/2104.07857>ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a></li><li><a href=https://arxiv.org/pdf/2310.04799>Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages</a></li><li><a href=https://arxiv.org/pdf/2107.03374>Evaluating Large Language Models Trained on Code</a></li><li><a href=https://arxiv.org/abs/2305.18290>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></li></ol><hr><h2 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h2><blockquote><p><strong>引用</strong>：转载或引用本文内容，请注明原作者与出处。</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Jan 2025). 构建特定领域的大语言模型.<br><a href=https://syhya.github.io/posts/2025-01-05-domain-llm-training>https://syhya.github.io/posts/2025-01-05-domain-llm-training</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2024domainllm</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;构建特定领域的大语言模型&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Jan&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-01-05-domain-llm-training/&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/zh/tags/nlp/>NLP</a></li><li><a href=https://syhya.github.io/zh/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/zh/tags/pre-training/>Pre-Training</a></li><li><a href=https://syhya.github.io/zh/tags/post-training/>Post-Training</a></li><li><a href=https://syhya.github.io/zh/tags/dpo/>DPO</a></li><li><a href=https://syhya.github.io/zh/tags/%E9%A2%86%E5%9F%9F%E6%A8%A1%E5%9E%8B/>领域模型</a></li><li><a href=https://syhya.github.io/zh/tags/deepspeed/>DeepSpeed</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/zh/posts/2025-01-16-group-query-attention/><span class=title>« 上一页</span><br><span>Transformer注意力机制：MHA、MQA与GQA的对比</span>
</a><a class=next href=https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/><span class=title>下一页 »</span><br><span>基于双卡 RTX 4090 搭建家用深度学习主机</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 构建特定领域的大语言模型 on x" href="https://x.com/intent/tweet/?text=%e6%9e%84%e5%bb%ba%e7%89%b9%e5%ae%9a%e9%a2%86%e5%9f%9f%e7%9a%84%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-05-domain-llm-training%2f&amp;hashtags=AI%2cNLP%2cLLM%2cPre-training%2cPost-training%2cDPO%2c%e9%a2%86%e5%9f%9f%e6%a8%a1%e5%9e%8b%2cDeepSpeed"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 构建特定领域的大语言模型 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-05-domain-llm-training%2f&amp;title=%e6%9e%84%e5%bb%ba%e7%89%b9%e5%ae%9a%e9%a2%86%e5%9f%9f%e7%9a%84%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b&amp;summary=%e6%9e%84%e5%bb%ba%e7%89%b9%e5%ae%9a%e9%a2%86%e5%9f%9f%e7%9a%84%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-05-domain-llm-training%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 构建特定领域的大语言模型 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-05-domain-llm-training%2f&title=%e6%9e%84%e5%bb%ba%e7%89%b9%e5%ae%9a%e9%a2%86%e5%9f%9f%e7%9a%84%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 构建特定领域的大语言模型 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-05-domain-llm-training%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 构建特定领域的大语言模型 on whatsapp" href="https://api.whatsapp.com/send?text=%e6%9e%84%e5%bb%ba%e7%89%b9%e5%ae%9a%e9%a2%86%e5%9f%9f%e7%9a%84%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-05-domain-llm-training%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 构建特定领域的大语言模型 on telegram" href="https://telegram.me/share/url?text=%e6%9e%84%e5%bb%ba%e7%89%b9%e5%ae%9a%e9%a2%86%e5%9f%9f%e7%9a%84%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-05-domain-llm-training%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 构建特定领域的大语言模型 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e6%9e%84%e5%bb%ba%e7%89%b9%e5%ae%9a%e9%a2%86%e5%9f%9f%e7%9a%84%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-05-domain-llm-training%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>