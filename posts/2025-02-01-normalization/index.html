<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Normalization in Deep Learning | Yue Shui Blog</title><meta name=keywords content="AI,NLP,Deep Learning,Normalization,Residual Connection,ResNet,Batch Normalization,Layer Normalization,Weight Normalization,RMS Normalization,Pre-Norm,Post-Norm,LLM"><meta name=description content="Introduction
In deep learning, the design of network architectures significantly impacts model performance and training efficiency. As model depth increases, training deep neural networks faces numerous challenges, such as the vanishing and exploding gradient problems. To address these challenges, residual connections and various normalization methods have been introduced and are widely used in modern deep learning models. This article will first introduce residual connections and two architectures: pre-norm and post-norm. Then, it will describe four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze why current mainstream large models tend to adopt an architecture combining RMSNorm and Pre-Norm."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-02-01-normalization/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-02-01-normalization/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-02-01-normalization/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-02-01-normalization/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Normalization in Deep Learning"><meta property="og:description" content="Introduction In deep learning, the design of network architectures significantly impacts model performance and training efficiency. As model depth increases, training deep neural networks faces numerous challenges, such as the vanishing and exploding gradient problems. To address these challenges, residual connections and various normalization methods have been introduced and are widely used in modern deep learning models. This article will first introduce residual connections and two architectures: pre-norm and post-norm. Then, it will describe four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze why current mainstream large models tend to adopt an architecture combining RMSNorm and Pre-Norm."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-01T12:00:00+08:00"><meta property="article:modified_time" content="2025-02-01T12:00:00+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Normalization"><meta property="article:tag" content="Residual Connection"><meta property="article:tag" content="ResNet"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Normalization in Deep Learning"><meta name=twitter:description content="Introduction
In deep learning, the design of network architectures significantly impacts model performance and training efficiency. As model depth increases, training deep neural networks faces numerous challenges, such as the vanishing and exploding gradient problems. To address these challenges, residual connections and various normalization methods have been introduced and are widely used in modern deep learning models. This article will first introduce residual connections and two architectures: pre-norm and post-norm. Then, it will describe four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze why current mainstream large models tend to adopt an architecture combining RMSNorm and Pre-Norm."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Normalization in Deep Learning","item":"https://syhya.github.io/posts/2025-02-01-normalization/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Normalization in Deep Learning","name":"Normalization in Deep Learning","description":"Introduction In deep learning, the design of network architectures significantly impacts model performance and training efficiency. As model depth increases, training deep neural networks faces numerous challenges, such as the vanishing and exploding gradient problems. To address these challenges, residual connections and various normalization methods have been introduced and are widely used in modern deep learning models. This article will first introduce residual connections and two architectures: pre-norm and post-norm. Then, it will describe four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze why current mainstream large models tend to adopt an architecture combining RMSNorm and Pre-Norm.\n","keywords":["AI","NLP","Deep Learning","Normalization","Residual Connection","ResNet","Batch Normalization","Layer Normalization","Weight Normalization","RMS Normalization","Pre-Norm","Post-Norm","LLM"],"articleBody":"Introduction In deep learning, the design of network architectures significantly impacts model performance and training efficiency. As model depth increases, training deep neural networks faces numerous challenges, such as the vanishing and exploding gradient problems. To address these challenges, residual connections and various normalization methods have been introduced and are widely used in modern deep learning models. This article will first introduce residual connections and two architectures: pre-norm and post-norm. Then, it will describe four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze why current mainstream large models tend to adopt an architecture combining RMSNorm and Pre-Norm.\nResidual Connections Residual Connection is a crucial innovation in deep neural networks, forming the core of Residual Networks (ResNet) (He, et al., 2015). Residual connections are a significant architectural design aimed at mitigating the vanishing gradient problem in deep network training and facilitating information flow within the network. By introducing shortcut/skip connections, they allow information to pass directly from shallow layers to deeper layers, thereby enhancing the model’s representational capacity and training stability.\nFig. 1. Residual learning: a building block. (Image source: He, et al., 2015)\nIn a standard residual connection, the input $x_l$ undergoes a series of transformation functions $ \\text{F}(\\cdot) $ and is then added to the original input $x_l$ to form the output $x_{l+1}$:\n$$ x_{l+1} = x_l + \\text{F}(x_l) $$Where:\n$x_l$ is the input to the $l$-th layer. $\\text{F}(x_l)$ represents the residual function composed of a series of non-linear transformations (e.g., convolutional layers, fully connected layers, activation functions, etc.). $x_{l+1}$ is the output of the $(l+1)$-th layer. The structure using residual connections has several advantages:\nMitigation of Vanishing Gradients: By directly passing gradients through shortcut paths, it effectively reduces gradient decay in deep networks, making it easier to train deeper models. Facilitation of Information Flow: Shortcut paths allow information to flow more freely between network layers, helping the network learn more complex feature representations. Optimization of the Learning Process: Residual connections make the loss function surface smoother, optimizing the model’s learning process and making it easier to converge to a better solution. Improvement of Model Performance: In various deep learning tasks such as image recognition and natural language processing, models using residual connections typically exhibit superior performance. Pre-Norm vs. Post-Norm When discussing normalization methods, Pre-Norm and Post-Norm are two critical architectural design choices, particularly prominent in Transformer models. The following will detail the definitions, differences, and impacts of both on model training.\nDefinitions Fig. 2. (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer. (Image source: Xiong, et al., 2020)\nFrom the figure above, we can intuitively see that the main difference between Post-Norm and Pre-Norm lies in the position of the normalization layer:\nPost-Norm: In traditional Transformer architectures, the normalization layer (such as LayerNorm) is typically placed after the residual connection.\n$$ \\text{Post-Norm}: \\quad x_{l+1} = \\text{Norm}(x_l + \\text{F}(x_l)) $$ Pre-Norm: Places the normalization layer before the residual connection.\n$$ \\text{Pre-Norm}: \\quad x_{l+1} = x_l + \\text{F}(\\text{Norm}(x_l)) $$ Comparative Analysis Feature Post-Norm Pre-Norm Normalization Position After residual connection Before residual connection Gradient Flow May lead to vanishing or exploding gradients, especially in deep models More stable gradients, helps in training deep models Training Stability Difficult to train deep models, requires complex optimization techniques Easier to train deep models, reduces reliance on learning rate scheduling Information Transfer Retains characteristics of the original input, aiding information transfer May cause compression or loss of input feature information Model Performance Performs better in shallow models or when strong regularization is needed Performs better in deep models, improves training stability and convergence speed Implementation Complexity Relatively straightforward to implement, but training may require more tuning Simple to implement, training process is more stable The differences between Pre-Norm and Post-Norm in model training can be understood from the perspective of gradient backpropagation:\nPre-Norm: Normalization operation is performed first, allowing gradients to be passed more directly to the preceding layers during backpropagation, reducing the risk of vanishing gradients. However, this may also weaken the actual contribution of each layer, reducing the effective depth of the model.\nPost-Norm: Normalization operation is performed last, helping to maintain the stability of each layer’s output, but in deep models, gradients may decay layer by layer, leading to training difficulties.\nThe DeepNet (Wang, et al., 2022) paper indicates that Pre-Norm is effective for training extremely deep Transformer models, while Post-Norm is difficult to scale to such depths.\nNormalization Methods In deep learning, there are numerous types of normalization methods, and different methods perform differently in various application scenarios. The following will detail four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze their advantages, disadvantages, and applicable scenarios.\nBatch Normalization Batch Normalization (Ioffe, et al., 2015) aims to alleviate the Internal Covariate Shift problem by standardizing the data of each batch, making its mean 0 and variance 1. Its mathematical expression is as follows:\n$$ \\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta $$Where:\n$x_i$ is the $i$-th sample in the input vector. $\\mu_{\\text{B}}$ is the mean of the current batch: $$ \\mu_{\\text{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i $$ where $m$ is the batch size. $\\sigma_{\\text{B}}^2$ is the variance of the current batch: $$ \\sigma_{\\text{B}}^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\left(x_i - \\mu_{\\text{B}}\\right)^2 $$ $\\epsilon$ is a very small constant used to prevent division by zero. $\\gamma$ and $\\beta$ are learnable scaling and shifting parameters. Advantages:\nAccelerated Training: Accelerates the convergence speed of the model through standardization. Regularization Effect: Reduces overfitting to some extent, decreasing the reliance on regularization techniques like Dropout. Mitigation of Vanishing Gradient Problem: Helps alleviate vanishing gradients, improving the training effect of deep networks. Disadvantages:\nNot Friendly to Small Batches: When the batch size is small, the estimation of mean and variance may be unstable, affecting the normalization effect. Batch Size Dependent: Requires a large batch size to obtain good statistical estimates, limiting its use in certain application scenarios. Complex Application in Certain Network Structures: Such as Recurrent Neural Networks (RNNs), requiring special handling to adapt to the dependency of time steps. Layer Normalization Layer Normalization (Ba, et al., 2016) normalizes across the feature dimension, making the features of each sample have the same mean and variance. Its mathematical expression is as follows:\n$$ \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta $$Where:\n$x$ is the input vector. $\\mu_{\\text{L}}$ is the mean across the feature dimension: $$ \\mu_{\\text{L}} = \\frac{1}{d} \\sum_{i=1}^{d} x_i $$ where $d$ is the size of the feature dimension. $\\sigma_{\\text{L}}^2$ is the variance across the feature dimension: $$ \\sigma_{\\text{L}}^2 = \\frac{1}{d} \\sum_{i=1}^{d} \\left(x_i - \\mu_{\\text{L}}\\right)^2 $$ $\\epsilon$ is a very small constant used to prevent division by zero. $\\gamma$ and $\\beta$ are learnable scaling and shifting parameters. Advantages:\nBatch Size Independent: Suitable for scenarios with small batch sizes or dynamic batch sizes, especially performing excellently in sequence models. Applicable to Various Network Structures: Performs well in Recurrent Neural Networks (RNNs) and Transformer models. Simplified Implementation: No need to rely on batch statistics, simplifying implementation in distributed training. Disadvantages:\nHigher Computational Cost: Compared to BatchNorm, the overhead of calculating mean and variance is slightly higher. May Not Improve Training Speed as Much as BatchNorm: In some cases, the effect of LayerNorm may not be as significant as BatchNorm. Weight Normalization Weight Normalization (Salimans, et al., 2016) decouples the norm and direction of the weight vector in neural networks by reparameterizing it, thereby simplifying the optimization process and accelerating training to some extent. Its mathematical expression is as follows:\n$$ w = \\frac{g}{\\lVert v \\rVert} \\cdot v $$$$ \\text{WeightNorm}(x) = w^T x + b $$Where:\n$w$ is the reparameterized weight vector. $g$ is a learnable scalar scaling parameter. $v$ is a learnable direction vector (with the same dimension as the original $w$). $\\lVert v \\rVert$ represents the Euclidean norm of $v$. $x$ is the input vector. $b$ is the bias term. Advantages:\nSimplified Optimization Objective: Separately controlling the norm and direction of weights helps accelerate convergence. Stable Training Process: In some cases, it can reduce gradient explosion or vanishing problems. Implementation Independent of Batch Size: Unrelated to the batch size of input data, broader applicability. Disadvantages:\nImplementation Complexity: Requires reparameterization of network layers, which may bring additional implementation costs. Caution Needed When Combined with Other Normalization Methods: When used in conjunction with BatchNorm, LayerNorm, etc., debugging and experimentation are needed to determine the best combination. RMS Normalization RMS Normalization (Zhang, et al., 2019) is a simplified normalization method that normalizes by only calculating the Root Mean Square (RMS) of the input vector, thereby reducing computational overhead. Its mathematical expression is as follows:\n$$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$Where:\n$x$ is the input vector. $d$ is the size of the feature dimension. $\\epsilon$ is a very small constant used to prevent division by zero. $\\gamma$ is a learnable scaling parameter. Advantages:\nHigh Computational Efficiency: Compared to LayerNorm, which requires calculating both mean and variance, RMSNorm only needs to calculate the root mean square, reducing computational overhead. Training Stability: By normalizing the input, it improves the training stability of the model, allowing it to train stably even with larger learning rates. Resource Optimization: Reduced computational overhead helps deploy models in resource-constrained environments, improving training and inference efficiency. Simplified Implementation: RMSNorm is relatively simple to implement, making it easy to integrate and optimize in complex models, reducing the complexity of engineering implementation. Disadvantages:\nInformation Loss: Using only the root mean square for normalization may lose some information, such as mean information. Limited Applicability: In some tasks, it may not perform as well as BatchNorm or LayerNorm. Code Example You can refer to normalization.py\nComparison of Normalization Methods The following two tables compare the main characteristics of BatchNorm, LayerNorm, WeightNorm, and RMSNorm.\nBatchNorm vs. LayerNorm Feature BatchNorm (BN) LayerNorm (LN) Calculated Statistics Batch mean and variance Per-sample mean and variance Operation Dimension Normalizes across all samples in a batch Normalizes across all features for each sample Applicable Scenarios Suitable for large batch data, Convolutional Neural Networks (CNNs) Suitable for small batch or sequential data, RNNs or Transformers Batch Size Dependency Strongly dependent on batch size Independent of batch size, suitable for small batch or single-sample tasks Learnable Parameters Scaling parameter $ \\gamma $ and shifting parameter $ \\beta $ Scaling parameter $ \\gamma $ and shifting parameter $ \\beta $ Formula $ \\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta $ $ \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta $ Computational Complexity Requires calculating batch mean and variance Requires calculating per-sample mean and variance Example Usage CNN, Vision Transformers RNN, Transformer, NLP WeightNorm vs. RMSNorm Feature WeightNorm (WN) RMSNorm (RMS) Calculated Statistics Decomposes weight vector into norm and direction Root Mean Square (RMS) of each sample Operation Dimension Reparameterizes along the dimension of the weight vector Normalizes across all features for each sample Applicable Scenarios Suitable for scenarios requiring more flexible weight control or accelerated convergence Suitable for tasks requiring efficient computation, such as RNNs or Transformers Batch Size Dependency Independent of batch size, unrelated to the dimension of input data Independent of batch size, suitable for small batch or single-sample tasks Learnable Parameters Scalar scaling $g$ and direction vector $v$ Scaling parameter $ \\gamma $ Formula $ \\text{WeightNorm}(x) = w^T x + b $ $ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $ Computational Complexity Reparameterization and update of parameters, slightly higher overhead, but requires modifying network layer implementation Only needs to calculate the root mean square of each sample, computationally efficient Example Usage Fully connected layers, convolutional layers, etc., in deep networks, improving training stability and convergence speed Transformer, NLP, efficient sequence tasks Through the above comparison, it can be seen that the four normalization methods have their own advantages and disadvantages:\nBatchNorm performs excellently in large batch data and convolutional neural networks but is sensitive to small batches. LayerNorm is suitable for various batch sizes, especially effective in RNNs and Transformers. WeightNorm simplifies the optimization process and accelerates convergence to some extent by reparameterizing the weight vector. RMSNorm provides a lightweight alternative in scenarios requiring efficient computation. Why do current mainstream LLMs use Pre-Norm and RMSNorm? In recent years, with the rise of large-scale language models (LLMs) such as GPT, LLaMA, and the Qwen series, RMSNorm and Pre-Norm have become the standard choices for these models.\nAdvantages of RMSNorm Fig. 3. RMSNorm vs. LayerNorm: A Comparison of Time Consumption (Image source: Zhang, et al., 2019)\nHigher Computational Efficiency\nReduced Operations: Only needs to calculate the Root Mean Square (RMS) of the input vector, without calculating mean and variance. Faster Training Speed: In actual tests, RMSNorm significantly shortens training time (as shown in the figure, reduced from 665s to 501s), which is particularly evident in large-scale model training. More Stable Training\nAdaptable to Larger Learning Rates: While maintaining stability, it can use larger learning rates, accelerating model convergence. Maintains Expressive Power: By simplifying the normalization process with an appropriate scaling parameter $ \\gamma $, it still maintains model performance. Resource Saving\nReduced Hardware Requirements: Less computational overhead not only improves speed but also reduces the occupation of hardware resources, suitable for deployment in resource-constrained environments. Advantages of Pre-Norm Easier to Train Deep Models\nStable Gradient Propagation: Performing normalization before residual connections can effectively alleviate gradient vanishing or explosion. Reduced Reliance on Complex Optimization Techniques: Even if the model is very deep, the training process remains stable. Accelerated Model Convergence\nEfficient Gradient Flow: Pre-Norm makes it easier for gradients to propagate to preceding layers, resulting in faster overall convergence speed. Conclusion Residual connections and normalization methods play crucial roles in deep learning models. Different normalization methods and network architecture designs have their own applicable scenarios, advantages, and disadvantages. By introducing residual connections, ResNet successfully trained extremely deep networks, significantly improving model expressiveness and training efficiency. Meanwhile, normalization methods such as BatchNorm, LayerNorm, WeightNorm, and RMSNorm each offer different advantages, adapting to different application needs.\nAs model scales continue to expand, choosing appropriate normalization methods and network architecture designs becomes particularly important. RMSNorm, due to its efficient computation and good training stability, combined with the Pre-Norm architecture design, has become the preferred choice for current mainstream LLMs. This combination not only improves the training efficiency of models but also ensures training stability and performance under large-scale parameters.\nReferences [1] He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[2] Xiong, Ruibin, et al. “On layer normalization in the transformer architecture.” International Conference on Machine Learning. PMLR, 2020.\n[3] Wang, Hongyu, et al. “Deepnet: Scaling transformers to 1,000 layers.” IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).\n[4] Ioffe, Sergey. “Batch normalization: Accelerating deep network training by reducing internal covariate shift.” arXiv preprint arXiv:1502.03167 (2015).\n[5] Ba, Jimmy Lei. “Layer normalization.” arXiv preprint arXiv:1607.06450 (2016).\n[6] Salimans, Tim, and Durk P. Kingma. “Weight normalization: A simple reparameterization to accelerate training of deep neural networks.” Advances in neural information processing systems 29 (2016).\n[7] Zhang, Biao, and Rico Sennrich. “Root mean square layer normalization.” Advances in Neural Information Processing Systems 32 (2019).\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Feb 2025). Normalization in Deep Learning.\nhttps://syhya.github.io/posts/2025-02-01-normalization\nOr\n@article{syhya2025normalization, title = \"Normalization in Deep Learning\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Feb\", url = \"https://syhya.github.io/posts/2025-02-01-normalization\" } ","wordCount":"2576","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-02-01T12:00:00+08:00","dateModified":"2025-02-01T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-02-01-normalization/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Normalization in Deep Learning</h1><div class=post-meta><span title='2025-02-01 12:00:00 +0800 +0800'>Created:&nbsp;2025-02-01</span>&nbsp;·&nbsp;Updated:&nbsp;2025-02-01&nbsp;·&nbsp;13 min&nbsp;·&nbsp;2576 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-02-01-normalization/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#residual-connections>Residual Connections</a></li><li><a href=#pre-norm-vs-post-norm>Pre-Norm vs. Post-Norm</a><ul><li><a href=#definitions>Definitions</a></li><li><a href=#comparative-analysis>Comparative Analysis</a></li></ul></li><li><a href=#normalization-methods>Normalization Methods</a><ul><li><a href=#batch-normalization>Batch Normalization</a></li><li><a href=#layer-normalization>Layer Normalization</a></li><li><a href=#weight-normalization>Weight Normalization</a></li><li><a href=#rms-normalization>RMS Normalization</a></li><li><a href=#code-example>Code Example</a></li><li><a href=#comparison-of-normalization-methods>Comparison of Normalization Methods</a><ul><li><a href=#batchnorm-vs-layernorm>BatchNorm vs. LayerNorm</a></li><li><a href=#weightnorm-vs-rmsnorm>WeightNorm vs. RMSNorm</a></li></ul></li></ul></li><li><a href=#why-do-current-mainstream-llms-use-pre-norm-and-rmsnorm>Why do current mainstream LLMs use Pre-Norm and RMSNorm?</a><ul><li><a href=#advantages-of-rmsnorm>Advantages of RMSNorm</a></li><li><a href=#advantages-of-pre-norm>Advantages of Pre-Norm</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>In deep learning, the design of network architectures significantly impacts model performance and training efficiency. As model depth increases, training deep neural networks faces numerous challenges, such as the vanishing and exploding gradient problems. To address these challenges, residual connections and various normalization methods have been introduced and are widely used in modern deep learning models. This article will first introduce residual connections and two architectures: pre-norm and post-norm. Then, it will describe four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze why current mainstream large models tend to adopt an architecture combining <strong>RMSNorm</strong> and <strong>Pre-Norm</strong>.</p><h2 id=residual-connections>Residual Connections<a hidden class=anchor aria-hidden=true href=#residual-connections>#</a></h2><p><strong>Residual Connection</strong> is a crucial innovation in deep neural networks, forming the core of Residual Networks (ResNet) (<a href=https://arxiv.org/abs/1512.03385>He, et al., 2015</a>). Residual connections are a significant architectural design aimed at mitigating the vanishing gradient problem in deep network training and facilitating information flow within the network. By introducing shortcut/skip connections, they allow information to pass directly from shallow layers to deeper layers, thereby enhancing the model&rsquo;s representational capacity and training stability.</p><figure class=align-center><img loading=lazy src=residual_connection.png#center alt="Fig. 1. Residual learning: a building block. (Image source: He, et al., 2015)" width=70%><figcaption><p>Fig. 1. Residual learning: a building block. (Image source: <a href=https://arxiv.org/abs/1502.03167>He, et al., 2015</a>)</p></figcaption></figure><p>In a standard residual connection, the input $x_l$ undergoes a series of transformation functions $ \text{F}(\cdot) $ and is then added to the original input $x_l$ to form the output $x_{l+1}$:</p>$$
x_{l+1} = x_l + \text{F}(x_l)
$$<p>Where:</p><ul><li>$x_l$ is the input to the $l$-th layer.</li><li>$\text{F}(x_l)$ represents the residual function composed of a series of non-linear transformations (e.g., convolutional layers, fully connected layers, activation functions, etc.).</li><li>$x_{l+1}$ is the output of the $(l+1)$-th layer.</li></ul><p>The structure using residual connections has several advantages:</p><ul><li><strong>Mitigation of Vanishing Gradients</strong>: By directly passing gradients through shortcut paths, it effectively reduces gradient decay in deep networks, making it easier to train deeper models.</li><li><strong>Facilitation of Information Flow</strong>: Shortcut paths allow information to flow more freely between network layers, helping the network learn more complex feature representations.</li><li><strong>Optimization of the Learning Process</strong>: Residual connections make the loss function surface smoother, optimizing the model&rsquo;s learning process and making it easier to converge to a better solution.</li><li><strong>Improvement of Model Performance</strong>: In various deep learning tasks such as image recognition and natural language processing, models using residual connections typically exhibit superior performance.</li></ul><h2 id=pre-norm-vs-post-norm>Pre-Norm vs. Post-Norm<a hidden class=anchor aria-hidden=true href=#pre-norm-vs-post-norm>#</a></h2><p>When discussing normalization methods, <strong>Pre-Norm</strong> and <strong>Post-Norm</strong> are two critical architectural design choices, particularly prominent in Transformer models. The following will detail the definitions, differences, and impacts of both on model training.</p><h3 id=definitions>Definitions<a hidden class=anchor aria-hidden=true href=#definitions>#</a></h3><figure class=align-center><img loading=lazy src=pre_post_norm_comparison.png#center alt="Fig. 2. (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer. (Image source: Xiong, et al., 2020)" width=50%><figcaption><p>Fig. 2. (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer. (Image source: <a href=https://arxiv.org/abs/2002.04745>Xiong, et al., 2020</a>)</p></figcaption></figure><p>From the figure above, we can intuitively see that the main difference between Post-Norm and Pre-Norm lies in the position of the normalization layer:</p><ul><li><p><strong>Post-Norm</strong>: In traditional Transformer architectures, the normalization layer (such as LayerNorm) is typically placed after the residual connection.</p>$$
\text{Post-Norm}: \quad x_{l+1} = \text{Norm}(x_l + \text{F}(x_l))
$$</li><li><p><strong>Pre-Norm</strong>: Places the normalization layer before the residual connection.</p>$$
\text{Pre-Norm}: \quad x_{l+1} = x_l + \text{F}(\text{Norm}(x_l))
$$</li></ul><h3 id=comparative-analysis>Comparative Analysis<a hidden class=anchor aria-hidden=true href=#comparative-analysis>#</a></h3><table><thead><tr><th>Feature</th><th>Post-Norm</th><th>Pre-Norm</th></tr></thead><tbody><tr><td><strong>Normalization Position</strong></td><td>After residual connection</td><td>Before residual connection</td></tr><tr><td><strong>Gradient Flow</strong></td><td>May lead to vanishing or exploding gradients, especially in deep models</td><td>More stable gradients, helps in training deep models</td></tr><tr><td><strong>Training Stability</strong></td><td>Difficult to train deep models, requires complex optimization techniques</td><td>Easier to train deep models, reduces reliance on learning rate scheduling</td></tr><tr><td><strong>Information Transfer</strong></td><td>Retains characteristics of the original input, aiding information transfer</td><td>May cause compression or loss of input feature information</td></tr><tr><td><strong>Model Performance</strong></td><td>Performs better in shallow models or when strong regularization is needed</td><td>Performs better in deep models, improves training stability and convergence speed</td></tr><tr><td><strong>Implementation Complexity</strong></td><td>Relatively straightforward to implement, but training may require more tuning</td><td>Simple to implement, training process is more stable</td></tr></tbody></table><p>The differences between Pre-Norm and Post-Norm in model training can be understood from the perspective of gradient backpropagation:</p><ul><li><p><strong>Pre-Norm</strong>: Normalization operation is performed first, allowing gradients to be passed more directly to the preceding layers during backpropagation, reducing the risk of vanishing gradients. However, this may also weaken the actual contribution of each layer, reducing the effective depth of the model.</p></li><li><p><strong>Post-Norm</strong>: Normalization operation is performed last, helping to maintain the stability of each layer&rsquo;s output, but in deep models, gradients may decay layer by layer, leading to training difficulties.</p></li></ul><p>The <strong>DeepNet</strong> (<a href=https://arxiv.org/abs/2203.00555>Wang, et al., 2022</a>) paper indicates that Pre-Norm is effective for training extremely deep Transformer models, while Post-Norm is difficult to scale to such depths.</p><h2 id=normalization-methods>Normalization Methods<a hidden class=anchor aria-hidden=true href=#normalization-methods>#</a></h2><p>In deep learning, there are numerous types of normalization methods, and different methods perform differently in various application scenarios. The following will detail four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze their advantages, disadvantages, and applicable scenarios.</p><h3 id=batch-normalization>Batch Normalization<a hidden class=anchor aria-hidden=true href=#batch-normalization>#</a></h3><p>Batch Normalization (<a href=https://arxiv.org/abs/1502.03167>Ioffe, et al., 2015</a>) aims to alleviate the Internal Covariate Shift problem by standardizing the data of each batch, making its mean 0 and variance 1. Its mathematical expression is as follows:</p>$$
\text{BatchNorm}(x_i) = \gamma \cdot \frac{x_i - \mu_{\text{B}}}{\sqrt{\sigma_{\text{B}}^2 + \epsilon}} + \beta
$$<p>Where:</p><ul><li>$x_i$ is the $i$-th sample in the input vector.</li><li>$\mu_{\text{B}}$ is the mean of the current batch:
$$
\mu_{\text{B}} = \frac{1}{m} \sum_{i=1}^{m} x_i
$$
where $m$ is the batch size.</li><li>$\sigma_{\text{B}}^2$ is the variance of the current batch:
$$
\sigma_{\text{B}}^2 = \frac{1}{m} \sum_{i=1}^{m} \left(x_i - \mu_{\text{B}}\right)^2
$$</li><li>$\epsilon$ is a very small constant used to prevent division by zero.</li><li>$\gamma$ and $\beta$ are learnable scaling and shifting parameters.</li></ul><p><strong>Advantages:</strong></p><ul><li><strong>Accelerated Training</strong>: Accelerates the convergence speed of the model through standardization.</li><li><strong>Regularization Effect</strong>: Reduces overfitting to some extent, decreasing the reliance on regularization techniques like Dropout.</li><li><strong>Mitigation of Vanishing Gradient Problem</strong>: Helps alleviate vanishing gradients, improving the training effect of deep networks.</li></ul><p><strong>Disadvantages:</strong></p><ul><li><strong>Not Friendly to Small Batches</strong>: When the batch size is small, the estimation of mean and variance may be unstable, affecting the normalization effect.</li><li><strong>Batch Size Dependent</strong>: Requires a large batch size to obtain good statistical estimates, limiting its use in certain application scenarios.</li><li><strong>Complex Application in Certain Network Structures</strong>: Such as Recurrent Neural Networks (RNNs), requiring special handling to adapt to the dependency of time steps.</li></ul><h3 id=layer-normalization>Layer Normalization<a hidden class=anchor aria-hidden=true href=#layer-normalization>#</a></h3><p>Layer Normalization (<a href=https://arxiv.org/abs/1607.06450>Ba, et al., 2016</a>) normalizes across the feature dimension, making the features of each sample have the same mean and variance. Its mathematical expression is as follows:</p>$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu_{\text{L}}}{\sqrt{\sigma_{\text{L}}^2 + \epsilon}} + \beta
$$<p>Where:</p><ul><li>$x$ is the input vector.</li><li>$\mu_{\text{L}}$ is the mean across the feature dimension:
$$
\mu_{\text{L}} = \frac{1}{d} \sum_{i=1}^{d} x_i
$$
where $d$ is the size of the feature dimension.</li><li>$\sigma_{\text{L}}^2$ is the variance across the feature dimension:
$$
\sigma_{\text{L}}^2 = \frac{1}{d} \sum_{i=1}^{d} \left(x_i - \mu_{\text{L}}\right)^2
$$</li><li>$\epsilon$ is a very small constant used to prevent division by zero.</li><li>$\gamma$ and $\beta$ are learnable scaling and shifting parameters.</li></ul><p><strong>Advantages:</strong></p><ul><li><strong>Batch Size Independent</strong>: Suitable for scenarios with small batch sizes or dynamic batch sizes, especially performing excellently in sequence models.</li><li><strong>Applicable to Various Network Structures</strong>: Performs well in Recurrent Neural Networks (RNNs) and Transformer models.</li><li><strong>Simplified Implementation</strong>: No need to rely on batch statistics, simplifying implementation in distributed training.</li></ul><p><strong>Disadvantages:</strong></p><ul><li><strong>Higher Computational Cost</strong>: Compared to BatchNorm, the overhead of calculating mean and variance is slightly higher.</li><li><strong>May Not Improve Training Speed as Much as BatchNorm</strong>: In some cases, the effect of LayerNorm may not be as significant as BatchNorm.</li></ul><h3 id=weight-normalization>Weight Normalization<a hidden class=anchor aria-hidden=true href=#weight-normalization>#</a></h3><p>Weight Normalization (<a href=https://arxiv.org/abs/1602.07868>Salimans, et al., 2016</a>) decouples the norm and direction of the weight vector in neural networks by reparameterizing it, thereby simplifying the optimization process and accelerating training to some extent. Its mathematical expression is as follows:</p>$$
w = \frac{g}{\lVert v \rVert} \cdot v
$$$$
\text{WeightNorm}(x) = w^T x + b
$$<p>Where:</p><ul><li>$w$ is the reparameterized weight vector.</li><li>$g$ is a learnable scalar scaling parameter.</li><li>$v$ is a learnable direction vector (with the same dimension as the original $w$).</li><li>$\lVert v \rVert$ represents the Euclidean norm of $v$.</li><li>$x$ is the input vector.</li><li>$b$ is the bias term.</li></ul><p><strong>Advantages:</strong></p><ul><li><strong>Simplified Optimization Objective</strong>: Separately controlling the norm and direction of weights helps accelerate convergence.</li><li><strong>Stable Training Process</strong>: In some cases, it can reduce gradient explosion or vanishing problems.</li><li><strong>Implementation Independent of Batch Size</strong>: Unrelated to the batch size of input data, broader applicability.</li></ul><p><strong>Disadvantages:</strong></p><ul><li><strong>Implementation Complexity</strong>: Requires reparameterization of network layers, which may bring additional implementation costs.</li><li><strong>Caution Needed When Combined with Other Normalization Methods</strong>: When used in conjunction with BatchNorm, LayerNorm, etc., debugging and experimentation are needed to determine the best combination.</li></ul><h3 id=rms-normalization>RMS Normalization<a hidden class=anchor aria-hidden=true href=#rms-normalization>#</a></h3><p>RMS Normalization (<a href=https://arxiv.org/abs/1910.07467>Zhang, et al., 2019</a>) is a simplified normalization method that normalizes by only calculating the Root Mean Square (RMS) of the input vector, thereby reducing computational overhead. Its mathematical expression is as follows:</p>$$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2 + \epsilon}} \cdot \gamma
$$<p>Where:</p><ul><li>$x$ is the input vector.</li><li>$d$ is the size of the feature dimension.</li><li>$\epsilon$ is a very small constant used to prevent division by zero.</li><li>$\gamma$ is a learnable scaling parameter.</li></ul><p><strong>Advantages:</strong></p><ul><li><strong>High Computational Efficiency</strong>: Compared to LayerNorm, which requires calculating both mean and variance, RMSNorm only needs to calculate the root mean square, reducing computational overhead.</li><li><strong>Training Stability</strong>: By normalizing the input, it improves the training stability of the model, allowing it to train stably even with larger learning rates.</li><li><strong>Resource Optimization</strong>: Reduced computational overhead helps deploy models in resource-constrained environments, improving training and inference efficiency.</li><li><strong>Simplified Implementation</strong>: RMSNorm is relatively simple to implement, making it easy to integrate and optimize in complex models, reducing the complexity of engineering implementation.</li></ul><p><strong>Disadvantages:</strong></p><ul><li><strong>Information Loss</strong>: Using only the root mean square for normalization may lose some information, such as mean information.</li><li><strong>Limited Applicability</strong>: In some tasks, it may not perform as well as BatchNorm or LayerNorm.</li></ul><h3 id=code-example>Code Example<a hidden class=anchor aria-hidden=true href=#code-example>#</a></h3><p>You can refer to <a href=https://github.com/syhya/syhya.github.io/blob/main/content/en/posts/2025-02-01-normalization/normalization.py>normalization.py</a></p><h3 id=comparison-of-normalization-methods>Comparison of Normalization Methods<a hidden class=anchor aria-hidden=true href=#comparison-of-normalization-methods>#</a></h3><p>The following two tables compare the main characteristics of BatchNorm, LayerNorm, WeightNorm, and RMSNorm.</p><h4 id=batchnorm-vs-layernorm>BatchNorm vs. LayerNorm<a hidden class=anchor aria-hidden=true href=#batchnorm-vs-layernorm>#</a></h4><table><thead><tr><th>Feature</th><th>BatchNorm (BN)</th><th>LayerNorm (LN)</th></tr></thead><tbody><tr><td><strong>Calculated Statistics</strong></td><td>Batch mean and variance</td><td>Per-sample mean and variance</td></tr><tr><td><strong>Operation Dimension</strong></td><td>Normalizes across all samples in a batch</td><td>Normalizes across all features for each sample</td></tr><tr><td><strong>Applicable Scenarios</strong></td><td>Suitable for large batch data, Convolutional Neural Networks (CNNs)</td><td>Suitable for small batch or sequential data, RNNs or Transformers</td></tr><tr><td><strong>Batch Size Dependency</strong></td><td>Strongly dependent on batch size</td><td>Independent of batch size, suitable for small batch or single-sample tasks</td></tr><tr><td><strong>Learnable Parameters</strong></td><td>Scaling parameter $ \gamma $ and shifting parameter $ \beta $</td><td>Scaling parameter $ \gamma $ and shifting parameter $ \beta $</td></tr><tr><td><strong>Formula</strong></td><td>$ \text{BatchNorm}(x_i) = \gamma \cdot \frac{x_i - \mu_{\text{B}}}{\sqrt{\sigma_{\text{B}}^2 + \epsilon}} + \beta $</td><td>$ \text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu_{\text{L}}}{\sqrt{\sigma_{\text{L}}^2 + \epsilon}} + \beta $</td></tr><tr><td><strong>Computational Complexity</strong></td><td>Requires calculating batch mean and variance</td><td>Requires calculating per-sample mean and variance</td></tr><tr><td><strong>Example Usage</strong></td><td>CNN, Vision Transformers</td><td>RNN, Transformer, NLP</td></tr></tbody></table><h4 id=weightnorm-vs-rmsnorm>WeightNorm vs. RMSNorm<a hidden class=anchor aria-hidden=true href=#weightnorm-vs-rmsnorm>#</a></h4><table><thead><tr><th>Feature</th><th>WeightNorm (WN)</th><th>RMSNorm (RMS)</th></tr></thead><tbody><tr><td><strong>Calculated Statistics</strong></td><td>Decomposes weight vector into norm and direction</td><td>Root Mean Square (RMS) of each sample</td></tr><tr><td><strong>Operation Dimension</strong></td><td>Reparameterizes along the dimension of the weight vector</td><td>Normalizes across all features for each sample</td></tr><tr><td><strong>Applicable Scenarios</strong></td><td>Suitable for scenarios requiring more flexible weight control or accelerated convergence</td><td>Suitable for tasks requiring efficient computation, such as RNNs or Transformers</td></tr><tr><td><strong>Batch Size Dependency</strong></td><td>Independent of batch size, unrelated to the dimension of input data</td><td>Independent of batch size, suitable for small batch or single-sample tasks</td></tr><tr><td><strong>Learnable Parameters</strong></td><td>Scalar scaling $g$ and direction vector $v$</td><td>Scaling parameter $ \gamma $</td></tr><tr><td><strong>Formula</strong></td><td>$ \text{WeightNorm}(x) = w^T x + b $</td><td>$ \text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2 + \epsilon}} \cdot \gamma $</td></tr><tr><td><strong>Computational Complexity</strong></td><td>Reparameterization and update of parameters, slightly higher overhead, but requires modifying network layer implementation</td><td>Only needs to calculate the root mean square of each sample, computationally efficient</td></tr><tr><td><strong>Example Usage</strong></td><td>Fully connected layers, convolutional layers, etc., in deep networks, improving training stability and convergence speed</td><td>Transformer, NLP, efficient sequence tasks</td></tr></tbody></table><p>Through the above comparison, it can be seen that the four normalization methods have their own advantages and disadvantages:</p><ul><li><strong>BatchNorm</strong> performs excellently in large batch data and convolutional neural networks but is sensitive to small batches.</li><li><strong>LayerNorm</strong> is suitable for various batch sizes, especially effective in RNNs and Transformers.</li><li><strong>WeightNorm</strong> simplifies the optimization process and accelerates convergence to some extent by reparameterizing the weight vector.</li><li><strong>RMSNorm</strong> provides a lightweight alternative in scenarios requiring efficient computation.</li></ul><h2 id=why-do-current-mainstream-llms-use-pre-norm-and-rmsnorm>Why do current mainstream LLMs use Pre-Norm and RMSNorm?<a hidden class=anchor aria-hidden=true href=#why-do-current-mainstream-llms-use-pre-norm-and-rmsnorm>#</a></h2><p>In recent years, with the rise of large-scale language models (LLMs) such as GPT, LLaMA, and the Qwen series, <strong>RMSNorm</strong> and <strong>Pre-Norm</strong> have become the standard choices for these models.</p><h3 id=advantages-of-rmsnorm>Advantages of RMSNorm<a hidden class=anchor aria-hidden=true href=#advantages-of-rmsnorm>#</a></h3><figure class=align-center><img loading=lazy src=rms_norm_time_benchmark.png#center alt="Fig. 3. RMSNorm vs. LayerNorm: A Comparison of Time Consumption (Image source: Zhang, et al., 2019)" width=80%><figcaption><p>Fig. 3. RMSNorm vs. LayerNorm: A Comparison of Time Consumption (Image source: <a href=https://arxiv.org/abs/1910.07467>Zhang, et al., 2019</a>)</p></figcaption></figure><ol><li><p><strong>Higher Computational Efficiency</strong></p><ul><li><strong>Reduced Operations</strong>: Only needs to calculate the Root Mean Square (RMS) of the input vector, without calculating mean and variance.</li><li><strong>Faster Training Speed</strong>: In actual tests, RMSNorm significantly shortens training time (as shown in the figure, reduced from <strong>665s</strong> to <strong>501s</strong>), which is particularly evident in large-scale model training.</li></ul></li><li><p><strong>More Stable Training</strong></p><ul><li><strong>Adaptable to Larger Learning Rates</strong>: While maintaining stability, it can use larger learning rates, accelerating model convergence.</li><li><strong>Maintains Expressive Power</strong>: By simplifying the normalization process with an appropriate scaling parameter $ \gamma $, it still maintains model performance.</li></ul></li><li><p><strong>Resource Saving</strong></p><ul><li><strong>Reduced Hardware Requirements</strong>: Less computational overhead not only improves speed but also reduces the occupation of hardware resources, suitable for deployment in resource-constrained environments.</li></ul></li></ol><h3 id=advantages-of-pre-norm>Advantages of Pre-Norm<a hidden class=anchor aria-hidden=true href=#advantages-of-pre-norm>#</a></h3><ol><li><p><strong>Easier to Train Deep Models</strong></p><ul><li><strong>Stable Gradient Propagation</strong>: Performing normalization before residual connections can effectively alleviate gradient vanishing or explosion.</li><li><strong>Reduced Reliance on Complex Optimization Techniques</strong>: Even if the model is very deep, the training process remains stable.</li></ul></li><li><p><strong>Accelerated Model Convergence</strong></p><ul><li><strong>Efficient Gradient Flow</strong>: Pre-Norm makes it easier for gradients to propagate to preceding layers, resulting in faster overall convergence speed.</li></ul></li></ol><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Residual connections and normalization methods play crucial roles in deep learning models. Different normalization methods and network architecture designs have their own applicable scenarios, advantages, and disadvantages. By introducing residual connections, ResNet successfully trained extremely deep networks, significantly improving model expressiveness and training efficiency. Meanwhile, normalization methods such as BatchNorm, LayerNorm, WeightNorm, and RMSNorm each offer different advantages, adapting to different application needs.</p><p>As model scales continue to expand, choosing appropriate normalization methods and network architecture designs becomes particularly important. <strong>RMSNorm</strong>, due to its efficient computation and good training stability, combined with the <strong>Pre-Norm</strong> architecture design, has become the preferred choice for current mainstream LLMs. This combination not only improves the training efficiency of models but also ensures training stability and performance under large-scale parameters.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] He, Kaiming, et al. <a href=https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf>&ldquo;Deep residual learning for image recognition.&rdquo;</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</p><p>[2] Xiong, Ruibin, et al. <a href=https://arxiv.org/abs/2002.04745>&ldquo;On layer normalization in the transformer architecture.&rdquo;</a> International Conference on Machine Learning. PMLR, 2020.</p><p>[3] Wang, Hongyu, et al. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496231">&ldquo;Deepnet: Scaling transformers to 1,000 layers.&rdquo;</a> IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).</p><p>[4] Ioffe, Sergey. <a href=https://arxiv.org/abs/1502.03167>&ldquo;Batch normalization: Accelerating deep network training by reducing internal covariate shift.&rdquo;</a> arXiv preprint arXiv:1502.03167 (2015).</p><p>[5] Ba, Jimmy Lei. <a href=https://arxiv.org/abs/1607.06450>&ldquo;Layer normalization.&rdquo;</a> arXiv preprint arXiv:1607.06450 (2016).</p><p>[6] Salimans, Tim, and Durk P. Kingma. <a href=https://proceedings.neurips.cc/paper_files/paper/2016/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf>&ldquo;Weight normalization: A simple reparameterization to accelerate training of deep neural networks.&rdquo;</a> Advances in neural information processing systems 29 (2016).</p><p>[7] Zhang, Biao, and Rico Sennrich. <a href=https://arxiv.org/abs/1910.07467>&ldquo;Root mean square layer normalization.&rdquo;</a> Advances in Neural Information Processing Systems 32 (2019).</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reprinting or citing the content of this article, please indicate the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Feb 2025). Normalization in Deep Learning.<br><a href=https://syhya.github.io/posts/2025-02-01-normalization>https://syhya.github.io/posts/2025-02-01-normalization</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025normalization</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;Normalization in Deep Learning&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Feb&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-02-01-normalization&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/nlp/>NLP</a></li><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/tags/normalization/>Normalization</a></li><li><a href=https://syhya.github.io/tags/residual-connection/>Residual Connection</a></li><li><a href=https://syhya.github.io/tags/resnet/>ResNet</a></li><li><a href=https://syhya.github.io/tags/batch-normalization/>Batch Normalization</a></li><li><a href=https://syhya.github.io/tags/layer-normalization/>Layer Normalization</a></li><li><a href=https://syhya.github.io/tags/weight-normalization/>Weight Normalization</a></li><li><a href=https://syhya.github.io/tags/rms-normalization/>RMS Normalization</a></li><li><a href=https://syhya.github.io/tags/pre-norm/>Pre-Norm</a></li><li><a href=https://syhya.github.io/tags/post-norm/>Post-Norm</a></li><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-02-08-dpo/><span class=title>« Prev</span><br><span>LLMs Alignment: DPO</span>
</a><a class=next href=https://syhya.github.io/posts/rl-introduction/><span class=title>Next »</span><br><span>Deep Reinforcement Learning (Ongoing Updates)</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Normalization in Deep Learning on x" href="https://x.com/intent/tweet/?text=Normalization%20in%20Deep%20Learning&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-01-normalization%2f&amp;hashtags=AI%2cNLP%2cDeepLearning%2cNormalization%2cResidualConnection%2cResNet%2cBatchNormalization%2cLayerNormalization%2cWeightNormalization%2cRMSNormalization%2cPre-Norm%2cPost-Norm%2cLLM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Normalization in Deep Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-01-normalization%2f&amp;title=Normalization%20in%20Deep%20Learning&amp;summary=Normalization%20in%20Deep%20Learning&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-01-normalization%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Normalization in Deep Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-01-normalization%2f&title=Normalization%20in%20Deep%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Normalization in Deep Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-01-normalization%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Normalization in Deep Learning on whatsapp" href="https://api.whatsapp.com/send?text=Normalization%20in%20Deep%20Learning%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-01-normalization%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Normalization in Deep Learning on telegram" href="https://telegram.me/share/url?text=Normalization%20in%20Deep%20Learning&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-01-normalization%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Normalization in Deep Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=Normalization%20in%20Deep%20Learning&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-01-normalization%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>