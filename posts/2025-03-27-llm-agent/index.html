<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Large Language Model Agents | Yue Shui Blog</title>
<meta name=keywords content="LLM,AI,Agent,Reinforcement Learning,Planning,Memory,Tool Use,Deep Research,ReAct,Reflexion,WebVoyager,OpenAI Operator,CoT,ToT,workflow"><meta name=description content="Agents
Since OpenAI released ChatGPT in October 2022, and with the subsequent emergence of projects such as AutoGPT and AgentGPT, LLM-related agents have gradually become a research hotspot and a promising direction for practical applications in AI in recent years. This article will introduce the basic concepts of agents, their core technologies, and the latest advances in their applications.
Large Language Model Agents
Large Language Model Agents (LLM agents) utilize LLMs as the system&rsquo;s brain, combined with modules such as planning, memory, and external tools, to achieve automated execution of complex tasks."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-03-27-llm-agent/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-03-27-llm-agent/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-03-27-llm-agent/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-03-27-llm-agent/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Large Language Model Agents"><meta property="og:description" content="Agents Since OpenAI released ChatGPT in October 2022, and with the subsequent emergence of projects such as AutoGPT and AgentGPT, LLM-related agents have gradually become a research hotspot and a promising direction for practical applications in AI in recent years. This article will introduce the basic concepts of agents, their core technologies, and the latest advances in their applications.
Large Language Model Agents Large Language Model Agents (LLM agents) utilize LLMs as the system’s brain, combined with modules such as planning, memory, and external tools, to achieve automated execution of complex tasks."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-27T10:00:00+00:00"><meta property="article:modified_time" content="2025-03-27T10:00:00+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="AI"><meta property="article:tag" content="Agent"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Planning"><meta property="article:tag" content="Memory"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Large Language Model Agents"><meta name=twitter:description content="Agents
Since OpenAI released ChatGPT in October 2022, and with the subsequent emergence of projects such as AutoGPT and AgentGPT, LLM-related agents have gradually become a research hotspot and a promising direction for practical applications in AI in recent years. This article will introduce the basic concepts of agents, their core technologies, and the latest advances in their applications.
Large Language Model Agents
Large Language Model Agents (LLM agents) utilize LLMs as the system&rsquo;s brain, combined with modules such as planning, memory, and external tools, to achieve automated execution of complex tasks."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Large Language Model Agents","item":"https://syhya.github.io/posts/2025-03-27-llm-agent/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Large Language Model Agents","name":"Large Language Model Agents","description":"Agents Since OpenAI released ChatGPT in October 2022, and with the subsequent emergence of projects such as AutoGPT and AgentGPT, LLM-related agents have gradually become a research hotspot and a promising direction for practical applications in AI in recent years. This article will introduce the basic concepts of agents, their core technologies, and the latest advances in their applications.\nLarge Language Model Agents Large Language Model Agents (LLM agents) utilize LLMs as the system\u0026rsquo;s brain, combined with modules such as planning, memory, and external tools, to achieve automated execution of complex tasks.\n","keywords":["LLM","AI","Agent","Reinforcement Learning","Planning","Memory","Tool Use","Deep Research","ReAct","Reflexion","WebVoyager","OpenAI Operator","CoT","ToT","workflow"],"articleBody":"Agents Since OpenAI released ChatGPT in October 2022, and with the subsequent emergence of projects such as AutoGPT and AgentGPT, LLM-related agents have gradually become a research hotspot and a promising direction for practical applications in AI in recent years. This article will introduce the basic concepts of agents, their core technologies, and the latest advances in their applications.\nLarge Language Model Agents Large Language Model Agents (LLM agents) utilize LLMs as the system’s brain, combined with modules such as planning, memory, and external tools, to achieve automated execution of complex tasks.\nUser Request: Users interact with the agent by inputting tasks through prompts. Agent: The system’s brain, consisting of one or more LLMs, responsible for overall coordination and task execution. Planning: Decomposes complex tasks into smaller subtasks and formulates execution plans, while continuously optimizing results through reflection. Memory: Includes short-term memory (using in-context learning to instantly capture task information) and long-term memory (using external vector storage to save and retrieve key information, ensuring information continuity for long-term tasks). Tools: Integrates external tools such as calculators, web searches, and code interpreters to call external data, execute code, and obtain the latest information. Fig. 1. The illustration of LLM Agent Framework. (Image source: DAIR.AI, 2024)\nReinforcement Learning Agents The goal of Reinforcement Learning (RL) is to train an agent to take a series of actions (actions, $a_t$) in a given environment. During the interaction, the agent transitions from one state (state, $s_t$) to the next, and receives a reward (reward, $r_t$) from the environment after each action. This interaction generates a complete trajectory (trajectory, $\\tau$), usually represented as:\n$$ \\tau = \\{(s_0, a_0, r_0), (s_1, a_1, r_1), \\dots, (s_T, a_T, r_T)\\}. $$The agent’s objective is to learn a policy (policy, $\\pi$), which is a rule for selecting actions in each state, to maximize the expected cumulative reward, often expressed as:\n$$ \\max_{\\pi} \\, \\mathbb{E}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right], $$ where $\\gamma \\in [0,1]$ is the discount factor, used to balance short-term and long-term rewards.\nFig. 2. The agent-environment interaction. (Image source: Sutton \u0026 Barto, 2018)\nIn the LLM context, the model can be viewed as an agent, and the “environment” can be understood as the user input and its corresponding expected response:\nState ($s_t$): Can be the current dialogue context or the user’s question. Action ($a_t$): The text output by the model (answers, generated content, etc.). Reward ($r_t$): Feedback from the user or system (such as user satisfaction, automatic scoring by a reward model, etc.). Trajectory ($\\tau$): The sequence of all text interactions from the initial dialogue to the end, which can be used to evaluate the overall performance of the model. Policy ($\\pi$): The rules that govern how the LLM generates text in each state (dialogue context), generally determined by the model’s parameters. For LLMs, traditionally, pre-training is first performed on a massive amount of offline data. In the subsequent reinforcement learning stage, the model is trained through human or model feedback to produce high-quality text that better aligns with human preferences or task requirements.\nComparison The following table shows the differences between the two:\nComparison Dimension LLM Agent RL Agent Core Principle Automates complex tasks through planning, memory, and tool utilization. Continuously optimizes the policy to maximize long-term rewards through a trial-and-error feedback loop of interaction with the environment. Optimization Method Does not directly update model parameters, primarily relies on context expansion, external memory, and tools to improve performance. Continuously and frequently updates policy model parameters, relying on reward signals from environmental feedback for optimization. Interaction Method Uses natural language to interact with users or external systems, flexibly calling various tools to obtain external information. Interacts with real or simulated environments, where the environment provides rewards or punishments, forming a closed-loop feedback. Implementation Goal Decomposes complex tasks and utilizes external resources to complete tasks, focusing on the quality and accuracy of task results. Maximizes long-term rewards, pursuing the optimal balance between short-term and long-term returns. As research deepens, the combination of LLM and RL agents presents more possibilities, such as:\nUsing reinforcement learning methods to train Reasoning LLMs (e.g., o1/o3), making them more suitable as base models for LLM agents. Simultaneously, recording the data and feedback of LLM agents executing tasks to provide rich training data for Reasoning LLMs, thereby improving model performance. Planning: Task Decomposition The core components of an LLM Agent include planning, memory, and tool use. These components work together to enable the agent to autonomously execute complex tasks.\nFig. 3. Overview of a LLM-powered autonomous agent system. (Image source: Weng, 2017)\nPlanning is crucial for the successful execution of complex tasks. It can be approached in different ways depending on the complexity and the need for iterative improvement. In simple scenarios, the planning module can use the LLM to pre-outline a detailed plan, including all necessary subtasks. This step ensures that the agent systematically performs task decomposition and follows a clear logical flow from the outset.\nChain of Thought Chain of Thought (CoT) (Wei et al. 2022) generates a series of short sentences describing the reasoning process, called reasoning steps. The purpose is to explicitly show the model’s reasoning path, helping the model better handle complex reasoning tasks. The figure below shows the difference between few-shot prompting (left) and chain-of-thought prompting (right). Few-shot prompting gets the wrong answer, while the chain-of-thought method guides the model to state the reasoning process step by step, more clearly reflecting the model’s logical process, thereby improving the accuracy and interpretability of the answer.\nFig. 4. The comparison example of few-shot prompting and CoT prompting. (Image source: Weng, 2023)\nZero-Shot CoT (Kojima et al. 2022) is a follow-up study of CoT that proposes an extremely simple zero-shot prompting method. They found that by simply adding the sentence Let's think step by step at the end of the question, the LLM can generate a chain of thought and obtain a more accurate answer.\nFig. 5. The comparison example of few-shot prompting and CoT prompting. (Image source: Kojima et al. 2022)\nSelf-Consistency Sampling Self-consistency sampling (Wang et al. 2022a) is a method that generates multiple diverse answers by sampling the same prompt multiple times with temperature \u003e 0 and selecting the best answer from them. Its core idea is to improve the accuracy and robustness of the final answer by sampling multiple reasoning paths and then using majority voting. The criteria for selecting the best answer may vary for different tasks. Generally, majority voting is used as a general solution. For tasks such as programming problems that are easy to verify, the answers can be verified by running an interpreter and combining unit tests. This is an optimization of CoT, and when used in conjunction with it, it can significantly improve the model’s performance in complex reasoning tasks.\nFig. 6. Overview of the Self-Consistency Method for Chain-of-Thought Reasoning. (Image source: Wang et al. 2022a)\nHere are some subsequent optimization efforts:\n(Wang et al. 2022b) subsequently used another ensemble learning method for optimization, increasing randomness by changing the order of examples or replacing human-written reasoning with model-generated reasoning, and then using majority voting. Fig. 7. An overview of different ways of composing rationale-augmented ensembles, depending on how the randomness of rationales is introduced. (Image source: Wang et al. 2022b)\nIf the training samples only provide the correct answer without reasoning, STaR (Self-Taught Reasoner)(Zelikman et al. 2022) can be used: (1) Let the LLM generate reasoning chains, and only keep the reasoning with the correct answer. (2) Fine-tune the model with the generated reasoning, and iterate repeatedly until convergence. Note that when temperature is high, it is easy to generate results with the correct answer but incorrect reasoning. If there is no standard answer, consider using majority voting as the “correct answer”. Fig. 8. An overview of STaR and a STaR-generated rationale on CommonsenseQA (Image source: Zelikman et al. 2022)\n(Fu et al. 2023) found that more complex examples (more reasoning steps) can improve model performance. When separating reasoning steps, the newline character \\n works better than step i, ., or ;. In addition, the complexity-based consistency strategy, which only performs majority voting on the top $k$ reasoning chains generated by complexity, can further optimize the model output. Replacing Q: with Question: in the prompt has also been shown to have an additional performance boost. Fig. 9. Sensitivity analysis on step formatting. Complex prompts consistently lead to better performance with regard to different step formatting. (Image source: Fu et al. 2023))\nTree of Thoughts Tree of Thoughts (ToT) (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thinking steps and generates multiple different ideas at each step, forming a tree structure. The search process can use breadth-first search (BFS) or depth-first search (DFS), and each state is evaluated by a classifier (or the LLM can be used for scoring) or majority voting. It consists of three main steps:\nExpand: Generate one or more candidate solutions. Score: Measure the quality of the candidate solutions. Prune: Keep the top $k$ best candidate solutions. If no solution is found (or the quality of the candidate solutions is not high enough), backtrack to the expansion step.\nFig. 10. Schematic illustrating various approaches to problem solving with LLMs (Image source: Yao et al. 2023)\nPlanning: Self-Reflection Self-Reflection is a key factor that enables agents to achieve iterative improvement by improving past action decisions and correcting previous errors. It plays a crucial role in real-world tasks where trial and error is inevitable.\nReAct The ReAct (Reason + Act) (Yao et al. 2023) framework achieves seamless integration of reasoning and action in LLMs by combining task-specific discrete actions and language space. This design not only enables the model to interact with the environment by calling external interfaces such as the Wikipedia search API, but also generates detailed reasoning trajectories in natural language, thereby solving complex problems.\nThe ReAct prompt template contains explicit thinking steps, and its basic format is as follows:\nThought：... Action：... Observation：... ...(Repeated many times) Fig. 11. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023)\nAs can be seen from the figure below, in both knowledge-intensive tasks and decision-making tasks, ReAct’s performance is significantly better than the basic method that relies only on Actor, thus demonstrating its advantages in improving reasoning effectiveness and interaction performance.\nFig. 12. PaLM-540B prompting results on HotpotQA and Fever. (Image source: Yao et al. 2023)\nReflexion Reflexion (Shinn et al. 2023) enables LLMs to iteratively optimize decisions through self-feedback and dynamic memory.\nThis method essentially draws on the idea of reinforcement learning. In the traditional Actor-Critic model, the Actor selects action $a_t$ based on the current state $s_t$, while the Critic gives an estimate (such as the value function $V(s_t)$ or the action-value function $Q(s_t,a_t)$) and feeds it back to the Actor for policy optimization. Correspondingly, in the three major components of Reflexion:\nActor: Played by the LLM, it outputs text and corresponding actions based on the environment state (including context and historical information). It can be denoted as:\n$$ a_t = \\pi_\\theta(s_t), $$where $\\pi_\\theta$ represents the policy obtained based on the parameter $\\theta$ (i.e., the weights or prompts of the LLM). The Actor interacts with the environment and generates a trajectory $\\tau = {(s_1,a_1,r_1), \\dots, (s_T,a_T,r_T)}$.\nEvaluator: Similar to the Critic, the Evaluator receives the trajectory generated by the Actor and outputs a reward signal $r_t$. In the Reflexion framework, the Evaluator can analyze the trajectory through pre-designed heuristic rules or an additional LLM, and then generate rewards. For example:\n$$ r_t = R(\\tau_t), $$where $R(\\cdot)$ is a reward function based on the current trajectory $\\tau_t$.\nSelf-Reflection: This module is equivalent to adding an additional self-regulation feedback mechanism outside the Actor-Critic. It integrates the current trajectory $\\tau$, reward signals ${r_t}$, and historical experience in long-term memory, and uses language generation capabilities to generate self-improvement suggestions for the next decision. This feedback information is then written to external memory, providing richer context for subsequent Actor decisions, thereby achieving iterative optimization similar to policy parameter $\\theta$ through dynamic adjustment of prompts without updating the internal parameters of the LLM.\nFig. 13. (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm (Image source: Shinn et al. 2023)\nThe core loop and algorithm description of Reflexion are as follows:\nInitialization\nInstantiate three models (all can be implemented by LLM): Actor, Evaluator, and Self-Reflection, denoted as $M_a, M_e, M_{sr}$ respectively. Initialize the policy $\\pi_\\theta$ (including the model parameters or prompts of the Actor, and initial memory, etc.). Let the Actor generate an initial trajectory $\\tau_0$ according to the current policy $\\pi_\\theta$. After $M_e$ evaluates it, $M_{sr}$ generates the first self-reflection text and stores it in long-term memory. Generate Trajectory\nIn each iteration, $M_a$ reads the current long-term memory and environmental observations, sequentially outputs actions ${a_1, a_2, \\ldots}$, interacts with the environment and obtains corresponding feedback, forming a new trajectory $\\tau_t$. $\\tau_t$ can be regarded as the short-term memory of this task, and is only used in this iteration. Evaluation\n$M_e$ outputs rewards or scores ${r_1, r_2, \\ldots}$ based on the trajectory $\\tau_t$ (i.e., the sequence of Actor’s actions and environmental feedback). This step corresponds to the internal feedback of $M_e$, or the results are directly given by the external environment. Self-Reflection\nThe $M_{sr}$ module integrates the trajectory $\\tau_t$ and the reward signal ${r_t}$ to generate self-correction or improvement suggestions $\\mathrm{sr}_t$ at the language level. Reflective text can be regarded as an analysis of errors or provide new启发思路, and is stored in long-term memory. In practice, we can vectorize the feedback information and store it in a vector database. Update and Repeat\nAfter appending the latest self-reflection text $\\mathrm{sr}_t$ to the long-term memory, the Actor can use RAG to retrieve historically relevant information from it in the next iteration to adjust the policy. Repeat the above steps until $M_e$ determines that the task is achieved or the maximum number of rounds is reached. In this loop, Reflexion relies on the continuous accumulation of self-reflection + long-term memory to improve decisions, rather than directly modifying model parameters. The following shows examples of Reflexion’s application in decision-making, programming, and reasoning tasks:\nFig. 14. Reflexion works on decision-making 4.1, programming 4.3, and reasoning 4.2 tasks (Image source: Shinn et al. 2023)\nIn an experiment on 100 HotPotQA questions, by comparing the CoT method and the method of adding episodic memory, the results show that after adding the self-reflection step at the end using the Reflexion method, its search, information retrieval, and reasoning capabilities are significantly improved.\nFig. 15. Comparative Analysis of Chain-of-Thought (CoT) and ReAct on the HotPotQA Benchmark (Image source: Shinn et al. 2023)\nDeepSeek R1 DeepSeek-R1 (DeepSeek-AI, 2025) represents a major breakthrough in the open-source community’s replication of OpenAI o1 (OpenAI, 2024), successfully training an advanced reasoning model with deep reflection capabilities through reinforcement learning techniques.\nFor a detailed description of the training process and technical implementation of DeepSeek R1, please refer to my previous blog post: OpenAI o1 Replication Progress: DeepSeek-R1.\nThe key transformation in the training process of DeepSeek-R1-Zero - as training progresses, the model gradually emerges with excellent self-evolution capabilities. This capability is reflected in three core aspects:\nSelf-Reflection: The model can trace back and critically evaluate previous reasoning steps. Active Exploration: When it finds that the current solution path is not ideal, it can autonomously find and try alternative solutions. Dynamic Thinking Adjustment: Adaptively adjust the number of generated tokens according to the complexity of the problem to achieve a deeper thinking process. This dynamic and spontaneous reasoning behavior significantly improves the model’s ability to solve complex problems, enabling it to respond to challenging tasks more efficiently and accurately.\nFig. 16. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)\nDeepSeek-R1-Zero also exhibited a typical “aha moment” during training. At this critical stage, the model suddenly realized that there was an error in the previous thinking path during the reasoning process, and then quickly adjusted the thinking direction, and finally successfully led to the correct answer. This phenomenon strongly proves that the model has developed strong self-correction and reflection capabilities during the reasoning process, similar to the aha experience in human thinking.\nFig. 17. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: DeepSeek-AI, 2025)\nMemory Human Memory Memory refers to the process of acquiring, storing, retaining, and retrieving information. Human memory is mainly divided into the following three categories:\nFig. 18. Categorization of human memory. (Image source: Weng, 2017)\nSensory Memory: Used to briefly retain sensory information after the original stimulus (visual, auditory, tactile, etc.) disappears, usually lasting for milliseconds or seconds. Sensory memory is divided into:\nVisual Memory: The instantaneous image or visual impression retained by the visual channel, generally lasting 0.25-0.5 seconds, is used to form visual continuity in video or animation scenes. Auditory Memory: The short-term storage of auditory information, which can last for several seconds, enables people to replay the sentences or sound clips they just heard. Tactile Memory: Used to retain short-term tactile or force information, generally lasting from milliseconds to seconds, such as the short-term finger perception when tapping the keyboard or reading Braille. Short-Term Memory: Stores the information we are currently aware of.\nLasts about 20-30 seconds, and the capacity is usually 7±2 items. Undertakes the temporary processing and maintenance of information during complex cognitive tasks such as learning and reasoning. Long-Term Memory: Can store information for days to decades, and the capacity is almost unlimited. Long-term memory is divided into:\nExplicit Memory: Can be consciously recalled, including episodic memory (personal experiences, event details) and semantic memory (facts and concepts). Implicit Memory: Unconscious memory, mainly related to skills and habits, such as riding a bicycle or touch typing. These three types of human memory are intertwined and together constitute our cognition and understanding of the world. In building LLM Agents, we can also learn from this classification of human memory:\nSensory Memory corresponds to the embedding representation of the LLM’s input raw data (such as text, pictures, and videos). Short-Term Memory corresponds to the LLM’s in-context learning, which is limited by the model’s context window max_tokens. When the dialogue length exceeds the window, early information will be truncated. Long-Term Memory corresponds to external vector storage or databases. Agents can retrieve historical information when needed based on RAG technology. LLM Agent Memory When an agent interacts with users in multiple rounds and performs multi-step tasks, it can utilize different forms of memory and environmental information to complete the workflow.\nFig. 19. An overview of the sources, forms, and operations of the memory in LLM-based agents. (Image source: Zhang et al. 2024)\nText Memory\nComplete Interaction: Records all dialogue and operation trajectories, helping the Agent trace back the context. Recent Interaction: Only retains dialogue content that is highly relevant to the current task, reducing unnecessary context occupation. Retrieved Interaction: The Agent can retrieve historical dialogues or records related to the current task from an external knowledge base and integrate them into the current context. External Knowledge: When the Agent encounters a knowledge gap, it can retrieve and obtain additional information through APIs or external storage. Parameterized Memory\nFine-tuning: Infuses new information or knowledge into the LLM, thereby expanding the model’s internal knowledge. Knowledge Editing: Modifies or updates existing knowledge at the model level, realizing dynamic adjustment of the memory of internal parameters of the model. Environment\nRepresents the entities and context involved when the Agent interacts with users and external systems, such as user Alice, tools or interfaces that may be accessed (such as ticketing systems, streaming platforms, etc.). Agent\nThe LLM Agent is responsible for read and write operations, that is, reading information from the external environment or knowledge base, and writing new actions or content. It also includes a series of management functions, such as merging, reflection, forgetting, etc., to dynamically maintain short-term and long-term memory. Another example is when an Agent needs to use both short-term and long-term memory to complete two different but related tasks:\nTask A Play Video: The Agent records the current plan, operations, and environment state (such as search, click, play video, etc.) in short-term memory, which is stored in memory and the LLM’s context window. Task B Download Game: The Agent utilizes the knowledge related to Arcane and League of Legend in long-term memory to quickly find a game download solution. The figure shows that the Agent searches on Google. We can regard Google’s knowledge base as an external knowledge source, and all new search, click, and download operations will also be updated to short-term memory. Fig. 20: Illustration of short-term memory and long-term memory in an LLM-brained GUI agent. (Image source: Zhang et al. 2024)\nCommon memory elements and their corresponding storage methods can be summarized in the following table:\nMemory Element Memory Type Description Storage Medium / Method Actions Short-Term Memory Historical action trajectories (e.g., clicking buttons, entering text) Memory, LLM context window Plan Short-Term Memory The previous step or the next step of the currently generated operation plan Memory, LLM context window Execution Results Short-Term Memory Results returned after action execution, error messages, and environmental feedback Memory, LLM context window Environment State Short-Term Memory Available buttons, page titles, system status, etc. in the current UI environment Memory, LLM context window Own Experience Long-Term Memory Historical task trajectories and execution steps Database, disk Self-Guidance Long-Term Memory Guidance rules and best practices summarized from historical successful trajectories Database, disk External Knowledge Long-Term Memory External knowledge bases, documents, or other data sources that assist in task completion External database, vector retrieval Task Success Metrics Long-Term Memory Records task success rate, failure rate, and other indicators for improvement and analysis Database, disk In addition, researchers have proposed some new training and storage methods to enhance the memory capabilities of LLMs:\nLongMem (Language Models Augmented with Long-Term Memory) (Wang, et al. 2023) enables LLMs to memorize long historical information. It adopts a decoupled network structure, freezing the original LLM parameters as a memory encoder, while using an Adaptive Residual Side-Network (SideNet) as a memory retriever for memory checking and reading.\nFig. 21. Overview of the memory caching and retrieval flow of LongMem. (Image source: Wang, et al. 2023)\nIt is mainly composed of three parts: Frozen LLM, Residual SideNet, and Cached Memory Bank. Its workflow is as follows:\nFirst, the long text sequence is split into fixed-length segments. Each segment is encoded layer by layer in the Frozen LLM, and the attention $K, V \\in \\mathbb{R}^{H \\times M \\times d}$ vector pairs are extracted from the $m$-th layer and cached in the Cached Memory Bank. When facing a new input sequence, the model retrieves the long-term memory bank based on the query-key of the current input, obtains the top $k$ key-value pairs (i.e., top-$k$ retrieval results) that are most relevant to the input, and integrates them into the subsequent language generation process; at the same time, the memory bank will remove the oldest content to ensure the availability of the latest context information. The Residual SideNet fuses the hidden layer output of the frozen LLM with the retrieved historical key-values during the inference stage to complete the effective modeling and context utilization of ultra-long text. Through this decoupled design, LongMem can flexibly schedule massive historical information without expanding its native context window, taking into account both speed and long-term memory capabilities.\nTool Use Tool use is an important part of LLM Agents. By giving LLMs the ability to call external tools, their functions are significantly expanded: they can not only generate natural language, but also obtain real-time information, perform complex calculations, and interact with various systems (such as databases, APIs, etc.), thereby effectively breaking through the limitations of pre-trained knowledge and avoiding the inefficient process of reinventing the wheel.\nTraditional LLMs mainly rely on pre-trained data for text generation, but this also makes them insufficient in mathematical operations, data retrieval, and real-time information updates. Through tool calls, the model can:\nImprove computing power: For example, by calling a dedicated calculator tool Wolfram, the model can perform more precise mathematical calculations, making up for its lack of arithmetic capabilities.\nObtain real-time information: Using search engines like Google, Bing, or database APIs, the model can access the latest information to ensure the timeliness and accuracy of the generated content.\nEnhance information credibility: With the support of external tools, the model can cite real data sources, reduce the risk of information fabrication, and improve overall credibility.\nImprove system transparency: Tracking API call records can help users understand the model’s decision-making process and provide a certain degree of interpretability.\nCurrently, various LLM applications based on tool calls have emerged in the industry. They use different strategies and architectures to achieve comprehensive coverage from simple tasks to complex multi-step reasoning.\nToolformer Toolformer (Schick, et al. 2023) is an LLM that can use external tools through simple APIs. It is trained by fine-tuning the GPT-J model, requiring only a few examples for each API. Toolformer learned to call tools including a question answering system, Wikipedia search, a calculator, a calendar, and a translation system:\nFig. 22. Examples of inputs and outputs for all APIs used. (Image source: Schick, et al. 2023)\nHuggingGPT HuggingGPT (Shen, et al. 2023) is a framework that uses ChatGPT as a task planner. It selects available models from HuggingFace by reading model descriptions to complete user tasks, and summarizes based on the execution results.\nFig. 23. Examples of inputs and outputs for all APIs used. (Image source: Shen, et al. 2023)\nThe system consists of the following four stages:\nTask Planning: Parses user requests into multiple subtasks. Each task contains four attributes: task type, ID, dependencies, and arguments. The paper uses few-shot prompting to guide the model in task splitting and planning. Model Selection: Assigns each subtask to different expert models, using a multiple-choice approach to determine the most suitable model. Due to the limited context length, models need to be initially filtered based on task type. Task Execution: Expert models execute the assigned specific tasks and record the results. The results are passed to the LLM for subsequent processing. Response Generation: Receives the execution results of each expert model and finally outputs a summary answer to the user. LLM Agent Applications Generative Agent The Generative Agent experiment (Park, et al. 2023) simulates realistic human behavior in a sandbox environment through 25 virtual characters driven by large language models. Its core design integrates memory, retrieval, reflection, and planning and reaction mechanisms, allowing the Agent to record and review its own experiences, and extract key information from them to guide subsequent actions and interactions.\nFig. 24. The screenshot of generative agent sandbox. (Image source: Park, et al. 2023)\nThe entire system uses a long-term memory module to record all observed events, combines a retrieval model to extract information based on recency, importance, and relevance, and then generates high-level inferences through a reflection mechanism, and finally converts these results into specific actions. This simulation experiment demonstrates emergent behaviors such as information diffusion, relationship memory, and social event coordination, providing a realistic human behavior simulation for interactive applications.\nFig. 25. The generative agent architecture. (Park, et al. 2023)\nWebVoyager WebVoyager (He et al. 2024) is an autonomous web interaction agent based on a large multimodal model that can control the mouse and keyboard for web browsing. WebVoyager uses the classic ReAct loop. In each interaction step, it views a browser screenshot annotated with a method similar to SoM (Set-of-Marks) (Yang, et al. 2023) – providing interaction hints by placing numerical labels on web elements – and then decides the next action. This visual annotation combined with the ReAct loop allows users to interact with web pages through natural language. For specifics, you can refer to the WebVoyager code using the LangGraph framework.\nFig. 26. The overall workflow of WebVoyager. (Image source: He et al. 2024)\nOpenAI Operator Operator (OpenAI, 2025) is an AI agent recently released by OpenAI, designed to autonomously perform web tasks. Operator can interact with web pages like a human user, completing specified tasks through typing, clicking, and scrolling. The core technology of Operator is the Computer-Using Agent (CUA) (OpenAI, 2025). CUA combines the visual capabilities of GPT-4o with stronger reasoning capabilities obtained through reinforcement learning, and is specially trained to interact with graphical user interfaces (GUIs), including buttons, menus, and text boxes that users see on the screen.\nFig. 27. Overview of OpenAI CUA. (Image source: OpenAI, 2025)\nCUA operates in an iterative loop that includes three stages:\nPerception: CUA “observes” the web page content by capturing browser screenshots. This vision-based input method enables it to understand the layout and elements of the page.\nReasoning: With the help of chain-of-thought reasoning, CUA evaluates the next action based on the current and previous screenshots and the actions that have been performed. This reasoning ability enables it to track task progress, review intermediate steps, and make adjustments as needed.\nAction: CUA interacts with the browser by simulating mouse and keyboard operations (such as clicking, typing, and scrolling). This enables it to perform various web tasks without specific API integration.\nThe difference between CUA and the previously existing WebVoyager is that this is an Agent specifically trained with reinforcement learning, rather than a fixed-process workflow built by directly calling GPT-4o. Although CUA is still in its early stages and has certain limitations, it has achieved SOTA results in the following benchmark tests.\nFig. 28. OpenAI CUA Benchmark Results. (Image source: OpenAI, 2025)\nDeep Research Deep Research is essentially a report generation system: given a user’s query, the system uses an LLM as the core Agent, and after multiple rounds of iterative information retrieval and analysis, it finally generates a structured and informative report. Currently, the implementation logic of various Deep Research systems can be mainly divided into two methods: Workflow Agent and RL Agent.\nWorkflow Agent vs RL Agent The Workflow Agent approach relies on developers pre-designing workflows and manually constructing Prompts to organize the entire report generation process. The main features include:\nTask Decomposition and Process Orchestration: The system breaks down the user query into several subtasks, such as generating an outline, information retrieval, content summarization, etc., and then executes them in a predetermined process sequence. Fixed Process: The calls and interactions between each stage are pre-set, similar to building a static flow chart or directed acyclic graph (DAG), ensuring that each step has a clear responsibility. Manual Design Dependence: This method mainly relies on the experience of engineers, and improves the output quality by repeatedly debugging Prompts. It is highly applicable but has limited flexibility. The LangGraph framework can be used to build and orchestrate workflows in the form of graphs.\nFig. 29. A workflow of the LangGraph. (Image source: LangGraph, 2025)\nCurrently, there are multiple open-source projects on Github that implement Deep Research Agents based on workflows, such as GPT Researcher and open deep research.\nFig. 30. An overview of the open deep research. (Image source: LangChain, 2025)\nThe RL Agent is another implementation method that optimizes the Agent’s multi-round search, analysis, and report writing process by training a reasoning model with RL. The main features include:\nAutonomous Decision-Making Ability: The system is trained through reinforcement learning, allowing the Agent to autonomously judge, make decisions, and adjust strategies when facing complex search and content integration tasks, thereby generating reports more efficiently. Continuous Optimization: Using a reward mechanism to score and provide feedback on the generation process, the Agent can continuously iterate and optimize its own strategy, improving the overall quality from task decomposition to final report generation. Reduced Manual Intervention: Compared to fixed processes that rely on manual Prompts, the reinforcement learning training method reduces the dependence on manual design and is more suitable for dealing with changing and complex real-world application scenarios. The following table summarizes the main differences between these two methods:\nFeature Workflow Agent RL Agent Process Design Pre-designed fixed workflow, clear task decomposition and process orchestration End-to-end learning, Agent autonomous decision-making and dynamic process adjustment Autonomous Decision-Making Relies on manually designed Prompts, the decision-making process is fixed and immutable Through reinforcement learning, the Agent can autonomously judge, make decisions, and optimize strategies Manual Intervention Requires a lot of manual design and debugging of Prompts, more manual intervention Reduces manual intervention, achieves automatic feedback and continuous optimization through a reward mechanism Flexibility and Adaptability Less adaptable to complex or changing scenarios, limited scalability More adaptable to changing and complex real-world scenarios, with high flexibility Optimization Mechanism Optimization mainly relies on the engineer’s experience adjustment, lacking an end-to-end feedback mechanism Uses reinforcement learning’s reward feedback to achieve continuous and automated performance improvement Implementation Difficulty Relatively straightforward to implement, but requires cumbersome process design and maintenance Requires training data and computing resources, with a larger initial development investment, but better long-term results Training Required No additional training required, only relies on manually constructed processes and Prompts Requires training the Agent through reinforcement learning to achieve autonomous decision-making OpenAI Deep Research OpenAI Deep Research (OpenAI, 2025) is an intelligent Agent officially released by OpenAI in February 2025, designed for complex scenarios. It can automatically search, filter, analyze, and integrate multi-source information, and finally generate high-quality comprehensive reports. The system uses o3 as the core base, and combined with reinforcement learning methods, significantly improves the accuracy and robustness of the multi-round iterative search and reasoning process.\nCompared with traditional ChatGPT plug-in search or conventional RAG technology, OpenAI Deep Research has the following outstanding advantages:\nReinforcement Learning-Driven Iterative Reasoning With the help of the o3 reasoning model and reinforcement learning training strategies, the Agent can continuously optimize its own reasoning path during the multi-round search and summarization process, effectively reducing the risk of distortion caused by error accumulation.\nIntegration and Cross-Validation of Multi-Source Information Breaking through the limitations of a single search engine, it can simultaneously call multiple authoritative data sources such as specific databases and professional knowledge bases, and form more reliable research conclusions through cross-validation.\nHigh-Quality Report Generation The LLM-as-a-judge scoring mechanism and strict evaluation criteria are introduced in the training phase, so that the system can conduct self-evaluation when outputting reports, thereby generating professional texts with clearer structures and more rigorous arguments.\nTraining Process The OpenAI Deep Research training process uses a browser interaction dataset specifically customized for research scenarios. Through these datasets, the model masters core browsing functions - including search, click, scroll, and file parsing; at the same time, it learns the ability to use Python tools in a sandbox environment for calculation, data analysis, and visualization. In addition, with the help of reinforcement learning training on these browsing tasks, the model can efficiently perform information retrieval, integration, and reasoning in a large number of websites, quickly locate key information, or generate comprehensive research reports.\nThese training datasets include both objective tasks with standard answers and automatic scoring, and open-ended tasks with detailed scoring rubrics. During the training process, the model’s responses are strictly compared with standard answers or scoring criteria, and the evaluation model provides feedback by using the CoT thinking process generated by the model.\nAt the same time, the training process reuses the safety datasets accumulated during the o1 model training phase, and specifically adds safety training data for Deep Research scenarios to ensure that the model strictly complies with relevant compliance and safety requirements during the automated search and browsing process.\nPerformance In the benchmark test Humanity’s Last Exam (Phan, et al. 2025), which evaluates the ability of AI to answer expert-level questions in various professional fields, the model achieved SOTA results.\nFig. 31. Humanity’s Last Exam Benchmark Results。(Image source: OpenAI, 2025)\nFuture Development Directions Agents show broad prospects, but to achieve reliable and widespread application, the following key challenges still need to be addressed:\nContext Window Limitation: The context window of LLMs limits the amount of information processed, affecting long-term planning and memory capabilities, and reducing task coherence. Current research explores external memory mechanisms and context compression techniques to enhance long-term memory and complex information processing capabilities. Currently, OpenAI’s latest model GPT-4.5 (OpenAI, 2025) has a maximum context window of 128k tokens.\nInterface Standardization and Interoperability: The current natural language-based tool interaction has the problem of inconsistent formats. The Model Context Protocol (MCP) (Anthropic, 2024) unifies the interaction between LLMs and applications through open standards, reducing development complexity, improving system stability, and cross-platform compatibility.\nTask Planning and Decomposition Capabilities: Agents have difficulty formulating coherent plans, effectively decomposing subtasks, and lack the ability to dynamically adjust in unexpected situations in complex tasks. More powerful planning algorithms, self-reflection mechanisms, and dynamic strategy adjustment methods are needed to flexibly respond to uncertain environments.\nComputing Resources and Economic Benefits: Deploying large model agents is costly due to multiple API calls and intensive computing, limiting some practical application scenarios. Optimization directions include efficient model structure, quantization technology, inference optimization, caching strategies, and intelligent scheduling mechanisms. With the development of dedicated GPU hardware such as NVIDIA DGX B200 and distributed technologies, computing efficiency is expected to be significantly improved.\nSecurity Protection and Privacy Assurance: Agents face security risks such as prompt injection, and need to establish sound authentication, access control, input validation, and sandbox environments. For multimodal input and external tools, it is necessary to strengthen data anonymization, the principle of least privilege, and audit logs to meet security and privacy compliance requirements.\nDecision Transparency and Interpretability: Agent decisions are difficult to explain, limiting their application in high-risk areas. Enhancing interpretability requires the development of visualization tools, chain-of-thought tracking, and decision reason generation mechanisms to improve decision transparency, enhance user trust, and meet regulatory requirements.\nReferences [1] DAIR.AI. “LLM Agents.” Prompt Engineering Guide, 2024.\n[2] Sutton, Richard S., and Andrew G. Barto. “Reinforcement Learning: An Introduction.” MIT Press, 2018.\n[3] Weng, Lilian. “LLM-powered Autonomous Agents.” Lil’Log, 2023.\n[4] Wei, Jason, et al. “Chain-of-thought prompting elicits reasoning in large language models.” Advances in neural information processing systems 35 (2022): 24824-24837.\n[5] Kojima, Takeshi, et al. “Large language models are zero-shot reasoners.” Advances in neural information processing systems 35 (2022): 22199-22213.\n[6] Wang, Xuezhi, et al. “Self-consistency improves chain of thought reasoning in language models.” arXiv preprint arXiv:2203.11171 (2022).\n[7] Wang, Xuezhi, et al. “Rationale-augmented ensembles in language models.” arXiv preprint arXiv:2207.00747 (2022).\n[8] Zelikman, Eric, et al. “Star: Bootstrapping reasoning with reasoning.” Advances in Neural Information Processing Systems 35 (2022): 15476-15488.\n[9] Fu, Yao, et al. “Complexity-based prompting for multi-step reasoning.” arXiv preprint arXiv:2210.00720 (2022).\n[10] Yao, Shunyu, et al. “Tree of thoughts: Deliberate problem solving with large language models.” Advances in neural information processing systems 36 (2023): 11809-11822.\n[11] Yao, Shunyu, et al. “React: Synergizing reasoning and acting in language models.” International Conference on Learning Representations (ICLR). 2023.\n[12] Shinn, Noah, et al. “Reflexion: Language agents with verbal reinforcement learning.” Advances in Neural Information Processing Systems 36 (2023): 8634-8652.\n[13] Guo, Daya, et al. “Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.” arXiv preprint arXiv:2501.12948 (2025).\n[14] OpenAI. “Introducing OpenAI o1” OpenAI, 2024.\n[15] Zhang, Zeyu, et al. “A survey on the memory mechanism of large language model based agents.” arXiv preprint arXiv:2404.13501 (2024).\n[16] Zhang, Chaoyun, et al. “Large language model-brained gui agents: A survey.” arXiv preprint arXiv:2411.18279 (2024).\n[17] Wang, Weizhi, et al. “Augmenting language models with long-term memory.” Advances in Neural Information Processing Systems 36 (2023): 74530-74543.\n[18] Schick, Timo, et al. “Toolformer: Language models can teach themselves to use tools.” Advances in Neural Information Processing Systems 36 (2023): 68539-68551.\n[19] Shen, Yongliang, et al. “Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.” Advances in Neural Information Processing Systems 36 (2023): 38154-38180.\n[20] Park, Joon Sung, et al. “Generative agents: Interactive simulacra of human behavior.” Proceedings of the 36th annual acm symposium on user interface software and technology. 2023.\n[21] He, Hongliang, et al. “WebVoyager: Building an end-to-end web agent with large multimodal models.” arXiv preprint arXiv:2401.13919 (2024).\n[22] Yang, Jianwei, et al. “Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v.” arXiv preprint arXiv:2310.11441 (2023).\n[23] OpenAI. “Introducing Operator.” OpenAI, 2025.\n[24] OpenAI. “Computer-Using Agent.” OpenAI, 2025.\n[25] OpenAI. “Introducing Deep Research.” OpenAI, 2025.\n[26] Phan, Long, et al. “Humanity’s Last Exam.” arXiv preprint arXiv:2501.14249 (2025).\n[27] OpenAI. “Introducing GPT-4.5.” OpenAI, 2025.\n[28] Anthropic. “Introducing the Model Context Protocol.” Anthropic, 2024.\n[29] LangGraph. “A workflow of the LangGraph.” LangGraph Tutorials, 2025.\n[30] Assaf Elovic. “GPT Researcher” GitHub Repository, 2025.\n[31] LangChain. “Open Deep Research” GitHub Repository, 2025.\nCitation Citation: Please cite the original author and source when reprinting or referencing the content of this article.\nCited as:\nYue Shui.(Mar 2025). Large Language Model Agents. https://syhya.github.io/posts/2025-03-27-llm-agent\nOr\n@article{syhya2025llm-agent, title = \"Large Language Model Agents\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Mar\", url = \"https://syhya.github.io/posts/2025-03-27-llm-agent\" } ","wordCount":"6791","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-03-27T10:00:00Z","dateModified":"2025-03-27T10:00:00Z","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-03-27-llm-agent/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Large Language Model Agents</h1><div class=post-meta><span title='2025-03-27 10:00:00 +0000 UTC'>2025-03-27</span>&nbsp;·&nbsp;32 min&nbsp;·&nbsp;6791 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-03-27-llm-agent/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#agents>Agents</a><ul><li><a href=#large-language-model-agents>Large Language Model Agents</a></li><li><a href=#reinforcement-learning-agents>Reinforcement Learning Agents</a></li><li><a href=#comparison>Comparison</a></li></ul></li><li><a href=#planning-task-decomposition>Planning: Task Decomposition</a><ul><li><a href=#chain-of-thought>Chain of Thought</a></li><li><a href=#self-consistency-sampling>Self-Consistency Sampling</a></li><li><a href=#tree-of-thoughts>Tree of Thoughts</a></li></ul></li><li><a href=#planning-self-reflection>Planning: Self-Reflection</a><ul><li><a href=#react>ReAct</a></li><li><a href=#reflexion>Reflexion</a></li><li><a href=#deepseek-r1>DeepSeek R1</a></li></ul></li><li><a href=#memory>Memory</a><ul><li><a href=#human-memory>Human Memory</a></li><li><a href=#llm-agent-memory>LLM Agent Memory</a></li></ul></li><li><a href=#tool-use>Tool Use</a><ul><li><a href=#toolformer>Toolformer</a></li><li><a href=#hugginggpt>HuggingGPT</a></li></ul></li><li><a href=#llm-agent-applications>LLM Agent Applications</a><ul><li><a href=#generative-agent>Generative Agent</a></li><li><a href=#webvoyager>WebVoyager</a></li><li><a href=#openai-operator>OpenAI Operator</a></li></ul></li><li><a href=#deep-research>Deep Research</a><ul><li><a href=#workflow-agent-vs-rl-agent>Workflow Agent vs RL Agent</a></li><li><a href=#openai-deep-research>OpenAI Deep Research</a><ul><li><a href=#training-process>Training Process</a></li><li><a href=#performance>Performance</a></li></ul></li></ul></li><li><a href=#future-development-directions>Future Development Directions</a></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><h2 id=agents>Agents<a hidden class=anchor aria-hidden=true href=#agents>#</a></h2><p>Since OpenAI released ChatGPT in October 2022, and with the subsequent emergence of projects such as <a href=https://github.com/Significant-Gravitas/AutoGPT>AutoGPT</a> and <a href=https://github.com/reworkd/AgentGPT>AgentGPT</a>, LLM-related agents have gradually become a research hotspot and a promising direction for practical applications in AI in recent years. This article will introduce the basic concepts of agents, their core technologies, and the latest advances in their applications.</p><h3 id=large-language-model-agents>Large Language Model Agents<a hidden class=anchor aria-hidden=true href=#large-language-model-agents>#</a></h3><p><strong>Large Language Model Agents (LLM agents)</strong> utilize LLMs as the system&rsquo;s brain, combined with modules such as planning, memory, and external tools, to achieve automated execution of complex tasks.</p><ul><li><strong>User Request:</strong> Users interact with the agent by inputting tasks through prompts.</li><li><strong>Agent:</strong> The system&rsquo;s brain, consisting of one or more LLMs, responsible for overall coordination and task execution.</li><li><strong>Planning:</strong> Decomposes complex tasks into smaller subtasks and formulates execution plans, while continuously optimizing results through reflection.</li><li><strong>Memory:</strong> Includes short-term memory (using in-context learning to instantly capture task information) and long-term memory (using external vector storage to save and retrieve key information, ensuring information continuity for long-term tasks).</li><li><strong>Tools:</strong> Integrates external tools such as calculators, web searches, and code interpreters to call external data, execute code, and obtain the latest information.</li></ul><figure class=align-center><img loading=lazy src=llm_agent.png#center alt="Fig. 1. The illustration of LLM Agent Framework. (Image source: DAIR.AI, 2024)" width=70%><figcaption><p>Fig. 1. The illustration of LLM Agent Framework. (Image source: <a href=https://www.promptingguide.ai/research/llm-agents#llm-agent-framework>DAIR.AI, 2024</a>)</p></figcaption></figure><h3 id=reinforcement-learning-agents>Reinforcement Learning Agents<a hidden class=anchor aria-hidden=true href=#reinforcement-learning-agents>#</a></h3><p>The goal of <strong>Reinforcement Learning (RL)</strong> is to train an agent to take a series of actions (actions, $a_t$) in a given environment. During the interaction, the agent transitions from one state (state, $s_t$) to the next, and receives a reward (reward, $r_t$) from the environment after each action. This interaction generates a complete trajectory (trajectory, $\tau$), usually represented as:</p>$$
\tau = \{(s_0, a_0, r_0), (s_1, a_1, r_1), \dots, (s_T, a_T, r_T)\}.
$$<p>The agent&rsquo;s objective is to learn a policy (policy, $\pi$), which is a rule for selecting actions in each state, to <strong>maximize the expected cumulative reward</strong>, often expressed as:</p>$$
\max_{\pi} \, \mathbb{E}\left[\sum_{t=0}^{T} \gamma^t r_t\right],
$$<p>where $\gamma \in [0,1]$ is the discount factor, used to balance short-term and long-term rewards.</p><figure class=align-center><img loading=lazy src=rl_agent.png#center alt="Fig. 2. The agent-environment interaction. (Image source: Sutton & Barto, 2018)" width=80%><figcaption><p>Fig. 2. The agent-environment interaction. (Image source: <a href=http://incompleteideas.net/book/the-book.html>Sutton & Barto, 2018</a>)</p></figcaption></figure><p>In the <strong>LLM</strong> context, the model can be viewed as an agent, and the &ldquo;environment&rdquo; can be understood as the user input and its corresponding expected response:</p><ul><li><strong>State ($s_t$)</strong>: Can be the current dialogue context or the user&rsquo;s question.</li><li><strong>Action ($a_t$)</strong>: The text output by the model (answers, generated content, etc.).</li><li><strong>Reward ($r_t$)</strong>: Feedback from the user or system (such as user satisfaction, automatic scoring by a reward model, etc.).</li><li><strong>Trajectory ($\tau$)</strong>: The sequence of all text interactions from the initial dialogue to the end, which can be used to evaluate the overall performance of the model.</li><li><strong>Policy ($\pi$)</strong>: The rules that govern how the LLM generates text in each state (dialogue context), generally determined by the model&rsquo;s parameters.</li></ul><p>For LLMs, traditionally, pre-training is first performed on a massive amount of offline data. In the subsequent reinforcement learning stage, the model is trained through human or model feedback to produce high-quality text that better aligns with human preferences or task requirements.</p><h3 id=comparison>Comparison<a hidden class=anchor aria-hidden=true href=#comparison>#</a></h3><p>The following table shows the differences between the two:</p><table><thead><tr><th><strong>Comparison Dimension</strong></th><th><strong>LLM Agent</strong></th><th><strong>RL Agent</strong></th></tr></thead><tbody><tr><td><strong>Core Principle</strong></td><td>Automates complex tasks through planning, memory, and tool utilization.</td><td>Continuously optimizes the policy to maximize long-term rewards through a trial-and-error feedback loop of interaction with the environment.</td></tr><tr><td><strong>Optimization Method</strong></td><td><strong>Does not directly update model parameters</strong>, primarily relies on context expansion, external memory, and tools to improve performance.</td><td><strong>Continuously and frequently updates policy model parameters</strong>, relying on reward signals from environmental feedback for optimization.</td></tr><tr><td><strong>Interaction Method</strong></td><td>Uses natural language to interact with users or external systems, flexibly calling various tools to obtain external information.</td><td>Interacts with real or simulated environments, where the environment provides rewards or punishments, forming a closed-loop feedback.</td></tr><tr><td><strong>Implementation Goal</strong></td><td>Decomposes complex tasks and utilizes external resources to complete tasks, focusing on the quality and accuracy of task results.</td><td>Maximizes long-term rewards, pursuing the optimal balance between short-term and long-term returns.</td></tr></tbody></table><p>As research deepens, the combination of LLM and RL agents presents more possibilities, such as:</p><ul><li>Using reinforcement learning methods to train Reasoning LLMs (e.g., o1/o3), making them more suitable as base models for LLM agents.</li><li>Simultaneously, recording the data and feedback of LLM agents executing tasks to provide rich training data for Reasoning LLMs, thereby improving model performance.</li></ul><h2 id=planning-task-decomposition>Planning: Task Decomposition<a hidden class=anchor aria-hidden=true href=#planning-task-decomposition>#</a></h2><p>The core components of an LLM Agent include <strong>planning</strong>, <strong>memory</strong>, and <strong>tool use</strong>. These components work together to enable the agent to autonomously execute complex tasks.</p><figure class=align-center><img loading=lazy src=llm_agent_overview.png#center alt="Fig. 3. Overview of a LLM-powered autonomous agent system. (Image source: Weng, 2017)" width=100%><figcaption><p>Fig. 3. Overview of a LLM-powered autonomous agent system. (Image source: <a href=https://lilianweng.github.io/posts/2023-06-23-agent/>Weng, 2017</a>)</p></figcaption></figure><p>Planning is crucial for the successful execution of complex tasks. It can be approached in different ways depending on the complexity and the need for iterative improvement. In simple scenarios, the planning module can use the LLM to pre-outline a detailed plan, including all necessary subtasks. This step ensures that the agent systematically performs <strong>task decomposition</strong> and follows a clear logical flow from the outset.</p><h3 id=chain-of-thought>Chain of Thought<a hidden class=anchor aria-hidden=true href=#chain-of-thought>#</a></h3><p><strong>Chain of Thought (CoT)</strong> (<a href=https://arxiv.org/abs/2201.11903>Wei et al. 2022</a>) generates a series of short sentences describing the reasoning process, called reasoning steps. The purpose is to explicitly show the model&rsquo;s reasoning path, helping the model better handle <strong>complex reasoning tasks</strong>. The figure below shows the difference between few-shot prompting (left) and chain-of-thought prompting (right). Few-shot prompting gets the wrong answer, while the chain-of-thought method guides the model to state the reasoning process step by step, more clearly reflecting the model&rsquo;s logical process, thereby improving the accuracy and interpretability of the answer.</p><figure class=align-center><img loading=lazy src=cot.png#center alt="Fig. 4. The comparison example of few-shot prompting and CoT prompting. (Image source: Weng, 2023)" width=100%><figcaption><p>Fig. 4. The comparison example of few-shot prompting and CoT prompting. (Image source: <a href=https://lilianweng.github.io/posts/2023-06-23-agent/>Weng, 2023</a>)</p></figcaption></figure><p><strong>Zero-Shot CoT</strong> (<a href=https://arxiv.org/abs/2205.11916>Kojima et al. 2022</a>) is a follow-up study of CoT that proposes an extremely simple zero-shot prompting method. They found that by simply adding the sentence <code>Let's think step by step</code> at the end of the question, the LLM can generate a chain of thought and obtain a more accurate answer.</p><figure class=align-center><img loading=lazy src=zero_shot_cot.png#center alt="Fig. 5. The comparison example of few-shot prompting and CoT prompting. (Image source: Kojima et al. 2022)" width=100%><figcaption><p>Fig. 5. The comparison example of few-shot prompting and CoT prompting. (Image source: <a href=https://arxiv.org/abs/2205.11916>Kojima et al. 2022</a>)</p></figcaption></figure><h3 id=self-consistency-sampling>Self-Consistency Sampling<a hidden class=anchor aria-hidden=true href=#self-consistency-sampling>#</a></h3><p><strong>Self-consistency sampling</strong> (<a href=https://arxiv.org/abs/2203.11171>Wang et al. 2022a</a>) is a method that generates <strong>multiple diverse answers</strong> by sampling the same prompt multiple times with <code>temperature > 0</code> and selecting the best answer from them. Its core idea is to improve the accuracy and robustness of the final answer by sampling multiple reasoning paths and then using majority voting. The criteria for selecting the best answer may vary for different tasks. Generally, <strong>majority voting</strong> is used as a general solution. For tasks such as programming problems that are easy to verify, the answers can be verified by running an interpreter and combining unit tests. This is an optimization of CoT, and when used in conjunction with it, it can significantly improve the model&rsquo;s performance in complex reasoning tasks.</p><figure class=align-center><img loading=lazy src=self_consistency.png#center alt="Fig. 6. Overview of the Self-Consistency Method for Chain-of-Thought Reasoning. (Image source: Wang et al. 2022a)" width=100%><figcaption><p>Fig. 6. Overview of the Self-Consistency Method for Chain-of-Thought Reasoning. (Image source: <a href=https://arxiv.org/abs/2203.11171>Wang et al. 2022a</a>)</p></figcaption></figure><p>Here are some subsequent optimization efforts:</p><ul><li>(<a href=https://arxiv.org/abs/2207.00747>Wang et al. 2022b</a>) subsequently used another ensemble learning method for optimization, increasing randomness by changing the order of examples or replacing human-written reasoning with model-generated reasoning, and then using majority voting.</li></ul><figure class=align-center><img loading=lazy src=rationale_augmented.png#center alt="Fig. 7. An overview of different ways of composing rationale-augmented ensembles, depending on how the randomness of rationales is introduced. (Image source: Wang et al. 2022b)" width=100%><figcaption><p>Fig. 7. An overview of different ways of composing rationale-augmented ensembles, depending on how the randomness of rationales is introduced. (Image source: <a href=https://arxiv.org/abs/2207.00747>Wang et al. 2022b</a>)</p></figcaption></figure><ul><li>If the training samples only provide the correct answer without reasoning, <strong>STaR (Self-Taught Reasoner)</strong>(<a href=https://arxiv.org/abs/2203.14465>Zelikman et al. 2022</a>) can be used:
(1) Let the LLM generate reasoning chains, and only keep the reasoning with the correct answer.
(2) Fine-tune the model with the generated reasoning, and iterate repeatedly until convergence. Note that when <code>temperature</code> is high, it is easy to generate results with the correct answer but incorrect reasoning. If there is no standard answer, consider using majority voting as the &ldquo;correct answer&rdquo;.</li></ul><figure class=align-center><img loading=lazy src=STaR.png#center alt="Fig. 8. An overview of STaR and a STaR-generated rationale on CommonsenseQA (Image source: Zelikman et al. 2022)" width=100%><figcaption><p>Fig. 8. An overview of STaR and a STaR-generated rationale on CommonsenseQA (Image source: <a href=https://arxiv.org/abs/2203.14465>Zelikman et al. 2022</a>)</p></figcaption></figure><ul><li>(<a href=https://arxiv.org/abs/2210.00720>Fu et al. 2023</a>) found that more complex examples (more reasoning steps) can improve model performance. When separating reasoning steps, the newline character <code>\n</code> works better than <code>step i</code>, <code>.</code>, or <code>;</code>. In addition, the complexity-based consistency strategy, which only performs majority voting on the top $k$ reasoning chains generated by complexity, can further optimize the model output. Replacing <code>Q:</code> with <code>Question:</code> in the prompt has also been shown to have an additional performance boost.</li></ul><figure class=align-center><img loading=lazy src=linebreak.png#center alt="Fig. 9. Sensitivity analysis on step formatting. Complex prompts consistently lead to better performance with regard to different step formatting. (Image source: Fu et al. 2023))" width=100%><figcaption><p>Fig. 9. Sensitivity analysis on step formatting. Complex prompts consistently lead to better performance with regard to different step formatting. (Image source: <a href=https://arxiv.org/abs/2210.00720>Fu et al. 2023)</a>)</p></figcaption></figure><h3 id=tree-of-thoughts>Tree of Thoughts<a hidden class=anchor aria-hidden=true href=#tree-of-thoughts>#</a></h3><p><strong>Tree of Thoughts (ToT)</strong> (<a href=https://arxiv.org/abs/2305.10601>Yao et al. 2023</a>) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thinking steps and generates multiple different ideas at each step, forming a tree structure. The search process can use breadth-first search (BFS) or depth-first search (DFS), and each state is evaluated by a classifier (or the LLM can be used for scoring) or majority voting. It consists of three main steps:</p><ul><li><strong>Expand</strong>: Generate one or more candidate solutions.</li><li><strong>Score</strong>: Measure the quality of the candidate solutions.</li><li><strong>Prune</strong>: Keep the top $k$ best candidate solutions.</li></ul><p>If no solution is found (or the quality of the candidate solutions is not high enough), backtrack to the expansion step.</p><figure class=align-center><img loading=lazy src=tot.png#center alt="Fig. 10. Schematic illustrating various approaches to problem solving with LLMs (Image source: Yao et al. 2023)" width=100%><figcaption><p>Fig. 10. Schematic illustrating various approaches to problem solving with LLMs (Image source: <a href=https://arxiv.org/abs/2305.10601>Yao et al. 2023</a>)</p></figcaption></figure><h2 id=planning-self-reflection>Planning: Self-Reflection<a hidden class=anchor aria-hidden=true href=#planning-self-reflection>#</a></h2><p><strong>Self-Reflection</strong> is a key factor that enables agents to achieve iterative improvement by improving past action decisions and correcting previous errors. It plays a crucial role in real-world tasks where trial and error is inevitable.</p><h3 id=react>ReAct<a hidden class=anchor aria-hidden=true href=#react>#</a></h3><p>The <strong>ReAct (Reason + Act)</strong> (<a href=https://arxiv.org/abs/2210.03629>Yao et al. 2023</a>) framework achieves seamless integration of reasoning and action in LLMs by combining task-specific discrete actions and language space. This design not only enables the model to interact with the environment by calling external interfaces such as the Wikipedia search API, but also generates detailed reasoning trajectories in natural language, thereby solving complex problems.</p><p>The ReAct prompt template contains explicit thinking steps, and its basic format is as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>Thought：...
</span></span><span class=line><span class=cl>Action：...
</span></span><span class=line><span class=cl>Observation：...
</span></span><span class=line><span class=cl>...(Repeated many times)
</span></span></code></pre></div><figure class=align-center><img loading=lazy src=ReAct.png#center alt="Fig. 11. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023)" width=100%><figcaption><p>Fig. 11. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: <a href=https://arxiv.org/abs/2210.03629>Yao et al. 2023</a>)</p></figcaption></figure><p>As can be seen from the figure below, in both knowledge-intensive tasks and decision-making tasks, ReAct&rsquo;s performance is significantly better than the basic method that relies only on <code>Actor</code>, thus demonstrating its advantages in improving reasoning effectiveness and interaction performance.</p><figure class=align-center><img loading=lazy src=ReAct_res.png#center alt="Fig. 12. PaLM-540B prompting results on HotpotQA and Fever. (Image source: Yao et al. 2023)" width=50%><figcaption><p>Fig. 12. PaLM-540B prompting results on HotpotQA and Fever. (Image source: <a href=https://arxiv.org/abs/2210.03629>Yao et al. 2023</a>)</p></figcaption></figure><h3 id=reflexion>Reflexion<a hidden class=anchor aria-hidden=true href=#reflexion>#</a></h3><p><strong>Reflexion</strong> (<a href=https://arxiv.org/abs/2303.11366>Shinn et al. 2023</a>) enables LLMs to iteratively optimize decisions through self-feedback and dynamic memory.</p><p>This method essentially draws on the idea of reinforcement learning. In the traditional Actor-Critic model, the Actor selects action $a_t$ based on the current state $s_t$, while the Critic gives an estimate (such as the value function $V(s_t)$ or the action-value function $Q(s_t,a_t)$) and feeds it back to the Actor for policy optimization. Correspondingly, in the three major components of Reflexion:</p><ul><li><p><strong>Actor</strong>: Played by the LLM, it outputs text and corresponding actions based on the environment state (including context and historical information). It can be denoted as:</p>$$
a_t = \pi_\theta(s_t),
$$<p>where $\pi_\theta$ represents the policy obtained based on the parameter $\theta$ (i.e., the weights or prompts of the LLM). The Actor interacts with the environment and generates a trajectory $\tau = {(s_1,a_1,r_1), \dots, (s_T,a_T,r_T)}$.</p></li><li><p><strong>Evaluator</strong>: Similar to the Critic, the Evaluator receives the trajectory generated by the Actor and outputs a reward signal $r_t$. In the Reflexion framework, the Evaluator can analyze the trajectory through pre-designed heuristic rules or an additional LLM, and then generate rewards. For example:</p>$$
r_t = R(\tau_t),
$$<p>where $R(\cdot)$ is a reward function based on the current trajectory $\tau_t$.</p></li><li><p><strong>Self-Reflection</strong>: This module is equivalent to adding an additional self-regulation feedback mechanism outside the Actor-Critic. It integrates the current trajectory $\tau$, reward signals ${r_t}$, and historical experience in long-term memory, and uses language generation capabilities to generate self-improvement suggestions for the next decision. This feedback information is then written to external memory, providing richer context for subsequent Actor decisions, thereby achieving iterative optimization similar to policy parameter $\theta$ through dynamic adjustment of prompts without updating the internal parameters of the LLM.</p></li></ul><figure class=align-center><img loading=lazy src=Reflexion.png#center alt="Fig. 13. (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm (Image source: Shinn et al. 2023)" width=100%><figcaption><p>Fig. 13. (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm (Image source: <a href=https://arxiv.org/abs/2303.11366>Shinn et al. 2023</a>)</p></figcaption></figure><p>The core loop and algorithm description of Reflexion are as follows:</p><ul><li><p><strong>Initialization</strong></p><ul><li>Instantiate three models (all can be implemented by LLM): Actor, Evaluator, and Self-Reflection, denoted as $M_a, M_e, M_{sr}$ respectively.</li><li>Initialize the policy $\pi_\theta$ (including the model parameters or prompts of the Actor, and initial memory, etc.).</li><li>Let the Actor generate an initial trajectory $\tau_0$ according to the current policy $\pi_\theta$. After $M_e$ evaluates it, $M_{sr}$ generates the first self-reflection text and stores it in long-term memory.</li></ul></li><li><p><strong>Generate Trajectory</strong></p><ul><li>In each iteration, $M_a$ reads the current long-term memory and environmental observations, sequentially outputs actions ${a_1, a_2, \ldots}$, interacts with the environment and obtains corresponding feedback, forming a new trajectory $\tau_t$. $\tau_t$ can be regarded as the short-term memory of this task, and is only used in this iteration.</li></ul></li><li><p><strong>Evaluation</strong></p><ul><li>$M_e$ outputs rewards or scores ${r_1, r_2, \ldots}$ based on the trajectory $\tau_t$ (i.e., the sequence of Actor&rsquo;s actions and environmental feedback). This step corresponds to the internal feedback of $M_e$, or the results are directly given by the external environment.</li></ul></li><li><p><strong>Self-Reflection</strong></p><ul><li>The $M_{sr}$ module integrates the trajectory $\tau_t$ and the reward signal ${r_t}$ to generate self-correction or improvement suggestions $\mathrm{sr}_t$ at the language level.</li><li>Reflective text can be regarded as an analysis of errors or provide new启发思路, and is stored in long-term memory. In practice, we can vectorize the feedback information and store it in a vector database.</li></ul></li><li><p><strong>Update and Repeat</strong></p><ul><li>After appending the latest self-reflection text $\mathrm{sr}_t$ to the long-term memory, the Actor can use RAG to retrieve historically relevant information from it in the next iteration to adjust the policy.</li><li>Repeat the above steps until $M_e$ determines that the task is achieved or the maximum number of rounds is reached. In this loop, Reflexion relies on the continuous accumulation of <strong>self-reflection + long-term memory</strong> to improve decisions, rather than directly modifying model parameters.</li></ul></li></ul><p>The following shows examples of Reflexion&rsquo;s application in decision-making, programming, and reasoning tasks:</p><figure class=align-center><img loading=lazy src=reflextion_examples.png#center alt="Fig. 14. Reflexion works on decision-making 4.1, programming 4.3, and reasoning 4.2 tasks (Image source: Shinn et al. 2023)" width=100%><figcaption><p>Fig. 14. Reflexion works on decision-making 4.1, programming 4.3, and reasoning 4.2 tasks (Image source: <a href=https://arxiv.org/abs/2303.11366>Shinn et al. 2023</a>)</p></figcaption></figure><p>In an experiment on 100 HotPotQA questions, by comparing the CoT method and the method of adding episodic memory, the results show that after adding the self-reflection step at the end using the Reflexion method, its search, information retrieval, and reasoning capabilities are significantly improved.</p><figure class=align-center><img loading=lazy src=reflextion_result.png#center alt="Fig. 15. Comparative Analysis of Chain-of-Thought (CoT) and ReAct on the HotPotQA Benchmark (Image source: Shinn et al. 2023)" width=100%><figcaption><p>Fig. 15. Comparative Analysis of Chain-of-Thought (CoT) and ReAct on the HotPotQA Benchmark (Image source: <a href=https://arxiv.org/abs/2303.11366>Shinn et al. 2023</a>)</p></figcaption></figure><h3 id=deepseek-r1>DeepSeek R1<a hidden class=anchor aria-hidden=true href=#deepseek-r1>#</a></h3><p><strong>DeepSeek-R1</strong> (<a href=https://arxiv.org/abs/2501.12948>DeepSeek-AI, 2025</a>) represents a major breakthrough in the open-source community&rsquo;s replication of OpenAI o1 (<a href=https://openai.com/o1/>OpenAI, 2024</a>), successfully training an advanced reasoning model with deep reflection capabilities through reinforcement learning techniques.</p><blockquote><p>For a detailed description of the training process and technical implementation of DeepSeek R1, please refer to my previous blog post: <a href=https://syhya.github.io/posts/2025-01-27-deepseek-r1/>OpenAI o1 Replication Progress: DeepSeek-R1</a>.</p></blockquote><p>The key transformation in the training process of DeepSeek-R1-Zero - as training progresses, the model gradually <strong>emerges</strong> with excellent <strong>self-evolution</strong> capabilities. This capability is reflected in three core aspects:</p><ul><li><strong>Self-Reflection</strong>: The model can trace back and critically evaluate previous reasoning steps.</li><li><strong>Active Exploration</strong>: When it finds that the current solution path is not ideal, it can autonomously find and try alternative solutions.</li><li><strong>Dynamic Thinking Adjustment</strong>: Adaptively adjust the number of generated tokens according to the complexity of the problem to achieve a deeper thinking process.</li></ul><p>This dynamic and spontaneous reasoning behavior significantly improves the model&rsquo;s ability to solve complex problems, enabling it to respond to challenging tasks more efficiently and accurately.</p><figure class=align-center><img loading=lazy src=deepseek_r1_zero_response_time.png#center alt="Fig. 16. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)" width=90%><figcaption><p>Fig. 16. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: <a href=https://arxiv.org/abs/2501.12948>DeepSeek-AI, 2025</a>)</p></figcaption></figure><p>DeepSeek-R1-Zero also exhibited a typical &ldquo;aha moment&rdquo; during training. At this critical stage, the model suddenly realized that there was an error in the previous thinking path during the reasoning process, and then quickly adjusted the thinking direction, and finally successfully led to the correct answer. This phenomenon strongly proves that the model has developed strong <strong>self-correction</strong> and <strong>reflection capabilities</strong> during the reasoning process, similar to the aha experience in human thinking.</p><figure class=align-center><img loading=lazy src=aha_moment.png#center alt="Fig. 17. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: DeepSeek-AI, 2025)" width=90%><figcaption><p>Fig. 17. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: <a href=https://arxiv.org/abs/2501.12948>DeepSeek-AI, 2025</a>)</p></figcaption></figure><h2 id=memory>Memory<a hidden class=anchor aria-hidden=true href=#memory>#</a></h2><h3 id=human-memory>Human Memory<a hidden class=anchor aria-hidden=true href=#human-memory>#</a></h3><p><strong>Memory</strong> refers to the process of acquiring, storing, retaining, and retrieving information. Human memory is mainly divided into the following three categories:</p><figure class=align-center><img loading=lazy src=category_human_memory.png#center alt="Fig. 18. Categorization of human memory. (Image source: Weng, 2017)" width=100%><figcaption><p>Fig. 18. Categorization of human memory. (Image source: <a href=https://lilianweng.github.io/posts/2023-06-23-agent/>Weng, 2017</a>)</p></figcaption></figure><ul><li><p><strong>Sensory Memory:</strong> Used to briefly retain sensory information after the original stimulus (visual, auditory, tactile, etc.) disappears, usually lasting for milliseconds or seconds. Sensory memory is divided into:</p><ul><li>Visual Memory: The instantaneous image or visual impression retained by the visual channel, generally lasting 0.25-0.5 seconds, is used to form visual continuity in video or animation scenes.</li><li>Auditory Memory: The short-term storage of auditory information, which can last for several seconds, enables people to replay the sentences or sound clips they just heard.</li><li>Tactile Memory: Used to retain short-term tactile or force information, generally lasting from milliseconds to seconds, such as the short-term finger perception when tapping the keyboard or reading Braille.</li></ul></li><li><p><strong>Short-Term Memory:</strong> Stores the information we are currently aware of.</p><ul><li>Lasts about 20-30 seconds, and the capacity is usually 7±2 items.</li><li>Undertakes the temporary processing and maintenance of information during complex cognitive tasks such as learning and reasoning.</li></ul></li><li><p><strong>Long-Term Memory:</strong> Can store information for days to decades, and the capacity is almost unlimited. Long-term memory is divided into:</p><ul><li>Explicit Memory: Can be consciously recalled, including episodic memory (personal experiences, event details) and semantic memory (facts and concepts).</li><li>Implicit Memory: Unconscious memory, mainly related to skills and habits, such as riding a bicycle or touch typing.</li></ul></li></ul><p>These three types of human memory are intertwined and together constitute our cognition and understanding of the world. In building LLM Agents, we can also learn from this classification of human memory:</p><ul><li><strong>Sensory Memory</strong> corresponds to the embedding representation of the LLM&rsquo;s input raw data (such as text, pictures, and videos).</li><li><strong>Short-Term Memory</strong> corresponds to the LLM&rsquo;s in-context learning, which is limited by the model&rsquo;s context window <code>max_tokens</code>. When the dialogue length exceeds the window, early information will be truncated.</li><li><strong>Long-Term Memory</strong> corresponds to external vector storage or databases. Agents can retrieve historical information when needed based on <strong>RAG</strong> technology.</li></ul><h3 id=llm-agent-memory>LLM Agent Memory<a hidden class=anchor aria-hidden=true href=#llm-agent-memory>#</a></h3><p>When an agent interacts with users in multiple rounds and performs multi-step tasks, it can utilize different forms of memory and environmental information to complete the workflow.</p><figure class=align-center><img loading=lazy src=llm_memory_overview.png#center alt="Fig. 19. An overview of the sources, forms, and operations of the memory in LLM-based agents. (Image source: Zhang et al. 2024)" width=100%><figcaption><p>Fig. 19. An overview of the sources, forms, and operations of the memory in LLM-based agents. (Image source: <a href=https://arxiv.org/abs/2404.13501>Zhang et al. 2024</a>)</p></figcaption></figure><ul><li><p><strong>Text Memory</strong></p><ul><li>Complete Interaction: Records all dialogue and operation trajectories, helping the Agent trace back the context.</li><li>Recent Interaction: Only retains dialogue content that is highly relevant to the current task, reducing unnecessary context occupation.</li><li>Retrieved Interaction: The Agent can retrieve historical dialogues or records related to the current task from an external knowledge base and integrate them into the current context.</li><li>External Knowledge: When the Agent encounters a knowledge gap, it can retrieve and obtain additional information through APIs or external storage.</li></ul></li><li><p><strong>Parameterized Memory</strong></p><ul><li>Fine-tuning: Infuses new information or knowledge into the LLM, thereby expanding the model&rsquo;s internal knowledge.</li><li>Knowledge Editing: Modifies or updates existing knowledge at the model level, realizing dynamic adjustment of the memory of internal parameters of the model.</li></ul></li><li><p><strong>Environment</strong></p><ul><li>Represents the entities and context involved when the Agent interacts with users and external systems, such as user Alice, tools or interfaces that may be accessed (such as ticketing systems, streaming platforms, etc.).</li></ul></li><li><p><strong>Agent</strong></p><ul><li>The LLM Agent is responsible for read and write operations, that is, reading information from the external environment or knowledge base, and writing new actions or content.</li><li>It also includes a series of management functions, such as merging, reflection, forgetting, etc., to dynamically maintain short-term and long-term memory.</li></ul></li></ul><p>Another example is when an Agent needs to use both short-term and long-term memory to complete two different but related tasks:</p><ul><li><strong>Task A Play Video</strong>: The Agent records the current plan, operations, and environment state (such as search, click, play video, etc.) in short-term memory, which is stored in memory and the LLM&rsquo;s context window.</li><li><strong>Task B Download Game</strong>: The Agent utilizes the knowledge related to Arcane and League of Legend in long-term memory to quickly find a game download solution. The figure shows that the Agent searches on Google. We can regard Google&rsquo;s knowledge base as an external knowledge source, and all new search, click, and download operations will also be updated to short-term memory.</li></ul><figure class=align-center><img loading=lazy src=gui_agent_memory_illustration.png#center alt="Fig. 20: Illustration of short-term memory and long-term memory in an LLM-brained GUI agent. (Image source: Zhang et al. 2024)" width=100%><figcaption><p>Fig. 20: Illustration of short-term memory and long-term memory in an LLM-brained GUI agent. (Image source: <a href=https://arxiv.org/abs/2411.18279>Zhang et al. 2024</a>)</p></figcaption></figure><p>Common memory elements and their corresponding storage methods can be summarized in the following table:</p><table><thead><tr><th><strong>Memory Element</strong></th><th><strong>Memory Type</strong></th><th><strong>Description</strong></th><th><strong>Storage Medium / Method</strong></th></tr></thead><tbody><tr><td>Actions</td><td>Short-Term Memory</td><td>Historical action trajectories (e.g., clicking buttons, entering text)</td><td>Memory, LLM context window</td></tr><tr><td>Plan</td><td>Short-Term Memory</td><td>The previous step or the next step of the currently generated operation plan</td><td>Memory, LLM context window</td></tr><tr><td>Execution Results</td><td>Short-Term Memory</td><td>Results returned after action execution, error messages, and environmental feedback</td><td>Memory, LLM context window</td></tr><tr><td>Environment State</td><td>Short-Term Memory</td><td>Available buttons, page titles, system status, etc. in the current UI environment</td><td>Memory, LLM context window</td></tr><tr><td>Own Experience</td><td>Long-Term Memory</td><td>Historical task trajectories and execution steps</td><td>Database, disk</td></tr><tr><td>Self-Guidance</td><td>Long-Term Memory</td><td>Guidance rules and best practices summarized from historical successful trajectories</td><td>Database, disk</td></tr><tr><td>External Knowledge</td><td>Long-Term Memory</td><td>External knowledge bases, documents, or other data sources that assist in task completion</td><td>External database, vector retrieval</td></tr><tr><td>Task Success Metrics</td><td>Long-Term Memory</td><td>Records task success rate, failure rate, and other indicators for improvement and analysis</td><td>Database, disk</td></tr></tbody></table><p>In addition, researchers have proposed some new training and storage methods to enhance the memory capabilities of LLMs:</p><p><strong>LongMem (Language Models Augmented with Long-Term Memory)</strong> (<a href=https://arxiv.org/abs/2306.07174>Wang, et al. 2023</a>) enables LLMs to memorize long historical information. It adopts a decoupled network structure, freezing the original LLM parameters as a memory encoder, while using an Adaptive Residual Side-Network (SideNet) as a memory retriever for memory checking and reading.</p><figure class=align-center><img loading=lazy src=LongMem.png#center alt="Fig. 21. Overview of the memory caching and retrieval flow of LongMem. (Image source: Wang, et al. 2023)" width=100%><figcaption><p>Fig. 21. Overview of the memory caching and retrieval flow of LongMem. (Image source: <a href=https://arxiv.org/abs/2306.07174>Wang, et al. 2023</a>)</p></figcaption></figure><p>It is mainly composed of three parts: <strong>Frozen LLM</strong>, <strong>Residual SideNet</strong>, and <strong>Cached Memory Bank</strong>. Its workflow is as follows:</p><ul><li>First, the long text sequence is split into fixed-length segments. Each segment is encoded layer by layer in the Frozen LLM, and the attention $K, V \in \mathbb{R}^{H \times M \times d}$ vector pairs are extracted from the $m$-th layer and cached in the Cached Memory Bank.</li><li>When facing a new input sequence, the model retrieves the long-term memory bank based on the query-key of the current input, obtains the top $k$ key-value pairs (i.e., top-$k$ retrieval results) that are most relevant to the input, and integrates them into the subsequent language generation process; at the same time, the memory bank will remove the oldest content to ensure the availability of the latest context information.</li><li>The Residual SideNet fuses the hidden layer output of the frozen LLM with the retrieved historical key-values during the inference stage to complete the effective modeling and context utilization of ultra-long text.</li></ul><p>Through this decoupled design, LongMem can flexibly schedule massive historical information without expanding its native context window, taking into account both speed and long-term memory capabilities.</p><h2 id=tool-use>Tool Use<a hidden class=anchor aria-hidden=true href=#tool-use>#</a></h2><p>Tool use is an important part of LLM Agents. By giving LLMs the ability to call external tools, their functions are significantly expanded: they can not only generate natural language, but also obtain real-time information, perform complex calculations, and interact with various systems (such as databases, APIs, etc.), thereby effectively breaking through the limitations of pre-trained knowledge and avoiding the inefficient process of reinventing the wheel.</p><p>Traditional LLMs mainly rely on pre-trained data for text generation, but this also makes them insufficient in mathematical operations, data retrieval, and real-time information updates. Through tool calls, the model can:</p><ul><li><p><strong>Improve computing power:</strong> For example, by calling a dedicated calculator tool <a href=https://gpt.wolfram.com/index.php.en>Wolfram</a>, the model can perform more precise mathematical calculations, making up for its lack of arithmetic capabilities.</p></li><li><p><strong>Obtain real-time information:</strong> Using search engines like Google, Bing, or database APIs, the model can access the latest information to ensure the timeliness and accuracy of the generated content.</p></li><li><p><strong>Enhance information credibility:</strong> With the support of external tools, the model can cite real data sources, reduce the risk of information fabrication, and improve overall credibility.</p></li><li><p><strong>Improve system transparency:</strong> Tracking API call records can help users understand the model&rsquo;s decision-making process and provide a certain degree of interpretability.</p></li></ul><p>Currently, various LLM applications based on tool calls have emerged in the industry. They use different strategies and architectures to achieve comprehensive coverage from simple tasks to complex multi-step reasoning.</p><h3 id=toolformer>Toolformer<a hidden class=anchor aria-hidden=true href=#toolformer>#</a></h3><p><strong>Toolformer</strong> (<a href=https://arxiv.org/abs/2302.04761>Schick, et al. 2023</a>) is an LLM that can use external tools through simple APIs. It is trained by fine-tuning the GPT-J model, requiring only a few examples for each API. Toolformer learned to call tools including a question answering system, Wikipedia search, a calculator, a calendar, and a translation system:</p><figure class=align-center><img loading=lazy src=Toolformer_api.png#center alt="Fig. 22. Examples of inputs and outputs for all APIs used. (Image source: Schick, et al. 2023)" width=100%><figcaption><p>Fig. 22. Examples of inputs and outputs for all APIs used. (Image source: <a href=https://arxiv.org/abs/2302.04761>Schick, et al. 2023</a>)</p></figcaption></figure><h3 id=hugginggpt>HuggingGPT<a hidden class=anchor aria-hidden=true href=#hugginggpt>#</a></h3><p><strong>HuggingGPT</strong> (<a href=https://arxiv.org/abs/2302.04761>Shen, et al. 2023</a>) is a framework that uses ChatGPT as a task planner. It selects available models from <a href=https://huggingface.co/>HuggingFace</a> by reading model descriptions to complete user tasks, and summarizes based on the execution results.</p><figure class=align-center><img loading=lazy src=HuggingGPT.png#center alt="Fig. 23. Examples of inputs and outputs for all APIs used. (Image source: Shen, et al. 2023)" width=100%><figcaption><p>Fig. 23. Examples of inputs and outputs for all APIs used. (Image source: <a href=https://arxiv.org/abs/2302.04761>Shen, et al. 2023</a>)</p></figcaption></figure><p>The system consists of the following four stages:</p><ul><li><strong>Task Planning</strong>: Parses user requests into multiple subtasks. Each task contains four attributes: task type, ID, dependencies, and arguments. The paper uses few-shot prompting to guide the model in task splitting and planning.</li><li><strong>Model Selection</strong>: Assigns each subtask to different expert models, using a multiple-choice approach to determine the most suitable model. Due to the limited context length, models need to be initially filtered based on task type.</li><li><strong>Task Execution</strong>: Expert models execute the assigned specific tasks and record the results. The results are passed to the LLM for subsequent processing.</li><li><strong>Response Generation</strong>: Receives the execution results of each expert model and finally outputs a summary answer to the user.</li></ul><h2 id=llm-agent-applications>LLM Agent Applications<a hidden class=anchor aria-hidden=true href=#llm-agent-applications>#</a></h2><h3 id=generative-agent>Generative Agent<a hidden class=anchor aria-hidden=true href=#generative-agent>#</a></h3><p>The <strong>Generative Agent</strong> experiment (<a href=https://arxiv.org/abs/2304.03442>Park, et al. 2023</a>) simulates realistic human behavior in a sandbox environment through 25 virtual characters driven by large language models. Its core design integrates memory, retrieval, reflection, and planning and reaction mechanisms, allowing the Agent to record and review its own experiences, and extract key information from them to guide subsequent actions and interactions.</p><figure class=align-center><img loading=lazy src=generative_agent_sandbox.png#center alt="Fig. 24. The screenshot of generative agent sandbox. (Image source: Park, et al. 2023)" width=100%><figcaption><p>Fig. 24. The screenshot of generative agent sandbox. (Image source: <a href=https://arxiv.org/abs/2304.03442>Park, et al. 2023</a>)</p></figcaption></figure><p>The entire system uses a long-term memory module to record all observed events, combines a retrieval model to extract information based on recency, importance, and relevance, and then generates high-level inferences through a reflection mechanism, and finally converts these results into specific actions. This simulation experiment demonstrates emergent behaviors such as information diffusion, relationship memory, and social event coordination, providing a realistic human behavior simulation for interactive applications.</p><figure class=align-center><img loading=lazy src=generative_agent_architecture.png#center alt="Fig. 25. The generative agent architecture. (Park, et al. 2023)" width=100%><figcaption><p>Fig. 25. The generative agent architecture. (<a href=https://arxiv.org/abs/2304.03442>Park, et al. 2023</a>)</p></figcaption></figure><h3 id=webvoyager>WebVoyager<a hidden class=anchor aria-hidden=true href=#webvoyager>#</a></h3><p><strong>WebVoyager</strong> (<a href=https://arxiv.org/abs/2401.13919>He et al. 2024</a>) is an autonomous web interaction agent based on a large multimodal model that can control the mouse and keyboard for web browsing. WebVoyager uses the classic ReAct loop. In each interaction step, it views a browser screenshot annotated with a method similar to SoM (Set-of-Marks) (<a href=https://arxiv.org/abs/2310.11441>Yang, et al. 2023</a>) – providing interaction hints by placing numerical labels on web elements – and then decides the next action. This visual annotation combined with the ReAct loop allows users to interact with web pages through natural language. For specifics, you can refer to the <a href=https://langchain-ai.github.io/langgraph/tutorials/web-navigation/web_voyager/>WebVoyager code</a> using the LangGraph framework.</p><figure class=align-center><img loading=lazy src=WebVoyager.png#center alt="Fig. 26. The overall workflow of WebVoyager. (Image source: He et al. 2024)" width=100%><figcaption><p>Fig. 26. The overall workflow of WebVoyager. (Image source: <a href=https://arxiv.org/abs/2401.13919>He et al. 2024</a>)</p></figcaption></figure><h3 id=openai-operator>OpenAI Operator<a hidden class=anchor aria-hidden=true href=#openai-operator>#</a></h3><p><strong>Operator</strong> (<a href=https://openai.com/index/introducing-operator/>OpenAI, 2025</a>) is an AI agent recently released by OpenAI, designed to autonomously perform web tasks. Operator can interact with web pages like a human user, completing specified tasks through typing, clicking, and scrolling. The core technology of Operator is the <strong>Computer-Using Agent (CUA)</strong> (<a href=https://openai.com/index/computer-using-agent/>OpenAI, 2025</a>). CUA combines the visual capabilities of GPT-4o with stronger reasoning capabilities obtained through reinforcement learning, and is specially trained to interact with graphical user interfaces (GUIs), including buttons, menus, and text boxes that users see on the screen.</p><figure class=align-center><img loading=lazy src=cua_overview.png#center alt="Fig. 27. Overview of OpenAI CUA. (Image source: OpenAI, 2025)" width=100%><figcaption><p>Fig. 27. Overview of OpenAI CUA. (Image source: <a href=https://openai.com/index/computer-using-agent/>OpenAI, 2025</a>)</p></figcaption></figure><p>CUA operates in an iterative loop that includes three stages:</p><ul><li><p><strong>Perception</strong>: CUA &ldquo;observes&rdquo; the web page content by capturing browser screenshots. This vision-based input method enables it to understand the layout and elements of the page.</p></li><li><p><strong>Reasoning</strong>: With the help of chain-of-thought reasoning, CUA evaluates the next action based on the current and previous screenshots and the actions that have been performed. This reasoning ability enables it to track task progress, review intermediate steps, and make adjustments as needed.</p></li><li><p><strong>Action</strong>: CUA interacts with the browser by simulating mouse and keyboard operations (such as clicking, typing, and scrolling). This enables it to perform various web tasks without specific API integration.</p></li></ul><p>The difference between CUA and the previously existing WebVoyager is that this is an Agent specifically trained with reinforcement learning, rather than a fixed-process workflow built by directly calling GPT-4o. Although CUA is still in its early stages and has certain limitations, it has achieved SOTA results in the following benchmark tests.</p><figure class=align-center><img loading=lazy src=cua_benchmark.png#center alt="Fig. 28. OpenAI CUA Benchmark Results. (Image source: OpenAI, 2025)" width=100%><figcaption><p>Fig. 28. OpenAI CUA Benchmark Results. (Image source: <a href=https://openai.com/index/computer-using-agent/>OpenAI, 2025</a>)</p></figcaption></figure><h2 id=deep-research>Deep Research<a hidden class=anchor aria-hidden=true href=#deep-research>#</a></h2><p>Deep Research is essentially a report generation system: given a user&rsquo;s query, the system uses an LLM as the core Agent, and after multiple rounds of iterative information retrieval and analysis, it finally generates a structured and informative report. Currently, the implementation logic of various Deep Research systems can be mainly divided into two methods: <strong>Workflow Agent</strong> and <strong>RL Agent</strong>.</p><h3 id=workflow-agent-vs-rl-agent>Workflow Agent vs RL Agent<a hidden class=anchor aria-hidden=true href=#workflow-agent-vs-rl-agent>#</a></h3><p>The Workflow Agent approach relies on developers pre-designing workflows and manually constructing Prompts to organize the entire report generation process. The main features include:</p><ul><li><strong>Task Decomposition and Process Orchestration</strong>: The system breaks down the user query into several subtasks, such as generating an outline, information retrieval, content summarization, etc., and then executes them in a predetermined process sequence.</li><li><strong>Fixed Process</strong>: The calls and interactions between each stage are pre-set, similar to building a static flow chart or directed acyclic graph (DAG), ensuring that each step has a clear responsibility.</li><li><strong>Manual Design Dependence</strong>: This method mainly relies on the experience of engineers, and improves the output quality by repeatedly debugging Prompts. It is highly applicable but has limited flexibility.</li></ul><p>The <a href=https://langchain-ai.github.io/langgraph/>LangGraph</a> framework can be used to build and orchestrate workflows in the form of graphs.</p><figure class=align-center><img loading=lazy src=langgraph_workflow.png#center alt="Fig. 29. A workflow of the LangGraph. (Image source: LangGraph, 2025)" width=100%><figcaption><p>Fig. 29. A workflow of the LangGraph. (Image source: <a href="https://langchain-ai.github.io/langgraph/tutorials/workflows/?h=workflow">LangGraph, 2025</a>)</p></figcaption></figure><p>Currently, there are multiple open-source projects on Github that implement Deep Research Agents based on workflows, such as <a href=https://github.com/assafelovic/gpt-researcher>GPT Researcher</a> and <a href=https://github.com/langchain-ai/open_deep_research>open deep research</a>.</p><figure class=align-center><img loading=lazy src=open_deep_research.png#center alt="Fig. 30. An overview of the open deep research. (Image source: LangChain, 2025)" width=100%><figcaption><p>Fig. 30. An overview of the open deep research. (Image source: <a href=https://github.com/langchain-ai/open_deep_research>LangChain, 2025</a>)</p></figcaption></figure><p>The RL Agent is another implementation method that optimizes the Agent&rsquo;s multi-round search, analysis, and report writing process by training a reasoning model with RL. The main features include:</p><ul><li><strong>Autonomous Decision-Making Ability</strong>: The system is trained through reinforcement learning, allowing the Agent to autonomously judge, make decisions, and adjust strategies when facing complex search and content integration tasks, thereby generating reports more efficiently.</li><li><strong>Continuous Optimization</strong>: Using a reward mechanism to score and provide feedback on the generation process, the Agent can continuously iterate and optimize its own strategy, improving the overall quality from task decomposition to final report generation.</li><li><strong>Reduced Manual Intervention</strong>: Compared to fixed processes that rely on manual Prompts, the reinforcement learning training method reduces the dependence on manual design and is more suitable for dealing with changing and complex real-world application scenarios.</li></ul><p>The following table summarizes the main differences between these two methods:</p><table><thead><tr><th>Feature</th><th>Workflow Agent</th><th>RL Agent</th></tr></thead><tbody><tr><td><strong>Process Design</strong></td><td>Pre-designed fixed workflow, clear task decomposition and process orchestration</td><td>End-to-end learning, Agent autonomous decision-making and dynamic process adjustment</td></tr><tr><td><strong>Autonomous Decision-Making</strong></td><td>Relies on manually designed Prompts, the decision-making process is fixed and immutable</td><td>Through reinforcement learning, the Agent can autonomously judge, make decisions, and optimize strategies</td></tr><tr><td><strong>Manual Intervention</strong></td><td>Requires a lot of manual design and debugging of Prompts, more manual intervention</td><td>Reduces manual intervention, achieves automatic feedback and continuous optimization through a reward mechanism</td></tr><tr><td><strong>Flexibility and Adaptability</strong></td><td>Less adaptable to complex or changing scenarios, limited scalability</td><td>More adaptable to changing and complex real-world scenarios, with high flexibility</td></tr><tr><td><strong>Optimization Mechanism</strong></td><td>Optimization mainly relies on the engineer&rsquo;s experience adjustment, lacking an end-to-end feedback mechanism</td><td>Uses reinforcement learning&rsquo;s reward feedback to achieve continuous and automated performance improvement</td></tr><tr><td><strong>Implementation Difficulty</strong></td><td>Relatively straightforward to implement, but requires cumbersome process design and maintenance</td><td>Requires training data and computing resources, with a larger initial development investment, but better long-term results</td></tr><tr><td><strong>Training Required</strong></td><td>No additional training required, only relies on manually constructed processes and Prompts</td><td>Requires training the Agent through reinforcement learning to achieve autonomous decision-making</td></tr></tbody></table><h3 id=openai-deep-research>OpenAI Deep Research<a hidden class=anchor aria-hidden=true href=#openai-deep-research>#</a></h3><p><strong>OpenAI Deep Research</strong> (<a href=https://openai.com/index/introducing-deep-research/>OpenAI, 2025</a>) is an intelligent Agent officially released by OpenAI in February 2025, designed for complex scenarios. It can automatically search, filter, analyze, and integrate multi-source information, and finally generate high-quality comprehensive reports. The system uses <a href=https://openai.com/index/openai-o3-mini/>o3</a> as the core base, and combined with reinforcement learning methods, significantly improves the accuracy and robustness of the multi-round iterative search and reasoning process.</p><p>Compared with traditional ChatGPT plug-in search or conventional RAG technology, OpenAI Deep Research has the following outstanding advantages:</p><ol><li><p><strong>Reinforcement Learning-Driven Iterative Reasoning</strong>
With the help of the <strong>o3 reasoning model</strong> and reinforcement learning training strategies, the Agent can continuously optimize its own reasoning path during the multi-round search and summarization process, effectively reducing the risk of distortion caused by error accumulation.</p></li><li><p><strong>Integration and Cross-Validation of Multi-Source Information</strong>
Breaking through the limitations of a single search engine, it can simultaneously call multiple authoritative data sources such as specific databases and professional knowledge bases, and form more reliable research conclusions through cross-validation.</p></li><li><p><strong>High-Quality Report Generation</strong>
The LLM-as-a-judge scoring mechanism and strict evaluation criteria are introduced in the training phase, so that the system can conduct self-evaluation when outputting reports, thereby generating professional texts with clearer structures and more rigorous arguments.</p></li></ol><h4 id=training-process>Training Process<a hidden class=anchor aria-hidden=true href=#training-process>#</a></h4><p>The OpenAI Deep Research training process uses a <strong>browser interaction dataset</strong> specifically customized for research scenarios. Through these datasets, the model masters core browsing functions - including search, click, scroll, and file parsing; at the same time, it learns the ability to use Python tools in a sandbox environment for calculation, data analysis, and visualization. In addition, with the help of reinforcement learning training on these browsing tasks, the model can efficiently perform information retrieval, integration, and reasoning in a large number of websites, quickly locate key information, or generate comprehensive research reports.</p><p>These training datasets include both objective tasks with standard answers and automatic scoring, and open-ended tasks with detailed scoring rubrics. During the training process, the model&rsquo;s responses are strictly compared with standard answers or scoring criteria, and the evaluation model provides feedback by using the CoT thinking process generated by the model.</p><p>At the same time, the training process reuses the safety datasets accumulated during the o1 model training phase, and specifically adds safety training data for Deep Research scenarios to ensure that the model strictly complies with relevant compliance and safety requirements during the automated search and browsing process.</p><h4 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h4><p>In the benchmark test <strong>Humanity&rsquo;s Last Exam</strong> (<a href=https://arxiv.org/abs/2501.14249>Phan, et al. 2025</a>), which evaluates the ability of AI to answer expert-level questions in various professional fields, the model achieved SOTA results.</p><figure class=align-center><img loading=lazy src=human_last_exam.png#center alt="Fig. 31. Humanity&rsquo;s Last Exam Benchmark Results。(Image source: OpenAI, 2025)" width=80%><figcaption><p>Fig. 31. Humanity&rsquo;s Last Exam Benchmark Results。(Image source: <a href=https://openai.com/index/introducing-deep-research/>OpenAI, 2025</a>)</p></figcaption></figure><h2 id=future-development-directions>Future Development Directions<a hidden class=anchor aria-hidden=true href=#future-development-directions>#</a></h2><p>Agents show broad prospects, but to achieve reliable and widespread application, the following key challenges still need to be addressed:</p><ul><li><p><strong>Context Window Limitation</strong>: The context window of LLMs limits the amount of information processed, affecting long-term planning and memory capabilities, and reducing task coherence. Current research explores external memory mechanisms and context compression techniques to enhance long-term memory and complex information processing capabilities. Currently, OpenAI&rsquo;s latest model <strong>GPT-4.5</strong> (<a href=https://openai.com/index/introducing-gpt-4-5/>OpenAI, 2025</a>) has a maximum context window of 128k tokens.</p></li><li><p><strong>Interface Standardization and Interoperability</strong>: The current natural language-based tool interaction has the problem of inconsistent formats. The <strong>Model Context Protocol (MCP)</strong> (<a href=https://www.anthropic.com/news/model-context-protocol>Anthropic, 2024</a>) unifies the interaction between LLMs and applications through open standards, reducing development complexity, improving system stability, and cross-platform compatibility.</p></li><li><p><strong>Task Planning and Decomposition Capabilities</strong>: Agents have difficulty formulating coherent plans, effectively decomposing subtasks, and lack the ability to dynamically adjust in unexpected situations in complex tasks. More powerful planning algorithms, self-reflection mechanisms, and dynamic strategy adjustment methods are needed to flexibly respond to uncertain environments.</p></li><li><p><strong>Computing Resources and Economic Benefits</strong>: Deploying large model agents is costly due to multiple API calls and intensive computing, limiting some practical application scenarios. Optimization directions include efficient model structure, quantization technology, inference optimization, caching strategies, and intelligent scheduling mechanisms. With the development of dedicated GPU hardware such as <a href=https://www.nvidia.com/en-sg/data-center/dgx-b200/>NVIDIA DGX B200</a> and distributed technologies, computing efficiency is expected to be significantly improved.</p></li><li><p><strong>Security Protection and Privacy Assurance</strong>: Agents face security risks such as prompt injection, and need to establish sound authentication, access control, input validation, and sandbox environments. For multimodal input and external tools, it is necessary to strengthen data anonymization, the principle of least privilege, and audit logs to meet security and privacy compliance requirements.</p></li><li><p><strong>Decision Transparency and Interpretability</strong>: Agent decisions are difficult to explain, limiting their application in high-risk areas. Enhancing interpretability requires the development of visualization tools, chain-of-thought tracking, and decision reason generation mechanisms to improve decision transparency, enhance user trust, and meet regulatory requirements.</p></li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] DAIR.AI. <a href=https://www.promptingguide.ai/research/llm-agents>&ldquo;LLM Agents.&rdquo;</a> Prompt Engineering Guide, 2024.</p><p>[2] Sutton, Richard S., and Andrew G. Barto. <a href=http://incompleteideas.net/book/the-book.html>&ldquo;Reinforcement Learning: An Introduction.&rdquo;</a> MIT Press, 2018.</p><p>[3] Weng, Lilian. <a href=https://lilianweng.github.io/posts/2023-06-23-agent/>&ldquo;LLM-powered Autonomous Agents.&rdquo;</a> Lil’Log, 2023.</p><p>[4] Wei, Jason, et al. <a href=https://arxiv.org/abs/2201.11903>&ldquo;Chain-of-thought prompting elicits reasoning in large language models.&rdquo;</a> Advances in neural information processing systems 35 (2022): 24824-24837.</p><p>[5] Kojima, Takeshi, et al. <a href=https://arxiv.org/abs/2205.11916>&ldquo;Large language models are zero-shot reasoners.&rdquo;</a> Advances in neural information processing systems 35 (2022): 22199-22213.</p><p>[6] Wang, Xuezhi, et al. <a href=https://arxiv.org/abs/2203.11171>&ldquo;Self-consistency improves chain of thought reasoning in language models.&rdquo;</a> arXiv preprint arXiv:2203.11171 (2022).</p><p>[7] Wang, Xuezhi, et al. <a href=https://arxiv.org/abs/2207.00747>&ldquo;Rationale-augmented ensembles in language models.&rdquo;</a> arXiv preprint arXiv:2207.00747 (2022).</p><p>[8] Zelikman, Eric, et al. <a href=https://arxiv.org/abs/2203.14465>&ldquo;Star: Bootstrapping reasoning with reasoning.&rdquo;</a> Advances in Neural Information Processing Systems 35 (2022): 15476-15488.</p><p>[9] Fu, Yao, et al. <a href=https://arxiv.org/abs/2210.00720>&ldquo;Complexity-based prompting for multi-step reasoning.&rdquo;</a> arXiv preprint arXiv:2210.00720 (2022).</p><p>[10] Yao, Shunyu, et al. <a href=https://arxiv.org/abs/2305.10601>&ldquo;Tree of thoughts: Deliberate problem solving with large language models.&rdquo;</a> Advances in neural information processing systems 36 (2023): 11809-11822.</p><p>[11] Yao, Shunyu, et al. <a href=https://arxiv.org/abs/2210.03629>&ldquo;React: Synergizing reasoning and acting in language models.&rdquo;</a> International Conference on Learning Representations (ICLR). 2023.</p><p>[12] Shinn, Noah, et al. <a href=https://arxiv.org/abs/2303.11366>&ldquo;Reflexion: Language agents with verbal reinforcement learning.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 8634-8652.</p><p>[13] Guo, Daya, et al. <a href=https://arxiv.org/abs/2501.12948>&ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.&rdquo;</a> arXiv preprint arXiv:2501.12948 (2025).</p><p>[14] OpenAI. <a href=https://openai.com/o1/>&ldquo;Introducing OpenAI o1&rdquo;</a> OpenAI, 2024.</p><p>[15] Zhang, Zeyu, et al. <a href=https://arxiv.org/abs/2404.13501>&ldquo;A survey on the memory mechanism of large language model based agents.&rdquo;</a> arXiv preprint arXiv:2404.13501 (2024).</p><p>[16] Zhang, Chaoyun, et al. <a href=https://arxiv.org/abs/2411.18279>&ldquo;Large language model-brained gui agents: A survey.&rdquo;</a> arXiv preprint arXiv:2411.18279 (2024).</p><p>[17] Wang, Weizhi, et al. <a href=https://arxiv.org/abs/2306.07174>&ldquo;Augmenting language models with long-term memory.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 74530-74543.</p><p>[18] Schick, Timo, et al. <a href=https://arxiv.org/abs/2302.04761>&ldquo;Toolformer: Language models can teach themselves to use tools.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 68539-68551.</p><p>[19] Shen, Yongliang, et al. <a href=https://arxiv.org/abs/2303.17580>&ldquo;Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 38154-38180.</p><p>[20] Park, Joon Sung, et al. <a href=https://arxiv.org/abs/2304.03442>&ldquo;Generative agents: Interactive simulacra of human behavior.&rdquo;</a> Proceedings of the 36th annual acm symposium on user interface software and technology. 2023.</p><p>[21] He, Hongliang, et al. <a href=https://arxiv.org/abs/2401.13919>&ldquo;WebVoyager: Building an end-to-end web agent with large multimodal models.&rdquo;</a> arXiv preprint arXiv:2401.13919 (2024).</p><p>[22] Yang, Jianwei, et al. <a href=https://arxiv.org/abs/2310.11441>&ldquo;Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v.&rdquo;</a> arXiv preprint arXiv:2310.11441 (2023).</p><p>[23] OpenAI. <a href=https://openai.com/index/introducing-operator/>&ldquo;Introducing Operator.&rdquo;</a> OpenAI, 2025.</p><p>[24] OpenAI. <a href=https://openai.com/index/computer-using-agent/>&ldquo;Computer-Using Agent.&rdquo;</a> OpenAI, 2025.</p><p>[25] OpenAI. <a href=https://openai.com/index/introducing-deep-research/>&ldquo;Introducing Deep Research.&rdquo;</a> OpenAI, 2025.</p><p>[26] Phan, Long, et al. <a href=https://arxiv.org/abs/2501.14249>&ldquo;Humanity&rsquo;s Last Exam.&rdquo;</a> arXiv preprint arXiv:2501.14249 (2025).</p><p>[27] OpenAI. <a href=https://openai.com/index/introducing-gpt-4-5/>&ldquo;Introducing GPT-4.5.&rdquo;</a> OpenAI, 2025.</p><p>[28] Anthropic. <a href=https://www.anthropic.com/news/model-context-protocol>&ldquo;Introducing the Model Context Protocol.&rdquo;</a> Anthropic, 2024.</p><p>[29] LangGraph. <a href="https://langchain-ai.github.io/langgraph/tutorials/workflows/?h=workflow">&ldquo;A workflow of the LangGraph.&rdquo;</a> LangGraph Tutorials, 2025.</p><p>[30] Assaf Elovic. <a href=https://github.com/assafelovic/gpt-researcher>&ldquo;GPT Researcher&rdquo;</a> GitHub Repository, 2025.</p><p>[31] LangChain. <a href=https://github.com/langchain-ai/open_deep_research>&ldquo;Open Deep Research&rdquo;</a> GitHub Repository, 2025.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: Please cite the original author and source when reprinting or referencing the content of this article.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui.(Mar 2025). Large Language Model Agents.
<a href=https://syhya.github.io/posts/2025-03-27-llm-agent>https://syhya.github.io/posts/2025-03-27-llm-agent</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025llm-agent</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;Large Language Model Agents&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Mar&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-03-27-llm-agent&#34;</span>  
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/agent/>Agent</a></li><li><a href=https://syhya.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://syhya.github.io/tags/planning/>Planning</a></li><li><a href=https://syhya.github.io/tags/memory/>Memory</a></li><li><a href=https://syhya.github.io/tags/tool-use/>Tool Use</a></li><li><a href=https://syhya.github.io/tags/deep-research/>Deep Research</a></li><li><a href=https://syhya.github.io/tags/react/>ReAct</a></li><li><a href=https://syhya.github.io/tags/reflexion/>Reflexion</a></li><li><a href=https://syhya.github.io/tags/webvoyager/>WebVoyager</a></li><li><a href=https://syhya.github.io/tags/openai-operator/>OpenAI Operator</a></li><li><a href=https://syhya.github.io/tags/cot/>CoT</a></li><li><a href=https://syhya.github.io/tags/tot/>ToT</a></li><li><a href=https://syhya.github.io/tags/workflow/>Workflow</a></li></ul><nav class=paginav><a class=next href=https://syhya.github.io/posts/2025-03-01-train-llm/><span class=title>Next »</span><br><span>Parallelism and Memory Optimization Techniques for Training Large Models</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on x" href="https://x.com/intent/tweet/?text=Large%20Language%20Model%20Agents&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f&amp;hashtags=LLM%2cAI%2cAgent%2cReinforcementLearning%2cPlanning%2cMemory%2cToolUse%2cDeepResearch%2cReAct%2cReflexion%2cWebVoyager%2cOpenAIOperator%2cCoT%2cToT%2cworkflow"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f&amp;title=Large%20Language%20Model%20Agents&amp;summary=Large%20Language%20Model%20Agents&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f&title=Large%20Language%20Model%20Agents"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on whatsapp" href="https://api.whatsapp.com/send?text=Large%20Language%20Model%20Agents%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on telegram" href="https://telegram.me/share/url?text=Large%20Language%20Model%20Agents&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Agents on ycombinator" href="https://news.ycombinator.com/submitlink?t=Large%20Language%20Model%20Agents&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-27-llm-agent%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>