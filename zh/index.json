[{"content":"背景 随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。\n本文基于作者的工作经验，总结了如何在现有通用模型的基础上，通过数据准备、模型训练、部署、评估及持续迭代，构建具备特定领域知识的大语言模型。\n为什么要向基座模型注入领域知识 挑战一：有限的领域知识 现有的预训练模型（如 GPT-4、Llama 3）主要基于通用语料库进行训练，缺乏对小众语言或专有领域的深入理解，导致模型在处理编程代码时表现不佳。\n挑战二：数据安全与合规 企业在处理敏感数据时，必须遵循严格的数据主权和合规性要求。将私有业务数据上传至第三方云服务存在安全隐患，因此需要在本地环境中完成数据处理与模型训练。\n挑战三：OpenAI微调的局限 主流商用 API 的微调方案通常较为基础，难以实现深层次的对齐和优化。对于需要高度定制化的领域模型，这种方法难以满足需求。\n注入知识两种方法 在实际项目中，常见的将领域知识注入基座模型的方法主要包括 微调 (Fine-Tuning) 和 检索增强生成 (RAG)。下文将详细对比这些方法，以帮助选择最适合的策略。\n方法对比 微调 (Fine-Tuning) 核心思路\n通过持续预训练、监督微调和偏好对齐，直接更新模型参数，使其掌握特定领域知识和任务模式。\n技术细节\n继续预训练 (CPT)：在大量领域特定的无监督数据上继续预训练基座模型。 监督微调 (SFT)：使用高质量的标注数据进行有监督微调。 偏好对齐 (DPO)：通过用户反馈优化模型输出。 参数微调方法：使用全参数微调或者结合 LoRA 等 PEFT 方法冻结部分参数并添加 adapter。 优势\n深度定制：模型内部权重更新，能够深入理解领域知识。 无需依赖外部检索：推理时不需额外的知识库支持，减少延迟和总的 token 消耗。 提升整体性能：在特定领域任务上表现显著优于通用模型。 劣势\n高计算成本：需要大量计算资源进行训练，尤其是 CPT 阶段。 训练周期长：从数据准备到模型训练再到优化，需要较长时间。 灾难性遗忘：模型可能在学习新知识的同时，遗忘原有的通用能力。 检索增强生成 (RAG) 核心思路\n构建领域知识库，在推理阶段检索相关文档，辅助模型生成更准确的回答，无需直接改变模型参数。\n技术细节\n数据处理：对领域文档进行预处理，按块大小和重叠量切分。 向量化：使用文本嵌入模型将文本块转换为向量，存储在向量数据库中。 召回：推理时通过相似度搜索召回相关文档，作为上下文信息或 few-shot 示例提供给基座模型。 优势\n保持通用能力：模型参数不变，仍保留通用语言能力。 快速更新：知识库可动态更新，无需重新训练模型。 计算效率：避免大规模训练，节省计算资源。 劣势\n依赖知识库质量：检索到的文档质量直接影响回答质量。 推理速度：检索过程可能增加推理延迟，并且需要更多的 token。 知识覆盖有限：模型内部知识仍受限于基座模型的预训练数据。 模型与训练资源 基座模型 以 Llama 3 系列 为例，其具有以下特点：\n参数规模\nLlama 3 系列涵盖从 1B 到 405B 参数的模型，广泛支持多语言处理、代码生成、推理，以及视觉和文本任务。小型模型（1B 和 3B）经过专门优化，适合边缘和移动设备，支持最大 128K 的上下文窗口，可高效处理本地任务，例如摘要生成、指令执行和文本重写。\n多模态能力\nLlama 3 的视觉模型（11B 和 90B 参数）在图像理解任务上的表现优于许多封闭模型，同时支持图像、视频和语音的多模态处理。所有模型均支持微调，便于针对特定领域进行定制化开发。\n开源与社区支持\nLlama 3 系列模型及其权重以开源形式发布，可通过 llama.com 和 Hugging Face 平台 获取，为开发者提供便捷的访问和应用支持。\n数据集限制\n虽然 Llama 3 模型本身以开源形式发布，但其训练所使用的数据集并未开源。因此，严格来说，Llama 3 并非完全开源的模型。这一限制可能会在解决灾难性遗忘问题时带来挑战，因为难以获得与原始训练完全一致的数据集。\n训练资源 训练大型语言模型需要强大的计算资源和高效的分布式训练框架。\n硬件资源\nGPU 集群：建议使用 NVIDIA A100 或 H100 GPU，4卡或8卡配置，通过 NVLink 或 InfiniBand 提升通信带宽。 存储资源：高性能 SSD（如 NVMe）以支持快速的数据读写。 软件框架\n并行框架：DeepSpeed、Megatron-LM 等分布式训练框架，支持大规模模型训练。 推理框架：vLLM、ollama 等，优化推理速度和资源利用。 并行策略\n数据并行：适用于单卡可容纳模型的情况，通过 DeepSpeed 的 ZeRO Stage 0 实现。 模型并行、流水线并行和张量并行：单卡无法容纳时，采用 ZeRO Stage 1、2、3 进行优化，或使用 ZeRO-Infinity 将参数和优化器状态部分卸载到 CPU 或 NVMe。 DeepSpeed ZeRO 分片策略对比 为了更好地理解 DeepSpeed 的 ZeRO 分片策略，以下将分为不同的部分进行详细说明。\nZeRO Stage 分片策略 ZeRO Stage 描述 显存占用 训练速度 ZeRO-0 纯数据并行，不进行任何分片。所有优化器状态、梯度和参数在每张 GPU 上完全复制。 最高 最快 ZeRO-1 分片优化器状态（例如动量和二阶矩），减少显存占用，但梯度和参数仍为数据并行。 高 略慢于 ZeRO-0 ZeRO-2 分片优化器状态和梯度，在 ZeRO-1 的基础上进一步减少显存占用。 中 慢于 ZeRO-1 ZeRO-3 分片优化器状态、梯度和模型参数，显存占用最低，适合大规模模型。但需要在前向和后向时进行参数广播（All-Gather/All-Reduce），通信量显著增加。 低 明显慢于 ZeRO-2，取决于模型大小和网络带宽 Offload 策略 Offload 类型 描述 显存占用 训练速度 ZeRO-1 + CPU Offload 在 ZeRO-1 的基础上，将优化器状态卸载到 CPU 内存；可进一步降低 GPU 显存占用，但需要 CPU-GPU 数据传输，依赖 PCIe 带宽，且占用 CPU 内存。 中偏低 慢于 ZeRO-1，受 CPU 性能和 PCIe 带宽影响 ZeRO-2 + CPU Offload 在 ZeRO-2 的基础上，将优化器状态卸载到 CPU 内存；对较大模型进一步降低 GPU 显存占用，但会增加 CPU-GPU 数据传输开销。 较低 慢于 ZeRO-2，受 CPU 性能和 PCIe 带宽影响 ZeRO-3 + CPU Offload 在 ZeRO-3 的基础上，将优化器状态和模型参数卸载到 CPU；GPU 显存占用最小，但 CPU-GPU 通信量极大，且 CPU 带宽远小于 GPU-GPU 通信。 极低 非常慢 ZeRO-Infinity (NVMe Offload) 基于 ZeRO-3，将优化器状态、梯度和参数卸载到 NVMe，突破 CPU 内存限制，适合超大规模模型；性能高度依赖 NVMe 并行读写速度。 极低需 NVMe 支持 慢于 ZeRO-3，但通常优于 ZeRO-3 + CPU Offload 通信量与性能影响 ZeRO-0/1/2\n通信以 梯度同步 为主，使用 All-Reduce 操作，通信量相对较低。 ZeRO-3\n需要对模型参数进行 All-Gather/All-Reduce 操作，通信量显著增大，网络带宽成为关键瓶颈，前后传播时的参数广播进一步加剧通信负担。 CPU Offload（ZeRO-1/2/3 + CPU）\n卸载优化器状态或参数到 CPU，减少 GPU 显存占用。 通信量主要来自 CPU \u0026lt;-\u0026gt; GPU 数据传输，带宽远低于 GPU-GPU 通信，极易造成性能瓶颈，尤其在 ZeRO-3 场景下。 NVMe Offload（ZeRO-Infinity）\n在 ZeRO-3 的基础上进一步卸载至 NVMe，突破 CPU 内存限制以支持超大规模模型。 性能强烈依赖 NVMe I/O 带宽 和并行度，若 NVMe 速度足够高，通常优于 CPU Offload；但在 I/O 性能较弱或高延迟场景下，效果可能不佳。 硬件与配置影响 硬件限制\nPCIe 带宽、网络带宽、NVMe I/O 等对 Offload 性能有显著影响，需根据硬件环境选择最佳策略。 补充说明\nCPU Offload 利用 CPU 内存并通过 PCIe 传输数据；NVMe Offload 则将状态保存于 NVMe 设备。 NVMe Offload 在 NVMe I/O 性能充足 时通常优于 CPU Offload，但需避免因 I/O 性能不足导致的性能瓶颈。 与官方文档对照\n建议结合 DeepSpeed 官方文档 获取最新、最准确的配置参数和性能调优建议。 数据准备：决定训练成败的核心 数据质量直接决定了模型的性能。数据准备包括数据收集、清洗、去重、分类与配比、脱敏等步骤。\n预训练数据 数据来源 公开数据集：如：the-stack-v2、Common Crawl 等。 企业自有数据：内部文档、代码库、业务日志等。 网络爬虫：通过爬虫技术采集领域相关的网页内容。 数据规模 建议使用至少数亿到数十亿个 token，以确保模型能够充分学习领域知识。 当数据量不足时，模型效果可能受限，建议采用数据增强的方法来补充数据。 数据处理 数据预处理\n格式统一：对来自多个数据源的无标注大量语料进行处理，确保其格式一致。推荐使用高效的存储格式，如 Parquet，以提高数据读取和处理的效率。 数据去重\n检测方法：使用 MinHash、SimHash 或余弦相似度等算法进行近似重复检测。 处理粒度：可选择按句子、段落或文档级别去重，根据任务需求灵活调整。 相似度阈值：设定合理的相似度阈值（如 0.9），删除重复度高于阈值的文本，确保数据多样性。 数据清洗\n文本过滤：结合规则和模型评分器（如 BERT/RoBERTa）去除乱码、拼写错误和低质量文本。 格式化处理：优先使用 JSON 格式处理数据，确保代码、Markdown 和 LaTeX 等特殊格式的准确性。 数据脱敏\n隐私保护：匿名化或去除人名、电话号码、邮箱、密码等敏感信息，确保数据合规。 不合规内容过滤：剔除含有违法、色情或种族歧视等内容的数据块。 数据混合与配比\n比例控制：例如，将 70% 的领域特定数据与 30% 的通用数据相结合，避免模型遗忘通用能力。 任务类型：确保数据包含代码生成、问答对话、文档摘要、多轮对话和数学推理等多种任务类型。 数据顺序\n逐步引导：采用课程学习（Curriculum Learning）方法，从简单、干净的数据开始训练，逐步引入更复杂或噪声较高的数据，优化模型的学习效率和收敛路径。 语义连贯性：利用上下文预训练（In-context Pretraining）技术，将语义相似的文档拼接在一起，增强上下文一致性，提升模型的语义理解深度与泛化能力。 监督微调数据 数据格式 可采用 Alpaca 或 Vicuna 风格，比如结构化为 [instruction, input, output] 的单轮和多轮对话。\n规模：几千条到几十万条，具体根据项目需求和计算资源决定。 质量：确保数据的高质量和多样性，避免模型学习到错误或偏见。 数据构建 在数据构建过程中，我们首先收集日常业务数据，并与业务专家共同构建基础问题。随后，利用大语言模型进行数据增强，以提升数据的多样性和鲁棒性。以下是具体的数据增强策略：\n数据增强策略 表达多样化\n通过大语言模型对现有数据进行改写，采用同义词替换和语法变换等方法，增加数据的多样性。\n鲁棒性增强\n构建包含拼写错误、混合语言等输入的提示（Prompt），以模拟真实场景，同时确保生成答案的高质量。\n知识蒸馏\n利用 GPT-4、Claude 等大型语言模型进行知识蒸馏，生成符合需求的问答数据对。\n复杂任务设计\n针对复杂场景（如多轮对话、逻辑推理等），手动设计高质量数据，以覆盖模型的能力边界。\n数据生成管道\n构建自动化数据生成流水线，将数据生成、筛选、格式化和校验等环节集成，提高整体效率。\n关键要点 任务类型标注：每条数据标注明确的任务类型，便于后续精细化分析和调优。 多轮对话与话题切换：构建多轮对话中上下文关联与话题转换的数据，确保模型能够学习话题切换与上下文关联的能力。 思维链（Chain of Thought）策略：分类、推理等任务可先用 COT 生成过程性答案，提高准确率。 数据飞轮：上线后持续收集用户真实问题，结合真实需求迭代数据；定期清洗，确保质量与多样性。 偏好数据 数据格式 三元组结构：[prompt, chosen answer, rejected answer] 标注细节： 多模型采样：使用多个不同训练阶段或不同数据配比的模型生成回答，增加数据多样性。 编辑与优化：标注人员可对选择的回答进行小幅修改，确保回答质量。 采样策略 多模型采样：部署多个不同版本的模型，对同一 prompt 生成不同回答。 对比标注：由人工或自动化系统对生成的回答进行对比，选择更优的回答对。 关键要点 数据多样性与覆盖：确保偏好数据涵盖各种场景和任务，避免模型在特定情境下表现不佳。 高质量标注：偏好数据的质量直接影响模型的对齐效果，需确保标注准确且一致。 训练流程 一个完整的特定领域大语言模型训练流程通常包括 继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO) 三个主要步骤，最终实现模型的部署与持续优化。\n三种方法对比 训练方法概览 训练方法 主要目标 数据需求 典型应用场景 继续预训练 (CPT) 继续在大规模无监督语料上进行预训练，注入新领域知识 大量无标签文本（至少数亿到数十亿 Token） 补足领域知识，如法律、医疗、金融等专业文本 监督微调 (SFT) 在有监督标注的数据上进行微调，强化特定任务和指令执行能力 定制化标注数据（指令/对话对等），从几千到几十万条 各类特定任务，如代码生成、问答、文本改写、复杂指令执行等 直接偏好对齐 (DPO) 利用偏好数据（正例 chosen vs. 负例 rejected）直接对齐人类偏好 偏好数据：[prompt, chosen, rejected](规模相对较小) 对齐人类反馈，如回答风格、合规性、安全性等 优势与挑战 继续预训练 (CPT) 优势:\n更好领域覆盖，全面提升模型在特定领域的理解和生成能力。 无需额外手动标注。 挑战/局限:\n需要大量高质量领域数据。 训练成本高，需大规模算力与时间。 可能引入领域偏见，需谨慎处理数据质量与分布。 监督微调 (SFT) 优势:\n快速获取可用的任务执行能力。 显著提升模型对特定场景的准确性。 挑战/局限:\n数据标注成本较高。 需谨慎选择标注数据以避免过拟合。 微调后可能削弱模型的通用性。 直接偏好对齐 (DPO) 优势:\n无需单独训练 Reward Model。 数据量需求较小，调优效率高。 挑战/局限:\n需要可靠的偏好标注。 对复杂、多样化场景仍需持续迭代收集更多偏好数据。 易受偏好数据分布的限制。 通用训练要点与技术细节 在进行 CPT、SFT、DPO 三种训练方法时，存在许多通用的训练要点和技术细节。以下部分将这些通用内容进行统一描述，以便于更好地理解和应用。\n数据处理与准备 数据质量：无论是 CPT、SFT 还是 DPO，数据的质量都是至关重要的。需要确保数据的准确性、无歧义性和多样性。 数据格式：统一的数据格式有助于简化训练流程。例如，使用 JSON 或其他结构化格式来存储训练数据。 数据增强：通过 LLM 重写和优化等方式增加数据多样性，提升模型的泛化能力。 学习率与优化 学习率设置：通常采用比预训练阶段更小的学习率，如从 3e-4 降低到 3e-5，具体数值视任务和数据量而定。 学习率调度：使用 warm-up 策略（如前 10% 步骤线性递增），随后采用线性衰减或余弦退火策略，确保训练过程平稳。 优化器选择：根据模型规模和硬件资源选择合适的优化器，比如 AdamW。 训练策略 全参数微调：在资源允许的情况下，优先进行全参数微调，以确保模型能够全面掌握新知识。 参数高效微调（PEFT）：如 LoRA，适用于计算资源有限的场景，通过冻结部分参数并添加 adapter 实现高效微调。 混合精度训练：在支持的 GPU 上使用 bf16 或 fp16，降低显存占用，提高训练速度。 训练稳定性：采用梯度裁剪、正则化、dropout、权重衰减等技术，防止梯度爆炸和模型过拟合。 Flash Attention：利用 Flash Attention 技术优化注意力机制的计算效率，提升训练速度和降低显存占用。 监控与调优 收敛监控：实时监控训练集和验证集的 loss 曲线，确保模型逐步收敛，必要时调整学习率和其他超参数。 Checkpoint：定期保留 Checkpoint，防止意外中断导致全部训练进度丢失。 早停机制：防止模型过拟合，适时停止训练，保存最佳模型状态。 模型评估：在训练过程中定期进行评估，确保模型性能符合预期。 继续预训练 (CPT) 目标 通过在大量领域特定的无监督数据上继续预训练基座模型，注入新的领域知识，使模型更好地理解和生成特定领域的内容。\n训练要点 流式加载\n实施流式数据加载，以便在训练过程中动态读取数据，防止超过最大内存，训练中断。\n全参数微调\n在进行模型训练时，通常需要更新模型的全部参数，以确保模型能够全面掌握新知识。\n全量微调相较于参数高效微调（如 LoRA）在领域知识注入方面效果更佳，尤其在运算资源充足的情况下，建议优先选择全参数微调。\n监督微调 (SFT) 目标 通过高质量的标注数据，训练模型执行特定任务，如代码生成、代码修复、复杂指令执行等，提升模型的实用性和准确性。\n训练要点 Epoch 数量\n在数据量充足的情况下通常 1 ~ 4 个 epoch 即可见到显著效果。 如果数据量不够，可以考虑加大 epoch 数量，但要注意过拟合的风险，建议进行数据增强。 数据增强与多样性\n确保训练数据涵盖多种任务类型和指令表达方式，提升模型的泛化能力。 包含多轮对话和鲁棒性数据，增强模型应对真实用户场景的能力。 直接偏好对齐 (DPO) 目标 通过用户反馈和偏好数据，优化模型输出，使其更符合人类的期望和需求，包括回答风格、安全性和可读性等方面。\nDPO 的特点 直接优化\n不需要单独训练 Reward Model，直接通过 (chosen, rejected) 数据对模型进行对比学习。\n高效性\n相较于 PPO，DPO 需要更少的数据和计算资源即可达到相似甚至更好的效果。\n动态适应\n每次有新数据时，模型能立即适应，无需重新训练 Reward Model。\n训练要点 偏好数据的收集\n部署多个不同训练阶段或不同数据配比的模型，生成多样化的回答。 通过人工或自动化方式标注 chosen 和 rejected 回答对，确保数据的多样性和质量。 对比学习\n通过最大化 chosen 回答的概率，最小化 rejected 回答的概率，优化模型参数。\n迭代优化\n持续收集用户反馈，生成新的偏好数据，进行循环迭代，逐步提升模型性能。 结合数据飞轮机制，实现模型的持续进化与优化。 常见问题与解决方案 重复输出 (Repetitive Outputs)\n问题：模型生成内容重复，连续打印停不下来。\n解决方案：\n数据去重与清洗：确保训练数据不含大量重复内容。 检查 EOT（End-of-Token）设置：防止模型连接打印无法停止。 通过 SFT/DPO 进行对齐：优化模型输出质量。 调整解码策略：如增加 top_k、repetition penalty 和 temperature 参数。 灾难性遗忘 (Catastrophic Forgetting)\n问题：模型在微调过程中遗忘原有的通用能力，可以看作是在新的数据集上过拟合，原本模型参数空间变化过大。\n解决方案：\n混合一部分通用数据：保持模型的通用能力。 调低学习率：减少对原有知识的冲击。 增加 Dropout Rate 和 Weight Decay：避免过拟合。 采用 LoRA 等参数高效微调方法：避免大规模参数更新。 使用 RAG 辅助：结合外部知识库提升模型表现。 Chat Vector: 通过模型权重的简单算术操作，快速为模型注入对话和通用能力。 实体关系与推理路径理解不足\n问题：模型难以正确理解复杂的实体关系和推理路径。\n解决方案：\n引入 Chain-of-Thought (CoT) 数据与强化推理训练：\n通过分步推理训练提升模型的能力，结合 强化微调 和 o1/o3 的训练方法。 扩展训练数据覆盖面：\n引入更多包含复杂实体关系和推理路径的多样化场景数据。 结合知识图谱建模：\n利用 GraphRAG 强化模型对实体关系的理解与推理能力。 模型部署与评估 部署 推理框架\nollama：基于 llama.cpp 的本地推理部署，可快速启动 vLLM：主打高并发、多用户场景下的推理吞吐量优化 量化：将模型量化为 8-bit 或 4-bit，进一步降低推理成本，提高部署效率。 集成 RAG \u0026amp; 智能体\nRAG：结合向量知识库，实时检索相关文档或代码片段，辅助模型生成更精准的回答。 智能体：利用 Function Call 或多轮对话机制，让模型调用外部工具或进行复杂推理，提升互动性和实用性。 Langgraph：封装 RAG 和 多智能体工作流，构建定制化的对话系统或代码自动生成平台。 评估 评估指标\nCPT 阶段：使用领域内测试集，评估困惑度（Perplexity，PPL）或者交叉熵（Cross Entropy），衡量模型对新知识的掌握程度。 SFT / DPO 阶段： Human 或模型评测：通过人工评分或自动化工具，评估回答的准确性、连贯性、可读性和安全性。 代码生成：构建大规模单元测试集，评估 pass@k 指标，衡量代码生成的正确率。 通用能力：在常见 benchmark（如 MMLU、CMMLU）对模型进行测试，确保模型在通用任务上的表现下降不大。 解码超参数\n一致性：在评估过程中保持 top_k、top_p、temperature、max_new_tokens 等解码参数一致，确保评估结果的可比性。 网格搜索：在算力允许的情况下，对不同解码参数组合进行评估，选择最优的参数配置。 数据飞轮与持续迭代 数据飞轮机制\n实时收集用户日志\n收集线上用户的真实 prompt 和生成的回答，覆盖多样化的使用场景和任务类型。\n自动或人工标注\n对收集到的用户 prompt 和回答进行偏好标注，生成新的 (chosen, rejected) 数据对。\n迭代训练\n将新生成的偏好数据加入到下一轮的 SFT/DPO 训练中，不断优化模型的回答质量和用户体验。\n鲁棒性数据\n包含拼写错误、混合语言、模糊指令等数据，提升模型在真实场景下的鲁棒性和应对能力。\n持续优化\n反馈循环：利用用户反馈，持续改进训练数据和模型表现，实现模型的自我优化和进化。 多模型协同：部署多个版本的模型，生成多样化的回答，通过对比学习提升模型的综合能力。 结合意图识别和多智能体推理 使用意图分类模型让大模型判断用户输入意图类别。基于意图类别与上下文类型的映射，监督推理路径，然后根据推理路径进行多路召回。将这些信息提供给训练好的模型，生成最终结果。\n总结 通过 继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO) 的组合方法，能够有效地在基座大模型上注入特定领域的知识，构建出具备高效解决业务问题能力的闭源大语言模型。关键步骤如下：\n数据准备\n高质量的数据收集、清洗、去重和分类，确保训练数据的多样性与准确性。 结合数据脱敏策略，保护隐私与合规。 模型训练\n通过 CPT 注入领域知识，SFT 学习特定任务模式，DPO 优化模型输出符合人类偏好和安全。 利用高效的并行训练框架和调参技巧，提升训练效率和资源利用率。 部署与评估\n采用高效的推理框架，结合 RAG 和 Agent 实现知识增强和功能扩展。 通过多维度评估，确保模型在各个阶段的表现符合预期。 持续迭代\n构建数据飞轮，实时收集用户反馈，不断优化训练数据和模型表现。 集成 RAG 和 Agent，实现模型能力的持续提升与扩展。 最终，通过系统化的流程和技术手段，能够构建一个不仅具备深厚领域知识，还能灵活应对复杂业务需求的长生命周期 AI 系统。\n参考资料 DeepSpeed Megatron-LM ollama vLLM GraphRAG The Llama 3 Herd of Models ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages Evaluating Large Language Models Trained on Code Direct Preference Optimization: Your Language Model is Secretly a Reward Model 引用 引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Jan 2025). 构建特定领域的大语言模型.\nhttps://syhya.github.io/posts/2025-01-05-domain-llm-training\nOr\n@article{syhya2024domainllm, title = \u0026#34;构建特定领域的大语言模型\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-05-domain-llm-training/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/","summary":"\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\n\u003cp\u003e随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。\u003c/p\u003e","title":"构建特定领域的大语言模型"},{"content":"租用 GPU 还是购买 GPU？ 在构建深度学习工作环境之前，首先需要综合考虑 使用周期、预算、数据隐私 以及 维护成本。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。\n租用 GPU 的优点：\n无需一次性投入高额硬件成本 可根据项目需求弹性扩容 云厂商通常提供数据合规与安全保障，省去硬件运维烦恼 购买 GPU 的优点：\n长期大规模使用时，整体成本更低 对内部数据和模型有更高的隐私与可控性 硬件可随时调整、升级，部署更灵活 个人建议\n如果预算有限或只是初学阶段，可先使用 Colab、Kaggle 或云 GPU； 当算力需求和隐私需求上升时，再考虑自建多卡服务器或租用多机多卡集群。 背景 在 2023 年 9 月，为了在工作之余继续对大模型（LLM）进行探索和研究，我组装了一台 双 RTX 4090 的个人 AI 实验服务器。该服务器已运行近一年，整体体验如下：\n噪音：服务器放在脚边，满负荷训练时风扇噪音较大，但在日常推理或中等负载下可接受 推理性能：双卡共计 48GB 显存，采用 4bit 量化方案时可运行到 70B 级别的模型（如 Llama 70B、Qwen 72B） 训练性能：在使用 DeepSpeed 的分布式和 offload 技术（ZeRO-3 + CPU offload）后，可对 34B 左右的模型（如 CodeLlama 34B）进行微调 性价比：对于个人或小团队的日常实验和中小规模模型训练而言，该配置较为实用；但若进行超大规模模型的全参数训练，仍需更多专业卡（如多卡 A100 或 H100 集群） 下图展示了不同大小模型、不同训练方法对显存的需求： Fig. 1. Hardware Requirement. (Image source: LLaMA-Factory)\n搭建思路与配置详情 整机预算在 4 万元人民币（约 6000 美元） 左右，以下是我最终选用的配置清单，仅供参考：\n配件 型号 价格 (元) 显卡 RTX 4090 * 2 25098 主板 + CPU AMD R9 7900X + 微星 MPG X670E CARBON 5157.55 内存 美商海盗船(USCORSAIR) 48GB*2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + 三星 990PRO 4TB 4587 电源 美商海盗船 AX1600i 2699 风扇 追风者 T30 12cm P * 6 1066.76 散热 利民 Thermalright FC140 BLACK 419 机箱 PHANTEKS 追风者 620PC 全塔 897.99 显卡延长线 追风者 FL60 PCI-E4.0 *16 399 总计：约 42723.3 元\nGPU 选择 对于大模型研究，浮点运算性能（TFLOPS） 和 显存容量 是最核心的指标。专业卡（A100、H100 等）虽有更高显存以及 NVLink，但价格动辄数十万人民币，对个人用户并不友好。根据 Tim Dettmers 的调研，RTX 4090 在单位美元算力方面表现非常亮眼，且支持 BF16、Flash Attention 等新特性，因此成为高性价比的选择。\n散热方式：涡轮 vs 风冷 vs 水冷 散热方式 优点 缺点 适用场景 涡轮风扇 体积紧凑；适合并行多卡部署 噪音大、整体散热效率一般 企业服务器机柜、多卡密集部署 风冷散热 性能与噪音平衡佳、维护简单 显卡体型通常较大 家用或个人研究（主机摆放空间足够） 水冷散热 散热能力突出、满载噪音更低 可能会出现漏液、价格更高 对静音要求极高或极限超频场景 家用推荐：风冷卡 兼顾散热效率、噪音和维护成本；相对于涡轮卡和水冷卡更友好。\nCPU \u0026amp; 主板 在深度学习场景中，CPU 主要负责数据预处理、管道调度以及多进程/多线程并行管理，确保数据能够以高吞吐量、低延迟的方式传递到 GPU。因此，CPU 的核心需求主要包括 充足的 PCIe 通道 和 卓越的多线程性能。\nIntel：13/14 代 i9（如 13900K）拥有 20 条 PCIe 主通道，能够满足双卡 x8 + x8 的需求 AMD：Ryzen 7000/9000 系列（如 7950X）提供 28 条（可用 24 条）PCIe 通道，支持双卡 x8 + x8，并为 M.2 SSD 提供足够带宽 微星 MPG X670E CARBON 主板 扩展性：支持 PCIe 5.0 和 DDR5 内存，具备充足的未来升级空间 稳定性：高规格供电设计，保障 CPU 与多显卡的稳定运行 接口丰富：支持多块 M.2 SSD 和 USB4，满足多样化使用需求 AMD Ryzen 9 7900X 特点 核心与线程：12 核心、24 线程，在深度学习场景中的数据预处理和多任务处理方面表现强劲 PCIe 带宽：提供 28 条（可用 24 条）PCIe 5.0 通道，可轻松支持双卡 x8 + x8，并为 M.2 SSD 提供高速带宽 能效比：基于 Zen 4 架构，性能与能耗平衡优秀，适合高性能计算需求 主板选购要点 空间布局 RTX 4090 尺寸庞大且卡槽较厚，需确认主板是否能同时容纳两张显卡；若存在空间或散热冲突，可使用显卡延长线竖放第二张卡。 PCIe 通道拆分 主板需至少支持双 PCIe 4.0 x8 + x8 配置，以避免出现 x16 + x2 的情况。x16 + x2 的带宽分配会显著限制第二块 GPU 的数据传输能力，进而影响 GPU 与 CPU 之间的数据交换效率。在大模型训练中，这种带宽瓶颈可能导致性能显著下降，严重影响整体训练效率。 扩展性 在双显卡插满的情况下，仍需确保主板具有足够的 M.2 SSD 插槽和外设接口 综合扩展性、性能与性价比等因素，我最终选择了 AMD Ryzen 9 7900X 搭配 微星 MPG X670E CARBON 主板 的组合。通过显卡延长线解决了 4090 双卡过厚导致的插槽冲突问题。\nBIOS 设置建议 内存优化 开启 XMP/EXPO（对应 Intel/AMD）以提升内存频率，增强带宽性能。 超频调整 如果需要进一步提升性能，可在 BIOS 中启用 PBO（Precision Boost Overdrive） 或 Intel Performance Tuning，并结合系统监控工具观察稳定性。 温度与稳定性 避免过度超频，注意控制温度，避免因崩溃或过热导致系统不稳定。 内存 深度学习训练中，内存会被大量占用用于数据加载、模型优化状态储存（尤其在多 GPU Zero-stage 并行场景下）。内存容量最好 ≥ 显存总容量的两倍。本配置中，使用了 48GB * 2（共 96GB），满足日常多任务和分布式训练的需求，减少内存不足导致的频繁 swap。\n硬盘 优先选用 M.2 NVMe SSD：其读写性能更优，对加载超大模型权重、缓存中间文件、训练日志等都有显著速度提升 容量建议 ≥ 2TB：随着大模型文件越来越庞大，2TB 往往很快就会被占满，建议根据自身需求选 4TB 或更多 SSD 品牌：三星、海力士或西部数据等主流大厂都拥有稳定的高端产品线 电源 双 4090 满载时整机功耗可达 900W~1000W 左右，CPU、主板和硬盘等还需额外功率余量。通常建议选择 1500W 以上 的铂金或钛金电源，以确保在高负载下电流供给稳定、降低电压波动带来的系统不稳定。\n我在此使用美商海盗船 AX1600i（数字电源），可以通过软件监控实时功耗，并提供充足冗余。\n散热与风扇 我采用 风冷 方案，包括：\nCPU 散热器：利民 FC140（双塔式气冷方案，兼顾了较高的散热效率和相对低噪音） 机箱风扇：追风者 T30 12cm * 6，保持机箱内部正压或者稍微正压的风道布局，保证显卡和供电模块的进风顺畅 在 GPU 长时间高负载训练（如分布式训练大型模型）时，机箱内的风道管理和风扇配置非常重要。建议使用监控软件及时查看 CPU、GPU、主板供电模块温度，适度调节风扇转速。\n散热进阶\n若对静音有更高要求，可考虑 Hybrid 散热（半水冷方案）或更精细的风扇调速曲线。 适度清理机箱灰尘、使用防尘网并定期更换导热硅脂也能提升散热和稳定性。 机箱 RTX 4090 体型巨大，且双卡堆叠时需要充足的内部空间和散热风道。全塔机箱能提供更好的走线空间和气流组织。我选用了 PHANTEKS 追风者 620PC，除了体型大、空间充裕外，也自带良好的线缆管理通道。\n装机完成后的图片如下：\nFig. 2. Computer\n系统与软件环境 操作系统方面强烈推荐 Linux，例如 Ubuntu 22.04 LTS，因其对 CUDA、NVIDIA 驱动以及常见深度学习框架有更好的支持和兼容性。大致流程如下：\n安装 OS：使用 Ubuntu 或其他 Linux 系统即可。 安装 NVIDIA 驱动：确保 nvidia-smi 能正确识别两张 4090:\nFig. 3. nvidia-smi Output\n安装 CUDA 工具链：通过 nvcc -V 确认版本信息:\nFig. 4. nvcc -V Output\n安装 cuDNN：确保深度学习框架可以调用 GPU 加速卷积和 RNN 等操作 测试框架：使用 PyTorch、TensorFlow 或 JAX 简单测试模型推理/训练是否正常 Docker 容器化： 利用 nvidia-container-toolkit 让容器直接访问 GPU 资源，避免主机环境污染。 在多机多卡环境下，还能结合 Kubernetes、Ray 或 Slurm 等进行集群调度与资源管理。 常用工具与框架推荐 训练框架\nLLaMA-Factory：对大语言模型训练/推理流程有较好封装，新手友好 DeepSpeed：支持大模型分布式训练、多种并行策略和优化功能 Megatron-LM：NVIDIA 官方的大规模语言模型训练框架，适合多机多卡场景 监控 \u0026amp; 可视化\nWeights \u0026amp; Biases 或 TensorBoard：实时监控训练过程中的损失函数、学习率等指标，支持远程可视化 推理工具\nollama：基于 llama.cpp 的本地推理部署，可快速启动 vLLM：主打高并发、多用户场景下的推理吞吐量优化 Framework Ollama vLLM 作用 简易本地部署 LLM 高并发 / 高吞吐的 LLM 推理 多请求处理 并发数增加时，推理速度下降明显 并发数增大也能保持较高吞吐 16 路并发 ~17 秒/请求 ~9 秒/请求 吞吐对比 Token 生成速度较慢 Token 生成速度可提升约 2 倍 极限并发 32 路以上并发时，性能衰减较严重 仍能平稳处理高并发 适用场景 个人项目或低并发应用 企业级或多用户并发访问 WebUI\nOpen-WebUI：基于 Web 界面的多合一 AI 界面，支持多种后端推理（ollama、OpenAI API 等），便于快速原型和可视化 进阶建议 开发与调试效率\n使用 SSH 工具提升远程开发效率，制作自定义容器镜像减少环境配置时间。\n量化与剪枝\n通过 4bit、8bit 量化和剪枝技术，减少模型参数和显存需求，优化推理性能。\n混合精度训练\n使用 BF16 或 FP16 提升训练速度，结合 GradScaler 提高数值稳定性。\nCPU 协同优化\n使用多线程、多进程或 RAM Disk 缓存优化数据加载，支持流式加载大规模预训练数据集。\n多机集群部署\n通过 InfiniBand 或高速以太网搭建集群，使用 Kubernetes 实现高效资源调度。\n总结 通过以上配置与思路，我成功搭建了一台 双卡 RTX 4090 深度学习主机。它在 推理 和 中小规模微调 场景中表现良好，对于想要在个人或小团队环境下进行大模型（LLM）科研或应用探索的人来说，这种方案兼具 性价比 与 灵活性。当然，如果要大规模全参数训练上百亿乃至上千亿参数的大模型，依然需要更多 GPU（如多卡 A100/H100 集群）。\n就个人体验而言，双 4090 在预算范围内提供了较好的训练与推理性能，可以满足绝大部分中小规模研究与实验需求，值得有条件的个人或小团队参考。\n参考资料 Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe 通道规格 AMD R5 7600X PCIe 通道规格 MSI MPG X670E CARBON 规格 nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI 版权声明与引用 声明：本文所涉及的配置清单、价格与建议仅供技术交流与研究参考。实际购买与部署请结合个人预算和需求进行综合评估。若因参考或采纳文中信息导致任何直接或间接后果，本文作者恕不承担责任。\n引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Dec 2024). 基于双卡 RTX 4090 搭建家用深度学习主机. https://syhya.github.io/posts/2024-12-21-build-gpu-server\nOr\n@article{syhya2024build, title = \u0026#34;基于双卡 RTX 4090 搭建家用深度学习主机\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Dec\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2024-12-21-build-gpu-server/\u0026#34; ","permalink":"https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/","summary":"\u003ch2 id=\"租用-gpu-还是购买-gpu\"\u003e租用 GPU 还是购买 GPU？\u003c/h2\u003e\n\u003cp\u003e在构建深度学习工作环境之前，首先需要综合考虑 \u003cstrong\u003e使用周期\u003c/strong\u003e、\u003cstrong\u003e预算\u003c/strong\u003e、\u003cstrong\u003e数据隐私\u003c/strong\u003e 以及 \u003cstrong\u003e维护成本\u003c/strong\u003e。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。\u003c/p\u003e","title":"基于双卡 RTX 4090 搭建家用深度学习主机"},{"content":"摘要 股票市场是金融市场的重要组成部分，近些年来，股票市场蓬勃发展，股票价格预测和量化投资策略研究吸引了许多领域的研究学者。其中最近几年随着人工智能和机器学习的发展，学者们从传统的统计学模型迁移到了人工智能算法，尤其是在深度学习热潮掀起后，神经网络在股票价格预测和量化投资策略研究中取得了不错的效果。深度学习的目标是学习多层次的特征，通过组合低级特征构建抽象的高级特征，从而挖掘数据的分布式特征表示，基于此进行复杂的非线性建模，从而实现预测任务。其中 RNN 被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价，交易信息都是序列数据，因此之前有很多研究者，基于 RNN 来预测股票价格。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题，之后出现了诸如 GRU，Peephole LSTM，BiLSTM 等 LSTM 的变体。但传统的股票预测模型有些并未考虑时间因素，有些仅考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。\n本文实验第一部分通过利用国内浦发银行和国外 IBM 的股票数据，分别建立了 LSTM，GRU，BiLSTM 的股票预测模型，通过比较这三种深度学习模型最后预测的结果，发现对于两个数据集都是 BiLSTM 模型优于其他模型，有更好的预测准确率。第二部分通过使用 A 股全市场的股票数据，并先使用 LightGBM 模型进行对 50 个因子的篮选，选出重要程度最高的 10 个因子。之后再用 BiLSTM 模型选取进行因子组合，建立量化投资策略，最后对该策略进行实证与回测，发现该策略优于市场基准指数，说明了 BiLSTM 模型在股票价格预测和量化投资的实际应用价值。\n关键词：量化投资；深度学习；神经网络模型；多因子选股；BiLSTM\n第一章 绪论 1.1 研究背景与意义 1.1.1 研究背景 从 1970 年逐渐兴起，量化投资进入各投资者的视野，一场新的革命就此拉开序幕，改变了从前被动管理和有效市场假说主导的投资组合管理局面。有效市场假说认为，在市场有效的前提下，股票价格能反映市场的所有信息，不存在超额收益。被动投资管理的投资理念是市场是有效的，更加注重资产类别，最常见的方法是购买指数基金并关注已发布指数的表现。而主动投资管理主要依赖于投资者对于市场和个股的主观判定，根据能够获取的公开数据，通过将数学模型应用于金融领域，对股票进行评判，从而构建投资组合获取收益。量化投资通过对大量历史数据进行统计处理，挖掘投资机会，规避主观因素，受到越来越多投资者的追捧。量化投资兴起以后，人们也逐渐利用各种技术来进行股票价格的预测，从而更好地建立量化投资策略。早期国内外学者采用统计学方法建模，来预测股票价格，如指数平均法，多元回归法，自回归平均移动模型等，但是由于股票市场受多种因素影响，同时数据量很大，导致股票预测难度很大，各种统计学模型预测效果并不令人满意。\n近几年来，机器学习，深度学习和神经网络等相关技术不断发展，为股票价格预测和量化策略的构建提供了技术支持，不少学者通过随机森林法，神经网络，支持向量机和卷积神经网络等方法完成新的突破。股票市场足够的历史数据加之以多元的技术支撑，为股票价格预测和量化策略的构建提供了有利条件。\n1.1.2 研究意义 从国家经济体系和金融市场的长远发展来看，对于股票价格预测模型和量化投资策略的研究必不可少。我国起步较晚，金融市场不够成熟，金融工具不够丰富，市场效率较低，但是近几年来国家逐渐放宽政策，大力建设金融市场，为量化投资的发展提供“温床”，发展量化投资及新兴金融技术可以提供我们国家金融市场弯道超车的机会。并且股票价格指数作为一项重要的经济指标，对于我国经济发展起着晴雨表的作用。\n从个人投资者和机构投资者的角度来看，构建股票价格预测模型和量化投资策略模型提高了市场的有效性。个人投资者的专业知识不够完善，投资行为具有一定的盲目性，构建相关模型，为其提供参考，能够减少判断失误的概率，改变个人投资者在资本市场处于相对弱势的地位。并且对于机构投资者而言，理性客观的模型加之以个人经验的判断，提高了决策的正确性，为投资行为提供了新的方向。\n综上，结合我国目前的发展现状，本文选取个股进行股票价格预测模型和 A 股全市场的股票进行量化策略研究有重要的现实研究意义。\n1.2 研究综述 White（1988）$^{[1]}$ 使用 BP 神经网络来预测 IBM 股票的日收益率。然而，由于 BP 神经网络模型易受梯度爆炸的影响，导致模型无法收敛到全局最小值，从而无法实现准确的预测。\nKimoto（1990）$^{[2]}$ 使用模块化神经网络技术开发了一个用于东证股价指数（Tokyo Stock Exchange Prices Indexes，TOPIX）预测的系统。该系统不仅成功预测了东京证券交易所的 TOPIX，还通过基于预测结果的股票交易模拟，实现了一定程度的盈利。\nG．Peter Zhang（2003）$^{[3]}$ 对差分整合移动平均自回归（Autoregressive Integrated Moving Average，ARIMA）模型和人工神经网络（Artificial Neural Network，ANN）模型在时间序列预测中的性能进行了对比研究。结果显示，ANN 模型在时间序列预测的精度上显著优于 ARIMA 模型。\nRyo Akita（2016）$^{[4]}$ 选取消费者物价指数、市盈率以及报纸上的各种事件作为特征，利用段落向量和 LSTM 网络构建了一个金融时间序列预测模型。通过东京证券交易所五十家上市公司的实际数据，验证了该模型在股票开盘价预测方面的有效性。\nKunihiro Miyazaki（2017）$^{[5]}$ 通过提取股票日线图像及每 30 分钟的股票价格数据，构建了一个针对东证核心 30 指数（Topix Core 30）及其成分股涨跌预测的模型。研究对比了多种模型，包括逻辑回归（Logistic Regression, LR）、随机森林（Random Forest, RF）、多层感知器（Multilayer Perceptron, MLP）、LSTM、CNN、PCA-CNN 和 CNN-LSTM。结果表明，LSTM 在预测性能上最优，CNN 表现一般，但结合 CNN 和 LSTM 的混合模型可以提升预测精度。\nTaewook Kim（2019）$^{[6]}$ 提出了一个 LSTM-CNN 混合模型，用于结合股票价格时间序列与股票价格图像两种数据表示形式的特征，以预测 S\u0026amp;P 500 指数的股价。研究表明，LSTM-CNN 模型在股价预测方面优于单一模型，并且这种预测对于构建量化投资策略具有一定的实际意义。\n1.3 论文的创新点 本文股票预测方面具有以下创新点：\n分别选用国内 A 股上海浦东发展银行和国外美股 IBM 的数据进行研究，避免单一市场研究的局限性。并且传统的 BP 模型从未考虑时间因素，要么像 LSTM 模型考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。实证过程中并与 LSTM 模型，GRU 模型进行对比，说明其能够提升预测准确率。 股票价格预测模型采用股票多特征进行训练，包括开盘价，闭盘价，最高价和交易量等特征，相对于单特征预测股票收盘价理论上更加精确，能更好地比较 LSTM，GRU 和 BiLSTM 对于股票的预测效果。 本文量化策略研究方面具有以下创新点：\n未使用市面上已有的常见因子，使用自己通过遗传规划算法（Genetic Programming，GP）和人工数据挖掘得到的多个价量因子，并通过 LightGBM 模型进行对 50 个因子的筛选，选出重要程度最高的 10 个因子。 传统的量化投资模型一般用 LSTM 模型和 CNN 模型构建量化投资策略，本文使用 A 股全市场的数据，利用 BiLSTM 模型选取进行因子组合，建立量化投资策略，最后对该策略进行实证与回测，发现该策略优于市场基准指数（中证全指），说明了 BiLSTM 模型在股票价格预测和量化投资的实际应用价值。 1.4 论文研究框架 本文基于深度学习算法分别进行了股票价格预测和量化策略研究，本文的具体研究框架如 Fig. 1 所示：\nFig. 1. Research Framework.\n本文具体研究框架内容如下：\n第一章为绪论。本章首先对股票价格预测和量化策略研究的研究意义和研究背景进行了介绍。随后对研究现状进行了综述，然后说明了本文相比现有的研究的创新点，最后简要阐述了本文的研究框架。\n第二章为相关理论基础。本章对本文研究中涉及到的深度学习模型和量化选股的基本理论进行了介绍。深度学习模型小节依次介绍了 RNN，LSTM，GRU 和 BiLSTM 这四个深度学习模型，其中重点介绍了 LSTM 模型的内部结构。量化选股理论小节依次介绍了均值－方差模型，资本资产定价模型，套利定价理论，多因子模型，Fama－French 三因子模型和五因子模型。本小节从各种金融理论和模型发展脉络中介绍了多因子量化选股的历程。\n第三章为 LSTM，GRU 和 BiLSTM 在股票价格预测中比较研究。本章首先介绍了实验所用国内及国外股票的数据集，然后对于数据进行归一化和数据划分的预处理步骤。紧接着说明了本章所使用 LSTM，GRU 和 BiLSTM 这三个模型的网络结构，模型的编译和超参数设置，并进行了实验得到实验结果。最后对实验结果进行分析和本章小结。\n第四章为基于 LightGBM-BiLSTM 的量化投资模型研究。本章首先大致介绍了实验步骤，然后分别介绍了实验所用的股票数据和因子数据。之后再对因子依次进行缺失值处理，去极值，因子标准化和因子中性化处理得到清洗后的因子。随后再利用 LightGBM 和 BiLSTM 分别进行因子选择和因子组合，最后根据得到的模型进行量化策略构建，并对量化策略进行回测。\n第五章为总结与展望。本章对于本文关于股票价格预测与量化投资策略的主要研究内容进行了总结，之后针对目前研究所存在的不足，对未来研究的方向进行了展望。\n第二章 相关理论基础 2.1 深度学习模型 2.1.1 RNN 循环神经网络（Recurrent Neural Network，RNN）被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价和交易信息都是序列数据，因此之前有很多工作，基于 RNN 来预测股票价格。RNN 采用十分简单的重复模块的链式结构，例如单个 tanh 层。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题。Fig. 2 是 RNN 结构图。\nFig. 2. RNN Structure Diagram. (Image source: Understanding LSTM Networks)\n2.1.2 LSTM 长短时记忆网络（Long Short-Term Memory，LSTM）是一种特殊的 RNN，能够学习长期依赖关系。它们是由 Hochreiter \u0026amp; Schmidhuber（1997）$^{[7]}$ 提出的，并在随后的工作中被许多人改进和推广。由于独特的设计结构，LSTM 有着间隙长度相对不敏感的特点，并且解决了传统 RNN 的梯度消失和梯度爆炸的问题。相对于传统 RNN 和隐马尔可夫模型（Hidden Markov Model，HMM）等其他时间序列模型，LSTM 能更好地处理和预测时间序列中间隔和延迟非常长的重要事件。因此，LSTM 广泛应用在机器翻译和语音识别的领域。\nLSTM 被明确设计为避免长期依赖问题。所有的递归神经网络都具有神经网络的重复模块链的形式，而 LSTM 对 RNN 的结构进行了改进。LSTM 并没有采用单一神经网络层，而是采用一种特殊的方式进行交互的四层结构。\nFig. 3. LSTM Structure Diagram 1. (Image source: Understanding LSTM Networks)\nFig. 4. LSTM Structure Diagram 2. (Image source: Understanding LSTM Networks)\n如 Fig.3 所示，黑线用来表示传输一个节点的输出向量到另一个节点的输入向量。神经网络层（Neural network layer）是带有 $\\sigma$ 激活函数或者 tanh 激活函数的处理模块；逐点运算（Pointwise operation）是代表向量与向量之间进行点乘运算；向量传输（Vector transfer）是表示信息传递方向；汇合（Concatenate）和复制（Copy）分别用两个黑线合在一起和两个黑线分开来表示信息的汇合和信息的复制。\n下面我们以 LSTM 为例，进行其结构详细的说明。\n遗忘门（forget gate） Fig. 5. Forget Gate Calculation (Image source: Understanding LSTM Networks)\n$$ f_{t} = \\sigma\\left(W_{f} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{f}\\right) $$参数说明：\n$h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $\\sigma$ ：sigmoid 激活函数 $W_{f}$ ：遗忘门的权重矩阵 $b_{f}$ ：遗忘门的偏差向量参数 第一步如 Fig.5 所示，是一个决定从细胞状态中丢弃的信息的过程。该过程由 sigmoid 函数计算得到 $f_{t}$ 的值（$f_{t}$ 的范围在 0 到 1 之间，其中 0 代表完全不通过，1 代表完全通过）来决定细胞状态 $C_{t-1}$ 通过或者部分通过。\n输入门（input gate） Fig. 6. Input Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} i_{t} \u0026= \\sigma\\left(W_{i} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{i}\\right) \\\\ \\tilde{C}_{t} \u0026= \\tanh\\left(W_{C} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{C}\\right) \\end{aligned} $$参数说明：\n$h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $\\sigma$ ：sigmoid 激活函数 $W_{i}$ ：输入门的权重矩阵 $b_{i}$ ：输入门的偏差向量参数 $W_{C}$ ：细胞状态的权重矩阵 $b_{C}$ ：细胞状态的偏差向量参数 $\\tanh$ ：tanh 激活函数 第二步如 Fig.6 所示，通过 sigmoid 函数计算我们要在细胞状态中存储什么信息，接下来通过一个 $\\tanh$ 层创建候选向量 $\\tilde{C}_{t}$，该向量将会被加到细胞的状态中。\nFig. 7. Current Cell State Calculation (Image source: Understanding LSTM Networks)\n$$ C_{t} = f_{t} * C_{t-1} + i_{t} * \\tilde{C}_{t} $$参数说明：\n$C_{t-1}$ ：上一时刻的细胞状态 $\\tilde{C}_{t}$ ：临时细胞状态 $i_{t}$ ：输入门的值 $f_{t}$ ：遗忘门的值 第三步如 Fig.7 所示，当前时刻的细胞状态 $C_t$ 通过结合遗忘门和输入门的作用计算得到。\n遗忘门 $f_t$ 对上一时刻的细胞状态 $C_{t-1}$ 进行加权，以丢弃不需要的信息。 输入门 $i_t$ 对候选细胞状态 $\\tilde{C}_t$ 进行加权，决定引入多少新信息。\n最终，两个部分相加，更新得出当前时刻的细胞状态 $C_t$。 输出门（output gate） Fig. 8. Output Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} o_{t} \u0026= \\sigma\\left(W_{o} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{o}\\right) \\\\ h_{t} \u0026= o_{t} * \\tanh\\left(C_{t}\\right) \\end{aligned} $$参数说明：\n$o_{t}$ ：输出门的值 $\\sigma$ ：sigmoid 激活函数 $W_{o}$ ：输出门的权重矩阵 $h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $b_{o}$ ：输出门的偏差向量参数 $h_{t}$ ：当前时刻的输出 $\\tanh$ ：tanh 激活函数 $C_{t}$ ：当前时刻的细胞状态 第四步如 Fig.8 所示，使用 sigmoid 函数计算输出门的值，最后通过 tanh 激活函数将这一时刻的细胞状态 $C_{t}$ 进行处理并与输出门的值 $o_{t}$ 相乘得到当前时刻的输出 $h_{t}$。\n2.1.3 GRU K. Cho（2014）$^{[8]}$ 提出了门控循环单元（Gated Recurrent Unit，GRU）。GRU 主要是在 LSTM 的基础上进行了简化和调整，将 LSTM 原有的遗忘门、输入门和输出门合并为更新门（update gate）和重置门（reset gate）。此外，GRU 还将细胞状态与隐藏状态合并，从而减少了模型的复杂性，同时在某些任务中仍能够达到与 LSTM 相当的性能。\n该模型在训练数据集比较大的情况下可以节省很多时间，在某些较小和较不频繁的数据集上表现出更好的性能$^{[9][10]}$。\nFig. 9. GRU Structure Diagram (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} z_{t} \u0026= \\sigma\\left(W_{z} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ r_{t} \u0026= \\sigma\\left(W_{r} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ \\tilde{h}_{t} \u0026= \\tanh\\left(W \\cdot \\left[r_{t} * h_{t-1}, x_{t}\\right]\\right) \\\\ h_{t} \u0026= \\left(1 - z_{t}\\right) * h_{t-1} + z_{t} * \\tilde{h}_{t} \\end{aligned} $$参数说明：\n$z_{t}$ ：更新门的值 $r_{t}$ ：重置门的值 $W_{z}$ ：更新门的权重矩阵 $W_{r}$ ：重置门的权重矩阵 $\\tilde{h}_{t}$ ：临时的输出 2.1.4 BiLSTM 双向长短时记忆网络（Bidirectional Long Short-Term Memory，BiLSTM）是由前向的 LSTM 与后向的 LSTM 结合成的。BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，能够学习到对时间有长期依赖性的信息，与单向 LSTM 相比可以更好地考虑未来数据的反向影响。Fig. 10 是 BiLSTM 结构图。\nFig. 10. BiLSTM Structure Diagram. (Image source: Baeldung)\n2.2 量化选股理论 2.2.1 均值－方差模型 量化选股策略起源于 20 世纪 50 年代，Markowitz（1952）$^{[11]}$ 提出了均值－方差模型（Mean-Variance Model）。该模型不仅奠定了现代投资组合理论的基础，将投资风险量化，还建立了一个描述风险和预期收益率的具体模型。它打破了以往仅对投资组合进行定性分析而缺乏定量分析的局面，将数学模型成功引入金融投资领域。\n$$ \\begin{aligned} \\mathrm{E}\\left(R_{p}\\right) \u0026= \\sum_{i=1}^{n} w_{i} \\mathrm{E}\\left(R_{i}\\right) \\\\ \\sigma_{p}^{2} \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\operatorname{Cov}\\left(R_{i}, R_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\sigma_{i} \\sigma_{j} \\rho_{ij} \\\\ \\sigma_{i} \u0026= \\sqrt{\\operatorname{Var}\\left(R_{i}\\right)}, \\quad \\rho_{ij} = \\operatorname{Corr}\\left(R_{i}, R_{j}\\right) \\end{aligned} $$$$ \\min \\sigma_{p}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} \\mathrm{E}\\left(R_{i}\\right) w_{i} = \\mu_{p}, \\quad \\sum_{i=1}^{n} w_{i} = 1 $$$$ \\begin{aligned} \\Omega \u0026= \\begin{pmatrix} \\sigma_{11} \u0026 \\cdots \u0026 \\sigma_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\sigma_{n1} \u0026 \\cdots \u0026 \\sigma_{nn} \\end{pmatrix} = \\begin{pmatrix} \\operatorname{Var}\\left(R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Cov}\\left(R_{1}, R_{n}\\right) \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\operatorname{Cov}\\left(R_{n}, R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Var}\\left(R_{n}\\right) \\end{pmatrix} \\\\ \\Omega^{-1} \u0026= \\begin{pmatrix} v_{11} \u0026 \\cdots \u0026 v_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ v_{n1} \u0026 \\cdots \u0026 v_{nn} \\end{pmatrix} \\\\ w_{i} \u0026= \\frac{1}{D}\\left(\\mu_{p} \\sum_{j=1}^{n} v_{ij}\\left(C \\mathrm{E}\\left(R_{j}\\right) - A\\right) + \\sum_{j=1}^{n} v_{ij}\\left(B - A \\mathrm{E}\\left(R_{j}\\right)\\right)\\right), \\quad i = 1, \\ldots, n \\end{aligned} $$$$ \\begin{aligned} A \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{j}\\right), \\quad B = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{i}\\right) \\mathrm{E}\\left(R_{j}\\right), \\quad C = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij}, \\quad D = BC - A^{2} \u003e 0 \\\\ \\sigma_{p}^{2} \u0026= \\frac{C \\mu_{p}^{2} - 2A \\mu_{p} + B}{D} \\end{aligned} $$其中：\n$\\mathrm{E}\\left(R_{p}\\right)$ 和 $\\mu_{p}$ 是投资组合 $p$ 的预期收益率 $\\mathrm{E}\\left(R_{i}\\right)$ 是资产 $i$ 的预期收益率 $\\sigma_{i}$ 是资产 $i$ 的标准差 $\\sigma_{j}$ 是资产 $j$ 的标准差 $w_{i}$ 是资产 $i$ 在投资组合中的比例 $\\sigma_{p}^{2}$ 是投资组合 $p$ 的方差 $\\rho_{ij}$ 是资产 $i$ 和资产 $j$ 之间的相关系数 通过以上公式$^{[12]}$，我们可以构建投资组合，来让我们的投资组合在一定的期望收益率的条件下，非系统风险降低到最小。\n2.2.2 资本资产定价模型 William Sharpe（1964）$^{[13]}$、John Lintner（1965）$^{[14]}$ 和 Jan Mossin（1966）$^{[15]}$ 提出了资本资产定价模型（Capital Asset Pricing Model，CAPM）。该模型认为，一项资产的预期收益与该资产的风险度量 $\\beta$ 值相关。这一模型通过简单的线性关系，将资产的预期收益率与市场风险联系起来，使得 Markowitz（1952）$^{[11]}$ 的投资组合选择理论更贴近现实世界，同时为多因子选股模型的建立奠定了理论基础。\n根据资本资产定价模型，对于一个给定的资产 $i$，它的预期收益率和市场投资组合的预期收益率之间的关系可以表示为：\n$$ E\\left(r_{i}\\right) = r_{f} + \\beta_{im}\\left[E\\left(r_{m}\\right) - r_{f}\\right] $$其中：\n$E\\left(r_{i}\\right)$ 是资产 $i$ 的预期收益率 $r_{f}$ 是无风险利率 $\\beta_{im}$（Beta）是资产 $i$ 的系统性风险系数，$\\beta_{im} = \\frac{\\operatorname{Cov}\\left(r_{i}, r_{m}\\right)}{\\operatorname{Var}\\left(r_{m}\\right)}$ $E\\left(r_{m}\\right)$ 是市场投资组合 $m$ 的预期收益率 $E\\left(r_{m}\\right) - r_{f}$ 是市场风险溢价 2.2.3 套利定价理论和多因子模型 Ross（1976）$^{[16]}$ 提出了套利定价理论（Arbitrage Pricing Theory，APT）。该理论认为，套利行为是形成市场均衡价格的决定性因素，通过在收益率形成过程中引入一系列因子构建线性相关关系，克服了资本资产定价模型（CAPM）的局限性，为后续研究者提供了重要的理论指导。\n套利定价理论被认为是多因子模型（Multiple-Factor Model，MFM）的理论基础，是资产价格模型的重要组成部分，也是资产价格理论的基石之一。多因子模型的一般表达形式为：\n$$ r_{j} = a_{j} + \\lambda_{j1} f_{1} + \\lambda_{j2} f_{2} + \\cdots + \\lambda_{jn} f_{n} + \\delta_{j} $$其中：\n$r_{j}$ 是资产 $j$ 的收益率 $a_{j}$ 是资产 $j$ 的常数 $f_{n}$ 是系统性因素 $\\lambda_{jn}$ 是因子载荷 $\\delta_{j}$ 是随机误差 2.2.4 Fama－French 三因子模型和五因子模型 Fama（1992）和 French（1992）$^{[17]}$ 使用横截面回归与时间序列结合的方法发现，股票市场的 $\\beta$ 值无法解释不同股票回报率的差异，而上市公司的市值、账面市值比和市盈率等可以显著解释股票回报率的差异。他们认为超额收益是对 CAPM 中 $\\beta$ 未能反映的风险因素的补偿，由此提出了 Fama－French 三因子模型。这三个因子分别为：\n市场风险溢价因子（Market Risk Premium）\n表示市场整体的系统性风险，即市场投资组合的预期收益减去无风险利率的差值。 衡量投资者承担系统性风险（即无法通过分散投资消除的风险）所期望的超额回报。 计算公式为：\n$$ \\text{Market Risk Premium} = E(R_m) - R_f $$ 其中 $E(R_m)$ 是市场的预期收益率，$R_f$ 是无风险利率。 市值因子（Size, SMB: Small Minus Big）\n表示小市值股票与大市值股票之间的收益差异。 小市值股票通常风险更高，但历史数据显示，其预期收益也往往高于大市值股票。 计算公式为：\n$$ SMB = R_{\\text{Small}} - R_{\\text{Big}} $$ 反映了市场对小市值股票的额外风险溢价的补偿。 账面市值比因子（Value, HML: High Minus Low）\n反映高账面市值比（即“价值型股票”）与低账面市值比（即“成长型股票”）之间的收益差异。 高账面市值比的股票通常定价较低（被市场低估），但长期来看可能获得较高回报。 计算公式为：\n$$ HML = R_{\\text{High}} - R_{\\text{Low}} $$ 低账面市值比的股票可能因市场对其过于乐观的预期而被高估。 该模型将 APT 模型中的因子具体化，并得出结论：投资小市值、高成长的股票具有高风险高收益的特性。Fama－French 三因子模型被广泛应用于现代投资行为的分析和实践中。\n随后，Fama（2015）和 French（2015）$^{[18]}$ 对三因子模型进行了扩展，新增了以下两个因子：\n盈利水平因子（Profitability, RMW: Robust Minus Weak）\n反映高盈利公司与低盈利公司之间的收益差异。 盈利能力强的公司（高 ROE、净利润率）更可能提供稳定且较高的回报。 计算公式为：\n$$ RMW = R_{\\text{Robust}} - R_{\\text{Weak}} $$ 投资水平因子（Investment, CMA: Conservative Minus Aggressive）\n反映保守型投资公司与激进型投资公司之间的收益差异。 激进型公司（扩张迅速，资本开支较高）通常伴随着更大的经营风险，而保守型公司（资本支出相对稳健）表现出更高的稳定性和收益。 计算公式为：\n$$ CMA = R_{\\text{Conservative}} - R_{\\text{Aggressive}} $$ Fama-French 三因子模型公式为：\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\epsilon_i $$Fama-French 五因子模型公式为：\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\beta_{i,RMW} \\cdot RMW + \\beta_{i,CMA} \\cdot CMA + \\epsilon_i $$其中：\n$R_i$: 股票 $i$ 的预期收益率 $R_f$: 无风险收益率 $R_m$: 市场组合的预期收益率 $R_m - R_f$: 市场风险溢价因子 $SMB$: 小市值减去大市值股票收益 $HML$: 高账面市值比减去低账面市值比股票收益 $RMW$: 高盈利能力减去低盈利能力股票收益 $CMA$: 低投资活动减去高投资活动股票收益 $\\beta_{i,*}$: 股票 $i$ 对应因子的敏感度 $\\epsilon_i$: 回归残差 2.2.5 模型对比表格 表 2.1 模型对比表格 以下表格总结了 均值-方差模型、资本资产定价模型 (CAPM)、套利定价理论 (APT) 和 Fama-French 模型 的核心内容及因子来源：\n模型 核心内容 因子来源 均值-方差模型 投资组合理论的基础，通过期望收益和协方差矩阵优化投资组合。 投资组合中资产的期望收益和协方差矩阵 资本资产定价模型 (CAPM) 通过市场风险因子（$\\beta$）解释资产收益，奠定多因子模型理论基础。 市场因子 $\\beta$ 套利定价理论 (APT) 多因子框架，允许多个经济变量解释资产收益，例如通胀率、利率等。 多因子（宏观经济变量，如通胀率、利率等） Fama-French 三因子模型 增加市值因子和账面市值比因子，改进对资产收益的解释能力。 市场因子、SMB（市值因子）、HML（账面市值比因子） Fama-French 五因子模型 在三因子模型基础上增加盈利因子和投资因子，进一步完善资产定价模型。 市场因子、SMB、HML、RMW（盈利因子）、CMA（投资因子） 以下表格总结了这些模型的优点及不足：\n表 2.2 模型优缺点对比表 模型 优点 不足 均值-方差模型 提供了系统化投资组合优化方法，奠定现代投资理论基础。 仅针对收益和方差进行优化，未明确风险补偿的来源。 资本资产定价模型 (CAPM) 简单易用，通过市场风险解释收益差异，为多因子模型提供理论基础。 假设单因子（市场风险）决定收益，忽略其他系统性风险因子。 套利定价理论 (APT) 允许多个因子解释资产收益，减少对单因子假设的依赖，更灵活。 未明确具体因子，实操性较低，仅提供框架。 Fama-French 三因子模型 通过增加市值因子和账面市值比因子，显著提高了对资产收益的解释能力。 忽略了盈利能力和投资行为等其他因子。 Fama-French 五因子模型 在三因子模型基础上增加盈利因子和投资因子，更全面地捕捉影响资产收益的关键变量。 模型复杂度较高，对数据要求高，仍可能遗漏某些潜在因子。 第三章 LSTM，GRU 和 BiLSTM 在股票价格预测中比较研究 3.1 实验数据介绍 国内外很多学者的研究以本国的股票指数为主，对于不同市场的单个股票研究相对较少。并且很少有研究将 LSTM、GRU、BiLSTM 这三个模型进行对比研究。因此本文分别选择国内 A 股上海浦东发展银行（简称浦发银行，代码600000）和美股 International Business Machines Corporation（简称 IBM）进行研究，这样更能准确地对我们使用的三个模型进行对比。其中浦发银行采用 2008 年 1 月 1 日到 2020 年 12 月 31 日的股票数据，共有 3114 条有效数据，数据来源于 Tushare 金融大数据平台。我们选取该数据集的日期（date）、开盘价（open）、收盘价（close）、最高价（high）、最低价（low）和成交量（volume）这 6 个特征进行实验。浦发银行的数据集除日期作为时间序列的索引以外，其他 5 个特征均作为自变量。IBM 采用 1990 年 1 月 2 日到 2018 年 11 月 15 日的股票数据，共有 7278 条有效数据，数据来源于雅虎财经。我们选取该数据集的日期（date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、调整后的收盘价（Adj Close）和成交量（volume）这 7 个特征进行实验。IBM 的数据集除日期作为时间序列的索引以外，其他 6 个特征均作为自变量。本次实验选择收盘价（close）作为被预测的变量。表3.1和表3.2分别是两个数据集的部分数据。\n表 3.1 浦发银行数据集部分展示 date open close high low volume code 2008-01-02 9.007 9.101 9.356 8.805 131583.90 600000 2008-01-03 9.007 8.645 9.101 8.426 211346.56 600000 2008-01-04 8659 9.009 9.111 8.501 139249.67 600000 2008-01-07 8.970 9.515 9.593 8.953 228043.01 600000 2008-01-08 9.551 9.583 9.719 9.517 161255.31 600000 2008-01-09 9.583 9.663 9.772 9.432 102510.92 600000 2008-01-10 9.701 9.680 9.836 9.602 217966.25 600000 2008-01-11 9.670 10.467 10.532 9.670 231544.21 600000 2008-01-14 10.367 10.059 10.433 10.027 142918.39 600000 2008-01-15 10.142 10.051 10.389 10.006 161221.52 600000 数据来源：Tushare\n表 3.2 IBM数据集部分展示 Date Open High Low Close Adj Close Volume 1990-01-02 23.6875 24.5313 23.6250 24.5000 6.590755 7041600 1990-01-03 24.6875 24.8750 24.5938 24.7188 6.649599 9464000 1990-01-04 24.7500 25.0938 24.7188 25.0000 6.725261 9674800 1990-01-05 24.9688 25.4063 24.8750 24.9375 6.708448 7570000 1990-01-08 24.8125 25.2188 24.8125 25.0938 6.750481 4625200 1990-01-09 25.1250 25.3125 24.8438 24.8438 6.683229 7048000 1990-01-10 24.8750 25.0000 24.6563 24.7500 6.658009 5945600 1990-01-11 24.8750 25.0938 24.8438 24.9688 6.716855 5905600 1990-01-12 24.6563 24.8125 24.4063 24.4688 6.582347 5390800 1990-01-15 24.4063 24.5938 24.3125 24.5313 6.599163 4035600 数据来源：雅虎财经\n3.2 实验数据预处理 3.2.1 数据的归一化 实验中各个特征在单位和量级上存在差异，比如股票价格和成交量之间量级差异巨大，会对我们实验最终预测的结果产生影响。因此我们采用 sklearn.preprocessing 库中的 MinMaxScaler 方法将数据的特征缩放至 0 到 1 之间。这样既能提升模型精度，也能提升模型收敛速度。归一化公式：\n$$ x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)} $$其中 $x^{\\prime}$ 是归一化后的数据，$x$ 是原始数据， $\\min (x)$ 是原始数据集的最小值， $\\max (x)$ 是原始数据集的最大值。之后在我们的实验过程中获得预测结果之后，还要将数据进行反归一化处理，之后才能进行对于股票价格的预测和模型评估。\n3.2.2 数据的划分 此处分别将浦发银行和 IBM 的整个实验数据集送入，设置循环核时间步（timestep）都为 60，每个时间步输入特征个数分别为 5 和 6。这样可以输入前 60 个交易日的数据，预测出第 61 天的收盘价。使我们的数据集符合之后要比较的三种神经网络模型输入的要求，依次是送入样本数，循环核时间展开步数和每个时间步输入特征个数。之后我们再将浦发银行的数据集按照 2488：311：255 的比例将归一化的数据集划分为训练集，验证集，测试集三个部分。将 IBM 的数据集按照 6550：364：304 的比例将归一化的数据集划分为训练集，验证集，测试集三个部分。我们这里划分出验证集的目的是为了方便进行调整模型的超参数以便优化各个模型之后再进行比较。\n3.3 模型网络结构 本文通过大量反复试验最终各个模型设置的网络结构如下表所示，其中层与层之间使用循环神经网络默认的 tanh 和 linear 作为激活函数，并且为了防止过拟合加入 Dropout，Dropout 的丢弃比例（rate）取值为 0.2。LSTM 和 GRU 每个循环层的神经元个数为 50，BiLSTM 循环层的神经元的个数为 100。LSTM、GRU、BiLSTM 每个模型分别采用四层 LSTM、GRU、BiLSTM 和一层全连接层，其中每层网络之间都设置了一个 Dropout。\n表 3.3 IBM的LSTM网络结构 Layer(type) Output Shape Param# lstm_1 (LSTM) (None, 60, 50) 11400 dropout_1 (Dropout) (None, 60, 50) 0 lstm_2 (LSTM) (None, 60, 50) 20200 dropout_2 (Dropout) (None, 60, 50) 0 lstm_3 (LSTM) (None, 60, 50) 20200 dropout_3 (Dropout) (None, 60, 50) 0 lstm_4 (LSTM) (None, 50) 20200 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params：72,051\nTrainable params：72,051\nNon-trainable params：0\n表 3.4 IBM的GRU网络结构 Layer(type) Output Shape Param# gru_1 (GRU) (None, 60, 50) 8550 dropout_1 (Dropout) (None, 60, 50) 0 gru_2 (GRU) (None, 60, 50) 15150 dropout_2 (Dropout) (None, 60, 50) 0 gru_3 (GRU) (None, 60, 50) 15150 dropout_3 (Dropout) (None, 60, 50) 0 gru_4 (GRU) (None, 50) 15150 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params：54,051\nTrainable params：54,051\nNon-trainable params：0\n表 3.5 IBM的BiLSTM网络结构 Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 60, 100) 22800 dropout_1 (Dropout) (None, 60, 100) 0 bidirectional_2 (Bidirection) (None, 60, 100) 60400 dropout_2 (Dropout) (None, 60, 100) 0 bidirectional_3 (Bidirection) (None, 60, 100) 60400 dropout_3 (Dropout) (None, 60, 100) 0 bidirectional_4 (Bidirection) (None, 100) 60400 dropout_4 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 1) 101 Total params：204,101\nTrainable params：204,101\nNon-trainable params：0\n3.4 模型和编译及超参数设置 本文模型在以验证集的损失函数最小为目标，进行不断的超参数调试之后，对于浦发银行的三个模型都选用 epochs=100，batch_size=32；对于 IBM 的三个模型都选用 epochs=50，batch_size=32。其中优化器都采用自适应矩估计（Adaptive moment estimation，Adam）$^{[19]}$。使用其 keras 包中的默认值，即 lr=0.001、beta_1=0.9、beta_2=0.999、epsilon=1e-08 和 decay=0.0。损失函数采用均方误差（Mean Square Error，MSE）。\n参数解释：\nlr：学习率 beta_1：一阶矩估计的指数衰减率 beta_2：二阶矩估计的指数衰减率 epsilon：模糊因子 decay：每次更新后的学习率衰减值 3.5 实验结果与分析 首先简单介绍一下模型使用评价的这几个指标。计算公式如下：\n均方误差（Mean Square Error，MSE）： $$ M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2} $$ 均方根误差（Root Mean Squared Error，RMSE)： $$ R M S E=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}} $$ 平均绝对误差（Mean Absolute Error，MAE）： $$ M A E=\\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-\\hat{Y}_{i}\\right| $$ \\( R^2 \\)（R Squared）： $$ \\begin{gathered} \\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} \\\\ R^{2}=1-\\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\end{gathered} $$其中：$n$ 是样本的数量，$Y$ 是股票实际的收盘价，$\\hat{Y}_{i}$ 是股票预测的收盘价， $\\bar{Y}$ 是股票平均的收盘价。MSE，RMSE 和 MAE 越小该模型越精确。 $R^{2}$ 评价模型系数拟合优度越大越好。\n3.5.1 浦发银行实验结果 表 3.6 浦发银行实验结果 LSTM GRU BiLSTM MSE 0.059781 0.069323 0.056454 RMSE 0.244501 0.263292 0.237601 MAE 0.186541 0.202665 0.154289 R-squared 0.91788 0.896214 0.929643 比较三个模型的评价指标，我们可以发现在浦发银行测试集上 BiLSTM 模型的 MSE、RMSE 和 MAE 都小于 LSTM 模型和 GRU 模型，而 R-Squared 都大于 LSTM 模型和 GRU 模型。我们通过对比 RMSE 发现，BiLSTM 相较于 LSTM 在验证集上有 2.90%的性能提升，BiLSTM 相较于 GRU 在验证集上有 10.81%的性能提升。\n3.5.2 IBM实验结果 表 3.7 IBM实验结果 LSTM GRU BiLSTM MSE 18.01311 12.938584 11.057501 RMSE 4.244186 3.597024 3.325282 MAE 3.793223 3.069033 2.732075 R-squared 0.789453 0.851939 0.883334 比较三个模型的评价指标，我们可以发现在 IBM 测试集上 BiLSTM 模型的 MSE、RMSE 和 MAE 都小于 LSTM 模型和 GRU 模型，而 R-Squared 都大于 LSTM 模型和 GRU 模型。我们通过对比 RMSE 发现，BiLSTM 相较于 LSTM 在验证集上有 27.63%的性能提升，BiLSTM 相较于 GRU 在验证集上有 8.17%的性能提升。\n3.6 本章小结 本章先是介绍了实验所选用的浦发银行和 IBM 两个数据集以及选用的特征，之后对数据集进行了归一化、数据划分的预处理步骤。同时详细的说明了实验所使用 LSTM、GRU 和 BiLSTM 模型的网络结构和超参数。最后得到了每个模型的损失函数图像和一系列的拟合图形。比较了模型的多个评价指标和拟合图像最终得到 BiLSTM 模型能够更好地对股票价格进行预测，为我们下一章研究 LightGBM-BiLSTM 的量化投资策略奠定了基础。\n第四章 基于 LightGBM-BiLSTM 的量化投资模型研究 4.1 实验步骤 Fig. 11. LightGBM-BiLSTM Diagram.\n如 Fig.11 所示，本实验先从因子库中选取 50 个因子。之后对因子依次进行去极值、标准化和缺损值填充的因子清洗步骤。再利用 LightGBM 模型进行因子选择，根据因子重要性进行排序得到前十的因子作为本横截面挑选出来的因子。紧接着使用 BiLSTM 建立多因子模型，最后再进行回测分析。\n4.2 实验数据 4.2.1 股票数据 本文采用的行情数据来源于 Tushare。具体数据集的特征如表4.1所示。\n表 4.1 数据集包含的特征 名称 类型 描述 ts_code str 股票代码 trade_date str 交易日期 open float 开盘价 high float 最高价 low float 最低价 close float 收盘价 pre_close float 昨收价 change float 涨跌额 pct_chg float 涨跌幅（未复权） vol float 成交量（手） amount float 成交额（千元） A股全市场日线数据集包含5,872,309行数据，即包含5,872,309个样本。如表4.2所示，A股全市场日线数据集数据集有以下11个特征，分别依次为股票代码（ts_code）、交易日期（trade_date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、昨收价（pre_close）、涨跌额（change）、换手率（turnover_rate）、交易金额（amount）、总市值（total_mv）和复权因子（adj_factor）。\n表 4.2 A股全市场日线数据集部分展示 ts_code trade_date open high low close pre_close change vol amount 600613.SH 20120104 8.20 8.20 7.84 7.86 8.16 -0.30 4762.98 3854.1000 600690.SH 20120104 9.00 9.17 8.78 8.78 8.93 -0.15 142288.41 127992.6050 300277.SZ 20120104 22.90 22.98 20.81 20.88 22.68 -1.80 12212.39 26797.1370 002403.SZ 20120104 8.87 8.90 8.40 8.40 8.84 -0.441 10331.97 9013.4317 300179.SZ 20120104 19.99 20.32 19.20 19.50 19.96 -0.46 1532.31 3008.0594 600000.SH 20120104 8.54 8.56 8.39 8.41 8.49 -0.08 342013.79 290229.5510 300282.SZ 20120104 22.90 23.33 21.02 21.02 23.35 -2.33 38408.60 86216.2356 002319.SZ 20120104 9.74 9.95 9.38 9.41 9.73 -0.32 4809.74 4671.4803 601991.SH 20120104 5.17 5.39 5.12 5.25 5.16 0.09 145268.38 76547.7490 000780.SZ 20120104 10.42 10.49 10.00 10.00 10.30 -0.30 20362.30 20830.1761 [5872309 rows x 11 columns]\n中证全指日线数据集包含5,057行数据，即包含5,057个样本。如表4.3所示，中证全指日线数据集有以下7个特征，分别依次为交易日期（trade_date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、交易量（volume）和昨收价（pre_close）。\n表 4.3 中证全指日线数据集部分展示 trade_date open high low close volume pre_close 2006-11-24 1564.3560 1579.3470 1549.9790 1576.1530 7.521819e+09 1567.0910 2006-11-27 1574.1130 1598.7440 1574.1130 1598.7440 7.212786e+09 1581.1530 2006-11-28 1597.7200 1604.7190 1585.3620 1596.8400 7.025637e+09 1598.7440 2006-11-29 1575.3030 1620.2870 1575.3030 1617.9880 7.250354e+09 1596.8400 2006-11-30 1621.4280 1657.3230 1621.4280 1657.3230 9.656888e+09 1617.9880 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 2020-11-11 5477.8870 5493.5867 5422.9110 5425.8017 5.604086e+10 5494.1042 2020-11-12 5439.2296 5454.3452 5413.9659 5435.1379 4.594251e+10 5425.8017 2020-11-13 5418.2953 5418.3523 5364.2031 5402.7702 4.688916e+10 5435.1379 2020-11-16 5422.3565 5456.7264 5391.9232 5456.7264 5.593672e+10 5402.7702 2020-11-17 5454.0696 5454.0696 5395.6052 5428.0765 5.857009e+10 5456.7264 [5057 rows x 7 columns]\n下表是原始的因子部分数据。依次经过上述因子缺失值填充、因子去极值、因子标准化和因子中性化这 4 个因子清洗的步骤后，得到如表展示的经过因子清洗后的因子部分数据。\n表 4.4 原始的因子数据 trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 表 4.5 清洗后的因子数据 sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 表 4.6 因子数据 价量因子构建 本文使用如下方式构建价量因子，构建价量因子的基础要素有两点：首先是基础字段，其次是算子。如表4.1所示，基础字段包括日频的最高价（high），最低价（low），开盘价（open），收盘价（close），上一日收盘价（pre_close），成交量（vol），涨跌（pct_chg），换手率（turnover_rate），交易金额（amount），总市值（total_mv）和复权因子（adj_factor）。\n表 4.7 基础字段表 编号 字段名 意义 high 最高价 当日成交订单中最高的价格 low 最低价 当日成交订单中最低的价格 open 开盘价 当日集合竞价成交的价格 close 收盘价 当日最后一笔成交订单的价格 pre_close 上一日收盘价 上一日最后一笔成交订单的价格 vol 成交 全天成交的股票数 pct_chg 涨跌 本日证券涨跌 turnover_rate 换手率 本日证券的换手率 amount 成交金额 全天成交的金额 total_mv 总市值 总股本数乘以当时股价得出的股票总价值 adj_factor 复权因子 权息修复比例 本文通过 gplearn提供的基础算子集和自己定义的一些特殊算子，得到如表4.8所示的算子列表。\n表 4.8 算子列表 算子 名称 定义 add(x, y) 和 \\( x + y\\)；点运算 \\(\\operatorname{div}(x, y)\\) 除 \\( x / y\\)；点运算 \\(\\operatorname{mul}(x, y)\\) 乘 \\( x \\cdot y\\)；点运算 \\(\\operatorname{sub}(x, y)\\) 减 \\( x - y\\)；点运算 neg(x) 负 \\(-x\\)；点运算 \\(\\log(x)\\) 对数 \\(\\log(x)\\)；点运算 max(x, y) 最大值 \\(x, y\\) 中数值较大的数；点运算 \\(\\min(x, y)\\) 最小值 \\(x, y\\) 中数值较小的数；点运算 delta_d(x) d 日差值 当日的 \\(x\\) 值减去 d 日前的 \\(x\\) 值；时序运算 delay_d(x) d 日延时 d 日前的 \\(x\\) 值；时序运算 Corr_d(x, y) d 日相关性 d 日 \\(x\\) 值和 d 日 \\(y\\) 值的相关性；时序运算 Max_d(x) d 日最大值 d 日 \\(x\\) 值的最大值；时序运算 Min_d(x) d 日最小值 d 日 \\(x\\) 值的最小值；时序运算 sort_d(x) d 日排序位置 d 日 \\(x\\) 值的排序值；时序运算 Argsortmin_d(x) d 日最小值位置 d 日 \\(x\\) 值的最小值的位置；时序运算 Argsortmax_d(x) d 日最大值位置 d 日 \\(x\\) 值的最大值的位置；时序运算 \\(\\operatorname{inv}(x)\\) 倒数 \\( 1 / x\\)；点运算 Std_d(x) d 日方差 d 日 \\(x\\) 值的方差；时序运算 abs(x) 绝对值 \\(\\lvert x\\rvert\\)；点运算 遗传规划 遗传规划（Genetic Programming, GP）的核心思想是使用进化算法在算子（operators）与基础字段（terminals）组合而成的巨大搜索空间中，自动“进化”出具有较强预测能力的因子表达式。对于本文中的因子挖掘来说，GP 的主要目标是从表4.7中的基础字段和表4.8中的算子所能组合成的所有可能表达式中，搜索并找到那些能对下一期股票收益有较好预测效果的因子。GP 的核心流程可分为以下几个步骤：\n1. 初始化（Initialization） 定义算子集与基础字段\n算子集（operators）如表4.8所示，包括加、减、乘、除、对数、绝对值、延时、移动最大/最小值、移动相关系数等运算。 基础字段（terminals）如表4.7所示，包括开盘价、收盘价、最高价、最低价、成交量、复权因子等。\n这些算子和基础字段可以视作因子表达式树中的“节点”，其中基础字段为叶子节点（终端节点），算子为内部节点。 随机生成初始种群\n在初始化阶段，根据给定的算子集与字段集，随机“拼接”生成一系列因子表达式（可表示为若干语法树或表达式树），形成初始种群。 例如，可能随机产生\n\\[ \\text{因子1}: \\mathrm{Max\\_5}\\bigl(\\mathrm{add}(\\mathrm{vol}, \\mathrm{close})\\bigr), \\quad \\text{因子2}: \\mathrm{sub}\\bigl(\\mathrm{adj\\_factor}, \\mathrm{neg}(\\mathrm{turnover\\_rate})\\bigr), \\dots \\] 每个因子表达式都将对应一个个体（individual）。 2. 适应度函数（Fitness Function） 度量因子的预测能力\n针对每个表达式（个体），我们需要评估它对未来收益或其他目标的预测能力。具体来说，可以在下一期股票收益 \\( r^{T+1} \\) 与当前期因子暴露度 \\( x_k^T \\) 之间，计算其相关系数（IC）或更综合的指标 IR（Information Ratio）来衡量。 设定目标\n若我们希望因子具有更高的相关性（IC），则可令适应度函数为 \\(\\lvert \\rho(x_k^T, r^{T+1})\\rvert\\)； 若我们希望因子的 IR 更高，则可设定适应度函数为 IR 值。 因子 IC 或 IR 越高，该表达式的“适应度”就越高。 \\[ \\text{Fitness} \\bigl(F(x)\\bigr) \\;=\\; \\begin{cases} \\lvert \\rho(x_k^T, r^{T+1})\\rvert \\quad \u0026\\text{(IC最大化)},\\\\[6pt] \\mathrm{IR}(x_k^T) \\quad \u0026\\text{(IR最大化)}. \\end{cases} \\] 其中 \\(\\rho(\\cdot)\\) 表示相关系数，\\(\\mathrm{IR}(\\cdot)\\) 为 IR 指标。\n3. 选择（Selection）、交叉（Crossover）与变异（Mutation） 选择（Selection）\n根据适应度函数的结果，将因子适应度高的表达式“保留”或“繁衍”，适应度较低的表达式则被淘汰。 这类似于生物进化中的“优胜劣汰”。 交叉（Crossover）\n将若干适应度较高的表达式（父本）随机选取一部分“节点”进行交换，从而得到新的表达式（子本）。 在表达式树结构中，可以将子树 A 与子树 B 互换，从而产生新的后代表达式。 例如，若表达式树 \\(\\mathrm{FactorA}\\) 的某个子树与表达式树 \\(\\mathrm{FactorB}\\) 的对应子树相交换，就生成了两个新的表达式。 变异（Mutation）\n以一定概率对表达式的某些节点进行随机变更，比如： 更换节点的算子（例如将 \\(\\mathrm{add}\\) 换为 \\(\\mathrm{sub}\\)）， 替换终端节点的基础字段（例如将 \\(\\mathrm{vol}\\) 换为 \\(\\mathrm{close}\\)）， 或随机改变运算参数（如移动窗口长度、平滑因子等）。 变异可以增加群体的多样性，避免过早收敛或陷入局部最优。 4. 迭代进化（Iteration） 循环执行\n将选择、交叉、变异的操作反复执行多代（generations）。 每一代都产生一个新的因子表达式种群，并对其进行适应度评估。 收敛与终止\n当进化达到预先设定的停止条件（如迭代次数、适应度阈值等）时，算法终止。 通常我们会选出若干个最终适应度较高的因子表达式，将它们视为进化结果。 5. 数学表征：搜索最优因子表达式 将上述过程抽象成下式，可以简单表示因子的搜索目标：\n\\[ F(x) \\;=\\; \\mathrm{GP}\\bigl(\\{\\text{operators}\\}, \\{\\text{terminals}\\}\\bigr), \\] 表示通过 GP 算法在给定算子集（operators）和基础字段集（terminals）上搜索出一个函数 \\(F(x)\\)。从最优化的角度看，我们希望找到：\n\\[ \\max_{F} \\bigl\\lvert \\rho(F^T, r^{T+1}) \\bigr\\rvert \\quad \\text{或者} \\quad \\max_{F} \\; \\mathrm{IR}\\bigl(F\\bigr), \\] 其中\n\\(\\rho(\\cdot)\\) 表示因子与下一期收益的相关系数（IC）， \\(\\mathrm{IR}(\\cdot)\\) 表示该因子的 IR 指标。 在实际应用中，我们会给定一段回测期，对每一代的候选因子进行打分（IC/IR 评估），通过选择、交叉和变异的迭代过程不断“进化”出更优质的因子。\n通过以上步骤，我们最终能够在庞大的算子组合与基础字段组合的搜索空间中，自动挖掘到一批对未来收益有较强预测能力、且具有较好稳健性（如 IR 较高）的因子表达式。\n表 4.9 挖掘出的部分因子 因子名 定义 0 Max＿25(add(turnover_rate, vol)) 1 Max＿30(vol) 2 Max＿25(turnover_rate) 3 Max＿35(add(vol, close)) 4 Max＿30(turnover_rate) 5 sub(Min＿20(neg(pre_close)), div(vol, adj_factor)) 6 Max＿60(max(vol, adj_factor)) 7 Max＿50(amount) 8 div(vol, neg(close)) 9 min(ArgSortMin＿25(pre_close), neg(vol)) 10 neg(max(vol, turnover_rate)) 11 mul(amount, neg(turnover_rate)) 12 inv(add(ArgSortMax＿40(change), inv(pct_chg))) 13 Std＿40(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))) 14 div(log(total_mv),amount) 15 div(neg(Max＿5(amount)), Min＿20(ArgSort＿60(high))) 16 Corr＿30(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))), add(log(Max＿10(pre_close)), high)) 17 ArgSort＿60(neg(turnover_rate)) \u0026hellip; \u0026hellip; 这些因子均是通过遗传规划从算子列表（表4.8）与基础字段列表（表4.7）中组合而得，具有不同的数学表达形式。\n因子有效性检验 当我们得到挖掘的因子之后，需要对因子进行有效性检验，常见的检验指标有信息系数（Information Coefficient，IC）和信息比率（Information Ratio，IR）。\n信息系数（IC）描述的是所选股票下期收益率和本期因子暴露度的线性相关程度，可以反应该因子进行收益率预测的稳健性 **信息比率（IR）**是超额收益的均值与超额收益的标准差之比，信息比率与夏普比率类似，主要区别在于夏普比率使用无风险收益作为基准，而信息比率使用风险指数作为基准。夏普比率有助于确定投资组合的绝对收益，信息比率有助于确定投资组合的相对收益。当我们计算了 IC 之后，可以根据 IC 的值再对 IR 进行计算。当 IR 大于 0.5 时，因子稳定获取超额收益能力较强。 实际计算中，因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 值一般是指所选股票第 \\(T\\) 期的因子 \\(k\\) 上的暴露度 \\( x_k^T \\) 与所选股票第 \\(T+1\\) 期的收益率 \\( r^{T+1} \\) 的相关系数；因子 \\(k\\) 的 \\( \\mathrm{IR} \\) 值为因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 的均值除以因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 的标准差，计算公式如下：\n$$ \\begin{gathered} I C=\\rho_{x_{k}^{T}, r^{T+1}}=\\frac{\\operatorname{cov}\\left(x_{k}^{T}, r^{T+1}\\right)}{\\sigma_{x_{k}^{T}} \\sigma_{r^{T+1}}}=\\frac{\\mathrm{E}\\left(x_{k}^{T} * r^{T+1}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right) \\mathrm{E}\\left(r^{T+1}\\right)}{\\sqrt{\\mathrm{E}\\left(\\left(x_{k}^{T}\\right)^{2}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right)^{2}} \\cdot \\sqrt{\\mathrm{E}\\left(\\left(r^{T+1}\\right)^{2}\\right)-\\mathrm{E}\\left(r^{T+1}\\right)^{2}}} \\\\ I R=\\frac{\\overline{I C}}{\\sigma_{I C}} \\end{gathered} $$其中：\n$x_{k}^{T}$ ：所选股票第 $T$ 期的因子 $k$ 上的暴露度 $r^{T+1}$ ：所选股票第 $T+1$ 期的收益率 $\\overline{I C}: I C$ 的均值 本文采用 IR 判断因子好坏，通过对大量不同的算子和基础数据的组合以及 IC 和 IR 的“篮选”，文章得到了本文所选用的 50 个价量因子。经过 IR 检测，按 IR 由高到低排序得到如下图所示的表格。从下表中我们可以看出来所选的 50 个价量因子的 IR 都大于 0.5 ，说明这些因子稳定获取超额收益能力较强。\n表 4.10 因子 IR 检验表 因子名 IR 因子名 IR 0 3.11 25 2.73 1 2.95 26 2.71 2 2.95 27 2.70 3 2.95 28 2.69 4 2.95 29 2.69 5 2.94 30 2.69 6 2.94 31 2.68 7 2.94 32 2.68 8 2.93 33 2.68 9 2.93 34 2.68 10 2.93 35 2.67 11 2.92 36 2.67 12 2.91 37 2.66 13 2.89 38 2.65 14 2.86 39 2.65 15 2.83 40 2.65 16 2.83 41 2.65 17 2.83 42 2.64 18 2.79 43 2.63 19 2.78 44 2.63 20 2.78 45 2.62 21 2.76 46 2.62 22 2.75 47 2.62 从该表可见，在所筛选的因子中，所有因子的 IR 均大于 0.5，具有较强且稳定的获取超额收益的能力。\n4.3 因子清洗 4.3.1 因子缺失值处理和去极值 对于因子的缺失值处理的方法有个案剔除法，均值替换法，回归替换法等方法。本文采用较为简单的均值替换法对缺损值进行处理，即利用因子的平均值来替代缺失的数据。对于因子去极值有中位数去极值，百分比去极值和 $3 \\sigma$ 去极值等方法。本文采用的是 $3 \\sigma$ 去极值法，该方法是利用统计学上的 $3 \\sigma$ 原则，将把离该因子均值三个标准差以上的极值因子转化到刚好离均值三个标准差的位置，具体计算公式如下：\n$$ X_i^{\\prime}= \\begin{cases} \\bar{X}+3 \\sigma \u0026 \\text{if } X_i \u003e \\bar{X} + 3 \\sigma \\\\ \\bar{X}-3 \\sigma \u0026 \\text{if } X_i \u003c \\bar{X} - 3 \\sigma \\\\ X_i \u0026 \\text{if } \\bar{X} - 3 \\sigma \u003c X_i \u003c \\bar{X} + 3 \\sigma \\end{cases} $$其中：\n$X_{i}$ ：因子处理之前的值 $\\bar{X}$ ：因子序列的均值 $\\sigma$ ：因子序列的标准差 $X_{i}^{\\prime}$ ：去极值后的因子的值 4.3.2 因子的标准化 本文实验选取了多个因子，并且各个因子量纲并不完全相同，为了我们方便进行比较和回归，我们还需对因子进行标准化处理。目前常用的具体的标准化方法有 Min－Max标准化，Z－score 标准化和 Decimal scaling 小数定标标准化等。本文选择 Z－score 标准化的方法。通过原始数据的均值和标准差进行数据的标准化，经过处理的数据符合标准正态分布，即均值为 0 ，标准差为 1 ，其标准化后的数值大小有正有负，得到标准正态分布曲线。\n本文采用 Z－score 标准化公式如下：\n$$ \\tilde{x}=\\frac{x_{i}-u}{\\sigma} $$其中：\n$x_{i}$ ：因子的原值 $u$ ：因子序列的均值 $\\sigma$ ：因子序列的标准差 $\\tilde{x}$ ：标准化后的因子值 4.3.3 因子的中性化 因子中性化是为了剔除其他因素对我们所选因子的影响，使我们构建量化投资策略组合所选择的股票更加分散，而不是集中在特定的行业或者市值的股票上，可以更好地分担投资组合的风险和解决因子多重共线性的问题。市值和行业是影响股票收益最主要的两种自变量，所以在进行因子清洗的过程中，还必须考虑市值和行业的影响。本文实证中我们采用仅纳入行业因子，而将市场因子包含在行业因子中的方法。针对因子的单因子回归模型见公式(31)，我们将以下回归模型的残差项作为因子中性化后的新的因子值。\n$$ \\tilde{r}_{j}^{t}=\\sum_{s=1}^{s} X_{j s}^{t} \\tilde{f}_{s}^{t}+X_{j k}^{t} \\tilde{f}_{k}^{t}+\\tilde{u}_{j}^{t} $$其中：\n$\\tilde{r}_{j}^{t}$ ：股票 $j$ 在第 $t$ 期的收益率 $X_{j s}^{t}$ ：股票 $j$ 在第 $t$ 期在行业 $s$ 上的暴露度 $\\tilde{f}_{s}^{t}$ ：行业在第 $t$ 期的收益率 $X_{j k}^{t}$ ：股票 $j$ 在第 $t$ 期在因子 $k$ 上的暴露度 $\\tilde{f}_{k}^{t}$ ：因子 $k$ 在第 $t$ 期的收益率 $\\tilde{u}_j^t$ ：一个 $0-1$ 哑变量，即如果股票 $j$ 属于行业 $s$ ，则暴露度为 1 ，否则为 0 在本文中，并不会对公司所属行业进行按照比例拆分，即股票 $j$ 只能属于一个特定的行业 $s$ ，在行业 $s$ 上的暴露度为 1 ，在其他所有行业的暴露度为 0 。本文使用申万宏源行业分类标准，具体分类情况依次为农林牧渔，采掘，化工，钢铁，有色金属，电子元器件，家用电器，食品饮料，纺织服装，轻工制造，医药生物，公用事业，交通运输，房地产，商业贸易，餐饮旅游，综合，建筑材料，建筑装饰，电器设备，国防军工，计算机，传媒，通信，银行，非银金融，汽车和机械设备这 28 类。下表为 2021 年 2 月 5 日的申万指数一级行业历史行情图。\n表 4.9 2021年2月5日的申万指数一级行业历史行情图 指数代码 指数名称 发布日期 开盘指数 最高指数 最低指数 收盘指数 成交量(亿殴) 成交额(亿元) 涨跌幅(%) 801010 农林牧渔 2021/2/5 0:00 4111.43 4271.09 4072.53 4081.81 15.81 307.82 -0.3 801020 采掘 2021/2/5 0:00 2344.62 2357.33 2288.97 2289.41 18.06 115.6 -2.25 801030 化工 2021/2/5 0:00 4087.77 4097.59 3910.67 3910.67 55.78 778.85 -3.95 801040 钢铁 2021/2/5 0:00 2253.78 2268.17 2243.48 2250.81 11.61 48.39 -1.02 801050 有色金属 2021/2/5 0:00 4212.1 4250.59 4035.99 4036.74 45.41 593.92 -4.43 801080 电子元器件 2021/2/5 0:00 4694.8 4694.8 4561.95 4561.95 52.67 850.79 -2.78 801110 家用电器 2021/2/5 0:00 10033.82 10171.26 9968.93 10096.83 8.55 149.18 0.83 801120 食品饮料 2021/2/5 0:00 30876.33 31545.02 30649.57 30931.69 11.32 657.11 0.47 801130 纺织服装 2021/2/5 0:00 1614.48 1633.89 1604.68 1607.63 6.28 57.47 -0.39 801140 轻工制造 2021/2/5 0:00 2782.07 2791.88 2735.48 2737.24 15.28 176.16 -1.35 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 数据来源：申银万国\n下表是原始的因子部分数据。依次经过上述因子缺失值填充、因子去极值、因子标准化和因子中性化这 4 个因子清洗的步骤后，得到如表展示的经过因子清洗后的因子部分数据。\n表 4.10 原始的因子数据 trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 表 4.11 清洗后的因子数据 sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.4 基于 LightGBM 的因子选择 Friedman（2001）$^{[20]}$ 提出的梯度提升决策树（Gradient Boosting Decision Tree，GBDT）是一种基于迭代的回归型决策树。其主要思想是通过逐步添加弱分类器（通常是决策树）来优化模型，使得整体模型能够最小化损失函数。GBDT 的模型可以表示为：\n$$ \\hat{y} = \\sum_{m=1}^{M} \\gamma_m h_m(\\mathbf{x}) $$其中：\n\\( M \\) 是迭代次数， \\( \\gamma_m \\) 是第 \\( m \\) 个弱分类器的权重， \\( h_m(\\mathbf{x}) \\) 是第 \\( m \\) 个决策树模型。 GBDT 的训练过程通过逐步拟合负梯度方向来最小化损失函数，具体更新公式为：\n$$ \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{N} L\\left(y_i, \\hat{y}_{i}^{(m-1)} + \\gamma h_m(\\mathbf{x}_i)\\right) $$其中，\\( L \\) 是损失函数，\\( y_i \\) 是真实值，\\( \\hat{y}_{i}^{(m-1)} \\) 是第 \\( m-1 \\) 次迭代后的预测值。\n轻量级梯度提升机（Light Gradient Boosting Machine，LightGBM)$^{[21]}$ 是一个高效实现 GBDT 算法的框架，最初由 Microsoft 开发，作为一个免费开源的分布式梯度提升框架。LightGBM 基于决策树算法，广泛应用于排名、分类及其他机器学习任务，开发重点在于性能和可伸缩性。其主要优势包括高效率的并行训练、更快的训练速度、更低的内存消耗、更好的准确率，以及支持分布式计算和快速处理海量数据$^{[22]}$。\nLightGBM 的核心算法基于以下优化目标：\n$$ L = \\sum_{i=1}^{N} l(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(h_m) $$其中，\\( l \\) 是损失函数，\\( \\Omega \\) 是正则化项，用于控制模型复杂度，通常表示为：\n$$ \\Omega(h_m) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 $$这里，\\( T \\) 是树的叶子数，\\( w_j \\) 是第 \\( j \\) 个叶子的权重，\\( \\gamma \\) 和 \\( \\lambda \\) 是正则化参数。\nLightGBM 采用基于梯度的单边采样（Gradient-based One-Side Sampling，GOSS）和互斥特征捆绑（Exclusive Feature Bundling，EFB）等技术，显著提升了训练效率和模型性能。\n在本研究中，训练过程中使用的损失函数为均方误差（Mean Squared Error，MSE），其定义为：\n$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$其中，\\( y \\) 为真实收益率，\\( \\hat{y} \\) 为模型预测的收益率，\\( N \\) 为样本数量。\n本小节算法的具体流程如下：\n数据准备：使用一年的每只股票的 50 个因子数据（A 股全市场数据）和历史未来一个月的收益率作为特征。\n模型训练：利用网格搜索法（Grid Search）优化 LightGBM 模型的超参数，训练模型以预测未来一个月的收益率。模型训练流程如图4.12所示。\n$$ \\text{参数优化：} \\quad \\theta^* = \\arg\\min_\\theta \\sum_{i=1}^{N} L(y_i, \\hat{y}_i(\\theta)) $$其中，\\( \\theta \\) 表示模型的超参数集合，\\( \\theta^* \\) 为最优参数。\n因子重要性计算：使用 LightGBM 的 feature_importances_ 方法计算各因子的特征重要性。特征重要性主要通过两个指标衡量：\nSplit：该特征在所有树中被用于分裂的次数。 Gain：该特征在所有分裂中带来的总增益（即损失函数的减少量）。 因子的特征重要性可以表示为：\n$$ \\text{Importance}_{\\text{split}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\mathbb{I}(f \\text{ 被用于第 } j \\text{ 个叶节点的分裂}) $$$$ \\text{Importance}_{\\text{gain}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\Delta L_{m,j} \\cdot \\mathbb{I}(f \\text{ 被用于第 } j \\text{ 个叶节点的分裂}) $$其中，\\( \\mathbb{I} \\) 是指示函数，\\( \\Delta L_{m,j} \\) 是因子 \\( f \\) 在第 \\( m \\) 棵树的第 \\( j \\) 个分裂中带来的损失减少量。\n因子筛选：根据模型计算的因子重要性进行排序，选择前十个重要性最高的因子作为本横截面分析中使用的因子。所选因子的重要性如表4.9所示。\n表 4.9 部分所选因子重要性排序 importance feature_name trade_date 35 factor_35 2010-08-11 27 factor_27 2010-08-11 33 factor_33 2010-08-11 20 factor_20 2010-08-11 24 factor_24 2010-08-11 45 factor_45 2010-08-11 37 factor_37 2010-08-11 49 factor_49 2010-08-11 19 factor_19 2010-08-11 47 factor_47 2010-08-11 22 factor_22 2010-09-09 20 factor_20 2010-09-09 30 factor_30 2010-09-09 24 factor_24 2010-09-09 代码实现片段：以下是训练过程所使用的部分代码，用于因子选择。 def feature_choice( self, days=21, is_local=False ): if is_local: feature_info = pd.read_hdf(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;)) else: factors = self.get_env().query_data(Factors_Data) factors = factors[ factors[COM_DATE] \u0026gt;= \u0026#39;2010-01-01\u0026#39; ] trade_list = list(set(factors[COM_DATE])) trade_list.sort() if len(trade_list) % days == 0: n = int(len(trade_list) / days) - 7 else: n = int(len(trade_list) / days) - 6 feature_info = pd.DataFrame() begin_index = 147 feature = list(factors.columns) feature.remove(COM_SEC) feature.remove(COM_DATE) feature.remove(Ret) for i in range(n): end_date = days * i + begin_index - 21 begin_date = days * i trade_date = days * i + begin_index print(trade_list[trade_date]) train_data = factors[ (factors[COM_DATE] \u0026lt;= trade_list[end_date]) \u0026amp; (factors[COM_DATE] \u0026gt;= trade_list[begin_date]) ] model = lgb.LGBMRegressor() model.fit(train_data[feature], train_data[Ret]) feature_info_cell = pd.DataFrame(columns=Info_Fields) feature_info_cell[Importance] = model.feature_importances_ feature_info_cell[Feature_Name] = model.feature_name_ feature_info_cell = feature_info_cell.sort_values(by=Importance).tail(10) feature_info_cell[COM_DATE] = trade_list[trade_date] feature_info = pd.concat( [feature_info, feature_info_cell], axis=0 ) h = pd.HDFStore(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;), \u0026#39;w\u0026#39;) h[\u0026#39;data\u0026#39;] = feature_info h.close() self.get_env().add_data(feature_info, Feature_Info) pass 通过上述流程，利用 LightGBM 高效地筛选出对预测未来收益率最具影响力的因子，从而提升模型的预测能力和解释性。\n4.5 基于 BiLSTM 的因子组合 本小节使用 BiLSTM 进行因子组合。BiLSTM 的具体原理在第二章已经介绍了，这里不再赘述。下面先介绍一下使用模型的具体网络结构，本文通过大量反复试验最终 BiLSTM 设置的网络结构如表4.10和 Fig. 12 所示。其中层与层之间使用循环神经网络默认的 tanh 和 linear 作为激活函数。并且为了防止过拟合加入 Dropout，但是如果 Dropout 使用过大的丢弃比例会出现欠拟合的现象，因此 Dropout 的丢弃比例取值为 0.01。最终模型的 BiLSTM 循环层的神经元个数为 100，采用一层 BiLSTM 层和三层全连接层，其中 BiLSTM 层和第一个全连接层之间设置了一个 Dropout。\n表 4.10 BiLSTM的网络结构 Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 100) 24400 dropout_1 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 256) 25856 dropout_2 (Dropout) (None, 256) 0 dense_2 (Dense) (None, 64) 16448 dense_3 (Dense) (None, 1) 0 Total params：66,769\nTrainable params：66,769\nNon-trainable params：0\n因为本次实验使用数据的数据量较大，因此选用 epochs=400，batch_size=1024。模型的损失函数采用均方误差（Mean Square Error，MSE）。其中优化器采用随机梯度下降（Stochastic Gradient Descent，SGD）。随机梯度下降相对于梯度下降（Gradient Descent，GD）有在信息冗余的情况下更能有效地利用信息，前期迭代效果卓越，适合处理大样本的数据这三个优势 $^{[23]}$。由于本实验训练数据量较大，使用 SGD 的话每次仅用一个样本来迭代，训练的速度很快，可以大大减少我们训练所花费的时间。使用其 keras 包中的默认值，即 lr=0.01、momentum=0.0、decay=0.0 和 nesterov=False。\n参数解释：\nlr：学习率 momentum：动量参数 decay：每次更新后的学习率衰减值 nesterov：确定是否使用 Nesterov 动量 本小节算法的具体流程如下：\n使用一年的每只股票的 10 个因子（LightGBM 选出来的因子）和历史未来一个月的收益率的 A 股全市场数据作为特征。 以一年每支股票未来一个月的收益率为预测目标，利用 BiLSTM 进行训练，如 Fig. 12 所示。 Fig. 12. Rolling Window\n一个月的样本外数据的实时因子数据通过训练好的 BiLSTM 模型，得到实时的未来一月的每只股票预期收益率。收益率如表4.11所示。 表 4.11 部分股票预测收益率表 sec_code trade_date y_hat 000001.SZ 2011/5/26 0.0424621 000002.SZ 2011/5/26 -0.1632174 000004.SZ 2011/5/26 -0.0642319 000005.SZ 2011/5/26 0.08154649 000006.SZ 2011/5/26 0.00093213 000007.SZ 2011/5/26 -0.073218 000008.SZ 2011/5/26 -0.0464256 000009.SZ 2011/5/26 -0.091549 000010.SZ 2011/5/26 0.08154649 000011.SZ 2011/5/26 -0.1219943 000012.SZ 2011/5/26 -0.1448984 000014.SZ 2011/5/26 0.09038845 000016.SZ 2011/5/26 -0.11225 代码实现片段：以下是训练过程所使用的部分代码，用于构建BiLSTM训练网络。 def build_net_blstm(self): model = ks.Sequential() model.add( ks.layers.Bidirectional(ks.layers.LSTM( 50 ),input_shape=(11,10)) ) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(256)) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(64)) model.add(ks.layers.Dense(1)) model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;mse\u0026#39;) model.summary() self.set_model(model) 4.6 量化策略和策略回测 4.6.1 回测指标 下面先对策略的一些常见回测指标进行介绍。评价指标包括累计收益率（Total Rate of Return）、年化收益率（Annualized Rate of Return）、年化波动率（Annualized volatility）、夏普比率（Sharpe ratio）、最大回撤率 (Maximum Drawdown，MDD)、年化换手率（Annualized turnover rate）和年化交易成本率（Annualized transaction cost rate），其中假定一年股市开盘252天，无风险利率默认为0.035，手续费默认0.002。\n累计收益率（Total Rate of Return）：在其他指标相同的情况下，累计收益率越大说明该策略越好，越能带来更大的收益。公式如下： $$ \\text{Total Rate of Return} = r_{p} = \\frac{P_{1} - P_{0}}{P_{0}} $$$P_{1}$ ：最终股票和现金的总价值\n$P_{0}$ ：初始股票和现金的总价值\n年化收益率（Annualized Rate of Return）：是将累计总收益率换算成以年为单位的几何平均收益率。在其他指标相同的情况下，年化收益率越大，代表该策略越好。公式如下： $$ \\text{Annualized Rate of Return} = R_{p} = \\left(1 + r_{p}\\right)^{\\frac{252}{t}} - 1 $$$r_{p}$ ：累计收益率\n$t$ ：投资策略执行的天数\n年化波动率（Annualized volatility）：定义为对象资产的年回报率的对数值的标准差。年化波动率用来衡量策略的风险性，波动率越大代表策略的风险越高。公式如下： $$ \\begin{aligned} \\text{Annualized volatility} = \\sigma_{p} \u0026= \\sqrt{\\frac{252}{t-1} \\sum_{i=1}^{t}\\left(r_{d} - \\bar{r}_{d}\\right)^{2}} \\\\ \\bar{r}_{d} \u0026= \\frac{1}{t} \\sum_{i=1}^{t} r_{d_{i}} \\end{aligned} $$$r_{d_{i}}$ ：第 $i$ 天日收益率\n$\\bar{r}_{d}$ ：日平均收益率\n$t$ ：投资策略执行的天数\n夏普比率（Sharpe ratio）：是由 Sharpe（1966）$^{[24]}$ 提出的。它代表投资者额外承受一单位风险，所获得的超额收益$^{[25]}$。这里给出年化的夏普比率计算公式： $$ S = \\frac{R_{p} - R_{f}}{\\sigma_{p}} $$$R_{p}$ ：年化收益率\n$R_{f}$ ：无风险收益率\n$\\sigma_{p}$ ：年化波动率\n最大回撤率 (Maximum Drawdown，MDD)：表示表示我们策略运行期间股票和现金的总价值走到最低点时的收益率回撤幅度的最大值。最大回测率用来策略最极端可能的亏损情况。 $$ MDD = \\frac{\\max \\left(V_{x} - V_{y}\\right)}{V_{x}} $$$V_{x}$ 和 $V_{y}$ 分别为策略组合在第 $x$ 天和第 $y$ 天的股票和现金的总价值，且 $x \u0026lt; y$。\n年化换手率（Annualized turnover rate）：用来衡量对投资组合里面的股票买卖的频繁程度。越大说明改投资组合换仓越频繁，交易成本也会越大。 $$ \\text{change} = \\frac{N \\times 252}{t} $$$t$ ：投资策略执行的天数\n$N$ ：总共买进和卖出的次数\n年化交易成本率（Annualized transaction cost rate）：用来衡量投资组合策略的交易成本，越大说明交易成本越高。 $$ c = \\left(1 + \\text{commison}\\right)^{\\text{change}} - 1 $$change：年化换手率\ncommison：手续费\n4.6.2 策略及回测结果 本文量化交易策略采用每隔一个月进行换仓（即调仓周期为28个交易日），每次换仓采取等额持股的方式买入 BiLSTM 预测出的预期收益率最高的25支股票，卖出原本所持有的股票。本文的回测时间和规则如下：\n回测时间：从 2012 年 1 月到 2020 年 10 月。 回测股票池：全 A 股，剔除特别处理（Special treatment，ST）股票。 交易手续费：买入时支付给券商交易佣金千分之二，卖出时支付给券商交易佣金千分之二，其中单笔交易佣金不满5元券商按5元收取交易佣金。 买卖规则：当天开盘涨停股票不能买入，跌停股票不能卖出。 表 4.12 策略回测结果 累计收益率 年化收益率 年化波动率 夏普比率 最大回撤 年化换手率 年化交易成本率 策略 701.00% 29.18% 33.44% 0.77 51.10% 51.10% 11.35% 基准 110.40% 9.70% 26.01% 0.24 58.49% 58.49% 0.00% Fig. 22. Net Profit Curve\n回测结果如表4.12和 Fig.22 所示。我的策略采用的是本章所介绍的 LightGBM-BiLSTM 量化策略。基准采用的中证全指（000985）。由上面的结果可以看到，本策略累计收益率为701.00%，远高于基准110.40%；年化收益率为29.18%，远高于基准9.70%；夏普率为0.77，高于基准0.24。这三项回测指标说明 LightGBM-BiLSTM 量化策略确实能够给投资者带来更大的收益。本策略年化波动率为33.44%大于基准26.01%，最大回撤为51.10%小于基准58.49%，这两项回测指标说明 LightGBM-BiLSTM 量化策略存在一定的风险，特别是很难抵御系统性风险的冲击。年化换手率为11.35%，年化交易成本率为2.29%，说明我们策略不是高频交易策略，交易成本较小。从收益曲线图可以看出 LightGBM-BiLSTM 量化策略在前两年的收益率和基准相差不大，并没有特别的优势。但从2015年4月左右开始 LightGBM-BiLSTM 量化策略的收益率明显好于基准的收益率。总体而言，该 LightGBM-BiLSTM 量化策略的收益率十分可观，但仍然存在一定的风险。\n第五章 总结与展望 5.1 总结 本文首先介绍了基于深度学习的股票价格预测和量化策略研究的研究背景和研究意义，然后分别介绍了股票价格预测和量化投资策略国内外的研究现状，之后说明了本文的创新点和研究框架。接着本论文在相关理论基础章节大致介绍了本文用到的深度学习模型和量化投资的发展历程。重点介绍了 LSTM，GRU，BiLSTM 这三个模型的基本结构，基本原理和特点。\n随后，本文利用浦发银行和 IBM 的日频数据，通过一系列的数据处理过程和特征提取来对数据进行预处理。然后介绍了 LSTM，GRU，BiLSTM 这三个模型的具体网络结构以及超参数的设定。紧接着我们使用 LSTM，GRU，BiLSTM 分别进行两只股票收盘价的预测和模型评估比较。实验结果表明对于两只股票而言都是 BiLSTM 预测效果更加准确。\n最后，本论文为了进一步说明 BiLSTM 在金融上的运用价值，构建了基于 LightGBM－BiLSTM 的量化投资模型。选取 A 股全市场的股票和多个因子依次进行因子清洗，基于 LightGBM 的因子选择和基于 LSTM 的因子组合等过程。接着，我们构建一定的投资策略并通过累计收益率，年化收益率，年化波动率和夏普比率等评估指标与基准的持有中证全指进行对比。通过对比发现 LightGBM－BiLSTM 量化投资模型能带来更好的收益，说明了利用深度学习构建量化投资策略的有效性。\n5.2 展望 本文虽然分别对比 LSTM，GRU，BiLSTM 这三个模型预测股票收盘价的效果和基于 LightGBM－BiLSTM 量化投资策略取得了一定的成果，但本文研究仍有一些不足之处。结合本文的研究成果，可以进一步进行以下研究和改进：\n预测目标多样化：本文在预测股票价格方面，选取的股票收盘价作为预测目标，虽然这一结果最直观，但 Bachelier（1900）$^{[26]}$ 提出的随机游走假说（Random Walk Hypothesis，RWH）认为股票的价格服从随机漫步，是不可预测的。虽然之后有许多行为经济学家证明这一观点不完全正确，但这也同时说明单纯预测股票的收盘价难度和可解释性不那么强 $^{[27][28]}$。因此可以选择股票波动率预测，股票涨跌判断和股票收益率预测等作为未来的研究的方向。 模型多样化对比：本文在预测股票价格方面，对比了 LSTM，GRU 和 BiLSTM 这三种循环神经网络模型并且说明了 BiLSTM 预测效果比较好，但仍然缺少和其他更多不同模型的对比研究。因此未来可以深入研究与 Autoregressive Integrated Moving Average (ARIMA)，卷积神经网络（Convolutional Neural Networks，CNN），深度神经网络（Deep Neural Networks，DNN）, CNN－LSTM, Transformer 和 TimeGPT 等单一或复合模型之间的对比。 因子多样化：本文在构建量化投资策略方面使用的因子都是技术面的价量因子，因子的种类单一。未来可以选择财务因子，情绪因子，成长因子等不同种类的因子，从而提高策略的性能。同时未来研究还可以适当的加入择时策略，在预测大盘上涨时增加仓位，在预测大盘下跌时减少仓位，赚取贝塔（beta，$\\beta$）的钱。 投资组合优化：本文的因子组合过程仍然不完善，未来可以利用二次规划的方法对投资组合进行优化。 高频交易策略研究：本文的量化投资策略方法采取的是低频交易的策略，未来可以利用股票的 tick 数据来研究高频策略和超高频策略。 参考文献 [1] White, H. “Economic prediction using neural networks: The case of IBM daily stock returns.” Proc. of ICNN. 1988, 2: 451-458.\n[2] Kimoto, T., Asakawa, K., Yoda, M., et al. “Stock market prediction system with modular neural networks.” Proc. of 1990 IJCNN International Joint Conference on Neural Networks. IEEE, 1990: 1-6.\n[3] Zhang, G. P. “Time series forecasting using a hybrid ARIMA and neural network model.” Neurocomputing. 2003, 50: 159-175.\n[4] Akita, R., Yoshihara, A., Matsubara, T., et al. “Deep learning for stock prediction using numerical and textual information.” Proc. of 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS). IEEE, 2016: 1-6.\n[5] 宮崎邦洋, 松尾豊. “Deep Learning を用いた株価予測の分析.” 人工知能学会全国大会論文集 第31回全国大会. 一般社団法人 人工知能学会, 2017: 2D3OS19a3-2D3OS19a3.\n[6] Kim, T., Kim, H. Y. “Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data.” PLoS ONE. 2019, 14(2): e0212320.\n[7] Hochreiter, S., Schmidhuber, J. “Long short-term memory.” Neural Computation. 1997, 9(8): 1735-1780.\n[8] Cho, K., Van Merriënboer, B., Gulcehre, C., et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078. 2014.\n[9] Chung, J., Gulcehre, C., Cho, K. H., et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” arXiv preprint arXiv:1412.3555. 2014.\n[10] Gruber, N., Jockisch, A. “Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?” Frontiers in Artificial Intelligence. 2020, 3(40): 1-6.\n[11] Markowitz, H. “Portfolio Selection.” The Journal of Finance. 1952, 7(1): 77-91. doi:10.2307/2975974.\n[12] Merton, R. C. “An analytic derivation of the efficient portfolio frontier.” Journal of Financial and Quantitative Analysis. 1972: 1851-1872.\n[13] Sharpe, W. F. “Capital asset prices: A theory of market equilibrium under conditions of risk.” The Journal of Finance. 1964, 19(3): 425-442.\n[14] Lintner, J. “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.” Review of Economics and Statistics. 1965, 47(1): 13-37.\n[15] Mossin, J. “Equilibrium in a capital asset market.” Econometrica: Journal of the Econometric Society. 1966: 768-783.\n[16] Ross, S. A. “The arbitrage theory of capital asset pricing.” Journal of Economic Theory. 1976, 13(3): 341-60.\n[17] Fama, E. F., French, K. R. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics. 1993, 33(1): 3-56.\n[18] Fama, E. F., French, K. R. “A five-factor asset pricing model.” Journal of Financial Economics. 2015, 116(1): 1-22.\n[19] Kingma, D. P., Ba, J. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980. 2014.\n[20] Friedman, J. H. “Greedy function approximation: A gradient boosting machine.” Annals of Statistics. 2001: 1189-1232.\n[21] Kopitar, L., Kocbek, P., Cilar, L., et al. “Early detection of type 2 diabetes mellitus using machine learning-based prediction models.” Scientific Reports. 2020, 10(1): 1-12.\n[22] Ke, G., Meng, Q., Finley, T., et al. “Lightgbm: A highly efficient gradient boosting decision tree.” Advances in Neural Information Processing Systems. 2017, 30: 3146-3154.\n[23] Bottou, L., Curtis, F. E., Nocedal, J. “Optimization methods for large-scale machine learning.” SIAM Review. 2018, 60(2): 223-311.\n[24] Sharpe, W. F. “Mutual fund performance.” The Journal of Business. 1966, 39(1): 119-138.\n[25] Sharpe, W. F. “The sharpe ratio.” Journal of Portfolio Management. 1994, 21(1): 49-58.\n[26] Bachelier, L. “Théorie de la spéculation.” Annales Scientifiques de l\u0026rsquo;École Normale Supérieure. 1900, 17: 21-86.\n[27] Fromlet, H. “Behavioral finance-theory and practical application: Systematic analysis of departures from the homo oeconomicus paradigm are essential for realistic financial research and analysis.” Business Economics. 2001: 63-69.\n[28] Lo, A. W. “The adaptive markets hypothesis.” The Journal of Portfolio Management. 2004, 30(5): 15-29.\n参考博客 Colah\u0026rsquo;s Blog. (2015, August 27). Understanding LSTM Networks. 引用 引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Apr 2021). 基于深度学习的股票价格预测和量化策略研究.\nhttps://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\nOr\n@article{syhya2021stockprediction, title = \u0026#34;基于深度学习的股票价格预测和量化策略研究\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2021\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2021-04-21-deep-learning-stock-prediction/","summary":"\u003ch2 id=\"摘要\"\u003e摘要\u003c/h2\u003e\n\u003cp\u003e股票市场是金融市场的重要组成部分，近些年来，股票市场蓬勃发展，股票价格预测和量化投资策略研究吸引了许多领域的研究学者。其中最近几年随着人工智能和机器学习的发展，学者们从传统的统计学模型迁移到了人工智能算法，尤其是在深度学习热潮掀起后，神经网络在股票价格预测和量化投资策略研究中取得了不错的效果。深度学习的目标是学习多层次的特征，通过组合低级特征构建抽象的高级特征，从而挖掘数据的分布式特征表示，基于此进行复杂的非线性建模，从而实现预测任务。其中 RNN 被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价，交易信息都是序列数据，因此之前有很多研究者，基于 RNN 来预测股票价格。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题，之后出现了诸如 GRU，Peephole LSTM，BiLSTM 等 LSTM 的变体。但传统的股票预测模型有些并未考虑时间因素，有些仅考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。\u003c/p\u003e","title":"基于深度学习的股票价格预测和量化策略研究"}]