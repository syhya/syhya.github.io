<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>基座大语言模型：LLaMA、DeepSeek 等系列（长期更新） | Yue Shui 博客</title>
<meta name=keywords content="AI,NLP,LLM,Pre-training,Post-training,Llama,DeepSeek"><meta name=description content="
注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。
引言
本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 LLaMA 和 DeepSeek 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2025-01-22-llm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-01-22-llm/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2025-01-22-llm/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="基座大语言模型：LLaMA、DeepSeek 等系列（长期更新）"><meta property="og:description" content=" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。
引言 本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 LLaMA 和 DeepSeek 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-22T12:00:00+08:00"><meta property="article:modified_time" content="2025-01-22T12:00:00+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Pre-Training"><meta property="article:tag" content="Post-Training"><meta property="article:tag" content="LLaMA"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="基座大语言模型：LLaMA、DeepSeek 等系列（长期更新）"><meta name=twitter:description content="
注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。
引言
本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 LLaMA 和 DeepSeek 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"基座大语言模型：LLaMA、DeepSeek 等系列（长期更新）","item":"https://syhya.github.io/zh/posts/2025-01-22-llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"基座大语言模型：LLaMA、DeepSeek 等系列（长期更新）","name":"基座大语言模型：LLaMA、DeepSeek 等系列（长期更新）","description":" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。\n引言 本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 LLaMA 和 DeepSeek 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。\n","keywords":["AI","NLP","LLM","Pre-training","Post-training","Llama","DeepSeek"],"articleBody":" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。\n引言 本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 LLaMA 和 DeepSeek 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。\nLLaMA 系列模型 Meta 推出的 LLaMA 系列大语言模型自 2023 年起对开源社区产生了深远影响。每一代模型都在规模、数据、性能及应用范围上有所突破。\nLLaMA1 LLaMA1 (Touvron et al., 2023) 于 2023 年 2 月发布，是 LLaMA 系列的开创性工作，以其优异的性能和开放的研究性质迅速获得关注。\n核心特点:\n参数规模: 提供 7B、13B、30B 和 65B 四个版本。 训练数据: 使用了超过 1.4 万亿 (1.4T) tokens 的公开数据集。 训练资源: 65B 模型在 2048 张 A100 80GB GPU 上训练约 21 天。 性能: LLaMA-65B 在多数基准上性能优于当时的 GPT-3 (175B)。 关键技术:\n架构优化: 采用 Pre-normalization 结合 RMSNorm 替代 LayerNorm，提升训练稳定性与速度。 激活函数: 使用 SwiGLU 替代 ReLU，并调整 FFN 隐藏层维度。 位置编码: 应用 Rotary Embeddings (RoPE) 增强长序列建模能力。 LLaMA2 LLaMA2 (Touvron et al., 2023) 作为第二代模型，在 2023 年中发布，带来了显著的性能提升和更长的上下文处理能力。\n核心特点:\n参数规模: 覆盖 7B、13B、34B、70B 四种规模。 训练数据: 扩展至 2 万亿 (2T) tokens，数据量增加约 40%。 上下文长度: 提升至 4096 tokens，是 LLaMA1 的两倍。 推理效率: 引入 分组查询注意力 (Group Query Attention, GQA) 优化推理速度和显存占用。 关键技术:\nGQA (Grouped Query Attention): 通过查询分组减少注意力计算开销，尤其利好大模型推理。 KV Cache: 推理时利用 KV 缓存加速解码过程。 Code Llama Fig. 1. The Code Llama specialization pipeline. (Source: Rozière et al., 2023)\nCode Llama (Rozière et al., 2023) 是基于 LLaMA2 针对代码任务进行特化训练和微调的版本。\n核心特点:\n参数规模: 提供 7B、13B、34B、70B 版本。 代码数据训练: 7B/13B/34B 基于 500B tokens 代码数据训练，70B 基于 1T tokens 代码数据训练。 长上下文支持: 通过 长上下文微调 (LCFT)，支持处理高达 16k 甚至 100k tokens 的代码序列。 关键技术:\n架构基础: 继承 LLaMA2 架构，针对代码进行优化。 数据构成: 主要使用公开代码库，辅以少量自然语言数据保持通用性。 填充中间 (FIM): 支持在代码中任意位置进行补全，适用于 IDE 场景。 长上下文微调 (LCFT): 使用 16k 序列长度进行额外微调，调整 RoPE 适应长输入。 指令微调: 结合安全指令和自生成单元测试数据，提升指令遵循和安全性。 Llama Guard Fig. 2. Example task instructions for the Llama Guard prompt and response classification tasks. (Source: Inan et al., 2023)\nLlama Guard (Inan et al., 2023) 是为 LLaMA 系列设计的安全过滤模型，用于评估和过滤潜在不安全内容。\n核心特点:\n多版本: Llama Guard 3 1B: 基础文本安全评估。 Llama Guard 3 8B: 增强版，特别关注代码解释器滥用 (S14) 检测。 Llama Guard 3 Vision (Chi et al., 2024): 支持图像和文本的综合多模态安全评估。 Fig. 3. Llama Guard 3 Vision classifies harmful content in the response classification task. (Source: Chi et al., 2024)\n关键技术:\n多模态处理: 使用特殊 \u003c|image|\u003e token 整合图像信息进行安全审查。 安全分类: 基于 ML Commons 的 13 个类别 (S1-S13)(Vidgen et al., 2024))，新增 S14 (代码解释器滥用)。 评估机制: 输入安全类别定义和对话内容，输出“安全/不安全”判定及违规类别。 LLaMA3 Fig. 4. Architecture comparison between Llama 2 and Llama 3. (Source: Umar Jamil)\nLLaMA3 (Grattafiori et al., 2024) 是 LLaMA 系列的第三代，在规模、多语言、多模态能力和效率上均有显著提升。\n核心特点:\n参数规模: 覆盖从 1B 到 405B 的广泛范围 (1B, 3B, 11B, 70B, 90B, 405B)。 训练数据: 达到 15 万亿 (15T) tokens，是 LLaMA2 的 7.5 倍。 Tokenizer: 采用效率更高的 tiktoken，词表从 32k 扩展至 128k。 上下文长度: 大幅提升至 128k tokens。 多语言与多模态: 支持 8 种语言，11B 和 90B 版本具备视觉语言处理能力。 轻量化: 1B 和 3B 版本通过剪枝和蒸馏技术，适配边缘设备。 关键技术:\n全面采用 GQA: 所有规模模型均使用 GQA 优化注意力计算。 先进训练方法: 结合 SFT、拒绝采样 (RS)、直接策略优化 (DPO) 等提升模型能力。 多模态整合: 支持图像、视频、语音的综合处理。 LLaMA 系列模型特性对比 特性 LLaMA1 LLaMA2 Code Llama Llama Guard LLaMA3 发布时间 2023年2月 2023年7月 2023年8月 2023年12月 2024年4月 基础模型 - - LLaMA2 LLaMA2/3 - 模型规模 7B, 13B, 30B, 65B 7B, 13B, 34B, 70B 7B, 13B, 34B, 70B 1B, 8B (+Vision) 1B, 3B, 11B, 70B, 90B, 405B 训练数据量 1.4T+ tokens 2T+ tokens + 500B/1T Code tokens 大约 40k 安全分类数据 15T+ tokens 上下文长度 2048 tokens 4096 tokens 16k-100k tokens 基于 LLaMA2/3 128k tokens Tokenizer SentencePiece (32k) SentencePiece (32k) SentencePiece (32k) 基于 LLaMA2/3 tiktoken (128k) 位置编码 RoPE RoPE RoPE (LCFT adjusted) RoPE RoPE 注意力/推理优化 MHA GQA + KV Cache 基于 LLaMA2 基于 LLaMA2/3 GQA + KV Cache 归一化 RMSNorm RMSNorm 基于 LLaMA2 RMSNorm RMSNorm 激活函数 SwiGLU SwiGLU 基于 LLaMA2 SwiGLU SwiGLU 模型类别 基座模型 基座模型 代码模型 安全分类模型 多模态基座模型 DeepSeek 系列模型 DeepSeek AI 专注于通用人工智能研究，推出了一系列高性能开源模型，尤其在代码和数学领域表现突出，并积极探索 MoE 等高效架构。\nDeepSeek LLM (Base Models) DeepSeek LLM (DeepSeek-AI, 2023) 是该系列的 foundational work，发布于 2023 年底，提供了强大的开源基础模型。\n核心特点:\n参数规模: 提供 7B 和 67B 两种规模的基础 (Base) 和对话 (Chat) 模型。 训练数据: 在 2 万亿 (2T) tokens 的高质量中英文语料上从头训练。 性能: 67B 模型在代码、数学、推理上优于 LLaMA-2 70B；Chat 版本优于 GPT-3.5。 Scaling Laws 研究: 强调高质量数据的重要性，发现高质量数据下扩展模型比扩展数据更有效。 关键技术:\n架构: 类 LLaMA 架构，调整了学习率调度器；67B 模型采用 GQA 提升推理效率。 数据处理: 严格的去重、过滤、重混流程保证数据质量。 对齐: Chat 版本使用 SFT 和 DPO 进行对齐。 DeepSeekMoE DeepSeekMoE (Dai et al., 2024) 是在混合专家 (MoE) 架构上的重要创新，旨在提升模型效率和专家特化程度。\n核心特点:\n核心创新: 细粒度专家分割: 将 FFN 专家进一步拆分，激活更多细粒度专家组合。 共享专家隔离: 部分专家始终激活处理通用知识，降低路由专家冗余。 效率: 在相似计算成本下性能优于传统 MoE，以更少计算量接近稠密模型性能。 关键技术:\n路由机制: Top-K 路由结合细粒度分割和共享专家。 负载均衡: 采用专家级和设备级平衡损失。 参数效率: 通过专家特化提高参数利用率。 DeepSeek-V2 DeepSeek-V2 (DeepSeek-AI, 2024) 是一款强大的开源 MoE 模型，平衡了模型强度、训练成本和推理效率。\n核心特点:\n参数规模: 236B 总参数，每个 token 激活 21B 参数 (稀疏激活)。 上下文长度: 支持高达 128K tokens。 核心架构: 结合 DeepSeekMoE 和创新的 多头潜在注意力 (MLA)。 效率提升: 训练成本比 DeepSeek 67B 降低 42.5%。 KV 缓存大小减少 93.3% (通过 MLA)。 最大生成吞吐量提升 5.76 倍。 性能: 发布时成为最强开源 MoE 之一。 关键技术:\nMLA (Multi-head Latent Attention): 通过低秩 Key-Value 联合压缩，显著降低推理显存占用，提升吞吐量。 DeepSeekMoE 应用: 应用于 FFN 层，实现稀疏计算和专家特化。 训练数据: 在 8.1T tokens 高质量多源语料上训练。 上下文扩展: 使用 YaRN 技术扩展上下文窗口。 DeepSeek-V3 DeepSeek-V3 (DeepSeek-AI, 2024) 是 DeepSeek 最新的旗舰 MoE 模型，性能接近顶尖闭源模型。\n核心特点:\n参数规模: 671B 总参数，每个 token 激活 37B 参数。 核心架构: 沿用 MLA 和 DeepSeekMoE，并引入新创新。 关键创新: 无辅助损失的负载均衡: 通过动态调整专家偏置实现均衡，避免辅助损失影响性能。 多 Token 预测 (MTP): 训练时预测多个未来 token，增加训练信号，提升性能。 训练效率: 采用 FP8 训练和优化框架 (DualPipe)，成本极低。 性能: 在知识、代码、数学、推理、长上下文等基准上达 SOTA 开源水平，可媲美 GPT-4o 等。 关键技术:\n架构: MLA + DeepSeekMoE + 无辅助损失均衡 + MTP。 训练数据: 在 14.8T tokens 高质量、多样化语料上训练，增加数学、编程、多语言比例。 知识蒸馏: 受益于 DeepSeek-R1 系列模型的推理能力蒸馏。 Tokenizer: 扩展并优化词汇表。 DeepSeek-Coder DeepSeek-Coder (Guo et al., 2024) 是专为代码智能设计的系列模型。\n核心特点:\n参数规模: 提供从 1.3B 到 33B 的多个版本。 训练数据: 在 2T tokens 代码密集型数据上训练 (含 87 种编程语言)。 核心技术: 仓库级预训练: 增强跨文件理解能力。 填充中间 (FIM): 提升代码补全能力。 上下文长度: 支持 16K tokens。 性能: 在代码生成、补全、跨文件理解等方面表现 SOTA。 关键技术:\n数据构成: 87% 代码，10% 英文代码相关 NL，3% 中文 NL。 训练目标: Next Token Prediction + FIM。 版本: Base 模型、Instruct 模型、v1.5 (增强 NL 和数学能力)。 DeepSeek-R1 DeepSeek-R1 (DeepSeek-AI, 2024) 是利用强化学习 (RL) 显著增强 LLM 推理能力的第一代模型。\n核心特点:\n核心方法: 广泛使用 RL 直接培养推理能力，减少对 SFT 依赖。 关键模型: DeepSeek-R1-Zero: 证明大规模 RL 可涌现复杂推理能力。 DeepSeek-R1: 多阶段训练 (SFT -\u003e RL -\u003e 拒绝采样 -\u003e RL)。 性能: 在 AIME、MATH、Codeforces 等推理基准上取得 SOTA 性能。 能力蒸馏: 成功将推理能力蒸馏到更小模型。 关键技术:\nRL 激励: 主要依赖基于规则的奖励系统。 训练流程: 精心设计的多阶段流程结合 SFT 和 RL。 涌现能力: RL 驱动模型发展出复杂推理行为 (如自验证、反思)。 DeepSeek 系列模型特性对比 特性 DeepSeek LLM DeepSeek-V2 DeepSeek-V3 DeepSeek-Coder DeepSeek-R1 发布时间 2023年11月 2024年5月 2024年10月 2024年1月 2024年10月 基础模型 - - - - DeepSeek-V3 模型最大规模 67B 236B 671B 33B 671B 激活参数量 67B (Dense) 21B (MoE) 37B (MoE) 1.3B - 33B (Dense) 37B (MoE) 训练数据量 2T tokens 8.1T tokens 14.8T tokens 2T Code tokens SFT 和 RL Data 上下文长度 4K (默认) / 32K (扩展) 128K tokens 128K tokens 16K tokens 128K tokens Tokenizer Custom BPE Custom BPE (Optimized) Custom BPE (Expanded) Custom BPE (Code) Inherited from V3 位置编码 RoPE RoPE RoPE RoPE RoPE 注意力/推理优化 GQA + KV Cache MoE + MLA + KV Cache MoE + MLA + KV Cache GQA + KV Cache MoE + MLA + KV Cache 归一化 RMSNorm RMSNorm RMSNorm RMSNorm RMSNorm 激活函数 SwiGLU/GeGLU SwiGLU/GeGLU SwiGLU/GeGLU SwiGLU/GeGLU SwiGLU/GeGLU 模型类别 基座模型 基座模型 基座模型 代码模型 推理模型 关键技术解析 以下是目前基座大模型所采用的关键技术的详细解析，包括数学公式和相关说明。\nRMS Normalization 在深度学习中，归一化技术在加速训练、提升模型性能和稳定性方面起着至关重要的作用。RMS Normalization (Zhang, et al., 2019) 是一种简化的归一化方法，通过仅计算输入向量的均方根（RMS）进行归一化，从而减少计算开销。其数学表达式如下：\n$$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$其中：\n\\( x \\) 为输入向量。 \\( d \\) 为特征维度的大小。 \\( \\epsilon \\) 为一个极小的常数，用于防止分母为零。 \\( \\gamma \\) 为可学习的缩放参数。 LLaMA3 中选择 RMSNorm 作为其归一化方法，主要基于以下考虑：\n计算效率：RMSNorm 相比 LayerNorm、BatchNorm 和WeightNorm 计算量更低，仅计算输入向量的均方根，适合 LLM 的高效训练。 训练稳定性：RMSNorm 在保持训练稳定性的同时，能够适应更大的学习率，促进模型的快速收敛。 资源优化：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。 简化实现：RMSNorm 的实现相对简单，便于在复杂模型中集成和优化。 关于各种 Norm 的对比和代码实现，可参考博客：Normalization in Deep Learning。\nFFN_SwiGLU Swish-Gated Linear Unit (Shazeer, 2020) 是 LLaMA 中用于增强前馈网络（Feed-Forward Network, FFN）非线性表达能力的关键技术。SwiGLU 结合了 Swish 激活函数和门控机制，显著提升了模型的表现力和性能。此外，与 PaLM (Chowdhery, 2022) 中使用的$4 d$隐藏维度不同，LLaMA 采用了 $\\frac{2}{3}d$ 的隐藏维度，从而在保持参数量和计算量不变的情况下，实现了更高的参数效率。\n$$ \\operatorname{FFN}_{\\mathrm{SwiGLU}}\\left(x, W_1, W_3, W_2\\right)=\\left(\\operatorname{Swish}\\left(x W_1\\right) \\otimes x W_3\\right) W_2 $$ 其中：\n\\( \\text{Swish}(x) = x \\cdot \\sigma(x) \\)（Swish 激活函数）。 \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)（Sigmoid 函数）。 \\( \\otimes \\) 表示逐元素相乘。 \\( W_1, W_2, W_3 \\) 为线性变换矩阵。 优势：\n增强非线性表达：SwiGLU 通过结合 Swish 激活函数与门控机制，能够更有效地捕捉复杂的模式和关系，提升 FFN 层的表达能力。 参数效率：采用 $\\frac{2}{3}d$ 的隐藏维度，在引入额外的线性变换矩阵的同时，保持了总参数量不变，实现了参数的高效利用。 性能提升：在多项基准测试中，FFN_SwiGLU 显著提升了模型的性能，尤其在处理复杂任务和长文本时表现尤为出色。例如，在文本生成和理解任务中，SwiGLU 帮助模型更好地理解上下文和长距离依赖关系。 实现细节：\n权重矩阵调整：为了保持与传统 FFN 层相同的参数量和计算量，SwiGLU 通过减少隐藏层的维度（例如，将隐藏层大小从 4d 调整为 $\\frac{2}{3}d$），在引入额外的线性变换矩阵的同时，确保整体模型的效率不受影响。 兼容性：SwiGLU 作为 GLU 家族的一员，能够无缝集成到现有的 Transformer 架构中，替代传统的 ReLU 或 GELU 激活函数，提升模型的整体性能。 实现代码可以参考这个文件：swiglu.py\nGrouped Query Attention (GQA) Grouped Query Attention (GQA) (Ainslie, 2023) 是 LLaMA3 中用于优化自注意力计算的关键技术。在大规模语言模型的推理过程中，每个注意力头（head）拥有独立的键（Key）和值（Value）参数会导致巨大的内存消耗。Grouped Query Attention (GQA) 旨在通过将多个查询（Query）头分组，并让每组共享一组键值头，从而在模型性能与推理效率之间取得更优的平衡。GQA 是 Multi-Head Attention (MHA) 和 Multi-Query Attention (MQA) 之间的一种折中方案：\nMHA：每个注意力头都有独立的 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)。 MQA：所有注意力头共享一组 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)。 GQA：将 \\(H\\) 个查询头划分为 \\(G\\) 组，每组共享一组 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)（其中 \\(1 \u003c G \u003c H\\)）。 1. 投影 (Projections) 给定输入序列 \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\)，首先通过线性变换投影得到查询、键和值矩阵：\n$$ \\mathbf{Q} = \\mathbf{X} W_Q, \\quad \\mathbf{K} = \\mathbf{X} W_K, \\quad \\mathbf{V} = \\mathbf{X} W_V, $$其中，\\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) 为可学习的投影矩阵。\n2. 头与分组 (Heads and Grouping) 头的切分：将 \\(\\mathbf{Q}\\)、\\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\) 分割成 \\(H\\) 个头，每个头的向量维度为 \\(d_{\\text{head}} = \\frac{d}{H}\\)。 $$ \\mathbf{Q} = [\\mathbf{Q}_1; \\mathbf{Q}_2; \\dots; \\mathbf{Q}_H], \\quad \\mathbf{K} = [\\mathbf{K}_1; \\mathbf{K}_2; \\dots; \\mathbf{K}_H], \\quad \\mathbf{V} = [\\mathbf{V}_1; \\mathbf{V}_2; \\dots; \\mathbf{V}_H] $$ 分组：将这 \\(H\\) 个查询头进一步划分为 \\(G\\) 组（\\(1 \u003c G \u003c H\\)）。对于第 \\(g\\) 组，包含 \\(\\frac{H}{G}\\) 个查询头，并共享一组键值头 \\(\\mathbf{K}^g\\) 和 \\(\\mathbf{V}^g\\)。 $$ \\mathcal{G} = \\left\\{ \\mathcal{G}_1, \\mathcal{G}_2, \\dots, \\mathcal{G}_G \\right\\}, \\quad |\\mathcal{G}_g| = \\frac{H}{G} \\quad \\forall g \\in \\{1, 2, \\dots, G\\} $$下图展示了 GQA 与传统 MHA 和 MQA 的对比，可见在 GQA 中，每组查询头公用一组键值头。\nFig. 5. Overview of Grouped Query Attention (GQA). (Image source: Ainslie et al., 2023)\n3. 组内注意力 (Intra-Group Attention) 对于第 \\(g\\) 组，令该组的查询向量为 \\(\\{\\mathbf{Q}_i\\}_{i \\in \\mathcal{G}_g}\\)，共享的键值向量为 \\(\\mathbf{K}^g\\) 和 \\(\\mathbf{V}^g\\)。组内注意力的计算公式为：\n$$ \\text{Attention}_g(\\mathbf{Q}_i, \\mathbf{K}^g, \\mathbf{V}^g) = \\text{softmax}\\left( \\frac{\\mathbf{Q}_i (\\mathbf{K}^g)^\\top}{\\sqrt{d_{\\text{head}}}} \\right) \\mathbf{V}^g $$其中，\\(\\sqrt{d_{\\text{head}}}\\) 为缩放因子，用于稳定梯度和数值计算。\n4. 拼接输出 (Concatenate \u0026 Output) 将所有组的注意力结果在通道维度上拼接，得到矩阵 \\(\\mathbf{O}\\)，然后通过线性变换矩阵 \\(W_O \\in \\mathbb{R}^{d \\times d}\\) 得到最终输出：\n$$ \\mathbf{O} = \\text{Concat}\\left( \\text{Attention}_1, \\text{Attention}_2, \\dots, \\text{Attention}_G \\right) W_O $$其中，\\(\\text{Concat}\\) 表示在通道维度上的拼接操作。\n更多关于注意力机制在 MHA、MQA 和 GQA 之间的详细对比及代码示例，可参考博客：Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA。\nRotary Positional Embeddings (RoPE) Rotary Positional Embeddings (RoPE) 是LLaMA3中用于表示序列中位置关系的技术，通过对Query和Key向量应用旋转变换，增强了模型对相对位置信息的感知能力。\n优势：\n相对位置感知：RoPE能够自然地捕捉词汇之间的相对位置关系，提升了长距离依赖的建模效果。 计算效率高：无需额外的计算，位置编码与词向量的结合在计算上是高效的，适用于大规模模型。 适应不同长度的序列：RoPE可以灵活处理不同长度的输入序列，不受固定位置编码的限制。 兼容线性注意力：RoPE 可以与线性注意力机制结合，保持注意力计算的线性复杂度，进一步提升处理长序列的效率。 理解单词在序列中的位置关系对于成功训练大型语言模型（LLM）至关重要。循环神经网络（RNN）通过递归计算隐藏状态来自然地捕捉序列中的位置信息。然而，Transformer这类基于自注意力机制的模型由于其并行计算的特性，无法直接感知单词之间的相对位置关系，因此需要额外的位置编码来提供这一信息。\n位置编码的方法主要分为绝对位置编码和相对位置编码两大类。RoPE 则是一种创新性的绝对位置编码方法，旨在结合绝对位置编码和相对位置编码的优点，通过旋转变换实现相对位置感知。\n绝对位置编码 绝对位置编码通过为每个位置生成一个固定或可训练的位置向量，并将其与词向量相加，从而为模型提供位置信息。常见的绝对位置编码方法包括：\n三角函数位置编码：例如，Vaswani 等人（2017）提出的使用正弦和余弦函数生成的位置编码。\n$$ p_i = \\left[\\sin\\left(\\frac{i}{10000^{2j/d}}\\right), \\cos\\left(\\frac{i}{10000^{2j/d}}\\right)\\right]_{j=1}^{d/2} $$ 其中，\\( p_i \\) 是位置 \\( i \\) 的位置编码向量，\\( d \\) 是词向量的维度。\n优点：\n实现简单，与词向量的结合方式直接。 缺点：\n无法自然地捕捉词汇之间的相对位置关系，限制了模型对长距离依赖的建模能力。 对于不同长度的序列，可能需要重新生成位置编码。 可训练位置编码：如BERT和GPT中使用的可训练位置编码。\n相对位置编码 相对位置编码旨在让模型关注词汇之间的相对距离，而不是绝对位置。这样，模型可以更灵活地处理不同长度的序列，并更有效地捕捉长距离依赖关系。\n常见方法：\nGoogle式相对位置编码：在论文《Self-Attention with Relative Position Representations》中，Shaw 等人（2018）提出了一种扩展自注意力机制以考虑相对位置的方法。\n优点：\n自然地捕捉词汇之间的相对位置信息，有助于长距离依赖的建模。 提高模型对序列长度的灵活适应能力。 缺点：\n实现相对复杂，尤其是在自注意力机制中的集成。 计算效率相对较低，特别是在处理长序列时。 旋转位置编码（RoPE）的原理与实现 RoPE 的设计思路 RoPE 的核心理念是通过旋转变换将绝对位置信息转化为相对位置信息，从而结合绝对位置编码和相对位置编码的优点。具体而言，RoPE 通过将查询（Query）和键（Key）向量分别乘以与其位置相关的旋转矩阵，使得内积计算中自然地体现了相对位置关系。\n数学表达式与推导 维情况下的 RoPE 推导 为了更深入理解 RoPE 在二维情况下的工作原理，以下通过数学推导展示其如何实现相对位置编码。\n引言\n在二维空间中，向量的旋转可以通过复数的乘法来简化理解。RoPE 利用这一性质，通过对查询（Query）和键（Key）向量施加旋转变换，实现相对位置编码。\n复数与二维向量的对应关系\n一个复数 \\( z = a + ib \\) 可以表示为二维向量 \\( \\mathbf{v} = [a, b]^T \\)。 复数的乘法对应于二维向量的旋转和缩放。 $$ e^{i\\theta} = \\cos\\theta + i\\sin\\theta $$ 可以将复数旋转表示为二维向量的旋转矩阵。\nRoPE 的基本操作\n假设有二维的查询向量 \\( \\mathbf{q}_m \\) 和键向量 \\( \\mathbf{k}_n \\)，分别位于位置 \\( m \\) 和 \\( n \\)。RoPE 的目标是通过旋转变换，使得它们的内积仅依赖于相对位置 \\( m - n \\)。\n步骤 1：表示为复数\n$$ \\mathbf{q}_m = q_m^{(1)} + i q_m^{(2)} \\\\ \\mathbf{k}_n = k_n^{(1)} + i k_n^{(2)} $$步骤 2：应用旋转变换\n$$ f_q(\\mathbf{q}_m, m) = \\mathbf{q}_m \\cdot e^{im\\theta} = (q_m^{(1)} + i q_m^{(2)}) (\\cos(m\\theta) + i\\sin(m\\theta)) \\\\ f_k(\\mathbf{k}_n, n) = \\mathbf{k}_n \\cdot e^{in\\theta} = (k_n^{(1)} + i k_n^{(2)}) (\\cos(n\\theta) + i\\sin(n\\theta)) $$ 其中，\\( \\theta \\) 是一个预先定义的常数，用于控制旋转的速度。\n步骤 3：计算内积\n$$ \\langle f_q(\\mathbf{q}_m, m), f_k(\\mathbf{k}_n, n) \\rangle = \\text{Re}\\left[ f_q(\\mathbf{q}_m, m) \\cdot \\overline{f_k(\\mathbf{k}_n, n)} \\right] $$ 其中，\\( \\overline{f_k(\\mathbf{k}_n, n)} \\) 是 \\( f_k \\) 的共轭复数。\n$$ \\langle f_q(\\mathbf{q}_m, m), f_k(\\mathbf{k}_n, n) \\rangle = (q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\\cos((m-n)\\theta) + (q_m^{(1)}k_n^{(2)} - q_m^{(2)}k_n^{(1)})\\sin((m-n)\\theta) $$ 这个结果表明，内积的计算中自然地引入了相对位置 \\( m - n \\) 的影响。\n步骤 4：将二维向量与旋转矩阵对应\n将复数形式转化为矩阵形式：\n$$ f_q(\\mathbf{q}_m, m) = R(m\\theta) \\mathbf{q}_m \\\\ f_k(\\mathbf{k}_n, n) = R(n\\theta) \\mathbf{k}_n $$$$ R(\\phi) = \\begin{bmatrix} \\cos\\phi \u0026 -\\sin\\phi \\\\ \\sin\\phi \u0026 \\cos\\phi \\end{bmatrix} $$因此，内积可以表示为：\n$$ \\langle f_q(\\mathbf{q}_m, m), f_k(\\mathbf{k}_n, n) \\rangle = [q_m^{(1)}, q_m^{(2)}] R((n - m)\\theta) \\mathbf{k}_n $$进一步展开： $$ = q_m^{(1)} k_n^{(1)} \\cos((m - n)\\theta) + q_m^{(2)} k_n^{(2)} \\cos((m - n)\\theta) \\\nq_m^{(1)} k_n^{(2)} \\sin((m - n)\\theta) - q_m^{(2)} k_n^{(1)} \\sin((m - n)\\theta) $$ 与前述结果一致，验证了 RoPE 在二维情况下实现相对位置编码的有效性。 总结二维 RoPE 的实现步骤：\n表示向量为复数：将二维的查询和键向量表示为复数形式。 应用旋转变换：根据各自的位置 \\( m \\) 和 \\( n \\)，分别将查询和键向量旋转 \\( m\\theta \\) 和 \\( n\\theta \\) 的角度。 计算内积的实部：通过复数的乘法和内积运算，确保内积结果仅依赖于相对位置 \\( m - n \\)。 利用旋转矩阵：通过旋转矩阵的性质，将复数形式转化为矩阵形式，使得旋转操作更加直观和易于扩展到高维情况。 3.2.2 高维情况下的 RoPE 对于词向量维度 \\( d \\) 为偶数的情况，RoPE 通过将向量拆分为 \\( d/2 \\) 个二维子向量，并对每个子向量应用独立的旋转矩阵，从而实现高维度的旋转位置编码。\n$$ f_{\\{q, k\\}}(x_m, m) = R_{\\Theta, m}^d \\cdot W_{\\{q, k\\}} \\cdot x_m $$ 其中，\\( R_{\\Theta, m}^d \\) 是一个块对角矩阵，由 \\( d/2 \\) 个二维旋转矩阵组成，每个旋转矩阵对应一个不同的角度 \\( \\theta_i = 10000^{-2(i-1)/d} \\)。\n$$ R_{\\Theta, m}^d = \\begin{bmatrix} \\cos(m\\theta_1) \u0026 -\\sin(m\\theta_1) \u0026 \u0026 \u0026 \\\\ \\sin(m\\theta_1) \u0026 \\cos(m\\theta_1) \u0026 \u0026 \u0026 \\\\ \u0026 \u0026 \\cos(m\\theta_2) \u0026 -\\sin(m\\theta_2) \u0026 \\\\ \u0026 \u0026 \\sin(m\\theta_2) \u0026 \\cos(m\\theta_2) \u0026 \\\\ \u0026 \u0026 \u0026 \u0026 \\ddots \\\\ \\end{bmatrix} $$$$ q_m^\\top k_n = (R_{\\Theta, m}^d W_q x_m)^\\top (R_{\\Theta, n}^d W_k x_n) = x_m^\\top W_q^\\top R_{\\Theta, m}^{d\\top} R_{\\Theta, n}^d W_k x_n = x_m^\\top W_q^\\top R_{\\Theta, n-m}^d W_k x_n $$ 其中，\\( R_{\\Theta, n-m}^d = R_{\\Theta, m}^{d\\top} R_{\\Theta, n}^d \\)，体现了相对位置信息。\n3.3 RoPE 的性质 3.3.1 远程衰减特性\n通过选择 \\( \\theta_i = 10000^{-2(i-1)/d} \\)，RoPE 的内积计算结果随着相对位置 \\( |m - n| \\) 增大而衰减。这一特性符合自然语言中的直觉，即距离较远的单词对当前词的影响应当较小。\n3.3.2 兼容线性注意力机制\nRoPE 的旋转操作保持了向量的范数不变，使其可以与线性注意力机制（如 Performer）无缝结合，进一步提升模型在处理长序列时的计算效率。\n4. 总结 RoPE 通过旋转变换将绝对位置信息转化为相对位置信息，结合了绝对位置编码和相对位置编码的优点。其核心机制确保了模型对相对位置关系的感知能力，并且保持了高效的计算性能，特别是在处理长序列任务时表现出色。通过详细的数学推导，特别是在二维情况下的实现，RoPE 展示了其在自注意力机制中引入相对位置编码的有效性和灵活性。\nBPE tiktoken tokenizer是LLaMA3采用的新一代分词器，相较于LLaMA2使用的SentencePiece BPE，tiktoken在以下方面有所改进：\n词汇表扩展：词汇表从32k扩展至128k，覆盖更多语言和专业术语，减少了分词次数，提升了生成质量。 编码效率：优化了编码算法，减少了分词时间，提高了处理速度。 生成质量：通过更细粒度的词汇表示，提升了模型生成文本的连贯性和准确性。 $$ \\text{Tokenize}(w) = \\text{BPE}(w) \\quad \\text{vs} \\quad \\text{Tokenize}(w) = \\text{tiktoken\\_BPE}(w) $$ 其中，\\( w \\) 为输入词汇，tiktoken_BPE通过更大词汇表减少了分词次数。 优势：\n减少分词次数：更大的词汇表使得更多词汇能作为单一token处理，减少了分词次数，提高了生成效率和质量。 提升生成质量：更细粒度的词汇表示，使模型在生成文本时能够更准确地表达复杂语义。 编码速度快：优化的编码算法提升了分词速度，适用于大规模模型的高效训练和推理。 轻量级模型 为了适应边缘设备和移动设备的需求，LLaMA3推出了1B和3B参数量的轻量级模型，采用以下技术：\n剪枝技术：通过系统性地移除网络中的冗余参数，减小模型规模，同时保持核心性能。 知识蒸馏：让小模型从大模型中学习，提升其在特定任务上的表现。 优化部署：针对移动设备的硬件架构进行优化，如针对Arm处理器的性能调优，确保模型在资源受限环境中的高效运行。 $$ \\text{Pruned\\_Model} = \\text{Prune}(\\text{Original\\_Model}, \\text{Pruning\\_Rate}) $$$$ \\text{Distilled\\_Model} = \\text{Distill}(\\text{Large\\_Model}, \\text{Small\\_Model}) $$ 其中，Prune表示剪枝操作，Distill表示知识蒸馏过程。 优势：\n适应资源受限设备：减小模型规模，使其适用于边缘设备和移动设备，推动了大语言模型的普及。 保持性能：通过剪枝和知识蒸馏技术，保持了模型的核心性能和表现。 高效运行：优化的模型结构和权重格式（如BFloat16）提升了计算效率，确保在移动设备上的高效运行。 训练方法 LLaMA3在训练数据和方法上进行了全面升级，采用了更大规模的数据和更先进的训练技术：\n预训练阶段：\n大规模数据扩展：训练数据量达到15万亿token，覆盖更多语言、专业领域和多模态数据，提升了模型的泛化能力和多语言支持。 扩展法则（Scaling Laws）： 根据Chinchilla扩展法则，优化模型的训练数据量和参数规模平衡，确保模型在关键任务上的最佳性能。 数学表达式： $$ \\text{Optimal Data} \\propto \\text{Model Size}^{4/3} $$ 这一公式指导了数据和模型规模的平衡，确保随着模型规模的增加，训练数据量也按比例增长，避免模型过拟合或欠拟合。 并行训练策略：\n数据并行：将训练数据分布到多个GPU上，提升数据处理速度。 模型并行：将模型的不同部分分布到多个GPU上，支持更大规模的模型训练。 流水并行：分阶段处理模型的不同部分，提高训练效率。 $$ \\text{Total Throughput} = \\text{Data Parallelism} \\times \\text{Model Parallelism} \\times \\text{Pipeline Parallelism} $$ 其中，总吞吐量（Total Throughput）是数据并行、模型并行和流水并行的乘积，显著提升了训练效率。 硬件优化：\n高效利用GPU：在16K GPU上实现每GPU超过400 TFLOPS的计算利用率，通过定制的24K GPU集群进行训练，确保训练过程的高效性和稳定性。 错误处理与存储优化： 自动错误检测与处理：确保训练过程的连续性和高效性。 可扩展存储系统：减少检查点和回滚的开销，提高数据存储效率。 微调阶段：\n多轮对齐步骤： 监督微调（SFT）：使用高质量的标注数据进一步优化模型性能。 拒绝采样（Rejection Sampling）：通过拒绝低质量内容，提升生成文本的质量。 近端策略优化（Proximal Policy Optimization, PPO）和直接策略优化（Direct Policy Optimization, DPO）：结合两者的优势，优化模型的生成策略，使其更符合人类偏好。 $$ \\mathcal{L}_{\\text{RLHF}} = \\mathbb{E}_{\\theta \\sim \\pi_{\\theta}} \\left[ r(s, a) \\right] $$ 其中，\\( \\mathcal{L}_{\\text{RLHF}} \\)为RLHF的损失函数，\\( \\pi_{\\theta} \\)为策略分布，\\( r(s, a) \\)为奖励函数。 多模态训练：\n视觉语言模型：结合图像和文本数据，提升模型在多模态任务中的表现。 代码数据扩展：增加代码token数量，提升模型在编程任务中的表现。 模型安全与质量控制：\n数据过滤pipeline： 启发式过滤器：基于规则的过滤，提高数据质量。 NSFW过滤器：去除不适内容，确保数据的安全性。 语义重复数据删除：使用语义分析技术，删除内容高度相似的数据。 文本分类器：预测数据质量，进一步优化数据集。 优化训练堆栈：\n高级训练堆栈：自动检测和处理训练过程中的错误，提升硬件可靠性。 性能调优：针对不同硬件平台进行优化，确保训练过程的高效性。 LLaMA3通过这些先进的训练方法和优化策略，显著提升了模型的性能和适应性，成为开源大语言模型领域的领先者。\n总结 参考资料 Hendrycks and Gimpel, 2016 GLU Variants Improve Transformer LLaMA: Open and Efficient Foundation Language Models LLaMA2: Open Foundation and Fine-Tuned Chat Models meta-llama repo ","wordCount":"9854","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2025-01-22T12:00:00+08:00","dateModified":"2025-01-22T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2025-01-22-llm/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">基座大语言模型：LLaMA、DeepSeek 等系列（长期更新）</h1><div class=post-meta><span title='2025-01-22 12:00:00 +0800 +0800'>2025-01-22</span>&nbsp;·&nbsp;20 分钟&nbsp;·&nbsp;9854 字&nbsp;·&nbsp;Yue Shui</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#引言>引言</a></li><li><a href=#llama-系列模型>LLaMA 系列模型</a><ul><li><a href=#llama1>LLaMA1</a></li><li><a href=#llama2>LLaMA2</a></li><li><a href=#code-llama>Code Llama</a></li><li><a href=#llama-guard>Llama Guard</a></li><li><a href=#llama3>LLaMA3</a></li><li><a href=#llama-系列模型特性对比>LLaMA 系列模型特性对比</a></li></ul></li><li><a href=#deepseek-系列模型>DeepSeek 系列模型</a><ul><li><a href=#deepseek-llm-base-models>DeepSeek LLM (Base Models)</a></li><li><a href=#deepseekmoe>DeepSeekMoE</a></li><li><a href=#deepseek-v2>DeepSeek-V2</a></li><li><a href=#deepseek-v3>DeepSeek-V3</a></li><li><a href=#deepseek-coder>DeepSeek-Coder</a></li><li><a href=#deepseek-r1>DeepSeek-R1</a></li><li><a href=#deepseek-系列模型特性对比>DeepSeek 系列模型特性对比</a></li></ul></li><li><a href=#关键技术解析>关键技术解析</a><ul><li><a href=#rms-normalization>RMS Normalization</a></li><li><a href=#ffn_swiglu>FFN_SwiGLU</a></li><li><a href=#grouped-query-attention-gqa>Grouped Query Attention (GQA)</a><ul><li><a href=#1-投影-projections>1. 投影 (Projections)</a></li><li><a href=#2-头与分组-heads-and-grouping>2. 头与分组 (Heads and Grouping)</a></li><li><a href=#3-组内注意力-intra-group-attention>3. 组内注意力 (Intra-Group Attention)</a></li><li><a href=#4-拼接输出-concatenate--output>4. 拼接输出 (Concatenate & Output)</a></li><li><a href=#rotary-positional-embeddings-rope>Rotary Positional Embeddings (RoPE)</a></li><li><a href=#绝对位置编码>绝对位置编码</a></li><li><a href=#相对位置编码>相对位置编码</a></li></ul></li><li><a href=#旋转位置编码rope的原理与实现>旋转位置编码（RoPE）的原理与实现</a><ul><li><a href=#rope-的设计思路>RoPE 的设计思路</a></li><li><a href=#数学表达式与推导>数学表达式与推导</a></li><li><a href=#33-rope-的性质>3.3 RoPE 的性质</a></li></ul></li><li><a href=#4-总结>4. 总结</a></li><li><a href=#bpe>BPE</a><ul><li><a href=#轻量级模型>轻量级模型</a></li><li><a href=#训练方法>训练方法</a></li></ul></li></ul></li><li><a href=#总结>总结</a></li><li><a href=#参考资料>参考资料</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p><strong>注意</strong>: 本文<strong>正在更新中</strong>，内容只是<strong>草稿版本</strong>，并不完善，后续会有变动。请随时关注最新版本。</p></blockquote><h2 id=引言>引言<a hidden class=anchor aria-hidden=true href=#引言>#</a></h2><p>本篇文章将系统梳理市面上开放权重的多种大语言模型，重点关注 <strong>LLaMA</strong> 和 <strong>DeepSeek</strong> 系列。我们将深入解析其模型架构、训练数据和训练方法，并通过表格揭示各版本间的核心差异与演进。</p><h2 id=llama-系列模型>LLaMA 系列模型<a hidden class=anchor aria-hidden=true href=#llama-系列模型>#</a></h2><p>Meta 推出的 <strong>LLaMA</strong> 系列大语言模型自 2023 年起对开源社区产生了深远影响。每一代模型都在规模、数据、性能及应用范围上有所突破。</p><h3 id=llama1>LLaMA1<a hidden class=anchor aria-hidden=true href=#llama1>#</a></h3><p><strong>LLaMA1</strong> (<a href=https://arxiv.org/abs/2302.13971>Touvron et al., 2023</a>) 于 2023 年 2 月发布，是 LLaMA 系列的开创性工作，以其优异的性能和开放的研究性质迅速获得关注。</p><p><strong>核心特点:</strong></p><ul><li><strong>参数规模</strong>: 提供 <strong>7B、13B、30B 和 65B</strong> 四个版本。</li><li><strong>训练数据</strong>: 使用了超过 <strong>1.4 万亿 (1.4T) tokens</strong> 的公开数据集。</li><li><strong>训练资源</strong>: 65B 模型在 2048 张 A100 80GB GPU 上训练约 21 天。</li><li><strong>性能</strong>: <strong>LLaMA-65B</strong> 在多数基准上性能优于当时的 <strong>GPT-3 (175B)</strong>。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>架构优化</strong>: 采用 <strong>Pre-normalization</strong> 结合 <strong>RMSNorm</strong> 替代 LayerNorm，提升训练稳定性与速度。</li><li><strong>激活函数</strong>: 使用 <strong>SwiGLU</strong> 替代 ReLU，并调整 FFN 隐藏层维度。</li><li><strong>位置编码</strong>: 应用 <strong>Rotary Embeddings (RoPE)</strong> 增强长序列建模能力。</li></ul><h3 id=llama2>LLaMA2<a hidden class=anchor aria-hidden=true href=#llama2>#</a></h3><p><strong>LLaMA2</strong> (<a href=https://arxiv.org/abs/2307.09288>Touvron et al., 2023</a>) 作为第二代模型，在 2023 年中发布，带来了显著的性能提升和更长的上下文处理能力。</p><p><strong>核心特点:</strong></p><ul><li><strong>参数规模</strong>: 覆盖 <strong>7B、13B、34B、70B</strong> 四种规模。</li><li><strong>训练数据</strong>: 扩展至 <strong>2 万亿 (2T) tokens</strong>，数据量增加约 40%。</li><li><strong>上下文长度</strong>: 提升至 <strong>4096 tokens</strong>，是 LLaMA1 的两倍。</li><li><strong>推理效率</strong>: 引入 <strong>分组查询注意力 (Group Query Attention, GQA)</strong> 优化推理速度和显存占用。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>GQA (Grouped Query Attention)</strong>: 通过查询分组减少注意力计算开销，尤其利好大模型推理。</li><li><strong>KV Cache</strong>: 推理时利用 KV 缓存加速解码过程。</li></ul><h3 id=code-llama>Code Llama<a hidden class=anchor aria-hidden=true href=#code-llama>#</a></h3><figure class=align-center><img loading=lazy src=codellama.png#center alt="Fig. 1. The Code Llama specialization pipeline. (Source: Rozière et al., 2023)" width=100%><figcaption><p>Fig. 1. The Code Llama specialization pipeline. (Source: <a href=https://arxiv.org/abs/2308.12950>Rozière et al., 2023</a>)</p></figcaption></figure><p><strong>Code Llama</strong> (<a href=https://arxiv.org/abs/2308.12950>Rozière et al., 2023</a>) 是基于 LLaMA2 针对<strong>代码任务</strong>进行特化训练和微调的版本。</p><p><strong>核心特点:</strong></p><ol><li><strong>参数规模</strong>: 提供 <strong>7B、13B、34B、70B</strong> 版本。</li><li><strong>代码数据训练</strong>: 7B/13B/34B 基于 <strong>500B tokens</strong> 代码数据训练，70B 基于 <strong>1T tokens</strong> 代码数据训练。</li><li><strong>长上下文支持</strong>: 通过 <strong>长上下文微调 (LCFT)</strong>，支持处理高达 <strong>16k 甚至 100k tokens</strong> 的代码序列。</li></ol><p><strong>关键技术:</strong></p><ul><li><strong>架构基础</strong>: 继承 LLaMA2 架构，针对代码进行优化。</li><li><strong>数据构成</strong>: 主要使用公开代码库，辅以少量自然语言数据保持通用性。</li><li><strong>填充中间 (FIM)</strong>: 支持在代码中任意位置进行补全，适用于 IDE 场景。</li><li><strong>长上下文微调 (LCFT)</strong>: 使用 16k 序列长度进行额外微调，调整 RoPE 适应长输入。</li><li><strong>指令微调</strong>: 结合安全指令和自生成单元测试数据，提升指令遵循和安全性。</li></ul><h3 id=llama-guard>Llama Guard<a hidden class=anchor aria-hidden=true href=#llama-guard>#</a></h3><figure class=align-center><img loading=lazy src=llama_guard.png#center alt="Fig. 2. Example task instructions for the Llama Guard prompt and response classification tasks. (Source: Inan et al., 2023)" width=100%><figcaption><p>Fig. 2. Example task instructions for the Llama Guard prompt and response classification tasks. (Source: <a href=https://arxiv.org/abs/2312.06674>Inan et al., 2023</a>)</p></figcaption></figure><p><strong>Llama Guard</strong> (<a href=https://arxiv.org/abs/2312.06674>Inan et al., 2023</a>) 是为 LLaMA 系列设计的<strong>安全过滤模型</strong>，用于评估和过滤潜在不安全内容。</p><p><strong>核心特点:</strong></p><ul><li><strong>多版本</strong>:<ul><li><strong>Llama Guard 3 1B</strong>: 基础文本安全评估。</li><li><strong>Llama Guard 3 8B</strong>: 增强版，特别关注代码解释器滥用 (S14) 检测。</li><li><strong>Llama Guard 3 Vision</strong> (<a href=https://arxiv.org/abs/2411.10414>Chi et al., 2024</a>): 支持<strong>图像和文本</strong>的综合多模态安全评估。</li></ul></li></ul><figure class=align-center><img loading=lazy src=llama_guard_vision.png#center alt="Fig. 3. Llama Guard 3 Vision classifies harmful content in the response classification task. (Source: Chi et al., 2024)" width=100%><figcaption><p>Fig. 3. Llama Guard 3 Vision classifies harmful content in the response classification task. (Source: <a href=https://arxiv.org/abs/2411.10414>Chi et al., 2024</a>)</p></figcaption></figure><p><strong>关键技术:</strong></p><ul><li><strong>多模态处理</strong>: 使用特殊 <code>&lt;|image|></code> token 整合图像信息进行安全审查。</li></ul><p><img alt="alt text" loading=lazy src=/zh/posts/2025-01-22-llm/image.png></p><ul><li><strong>安全分类</strong>: 基于 ML Commons 的 13 个类别 (S1-S13)(<a href=https://arxiv.org/abs/2404.12241>Vidgen et al., 2024)</a>)，新增 S14 (代码解释器滥用)。</li><li><strong>评估机制</strong>: 输入安全类别定义和对话内容，输出“安全/不安全”判定及违规类别。</li></ul><h3 id=llama3>LLaMA3<a hidden class=anchor aria-hidden=true href=#llama3>#</a></h3><figure class=align-center><img loading=lazy src=llama3_architecture.png#center alt="Fig. 4. Architecture comparison between Llama 2 and Llama 3. (Source: Umar Jamil)" width=80%><figcaption><p>Fig. 4. Architecture comparison between Llama 2 and Llama 3. (Source: <a href=https://github.com/hkproj/pytorch-llama/blob/main/Slides.pdf>Umar Jamil</a>)</p></figcaption></figure><p><strong>LLaMA3</strong> (<a href=https://arxiv.org/abs/2407.21783>Grattafiori et al., 2024</a>) 是 LLaMA 系列的第三代，在规模、多语言、多模态能力和效率上均有显著提升。</p><p><strong>核心特点:</strong></p><ul><li><strong>参数规模</strong>: 覆盖从 <strong>1B 到 405B</strong> 的广泛范围 (1B, 3B, 11B, 70B, 90B, 405B)。</li><li><strong>训练数据</strong>: 达到 <strong>15 万亿 (15T) tokens</strong>，是 LLaMA2 的 7.5 倍。</li><li><strong>Tokenizer</strong>: 采用效率更高的 <code>tiktoken</code>，词表从 32k 扩展至 <strong>128k</strong>。</li><li><strong>上下文长度</strong>: 大幅提升至 <strong>128k tokens</strong>。</li><li><strong>多语言与多模态</strong>: 支持 8 种语言，11B 和 90B 版本具备<strong>视觉语言处理能力</strong>。</li><li><strong>轻量化</strong>: 1B 和 3B 版本通过剪枝和蒸馏技术，<strong>适配边缘设备</strong>。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>全面采用 GQA</strong>: 所有规模模型均使用 GQA 优化注意力计算。</li><li><strong>先进训练方法</strong>: 结合 <strong>SFT、拒绝采样 (RS)、直接策略优化 (DPO)</strong> 等提升模型能力。</li><li><strong>多模态整合</strong>: 支持图像、视频、语音的综合处理。</li></ul><h3 id=llama-系列模型特性对比>LLaMA 系列模型特性对比<a hidden class=anchor aria-hidden=true href=#llama-系列模型特性对比>#</a></h3><table><thead><tr><th>特性</th><th>LLaMA1</th><th>LLaMA2</th><th>Code Llama</th><th>Llama Guard</th><th>LLaMA3</th></tr></thead><tbody><tr><td><strong>发布时间</strong></td><td>2023年2月</td><td>2023年7月</td><td>2023年8月</td><td>2023年12月</td><td>2024年4月</td></tr><tr><td><strong>基础模型</strong></td><td>-</td><td>-</td><td>LLaMA2</td><td>LLaMA2/3</td><td>-</td></tr><tr><td><strong>模型规模</strong></td><td>7B, 13B, 30B, 65B</td><td>7B, 13B, 34B, 70B</td><td>7B, 13B, 34B, 70B</td><td>1B, 8B (+Vision)</td><td>1B, 3B, 11B, 70B, 90B, 405B</td></tr><tr><td><strong>训练数据量</strong></td><td>1.4T+ tokens</td><td>2T+ tokens</td><td>+ 500B/1T Code tokens</td><td>大约 40k 安全分类数据</td><td><strong>15T+ tokens</strong></td></tr><tr><td><strong>上下文长度</strong></td><td>2048 tokens</td><td>4096 tokens</td><td><strong>16k-100k tokens</strong></td><td>基于 LLaMA2/3</td><td><strong>128k tokens</strong></td></tr><tr><td><strong>Tokenizer</strong></td><td>SentencePiece (32k)</td><td>SentencePiece (32k)</td><td>SentencePiece (32k)</td><td>基于 LLaMA2/3</td><td><strong>tiktoken (128k)</strong></td></tr><tr><td><strong>位置编码</strong></td><td>RoPE</td><td>RoPE</td><td>RoPE (LCFT adjusted)</td><td>RoPE</td><td>RoPE</td></tr><tr><td><strong>注意力/推理优化</strong></td><td>MHA</td><td><strong>GQA</strong> + KV Cache</td><td>基于 LLaMA2</td><td>基于 LLaMA2/3</td><td><strong>GQA</strong> + KV Cache</td></tr><tr><td><strong>归一化</strong></td><td>RMSNorm</td><td>RMSNorm</td><td>基于 LLaMA2</td><td>RMSNorm</td><td>RMSNorm</td></tr><tr><td><strong>激活函数</strong></td><td>SwiGLU</td><td>SwiGLU</td><td>基于 LLaMA2</td><td>SwiGLU</td><td>SwiGLU</td></tr><tr><td><strong>模型类别</strong></td><td>基座模型</td><td>基座模型</td><td>代码模型</td><td>安全分类模型</td><td>多模态基座模型</td></tr></tbody></table><h2 id=deepseek-系列模型>DeepSeek 系列模型<a hidden class=anchor aria-hidden=true href=#deepseek-系列模型>#</a></h2><p><strong>DeepSeek AI</strong> 专注于通用人工智能研究，推出了一系列高性能开源模型，尤其在代码和数学领域表现突出，并积极探索 MoE 等高效架构。</p><hr><h3 id=deepseek-llm-base-models>DeepSeek LLM (Base Models)<a hidden class=anchor aria-hidden=true href=#deepseek-llm-base-models>#</a></h3><p><strong>DeepSeek LLM</strong> (<a href=https://github.com/deepseek-ai/deepseek-llm>DeepSeek-AI, 2023</a>) 是该系列的 foundational work，发布于 2023 年底，提供了强大的开源基础模型。</p><p><strong>核心特点:</strong></p><ul><li><strong>参数规模</strong>: 提供 <strong>7B</strong> 和 <strong>67B</strong> 两种规模的基础 (Base) 和对话 (Chat) 模型。</li><li><strong>训练数据</strong>: 在 <strong>2 万亿 (2T) tokens</strong> 的高质量中英文语料上从头训练。</li><li><strong>性能</strong>: <strong>67B 模型</strong>在代码、数学、推理上优于 LLaMA-2 70B；Chat 版本优于 GPT-3.5。</li><li><strong>Scaling Laws 研究</strong>: 强调高质量数据的重要性，发现高质量数据下扩展模型比扩展数据更有效。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>架构</strong>: 类 LLaMA 架构，调整了学习率调度器；67B 模型采用 <strong>GQA</strong> 提升推理效率。</li><li><strong>数据处理</strong>: 严格的去重、过滤、重混流程保证数据质量。</li><li><strong>对齐</strong>: Chat 版本使用 <strong>SFT</strong> 和 <strong>DPO</strong> 进行对齐。</li></ul><hr><h3 id=deepseekmoe>DeepSeekMoE<a hidden class=anchor aria-hidden=true href=#deepseekmoe>#</a></h3><p><strong>DeepSeekMoE</strong> (<a href=https://arxiv.org/abs/2401.06066>Dai et al., 2024</a>) 是在<strong>混合专家 (MoE)</strong> 架构上的重要创新，旨在提升模型效率和专家特化程度。</p><p><strong>核心特点:</strong></p><ul><li><strong>核心创新</strong>:<ol><li><strong>细粒度专家分割</strong>: 将 FFN 专家进一步拆分，激活更多细粒度专家组合。</li><li><strong>共享专家隔离</strong>: 部分专家始终激活处理通用知识，降低路由专家冗余。</li></ol></li><li><strong>效率</strong>: 在相似计算成本下性能优于传统 MoE，以更少计算量接近稠密模型性能。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>路由机制</strong>: Top-K 路由结合细粒度分割和共享专家。</li><li><strong>负载均衡</strong>: 采用专家级和设备级平衡损失。</li><li><strong>参数效率</strong>: 通过专家特化提高参数利用率。</li></ul><hr><h3 id=deepseek-v2>DeepSeek-V2<a hidden class=anchor aria-hidden=true href=#deepseek-v2>#</a></h3><p><strong>DeepSeek-V2</strong> (<a href=https://github.com/deepseek-ai/DeepSeek-V2>DeepSeek-AI, 2024</a>) 是一款强大的开源 <strong>MoE</strong> 模型，平衡了模型强度、训练成本和推理效率。</p><p><img alt="alt text" loading=lazy src=/zh/posts/2025-01-22-llm/image-2.png></p><p><strong>核心特点:</strong></p><ul><li><strong>参数规模</strong>: <strong>236B</strong> 总参数，每个 token 激活 <strong>21B</strong> 参数 (稀疏激活)。</li><li><strong>上下文长度</strong>: 支持高达 <strong>128K tokens</strong>。</li><li><strong>核心架构</strong>: 结合 <strong>DeepSeekMoE</strong> 和创新的 <strong>多头潜在注意力 (MLA)</strong>。</li><li><strong>效率提升</strong>:<ul><li>训练成本比 DeepSeek 67B 降低 42.5%。</li><li><strong>KV 缓存大小减少 93.3%</strong> (通过 MLA)。</li><li>最大生成吞吐量提升 5.76 倍。</li></ul></li><li><strong>性能</strong>: 发布时成为最强开源 MoE 之一。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>MLA (Multi-head Latent Attention)</strong>: 通过低秩 Key-Value 联合压缩，<strong>显著降低推理显存占用</strong>，提升吞吐量。</li><li><strong>DeepSeekMoE 应用</strong>: 应用于 FFN 层，实现稀疏计算和专家特化。</li><li><strong>训练数据</strong>: 在 <strong>8.1T tokens</strong> 高质量多源语料上训练。</li><li><strong>上下文扩展</strong>: 使用 <strong>YaRN</strong> 技术扩展上下文窗口。</li></ul><h3 id=deepseek-v3>DeepSeek-V3<a hidden class=anchor aria-hidden=true href=#deepseek-v3>#</a></h3><p><strong>DeepSeek-V3</strong> (<a href=https://github.com/deepseek-ai/DeepSeek-V3>DeepSeek-AI, 2024</a>) 是 DeepSeek 最新的旗舰 <strong>MoE</strong> 模型，性能接近顶尖闭源模型。</p><p><img alt="alt text" loading=lazy src=/zh/posts/2025-01-22-llm/image-1.png></p><p><strong>核心特点:</strong></p><ul><li><strong>参数规模</strong>: <strong>671B</strong> 总参数，每个 token 激活 <strong>37B</strong> 参数。</li><li><strong>核心架构</strong>: 沿用 <strong>MLA</strong> 和 <strong>DeepSeekMoE</strong>，并引入新创新。</li><li><strong>关键创新</strong>:<ol><li><strong>无辅助损失的负载均衡</strong>: 通过动态调整专家偏置实现均衡，避免辅助损失影响性能。</li><li><strong>多 Token 预测 (MTP)</strong>: 训练时预测多个未来 token，增加训练信号，提升性能。</li></ol></li><li><strong>训练效率</strong>: 采用 <strong>FP8</strong> 训练和优化框架 (DualPipe)，成本极低。</li><li><strong>性能</strong>: 在知识、代码、数学、推理、长上下文等基准上达 <strong>SOTA 开源水平</strong>，可媲美 GPT-4o 等。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>架构</strong>: MLA + DeepSeekMoE + 无辅助损失均衡 + MTP。</li><li><strong>训练数据</strong>: 在 <strong>14.8T tokens</strong> 高质量、多样化语料上训练，增加数学、编程、多语言比例。</li><li><strong>知识蒸馏</strong>: 受益于 <strong>DeepSeek-R1</strong> 系列模型的推理能力蒸馏。</li><li><strong>Tokenizer</strong>: 扩展并优化词汇表。</li></ul><h3 id=deepseek-coder>DeepSeek-Coder<a hidden class=anchor aria-hidden=true href=#deepseek-coder>#</a></h3><p><strong>DeepSeek-Coder</strong> (<a href=https://arxiv.org/abs/2401.14196>Guo et al., 2024</a>) 是专为<strong>代码智能</strong>设计的系列模型。</p><p><strong>核心特点:</strong></p><ul><li><strong>参数规模</strong>: 提供从 <strong>1.3B 到 33B</strong> 的多个版本。</li><li><strong>训练数据</strong>: 在 <strong>2T tokens</strong> 代码密集型数据上训练 (含 87 种编程语言)。</li><li><strong>核心技术</strong>:<ol><li><strong>仓库级预训练</strong>: 增强跨文件理解能力。</li><li><strong>填充中间 (FIM)</strong>: 提升代码补全能力。</li></ol></li><li><strong>上下文长度</strong>: 支持 <strong>16K tokens</strong>。</li><li><strong>性能</strong>: 在代码生成、补全、跨文件理解等方面表现 <strong>SOTA</strong>。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>数据构成</strong>: 87% 代码，10% 英文代码相关 NL，3% 中文 NL。</li><li><strong>训练目标</strong>: Next Token Prediction + FIM。</li><li><strong>版本</strong>: Base 模型、Instruct 模型、v1.5 (增强 NL 和数学能力)。</li></ul><h3 id=deepseek-r1>DeepSeek-R1<a hidden class=anchor aria-hidden=true href=#deepseek-r1>#</a></h3><p><strong>DeepSeek-R1</strong> (<a href=https://github.com/deepseek-ai/DeepSeek-R1>DeepSeek-AI, 2024</a>) 是利用<strong>强化学习 (RL)</strong> 显著增强 LLM <strong>推理能力</strong>的第一代模型。</p><p><strong>核心特点:</strong></p><ul><li><strong>核心方法</strong>: 广泛使用 <strong>RL</strong> 直接培养推理能力，减少对 SFT 依赖。</li><li><strong>关键模型</strong>:<ul><li><strong>DeepSeek-R1-Zero</strong>: 证明大规模 RL 可涌现复杂推理能力。</li><li><strong>DeepSeek-R1</strong>: 多阶段训练 (SFT -> RL -> 拒绝采样 -> RL)。</li></ul></li><li><strong>性能</strong>: 在 <strong>AIME、MATH、Codeforces</strong> 等推理基准上取得 <strong>SOTA</strong> 性能。</li><li><strong>能力蒸馏</strong>: 成功将推理能力蒸馏到更小模型。</li></ul><p><strong>关键技术:</strong></p><ul><li><strong>RL 激励</strong>: 主要依赖基于规则的奖励系统。</li><li><strong>训练流程</strong>: 精心设计的多阶段流程结合 SFT 和 RL。</li><li><strong>涌现能力</strong>: RL 驱动模型发展出复杂推理行为 (如自验证、反思)。</li></ul><hr><h3 id=deepseek-系列模型特性对比>DeepSeek 系列模型特性对比<a hidden class=anchor aria-hidden=true href=#deepseek-系列模型特性对比>#</a></h3><table><thead><tr><th>特性</th><th>DeepSeek LLM</th><th>DeepSeek-V2</th><th>DeepSeek-V3</th><th>DeepSeek-Coder</th><th>DeepSeek-R1</th></tr></thead><tbody><tr><td><strong>发布时间</strong></td><td>2023年11月</td><td>2024年5月</td><td>2024年10月</td><td>2024年1月</td><td>2024年10月</td></tr><tr><td><strong>基础模型</strong></td><td>-</td><td>-</td><td>-</td><td>-</td><td>DeepSeek-V3</td></tr><tr><td><strong>模型最大规模</strong></td><td>67B</td><td>236B</td><td>671B</td><td>33B</td><td>671B</td></tr><tr><td><strong>激活参数量</strong></td><td>67B (Dense)</td><td><strong>21B (MoE)</strong></td><td><strong>37B (MoE)</strong></td><td>1.3B - 33B (Dense)</td><td><strong>37B (MoE)</strong></td></tr><tr><td><strong>训练数据量</strong></td><td>2T tokens</td><td>8.1T tokens</td><td><strong>14.8T tokens</strong></td><td>2T Code tokens</td><td>SFT 和 RL Data</td></tr><tr><td><strong>上下文长度</strong></td><td>4K (默认) / 32K (扩展)</td><td><strong>128K tokens</strong></td><td><strong>128K tokens</strong></td><td>16K tokens</td><td><strong>128K tokens</strong></td></tr><tr><td><strong>Tokenizer</strong></td><td>Custom BPE</td><td>Custom BPE (Optimized)</td><td>Custom BPE (Expanded)</td><td>Custom BPE (Code)</td><td>Inherited from V3</td></tr><tr><td><strong>位置编码</strong></td><td>RoPE</td><td>RoPE</td><td>RoPE</td><td>RoPE</td><td>RoPE</td></tr><tr><td><strong>注意力/推理优化</strong></td><td>GQA + KV Cache</td><td><strong>MoE + MLA</strong> + KV Cache</td><td><strong>MoE + MLA</strong> + KV Cache</td><td>GQA + KV Cache</td><td><strong>MoE + MLA</strong> + KV Cache</td></tr><tr><td><strong>归一化</strong></td><td>RMSNorm</td><td>RMSNorm</td><td>RMSNorm</td><td>RMSNorm</td><td>RMSNorm</td></tr><tr><td><strong>激活函数</strong></td><td>SwiGLU/GeGLU</td><td>SwiGLU/GeGLU</td><td>SwiGLU/GeGLU</td><td>SwiGLU/GeGLU</td><td>SwiGLU/GeGLU</td></tr><tr><td><strong>模型类别</strong></td><td>基座模型</td><td>基座模型</td><td>基座模型</td><td>代码模型</td><td>推理模型</td></tr></tbody></table><h2 id=关键技术解析>关键技术解析<a hidden class=anchor aria-hidden=true href=#关键技术解析>#</a></h2><p>以下是目前基座大模型所采用的关键技术的详细解析，包括数学公式和相关说明。</p><h3 id=rms-normalization>RMS Normalization<a hidden class=anchor aria-hidden=true href=#rms-normalization>#</a></h3><p>在深度学习中，归一化技术在加速训练、提升模型性能和稳定性方面起着至关重要的作用。RMS Normalization (<a href=https://arxiv.org/abs/1910.07467>Zhang, et al., 2019</a>) 是一种简化的归一化方法，通过仅计算输入向量的均方根（RMS）进行归一化，从而减少计算开销。其数学表达式如下：</p>$$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2 + \epsilon}} \cdot \gamma
$$<p>其中：</p><ul><li>\( x \) 为输入向量。</li><li>\( d \) 为特征维度的大小。</li><li>\( \epsilon \) 为一个极小的常数，用于防止分母为零。</li><li>\( \gamma \) 为可学习的缩放参数。</li></ul><p>LLaMA3 中选择 <strong>RMSNorm</strong> 作为其归一化方法，主要基于以下考虑：</p><ul><li><strong>计算效率</strong>：RMSNorm 相比 LayerNorm、BatchNorm 和WeightNorm 计算量更低，仅计算输入向量的均方根，适合 LLM 的高效训练。</li><li><strong>训练稳定性</strong>：RMSNorm 在保持训练稳定性的同时，能够适应更大的学习率，促进模型的快速收敛。</li><li><strong>资源优化</strong>：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。</li><li><strong>简化实现</strong>：RMSNorm 的实现相对简单，便于在复杂模型中集成和优化。</li></ul><blockquote><p>关于各种 Norm 的对比和代码实现，可参考博客：<a href=https://syhya.github.io/posts/2025-02-01-normalization/>Normalization in Deep Learning</a>。</p></blockquote><h3 id=ffn_swiglu>FFN_SwiGLU<a hidden class=anchor aria-hidden=true href=#ffn_swiglu>#</a></h3><p>Swish-Gated Linear Unit (<a href=https://arxiv.org/abs/2002.05202v1>Shazeer, 2020</a>) 是 LLaMA 中用于增强前馈网络（Feed-Forward Network, FFN）非线性表达能力的关键技术。SwiGLU 结合了 Swish 激活函数和门控机制，显著提升了模型的表现力和性能。此外，与 PaLM (<a href=https://arxiv.org/abs/2204.02311>Chowdhery, 2022</a>) 中使用的$4 d$隐藏维度不同，LLaMA 采用了 $\frac{2}{3}d$ 的隐藏维度，从而在保持参数量和计算量不变的情况下，实现了更高的参数效率。</p>$$
\operatorname{FFN}_{\mathrm{SwiGLU}}\left(x, W_1, W_3, W_2\right)=\left(\operatorname{Swish}\left(x W_1\right) \otimes x W_3\right) W_2
$$<p>其中：</p><ul><li>\( \text{Swish}(x) = x \cdot \sigma(x) \)（Swish 激活函数）。</li><li>\( \sigma(x) = \frac{1}{1 + e^{-x}} \)（Sigmoid 函数）。</li><li>\( \otimes \) 表示逐元素相乘。</li><li>\( W_1, W_2, W_3 \) 为线性变换矩阵。</li></ul><p><strong>优势</strong>：</p><ul><li><strong>增强非线性表达</strong>：SwiGLU 通过结合 Swish 激活函数与门控机制，能够更有效地捕捉复杂的模式和关系，提升 FFN 层的表达能力。</li><li><strong>参数效率</strong>：采用 $\frac{2}{3}d$ 的隐藏维度，在引入额外的线性变换矩阵的同时，保持了总参数量不变，实现了参数的高效利用。</li><li><strong>性能提升</strong>：在多项基准测试中，FFN_SwiGLU 显著提升了模型的性能，尤其在处理复杂任务和长文本时表现尤为出色。例如，在文本生成和理解任务中，SwiGLU 帮助模型更好地理解上下文和长距离依赖关系。</li></ul><p><strong>实现细节</strong>：</p><ul><li><strong>权重矩阵调整</strong>：为了保持与传统 FFN 层相同的参数量和计算量，SwiGLU 通过减少隐藏层的维度（例如，将隐藏层大小从 4d 调整为 $\frac{2}{3}d$），在引入额外的线性变换矩阵的同时，确保整体模型的效率不受影响。</li><li><strong>兼容性</strong>：SwiGLU 作为 GLU 家族的一员，能够无缝集成到现有的 Transformer 架构中，替代传统的 ReLU 或 GELU 激活函数，提升模型的整体性能。</li></ul><blockquote><p>实现代码可以参考这个文件：<a href=https://github.com/syhya/syhya.github.io/blob/main/content/zh/posts/2025-01-22-llama3/swiglu.py>swiglu.py</a></p></blockquote><h3 id=grouped-query-attention-gqa>Grouped Query Attention (GQA)<a hidden class=anchor aria-hidden=true href=#grouped-query-attention-gqa>#</a></h3><p>Grouped Query Attention (GQA) (<a href=https://arxiv.org/pdf/2305.13245>Ainslie, 2023</a>) 是 LLaMA3 中用于优化自注意力计算的关键技术。在大规模语言模型的推理过程中，每个注意力头（head）拥有独立的键（Key）和值（Value）参数会导致巨大的内存消耗。<strong>Grouped Query Attention (GQA)</strong> 旨在通过将多个查询（Query）头分组，并让每组共享一组键值头，从而在模型性能与推理效率之间取得更优的平衡。GQA 是 <strong>Multi-Head Attention (MHA)</strong> 和 <strong>Multi-Query Attention (MQA)</strong> 之间的一种折中方案：</p><ul><li><strong>MHA</strong>：每个注意力头都有独立的 \(\mathbf{K}\) 和 \(\mathbf{V}\)。</li><li><strong>MQA</strong>：所有注意力头共享一组 \(\mathbf{K}\) 和 \(\mathbf{V}\)。</li><li><strong>GQA</strong>：将 \(H\) 个查询头划分为 \(G\) 组，每组共享一组 \(\mathbf{K}\) 和 \(\mathbf{V}\)（其中 \(1 < G < H\)）。</li></ul><h4 id=1-投影-projections>1. 投影 (Projections)<a hidden class=anchor aria-hidden=true href=#1-投影-projections>#</a></h4><p>给定输入序列 \(\mathbf{X} \in \mathbb{R}^{B \times S \times d}\)，首先通过线性变换投影得到查询、键和值矩阵：</p>$$
\mathbf{Q} = \mathbf{X} W_Q, \quad
\mathbf{K} = \mathbf{X} W_K, \quad
\mathbf{V} = \mathbf{X} W_V,
$$<p>其中，\(W_Q, W_K, W_V \in \mathbb{R}^{d \times d}\) 为可学习的投影矩阵。</p><h4 id=2-头与分组-heads-and-grouping>2. 头与分组 (Heads and Grouping)<a hidden class=anchor aria-hidden=true href=#2-头与分组-heads-and-grouping>#</a></h4><ul><li><strong>头的切分</strong>：将 \(\mathbf{Q}\)、\(\mathbf{K}\) 和 \(\mathbf{V}\) 分割成 \(H\) 个头，每个头的向量维度为 \(d_{\text{head}} = \frac{d}{H}\)。</li></ul>$$
\mathbf{Q} = [\mathbf{Q}_1; \mathbf{Q}_2; \dots; \mathbf{Q}_H], \quad
\mathbf{K} = [\mathbf{K}_1; \mathbf{K}_2; \dots; \mathbf{K}_H], \quad
\mathbf{V} = [\mathbf{V}_1; \mathbf{V}_2; \dots; \mathbf{V}_H]
$$<ul><li><strong>分组</strong>：将这 \(H\) 个查询头进一步划分为 \(G\) 组（\(1 < G < H\)）。对于第 \(g\) 组，包含 \(\frac{H}{G}\) 个查询头，并共享一组键值头 \(\mathbf{K}^g\) 和 \(\mathbf{V}^g\)。</li></ul>$$
\mathcal{G} = \left\{ \mathcal{G}_1, \mathcal{G}_2, \dots, \mathcal{G}_G \right\}, \quad |\mathcal{G}_g| = \frac{H}{G} \quad \forall g \in \{1, 2, \dots, G\}
$$<p>下图展示了 GQA 与传统 MHA 和 MQA 的对比，可见在 GQA 中，<strong>每组查询头公用一组键值头</strong>。</p><figure class=align-center><img loading=lazy src=attention_comparison.png#center alt="Fig. 5. Overview of Grouped Query Attention (GQA). (Image source: Ainslie et al., 2023)" width=100%><figcaption><p>Fig. 5. Overview of Grouped Query Attention (GQA). (Image source: <a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>)</p></figcaption></figure><h4 id=3-组内注意力-intra-group-attention>3. 组内注意力 (Intra-Group Attention)<a hidden class=anchor aria-hidden=true href=#3-组内注意力-intra-group-attention>#</a></h4><p>对于第 \(g\) 组，令该组的查询向量为 \(\{\mathbf{Q}_i\}_{i \in \mathcal{G}_g}\)，共享的键值向量为 \(\mathbf{K}^g\) 和 \(\mathbf{V}^g\)。组内注意力的计算公式为：</p>$$
\text{Attention}_g(\mathbf{Q}_i, \mathbf{K}^g, \mathbf{V}^g) = \text{softmax}\left( \frac{\mathbf{Q}_i (\mathbf{K}^g)^\top}{\sqrt{d_{\text{head}}}} \right) \mathbf{V}^g
$$<p>其中，\(\sqrt{d_{\text{head}}}\) 为缩放因子，用于稳定梯度和数值计算。</p><h4 id=4-拼接输出-concatenate--output>4. 拼接输出 (Concatenate & Output)<a hidden class=anchor aria-hidden=true href=#4-拼接输出-concatenate--output>#</a></h4><p>将所有组的注意力结果在通道维度上拼接，得到矩阵 \(\mathbf{O}\)，然后通过线性变换矩阵 \(W_O \in \mathbb{R}^{d \times d}\) 得到最终输出：</p>$$
\mathbf{O} = \text{Concat}\left( \text{Attention}_1, \text{Attention}_2, \dots, \text{Attention}_G \right) W_O
$$<p>其中，\(\text{Concat}\) 表示在通道维度上的拼接操作。</p><blockquote><p>更多关于注意力机制在 <strong>MHA</strong>、<strong>MQA</strong> 和 <strong>GQA</strong> 之间的详细对比及代码示例，可参考博客：<a href=https://syhya.github.io/posts/2025-01-16-group-query-attention/#grouped-query-attention-gqa>Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA</a>。</p></blockquote><h4 id=rotary-positional-embeddings-rope>Rotary Positional Embeddings (RoPE)<a hidden class=anchor aria-hidden=true href=#rotary-positional-embeddings-rope>#</a></h4><p><strong>Rotary Positional Embeddings (RoPE)</strong> 是LLaMA3中用于表示序列中位置关系的技术，通过对Query和Key向量应用旋转变换，增强了模型对相对位置信息的感知能力。</p><p><strong>优势</strong>：</p><ol><li><strong>相对位置感知</strong>：RoPE能够自然地捕捉词汇之间的相对位置关系，提升了长距离依赖的建模效果。</li><li><strong>计算效率高</strong>：无需额外的计算，位置编码与词向量的结合在计算上是高效的，适用于大规模模型。</li><li><strong>适应不同长度的序列</strong>：RoPE可以灵活处理不同长度的输入序列，不受固定位置编码的限制。</li><li><strong>兼容线性注意力</strong>：RoPE 可以与线性注意力机制结合，保持注意力计算的线性复杂度，进一步提升处理长序列的效率。</li></ol><p>理解单词在序列中的位置关系对于成功训练大型语言模型（LLM）至关重要。循环神经网络（RNN）通过递归计算隐藏状态来自然地捕捉序列中的位置信息。然而，Transformer这类基于自注意力机制的模型由于其并行计算的特性，无法直接感知单词之间的相对位置关系，因此需要额外的位置编码来提供这一信息。</p><p>位置编码的方法主要分为绝对位置编码和相对位置编码两大类。RoPE 则是一种创新性的绝对位置编码方法，旨在结合绝对位置编码和相对位置编码的优点，通过旋转变换实现相对位置感知。</p><h4 id=绝对位置编码>绝对位置编码<a hidden class=anchor aria-hidden=true href=#绝对位置编码>#</a></h4><p>绝对位置编码通过为每个位置生成一个固定或可训练的位置向量，并将其与词向量相加，从而为模型提供位置信息。常见的绝对位置编码方法包括：</p><ul><li><p><strong>三角函数位置编码</strong>：例如，Vaswani 等人（2017）提出的使用正弦和余弦函数生成的位置编码。</p>$$
p_i = \left[\sin\left(\frac{i}{10000^{2j/d}}\right), \cos\left(\frac{i}{10000^{2j/d}}\right)\right]_{j=1}^{d/2}
$$<p>其中，\( p_i \) 是位置 \( i \) 的位置编码向量，\( d \) 是词向量的维度。</p><p><strong>优点</strong>：</p><ul><li>实现简单，与词向量的结合方式直接。</li></ul><p><strong>缺点</strong>：</p><ul><li>无法自然地捕捉词汇之间的相对位置关系，限制了模型对长距离依赖的建模能力。</li><li>对于不同长度的序列，可能需要重新生成位置编码。</li></ul></li><li><p><strong>可训练位置编码</strong>：如BERT和GPT中使用的可训练位置编码。</p></li></ul><h4 id=相对位置编码>相对位置编码<a hidden class=anchor aria-hidden=true href=#相对位置编码>#</a></h4><p>相对位置编码旨在让模型关注词汇之间的相对距离，而不是绝对位置。这样，模型可以更灵活地处理不同长度的序列，并更有效地捕捉长距离依赖关系。</p><p><strong>常见方法</strong>：</p><ul><li><p><strong>Google式相对位置编码</strong>：在论文《Self-Attention with Relative Position Representations》中，Shaw 等人（2018）提出了一种扩展自注意力机制以考虑相对位置的方法。</p><p><strong>优点</strong>：</p><ul><li>自然地捕捉词汇之间的相对位置信息，有助于长距离依赖的建模。</li><li>提高模型对序列长度的灵活适应能力。</li></ul><p><strong>缺点</strong>：</p><ul><li>实现相对复杂，尤其是在自注意力机制中的集成。</li><li>计算效率相对较低，特别是在处理长序列时。</li></ul></li></ul><h3 id=旋转位置编码rope的原理与实现>旋转位置编码（RoPE）的原理与实现<a hidden class=anchor aria-hidden=true href=#旋转位置编码rope的原理与实现>#</a></h3><h4 id=rope-的设计思路>RoPE 的设计思路<a hidden class=anchor aria-hidden=true href=#rope-的设计思路>#</a></h4><p>RoPE 的核心理念是通过旋转变换将绝对位置信息转化为相对位置信息，从而结合绝对位置编码和相对位置编码的优点。具体而言，RoPE 通过将查询（Query）和键（Key）向量分别乘以与其位置相关的旋转矩阵，使得内积计算中自然地体现了相对位置关系。</p><h4 id=数学表达式与推导>数学表达式与推导<a hidden class=anchor aria-hidden=true href=#数学表达式与推导>#</a></h4><h5 id=维情况下的-rope-推导>维情况下的 RoPE 推导<a hidden class=anchor aria-hidden=true href=#维情况下的-rope-推导>#</a></h5><p>为了更深入理解 RoPE 在二维情况下的工作原理，以下通过数学推导展示其如何实现相对位置编码。</p><p><strong>引言</strong></p><p>在二维空间中，向量的旋转可以通过复数的乘法来简化理解。RoPE 利用这一性质，通过对查询（Query）和键（Key）向量施加旋转变换，实现相对位置编码。</p><p><strong>复数与二维向量的对应关系</strong></p><ul><li>一个复数 \( z = a + ib \) 可以表示为二维向量 \( \mathbf{v} = [a, b]^T \)。</li><li>复数的乘法对应于二维向量的旋转和缩放。</li></ul>$$
e^{i\theta} = \cos\theta + i\sin\theta
$$<p>可以将复数旋转表示为二维向量的旋转矩阵。</p><p><strong>RoPE 的基本操作</strong></p><p>假设有二维的查询向量 \( \mathbf{q}_m \) 和键向量 \( \mathbf{k}_n \)，分别位于位置 \( m \) 和 \( n \)。RoPE 的目标是通过旋转变换，使得它们的内积仅依赖于相对位置 \( m - n \)。</p><p><strong>步骤 1：表示为复数</strong></p>$$
\mathbf{q}_m = q_m^{(1)} + i q_m^{(2)} \\
\mathbf{k}_n = k_n^{(1)} + i k_n^{(2)}
$$<p><strong>步骤 2：应用旋转变换</strong></p>$$
f_q(\mathbf{q}_m, m) = \mathbf{q}_m \cdot e^{im\theta} = (q_m^{(1)} + i q_m^{(2)}) (\cos(m\theta) + i\sin(m\theta)) \\
f_k(\mathbf{k}_n, n) = \mathbf{k}_n \cdot e^{in\theta} = (k_n^{(1)} + i k_n^{(2)}) (\cos(n\theta) + i\sin(n\theta))
$$<p>其中，\( \theta \) 是一个预先定义的常数，用于控制旋转的速度。</p><p><strong>步骤 3：计算内积</strong></p>$$
\langle f_q(\mathbf{q}_m, m), f_k(\mathbf{k}_n, n) \rangle = \text{Re}\left[ f_q(\mathbf{q}_m, m) \cdot \overline{f_k(\mathbf{k}_n, n)} \right]
$$<p>其中，\( \overline{f_k(\mathbf{k}_n, n)} \) 是 \( f_k \) 的共轭复数。</p>$$
\langle f_q(\mathbf{q}_m, m), f_k(\mathbf{k}_n, n) \rangle = (q_m^{(1)}k_n^{(1)} + q_m^{(2)}k_n^{(2)})\cos((m-n)\theta) + (q_m^{(1)}k_n^{(2)} - q_m^{(2)}k_n^{(1)})\sin((m-n)\theta)
$$<p>这个结果表明，内积的计算中自然地引入了相对位置 \( m - n \) 的影响。</p><p><strong>步骤 4：将二维向量与旋转矩阵对应</strong></p><p>将复数形式转化为矩阵形式：</p>$$
f_q(\mathbf{q}_m, m) = R(m\theta) \mathbf{q}_m \\
f_k(\mathbf{k}_n, n) = R(n\theta) \mathbf{k}_n
$$$$
R(\phi) = \begin{bmatrix}
\cos\phi & -\sin\phi \\
\sin\phi & \cos\phi
\end{bmatrix}
$$<p>因此，内积可以表示为：</p>$$
\langle f_q(\mathbf{q}_m, m), f_k(\mathbf{k}_n, n) \rangle = [q_m^{(1)}, q_m^{(2)}] R((n - m)\theta) \mathbf{k}_n
$$<p>进一步展开：
$$
= q_m^{(1)} k_n^{(1)} \cos((m - n)\theta) + q_m^{(2)} k_n^{(2)} \cos((m - n)\theta) \</p><ul><li>q_m^{(1)} k_n^{(2)} \sin((m - n)\theta) - q_m^{(2)} k_n^{(1)} \sin((m - n)\theta)
$$
与前述结果一致，验证了 RoPE 在二维情况下实现相对位置编码的有效性。</li></ul><p><strong>总结二维 RoPE 的实现步骤</strong>：</p><ol><li><strong>表示向量为复数</strong>：将二维的查询和键向量表示为复数形式。</li><li><strong>应用旋转变换</strong>：根据各自的位置 \( m \) 和 \( n \)，分别将查询和键向量旋转 \( m\theta \) 和 \( n\theta \) 的角度。</li><li><strong>计算内积的实部</strong>：通过复数的乘法和内积运算，确保内积结果仅依赖于相对位置 \( m - n \)。</li><li><strong>利用旋转矩阵</strong>：通过旋转矩阵的性质，将复数形式转化为矩阵形式，使得旋转操作更加直观和易于扩展到高维情况。</li></ol><h5 id=322-高维情况下的-rope>3.2.2 高维情况下的 RoPE<a hidden class=anchor aria-hidden=true href=#322-高维情况下的-rope>#</a></h5><p>对于词向量维度 \( d \) 为偶数的情况，RoPE 通过将向量拆分为 \( d/2 \) 个二维子向量，并对每个子向量应用独立的旋转矩阵，从而实现高维度的旋转位置编码。</p>$$
f_{\{q, k\}}(x_m, m) = R_{\Theta, m}^d \cdot W_{\{q, k\}} \cdot x_m
$$<p>其中，\( R_{\Theta, m}^d \) 是一个块对角矩阵，由 \( d/2 \) 个二维旋转矩阵组成，每个旋转矩阵对应一个不同的角度 \( \theta_i = 10000^{-2(i-1)/d} \)。</p>$$
R_{\Theta, m}^d = \begin{bmatrix}
\cos(m\theta_1) & -\sin(m\theta_1) & & & \\
\sin(m\theta_1) & \cos(m\theta_1) & & & \\
& & \cos(m\theta_2) & -\sin(m\theta_2) & \\
& & \sin(m\theta_2) & \cos(m\theta_2) & \\
& & & & \ddots \\
\end{bmatrix}
$$$$
q_m^\top k_n = (R_{\Theta, m}^d W_q x_m)^\top (R_{\Theta, n}^d W_k x_n) = x_m^\top W_q^\top R_{\Theta, m}^{d\top} R_{\Theta, n}^d W_k x_n = x_m^\top W_q^\top R_{\Theta, n-m}^d W_k x_n
$$<p>其中，\( R_{\Theta, n-m}^d = R_{\Theta, m}^{d\top} R_{\Theta, n}^d \)，体现了相对位置信息。</p><h4 id=33-rope-的性质>3.3 RoPE 的性质<a hidden class=anchor aria-hidden=true href=#33-rope-的性质>#</a></h4><p><strong>3.3.1 远程衰减特性</strong></p><p>通过选择 \( \theta_i = 10000^{-2(i-1)/d} \)，RoPE 的内积计算结果随着相对位置 \( |m - n| \) 增大而衰减。这一特性符合自然语言中的直觉，即距离较远的单词对当前词的影响应当较小。</p><p><strong>3.3.2 兼容线性注意力机制</strong></p><p>RoPE 的旋转操作保持了向量的范数不变，使其可以与线性注意力机制（如 Performer）无缝结合，进一步提升模型在处理长序列时的计算效率。</p><hr><h3 id=4-总结>4. 总结<a hidden class=anchor aria-hidden=true href=#4-总结>#</a></h3><p>RoPE 通过旋转变换将绝对位置信息转化为相对位置信息，结合了绝对位置编码和相对位置编码的优点。其核心机制确保了模型对相对位置关系的感知能力，并且保持了高效的计算性能，特别是在处理长序列任务时表现出色。通过详细的数学推导，特别是在二维情况下的实现，RoPE 展示了其在自注意力机制中引入相对位置编码的有效性和灵活性。</p><h3 id=bpe>BPE<a hidden class=anchor aria-hidden=true href=#bpe>#</a></h3><p><strong>tiktoken</strong> tokenizer是LLaMA3采用的新一代分词器，相较于LLaMA2使用的SentencePiece BPE，tiktoken在以下方面有所改进：</p><ol><li><strong>词汇表扩展</strong>：词汇表从32k扩展至128k，覆盖更多语言和专业术语，减少了分词次数，提升了生成质量。</li><li><strong>编码效率</strong>：优化了编码算法，减少了分词时间，提高了处理速度。</li><li><strong>生成质量</strong>：通过更细粒度的词汇表示，提升了模型生成文本的连贯性和准确性。</li></ol>$$
\text{Tokenize}(w) = \text{BPE}(w) \quad \text{vs} \quad \text{Tokenize}(w) = \text{tiktoken\_BPE}(w)
$$<ul><li>其中，\( w \) 为输入词汇，tiktoken_BPE通过更大词汇表减少了分词次数。</li></ul><p><strong>优势</strong>：</p><ul><li><strong>减少分词次数</strong>：更大的词汇表使得更多词汇能作为单一token处理，减少了分词次数，提高了生成效率和质量。</li><li><strong>提升生成质量</strong>：更细粒度的词汇表示，使模型在生成文本时能够更准确地表达复杂语义。</li><li><strong>编码速度快</strong>：优化的编码算法提升了分词速度，适用于大规模模型的高效训练和推理。</li></ul><h4 id=轻量级模型>轻量级模型<a hidden class=anchor aria-hidden=true href=#轻量级模型>#</a></h4><p>为了适应边缘设备和移动设备的需求，LLaMA3推出了<strong>1B和3B参数量的轻量级模型</strong>，采用以下技术：</p><ol><li><strong>剪枝技术</strong>：通过系统性地移除网络中的冗余参数，减小模型规模，同时保持核心性能。</li><li><strong>知识蒸馏</strong>：让小模型从大模型中学习，提升其在特定任务上的表现。</li><li><strong>优化部署</strong>：针对移动设备的硬件架构进行优化，如针对Arm处理器的性能调优，确保模型在资源受限环境中的高效运行。</li></ol>$$
\text{Pruned\_Model} = \text{Prune}(\text{Original\_Model}, \text{Pruning\_Rate})
$$$$
\text{Distilled\_Model} = \text{Distill}(\text{Large\_Model}, \text{Small\_Model})
$$<ul><li>其中，Prune表示剪枝操作，Distill表示知识蒸馏过程。</li></ul><p><strong>优势</strong>：</p><ul><li><strong>适应资源受限设备</strong>：减小模型规模，使其适用于边缘设备和移动设备，推动了大语言模型的普及。</li><li><strong>保持性能</strong>：通过剪枝和知识蒸馏技术，保持了模型的核心性能和表现。</li><li><strong>高效运行</strong>：优化的模型结构和权重格式（如BFloat16）提升了计算效率，确保在移动设备上的高效运行。</li></ul><h4 id=训练方法>训练方法<a hidden class=anchor aria-hidden=true href=#训练方法>#</a></h4><p><strong>LLaMA3</strong>在训练数据和方法上进行了全面升级，采用了更大规模的数据和更先进的训练技术：</p><ol><li><p><strong>预训练阶段</strong>：</p><ul><li><strong>大规模数据扩展</strong>：训练数据量达到15万亿token，覆盖更多语言、专业领域和多模态数据，提升了模型的泛化能力和多语言支持。</li><li><strong>扩展法则（Scaling Laws）</strong>：<ul><li>根据Chinchilla扩展法则，优化模型的训练数据量和参数规模平衡，确保模型在关键任务上的最佳性能。</li><li>数学表达式：
$$
\text{Optimal Data} \propto \text{Model Size}^{4/3}
$$
这一公式指导了数据和模型规模的平衡，确保随着模型规模的增加，训练数据量也按比例增长，避免模型过拟合或欠拟合。</li></ul></li></ul></li><li><p><strong>并行训练策略</strong>：</p><ul><li><strong>数据并行</strong>：将训练数据分布到多个GPU上，提升数据处理速度。</li><li><strong>模型并行</strong>：将模型的不同部分分布到多个GPU上，支持更大规模的模型训练。</li><li><strong>流水并行</strong>：分阶段处理模型的不同部分，提高训练效率。</li></ul>$$
\text{Total Throughput} = \text{Data Parallelism} \times \text{Model Parallelism} \times \text{Pipeline Parallelism}
$$<ul><li>其中，总吞吐量（Total Throughput）是数据并行、模型并行和流水并行的乘积，显著提升了训练效率。</li></ul></li><li><p><strong>硬件优化</strong>：</p><ul><li><strong>高效利用GPU</strong>：在16K GPU上实现每GPU超过400 TFLOPS的计算利用率，通过定制的24K GPU集群进行训练，确保训练过程的高效性和稳定性。</li><li><strong>错误处理与存储优化</strong>：<ul><li><strong>自动错误检测与处理</strong>：确保训练过程的连续性和高效性。</li><li><strong>可扩展存储系统</strong>：减少检查点和回滚的开销，提高数据存储效率。</li></ul></li></ul></li><li><p><strong>微调阶段</strong>：</p><ul><li><strong>多轮对齐步骤</strong>：<ul><li><strong>监督微调（SFT）</strong>：使用高质量的标注数据进一步优化模型性能。</li><li><strong>拒绝采样（Rejection Sampling）</strong>：通过拒绝低质量内容，提升生成文本的质量。</li><li><strong>近端策略优化（Proximal Policy Optimization, PPO）和直接策略优化（Direct Policy Optimization, DPO）</strong>：结合两者的优势，优化模型的生成策略，使其更符合人类偏好。</li></ul></li></ul>$$
\mathcal{L}_{\text{RLHF}} = \mathbb{E}_{\theta \sim \pi_{\theta}} \left[ r(s, a) \right]
$$<ul><li>其中，\( \mathcal{L}_{\text{RLHF}} \)为RLHF的损失函数，\( \pi_{\theta} \)为策略分布，\( r(s, a) \)为奖励函数。</li></ul></li><li><p><strong>多模态训练</strong>：</p><ul><li><strong>视觉语言模型</strong>：结合图像和文本数据，提升模型在多模态任务中的表现。</li><li><strong>代码数据扩展</strong>：增加代码token数量，提升模型在编程任务中的表现。</li></ul></li><li><p><strong>模型安全与质量控制</strong>：</p><ul><li><strong>数据过滤pipeline</strong>：<ul><li><strong>启发式过滤器</strong>：基于规则的过滤，提高数据质量。</li><li><strong>NSFW过滤器</strong>：去除不适内容，确保数据的安全性。</li><li><strong>语义重复数据删除</strong>：使用语义分析技术，删除内容高度相似的数据。</li><li><strong>文本分类器</strong>：预测数据质量，进一步优化数据集。</li></ul></li></ul></li><li><p><strong>优化训练堆栈</strong>：</p><ul><li><strong>高级训练堆栈</strong>：自动检测和处理训练过程中的错误，提升硬件可靠性。</li><li><strong>性能调优</strong>：针对不同硬件平台进行优化，确保训练过程的高效性。</li></ul></li></ol><p><strong>LLaMA3</strong>通过这些先进的训练方法和优化策略，显著提升了模型的性能和适应性，成为开源大语言模型领域的领先者。</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><h2 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h2><ol><li><a href=https://arxiv.org/pdf/1606.08415.pdf>Hendrycks and Gimpel, 2016</a></li><li><a href=https://arxiv.org/pdf/2002.05202.pdf>GLU Variants Improve Transformer</a></li><li><a href=https://arxiv.org/abs/2302.13971>LLaMA: Open and Efficient Foundation Language Models</a></li><li><a href=https://arxiv.org/pdf/2307.09288>LLaMA2: Open Foundation and Fine-Tuned Chat Models</a></li><li><a href=https://github.com/meta-llama/llama/blob/main/llama/model.py>meta-llama repo</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/zh/tags/nlp/>NLP</a></li><li><a href=https://syhya.github.io/zh/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/zh/tags/pre-training/>Pre-Training</a></li><li><a href=https://syhya.github.io/zh/tags/post-training/>Post-Training</a></li><li><a href=https://syhya.github.io/zh/tags/llama/>LLaMA</a></li><li><a href=https://syhya.github.io/zh/tags/deepseek/>DeepSeek</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/zh/posts/2025-01-27-deepseek-r1/><span class=title>« 上一页</span><br><span>OpenAI o1复现进展：DeepSeek-R1</span>
</a><a class=next href=https://syhya.github.io/zh/posts/2025-01-16-group-query-attention/><span class=title>下一页 »</span><br><span>Transformer注意力机制：MHA、MQA与GQA的对比</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型：LLaMA、DeepSeek 等系列（长期更新） on x" href="https://x.com/intent/tweet/?text=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%9aLLaMA%e3%80%81DeepSeek%20%e7%ad%89%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%ef%bc%89&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llm%2f&amp;hashtags=AI%2cNLP%2cLLM%2cPre-training%2cPost-training%2cLlama%2cDeepSeek"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型：LLaMA、DeepSeek 等系列（长期更新） on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llm%2f&amp;title=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%9aLLaMA%e3%80%81DeepSeek%20%e7%ad%89%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%ef%bc%89&amp;summary=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%9aLLaMA%e3%80%81DeepSeek%20%e7%ad%89%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%ef%bc%89&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型：LLaMA、DeepSeek 等系列（长期更新） on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llm%2f&title=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%9aLLaMA%e3%80%81DeepSeek%20%e7%ad%89%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%ef%bc%89"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型：LLaMA、DeepSeek 等系列（长期更新） on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型：LLaMA、DeepSeek 等系列（长期更新） on whatsapp" href="https://api.whatsapp.com/send?text=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%9aLLaMA%e3%80%81DeepSeek%20%e7%ad%89%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%ef%bc%89%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型：LLaMA、DeepSeek 等系列（长期更新） on telegram" href="https://telegram.me/share/url?text=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%9aLLaMA%e3%80%81DeepSeek%20%e7%ad%89%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%ef%bc%89&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基座大语言模型：LLaMA、DeepSeek 等系列（长期更新） on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e5%9f%ba%e5%ba%a7%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%ef%bc%9aLLaMA%e3%80%81DeepSeek%20%e7%ad%89%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%ef%bc%89&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llm%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>