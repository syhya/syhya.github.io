[{"content":"DeepSeek AI 近期发布 DeepSeek-R1 (DeepSeek-AI, 2025)，其推理性能在多个 benchmark 上已接近 OpenAI o1 (OpenAI, 2024)的水平，是开源社区成功复现 o1 的重要一步。R1 相关代码可以参考huggingface 尝试开源复现 open-r1 项目。以往的研究多依赖于海量的监督数据来提升大语言模型（Large Language Model, LLM）性能，但 DeepSeek-R1 及其早期实验 DeepSeek-R1-Zero 的成功，有力证明了纯粹大规模强化学习在提升 LLM 推理能力方面的潜力。其印证了 Richard Sutton 在 “The Bitter Lesson” 中提出的深刻见解:\nOne thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning. (Richard Sutton, 2019)\n符号 下面列举了文章所使用的数学公式，可以帮你更轻松阅读。\n符号 含义 \\( q \\) 或 \\( Q \\) 问题，用户提出的输入或指令 \\( o \\) 或 \\( O \\) 输出，模型生成的文本回复或答案 \\( t \\) token 索引，表示输出文本中的第 \\( t \\) 个 token 的位置 \\( o_t \\) 输出文本 \\( o \\) 中的第 \\( t \\) 个 token \\( o_{\u0026lt;t} \\) 输出文本 \\( o \\) 中前 \\( t-1 \\) 个 tokens \\( \u0026#124;o\u0026#124; \\) 输出文本 \\( o \\) 的长度，通常指 token 的数量 \\( G \\) 输出组的大小，在 GRPO 算法中，为每个问题采样的输出数量 \\( \\pi_\\theta, \\pi_{\\theta_{\\text{old}}}, \\pi_{\\text{ref}}, \\pi_{\\text{sft}} \\) 策略模型及其变体，用于生成文本输出或作为参考模型 \\( A_t, A_i \\) 优势函数与相对优势值 \\( \\varepsilon \\) 剪辑超参数, 用于限制重要性采样率的范围，保证策略更新的稳定性 \\( \\beta \\) 正则化系数，用于控制 KL 散度惩罚项在目标函数中的权重 \\( \\mathbb{D}_{KL} \\) KL 散度，衡量两个概率分布之间差异的度量，用于约束新策略与参考策略的距离 \\( \\mathcal{J}, \\mathcal{L} \\) 目标函数与损失函数 \\( \\mathbb{E} \\) 期望，表示对随机变量的平均值，在目标函数中表示对样本数据的平均 \\( P_{\\text{sft}}(Q, O) \\) SFT 数据集的分布，表示 \\( SFT \\) 数据集中问题 \\( Q \\) 和输出 \\( O \\) 的联合概率分布 \\( P_{\\text{sft}}(Q) \\) SFT 数据集中问题的分布，表示 \\( SFT \\) 数据集中问题 \\( Q \\) 的边缘概率分布 \\( \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\) 策略模型在给定问题 \\( q \\) 和之前生成的 tokens: \\( o_{\u0026lt;t} \\) 的条件下，生成第 \\( t \\) 个 token: \\( o_t \\) 条件概率 \\( \\mathbb{I}(o) \\) 判断输出 \\( o \\) 的答案是否为高质量的函数，高质量时为 1，否则为 0 \\( r(o) \\) 奖励函数，评估模型输出 \\( o \\) 质量的函数 \\( r_i \\) 第 \\( i \\) 个输出的奖励值 \\( \\nabla_{\\theta} \\) 梯度算子，表示对函数关于模型参数 \\( \\theta \\) 求梯度 \\( \\mathcal{N}(\\mu, 1) \\) 正态分布，均值为 \\( \\mu \\)，标准差为 1 \\( \\binom{a}{b} \\) 二项式系数，表示从 \\( a \\) 个元素中选择 \\( b \\) 个元素的组合数 \\( r(o) = \\frac{\\pi_{\\text{ref}}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\) 概率比值，参考模型与当前策略模型生成输出 \\( o \\) 的概率之比 训练流程概要 DeepSeek-R1 系列模型的训练是一个多阶段的过程，旨在构建具备卓越推理能力和通用语言能力的大型语言模型。整个训练流程从 DeepSeek-V3 (DeepSeek-AI, 2024) 模型出发，逐步迭代优化，最终得到不同版本的 DeepSeek-R1 模型。\nFig. 1. DeepSeek R1 Training Pipeline. (Image source: Harris Chan\u0026rsquo;s Tweet)\n如图1清晰的显示了 DeepSeek-R1 整个训练流程，主要分为以下几个关键阶段：\n基座模型与初步微调: 流程的起点是 DeepSeek-V3 Base 模型。首先，使用监督式微调 (SFT) 技术，在 冷启动长文本 CoT 数据 上对基础模型进行初步训练，赋予模型初步的推理能力。\n强化推理能力: 在 SFT 基础上，采用面向推理的强化学习方法，具体为组相对策略优化 (GRPO) 算法，并结合基于规则的奖励和CoT 语言一致性奖励，进一步提升模型的推理能力。\n推理数据生成与拒绝采样: 利用推理提示和拒绝采样技术，并以规则和让 DeepSeek-V3 模型进行评判数据质量，生成高质量的推理数据。\n非推理数据生成: 使用 CoT 提示方法，让 DeepSeek-V3 模型进行数据增强，生成非推理数据并且结合原始 SFT 数据，以提升模型的通用语言能力。\n蒸馏: 将推理数据和非推理数据结合，用于蒸馏训练。通过 SFT，将 DeepSeek-V3 的能力迁移到一系列小型模型 (Qwen 和 Llama 系列)，得到 DeepSeek-R1-Distill 系列模型。\n最终模型微调: 对 DeepSeek-V3 模型再次进行 SFT 和强化学习微调。强化学习阶段采用 推理和偏好奖励，并使用多样化的训练提示，最终得到 DeepSeek-R1 模型。\nDeepSeek-R1-Zero: 通过 GRPO 算法直接在 DeepSeek-V3 Base 上进行训练得到，作为其他模型的对比基准。\n接下来博主将深入分析 DeepSeek-R1 训练流程中的关键技术和方法。\nDeepSeek-R1-Zero PPO 近端策略优化 (Proximal Policy Optimization, PPO) (Schulman et al., 2017) 算法是一种广泛应用于强化学习的经典算法，在 InstructGPT(Ouyang et al., 2022) 论文中被证明是训练 LLM 强化学习微调阶段的有效且稳定的方法。\n强化学习核心思想是让智能体 (Agent) 在与环境的交互中学习，通过试错来最大化累积奖励。在LLM场景下，模型本身就是智能体，“环境” 可以理解为用户提出的问题和期望的回答方式。策略 (Policy) \\(\\pi_\\theta\\) 代表了智能体的行为准则，即给定一个输入 (例如问题 \\(q\\))，策略会输出一个动作 (例如生成文本 \\(o\\))。策略 \\(\\pi_\\theta\\) 通常由一个神经网络模型参数化，训练的目标是找到最优的参数 \\(\\theta\\)，使得策略能够生成高质量的输出。\nActor-Critic 框架是强化学习中常用的一种架构，PPO 也属于 Actor-Critic 算法。Actor-Critic 框架包含两个核心组件：\nActor (策略模型)：负责学习策略 \\(\\pi_\\theta\\)，即如何根据当前状态 (例如用户问题) 选择动作 (生成文本)。 Critic (价值模型)：负责评估 Actor 策略的好坏，通常通过学习一个价值函数 \\(V(s)\\) 或 \\(Q(s, a)\\) 来实现。价值函数预测在给定状态 \\(s\\) (或状态-动作对 \\((s, a)\\)) 下，未来能够获得的累积奖励的期望值。 PPO 的目标是改进策略模型 (Actor)，使其能够生成更高质量的输出，同时借助价值模型 (Critic) 来稳定训练过程。PPO 通过最大化以下目标函数来更新策略模型 \\(\\pi_{\\theta}\\)：\n\\[ \\mathcal{J}_{PPO}(\\theta) = \\mathbb{E}\\!\\Biggl[ \\min\\Bigl( \\underbrace{\\frac{\\pi_\\theta(a\\!\\mid\\!s)}{\\pi_{\\theta_{\\text{old}}}(a\\!\\mid\\!s)}}_{\\text{重要性采样率}}\\underbrace{A_t}_{\\text{优势函数}},\\, \\operatorname{clip}\\Bigl( \\underbrace{\\frac{\\pi_\\theta(a\\!\\mid\\!s)}{\\pi_{\\theta_{\\text{old}}}(a\\!\\mid\\!s)}}_{\\text{重要性采样率}}, 1-\\varepsilon,\\, 1+\\varepsilon \\Bigr)\\underbrace{A_t}_{\\text{优势函数}} \\Bigr) \\Biggr] \\]参数说明：\n期望 \\(\\mathbb{E}[\\cdot]\\)：表示对样本的平均。在实际训练中，我们会采样一批数据 (例如用户问题和模型生成的回答)，然后计算这批数据的平均目标函数值。\n重要性采样率：衡量当前策略 \\(\\pi_\\theta\\) 与旧策略 \\(\\pi_{\\theta_{\\text{old}}}\\) 在动作 \\(a\\) 上的概率比值。PPO 采用 近端策略更新 的思想，限制每次策略更新的幅度，避免策略变化过大导致训练不稳定。\n优势函数 \\(A_t\\)：评估在状态 \\(s\\) 下采取动作 \\(a\\) 相对于平均水平的优势。优势函数通常由 Critic 模型 (价值网络) 估计得到，可以是优势估计 (Advantage Estimation) 或 广义优势估计 (Generalized Advantage Estimation, GAE) 等方法。优势函数 \\(A_t\\) 越大，表示当前动作 \\(a\\) 越好，策略模型应该增加采取该动作的概率。\nclip：PPO 的核心机制之一，本质上可以看作是一个惩罚函数，用于限制重要性采样率的范围在 \\([1-\\varepsilon, 1+\\varepsilon]\\) 之间，其中 \\(\\varepsilon\\) 是一个超参数 (通常设置为 0.2)。剪辑操作防止策略更新步幅过大，提高训练的稳定性。\nclip 函数通过限制重要性采样率来惩罚过大或过小的策略更新幅度。\n当重要性采样率超出 \\([1-\\varepsilon, 1+\\varepsilon]\\) 范围时，clip 函数会将其限制在该范围内，从而降低目标函数的增益 (或减少损失)。 对于正向更新 (\\(A_t \u003e 0\\))： 如果重要性采样率过大 (超过 \\(1+\\varepsilon\\))，clip 会将其限制为 \\(1+\\varepsilon\\)，降低了实际的更新幅度，惩罚了过于激进的策略改进。 对于负向更新 (\\(A_t \u003c 0\\))： 如果重要性采样率过小 (小于 \\(1-\\varepsilon\\))，clip 会将其限制为 \\(1-\\varepsilon\\)，同样限制了更新幅度，避免策略发生剧烈变化。 目标函数取 clip 之前和 clip 之后的最小值，确保在重要性采样率超出范围时，PPO 会对策略更新进行惩罚，保证策略更新的“保守性”。\n在实际优化过程中，我们通常将 PPO 损失函数 \\(\\mathcal{L}_{PPO}(\\theta)\\) 定义为目标函数的负值，通过最小化损失来最大化目标函数：\n\\[ \\mathcal{L}_{PPO}(\\theta) = -\\,\\mathcal{J}_{PPO}(\\theta). \\]PPO 算法因其 简单有效、相对稳定 的特点，成为强化学习领域的基准算法之一，并在各种任务中取得了成功，包括大型语言模型的强化学习微调。PPO 通常被认为比早期的 TRPO 等方法更稳定，但在大模型上的具体应用仍需要细致的超参数调优。在大语言模型场景下，如果价值网络与策略网络完全分离且规模相当，势必会带来更多的计算与内存开销。为解决这些问题，DeepSeek 团队提出了组相对策略优化 (GRPO)算法。\nGRPO 组相对策略优化 (Group Relative Policy Optimization, GRPO) (Shao, et al., 2024) 是 DeepSeek 团队为训练 DeepSeek-R1-Zero 这样的大语言模型而专门设计的一种高效稳定的强化学习算法。GRPO 的核心创新在于摒弃了传统Actor-Critic 框架中对独立价值网络 (critic model) 的依赖，降低了计算成本，并提高了训练的稳定性。 从广义上讲，GRPO 可以被视为一种 Actor-Only 的强化学习方法。\nGRPO 的灵感来源于 相对评估 的思想。在许多实际场景中，我们往往更容易判断一组事物之间的相对好坏，而不是给出绝对的价值评估。例如，在评价一组学生的作业时，老师可能更容易比较不同作业之间的优劣，而不是给每份作业打一个绝对分数。GRPO 将这种相对评估的思想引入强化学习，通过 组内相对评分来构建基准 (baseline)，完全替代了对价值网络的依赖。\n具体而言，对于每个问题 \\( q \\)，GRPO 会从旧策略 \\( \\pi_{\\theta_{\\text{old}}} \\) 中采样一组输出 \\( \\{o_1, o_2, \\ldots, o_G\\} \\)，形成一个 输出组。然后，通过最大化下面的目标函数来更新策略模型 \\( \\pi_{\\theta} \\)：\n\\[ \\begin{aligned} \\mathcal{J}_{GRPO}(\\theta) \u0026 = \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O \\mid q)\\right] \\\\ \u0026 \\quad \\frac{1}{G} \\sum_{i=1}^G \\Biggl( \\min\\biggl( \\underbrace{\\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)}}_{\\text{重要性采样率}} \\,\\underbrace{A_i}_{\\text{相对优势值}},\\, \\operatorname{clip}\\Bigl( \\underbrace{\\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)}}_{\\text{重要性采样率}}, 1-\\varepsilon,\\, 1+\\varepsilon \\Bigr)\\,\\underbrace{A_i}_{\\text{相对优势值}} \\biggr) \\;-\\;\\beta\\,\\underbrace{\\mathbb{D}_{KL}\\bigl(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\bigr)}_{\\text{KL 散度惩罚项}} \\Biggr), \\end{aligned} \\]与 PPO 的目标函数类似，GRPO 的目标函数也包含重要性采样率和 clip，用于保证策略更新的稳定性。不同之处在于：\n相对优势值 \\(A_i\\)：GRPO 使用 相对优势值 \\(A_i\\) 代替 PPO 中的优势函数 \\(A_t\\)。相对优势值 \\(A_i\\) 是根据 组内奖励 计算得到的，无需价值网络估计。 KL 散度惩罚项 \\(\\mathbb{D}_{KL}\\bigl(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\bigr)\\)：为了进一步约束策略更新，GRPO 引入了 KL 散度惩罚项，限制新策略 \\(\\pi_\\theta\\) 与参考策略 \\(\\pi_{\\text{ref}}\\) 之间的差异过大。 Fig. 2. The comparison of PPO and GRPO. (Image source: DeepSeek-AI, 2024)\n从上图2我们可以看出GRPO 的核心创新在于 相对优势值 \\(A_i\\) 的计算方式。与 PPO 不同，GRPO 不依赖于独立的价值网络，而是直接利用 组内奖励 进行相对评估。对于每个输出组 \\( \\{o_1, o_2, \\ldots, o_G\\} \\)，GRPO 首先获取每个输出对应的奖励值 \\( \\{r_1, r_2, \\ldots, r_G\\} \\)。然后，根据以下公式计算相对优势值 \\( A_i \\)：\n\\[ A_i = \\frac{\\,r_i \\;-\\; \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})\\,}{ \\text{std}\\bigl(\\{r_1, r_2, \\ldots, r_G\\}\\bigr)}. \\]相对优势值 \\( A_i \\) 通过 标准化 组内奖励 \\( \\{r_1, r_2, \\ldots, r_G\\} \\) 得到，具有 零均值和单位方差，更好地反映了每个输出在组内的相对优劣程度。\nGRPO 采用 相对评估 的方式，具有以下优点：\n无需训练价值网络：避免了训练大规模价值网络带来的计算开销和不稳定性。 降低价值估计方差：相对评估关注组内输出的相对优劣，而不是绝对价值，降低了估计方差，提高了训练稳定性。 更符合奖励模型的比较特性：奖励模型通常基于比较数据训练，GRPO 的相对评估方式与之更契合。 更适用于序列生成任务的信用分配：即使奖励是稀疏的，GRPO 也能有效学习，因为它关注同组输出之间的相对好坏。 Schulman 无偏估计器 KL 散度 \\(\\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right)\\) 衡量了策略 \\(\\pi_\\theta\\) 相对于参考策略 \\(\\pi_{\\text{ref}}\\) 的信息损失，其标准定义为：\n\\[ \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) = \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\log \\frac{\\pi_\\theta(o \\mid q)}{\\pi_{\\text{ref}}(o \\mid q)} \\right]. \\]如前所述，直接计算上述期望在实际中面临挑战。为了解决这个问题，GRPO 采用了 Schulman 无偏估计器 (Schulman, 2020)。与公式中可能使用的 KL 散度惩罚项不同，我们使用以下无偏估计器来估计 \\(\\pi_\\theta\\) 和 \\(\\pi_{ref}\\) 之间的 KL 散度：\n$$ \\mathbb{D}_{K L}\\left[\\pi_{\\theta}| | \\pi_{r e f}\\right]=\\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}{\\pi_{\\theta}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}{\\pi_{\\theta}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}-1. $$为了理解这个估计器的优点，我们首先从数学上推导其无偏性。\n无偏性证明 为了简化符号，我们令 \\(r(o) = \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)}\\)。则 Schulman 估计器可以写成：\n\\[ \\hat{D}_{KL}(o) = r(o) - \\log r(o) - 1. \\]我们需要证明，当 \\(o\\) 从 \\(\\pi_\\theta(\\cdot|q)\\) 中采样时，\\(\\hat{D}_{KL}(o)\\) 的期望等于真实的 KL 散度 \\(\\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right)\\)。\n\\[ \\begin{aligned} \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} [\\hat{D}_{KL}(o)] \u0026= \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} [r(o) - \\log r(o) - 1] \\\\ \u0026= \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - 1 \\right] \\\\ \u0026= \\sum_{o} \\pi_\\theta(o \\mid q) \\left[ \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - 1 \\right] \\quad (\\text{离散情况，连续情况为积分}) \\\\ \u0026= \\sum_{o} \\left[ \\pi_{ref}(o \\mid q) - \\pi_\\theta(o \\mid q) \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\pi_\\theta(o \\mid q) \\right] \\\\ \u0026= \\underbrace{\\sum_{o} \\pi_{ref}(o \\mid q)}_{=1} - \\underbrace{\\sum_{o} \\pi_\\theta(o \\mid q) \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)}}_{=-\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})} - \\underbrace{\\sum_{o} \\pi_\\theta(o \\mid q)}_{=1} \\\\ \u0026= 1 - (-\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})) - 1 \\\\ \u0026= \\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref}). \\end{aligned} \\]因此，我们证明了 \\(\\hat{D}_{KL}(o)\\) 是 \\(\\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right)\\) 的无偏估计。\n三种 KL 散度估计器对比 为了直观理解三种估计器的差异，以下表格列出了它们的数学表达式，其中 \\( r(o) = \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\)：\n估计器 数学表达式 主要特点 k1 (朴素估计器) \\(\\hat{D}_{KL}^{(k1)}(o) = \\log \\frac{\\pi_\\theta(o \\mid q)}{\\pi_{ref}(o \\mid q)} = \\log \\frac{1}{r(o)}\\) 简单直接，对应 KL 散度定义；方差较高，估计结果波动较大。 k2 (平方对数比估计器) \\(\\hat{D}_{KL}^{(k2)}(o) = \\frac{1}{2} (\\log r(o))^2\\) 使用对数比的平方，始终为正，降低方差；引入偏差，尤其在分布差异大时。 k3 (Schulman 无偏估计器) \\(\\hat{D}_{KL}^{(k3)}(o) = r(o) - \\log r(o) - 1\\) 结合了比值 \\( r(o) \\) 和对数比 \\(\\log r(o)\\)；无偏，较低方差，估计稳定。 k1 (朴素估计器): 无偏简单直接，但方差较高，导致估计结果不稳定。 k2 (平方对数比估计器): 降低了方差，但引入了偏差，特别是在分布差异较大时偏差显著。 k3 (Schulman 无偏估计器): 兼具无偏性和较低的方差，提供了稳定的估计结果。 实验结果 为了评估三种 KL 散度估计器的性能，我们进行了数值实验，结果如下表所示。实验中，固定分布 \\( q = \\mathcal{N}(0, 1) \\)，通过改变分布 \\( p = \\mathcal{N}(\\mu, 1) \\) 的均值 \\(\\mu\\) 来控制真实的 KL 散度 \\(\\mathbb{D}_{KL}(p \\| q)\\)。使用5亿个样本进行 Monte Carlo 估计，并重复实验以获得稳定结果。\n实验代码可以参考 unbiased_kl_divergence.py\n真实 KL 散度 估计器 平均估计值 标准差 相对偏差 (%) 0.005 k1 0.005 0.1 0.0387 0.005 k2 0.005 0.0071 0.2415 0.005 k3 0.005 0.0071 -0.0082 0.125 k1 0.125 0.5 -0.0389 0.125 k2 0.1328 0.1875 6.2500 0.125 k3 0.125 0.1845 0.0072 0.5 k1 0.5 1 -0.0018 0.5 k2 0.625 0.866 25.0004 0.5 k3 0.5 0.8478 0.0052 朴素估计器 (k1):\n无偏性: 平均估计值与真实 KL 散度高度吻合，相对偏差接近 0%。 方差: 标准差高于 k3，且随着真实 KL 散度的增加而增大，导致估计结果不稳定。 平方对数比估计器 (k2):\n无偏性: 存在一定偏差，且偏差随真实 KL 散度的增加而显著增大（例如，真实 KL 为 0.5 时相对偏差达到 25%）。 方差: 在较低的真实 KL 散度下方差较低，但整体表现不稳定。 Schulman 无偏估计器 (k3):\n无偏性: 实验结果显示相对偏差极小，几乎为 0%，验证了其无偏性。 方差: 标准差明显低于 k1，且与 k1 相比在所有 KL 散度下均表现出更低的方差，尤其在较低 KL 散度时优势显著。 优点总结 无偏性: 理论和实验结果均表明，k3 是无偏估计器，能够准确反映真实的 KL 散度。 正定性: 估计值始终为非负，符合 KL 散度的性质。 较低的方差: 相较于 k1，k3 显著降低了估计方差，提供了更稳定的估计结果，尤其在 KL 散度较小时表现突出。 Schulman 无偏估计器 \\( \\hat{D}_{KL}^{(k3)}(o) = r(o) - \\log r(o) - 1 \\) 为 KL 散度提供了一种兼具无偏性和低方差的估计方法。其无偏性确保了估计的准确性，而较低的方差提升了估计的稳定性，特别适用于需要稳定梯度信号的强化学习场景，如策略优化。基于这些优势，GRPO 算法选择使用 k3 作为惩罚策略偏离的估计器，从而保证训练过程的稳定性和最终策略的性能。\n在实际优化中，GRPO 损失函数 \\(\\mathcal{L}_{GRPO}(\\theta)\\) 被定义为目标函数 \\(\\mathcal{J}_{GRPO}(\\theta)\\) 的负值，通过最小化损失函数 \\(\\mathcal{L}_{GRPO}(\\theta)\\) 来实现目标函数 \\(\\mathcal{J}_{GRPO}(\\theta)\\) 的最大化：\n\\[ \\mathcal{L}_{GRPO}(\\theta) = -\\,\\mathcal{J}_{GRPO}(\\theta) \\]PPO 与 GRPO 对比 为更清晰理解 PPO 和 GRPO 的异同，以下表格对两种算法进行对比：\n特性 PPO GRPO 是否 Actor-Critic 是 是 (广义上可以认为是 Actor-Only) 是否价值网络 需要独立的价值网络 (Critic) 无需独立的价值网络 优势函数估计 通过价值网络估计绝对优势值 通过组内奖励相对评估相对优势值 计算开销 较高，需要训练价值网络 较低，无需训练价值网络 训练稳定性 相对较好，但价值网络训练可能引入不稳定性 更好，避免了价值网络训练带来的不稳定性 算法复杂度 相对复杂，需要维护和更新策略网络和价值网络 相对简单，只需维护和更新策略网络 适用场景 广泛适用于各种强化学习任务，包括中小规模语言模型微调 特别适用于大语言模型的强化学习微调，注重效率和稳定性 信用分配 依赖价值网络进行时间差分学习，处理信用分配问题 依赖最终奖励和组内相对评估，也可辅助中间奖励 方差问题 价值网络估计可能引入方差 组内相对优势估计在小组规模下可能存在方差，可通过增大组规模等缓解 从表中可以看出，PPO 是一种通用且强大的强化学习算法，但其训练价值网络的机制在大语言模型场景下带来了额外的计算负担和潜在的不稳定性。GRPO 通过引入组相对评分，巧妙地规避了对价值网络的需求，在保证性能的同时，显著降低了计算成本，并提升了训练稳定性。这使得 GRPO 成为在训练资源不多的情况下训练 DeepSeek-R1-Zero 这样 LLM 的理想选择。\n代码生成评估指标 代码生成会采用更严谨的测试方法。通过编译器执行模型生成的代码，并使用预定义的测试用例进行多次单元测试，以判断代码的正确性。常用的评估指标包括 pass@k(Chen et al., 2021) 和 cons@N(OpenAI, 2024)。\npass@k: 衡量模型在生成 k 个代码样本时，至少有一个样本能够通过所有预定义测试用例的概率。\npass@k 有偏估计公式 \\[ \\text{Simplified pass@k} = \\frac{1}{P} \\sum_{i=1}^{P} C_i \\]\\[ C_i = \\begin{cases} 1 \u0026 \\text{如果生成的 k 个样本中至少有一个是正确的} \\\\ 0 \u0026 \\text{如果生成的 k 个样本全部都不正确} \\end{cases} \\]参数说明:\n\\( P \\): 评估的问题总数。 \\( C_i \\): 对于第 \\(i\\) 个问题，如果生成的 \\(k\\) 个样本中至少有一个是正确的，则 \\(C_i = 1\\)，否则 \\(C_i = 0\\)。 \\( \\sum_{i=1}^{P} C_i \\): 表示在所有 \\(P\\) 个问题中，被 “解决” 的问题总数。 \\( \\frac{1}{P} \\sum_{i=1}^{P} C_i \\): 表示 “解决” 问题的比例，即准确率。 公式含义: 这种简化方法直接计算 生成 k 个样本后，至少有一个样本正确的比例。 虽然这种方法提供的是 pass@k 的 有偏估计，可能会略微高估真实值，但它在实践中非常常用，因为它 直观、易于计算，并且在样本量足够大时，能够提供对模型性能的合理近似。尤其在工业界和快速评估场景中，这种简化方法非常实用。\n然而，LLM 在推理解码时会受到 temperature、top_p（核采样概率）、top_k（候选词数量）和repetition_penalty 等参数的影响。这些参数会使代码生成结果有随机性和多样性，并且当样本 K 比较少的如果设置随机过高的参数，会影响 pass@k 的评估结果。因此，采用无偏估计方法能够更准确地反映模型的真实性能。\npass@k 的无偏估计公式 \\[ \\text { pass @ } k:=\\underset{\\text { Problems }}{\\mathbb{E}}\\left[1-\\frac{\\binom{n-c}{k}}{\\binom{n}{k}}\\right] \\]参数说明:\n\\( n \\): 为每个问题生成的代码样本总数。 \\( c \\): 在 \\( n \\) 个样本中，能够通过所有单元测试的正确样本数。 \\( k \\): pass@\\(k\\) 指标中的参数 \\(k\\)，表示我们考虑的生成样本数量。 \\( \\binom{a}{b} \\): 表示二项式系数，计算从 \\(a\\) 个元素中选择 \\(b\\) 个元素的组合数。 \\( \\underset{\\text { Problems }}{\\mathbb{E}} \\): 表示对所有评估问题的期望值（平均值）。 公式含义:\n公式实际上计算的是至少有一个正确样本的概率。公式 \\( \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\) 计算的是在生成的 \\(n\\) 个样本中，随机抽取 \\(k\\) 个样本，且这 \\(k\\) 个样本都不正确的概率。我们用 1 减去这个概率，就得到了 在 \\(n\\) 个样本中，随机抽取 \\(k\\) 个样本，且这 \\(k\\) 个样本中至少有一个正确的概率， 这就是 pass@\\(k\\) 指标的含义。 这个公式提供的是 pass@k 的 无偏估计，更适用于学术研究等需要精确评估的场景。 在实际计算中，通常会生成远大于 \\(k\\) 的样本数 \\(n\\) (例如论文中使用 \\(n=200\\), \\(k \\leq 100\\))，以更稳定地估计 pass@\\(k\\)。 pass@k 简化乘积形式 为了更方便数值计算，原始公式还可以转化为以下乘积形式，它仍然是无偏估计，并能避免数值溢出问题：\n\\[ \\text { pass @ } k = \\underset{\\text { Problems }}{\\mathbb{E}}\\left[1 - \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i}\\right] \\]推导过程:\n至少有一个正确样本的反面是所有 k 个样本都不正确。 因此，pass@k 等于 1 减去 所有 k 个样本都不正确的概率。\n考虑不放回抽样的场景。假设我们从 \\(n\\) 个样本中抽取 \\(k\\) 个样本，要计算这 \\(k\\) 个样本都不正确的概率。总共有 \\(n\\) 个样本，其中 \\(n-c\\) 个是不正确的。\n第一次抽取时，抽到不正确样本的概率为 \\( \\frac{n-c}{n} \\)。\n在第一次抽取到不正确样本的条件下，第二次抽取时，剩余 \\(n-1\\) 个样本中，有 \\(n-c-1\\) 个不正确样本。因此，第二次仍然抽到不正确样本的条件概率为 \\( \\frac{n-c-1}{n-1} \\)。\n以此类推，第 \\(i\\) 次抽取时 ( \\(i\\) 从 1 到 \\(k\\) )，在之前 \\(i-1\\) 次都抽到不正确样本的条件下，第 \\(i\\) 次仍然抽到不正确样本的条件概率为 \\( \\frac{n-c-(i-1)}{n-(i-1)} = \\frac{n-c-i+1}{n-i+1} \\)。 为了与公式中的索引 \\(i=0\\) 对齐，我们将索引改为从 \\(i=0\\) 到 \\(k-1\\)，则第 \\(i+1\\) 次抽取时 ( \\(i\\) 从 0 到 \\(k-1\\) )，条件概率为 \\( \\frac{n-c-i}{n-i} \\)。\n将这 \\(k\\) 次抽取的条件概率连乘，即可得到所有 \\(k\\) 个样本都不正确的概率：\n\\[ P(\\text{所有 k 个样本都不正确}) = \\frac{n-c}{n} \\times \\frac{n-c-1}{n-1} \\times \\cdots \\times \\frac{n-c-k+1}{n-k+1} = \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i} \\] 最终，pass@k 的简化公式为：\n\\[ \\text { pass @ } k = \\underset{\\text { Problems }}{\\mathbb{E}}\\left[1 - \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i}\\right] \\] 这个乘积形式的公式，避免了直接计算可能数值很大的二项式系数，更易于理解和数值计算，尤其是在编程实现时，可以逐项累乘，有效防止数值溢出。\ncons@N cons@N: 通过生成 N 个样本，并从中选择出现频率最高的答案作为最终答案，评估该答案的准确率。在 DeepSeek-R1-Zero 的评估中，使用了 cons@64，即生成 64 个样本，并取其中出现次数最多的答案作为最终答案进行评估。\n\\[ \\text{cons@N} = \\frac{1}{P} \\sum_{i=1}^{P} \\mathbb{I}(\\text{ConsensusAnswer}_i \\text{ is correct}) \\]参数说明:\n\\( P \\)：评估的问题总数。 \\( \\text{ConsensusAnswer}_i \\)：通过多数投票得到的 共识答案。 \\( \\mathbb{I}(\\text{ConsensusAnswer}_i \\text{ is correct}) \\)：指示函数，若共识答案正确，则为 1，否则为 0。 公式含义： 计算在所有评估问题中，共识答案正确的比例。通过增加生成样本数 \\(N\\)，并采用多数投票策略，cons@N 指标能够更稳定和可靠地评估模型的平均性能。在模型生成结果存在一定随机性的情况下，该指标可以验证模型输出的一致性和准确性。\n奖励模型 奖励模型在 LLM 的研发中至关重要，主要应用于以下关键环节：\n基于人类反馈的强化学习: 在基于人类反馈的强化学习（RLHF）流程中，奖励模型用于评估模型生成结果的质量，并为后续的强化学习提供奖励信号。\n拒绝采样的关键工具: 在拒绝采样过程中，奖励模型对大量候选结果进行评分，筛选出高质量样本用于监督微调（SFT）。拒绝采样是自动化样本工程的重要方法，而奖励模型是其核心组成部分。\n业务场景中的判别器: 在实际应用中，奖励模型作为 LLM 输出结果的判别器或校验器，评估生成结果的质量。只有得分超过预设阈值的结果才会输出，否者进行再生成或降级处理，提高输出的可靠性和安全性。\nORM 与 PRM Fig. 3. Outcome reward vs Process reward. (Image source: Zeng et al., 2024)\n当前奖励模型主要分为两种范式：结果奖励模型(Outcome Reward Model, ORM) 和 过程奖励模型(Process Reward Model, PRM)。上图3直观的展示了这两种奖励模型的区别。以下表格也对比了这两种模型的主要特性：\n特性 ORM PRM 定义 对模型生成的完整结果进行整体评分 在内容生成过程中，对每一步或每个阶段进行细粒度评分 主要优势 简单直接，易于实现 对整体结果进行全面评估 提供更精细的奖励信号 有助于指导模型生成过程的每个步骤 主要劣势 方差较高，估计结果波动较大 缺乏过程中的反馈 训练和应用更为复杂 可能引入偏差，尤其在分布差异大时 适用场景 需要整体评估生成结果的任务 需要细粒度控制生成过程的任务，如分步推理或复杂生成任务 避免奖励欺骗的能力 中等，依赖于整体评分的准确性 较低，可通过优化每一步的奖励而非整体表现来作弊 训练复杂度 较低，无需对生成过程进行额外的监督 较高，需要在生成的每一步进行评分，增加了计算和数据需求 可解释性 高，评分基于最终结果 较低，评分涉及生成过程的多个步骤，难以全面理解每一步的评分依据 采用 ORM 为了训练 DeepSeek-R1-Zero，DeepSeek 团队选择了ORM，而非PRM。此选择基于以下考虑：\n避免奖励欺骗（Reward Hacking）\nPRM在大规模 RL 训练中，容易被智能体利用，导致奖励欺骗（Gao et al., 2022）。模型可能采取“旁门左道”的策略以最大化奖励，而非提升推理能力。基于规则的奖励系统通过明确且可解释的规则，有效避免了奖励欺骗问题。\n基于规则的奖励系统在问题场景复杂或需要创造性回答时，可能难以覆盖所有类型的问题，规则设计可能存在漏洞被模型利用。\n降低训练复杂度\n训练 PRM 需要大量计算资源和数据，增加了训练流程的复杂性。而基于规则的奖励系统无需额外训练，规则一旦确定即可直接应用，简化了训练流程。基于规则的奖励系统特别适合自动判分或目标明确的任务，如数学题、LeetCode 编程题及对输出格式有明确要求的任务。对于开放式对话或创意类任务，则可能需要结合人类反馈或训练好的奖励模型。\n奖励机制 DeepSeek-R1-Zero 的奖励系统采用双重奖励机制，通过预定义的规则进行自动化评估，确保评估过程的高效性和实时性。这套系统主要包含以下两种类型的奖励：\n1. 准确性奖励 (Accuracy Reward)\n定义： 衡量模型输出结果的正确性，是奖励系统中最关键的部分。 实现方式： 根据不同任务类型采用不同的验证方法： 数学问题： 验证最终答案是否与标准答案一致。 代码生成： 通过编译器执行模型生成的代码，并使用预设的单元测试用例进行多次测试，判断代码的正确性。 目的： 引导模型生成准确、可靠的输出结果。 2. 格式奖励 (Format Reward)\n定义： 为了提升模型输出的可读性和结构性，方便后续分析和评估而引入的奖励机制。 评估方式： 在强化学习训练过程中，通过预定义的规则系统进行自动化评估。 目的： 鼓励模型生成结构化的输出，例如包含思考过程和最终答案，使其更易于理解和分析。 DeepSeek-R1-Zero 的奖励函数 \\(r(o)\\) 由准确性奖励和格式奖励加权求和构成：\n$$ r(o) = r_{\\text{accuracy}}(o) + \\lambda \\cdot r_{\\text{format_effective}}(o) $$其中，有效格式奖励 \\(r_{\\text{format_effective}}(o)\\) 的计算方式如下：\n$$ r_{\\text{format_effective}}(o) = \\begin{cases} r_{\\text{format}}(o) \u0026 \\text{如果 } o \\text{ 的基本格式符合要求} \\\\ 0 \u0026 \\text{如果 } o \\text{ 的基本格式不符合要求} \\end{cases} $$基础格式奖励 \\(r_{\\text{format}}(o)\\) 则根据格式规范的符合程度进行分级：\n$$ r_{\\text{format}}(o) = \\begin{cases} R_{\\text{format_full}} \u0026 \\text{如果 } o \\text{ 的格式完全符合规范} \\\\ R_{\\text{format_partial}} \u0026 \\text{如果 } o \\text{ 的格式部分符合规范} \\\\ 0 \u0026 \\text{如果 } o \\text{ 的格式不符合规范} \\end{cases} $$实验流程 训练模板 为了引导基模型遵循指定的指令，DeepSeek 团队设计了一个简洁而有效的训练模板。该模板要求模型首先生成推理过程（放在 \u0026lt;think\u0026gt; 和 \u0026lt;/think\u0026gt; 标签之间），然后再提供最终答案（放在 \u0026lt;answer\u0026gt; 和 \u0026lt;/answer\u0026gt; 标签之间）。这种结构化的格式，不仅确保了输出的可读性，更使研究人员能够清晰地观察模型在 RL 训练过程中的推理过程，从而更准确地评估模型的学习进展。\n角色 提示内容 助手回复 用户 prompt (用户提出的问题) 助手： \u0026lt;think\u0026gt; 推理过程 \u0026lt;/think\u0026gt; \u0026lt;answer\u0026gt; 答案 \u0026lt;/answer\u0026gt; \u0026lt;think\u0026gt; 和 \u0026lt;/think\u0026gt; (思维过程标签): 用于包裹模型的中间推理步骤，清晰展示模型的思考过程，便于理解模型的推理逻辑和进行错误分析。 \u0026lt;answer\u0026gt; 和 \u0026lt;/answer\u0026gt; (最终答案标签): 用于包裹模型的最终答案，方便程序自动化提取答案部分，进行高效的评估和后续处理。 评估流程 准确性评估： 评估模型输出 \\(o\\) 的答案是否正确，计算准确性奖励 \\(r_{\\text{accuracy}}(o)\\)。 基本格式检查： 检查输出 \\(o\\) 的基本格式是否符合预定义要求，例如是否包含必要的标签 \u0026lt;think\u0026gt; 和 \u0026lt;answer\u0026gt;，以及标签是否正确闭合和嵌套。 有效格式奖励判断： 基本格式不符合： 有效格式奖励 \\(r_{\\text{format_effective}}(o) = 0\\)。 基本格式符合： 进一步评估格式规范程度，计算基础格式奖励 \\(r_{\\text{format}}(o)\\)。 最终奖励计算： 将准确性奖励 \\(r_{\\text{accuracy}}(o)\\) 和有效格式奖励 \\(r_{\\text{format_effective}}(o)\\) 进行线性加权求和，得到最终奖励 \\(r(o)\\)。 通过结合准确性奖励和格式奖励，DeepSeek-R1-Zero 的奖励系统不仅关注模型输出的正确性，更重视输出结果的结构化和可读性。这使得模型不仅能够给出正确的答案，还能展现其思考过程，使其更像一个具备推理能力的智能体，而不仅仅是一个简单的答案输出机器。\n实验结果 Fig. 4. Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. (Image source: DeepSeek-AI, 2025)\n图4展示了不同模型在多项基准测试上的表现。在 AIME 2024 基准测试中，DeepSeek-R1-Zero 模型的 pass@1 分数达到了71.0%，此外 cons@64 分数为86.7%，与 OpenAI o1-0912 模型相当。\nFig. 5. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)\n图5显示了随着训练的深入，DeepSeek-R1-Zero 模型展现出自发 自我进化 的能力。模型根据问题的复杂程度，动态地分配“思考时间”，对于更复杂的问题，会自发生成更长的推理链，进行更深入的思考。这种“思考时间”自适应调整，并非人为设定，而是模型在 RL 训练过程中自发涌现的行为，充分体现了强化学习驱动下，模型推理能力的自主提升。\nDeepSeek-R1 训练流程 为了在 DeepSeek-R1-Zero 的基础上进一步提升模型性能，DeepSeek 团队采用了 多阶段训练 策略，并将 冷启动数据 融入到训练流程中。DeepSeek-R1 的训练流程主要包括以下四个阶段，体现了从初步策略初始化到全面能力提升的进阶之路：\n冷启动 (Cold Start): 利用高质量的长思维链 (CoT) 数据，对 DeepSeek-V3-Base 基模型进行初步的监督微调，为后续强化学习奠定基础。\n面向推理的强化学习 (Reasoning-Oriented RL): 在冷启动模型基础上，应用强化学习算法，专注于增强模型在推理密集型任务中的能力。\n拒绝采样与监督微调 (Rejection Sampling \u0026amp; SFT): 通过拒绝采样技术筛选高质量推理数据，并结合非推理数据进行监督微调，进一步提升模型推理能力和通用能力。\n面向所有场景的强化学习 (All-Scenario RL): 综合考虑推理和非推理任务，进行第二阶段强化学习，使模型与人类偏好对齐，提升在更广泛场景下的表现。\n冷启动 在 DeepSeek-R1 的训练流程中，冷启动阶段至关重要，它如同引擎的点火器，为后续复杂的强化学习过程奠定坚实基础。监督微调是冷启动阶段的核心技术。\n训练目标 冷启动阶段的目标明确而关键：利用高质量的思维链 (Chain-of-Thought, CoT) 数据，对 DeepSeek-V3-Base 基模型进行初步微调。这次微调旨在快速赋予模型以下核心能力：\n初步推理能力： 引导模型学习模仿人类的推理过程，为更复杂的推理打下基础。 良好文本生成质量： 确保模型输出文本的流畅性和自然度，提升用户体验。 这些 CoT 数据如同模型的“启动燃料”，帮助模型快速掌握人类的推理模式，并为后续强化学习提供良好的策略初始化，有效避免 RL 训练初期从零开始探索的低效和不稳定性。\n数据构建 为了构建高质量的冷启动数据，DeepSeek 团队进行了多方探索，最终整合了以下高效方法：\n少量示例引导 (Few-shot Prompting)： 利用少量高质量的示例，引导模型生成更长、更具深度和逻辑性的 CoT 数据。 模型生成 + 反思验证： 直接 Prompt 模型生成答案，并加入反思和验证环节，确保答案的质量和推理的正确性。 优化 R1-Zero 输出： 收集 DeepSeek-R1-Zero 模型的输出，通过人工标注和优化，提升数据的可读性和整体质量。 通过上述策略，DeepSeek 团队积累了数千条高质量的冷启动数据，并以此为基础对 DeepSeek-V3-Base 进行了微调，作为强化学习的坚实起点。\n冷启动优点 相比于直接以 DeepSeek-R1-Zero 作为起点，冷启动数据带来了多项显著优势，为后续训练奠定了更优的基础：\n显著提升可读性 (Improved Readability)：\nDeepSeek-R1-Zero 的输出存在可读性挑战，例如语言混合、缺乏结构化格式等。 冷启动数据特别设计了更易读的输出模式，包括： 添加摘要 (Summary)： 在回复末尾添加精炼的摘要，快速提炼核心结论。 过滤不良回复： 去除不友好或低质量的回复，确保数据纯净度。 结构化输出格式： 采用 | special_token | \u0026lt;reasoning_process\u0026gt; | special_token | \u0026lt;summary\u0026gt; 格式，清晰呈现推理过程和总结。 性能显著提升 (Enhanced Performance)：\n通过精心设计融入人类先验知识的数据模式，DeepSeek 团队观察到模型性能相较 R1-Zero 有了显著提升。 这进一步验证了迭代训练是提升推理模型性能的有效路径。 更优的策略初始化 (Superior Policy Initialization)：\n冷启动阶段的 SFT 核心在于策略初始化。 策略初始化是构建 Reasoing LLM，例如 OpenAI o1 系列的关键步骤。通过学习高质量 CoT 数据，模型初步掌握人类推理模式，并具备生成结构化推理过程的能力，为后续强化学习训练奠定坚实基础，避免了从零开始探索的困境。 监督微调 监督微调 (Supervised Fine-tuning, SFT) 的核心目标是通过在有监督标注的数据上微调模型，使其预测结果尽可能接近真实标签。 这旨在提升模型在特定任务和指令执行方面的能力。\n损失函数 SFT 的训练目标是最小化模型预测与真实标签之间的差异。损失函数通常采用交叉熵损失 (Cross-Entropy Loss)，也称为负对数似然 (Negative Log-Likelihood)，用于衡量模型预测 token 分布与真实 token 分布之间的差异。 为了平衡不同长度输出序列的贡献，我们通常会将损失函数归一化到每个 token 的平均损失。\n损失函数公式如下：\n\\[ \\mathcal{L}_{SFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\log \\pi_\\theta\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]参数说明:\n\\(\\mathcal{L}_{SFT}(\\theta)\\)：SFT 损失函数，通过调整模型参数 \\(\\theta\\) 最小化该函数。 \\(\\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}[\\cdot]\\)：在 SFT 数据集分布 \\(P_{sft}(Q, O)\\) 上的期望。 \\(P_{sft}(Q, O)\\)：SFT 数据集分布，\\(q\\) 代表问题 (Query)，\\(o\\) 代表对应的标准答案输出 (Output)。 \\((q, o)\\)：从 SFT 数据集中采样的 问题-答案对。 \\(|o|\\)：标准答案输出的 token 长度。 \\(o_t\\)：标准答案输出的第 \\(t\\) 个 token。 \\(o_{\u0026lt;t} \\)：标准答案输出的前 \\(t-1\\) 个 tokens。 \\(\\pi_\\theta\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\)：给定问题 \\(q\\) 和前文 \\(o_{\u0026lt;t} \\)，模型预测 token \\(o_t\\) 的概率。 \\(\\frac{1}{|o|}\\): 长度归一化因子，将总损失除以输出序列长度，得到每个 token 的平均损失。 SFT 损失函数旨在惩罚模型预测与标准答案之间的偏差。 对于给定的问题 \\(q\\) 和标准答案 \\(o\\)，损失函数计算模型预测答案 \\(o\\) 中每个 token \\(o_t\\) 的概率 \\(\\pi_\\theta(o_t | q, o_{\u0026lt;t} )\\)。通过除以输出长度 \\(|o|\\)，损失函数被归一化为每个 token 的平均负对数似然。\n模型准确预测标准答案 token 时，\\(\\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\approx 1\\)，\\(\\log \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\approx 0\\)，损失值接近最小值。 模型预测偏离标准答案时，\\(\\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\) 较小，\\(\\log \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} )\\) 为负数且绝对值较大，损失值增大。 最小化 SFT 损失函数的过程，就是让模型学习生成与训练数据集中标准答案尽可能相似文本的过程。从负对数似然角度看，目标是找到最优模型参数 \\(\\theta\\)，最大化模型生成训练数据答案 \\(o\\) 的概率，等价于最小化生成答案 \\(o\\) 的负对数似然。高质量的 CoT 数据蕴含人类对推理和结果的偏好，因此 SFT 也可视为让模型学习并拟合人类推理偏好的过程。\n梯度 SFT 损失函数的梯度用于指导模型参数更新，以降低损失值。 损失函数关于模型参数 \\(\\theta\\) 的梯度为：\n\\[ \\nabla_{\\theta} \\mathcal{L}_{SFT} = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]参数说明:\n\\(\\nabla_{\\theta} \\mathcal{L}_{SFT}\\)：SFT 损失函数关于参数 \\(\\theta\\) 的梯度，指示损失函数值下降最快的方向。 \\(\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\): token 概率对数 \\(\\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\) 关于参数 \\(\\theta\\) 的梯度。 \\(\\frac{1}{|o|}\\): 长度归一化因子，与损失函数保持一致，梯度也是每个 token 平均损失的梯度。 实际计算梯度时，通常使用随机梯度下降算法，沿着梯度下降方向更新模型参数，逐步最小化损失函数，提升模型生成标准答案的准确性。\n梯度系数\n在 SFT 阶段，梯度系数通常设置为 1，这意味着所有训练样本对模型参数的更新贡献相同，模型平等地学习每个示例，力求最小化在整个数据集上的平均损失。\n数据来源与人类偏好 数据来源 (Data Source)：SFT 数据集主要由高质量的长链思维 (CoT) 示例构成，代表了期望模型学习的“标准答案”，用于指导损失函数最小化。 数据可能来自人工标注或更强大的模型生成。可参考 Open-o1 项目的 SFT 数据集 OpenO1-SFT，包含长 CoT 回复。 人类偏好 (Human Preference)：在 SFT 阶段，人类选择可以被视为隐式的奖励函数。 高质量 CoT 数据体现了人类对模型推理和输出的期望，模型通过学习这些数据，最小化与人类期望输出的偏差，从而拟合人类偏好。 面向推理的强化学习 在冷启动微调后，DeepSeek 团队通过强化学习 (RL) 进一步提升模型在推理密集型任务（如编码、数学、科学和逻辑推理）中的能力。 此阶段的核心在于最大化奖励函数，引导模型学习更有效的推理策略。\n奖励函数 为了解决推理过程中 CoT 语言混合问题，DeepSeek 团队引入了语言一致性奖励，并将其与任务奖励结合，构成总奖励函数：\n\\[ r(o) = r_{\\text{task}}(o) + \\alpha \\cdot r_{\\text{lang_consistency}}(o) \\]参数说明:\n\\(r(o)\\)：总奖励函数，RL 训练的目标是最大化该函数。 \\(r_{\\text{task}}(o)\\)：基于任务完成情况的任务奖励，衡量模型推理的准确性。 \\(r_{\\text{lang_consistency}}(o)\\)：语言一致性奖励，衡量 CoT 输出的语言纯度。 \\(\\alpha\\)：超参数，平衡任务奖励和语言一致性奖励的权重。 总奖励函数是任务奖励和语言一致性奖励的加权和。最大化 \\(r(o)\\) 驱动模型在提升推理准确性的同时，保持 CoT 输出的语言一致性。 \\(\\alpha\\) 的作用是调整模型对语言一致性的重视程度。\n训练目标 通过最大化上述奖励函数，DeepSeek 团队在冷启动微调后的模型上进行 RL 训练，优化模型参数，使其在推理任务上获得更高的奖励值，最终提升推理能力。\nRFT 拒绝采样微调 (Rejection Sampling Fine-tuning, RFT) 旨在通过精炼训练数据提升模型通用能力。其核心思想是最小化选择性损失函数，引导模型学习高质量输出的生成模式。\n损失函数 RFT 采用拒绝采样策略，区分推理数据和非推理数据的生成与选择过程，构建高质量 SFT 数据集。训练目标是最小化以下损失函数：\n\\[ \\mathcal{L}_{RFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q) \\times \\pi_{sft}(O \\mid q)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\mathbb{I}(o) \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]其中，指示函数 \\(\\mathbb{I}(o)\\) 定义为：\n\\[ \\mathbb{I}(o) = \\begin{cases} 1, \u0026 \\text{如果输出 } o \\text{ 被判定为高质量} \\\\ 0, \u0026 \\text{否则} \\end{cases} \\]参数说明:\n\\(\\mathcal{L}_{RFT}(\\theta)\\): RFT 损失函数。 \\(P_{sft}(Q)\\): 问题 \\(q\\) 的分布。 \\(\\pi_{sft}(O \\mid q)\\): 给定问题 \\(q\\)，SFT 模型生成输出 \\(O\\) 的条件概率分布。 \\(\\mathbb{I}(o)\\): 指示函数，用于选择高质量答案。当输出 \\(o\\) 被判定为高质量时为 1，否则为 0。 RFT 损失函数基于交叉熵损失，通过指示函数 \\(\\mathbb{I}(o)\\) 选择性地学习高质量输出：\n高质量输出 (\\(\\mathbb{I}(o) = 1\\)): 损失函数退化为标准交叉熵损失，模型根据高质量答案的负对数似然更新参数，最小化模型预测与高质量答案的差异。 低质量输出 (\\(\\mathbb{I}(o) = 0\\)): 损失函数为零，低质量答案不参与参数更新。 RFT 通过最小化损失函数，引导模型专注于学习高质量答案的生成模式，实现选择性学习。\n数据生成 高质量数据 (推理数据): 通过 RL 模型生成候选答案，使用奖励模型（或 DeepSeek-V3 模型）评分，拒绝采样保留高分答案。 SFT 数据 (非推理数据): 复用 DeepSeek-V3 的 SFT 数据集及其生成流程。 训练过程 使用拒绝采样得到的高质量数据集，对 DeepSeek-V3-Base 模型进行监督微调，最小化 RFT 损失函数，提升模型推理和通用能力。\nRFT 迭代精炼数据和重训练模型，期望模型在每轮迭代学习更高质量数据模式，最终收敛到高质量输出模型。 迭代过程中，训练数据分布 \\(P_{sft}(Q, O)\\) 逐渐聚焦于高质量数据，使模型在损失最小化过程中不断提升生成高质量输出的能力。\nOnRFT 在线拒绝采样微调 (Online Rejection Sampling Fine-tuning, OnRFT) 目标与 RFT 类似，都是通过最小化选择性损失函数学习高质量输出模式。OnRFT 与 RFT 的主要区别在于数据采样方式，损失函数形式与 RFT 保持一致。OnRFT 的损失函数梯度为：\n\\[ \\nabla_{\\theta} \\mathcal{L}_{OnRFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q) \\times \\pi_{\\theta}(O \\mid q)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\mathbb{I}(o) \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]参数说明:\n\\(\\nabla_{\\theta} \\mathcal{L}_{OnRFT}\\): OnRFT 损失函数关于模型参数 \\(\\theta\\) 的梯度，指示损失函数下降方向。 \\(\\pi_{\\theta}(O \\mid q)\\): 给定问题 \\(q\\)，当前训练模型 生成输出 \\(O\\) 的条件概率分布。 RFT 与 OnRFT 对比 下面表格简单对比了 RFT 和OnRFT 的主要区别。\n特性 RFT OnRFT 数据生成方式 离线 (Offline) 在线 (Online) 数据生成模型 SFT 模型 \\(\\pi_{sft}\\) 当前训练模型 \\(\\pi_{\\theta}\\) 拒绝采样数据来源 预生成 SFT 数据集 训练时实时生成数据 数据循环 分离 在线循环 损失函数机制 选择性交叉熵损失，选择高质量输出学习 选择性交叉熵损失，选择高质量输出学习 训练数据分布变化 逐渐聚焦于高质量数据 动态变化，贴合当前模型能力 面向所有场景的强化学习 为了进一步对齐人类偏好，DeepSeek 团队进行了第二阶段 RL，目标是在最大化奖励函数的同时，提升模型的有用性 (Helpfulness) 和 无害性 (Harmlessness)，并兼顾推理能力。此阶段仍然是通过最大化奖励函数来指导模型训练，但奖励函数的设计更加复杂，以反映多维度的优化目标。\n此阶段的 RL 训练结合了：\n多样化的 Prompt 分布: 覆盖更广泛的场景，包括推理和通用任务。 多目标奖励信号: 推理数据: 沿用基于规则的任务奖励，侧重推理准确性。最大化任务奖励，引导模型最小化推理错误。 通用数据: 使用奖励模型捕捉人类对有用性和无害性的偏好。奖励模型的目标是学习人类偏好，并输出与人类偏好一致的奖励信号，RL 训练的目标是最大化奖励模型给出的奖励值，从而间接最小化模型输出与人类偏好之间的偏差。 蒸馏 为了将 DeepSeek-R1 的强大推理能力迁移到更高效的小型模型上，DeepSeek 团队采用了蒸馏（Distillation）（Hinton et al., 2015）技术。蒸馏过程主要包括以下步骤：\n数据生成 (Data Generation): 利用训练好的 DeepSeek-R1 模型，生成约 80 万条高质量的推理数据。这些数据不仅包括推理密集型任务 (如数学题、编程题)，也涵盖了通用任务 (如问答、对话)，以保证蒸馏数据的多样性和覆盖面。\n模型微调 (Model Fine-tuning): 将生成的 80 万条高质量推理数据，用于微调小型密集模型。蒸馏实验选择了 Qwen 和 Llama 系列模型作为 Student 模型，涵盖了从 1.5B 到 70B 参数的多种模型规模，以探索蒸馏技术在不同模型规模下的效果。选取的 Student 模型包括 Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, 和 Llama-3.3-70B-Instruct。\n性能评估 (Performance Evaluation): 在多个推理相关的 benchmark 中，对蒸馏后的模型进行全面的性能评估。评估结果旨在验证蒸馏技术是否能够有效地将大型模型的推理能力迁移到小型模型，并考察蒸馏后的小型模型在推理能力上是否能够达到甚至超越大型模型的水平。\nKL 散度蒸馏 除了直接使用 Teacher 模型生成的文本输出作为伪标签进行 SFT 蒸馏外，更严谨的方法是将 Teacher 模型生成的 token 概率分布 \\(\\pi_{\\text{teacher}}\\) 也纳入考虑。KL 散度蒸馏 是一种常用的方法，它不仅让 Student 模型学习 Teacher 模型的文本输出，也学习 Teacher 模型的 token 概率分布。通过最小化 Student 模型和 Teacher 模型输出概率分布之间的 KL 散度，可以更充分地将 Teacher 模型的知识迁移到 Student 模型中。但在实际工程中，直接使用 Teacher 模型的文本输出作为伪标签进行 SFT 蒸馏，通常也能取得足够好的效果，并且实现更简单。\n实验结果 实验结果如图6所示：\nFig. 6. Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. (Image source: DeepSeek-AI, 2025)\n实验结果表明，这种直接的 SFT 蒸馏方法能够显著提升小型模型的推理能力。特别是蒸馏之后的Llama-3.3-70B-Instruct模型在多个 benchmark 上优于 o1 模型，仅通过 SFT 蒸馏就取得了如此显著的效果。这里博主有些怀疑模型在这几个 benchmark 上过拟合了或者存在数据泄漏的风险，后续还需要在更多的 benchmark 上进行测试。\n讨论 DeepSeek-R1 在多阶段训练框架基础上，探索了 Reasoning Model 训练技术的简化路径，主要包括以下几点:\n线性化思维过程：CoT 替代 MCTS\n传统强化学习 AI，如围棋和象棋，曾依赖蒙特卡洛树搜索 (Monte Carlo Tree Search, MCTS)。DeepSeek-R1 等模型则探索使用自回归的链式思维方法简化推理过程，逐步摒弃了计算复杂度高的 MCTS。 CoT 将复杂推理分解为线性步骤，模型像解题一样逐步推理，而非 MCTS 的穷举式搜索。这种线性化思维降低了计算复杂度，更符合人类思维习惯，使模型更易学习复杂推理策略。 消除独立价值网络：简化 RL 架构\n传统强化学习 (如 PPO) 通常需要独立的策略网络和价值网络。DeepSeek-R1 等研究发现，强化的策略网络或简化的价值评估方法 (如 GRPO 的组相对评分) 可替代独立价值网络。 这简化了 RL 训练架构，降低了资源需求，提高了效率。表明大语言模型的策略网络已具备强大的价值评估能力，无需额外价值网络。 聚焦最终结果奖励：最小化奖励信号\nDeepSeek-R1 采用更加简单的 ORM 奖励策略，主要关注最终结果的准确性奖励，弱化中间推理步骤奖励。这种策略受 AlphaZero (Silver et al., 2017) 启发，后者仅关注胜负。 对于 Reasoning Model，最终结果奖励可能比 PRM 更有效，能帮助模型更自然地学习“思维方式”，减少繁琐的逐步监督。 增加思考时间：模型自发涌现深度思考\nDeepSeek-R1-Zero 训练中展现出自发 增加思考时间 的能力。模型随训练深入，根据问题复杂度自适应分配更多“思考时间”，生成更长推理序列。这种“思考时间”增加是模型 RL 训练中自发涌现的行为。 思考时间增加反映模型更深入探索和优化思维过程。复杂问题需要更多推理步骤才能找到答案。DeepSeek-R1-Zero 的自我进化能力印证了强化学习在提升模型推理能力方面的潜力。 总结 DeepSeek-R1 的成功展示了 RL 提升 LLM 推理能力的巨大潜力。DeepSeek-R1 采用的 GRPO 算法在计算效率、优化稳定性、奖励鲁棒性等方面优于 PPO 和 DPO，并通过简化模型架构降低了训练资源消耗。DeepSeek-R1 为开源 Reasoning Model 复现 o1 提供了一条值得参考的路径。\n参考文献 [1] OpenAI O1 [网站]. OpenAI, 2024. (OpenAI O1 官方介绍页面)\n[2] Jaech A, et al. Openai o1 system card [J]. arXiv preprint arXiv:2412.16720, 2024.\n[3] Open-r1 [网站]. GitHub, 2024. (Open-r1 开源项目 GitHub 仓库)\n[4] Sutton R. The bitter lesson [J]. Incomplete Ideas (blog), 2019, 13(1): 38.\n[5] Liu A, et al. Deepseek-v3 technical report [J]. arXiv preprint arXiv:2412.19437, 2024.\n[6] Schulman J, et al. Proximal policy optimization algorithms [J]. arXiv preprint arXiv:1707.06347, 2017.\n[7] Ouyang L, et al. Training language models to follow instructions with human feedback [J]. Advances in Neural Information Processing Systems, 2022, 35: 27730-27744. https://arxiv.org/abs/2203.02155\n[8] Shao Z, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models [J]. arXiv preprint arXiv:2402.03300, 2024.\n[9] J. Schulman. Approximating kl divergence, 2020.\n[10] Gao L, Schulman J, Hilton J. Scaling laws for reward model overoptimization [C]// International Conference on Machine Learning. PMLR, 2023.\n[11] Chen M, et al. Evaluating large language models trained on code [J]. arXiv preprint arXiv:2107.03374, 2021.\n[12] Learning to Reason with LLMs [网站]. OpenAI, 2024. (OpenAI 关于 LLM 推理能力的博客文章)\n[13] AMC [网站]. Mathematical Association of America (MAA), 2024. (美国数学竞赛 AMC 官方网站)\n[14] Open-O1 [网站]. GitHub, 2024. (Open-O1 开源项目 GitHub 仓库)\n[15] Zeng Z, et al. Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective [J]. arXiv preprint arXiv:2412.14135, 2024.\n[16] Hinton G. Distilling the Knowledge in a Neural Network [J]. arXiv preprint arXiv:1503.02531, 2015.\n[17] Silver D, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm [J]. arXiv preprint arXiv:1712.01815, 2017.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Jan 2025). OpenAI o1复现进展：DeepSeek-R1. https://syhya.github.io/posts/2025-01-27-deepseek-r1\nOr\n@article{syhya2025deepseekr1, title = \u0026#34;OpenAI o1复现进展：DeepSeek-R1\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-27-deepseek-r1\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-01-27-deepseek-r1/","summary":"\u003cp\u003eDeepSeek AI 近期发布 \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/2501.12948\"\u003eDeepSeek-AI, 2025\u003c/a\u003e)，其推理性能在多个      benchmark 上已接近 OpenAI o1 (\u003ca href=\"https://openai.com/o1/\"\u003eOpenAI, 2024\u003c/a\u003e)的水平，是开源社区成功复现 o1 的重要一步。R1 相关代码可以参考huggingface 尝试开源复现 \u003ca href=\"https://github.com/huggingface/open-r1\"\u003eopen-r1\u003c/a\u003e 项目。以往的研究多依赖于海量的监督数据来提升大语言模型（Large Language Model, LLM）性能，但 DeepSeek-R1 及其早期实验 DeepSeek-R1-Zero 的成功，有力证明了纯粹大规模强化学习在提升 LLM 推理能力方面的潜力。其印证了 Richard Sutton 在 “The Bitter Lesson” 中提出的深刻见解:\u003c/p\u003e","title":"OpenAI o1复现进展：DeepSeek-R1"},{"content":"背景 Transformer (Vaswani et al., 2017）是一种基于编码器-解码器架构的模型。此模型在自然处理领域中展示了卓越的性能，随后一系列模型在此基础上进行了优化，例如仅使用编码器的 BERT (Devlin et al., 2018）或仅使用解码器的 GPT (Radford et al., 2018）系列，以及后续的大型语言模型如 LLaMA (Touvron et al., 2023）和 GPT-4 (OpenAI al., 2024）系列，这些模型大多采用了仅解码器的结构。\n符号 符号 含义 \\(B\\) 批量大小（Batch Size） \\(S\\) 序列长度（Sequence Length） \\(d\\) 隐藏维度 / 模型维度（Model Size / Hidden Dimension） \\(H\\) 注意力头数量（Number of Heads in Multi-Head Attention） \\(G\\) 分组数量（Group Number），用于分组查询注意力（GQA） \\(d_{\\text{head}} = \\frac{d}{H}\\) 每个注意力头的维度 \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) 输入序列，批量为 \\(B\\)，序列长度为 \\(S\\)，隐藏维度为 \\(d\\) \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{B \\times S \\times d}\\) 经过线性变换后的 Query、Key、Value 矩阵 \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) 分别对应生成 \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) 的可训练线性映射矩阵 \\(W_O \\in \\mathbb{R}^{d \\times d}\\) 多头 / 分组注意力输出后，用于映射回原始维度 \\(d\\) 的可训练线性映射矩阵 \\(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h \\in \\mathbb{R}^{B \\times S \\times d_{\\text{head}}}\\) 第 \\(h\\) 个注意力头对应的 Query、Key、Value 子矩阵 \\(\\mathbf{K}^*, \\mathbf{V}^*\\) 在多查询注意力（MQA）中，将所有头的 \\(\\mathbf{K}_h, \\mathbf{V}_h\\) 平均或合并后得到的共享 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\) \\(\\mathbf{q}, \\mathbf{k}\\in \\mathbb{R}^{d_{\\text{head}}}\\) 在缩放点积注意力的随机向量示例中，用于数学推导（中心极限定理）的单个查询向量和单个键向量 Transformer中的注意力机制 Transformer模型的核心在于自注意力机制（Self-Attention），它允许模型在处理序列数据时，动态地关注序列中的不同部分。具体来说，给定一个输入序列 \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\)（批大小 \\(B\\)，序列长度 \\(S\\)，隐藏维度 \\(d\\)），Transformer会通过三个线性层分别投影为查询（Query, \\(\\mathbf{Q}\\)）、键（Key, \\(\\mathbf{K}\\)）和值（Value, \\(\\mathbf{V}\\)）：\n\\[ \\mathbf{Q} = \\mathbf{X} W_Q, \\quad \\mathbf{K} = \\mathbf{X} W_K, \\quad \\mathbf{V} = \\mathbf{X} W_V \\]其中，\\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) 是可训练的权重矩阵。多头注意力通过将这些投影分成多个头，每个头负责不同的子空间表示，从而增强模型的表示能力。\n注意力机制有多种形式，Transformer 依赖于缩放点积注意力（Scaled Dot-Product Attention）：给定查询矩阵 \\(\\mathbf{Q}\\)、键矩阵 \\(\\mathbf{K}\\) 和值矩阵 \\(\\mathbf{V}\\)，输出是值向量的加权和，其中每个值的权重由查询与对应键的点积决定：\n\\[ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V} \\] Fig. 1. Scaled Dot-Product Attention. (Image source: Vaswani et al., 2017)\n多头注意力（MHA） 多头注意力（MHA）将 \\(\\mathbf{Q}\\)、\\(\\mathbf{K}\\)、\\(\\mathbf{V}\\) 分成多个头，每个头有独立的 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)，从而增加了模型的容量和灵活性：\n\\[ \\text{MHA}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]其中，每个头的计算为：\n\\[ \\text{head}_h = \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q}_h \\mathbf{K}_h^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V}_h \\] Fig. 2. Multi-Head Attention. (Image source: Vaswani et al., 2017)\n使用多头注意力机制的好处 捕捉不同的特征：单头注意力机制只能关注输入序列中的一种特征或模式，而多头注意力机制可以通过多个注意力头同时关注不同的特征或模式，使模型能够更全面地理解输入数据。 增强模型的表达能力：每个注意力头可以学习不同的表示方式，增强模型的表达能力。不同的注意力头可以关注输入序列的不同部分或不同关系，帮助模型更好地捕捉复杂的依赖关系。 提高稳定性和性能：多头注意力机制通过多个注意力头的平均或组合，减少单个注意力头的噪声和不稳定性，提高模型的稳定性和性能。 并行计算：多头注意力机制可以在计算上并行化，因为每个注意力头的计算是独立的。这有助于提高计算效率，特别是在使用GPU或TPU等硬件加速器时。 缩放点积注意力中的Softmax Softmax函数将一个向量 \\(\\mathbf{z} = [z_1, z_2, \\dots, z_n]\\) 转换为一个概率分布 \\(\\mathbf{y} = [y_1, y_2, \\dots, y_n]\\)，其定义如下：\n\\[ y_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)} \\quad \\text{对于} \\quad i = 1, 2, \\dots, n \\]在注意力机制中，softmax函数用于将缩放后的点积 \\(\\tfrac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\) 转换为注意力权重：\n\\[ \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr) = \\Bigl[ \\frac{\\exp\\Bigl(\\frac{Q_1 \\cdot K_1}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_1 \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)}, \\dots, \\frac{\\exp\\Bigl(\\frac{Q_S \\cdot K_S}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_S \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)} \\Bigr] \\]在 Transformer 的注意力机制中，缩放点积注意力公式中的缩放因子 \\(\\sqrt{d_{\\text{head}}}\\) 是为了确保在进行 softmax 之前，点积的结果不会因为向量维度的增加而变得过大。这主要有以下几个原因：\n防止梯度消失：通过缩放注意力得分，可以避免输入 softmax 函数的值过大，从而防止梯度在反向传播过程中出现消失的情况。\n数值不稳定性：缩放注意力得分可以使得 softmax 函数的输入值范围更加合理，避免数值过于极端，从而提升模型的数值稳定性和训练效果。特别是当向量维度较大时，未经缩放的点积结果可能导致 softmax 的指数函数值过大，进而引发溢出问题。\n数学解释：假设向量 \\(\\mathbf{q}\\) 和 \\(\\mathbf{k}\\) 的各分量独立同分布，均值为 0，方差为 1。它们的点积 \\(\\mathbf{q} \\cdot \\mathbf{k}\\) 的均值为 0，方差为 \\(d_{\\text{head}}\\)。为了防止点积的方差随维度 \\(d_{\\text{head}}\\) 增加而变大，需要对其进行缩放处理。通过将点积除以 \\(\\sqrt{d_{\\text{head}}}\\)，可以使缩放后的点积的方差为 1，与 \\(d_{\\text{head}}\\) 无关。\n根据统计学原理，当将随机变量除以一个常数时，其方差会按该常数的平方倒数缩放。因此，缩放因子 \\(\\tfrac{1}{\\sqrt{d_{\\text{head}}}}\\) 可以有效控制注意力得分的规模，从而提高数值稳定性。以下是详细推导过程：\n假设 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^{d_{\\text{head}}}\\)，各分量独立同分布，均值为 0，方差为 1，则它们的点积为：\n\\[ \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d_{\\text{head}}} q_i k_i \\]根据中心极限定理，当 \\(d_{\\text{head}}\\) 较大时，点积 \\(\\mathbf{q} \\cdot \\mathbf{k}\\) 近似服从均值为 0、方差为 \\(d_{\\text{head}}\\) 的正态分布：\n\\[ \\mathbf{q} \\cdot \\mathbf{k} \\sim \\mathcal{N}(0, d_{\\text{head}}) \\]为了使缩放后的点积具有单位方差，我们将点积除以 \\(\\sqrt{d_{\\text{head}}}\\)：\n\\[ \\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}} \\;\\sim\\; \\mathcal{N}\\!\\Bigl(0, \\frac{d_{\\text{head}}}{d_{\\text{head}}}\\Bigr) = \\mathcal{N}(0, 1) \\]因此，经过缩放后，点积 \\(\\tfrac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}}\\) 的方差恒为 1，与维度 \\(d_{\\text{head}}\\) 无关。这种缩放操作能够保持点积在一个稳定的范围内，避免 softmax 函数在计算中因输入值过大或过小而产生数值不稳定性。\n多查询注意力（MQA） 多查询注意力（MQA）(Shazeer, 2019) 通过让所有查询头（Query Heads）共享同一组键（Key）\\(\\mathbf{K}\\) 和值（Value）\\(\\mathbf{V}\\)，从而显著减少了显存带宽的需求。具体地，如果我们将传统多头注意力（MHA）中的所有 \\(\\mathbf{K}_h\\) 和 \\(\\mathbf{V}_h\\) 做如下平均：\n\\[ \\mathbf{K}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{K}_h, \\quad \\mathbf{V}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{V}_h, \\]其中 \\(H\\) 表示查询头的数量，\\(\\mathbf{K}_h\\) 和 \\(\\mathbf{V}_h\\) 分别表示第 \\(h\\) 个头对应的键和值。那么在推理过程中，每个头只需要使用同一个 \\(\\mathbf{K}^*\\) 和 \\(\\mathbf{V}^*\\)，从而大幅降低对显存带宽的占用。最后再将所有头输出拼接并映射回输出空间：\n\\[ \\text{MQA}(\\mathbf{Q}, \\mathbf{K}^*, \\mathbf{V}^*) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]由于键和值只保留了一组，MQA 推理速度更快，但在某些场景下，模型的表达能力和性能会受到一定限制。\n分组查询注意力（GQA） 分组查询注意力（GQA） (Ainslie, 2023) 是介于 MHA 和 MQA 之间的一种折中方案。它通过将查询头分为多个组，让每组共享一组 $\\mathbf{K}$ 和 $\\mathbf{V}$ 头，以在推理速度和模型性能之间取得平衡。每组包含 $\\frac{H}{G}$ 个查询头，每组共享一组 $\\mathbf{K}$ 和 $\\mathbf{V}$ 头。其具体流程如下：\n投影：将输入 $\\mathbf{X}$ 通过线性变换分别投影为 $\\mathbf{Q}$、$\\mathbf{K}$、$\\mathbf{V}$。 分组 Query：将 $\\mathbf{Q}$ 划分为 $H$ 个头后，再将这些头进一步划分为 $G$ 组。 分组 Key/Value：将 $\\mathbf{K}$ 和 $\\mathbf{V}$ 划分为 $G$ 组，每组共享一组 $\\mathbf{K}$ 和 $\\mathbf{V}$。 组内注意力：对每组的 $\\mathbf{Q}$ 与各自组共享的 $\\mathbf{K}$ 和 $\\mathbf{V}$ 进行注意力计算。 拼接输出：将各组的注意力结果在通道维度上拼接，最后通过线性层得到最终输出。 三种 Attention 方法之间的联系 Fig. 3. Overview of grouped-query method. (Image source: Ainslie et al., 2023)\n图3直观展示了这三种注意力机制的关系：多头注意力（MHA）为每个查询头都保留独立的 $\\mathbf{K}$ 和 $\\mathbf{V}$；多查询注意力（MQA）则所有查询头共享同一组 $\\mathbf{K}$ 和 $\\mathbf{V}$；分组查询注意力（GQA）则在两者之间，通过分组共享的方式兼顾速度与性能。\n当 $G=1$ 时：所有查询头共享同一组 $\\mathbf{K}$ 和 $\\mathbf{V}$。此时 GQA 退化为多查询注意力（MQA）。\n$\\mathbf{K}/\\mathbf{V}$ 头数量：$1$ 模型行为：所有头使用相同的 $\\mathbf{K}$ 和 $\\mathbf{V}$ 进行注意力计算，显著降低显存带宽需求。 当 $G=H$ 时：每个查询头都拥有独立的 $\\mathbf{K}$ 和 $\\mathbf{V}$。此时 GQA 退化为多头注意力（MHA）。\n$\\mathbf{K}/\\mathbf{V}$ 头数量：$H$ 模型行为：每个头使用完全独立的 $\\mathbf{K}$ 和 $\\mathbf{V}$，保留 MHA 的高模型容量和性能。 通过调整分组数量 $G$，GQA 在 MHA 和 MQA 之间实现了灵活切换，能够在保持较高模型性能的同时，兼顾推理速度的提升。\n实现代码片段 下面是使用 PyTorch 简单实现的 MHA 、MQA和 GQA 的代码, 其中 GQA采用了广播（Boardcast）和复制（Repeat）两种方法。此外 需要注意的是，在实际的 LLaMA3 源代码中，GQA 的实现还引入了 KV Cache。为简化示例，以下代码并未包含该部分。如果感兴趣，可以参考源代码 model.py 获取更完整的代码细节。\nMHA 代码片段 multi_head_attention.py\nimport math import torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # (nums_head * head_dim = hidden_dim) assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(dropout_rate) # Define linear projection layers self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): # x has shape: (batch_size, seq_len, hidden_dim) batch_size, seq_len, _ = x.size() # Q, K, V each has shape: (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) K = self.k_proj(x) V = self.v_proj(x) # Reshaping from (batch_size, seq_len, hidden_dim) to (batch_size, seq_len, nums_head, head_dim) # Then transpose to (batch_size, nums_head, seq_len, head_dim) # q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3) # [Another approach to do it] q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) k = K.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Matrix multiplication: (batch_size, nums_head, seq_len, head_dim) * (batch_size, nums_head, head_dim, seq_len) # Resulting shape: (batch_size, nums_head, seq_len, seq_len) # Note that the scaling factor uses head_dim, not hidden_dim. attention_val = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) print(f\u0026#34;attention_mask shape is {attention_mask.size()}\u0026#34;) if attention_mask is not None: # If attention_mask is provided, it should have shape (batch_size, nums_head, seq_len, seq_len). assert attention_val.size() == attention_mask.size() attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # Apply softmax along the last dimension to get attention weights. attention_weight = torch.softmax(attention_val, dim=-1) # Dropout on attention weights attention_weight = self.dropout(attention_weight) # Multiply attention weights with V: # (batch_size, nums_head, seq_len, seq_len) * (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v # Transpose back: (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) # # Note: The transpose operation changes the dimension ordering but does not change the memory layout, # resulting in a non-contiguous tensor. The contiguous() method makes the tensor contiguous in memory, # allowing subsequent view or reshape operations without error. output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) # output = output_mid.permute(0, 2, 1, 3).reshpae(batch_size, seq_len, self.hidden_dim) # # [Another approach to do it] output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 # attention_mask has shape: (batch_size, nums_head, seq_len, seq_len). # Here we use a lower-triangular mask to simulate causal masking. attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_head_attention = MultiHeadAttention(hidden_dim=hidden_dim, nums_head=nums_head) x_forward = multi_head_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) MQA 代码片段 multi_query_attention.py\nimport torch import torch.nn as nn import math class MultiQueryAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(p=dropout) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # For kv, project: hidden_dim -\u0026gt; head_dim self.k_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.v_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() Q, K, V = self.q_proj(x), self.k_proj(x), self.v_proj(x) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Broadcast k and v to match q\u0026#39;s dimensions for attention computation # k -\u0026gt; (batch_size, 1, seq_len, head_dim) # v -\u0026gt; (batch_size, 1, seq_len, head_dim) k = K.unsqueeze(1) v = V.unsqueeze(1) # (batch_size, head_num, seq_len, head_dim) * (batch_size, 1, head_dim, seq_len) # -\u0026gt; (batch_size, head_num, seq_len, seq_len) attention_val = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) attention_weight = torch.softmax(attention_val, dim=-1) print(f\u0026#34;attention_weight is {attention_weight}\u0026#34;) attention_weight = self.dropout(attention_weight) # (batch_size, head_num, seq_len, seq_len) * (batch_size, 1, seq_len, head_dim) # -\u0026gt; (batch_size, head_num, seq_len, head_dim) output_tmp = attention_weight @ v # -\u0026gt; (batch_size, seq_len, head_num, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_query_attention = MultiQueryAttention(hidden_dim=hidden_dim, nums_head=nums_head, dropout=0.2) x_forward = multi_query_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) GQA 代码片段 group_query_attention.py\nimport math import torch import torch.nn as nn class GQABroadcast(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # Total number of Q heads (H) self.nums_kv_head = nums_kv_head # Number of K, V heads (G, groups) assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head # Number of Q heads per group self.q_heads_per_group = nums_head // nums_kv_head self.dropout = nn.Dropout(dropout_rate) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # Projection output dimensions for K, V = nums_kv_head * head_dim self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask= None): batch_size, seq_len, _ = x.size() Q = self.q_proj(x) # (batch_size, seq_len, hidden_dim) K = self.k_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) V = self.v_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) # Q: (batch_size, seq_len, hidden_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2).contiguous() q = q.view(batch_size, self.nums_kv_head, self.q_heads_per_group, seq_len, self.head_dim) # K, V: (batch_size, seq_len, nums_kv_head * head_dim) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim # -\u0026gt; (batch_size, nums_kv_head, 1, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) # q: (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) * (batch_size, nums_kv_head, 1, head_dim, seq_len) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_val = q @ k.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) * (batch_size, nums_kv_head, 1, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) output_tmp = attention_weight @ v # (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = output_tmp.view(batch_size, self.nums_head, seq_len, self.head_dim) # (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) -\u0026gt; (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output class GQARepeat(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head self.nums_kv_head = nums_kv_head assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head self.q_head_per_group = nums_head // nums_kv_head self.q_proj = nn.Linear(hidden_dim, nums_head * self.head_dim) self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) self.dropout = nn.Dropout(dropout_rate) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() # (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) # (batch_size, seq_len, nums_kv_head * self.head_dim) K = self.k_proj(x) V = self.v_proj(x) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) # (batch_size, nums_head, seq_len, head_dim) k_repeat = k.repeat_interleave(self.q_head_per_group, dim=1) v_repeat = v.repeat_interleave(self.q_head_per_group, dim=1) # (batch_size, nums_head, seq_len, seq_len) attention_val = q @ k_repeat.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#39;-inf\u0026#39;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v_repeat # (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 16) batch_size, seq_len, hidden_dim = x.size() nums_head = 8 head_dim = hidden_dim // nums_head nums_kv_head = 4 q_heads_per_group = nums_head // nums_kv_head # v1: Boardcast # attention_mask_v1 has shape: (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_mask_v1 = torch.tril(torch.ones(batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len)) gqa_boradcast = GQABroadcast(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v1 = gqa_boradcast.forward(x, attention_mask=attention_mask_v1) # print(x_forward_v1) print(x_forward_v1.size()) # v2: Repeat # attention_mask_v2 has shape: (batch_size, nums_head, seq_len, seq_len) attention_mask_v2 = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) gqa_repeat = GQARepeat(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v2 = gqa_repeat.forward(x, attention_mask=attention_mask_v2) # print(x_forward_v2) print(x_forward_v2.size()) 时间与空间复杂度分析 说明：下文针对的是一次前向传播（forward propagation）的复杂度；在训练时，还需要额外考虑反向传播（backward propagation）与参数更新。反向传播不仅依赖前向传播保存的中间激活值，还需额外计算梯度和存储中间导数，通常使得总计算量和内存占用比前向传播高，导致训练耗时为前向传播的数倍。\n在分析不同注意力机制（MHA、MQA、GQA）时，我们主要关注它们在 自注意力（self-attention） 或 交叉注意力（cross-attention） 过程中，进行前向传播时的时间复杂度和空间复杂度。即使它们在实现细节上（例如是否共享 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)）有所不同，但从计算量和主要的缓存/显存使用角度来看，其量级大致保持一致。\n假设每个位置都会生成查询 \\(\\mathbf{Q}\\)、键 \\(\\mathbf{K}\\) 和值 \\(\\mathbf{V}\\) 的表征，且各矩阵按批量和头数拆分之后的形状如同下式所示：\n\\[ \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\;\\in\\; \\mathbb{R}^{B \\times H \\times S \\times d_{\\text{head}}} \\]时间复杂度分析 矩阵乘法的通用时间复杂度 对于形状为 $m \\times n$ 的矩阵 $\\mathbf{A}$ 与形状为 $n \\times p$ 的矩阵 $\\mathbf{B}$ 进行乘法 $\\mathbf{A}\\mathbf{B}$，其时间复杂度一般表示为：\n$$ \\mathcal{O}(m \\times n \\times p) $$在注意力机制的计算中，这一基本结论常用于分析 $\\mathbf{Q}\\mathbf{K}^\\top$ 以及注意力分数与 $\\mathbf{V}$ 的乘法等。\n自注意力计算的主要步骤及复杂度 点积计算 ($\\mathbf{Q}\\mathbf{K}^\\top$)\n$\\mathbf{Q}$ 形状：$B \\times H \\times S \\times d_{\\text{head}}$\n$\\mathbf{K}$ 形状：$B \\times H \\times S \\times d_{\\text{head}}$\n因此 $\\mathbf{Q}\\mathbf{K}^\\top$ 的结果形状为 $B \\times H \\times S \\times S$。\n具体的计算量可以视为：对每个批次、每个头，以及序列内所有位置对 $(S \\times S)$ 的点积，其中每个点积涉及 $d_{\\text{head}}$ 维度的乘加运算。\n故其时间复杂度为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S \\times S \\times d_{\\text{head}}\\bigr) \\;=\\; \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) $$ softmax 操作\n在得到的注意力分数矩阵 $B \\times H \\times S \\times S$ 上进行逐元素的 softmax 运算。\nsoftmax 对矩阵的每个元素执行指数与归一化操作，其复杂度一般为：\n$$ \\mathcal{O}(\\text{元素数}) = \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) $$ 相对于上一步的矩阵乘法，其依赖维度 $d_{\\text{head}}$ 的项可以忽略。因此常将其视为比矩阵乘法更小的开销。\n加权平均（注意力分数与 $\\mathbf{V}$ 的乘法）\n$\\mathbf{V}$ 形状：$B \\times H \\times S \\times d_{\\text{head}}$\n注意力分数矩阵形状：$B \\times H \\times S \\times S$\n将每个位置的注意力分数与对应的 $\\mathbf{V}$ 向量乘加之后，输出仍是 $B \\times H \\times S \\times d_{\\text{head}}$。\n其时间复杂度与 $\\mathbf{Q}\\mathbf{K}^\\top$ 的分析类似：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) $$ 将上述三步综合，最主要的开销来自两次矩阵乘法，各为 $\\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}})$。因此在一次完整前向计算时，量级可写为：\n$$ \\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}}) = \\mathcal{O}(B \\times S^2 \\times d). $$（这里用到了 $d_{\\text{head}} = \\frac{d}{H}$）\n增量解码/推理场景（KV Cache）下的时间复杂度 Fig. 4. KV cache example. (Image source: Efficient NLP YouTube Channel)\n参考图4在推理场景（尤其自回归生成）中，通常会使用 KV Cache 来缓存先前时刻的 $\\mathbf{K}$, $\\mathbf{V}$，从而避免重复计算。此时，每生成一个新 token（即处理一个新的时间步）只需：\n对新 token 计算 $\\mathbf{Q}$（及对应的 $\\mathbf{K}$, $\\mathbf{V}$）\n若只保留了投影权重，则新产生的 $\\mathbf{Q}$ 和当前时刻的 $\\mathbf{K}$, $\\mathbf{V}$ 仅涉及 $\\mathcal{O}(d^2)$ 参数乘法，但这是对单个 token而言，相对开销不大。 与已有 KV Cache 做注意力\nKV Cache 中存储了所有先前时刻的 $\\mathbf{K}$, $\\mathbf{V}$，形状约为：\n$$ B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} $$此时 $S_{\\text{past}}$ 表示已经生成的序列长度。\n新的 $\\mathbf{Q}$ 形状是 $B \\times H \\times 1 \\times d_{\\text{head}}$，故新 token 的注意力分数计算为：\n$$ \\mathbf{Q}\\mathbf{K}^\\top : \\; \\mathcal{O}\\bigl(B \\times H \\times 1 \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$ 同理，对 $\\mathbf{V}$ 的加权得到新 token 的输出，也有相同量级：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$ 更新 KV Cache\n将新产生的 $\\mathbf{K}$, $\\mathbf{V}$ 追加到 KV Cache 中，以备下一个时间步使用。此操作在时间复杂度上只是简单的 concat/append，主要在空间上会不断增长。 因此，在增量解码时，每个新 token 的计算量约为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$而不是一次性地进行 $S \\times S$ 规模的注意力计算。若要生成长度为 $S$ 的序列，总体时间在理想情况下也可归纳为\n$$ \\sum_{k=1}^{S} \\mathcal{O}\\bigl(B \\times H \\times k \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) $$与一次性计算的复杂度同阶，只是一次性计算与逐步计算的差异。每步只处理 1 个 token 的注意力时，峰值的临时计算量更小，也无需存储完整的 $S \\times S$ 注意力分数矩阵。\n时间复杂度总结 MHA（多头注意力）：头数多，但每个头分别计算 $\\mathbf{K}$, $\\mathbf{V}$。 MQA（多查询注意力）：多个头共享 $\\mathbf{K}$, $\\mathbf{V}$。 GQA（分组注意力）：将 $H$ 个头分成 $G$ 个组，每组共享一组 $\\mathbf{K}$, $\\mathbf{V}$。 不论 MHA / MQA / GQA，在 完整前向 下，它们的主要矩阵乘法复杂度均为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) $$而在增量推理场景（KV Cache）下，单步计算复杂度降低为\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$但需要在多步解码过程中维护并更新 KV Cache。\n空间复杂度分析 空间复杂度既包括模型参数（权重参数）的规模，也包括前向计算时需要的中间激活值（尤其是注意力得分矩阵、加权结果，以及可能的 KV Cache）的规模。\n模型参数规模 线性投影层的参数\n对输入向量（维度 $d$）投影到 $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ 的维度：\n$$ \\underbrace{d \\times d}_{\\mathbf{Q}\\text{的投影}} \\;+\\; \\underbrace{d \\times d}_{\\mathbf{K}\\text{的投影}} \\;+\\; \\underbrace{d \\times d}_{\\mathbf{V}\\text{的投影}} = 3d^2 $$ 一般而言，这些参数会再根据头数 $H$ 切分成多头的形式，但总和并不因为头数增加而改变。故其量级为 $\\mathcal{O}(d^2)$。\n输出合并层的参数\n将多头输出拼接后再投影回维度 $d$ 时，通常还会有一个 $d \\times d$ 的线性层。这也同样是 $\\mathcal{O}(d^2)$。\n因此，若单独把二者相加，有\n$$ 3d^2 + d^2 = 4d^2 $$ 仍然可记作 $\\mathcal{O}(d^2)$。\n前向计算的中间激活值 在进行训练或完整前向时，需要缓存如下主要张量：\n注意力分数矩阵\n形状为 $B \\times H \\times S \\times S$。无论使用 MHA、MQA 还是 GQA，每个头（或组）都需要计算与 $\\mathbf{Q}\\mathbf{K}^\\top$ 相关的注意力分数，其规模量级为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) $$ 加权后的输出\n形状为 $B \\times H \\times S \\times d_{\\text{head}}$，对应每个位置在前向计算中得到的注意力上下文向量。其量级为：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S \\times d\\bigr) $$ 不同注意力机制下的 $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ 存储\n一般在反向传播时，需要缓存 $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ 的前向输出（或中间梯度）。若要显式存储，其形状及规模通常如下：\nMHA（多头注意力） $\\mathbf{Q}$: $B \\times H \\times S \\times d_{\\text{head}}$ $\\mathbf{K}$, $\\mathbf{V}$: $B \\times H \\times S \\times d_{\\text{head}}$ MQA（多查询注意力） $\\mathbf{Q}$: $B \\times H \\times S \\times d_{\\text{head}}$ $\\mathbf{K}$, $\\mathbf{V}$（共享）: $B \\times S \\times d$ GQA（分组注意力） $\\mathbf{Q}$: $B \\times H \\times S \\times d_{\\text{head}}$ $\\mathbf{K}$, $\\mathbf{V}$（分组共享）: $B \\times G \\times S \\times d_{\\text{head}}$, 其中 $G \\times d_{\\text{head}} = d$ 增量解码（KV Cache）下的空间消耗 在推理（增量解码）场景，往往会使用 KV Cache 来保存先前时刻的所有 Key、Value，以免反复计算。此时的存储结构通常是：\nKV Cache 维度（以 MHA 为例）：\n$$ \\mathbf{K}, \\mathbf{V} : B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} $$随着生成序列长度 $S_{\\text{past}}$ 的增长，KV Cache 会线性增大。\n单步注意力分数矩阵：\n由于每次只对新 token 进行注意力计算，分数矩阵的形状约为\n$$ B \\times H \\times 1 \\times S_{\\text{past}} $$显著小于训练时的 $B \\times H \\times S \\times S$。\n因此，增量解码时，大部分临时激活开销（如完整的 $S \\times S$ 矩阵）不再需要，但需要为 KV Cache 额外分配一份 $\\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}})$ 的显存，并随着序列长度增长而累积。\n综合空间复杂度 训练/完整前向\n主要激活值（注意力分数矩阵 + 输出 + Q, K, V 显式缓存）可合并表示为\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) $$当 $S$ 较大时，$B \\times H \\times S^2$ 常是主要瓶颈。\n推理/增量解码（KV Cache）\n无需完整的 $S^2$ 注意力分数矩阵，但需要一份\n$$ \\mathbf{K},\\mathbf{V}\\text{ Cache}: \\;\\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}) $$会随着解码步数 $S_{\\text{past}}$ 增长而线性增加。\n单次注意力分数仅为 $B \\times H \\times 1 \\times S_{\\text{past}}$ 的临时存储，量级显著小于训练场景。\n结论与对比 时间复杂度\n对于自注意力机制，无论是 MHA、MQA 还是 GQA，在完整前向场景下（训练时亦会包含该前向过程），主要的矩阵运算都保持相同量级：\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) $$ 在 增量推理（KV Cache） 场景下，每个新 token 只需\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$的计算，但需要维护并更新 KV Cache。\n空间复杂度\n模型参数：三者都在 $\\mathcal{O}(d^2)$ 量级。\n中间激活值（训练/完整前向）：主要由注意力分数矩阵和输出决定，量级为\n$$ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) $$ 增量解码（KV Cache）：节省了 $S^2$ 大小的临时分数矩阵，但需要一份随着 $S_{\\text{past}}$ 增长的 K, V 缓存\n$$ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) $$ MQA / GQA 的优势\n虽然从大 $S$ 场景的理论时间复杂度看，MQA、GQA 与 MHA 并无数量级的差别，但它们在键、值共享（或分组共享）带来的实际带宽、缓存访存效率方面，往往能在工程实现中取得更好的显存和速度性能。 下表总结了 MHA、MQA 和 GQA 三种注意力机制的主要差异：\n特性 多头注意力 (MHA) 多查询注意力 (MQA) 分组查询注意力 (GQA) K/V头数量 与头数量相同（$H$） 单一K/V头 分组数（$G$）组，每组1个K/V头 推理时间 较慢 最快 较快，但略高于MQA 显存带宽需求 最高，$H$倍的K/V加载 最低，仅1个K/V头 介于MHA和MQA之间，$G$倍的K/V加载 模型容量 最高 最低 中等，取决于分组数$G$ 性能表现 最佳 略低于MHA 接近MHA，显著优于MQA 向上训练需求 无需 高，需要更多的稳定性和调整 较低，GQA模型在少量数据进行向上训练后即可稳定运行 适用场景 高性能需求但推理速度不敏感的应用 推理速度要求极高，且对模型性能要求较低的场景 需要在推理速度和模型性能之间取得平衡的应用 实验结果 性能测试 本实验在一台配备双 NVIDIA RTX 4090 GPU 的环境下进行，采用数据并行（Data Parallel, DP）方式，将批量大小（batch size）均匀拆分到两张 GPU 上。实验仅测试了前向传播的性能表现，包括平均延迟时间（Time_mean，单位：ms）和峰值显存占用（Peak_Mem_mean，单位：MB），以评估不同注意力机制（MHA、MQA 和 GQA）在推理阶段的资源需求和效率。\n实验代码请参考benchmark_attention.py。 测试基于 Llama3 8B 参数超参数设置 Fig. 5. Overview of the key hyperparameters of Llama 3. (Image source: Grattafiori et al., 2024)\n主要设置参数如下:\n总层数：32 层。 隐藏层维度：4096。 多头注意力总头数：32。 不同组数（nums_kv_head）配置：32（MHA）、1（MQA）、8（GQA-8）。 实验结果 本节主要介绍在不同序列长度（512、1024 和 1536）下，多头注意力（MHA）、多查询注意力（MQA）以及组查询注意力（GQA-8）的实验表现，包含时间延迟和显存占用两个方面的数据。为了方便对比，下表给出了三种注意力机制的具体测试结果。\nModel Size Method nums_kv_head Seq Length Time_mean (ms) Peak_Mem_mean (MB) Llama3 8B GQA-8 8 512 40.8777 2322.328 Llama3 8B MHA 32 512 53.0167 2706.375 Llama3 8B MQA 1 512 37.3592 2210.314 Llama3 8B GQA-8 8 1024 85.5850 6738.328 Llama3 8B MQA 1 1024 80.8002 6570.314 Llama3 8B MHA 32 1024 102.0514 7314.375 Llama3 8B GQA-8 8 1536 147.5949 13586.328 Llama3 8B MHA 32 1536 168.8142 14354.375 Llama3 8B MQA 1 1536 141.5059 13362.314 Fig. 6. Average Time Benchmark.\nFig. 7. Average Peak Memory Benchmark.\n在显存和时间开销敏感的场景下，MQA 和 GQA-8 是更高效的选择，其中 MQA 表现最优，但可能在模型性能能力上有所不足；GQA-8 则在效率和性能之间达到了良好的平衡。\nGQA论文实验结果 推理性能 Fig. 8. Inference time and performance comparison. (Image source: Ainslie et al., 2023)\nFig. 9. Additional Experimental Results. (Image source: Ainslie et al., 2023)\n从实验结果可以看出：\n推理速度：\nMHA-XXL 的推理时间显著高于 MHA-Large，主要由于其更大的头数量和模型规模。 MQA-XXL 和 GQA-8-XXL 相比 MHA-XXL，推理时间分别减少至约1/6和1/5。 性能表现：\nMHA-XXL 在所有任务上表现最佳，但推理时间较长。 MQA-XXL 在推理速度上具有优势，平均分仅略低于 MHA-XXL。 GQA-8-XXL 在推理速度上接近 MQA-XXL，但在性能上几乎与 MHA-XXL 持平，显示出 GQA 的高效性和优越性。 CheckPoint转化 Fig. 10. Ablation Study on Checkpoint Conversion Methods. (Image source: Ainslie et al., 2023)\n图10证明了均值池化方法在保留模型信息方面表现最佳，选择第一个头次之，随机初始化效果最差。均值池化有效地融合了多个 $\\mathbf{K}$ 和 $\\mathbf{V}$ 头的信息，保持了模型性能。\n向上训练比例 Fig. 11. Ablation Study on Uptraining Ratios. (Image source: Ainslie et al., 2023)\n图11展示了以MHA模型为基准，T5 XXL模型在MQA和GQA上的性能随着向上训练的数据量增加变化情况。\nGQA：即使在仅进行转换（无向上训练）的情况下，GQA已具备一定性能，随着向上训练比例增加，性能持续提升。 MQA：需要至少5%比例的预训练数据进行向上训练才能达到实用的性能，且随着比例增加，性能提升趋于平缓。 分组数量对推理速度的影响 Fig. 12. Effect of the Number of GQA Groups on Inference Speed. (Image source: Ainslie et al., 2023)\n从图12可以发现随着分组数的增加，推理时间略有上升，但相较于MHA，仍然保持显著的速度优势。选择适中的分组数比如8可以在速度和性能之间取得良好平衡。图3也显示了 llama3 从 7B 到 405B 参数的模型都是才采用8作为分组数（key/value heads = 8）。\n其他优化方法 除了注意力机制的优化，研究者们还提出了多种方法以提升Transformer模型的推理和训练效率：\nLoRA (HU et al., 2021): 通过在预训练模型的权重矩阵上添加低秩矩阵来实现高效的参数微调。 Flash Attention（Dao et al., 2022）：通过优化注意力计算，减少内存和计算开销。 量化技术 LLM.int8（Dettmers et al., 2022)和 GPTQ (Frantar et al., 2022)：通过降低模型权重和激活的精度，减少显存占用和计算成本。 模型蒸馏（Hinton et al., 2015）：通过训练小模型模仿大模型的行为，减小模型规模。 投机采样 Speculative Sampling（Chen et al., 2023）：通过并行生成和筛选，提升生成效率。 关键总结 向上训练方法能够有效利用已有的MHA模型的Checkpoint，通过少量的额外训练，将其转化为更高效的MQA或GQA模型，显著降低了训练成本。 分组查询注意力（GQA） 在推理效率和模型性能之间取得了良好的平衡，尤其适用于需要高效推理和高性能的应用场景。 实验结果表明，GQA能够在保持与MHA模型相近的性能的同时，显著提升推理速度，适合大规模模型部署和实时应用。 参考文献 [1] Vaswani A. Attention is all you need [J]. Advances in Neural Information Processing Systems, 2017.\n[2] Devlin J. Bert: Pre-training of deep bidirectional transformers for language understanding [J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] Radford A. Improving language understanding by generative pre-training [J]. 2018.\n[4] Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models [J]. arXiv preprint arXiv:2302.13971, 2023.\n[5] Achiam J, Adler S, Agarwal S, et al. Gpt-4 technical report [J]. arXiv preprint arXiv:2303.08774, 2023.\n[6] Shazeer N. Fast transformer decoding: One write-head is all you need [J]. arXiv preprint arXiv:1911.02150, 2019.\n[7] Ainslie J, Lee-Thorp J, de Jong M, et al. Gqa: Training generalized multi-query transformer models from multi-head checkpoints [J]. arXiv preprint arXiv:2305.13245, 2023.\n[8] Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models [J]. arXiv preprint arXiv:2106.09685, 2021.\n[9] Dettmers T, Lewis M, Belkada Y, et al. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale [J]. Advances in Neural Information Processing Systems, 2022, 35: 30318-30332.\n[10] Frantar E, Ashkboos S, Hoefler T, et al. Gptq: Accurate post-training quantization for generative pre-trained transformers [J]. arXiv preprint arXiv:2210.17323, 2022.\n[11] Hinton G. Distilling the Knowledge in a Neural Network [J]. arXiv preprint arXiv:1503.02531, 2015.\n[12] Chen C, Borgeaud S, Irving G, et al. Accelerating large language model decoding with speculative sampling [J]. arXiv preprint arXiv:2302.01318, 2023.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Jan 2025). Transformer注意力机制：MHA、MQA与GQA的对比.\nhttps://syhya.github.io/posts/2025-01-16-group-query-attention/\nOr\n@article{syhya2025gqa, title = \u0026#34;Transformer注意力机制：MHA、MQA与GQA的对比\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-16-group-query-attention/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-01-16-group-query-attention/","summary":"\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\n\u003cp\u003eTransformer (\u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eVaswani et al., 2017\u003c/a\u003e）是一种基于编码器-解码器架构的模型。此模型在自然处理领域中展示了卓越的性能，随后一系列模型在此基础上进行了优化，例如仅使用编码器的 BERT (\u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eDevlin et al., 2018\u003c/a\u003e）或仅使用解码器的 GPT (\u003ca href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\"\u003eRadford et al., 2018\u003c/a\u003e）系列，以及后续的大型语言模型如 LLaMA (\u003ca href=\"https://arxiv.org/abs/2302.13971\"\u003eTouvron et al., 2023\u003c/a\u003e）和 GPT-4 (\u003ca href=\"https://arxiv.org/abs/2303.08774\"\u003eOpenAI al., 2024\u003c/a\u003e）系列，这些模型大多采用了仅解码器的结构。\u003c/p\u003e","title":"Transformer注意力机制：MHA、MQA与GQA的对比"},{"content":"背景 随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。\n本文基于作者的工作经验，总结了如何在现有通用模型的基础上，通过数据准备、模型训练、部署、评估及持续迭代，构建具备特定领域知识的大语言模型。\n为什么要向基座模型注入领域知识 挑战一：有限的领域知识 现有的预训练模型（如 GPT-4、Llama 3）主要基于通用语料库进行训练，缺乏对小众语言或专有领域的深入理解，导致模型在处理编程代码时表现不佳。\n挑战二：数据安全与合规 企业在处理敏感数据时，必须遵循严格的数据主权和合规性要求。将私有业务数据上传至第三方云服务存在安全隐患，因此需要在本地环境中完成数据处理与模型训练。\n挑战三：OpenAI微调的局限 主流商用 API 的微调方案通常较为基础，难以实现深层次的对齐和优化。对于需要高度定制化的领域模型，这种方法难以满足需求。\n注入知识两种方法 在实际项目中，常见的将领域知识注入基座模型的方法主要包括 微调 (Fine-Tuning) 和 检索增强生成 (RAG)。下文将详细对比这些方法，以帮助选择最适合的策略。\n方法对比 微调 (Fine-Tuning) 核心思路\n通过持续预训练、监督微调和偏好对齐，直接更新模型参数，使其掌握特定领域知识和任务模式。\n技术细节\n继续预训练 (CPT)：在大量领域特定的无监督数据上继续预训练基座模型。 监督微调 (SFT)：使用高质量的标注数据进行有监督微调。 偏好对齐 (DPO)：通过用户反馈优化模型输出。 参数微调方法：使用全参数微调或者结合 LoRA 等 PEFT 方法冻结部分参数并添加 adapter。 优势\n深度定制：模型内部权重更新，能够深入理解领域知识。 无需依赖外部检索：推理时不需额外的知识库支持，减少延迟和总的 token 消耗。 提升整体性能：在特定领域任务上表现显著优于通用模型。 劣势\n高计算成本：需要大量计算资源进行训练，尤其是 CPT 阶段。 训练周期长：从数据准备到模型训练再到优化，需要较长时间。 灾难性遗忘：模型可能在学习新知识的同时，遗忘原有的通用能力。 检索增强生成 (RAG) 核心思路\n构建领域知识库，在推理阶段检索相关文档，辅助模型生成更准确的回答，无需直接改变模型参数。\n技术细节\n数据处理：对领域文档进行预处理，按块大小和重叠量切分。 向量化：使用文本嵌入模型将文本块转换为向量，存储在向量数据库中。 召回：推理时通过相似度搜索召回相关文档，作为上下文信息或 few-shot 示例提供给基座模型。 优势\n保持通用能力：模型参数不变，仍保留通用语言能力。 快速更新：知识库可动态更新，无需重新训练模型。 计算效率：避免大规模训练，节省计算资源。 劣势\n依赖知识库质量：检索到的文档质量直接影响回答质量。 推理速度：检索过程可能增加推理延迟，并且需要更多的 token。 知识覆盖有限：模型内部知识仍受限于基座模型的预训练数据。 模型与训练资源 基座模型 以 Llama 3 系列 为例，其具有以下特点：\n参数规模\nLlama 3 系列涵盖从 1B 到 405B 参数的模型，广泛支持多语言处理、代码生成、推理，以及视觉和文本任务。小型模型（1B 和 3B）经过专门优化，适合边缘和移动设备，支持最大 128K 的上下文窗口，可高效处理本地任务，例如摘要生成、指令执行和文本重写。\n多模态能力\nLlama 3 的视觉模型（11B 和 90B 参数）在图像理解任务上的表现优于许多封闭模型，同时支持图像、视频和语音的多模态处理。所有模型均支持微调，便于针对特定领域进行定制化开发。\n开源与社区支持\nLlama 3 系列模型及其权重以开源形式发布，可通过 llama.com 和 Hugging Face 平台 获取，为开发者提供便捷的访问和应用支持。\n数据集限制\n虽然 Llama 3 模型本身以开源形式发布，但其训练所使用的数据集并未开源。因此，严格来说，Llama 3 并非完全开源的模型。这一限制可能会在解决灾难性遗忘问题时带来挑战，因为难以获得与原始训练完全一致的数据集。\n训练资源 训练大型语言模型需要强大的计算资源和高效的分布式训练框架。\n硬件资源\nGPU 集群：建议使用 NVIDIA A100 或 H100 GPU，4卡或8卡配置，通过 NVLink 或 InfiniBand 提升通信带宽。 存储资源：高性能 SSD（如 NVMe）以支持快速的数据读写。 软件框架\n并行框架：DeepSpeed、Megatron-LM 等分布式训练框架，支持大规模模型训练。 推理框架：vLLM、ollama 等，优化推理速度和资源利用。 并行策略\n数据并行：适用于单卡可容纳模型的情况，通过 DeepSpeed 的 ZeRO Stage 0 实现。 模型并行、流水线并行和张量并行：单卡无法容纳时，采用 ZeRO Stage 1、2、3 进行优化，或使用 ZeRO-Infinity 将参数和优化器状态部分卸载到 CPU 或 NVMe。 DeepSpeed ZeRO 分片策略对比 为了更好地理解 DeepSpeed 的 ZeRO 分片策略，以下将分为不同的部分进行详细说明。\nZeRO Stage 分片策略 ZeRO Stage 描述 显存占用 训练速度 ZeRO-0 纯数据并行，不进行任何分片。所有优化器状态、梯度和参数在每张 GPU 上完全复制。 最高 最快 ZeRO-1 分片优化器状态（例如动量和二阶矩），减少显存占用，但梯度和参数仍为数据并行。 高 略慢于 ZeRO-0 ZeRO-2 分片优化器状态和梯度，在 ZeRO-1 的基础上进一步减少显存占用。 中 慢于 ZeRO-1 ZeRO-3 分片优化器状态、梯度和模型参数，显存占用最低，适合大规模模型。但需要在前向和后向时进行参数广播（All-Gather/All-Reduce），通信量显著增加。 低 明显慢于 ZeRO-2，取决于模型大小和网络带宽 Offload 策略 Offload 类型 描述 显存占用 训练速度 ZeRO-1 + CPU Offload 在 ZeRO-1 的基础上，将优化器状态卸载到 CPU 内存；可进一步降低 GPU 显存占用，但需要 CPU-GPU 数据传输，依赖 PCIe 带宽，且占用 CPU 内存。 中偏低 慢于 ZeRO-1，受 CPU 性能和 PCIe 带宽影响 ZeRO-2 + CPU Offload 在 ZeRO-2 的基础上，将优化器状态卸载到 CPU 内存；对较大模型进一步降低 GPU 显存占用，但会增加 CPU-GPU 数据传输开销。 较低 慢于 ZeRO-2，受 CPU 性能和 PCIe 带宽影响 ZeRO-3 + CPU Offload 在 ZeRO-3 的基础上，将优化器状态和模型参数卸载到 CPU；GPU 显存占用最小，但 CPU-GPU 通信量极大，且 CPU 带宽远小于 GPU-GPU 通信。 极低 非常慢 ZeRO-Infinity (NVMe Offload) 基于 ZeRO-3，将优化器状态、梯度和参数卸载到 NVMe，突破 CPU 内存限制，适合超大规模模型；性能高度依赖 NVMe 并行读写速度。 极低需 NVMe 支持 慢于 ZeRO-3，但通常优于 ZeRO-3 + CPU Offload 通信量与性能影响 ZeRO-0/1/2\n通信以 梯度同步 为主，使用 All-Reduce 操作，通信量相对较低。 ZeRO-3\n需要对模型参数进行 All-Gather/All-Reduce 操作，通信量显著增大，网络带宽成为关键瓶颈，前后传播时的参数广播进一步加剧通信负担。 CPU Offload（ZeRO-1/2/3 + CPU）\n卸载优化器状态或参数到 CPU，减少 GPU 显存占用。 通信量主要来自 CPU \u0026lt;-\u0026gt; GPU 数据传输，带宽远低于 GPU-GPU 通信，极易造成性能瓶颈，尤其在 ZeRO-3 场景下。 NVMe Offload（ZeRO-Infinity）\n在 ZeRO-3 的基础上进一步卸载至 NVMe，突破 CPU 内存限制以支持超大规模模型。 性能强烈依赖 NVMe I/O 带宽 和并行度，若 NVMe 速度足够高，通常优于 CPU Offload；但在 I/O 性能较弱或高延迟场景下，效果可能不佳。 硬件与配置影响 硬件限制\nPCIe 带宽、网络带宽、NVMe I/O 等对 Offload 性能有显著影响，需根据硬件环境选择最佳策略。 补充说明\nCPU Offload 利用 CPU 内存并通过 PCIe 传输数据；NVMe Offload 则将状态保存于 NVMe 设备。 NVMe Offload 在 NVMe I/O 性能充足 时通常优于 CPU Offload，但需避免因 I/O 性能不足导致的性能瓶颈。 与官方文档对照\n建议结合 DeepSpeed 官方文档 获取最新、最准确的配置参数和性能调优建议。 数据准备：决定训练成败的核心 数据质量直接决定了模型的性能。数据准备包括数据收集、清洗、去重、分类与配比、脱敏等步骤。\n预训练数据 数据来源 公开数据集：如：the-stack-v2、Common Crawl 等。 企业自有数据：内部文档、代码库、业务日志等。 网络爬虫：通过爬虫技术采集领域相关的网页内容。 数据规模 建议使用至少数亿到数十亿个 token，以确保模型能够充分学习领域知识。 当数据量不足时，模型效果可能受限，建议采用数据增强的方法来补充数据。 数据处理 数据预处理\n格式统一：对来自多个数据源的无标注大量语料进行处理，确保其格式一致。推荐使用高效的存储格式，如 Parquet，以提高数据读取和处理的效率。 数据去重\n检测方法：使用 MinHash、SimHash 或余弦相似度等算法进行近似重复检测。 处理粒度：可选择按句子、段落或文档级别去重，根据任务需求灵活调整。 相似度阈值：设定合理的相似度阈值（如 0.9），删除重复度高于阈值的文本，确保数据多样性。 数据清洗\n文本过滤：结合规则和模型评分器（如 BERT/RoBERTa）去除乱码、拼写错误和低质量文本。 格式化处理：优先使用 JSON 格式处理数据，确保代码、Markdown 和 LaTeX 等特殊格式的准确性。 数据脱敏\n隐私保护：匿名化或去除人名、电话号码、邮箱、密码等敏感信息，确保数据合规。 不合规内容过滤：剔除含有违法、色情或种族歧视等内容的数据块。 数据混合与配比\n比例控制：例如，将 70% 的领域特定数据与 30% 的通用数据相结合，避免模型遗忘通用能力。 任务类型：确保数据包含代码生成、问答对话、文档摘要、多轮对话和数学推理等多种任务类型。 数据顺序\n逐步引导：采用课程学习（Curriculum Learning）方法，从简单、干净的数据开始训练，逐步引入更复杂或噪声较高的数据，优化模型的学习效率和收敛路径。 语义连贯性：利用上下文预训练（In-context Pretraining）技术，将语义相似的文档拼接在一起，增强上下文一致性，提升模型的语义理解深度与泛化能力。 监督微调数据 数据格式 可采用 Alpaca 或 Vicuna 风格，比如结构化为 [instruction, input, output] 的单轮和多轮对话。\n规模：几千条到几十万条，具体根据项目需求和计算资源决定。 质量：确保数据的高质量和多样性，避免模型学习到错误或偏见。 数据构建 在数据构建过程中，我们首先收集日常业务数据，并与业务专家共同构建基础问题。随后，利用大语言模型进行数据增强，以提升数据的多样性和鲁棒性。以下是具体的数据增强策略：\n数据增强策略 表达多样化\n通过大语言模型对现有数据进行改写，采用同义词替换和语法变换等方法，增加数据的多样性。\n鲁棒性增强\n构建包含拼写错误、混合语言等输入的提示（Prompt），以模拟真实场景，同时确保生成答案的高质量。\n知识蒸馏\n利用 GPT-4、Claude 等大型语言模型进行知识蒸馏，生成符合需求的问答数据对。\n复杂任务设计\n针对复杂场景（如多轮对话、逻辑推理等），手动设计高质量数据，以覆盖模型的能力边界。\n数据生成管道\n构建自动化数据生成流水线，将数据生成、筛选、格式化和校验等环节集成，提高整体效率。\n关键要点 任务类型标注：每条数据标注明确的任务类型，便于后续精细化分析和调优。 多轮对话与话题切换：构建多轮对话中上下文关联与话题转换的数据，确保模型能够学习话题切换与上下文关联的能力。 思维链（Chain of Thought）策略：分类、推理等任务可先用 COT 生成过程性答案，提高准确率。 数据飞轮：上线后持续收集用户真实问题，结合真实需求迭代数据；定期清洗，确保质量与多样性。 偏好数据 数据格式 三元组结构：[prompt, chosen answer, rejected answer] 标注细节： 多模型采样：使用多个不同训练阶段或不同数据配比的模型生成回答，增加数据多样性。 编辑与优化：标注人员可对选择的回答进行小幅修改，确保回答质量。 采样策略 多模型采样：部署多个不同版本的模型，对同一 prompt 生成不同回答。 对比标注：由人工或自动化系统对生成的回答进行对比，选择更优的回答对。 关键要点 数据多样性与覆盖：确保偏好数据涵盖各种场景和任务，避免模型在特定情境下表现不佳。 高质量标注：偏好数据的质量直接影响模型的对齐效果，需确保标注准确且一致。 训练流程 一个完整的特定领域大语言模型训练流程通常包括 继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO) 三个主要步骤，最终实现模型的部署与持续优化。\n三种方法对比 训练方法概览 训练方法 主要目标 数据需求 典型应用场景 继续预训练 (CPT) 继续在大规模无监督语料上进行预训练，注入新领域知识 大量无标签文本（至少数亿到数十亿 Token） 补足领域知识，如法律、医疗、金融等专业文本 监督微调 (SFT) 在有监督标注的数据上进行微调，强化特定任务和指令执行能力 定制化标注数据（指令/对话对等），从几千到几十万条 各类特定任务，如代码生成、问答、文本改写、复杂指令执行等 直接偏好对齐 (DPO) 利用偏好数据（正例 chosen vs. 负例 rejected）直接对齐人类偏好 偏好数据：[prompt, chosen, rejected](规模相对较小) 对齐人类反馈，如回答风格、合规性、安全性等 优势与挑战 继续预训练 (CPT) 优势:\n更好领域覆盖，全面提升模型在特定领域的理解和生成能力。 无需额外手动标注。 挑战/局限:\n需要大量高质量领域数据。 训练成本高，需大规模算力与时间。 可能引入领域偏见，需谨慎处理数据质量与分布。 监督微调 (SFT) 优势:\n快速获取可用的任务执行能力。 显著提升模型对特定场景的准确性。 挑战/局限:\n数据标注成本较高。 需谨慎选择标注数据以避免过拟合。 微调后可能削弱模型的通用性。 直接偏好对齐 (DPO) 优势:\n无需单独训练 Reward Model。 数据量需求较小，调优效率高。 挑战/局限:\n需要可靠的偏好标注。 对复杂、多样化场景仍需持续迭代收集更多偏好数据。 易受偏好数据分布的限制。 通用训练要点与技术细节 在进行 CPT、SFT、DPO 三种训练方法时，存在许多通用的训练要点和技术细节。以下部分将这些通用内容进行统一描述，以便于更好地理解和应用。\n数据处理与准备 数据质量：无论是 CPT、SFT 还是 DPO，数据的质量都是至关重要的。需要确保数据的准确性、无歧义性和多样性。 数据格式：统一的数据格式有助于简化训练流程。例如，使用 JSON 或其他结构化格式来存储训练数据。 数据增强：通过 LLM 重写和优化等方式增加数据多样性，提升模型的泛化能力。 学习率与优化 学习率设置：通常采用比预训练阶段更小的学习率，如从 3e-4 降低到 3e-5，具体数值视任务和数据量而定。 学习率调度：使用 warm-up 策略（如前 10% 步骤线性递增），随后采用线性衰减或余弦退火策略，确保训练过程平稳。 优化器选择：根据模型规模和硬件资源选择合适的优化器，比如 AdamW。 训练策略 全参数微调：在资源允许的情况下，优先进行全参数微调，以确保模型能够全面掌握新知识。 参数高效微调（PEFT）：如 LoRA，适用于计算资源有限的场景，通过冻结部分参数并添加 adapter 实现高效微调。 混合精度训练：在支持的 GPU 上使用 bf16 或 fp16，降低显存占用，提高训练速度。 训练稳定性：采用梯度裁剪、正则化、dropout、权重衰减等技术，防止梯度爆炸和模型过拟合。 Flash Attention：利用 Flash Attention 技术优化注意力机制的计算效率，提升训练速度和降低显存占用。 监控与调优 收敛监控：实时监控训练集和验证集的 loss 曲线，确保模型逐步收敛，必要时调整学习率和其他超参数。 Checkpoint：定期保留 Checkpoint，防止意外中断导致全部训练进度丢失。 早停机制：防止模型过拟合，适时停止训练，保存最佳模型状态。 模型评估：在训练过程中定期进行评估，确保模型性能符合预期。 继续预训练 (CPT) 目标 通过在大量领域特定的无监督数据上继续预训练基座模型，注入新的领域知识，使模型更好地理解和生成特定领域的内容。\n训练要点 流式加载\n实施流式数据加载，以便在训练过程中动态读取数据，防止超过最大内存，训练中断。\n全参数微调\n在进行模型训练时，通常需要更新模型的全部参数，以确保模型能够全面掌握新知识。\n全量微调相较于参数高效微调（如 LoRA）在领域知识注入方面效果更佳，尤其在运算资源充足的情况下，建议优先选择全参数微调。\n监督微调 (SFT) 目标 通过高质量的标注数据，训练模型执行特定任务，如代码生成、代码修复、复杂指令执行等，提升模型的实用性和准确性。\n训练要点 Epoch 数量\n在数据量充足的情况下通常 1 ~ 4 个 epoch 即可见到显著效果。 如果数据量不够，可以考虑加大 epoch 数量，但要注意过拟合的风险，建议进行数据增强。 数据增强与多样性\n确保训练数据涵盖多种任务类型和指令表达方式，提升模型的泛化能力。 包含多轮对话和鲁棒性数据，增强模型应对真实用户场景的能力。 直接偏好对齐 (DPO) 目标 通过用户反馈和偏好数据，优化模型输出，使其更符合人类的期望和需求，包括回答风格、安全性和可读性等方面。\nDPO 的特点 直接优化\n不需要单独训练 Reward Model，直接通过 (chosen, rejected) 数据对模型进行对比学习。\n高效性\n相较于 PPO，DPO 需要更少的数据和计算资源即可达到相似甚至更好的效果。\n动态适应\n每次有新数据时，模型能立即适应，无需重新训练 Reward Model。\n训练要点 偏好数据的收集\n部署多个不同训练阶段或不同数据配比的模型，生成多样化的回答。 通过人工或自动化方式标注 chosen 和 rejected 回答对，确保数据的多样性和质量。 对比学习\n通过最大化 chosen 回答的概率，最小化 rejected 回答的概率，优化模型参数。\n迭代优化\n持续收集用户反馈，生成新的偏好数据，进行循环迭代，逐步提升模型性能。 结合数据飞轮机制，实现模型的持续进化与优化。 常见问题与解决方案 重复输出 (Repetitive Outputs)\n问题：模型生成内容重复，连续打印停不下来。\n解决方案：\n数据去重与清洗：确保训练数据不含大量重复内容。 检查 EOT（End-of-Token）设置：防止模型连接打印无法停止。 通过 SFT/DPO 进行对齐：优化模型输出质量。 调整解码策略：如增加 top_k、repetition penalty 和 temperature 参数。 灾难性遗忘 (Catastrophic Forgetting)\n问题：模型在微调过程中遗忘原有的通用能力，可以看作是在新的数据集上过拟合，原本模型参数空间变化过大。\n解决方案：\n混合一部分通用数据：保持模型的通用能力。 调低学习率：减少对原有知识的冲击。 增加 Dropout Rate 和 Weight Decay：避免过拟合。 采用 LoRA 等参数高效微调方法：避免大规模参数更新。 使用 RAG 辅助：结合外部知识库提升模型表现。 Chat Vector: 通过模型权重的简单算术操作，快速为模型注入对话和通用能力。 实体关系与推理路径理解不足\n问题：模型难以正确理解复杂的实体关系和推理路径。\n解决方案：\n引入 Chain-of-Thought (CoT) 数据与强化推理训练：\n通过分步推理训练提升模型的能力，结合 强化微调 和 o1/o3 的训练方法。 扩展训练数据覆盖面：\n引入更多包含复杂实体关系和推理路径的多样化场景数据。 结合知识图谱建模：\n利用 GraphRAG 强化模型对实体关系的理解与推理能力。 模型部署与评估 部署 推理框架\nollama：基于 llama.cpp 的本地推理部署，可快速启动 vLLM：主打高并发、多用户场景下的推理吞吐量优化 量化：将模型量化为 8-bit 或 4-bit，进一步降低推理成本，提高部署效率。 集成 RAG \u0026amp; 智能体\nRAG：结合向量知识库，实时检索相关文档或代码片段，辅助模型生成更精准的回答。 智能体：利用 Function Call 或多轮对话机制，让模型调用外部工具或进行复杂推理，提升互动性和实用性。 Langgraph：封装 RAG 和 多智能体工作流，构建定制化的对话系统或代码自动生成平台。 评估 评估指标\nCPT 阶段：使用领域内测试集，评估困惑度（Perplexity，PPL）或者交叉熵（Cross Entropy），衡量模型对新知识的掌握程度。 SFT / DPO 阶段： Human 或模型评测：通过人工评分或自动化工具，评估回答的准确性、连贯性、可读性和安全性。 代码生成：构建大规模单元测试集，评估 pass@k 指标，衡量代码生成的正确率。 通用能力：在常见 benchmark（如 MMLU、CMMLU）对模型进行测试，确保模型在通用任务上的表现下降不大。 解码超参数\n一致性：在评估过程中保持 top_k、top_p、temperature、max_new_tokens 等解码参数一致，确保评估结果的可比性。 网格搜索：在算力允许的情况下，对不同解码参数组合进行评估，选择最优的参数配置。 数据飞轮与持续迭代 数据飞轮机制\n实时收集用户日志\n收集线上用户的真实 prompt 和生成的回答，覆盖多样化的使用场景和任务类型。\n自动或人工标注\n对收集到的用户 prompt 和回答进行偏好标注，生成新的 (chosen, rejected) 数据对。\n迭代训练\n将新生成的偏好数据加入到下一轮的 SFT/DPO 训练中，不断优化模型的回答质量和用户体验。\n鲁棒性数据\n包含拼写错误、混合语言、模糊指令等数据，提升模型在真实场景下的鲁棒性和应对能力。\n持续优化\n反馈循环：利用用户反馈，持续改进训练数据和模型表现，实现模型的自我优化和进化。 多模型协同：部署多个版本的模型，生成多样化的回答，通过对比学习提升模型的综合能力。 结合意图识别和多智能体推理 使用意图分类模型让大模型判断用户输入意图类别。基于意图类别与上下文类型的映射，监督推理路径，然后根据推理路径进行多路召回。将这些信息提供给训练好的模型，生成最终结果。\n总结 通过 继续预训练 (CPT) → 监督微调 (SFT) → 直接偏好对齐 (DPO) 的组合方法，能够有效地在基座大模型上注入特定领域的知识，构建出具备高效解决业务问题能力的闭源大语言模型。关键步骤如下：\n数据准备\n高质量的数据收集、清洗、去重和分类，确保训练数据的多样性与准确性。 结合数据脱敏策略，保护隐私与合规。 模型训练\n通过 CPT 注入领域知识，SFT 学习特定任务模式，DPO 优化模型输出符合人类偏好和安全。 利用高效的并行训练框架和调参技巧，提升训练效率和资源利用率。 部署与评估\n采用高效的推理框架，结合 RAG 和 Agent 实现知识增强和功能扩展。 通过多维度评估，确保模型在各个阶段的表现符合预期。 持续迭代\n构建数据飞轮，实时收集用户反馈，不断优化训练数据和模型表现。 集成 RAG 和 Agent，实现模型能力的持续提升与扩展。 最终，通过系统化的流程和技术手段，能够构建一个不仅具备深厚领域知识，还能灵活应对复杂业务需求的长生命周期 AI 系统。\n参考资料 DeepSpeed Megatron-LM ollama vLLM GraphRAG The Llama 3 Herd of Models ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages Evaluating Large Language Models Trained on Code Direct Preference Optimization: Your Language Model is Secretly a Reward Model 引用 引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Jan 2025). 构建特定领域的大语言模型.\nhttps://syhya.github.io/posts/2025-01-05-domain-llm-training\nOr\n@article{syhya2024domainllm, title = \u0026#34;构建特定领域的大语言模型\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-05-domain-llm-training/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/","summary":"\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\n\u003cp\u003e随着大语言模型（LLM）在各行业的广泛应用，企业和研究团队面临将通用模型适配特定领域的迫切需求。通用大语言模型在处理特定领域任务时，往往无法满足深度需求。例如，在闭源编程语言的应用中，现有开源模型对其语法和语义的理解不足，导致在代码生成和纠错等任务中表现不佳。因此，注入领域知识并训练专属的大语言模型，成为提升开发效率和代码质量的关键步骤。\u003c/p\u003e","title":"构建特定领域的大语言模型"},{"content":"租用 GPU 还是购买 GPU？ 在构建深度学习工作环境之前，首先需要综合考虑 使用周期、预算、数据隐私 以及 维护成本。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。\n租用 GPU 的优点：\n无需一次性投入高额硬件成本 可根据项目需求弹性扩容 云厂商通常提供数据合规与安全保障，省去硬件运维烦恼 购买 GPU 的优点：\n长期大规模使用时，整体成本更低 对内部数据和模型有更高的隐私与可控性 硬件可随时调整、升级，部署更灵活 个人建议\n如果预算有限或只是初学阶段，可先使用 Colab、Kaggle 或云 GPU； 当算力需求和隐私需求上升时，再考虑自建多卡服务器或租用多机多卡集群。 背景 在 2023 年 9 月，为了在工作之余继续对大模型（LLM）进行探索和研究，我组装了一台 双 RTX 4090 的个人 AI 实验服务器。该服务器已运行近一年，整体体验如下：\n噪音：服务器放在脚边，满负荷训练时风扇噪音较大，但在日常推理或中等负载下可接受 推理性能：双卡共计 48GB 显存，采用 4bit 量化方案时可运行到 70B 级别的模型（如 Llama 70B、Qwen 72B） 训练性能：在使用 DeepSpeed 的分布式和 offload 技术（ZeRO-3 + CPU offload）后，可对 34B 左右的模型（如 CodeLlama 34B）进行微调 性价比：对于个人或小团队的日常实验和中小规模模型训练而言，该配置较为实用；但若进行超大规模模型的全参数训练，仍需更多专业卡（如多卡 A100 或 H100 集群） 下图展示了不同大小模型、不同训练方法对显存的需求： Fig. 1. Hardware Requirement. (Image source: LLaMA-Factory)\n搭建思路与配置详情 整机预算在 4 万元人民币（约 6000 美元） 左右，以下是我最终选用的配置清单，仅供参考：\n配件 型号 价格 (元) 显卡 RTX 4090 * 2 25098 主板 + CPU AMD R9 7900X + 微星 MPG X670E CARBON 5157.55 内存 美商海盗船(USCORSAIR) 48GB*2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + 三星 990PRO 4TB 4587 电源 美商海盗船 AX1600i 2699 风扇 追风者 T30 12cm P * 6 1066.76 散热 利民 Thermalright FC140 BLACK 419 机箱 PHANTEKS 追风者 620PC 全塔 897.99 显卡延长线 追风者 FL60 PCI-E4.0 *16 399 总计：约 42723.3 元\nGPU 选择 对于大模型研究，浮点运算性能（TFLOPS） 和 显存容量 是最核心的指标。专业卡（A100、H100 等）虽有更高显存以及 NVLink，但价格动辄数十万人民币，对个人用户并不友好。根据 Tim Dettmers 的调研，RTX 4090 在单位美元算力方面表现非常亮眼，且支持 BF16、Flash Attention 等新特性，因此成为高性价比的选择。\n散热方式：涡轮 vs 风冷 vs 水冷 散热方式 优点 缺点 适用场景 涡轮风扇 体积紧凑；适合并行多卡部署 噪音大、整体散热效率一般 企业服务器机柜、多卡密集部署 风冷散热 性能与噪音平衡佳、维护简单 显卡体型通常较大 家用或个人研究（主机摆放空间足够） 水冷散热 散热能力突出、满载噪音更低 可能会出现漏液、价格更高 对静音要求极高或极限超频场景 家用推荐：风冷卡 兼顾散热效率、噪音和维护成本；相对于涡轮卡和水冷卡更友好。\nCPU \u0026amp; 主板 在深度学习场景中，CPU 主要负责数据预处理、管道调度以及多进程/多线程并行管理，确保数据能够以高吞吐量、低延迟的方式传递到 GPU。因此，CPU 的核心需求主要包括 充足的 PCIe 通道 和 卓越的多线程性能。\nIntel：13/14 代 i9（如 13900K）拥有 20 条 PCIe 主通道，能够满足双卡 x8 + x8 的需求 AMD：Ryzen 7000/9000 系列（如 7950X）提供 28 条（可用 24 条）PCIe 通道，支持双卡 x8 + x8，并为 M.2 SSD 提供足够带宽 微星 MPG X670E CARBON 主板 扩展性：支持 PCIe 5.0 和 DDR5 内存，具备充足的未来升级空间 稳定性：高规格供电设计，保障 CPU 与多显卡的稳定运行 接口丰富：支持多块 M.2 SSD 和 USB4，满足多样化使用需求 AMD Ryzen 9 7900X 特点 核心与线程：12 核心、24 线程，在深度学习场景中的数据预处理和多任务处理方面表现强劲 PCIe 带宽：提供 28 条（可用 24 条）PCIe 5.0 通道，可轻松支持双卡 x8 + x8，并为 M.2 SSD 提供高速带宽 能效比：基于 Zen 4 架构，性能与能耗平衡优秀，适合高性能计算需求 主板选购要点 空间布局 RTX 4090 尺寸庞大且卡槽较厚，需确认主板是否能同时容纳两张显卡；若存在空间或散热冲突，可使用显卡延长线竖放第二张卡。 PCIe 通道拆分 主板需至少支持双 PCIe 4.0 x8 + x8 配置，以避免出现 x16 + x2 的情况。x16 + x2 的带宽分配会显著限制第二块 GPU 的数据传输能力，进而影响 GPU 与 CPU 之间的数据交换效率。在大模型训练中，这种带宽瓶颈可能导致性能显著下降，严重影响整体训练效率。 扩展性 在双显卡插满的情况下，仍需确保主板具有足够的 M.2 SSD 插槽和外设接口 综合扩展性、性能与性价比等因素，我最终选择了 AMD Ryzen 9 7900X 搭配 微星 MPG X670E CARBON 主板 的组合。通过显卡延长线解决了 4090 双卡过厚导致的插槽冲突问题。\nBIOS 设置建议 内存优化 开启 XMP/EXPO（对应 Intel/AMD）以提升内存频率，增强带宽性能。 超频调整 如果需要进一步提升性能，可在 BIOS 中启用 PBO（Precision Boost Overdrive） 或 Intel Performance Tuning，并结合系统监控工具观察稳定性。 温度与稳定性 避免过度超频，注意控制温度，避免因崩溃或过热导致系统不稳定。 内存 深度学习训练中，内存会被大量占用用于数据加载、模型优化状态储存（尤其在多 GPU Zero-stage 并行场景下）。内存容量最好 ≥ 显存总容量的两倍。本配置中，使用了 48GB * 2（共 96GB），满足日常多任务和分布式训练的需求，减少内存不足导致的频繁 swap。\n硬盘 优先选用 M.2 NVMe SSD：其读写性能更优，对加载超大模型权重、缓存中间文件、训练日志等都有显著速度提升 容量建议 ≥ 2TB：随着大模型文件越来越庞大，2TB 往往很快就会被占满，建议根据自身需求选 4TB 或更多 SSD 品牌：三星、海力士或西部数据等主流大厂都拥有稳定的高端产品线 电源 双 4090 满载时整机功耗可达 900W~1000W 左右，CPU、主板和硬盘等还需额外功率余量。通常建议选择 1500W 以上 的铂金或钛金电源，以确保在高负载下电流供给稳定、降低电压波动带来的系统不稳定。\n我在此使用美商海盗船 AX1600i（数字电源），可以通过软件监控实时功耗，并提供充足冗余。\n散热与风扇 我采用 风冷 方案，包括：\nCPU 散热器：利民 FC140（双塔式气冷方案，兼顾了较高的散热效率和相对低噪音） 机箱风扇：追风者 T30 12cm * 6，保持机箱内部正压或者稍微正压的风道布局，保证显卡和供电模块的进风顺畅 在 GPU 长时间高负载训练（如分布式训练大型模型）时，机箱内的风道管理和风扇配置非常重要。建议使用监控软件及时查看 CPU、GPU、主板供电模块温度，适度调节风扇转速。\n散热进阶\n若对静音有更高要求，可考虑 Hybrid 散热（半水冷方案）或更精细的风扇调速曲线。 适度清理机箱灰尘、使用防尘网并定期更换导热硅脂也能提升散热和稳定性。 机箱 RTX 4090 体型巨大，且双卡堆叠时需要充足的内部空间和散热风道。全塔机箱能提供更好的走线空间和气流组织。我选用了 PHANTEKS 追风者 620PC，除了体型大、空间充裕外，也自带良好的线缆管理通道。\n装机完成后的图片如下：\nFig. 2. Computer\n系统与软件环境 操作系统方面强烈推荐 Linux，例如 Ubuntu 22.04 LTS，因其对 CUDA、NVIDIA 驱动以及常见深度学习框架有更好的支持和兼容性。大致流程如下：\n安装 OS：使用 Ubuntu 或其他 Linux 系统即可。 安装 NVIDIA 驱动：确保 nvidia-smi 能正确识别两张 4090:\nFig. 3. nvidia-smi Output\n安装 CUDA 工具链：通过 nvcc -V 确认版本信息:\nFig. 4. nvcc -V Output\n安装 cuDNN：确保深度学习框架可以调用 GPU 加速卷积和 RNN 等操作 测试框架：使用 PyTorch、TensorFlow 或 JAX 简单测试模型推理/训练是否正常 Docker 容器化： 利用 nvidia-container-toolkit 让容器直接访问 GPU 资源，避免主机环境污染。 在多机多卡环境下，还能结合 Kubernetes、Ray 或 Slurm 等进行集群调度与资源管理。 常用工具与框架推荐 训练框架\nLLaMA-Factory：对大语言模型训练/推理流程有较好封装，新手友好 DeepSpeed：支持大模型分布式训练、多种并行策略和优化功能 Megatron-LM：NVIDIA 官方的大规模语言模型训练框架，适合多机多卡场景 监控 \u0026amp; 可视化\nWeights \u0026amp; Biases 或 TensorBoard：实时监控训练过程中的损失函数、学习率等指标，支持远程可视化 推理工具\nollama：基于 llama.cpp 的本地推理部署，可快速启动 vLLM：主打高并发、多用户场景下的推理吞吐量优化 Framework Ollama vLLM 作用 简易本地部署 LLM 高并发 / 高吞吐的 LLM 推理 多请求处理 并发数增加时，推理速度下降明显 并发数增大也能保持较高吞吐 16 路并发 ~17 秒/请求 ~9 秒/请求 吞吐对比 Token 生成速度较慢 Token 生成速度可提升约 2 倍 极限并发 32 路以上并发时，性能衰减较严重 仍能平稳处理高并发 适用场景 个人项目或低并发应用 企业级或多用户并发访问 WebUI\nOpen-WebUI：基于 Web 界面的多合一 AI 界面，支持多种后端推理（ollama、OpenAI API 等），便于快速原型和可视化 进阶建议 开发与调试效率\n使用 SSH 工具提升远程开发效率，制作自定义容器镜像减少环境配置时间。\n量化与剪枝\n通过 4bit、8bit 量化和剪枝技术，减少模型参数和显存需求，优化推理性能。\n混合精度训练\n使用 BF16 或 FP16 提升训练速度，结合 GradScaler 提高数值稳定性。\nCPU 协同优化\n使用多线程、多进程或 RAM Disk 缓存优化数据加载，支持流式加载大规模预训练数据集。\n多机集群部署\n通过 InfiniBand 或高速以太网搭建集群，使用 Kubernetes 实现高效资源调度。\n总结 通过以上配置与思路，我成功搭建了一台 双卡 RTX 4090 深度学习主机。它在 推理 和 中小规模微调 场景中表现良好，对于想要在个人或小团队环境下进行大模型（LLM）科研或应用探索的人来说，这种方案兼具 性价比 与 灵活性。当然，如果要大规模全参数训练上百亿乃至上千亿参数的大模型，依然需要更多 GPU（如多卡 A100/H100 集群）。\n就个人体验而言，双 4090 在预算范围内提供了较好的训练与推理性能，可以满足绝大部分中小规模研究与实验需求，值得有条件的个人或小团队参考。\n参考资料 Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe 通道规格 AMD R5 7600X PCIe 通道规格 MSI MPG X670E CARBON 规格 nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI 版权声明与引用 声明：本文所涉及的配置清单、价格与建议仅供技术交流与研究参考。实际购买与部署请结合个人预算和需求进行综合评估。若因参考或采纳文中信息导致任何直接或间接后果，本文作者恕不承担责任。\n引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Dec 2024). 基于双卡 RTX 4090 搭建家用深度学习主机. https://syhya.github.io/posts/2024-12-21-build-gpu-server\nOr\n@article{syhya2024build, title = \u0026#34;基于双卡 RTX 4090 搭建家用深度学习主机\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Dec\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2024-12-21-build-gpu-server/\u0026#34; ","permalink":"https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/","summary":"\u003ch2 id=\"租用-gpu-还是购买-gpu\"\u003e租用 GPU 还是购买 GPU？\u003c/h2\u003e\n\u003cp\u003e在构建深度学习工作环境之前，首先需要综合考虑 \u003cstrong\u003e使用周期\u003c/strong\u003e、\u003cstrong\u003e预算\u003c/strong\u003e、\u003cstrong\u003e数据隐私\u003c/strong\u003e 以及 \u003cstrong\u003e维护成本\u003c/strong\u003e。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。\u003c/p\u003e","title":"基于双卡 RTX 4090 搭建家用深度学习主机"},{"content":"摘要 股票市场是金融市场的重要组成部分，近些年来，股票市场蓬勃发展，股票价格预测和量化投资策略研究吸引了许多领域的研究学者。其中最近几年随着人工智能和机器学习的发展，学者们从传统的统计学模型迁移到了人工智能算法，尤其是在深度学习热潮掀起后，神经网络在股票价格预测和量化投资策略研究中取得了不错的效果。深度学习的目标是学习多层次的特征，通过组合低级特征构建抽象的高级特征，从而挖掘数据的分布式特征表示，基于此进行复杂的非线性建模，从而实现预测任务。其中 RNN 被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价，交易信息都是序列数据，因此之前有很多研究者，基于 RNN 来预测股票价格。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题，之后出现了诸如 GRU，Peephole LSTM，BiLSTM 等 LSTM 的变体。但传统的股票预测模型有些并未考虑时间因素，有些仅考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。\n本文实验第一部分通过利用国内浦发银行和国外 IBM 的股票数据，分别建立了 LSTM，GRU，BiLSTM 的股票预测模型，通过比较这三种深度学习模型最后预测的结果，发现对于两个数据集都是 BiLSTM 模型优于其他模型，有更好的预测准确率。第二部分通过使用 A 股全市场的股票数据，并先使用 LightGBM 模型进行对 50 个因子的篮选，选出重要程度最高的 10 个因子。之后再用 BiLSTM 模型选取进行因子组合，建立量化投资策略，最后对该策略进行实证与回测，发现该策略优于市场基准指数，说明了 BiLSTM 模型在股票价格预测和量化投资的实际应用价值。\n关键词：量化投资；深度学习；神经网络模型；多因子选股；BiLSTM\n第一章 绪论 1.1 研究背景与意义 1.1.1 研究背景 从 1970 年逐渐兴起，量化投资进入各投资者的视野，一场新的革命就此拉开序幕，改变了从前被动管理和有效市场假说主导的投资组合管理局面。有效市场假说认为，在市场有效的前提下，股票价格能反映市场的所有信息，不存在超额收益。被动投资管理的投资理念是市场是有效的，更加注重资产类别，最常见的方法是购买指数基金并关注已发布指数的表现。而主动投资管理主要依赖于投资者对于市场和个股的主观判定，根据能够获取的公开数据，通过将数学模型应用于金融领域，对股票进行评判，从而构建投资组合获取收益。量化投资通过对大量历史数据进行统计处理，挖掘投资机会，规避主观因素，受到越来越多投资者的追捧。量化投资兴起以后，人们也逐渐利用各种技术来进行股票价格的预测，从而更好地建立量化投资策略。早期国内外学者采用统计学方法建模，来预测股票价格，如指数平均法，多元回归法，自回归平均移动模型等，但是由于股票市场受多种因素影响，同时数据量很大，导致股票预测难度很大，各种统计学模型预测效果并不令人满意。\n近几年来，机器学习，深度学习和神经网络等相关技术不断发展，为股票价格预测和量化策略的构建提供了技术支持，不少学者通过随机森林法，神经网络，支持向量机和卷积神经网络等方法完成新的突破。股票市场足够的历史数据加之以多元的技术支撑，为股票价格预测和量化策略的构建提供了有利条件。\n1.1.2 研究意义 从国家经济体系和金融市场的长远发展来看，对于股票价格预测模型和量化投资策略的研究必不可少。我国起步较晚，金融市场不够成熟，金融工具不够丰富，市场效率较低，但是近几年来国家逐渐放宽政策，大力建设金融市场，为量化投资的发展提供“温床”，发展量化投资及新兴金融技术可以提供我们国家金融市场弯道超车的机会。并且股票价格指数作为一项重要的经济指标，对于我国经济发展起着晴雨表的作用。\n从个人投资者和机构投资者的角度来看，构建股票价格预测模型和量化投资策略模型提高了市场的有效性。个人投资者的专业知识不够完善，投资行为具有一定的盲目性，构建相关模型，为其提供参考，能够减少判断失误的概率，改变个人投资者在资本市场处于相对弱势的地位。并且对于机构投资者而言，理性客观的模型加之以个人经验的判断，提高了决策的正确性，为投资行为提供了新的方向。\n综上，结合我国目前的发展现状，本文选取个股进行股票价格预测模型和 A 股全市场的股票进行量化策略研究有重要的现实研究意义。\n1.2 研究综述 White（1988）$^{[1]}$ 使用 BP 神经网络来预测 IBM 股票的日收益率。然而，由于 BP 神经网络模型易受梯度爆炸的影响，导致模型无法收敛到全局最小值，从而无法实现准确的预测。\nKimoto（1990）$^{[2]}$ 使用模块化神经网络技术开发了一个用于东证股价指数（Tokyo Stock Exchange Prices Indexes，TOPIX）预测的系统。该系统不仅成功预测了东京证券交易所的 TOPIX，还通过基于预测结果的股票交易模拟，实现了一定程度的盈利。\nG．Peter Zhang（2003）$^{[3]}$ 对差分整合移动平均自回归（Autoregressive Integrated Moving Average，ARIMA）模型和人工神经网络（Artificial Neural Network，ANN）模型在时间序列预测中的性能进行了对比研究。结果显示，ANN 模型在时间序列预测的精度上显著优于 ARIMA 模型。\nRyo Akita（2016）$^{[4]}$ 选取消费者物价指数、市盈率以及报纸上的各种事件作为特征，利用段落向量和 LSTM 网络构建了一个金融时间序列预测模型。通过东京证券交易所五十家上市公司的实际数据，验证了该模型在股票开盘价预测方面的有效性。\nKunihiro Miyazaki（2017）$^{[5]}$ 通过提取股票日线图像及每 30 分钟的股票价格数据，构建了一个针对东证核心 30 指数（Topix Core 30）及其成分股涨跌预测的模型。研究对比了多种模型，包括逻辑回归（Logistic Regression, LR）、随机森林（Random Forest, RF）、多层感知器（Multilayer Perceptron, MLP）、LSTM、CNN、PCA-CNN 和 CNN-LSTM。结果表明，LSTM 在预测性能上最优，CNN 表现一般，但结合 CNN 和 LSTM 的混合模型可以提升预测精度。\nTaewook Kim（2019）$^{[6]}$ 提出了一个 LSTM-CNN 混合模型，用于结合股票价格时间序列与股票价格图像两种数据表示形式的特征，以预测 S\u0026amp;P 500 指数的股价。研究表明，LSTM-CNN 模型在股价预测方面优于单一模型，并且这种预测对于构建量化投资策略具有一定的实际意义。\n1.3 论文的创新点 本文股票预测方面具有以下创新点：\n分别选用国内 A 股上海浦东发展银行和国外美股 IBM 的数据进行研究，避免单一市场研究的局限性。并且传统的 BP 模型从未考虑时间因素，要么像 LSTM 模型考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。实证过程中并与 LSTM 模型，GRU 模型进行对比，说明其能够提升预测准确率。 股票价格预测模型采用股票多特征进行训练，包括开盘价，闭盘价，最高价和交易量等特征，相对于单特征预测股票收盘价理论上更加精确，能更好地比较 LSTM，GRU 和 BiLSTM 对于股票的预测效果。 本文量化策略研究方面具有以下创新点：\n未使用市面上已有的常见因子，使用自己通过遗传规划算法（Genetic Programming，GP）和人工数据挖掘得到的多个价量因子，并通过 LightGBM 模型进行对 50 个因子的筛选，选出重要程度最高的 10 个因子。 传统的量化投资模型一般用 LSTM 模型和 CNN 模型构建量化投资策略，本文使用 A 股全市场的数据，利用 BiLSTM 模型选取进行因子组合，建立量化投资策略，最后对该策略进行实证与回测，发现该策略优于市场基准指数（中证全指），说明了 BiLSTM 模型在股票价格预测和量化投资的实际应用价值。 1.4 论文研究框架 本文基于深度学习算法分别进行了股票价格预测和量化策略研究，本文的具体研究框架如 Fig. 1 所示：\nFig. 1. Research Framework.\n本文具体研究框架内容如下：\n第一章为绪论。本章首先对股票价格预测和量化策略研究的研究意义和研究背景进行了介绍。随后对研究现状进行了综述，然后说明了本文相比现有的研究的创新点，最后简要阐述了本文的研究框架。\n第二章为相关理论基础。本章对本文研究中涉及到的深度学习模型和量化选股的基本理论进行了介绍。深度学习模型小节依次介绍了 RNN，LSTM，GRU 和 BiLSTM 这四个深度学习模型，其中重点介绍了 LSTM 模型的内部结构。量化选股理论小节依次介绍了均值－方差模型，资本资产定价模型，套利定价理论，多因子模型，Fama－French 三因子模型和五因子模型。本小节从各种金融理论和模型发展脉络中介绍了多因子量化选股的历程。\n第三章为 LSTM，GRU 和 BiLSTM 在股票价格预测中比较研究。本章首先介绍了实验所用国内及国外股票的数据集，然后对于数据进行归一化和数据划分的预处理步骤。紧接着说明了本章所使用 LSTM，GRU 和 BiLSTM 这三个模型的网络结构，模型的编译和超参数设置，并进行了实验得到实验结果。最后对实验结果进行分析和本章小结。\n第四章为基于 LightGBM-BiLSTM 的量化投资模型研究。本章首先大致介绍了实验步骤，然后分别介绍了实验所用的股票数据和因子数据。之后再对因子依次进行缺失值处理，去极值，因子标准化和因子中性化处理得到清洗后的因子。随后再利用 LightGBM 和 BiLSTM 分别进行因子选择和因子组合，最后根据得到的模型进行量化策略构建，并对量化策略进行回测。\n第五章为总结与展望。本章对于本文关于股票价格预测与量化投资策略的主要研究内容进行了总结，之后针对目前研究所存在的不足，对未来研究的方向进行了展望。\n第二章 相关理论基础 2.1 深度学习模型 2.1.1 RNN 循环神经网络（Recurrent Neural Network，RNN）被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价和交易信息都是序列数据，因此之前有很多工作，基于 RNN 来预测股票价格。RNN 采用十分简单的重复模块的链式结构，例如单个 tanh 层。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题。Fig. 2 是 RNN 结构图。\nFig. 2. RNN Structure Diagram. (Image source: Understanding LSTM Networks)\n2.1.2 LSTM 长短时记忆网络（Long Short-Term Memory，LSTM）是一种特殊的 RNN，能够学习长期依赖关系。它们是由 Hochreiter \u0026amp; Schmidhuber（1997）$^{[7]}$ 提出的，并在随后的工作中被许多人改进和推广。由于独特的设计结构，LSTM 有着间隙长度相对不敏感的特点，并且解决了传统 RNN 的梯度消失和梯度爆炸的问题。相对于传统 RNN 和隐马尔可夫模型（Hidden Markov Model，HMM）等其他时间序列模型，LSTM 能更好地处理和预测时间序列中间隔和延迟非常长的重要事件。因此，LSTM 广泛应用在机器翻译和语音识别的领域。\nLSTM 被明确设计为避免长期依赖问题。所有的递归神经网络都具有神经网络的重复模块链的形式，而 LSTM 对 RNN 的结构进行了改进。LSTM 并没有采用单一神经网络层，而是采用一种特殊的方式进行交互的四层结构。\nFig. 3. LSTM Structure Diagram 1. (Image source: Understanding LSTM Networks)\nFig. 4. LSTM Structure Diagram 2. (Image source: Understanding LSTM Networks)\n如 Fig.3 所示，黑线用来表示传输一个节点的输出向量到另一个节点的输入向量。神经网络层（Neural network layer）是带有 $\\sigma$ 激活函数或者 tanh 激活函数的处理模块；逐点运算（Pointwise operation）是代表向量与向量之间进行点乘运算；向量传输（Vector transfer）是表示信息传递方向；汇合（Concatenate）和复制（Copy）分别用两个黑线合在一起和两个黑线分开来表示信息的汇合和信息的复制。\n下面我们以 LSTM 为例，进行其结构详细的说明。\n遗忘门（forget gate） Fig. 5. Forget Gate Calculation (Image source: Understanding LSTM Networks)\n$$ f_{t} = \\sigma\\left(W_{f} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{f}\\right) $$参数说明：\n$h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $\\sigma$ ：sigmoid 激活函数 $W_{f}$ ：遗忘门的权重矩阵 $b_{f}$ ：遗忘门的偏差向量参数 第一步如 Fig.5 所示，是一个决定从细胞状态中丢弃的信息的过程。该过程由 sigmoid 函数计算得到 $f_{t}$ 的值（$f_{t}$ 的范围在 0 到 1 之间，其中 0 代表完全不通过，1 代表完全通过）来决定细胞状态 $C_{t-1}$ 通过或者部分通过。\n输入门（input gate） Fig. 6. Input Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} i_{t} \u0026= \\sigma\\left(W_{i} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{i}\\right) \\\\ \\tilde{C}_{t} \u0026= \\tanh\\left(W_{C} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{C}\\right) \\end{aligned} $$参数说明：\n$h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $\\sigma$ ：sigmoid 激活函数 $W_{i}$ ：输入门的权重矩阵 $b_{i}$ ：输入门的偏差向量参数 $W_{C}$ ：细胞状态的权重矩阵 $b_{C}$ ：细胞状态的偏差向量参数 $\\tanh$ ：tanh 激活函数 第二步如 Fig.6 所示，通过 sigmoid 函数计算我们要在细胞状态中存储什么信息，接下来通过一个 $\\tanh$ 层创建候选向量 $\\tilde{C}_{t}$，该向量将会被加到细胞的状态中。\nFig. 7. Current Cell State Calculation (Image source: Understanding LSTM Networks)\n$$ C_{t} = f_{t} * C_{t-1} + i_{t} * \\tilde{C}_{t} $$参数说明：\n$C_{t-1}$ ：上一时刻的细胞状态 $\\tilde{C}_{t}$ ：临时细胞状态 $i_{t}$ ：输入门的值 $f_{t}$ ：遗忘门的值 第三步如 Fig.7 所示，当前时刻的细胞状态 $C_t$ 通过结合遗忘门和输入门的作用计算得到。\n遗忘门 $f_t$ 对上一时刻的细胞状态 $C_{t-1}$ 进行加权，以丢弃不需要的信息。 输入门 $i_t$ 对候选细胞状态 $\\tilde{C}_t$ 进行加权，决定引入多少新信息。\n最终，两个部分相加，更新得出当前时刻的细胞状态 $C_t$。 输出门（output gate） Fig. 8. Output Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} o_{t} \u0026= \\sigma\\left(W_{o} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{o}\\right) \\\\ h_{t} \u0026= o_{t} * \\tanh\\left(C_{t}\\right) \\end{aligned} $$参数说明：\n$o_{t}$ ：输出门的值 $\\sigma$ ：sigmoid 激活函数 $W_{o}$ ：输出门的权重矩阵 $h_{t-1}$ ：前一时刻的输出 $x_{t}$ ：当前时刻的输入 $b_{o}$ ：输出门的偏差向量参数 $h_{t}$ ：当前时刻的输出 $\\tanh$ ：tanh 激活函数 $C_{t}$ ：当前时刻的细胞状态 第四步如 Fig.8 所示，使用 sigmoid 函数计算输出门的值，最后通过 tanh 激活函数将这一时刻的细胞状态 $C_{t}$ 进行处理并与输出门的值 $o_{t}$ 相乘得到当前时刻的输出 $h_{t}$。\n2.1.3 GRU K. Cho（2014）$^{[8]}$ 提出了门控循环单元（Gated Recurrent Unit，GRU）。GRU 主要是在 LSTM 的基础上进行了简化和调整，将 LSTM 原有的遗忘门、输入门和输出门合并为更新门（update gate）和重置门（reset gate）。此外，GRU 还将细胞状态与隐藏状态合并，从而减少了模型的复杂性，同时在某些任务中仍能够达到与 LSTM 相当的性能。\n该模型在训练数据集比较大的情况下可以节省很多时间，在某些较小和较不频繁的数据集上表现出更好的性能$^{[9][10]}$。\nFig. 9. GRU Structure Diagram (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} z_{t} \u0026= \\sigma\\left(W_{z} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ r_{t} \u0026= \\sigma\\left(W_{r} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ \\tilde{h}_{t} \u0026= \\tanh\\left(W \\cdot \\left[r_{t} * h_{t-1}, x_{t}\\right]\\right) \\\\ h_{t} \u0026= \\left(1 - z_{t}\\right) * h_{t-1} + z_{t} * \\tilde{h}_{t} \\end{aligned} $$参数说明：\n$z_{t}$ ：更新门的值 $r_{t}$ ：重置门的值 $W_{z}$ ：更新门的权重矩阵 $W_{r}$ ：重置门的权重矩阵 $\\tilde{h}_{t}$ ：临时的输出 2.1.4 BiLSTM 双向长短时记忆网络（Bidirectional Long Short-Term Memory，BiLSTM）是由前向的 LSTM 与后向的 LSTM 结合成的。BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，能够学习到对时间有长期依赖性的信息，与单向 LSTM 相比可以更好地考虑未来数据的反向影响。Fig. 10 是 BiLSTM 结构图。\nFig. 10. BiLSTM Structure Diagram. (Image source: Baeldung)\n2.2 量化选股理论 2.2.1 均值－方差模型 量化选股策略起源于 20 世纪 50 年代，Markowitz（1952）$^{[11]}$ 提出了均值－方差模型（Mean-Variance Model）。该模型不仅奠定了现代投资组合理论的基础，将投资风险量化，还建立了一个描述风险和预期收益率的具体模型。它打破了以往仅对投资组合进行定性分析而缺乏定量分析的局面，将数学模型成功引入金融投资领域。\n$$ \\begin{aligned} \\mathrm{E}\\left(R_{p}\\right) \u0026= \\sum_{i=1}^{n} w_{i} \\mathrm{E}\\left(R_{i}\\right) \\\\ \\sigma_{p}^{2} \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\operatorname{Cov}\\left(R_{i}, R_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\sigma_{i} \\sigma_{j} \\rho_{ij} \\\\ \\sigma_{i} \u0026= \\sqrt{\\operatorname{Var}\\left(R_{i}\\right)}, \\quad \\rho_{ij} = \\operatorname{Corr}\\left(R_{i}, R_{j}\\right) \\end{aligned} $$$$ \\min \\sigma_{p}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} \\mathrm{E}\\left(R_{i}\\right) w_{i} = \\mu_{p}, \\quad \\sum_{i=1}^{n} w_{i} = 1 $$$$ \\begin{aligned} \\Omega \u0026= \\begin{pmatrix} \\sigma_{11} \u0026 \\cdots \u0026 \\sigma_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\sigma_{n1} \u0026 \\cdots \u0026 \\sigma_{nn} \\end{pmatrix} = \\begin{pmatrix} \\operatorname{Var}\\left(R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Cov}\\left(R_{1}, R_{n}\\right) \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\operatorname{Cov}\\left(R_{n}, R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Var}\\left(R_{n}\\right) \\end{pmatrix} \\\\ \\Omega^{-1} \u0026= \\begin{pmatrix} v_{11} \u0026 \\cdots \u0026 v_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ v_{n1} \u0026 \\cdots \u0026 v_{nn} \\end{pmatrix} \\\\ w_{i} \u0026= \\frac{1}{D}\\left(\\mu_{p} \\sum_{j=1}^{n} v_{ij}\\left(C \\mathrm{E}\\left(R_{j}\\right) - A\\right) + \\sum_{j=1}^{n} v_{ij}\\left(B - A \\mathrm{E}\\left(R_{j}\\right)\\right)\\right), \\quad i = 1, \\ldots, n \\end{aligned} $$$$ \\begin{aligned} A \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{j}\\right), \\quad B = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{i}\\right) \\mathrm{E}\\left(R_{j}\\right), \\quad C = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij}, \\quad D = BC - A^{2} \u003e 0 \\\\ \\sigma_{p}^{2} \u0026= \\frac{C \\mu_{p}^{2} - 2A \\mu_{p} + B}{D} \\end{aligned} $$其中：\n$\\mathrm{E}\\left(R_{p}\\right)$ 和 $\\mu_{p}$ 是投资组合 $p$ 的预期收益率 $\\mathrm{E}\\left(R_{i}\\right)$ 是资产 $i$ 的预期收益率 $\\sigma_{i}$ 是资产 $i$ 的标准差 $\\sigma_{j}$ 是资产 $j$ 的标准差 $w_{i}$ 是资产 $i$ 在投资组合中的比例 $\\sigma_{p}^{2}$ 是投资组合 $p$ 的方差 $\\rho_{ij}$ 是资产 $i$ 和资产 $j$ 之间的相关系数 通过以上公式$^{[12]}$，我们可以构建投资组合，来让我们的投资组合在一定的期望收益率的条件下，非系统风险降低到最小。\n2.2.2 资本资产定价模型 William Sharpe（1964）$^{[13]}$、John Lintner（1965）$^{[14]}$ 和 Jan Mossin（1966）$^{[15]}$ 提出了资本资产定价模型（Capital Asset Pricing Model，CAPM）。该模型认为，一项资产的预期收益与该资产的风险度量 $\\beta$ 值相关。这一模型通过简单的线性关系，将资产的预期收益率与市场风险联系起来，使得 Markowitz（1952）$^{[11]}$ 的投资组合选择理论更贴近现实世界，同时为多因子选股模型的建立奠定了理论基础。\n根据资本资产定价模型，对于一个给定的资产 $i$，它的预期收益率和市场投资组合的预期收益率之间的关系可以表示为：\n$$ E\\left(r_{i}\\right) = r_{f} + \\beta_{im}\\left[E\\left(r_{m}\\right) - r_{f}\\right] $$其中：\n$E\\left(r_{i}\\right)$ 是资产 $i$ 的预期收益率 $r_{f}$ 是无风险利率 $\\beta_{im}$（Beta）是资产 $i$ 的系统性风险系数，$\\beta_{im} = \\frac{\\operatorname{Cov}\\left(r_{i}, r_{m}\\right)}{\\operatorname{Var}\\left(r_{m}\\right)}$ $E\\left(r_{m}\\right)$ 是市场投资组合 $m$ 的预期收益率 $E\\left(r_{m}\\right) - r_{f}$ 是市场风险溢价 2.2.3 套利定价理论和多因子模型 Ross（1976）$^{[16]}$ 提出了套利定价理论（Arbitrage Pricing Theory，APT）。该理论认为，套利行为是形成市场均衡价格的决定性因素，通过在收益率形成过程中引入一系列因子构建线性相关关系，克服了资本资产定价模型（CAPM）的局限性，为后续研究者提供了重要的理论指导。\n套利定价理论被认为是多因子模型（Multiple-Factor Model，MFM）的理论基础，是资产价格模型的重要组成部分，也是资产价格理论的基石之一。多因子模型的一般表达形式为：\n$$ r_{j} = a_{j} + \\lambda_{j1} f_{1} + \\lambda_{j2} f_{2} + \\cdots + \\lambda_{jn} f_{n} + \\delta_{j} $$其中：\n$r_{j}$ 是资产 $j$ 的收益率 $a_{j}$ 是资产 $j$ 的常数 $f_{n}$ 是系统性因素 $\\lambda_{jn}$ 是因子载荷 $\\delta_{j}$ 是随机误差 2.2.4 Fama－French 三因子模型和五因子模型 Fama（1992）和 French（1992）$^{[17]}$ 使用横截面回归与时间序列结合的方法发现，股票市场的 $\\beta$ 值无法解释不同股票回报率的差异，而上市公司的市值、账面市值比和市盈率等可以显著解释股票回报率的差异。他们认为超额收益是对 CAPM 中 $\\beta$ 未能反映的风险因素的补偿，由此提出了 Fama－French 三因子模型。这三个因子分别为：\n市场风险溢价因子（Market Risk Premium）\n表示市场整体的系统性风险，即市场投资组合的预期收益减去无风险利率的差值。 衡量投资者承担系统性风险（即无法通过分散投资消除的风险）所期望的超额回报。 计算公式为：\n$$ \\text{Market Risk Premium} = E(R_m) - R_f $$ 其中 $E(R_m)$ 是市场的预期收益率，$R_f$ 是无风险利率。 市值因子（Size, SMB: Small Minus Big）\n表示小市值股票与大市值股票之间的收益差异。 小市值股票通常风险更高，但历史数据显示，其预期收益也往往高于大市值股票。 计算公式为：\n$$ SMB = R_{\\text{Small}} - R_{\\text{Big}} $$ 反映了市场对小市值股票的额外风险溢价的补偿。 账面市值比因子（Value, HML: High Minus Low）\n反映高账面市值比（即“价值型股票”）与低账面市值比（即“成长型股票”）之间的收益差异。 高账面市值比的股票通常定价较低（被市场低估），但长期来看可能获得较高回报。 计算公式为：\n$$ HML = R_{\\text{High}} - R_{\\text{Low}} $$ 低账面市值比的股票可能因市场对其过于乐观的预期而被高估。 该模型将 APT 模型中的因子具体化，并得出结论：投资小市值、高成长的股票具有高风险高收益的特性。Fama－French 三因子模型被广泛应用于现代投资行为的分析和实践中。\n随后，Fama（2015）和 French（2015）$^{[18]}$ 对三因子模型进行了扩展，新增了以下两个因子：\n盈利水平因子（Profitability, RMW: Robust Minus Weak）\n反映高盈利公司与低盈利公司之间的收益差异。 盈利能力强的公司（高 ROE、净利润率）更可能提供稳定且较高的回报。 计算公式为：\n$$ RMW = R_{\\text{Robust}} - R_{\\text{Weak}} $$ 投资水平因子（Investment, CMA: Conservative Minus Aggressive）\n反映保守型投资公司与激进型投资公司之间的收益差异。 激进型公司（扩张迅速，资本开支较高）通常伴随着更大的经营风险，而保守型公司（资本支出相对稳健）表现出更高的稳定性和收益。 计算公式为：\n$$ CMA = R_{\\text{Conservative}} - R_{\\text{Aggressive}} $$ Fama-French 三因子模型公式为：\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\epsilon_i $$Fama-French 五因子模型公式为：\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\beta_{i,RMW} \\cdot RMW + \\beta_{i,CMA} \\cdot CMA + \\epsilon_i $$其中：\n$R_i$: 股票 $i$ 的预期收益率 $R_f$: 无风险收益率 $R_m$: 市场组合的预期收益率 $R_m - R_f$: 市场风险溢价因子 $SMB$: 小市值减去大市值股票收益 $HML$: 高账面市值比减去低账面市值比股票收益 $RMW$: 高盈利能力减去低盈利能力股票收益 $CMA$: 低投资活动减去高投资活动股票收益 $\\beta_{i,*}$: 股票 $i$ 对应因子的敏感度 $\\epsilon_i$: 回归残差 2.2.5 模型对比表格 以下表格总结了 均值-方差模型、资本资产定价模型 (CAPM)、套利定价理论 (APT) 和 Fama-French 模型 的核心内容及因子来源：\n模型 核心内容 因子来源 均值-方差模型 投资组合理论的基础，通过期望收益和协方差矩阵优化投资组合。 投资组合中资产的期望收益和协方差矩阵 资本资产定价模型 (CAPM) 通过市场风险因子（$\\beta$）解释资产收益，奠定多因子模型理论基础。 市场因子 $\\beta$ 套利定价理论 (APT) 多因子框架，允许多个经济变量解释资产收益，例如通胀率、利率等。 多因子（宏观经济变量，如通胀率、利率等） Fama-French 三因子模型 增加市值因子和账面市值比因子，改进对资产收益的解释能力。 市场因子、SMB（市值因子）、HML（账面市值比因子） Fama-French 五因子模型 在三因子模型基础上增加盈利因子和投资因子，进一步完善资产定价模型。 市场因子、SMB、HML、RMW（盈利因子）、CMA（投资因子） 以下表格总结了这些模型的优点及不足：\n模型 优点 不足 均值-方差模型 提供了系统化投资组合优化方法，奠定现代投资理论基础。 仅针对收益和方差进行优化，未明确风险补偿的来源。 资本资产定价模型 (CAPM) 简单易用，通过市场风险解释收益差异，为多因子模型提供理论基础。 假设单因子（市场风险）决定收益，忽略其他系统性风险因子。 套利定价理论 (APT) 允许多个因子解释资产收益，减少对单因子假设的依赖，更灵活。 未明确具体因子，实操性较低，仅提供框架。 Fama-French 三因子模型 通过增加市值因子和账面市值比因子，显著提高了对资产收益的解释能力。 忽略了盈利能力和投资行为等其他因子。 Fama-French 五因子模型 在三因子模型基础上增加盈利因子和投资因子，更全面地捕捉影响资产收益的关键变量。 模型复杂度较高，对数据要求高，仍可能遗漏某些潜在因子。 第三章 LSTM，GRU 和 BiLSTM 在股票价格预测中比较研究 3.1 实验数据介绍 国内外很多学者的研究以本国的股票指数为主，对于不同市场的单个股票研究相对较少。并且很少有研究将 LSTM、GRU、BiLSTM 这三个模型进行对比研究。因此本文分别选择国内 A 股上海浦东发展银行（简称浦发银行，代码600000）和美股 International Business Machines Corporation（简称 IBM）进行研究，这样更能准确地对我们使用的三个模型进行对比。其中浦发银行采用 2008 年 1 月 1 日到 2020 年 12 月 31 日的股票数据，共有 3114 条有效数据，数据来源于 Tushare 金融大数据平台。我们选取该数据集的日期（date）、开盘价（open）、收盘价（close）、最高价（high）、最低价（low）和成交量（volume）这 6 个特征进行实验。浦发银行的数据集除日期作为时间序列的索引以外，其他 5 个特征均作为自变量。IBM 采用 1990 年 1 月 2 日到 2018 年 11 月 15 日的股票数据，共有 7278 条有效数据，数据来源于雅虎财经。我们选取该数据集的日期（date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、调整后的收盘价（Adj Close）和成交量（volume）这 7 个特征进行实验。IBM 的数据集除日期作为时间序列的索引以外，其他 6 个特征均作为自变量。本次实验选择收盘价（close）作为被预测的变量。表3.1.1和表3.1.2分别是两个数据集的部分数据。\n3.1.1 浦发银行数据集部分展示 date open close high low volume code 2008-01-02 9.007 9.101 9.356 8.805 131583.90 600000 2008-01-03 9.007 8.645 9.101 8.426 211346.56 600000 2008-01-04 8659 9.009 9.111 8.501 139249.67 600000 2008-01-07 8.970 9.515 9.593 8.953 228043.01 600000 2008-01-08 9.551 9.583 9.719 9.517 161255.31 600000 2008-01-09 9.583 9.663 9.772 9.432 102510.92 600000 2008-01-10 9.701 9.680 9.836 9.602 217966.25 600000 2008-01-11 9.670 10.467 10.532 9.670 231544.21 600000 2008-01-14 10.367 10.059 10.433 10.027 142918.39 600000 2008-01-15 10.142 10.051 10.389 10.006 161221.52 600000 数据来源：Tushare\n3.1.2 IBM数据集部分展示 Date Open High Low Close Adj Close Volume 1990-01-02 23.6875 24.5313 23.6250 24.5000 6.590755 7041600 1990-01-03 24.6875 24.8750 24.5938 24.7188 6.649599 9464000 1990-01-04 24.7500 25.0938 24.7188 25.0000 6.725261 9674800 1990-01-05 24.9688 25.4063 24.8750 24.9375 6.708448 7570000 1990-01-08 24.8125 25.2188 24.8125 25.0938 6.750481 4625200 1990-01-09 25.1250 25.3125 24.8438 24.8438 6.683229 7048000 1990-01-10 24.8750 25.0000 24.6563 24.7500 6.658009 5945600 1990-01-11 24.8750 25.0938 24.8438 24.9688 6.716855 5905600 1990-01-12 24.6563 24.8125 24.4063 24.4688 6.582347 5390800 1990-01-15 24.4063 24.5938 24.3125 24.5313 6.599163 4035600 数据来源：雅虎财经\n3.2 实验数据预处理 3.2.1 数据的归一化 实验中各个特征在单位和量级上存在差异，比如股票价格和成交量之间量级差异巨大，会对我们实验最终预测的结果产生影响。因此我们采用 sklearn.preprocessing 库中的 MinMaxScaler 方法将数据的特征缩放至 0 到 1 之间。这样既能提升模型精度，也能提升模型收敛速度。归一化公式：\n$$ x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)} $$其中 $x^{\\prime}$ 是归一化后的数据，$x$ 是原始数据， $\\min (x)$ 是原始数据集的最小值， $\\max (x)$ 是原始数据集的最大值。之后在我们的实验过程中获得预测结果之后，还要将数据进行反归一化处理，之后才能进行对于股票价格的预测和模型评估。\n3.2.2 数据的划分 此处分别将浦发银行和 IBM 的整个实验数据集送入，设置循环核时间步（timestep）都为 60，每个时间步输入特征个数分别为 5 和 6。这样可以输入前 60 个交易日的数据，预测出第 61 天的收盘价。使我们的数据集符合之后要比较的三种神经网络模型输入的要求，依次是送入样本数，循环核时间展开步数和每个时间步输入特征个数。之后我们再将浦发银行的数据集按照 2488：311：255 的比例将归一化的数据集划分为训练集，验证集，测试集三个部分。将 IBM 的数据集按照 6550：364：304 的比例将归一化的数据集划分为训练集，验证集，测试集三个部分。我们这里划分出验证集的目的是为了方便进行调整模型的超参数以便优化各个模型之后再进行比较。\n3.3 模型网络结构 本文通过大量反复试验最终各个模型设置的网络结构如下表所示，其中层与层之间使用循环神经网络默认的 tanh 和 linear 作为激活函数，并且为了防止过拟合加入 Dropout，Dropout 的丢弃比例（rate）取值为 0.2。LSTM 和 GRU 每个循环层的神经元个数为 50，BiLSTM 循环层的神经元的个数为 100。LSTM、GRU、BiLSTM 每个模型分别采用四层 LSTM、GRU、BiLSTM 和一层全连接层，其中每层网络之间都设置了一个 Dropout。\n3.3.1 IBM的LSTM网络结构 Layer(type) Output Shape Param# lstm_1 (LSTM) (None, 60, 50) 11400 dropout_1 (Dropout) (None, 60, 50) 0 lstm_2 (LSTM) (None, 60, 50) 20200 dropout_2 (Dropout) (None, 60, 50) 0 lstm_3 (LSTM) (None, 60, 50) 20200 dropout_3 (Dropout) (None, 60, 50) 0 lstm_4 (LSTM) (None, 50) 20200 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params：72,051\nTrainable params：72,051\nNon-trainable params：0\n3.3.2 IBM的GRU网络结构 Layer(type) Output Shape Param# gru_1 (GRU) (None, 60, 50) 8550 dropout_1 (Dropout) (None, 60, 50) 0 gru_2 (GRU) (None, 60, 50) 15150 dropout_2 (Dropout) (None, 60, 50) 0 gru_3 (GRU) (None, 60, 50) 15150 dropout_3 (Dropout) (None, 60, 50) 0 gru_4 (GRU) (None, 50) 15150 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params：54,051\nTrainable params：54,051\nNon-trainable params：0\n3.3.3 IBM的BiLSTM网络结构 Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 60, 100) 22800 dropout_1 (Dropout) (None, 60, 100) 0 bidirectional_2 (Bidirection) (None, 60, 100) 60400 dropout_2 (Dropout) (None, 60, 100) 0 bidirectional_3 (Bidirection) (None, 60, 100) 60400 dropout_3 (Dropout) (None, 60, 100) 0 bidirectional_4 (Bidirection) (None, 100) 60400 dropout_4 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 1) 101 Total params：204,101\nTrainable params：204,101\nNon-trainable params：0\n3.4 模型和编译及超参数设置 本文模型在以验证集的损失函数最小为目标，进行不断的超参数调试之后，对于浦发银行的三个模型都选用 epochs=100，batch_size=32；对于 IBM 的三个模型都选用 epochs=50，batch_size=32。其中优化器都采用自适应矩估计（Adaptive moment estimation，Adam）$^{[19]}$。使用其 keras 包中的默认值，即 lr=0.001、beta_1=0.9、beta_2=0.999、epsilon=1e-08 和 decay=0.0。损失函数采用均方误差（Mean Square Error，MSE）。\n参数解释：\nlr：学习率 beta_1：一阶矩估计的指数衰减率 beta_2：二阶矩估计的指数衰减率 epsilon：模糊因子 decay：每次更新后的学习率衰减值 3.5 实验结果与分析 首先简单介绍一下模型使用评价的这几个指标。计算公式如下：\n均方误差（Mean Square Error，MSE）： $$ M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2} $$ 均方根误差（Root Mean Squared Error，RMSE)： $$ R M S E=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}} $$ 平均绝对误差（Mean Absolute Error，MAE）： $$ M A E=\\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-\\hat{Y}_{i}\\right| $$ \\( R^2 \\)（R Squared）： $$ \\begin{gathered} \\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} \\\\ R^{2}=1-\\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\end{gathered} $$其中：$n$ 是样本的数量，$Y$ 是股票实际的收盘价，$\\hat{Y}_{i}$ 是股票预测的收盘价， $\\bar{Y}$ 是股票平均的收盘价。MSE，RMSE 和 MAE 越小该模型越精确。 $R^{2}$ 评价模型系数拟合优度越大越好。\n3.5.1 浦发银行实验结果 LSTM GRU BiLSTM MSE 0.059781 0.069323 0.056454 RMSE 0.244501 0.263292 0.237601 MAE 0.186541 0.202665 0.154289 R-squared 0.91788 0.896214 0.929643 比较三个模型的评价指标，我们可以发现在浦发银行测试集上 BiLSTM 模型的 MSE、RMSE 和 MAE 都小于 LSTM 模型和 GRU 模型，而 R-Squared 都大于 LSTM 模型和 GRU 模型。我们通过对比 RMSE 发现，BiLSTM 相较于 LSTM 在验证集上有 2.90%的性能提升，BiLSTM 相较于 GRU 在验证集上有 10.81%的性能提升。\n3.5.2 IBM实验结果 LSTM GRU BiLSTM MSE 18.01311 12.938584 11.057501 RMSE 4.244186 3.597024 3.325282 MAE 3.793223 3.069033 2.732075 R-squared 0.789453 0.851939 0.883334 比较三个模型的评价指标，我们可以发现在 IBM 测试集上 BiLSTM 模型的 MSE、RMSE 和 MAE 都小于 LSTM 模型和 GRU 模型，而 R-Squared 都大于 LSTM 模型和 GRU 模型。我们通过对比 RMSE 发现，BiLSTM 相较于 LSTM 在验证集上有 27.63%的性能提升，BiLSTM 相较于 GRU 在验证集上有 8.17%的性能提升。\n3.6 本章小结 本章先是介绍了实验所选用的浦发银行和 IBM 两个数据集以及选用的特征，之后对数据集进行了归一化、数据划分的预处理步骤。同时详细的说明了实验所使用 LSTM、GRU 和 BiLSTM 模型的网络结构和超参数。最后得到了每个模型的损失函数图像和一系列的拟合图形。比较了模型的多个评价指标和拟合图像最终得到 BiLSTM 模型能够更好地对股票价格进行预测，为我们下一章研究 LightGBM-BiLSTM 的量化投资策略奠定了基础。\n第四章 基于 LightGBM-BiLSTM 的量化投资模型研究 4.1 实验步骤 Fig. 11. LightGBM-BiLSTM Diagram.\n如 Fig.11 所示，本实验先从因子库中选取 50 个因子。之后对因子依次进行去极值、标准化和缺损值填充的因子清洗步骤。再利用 LightGBM 模型进行因子选择，根据因子重要性进行排序得到前十的因子作为本横截面挑选出来的因子。紧接着使用 BiLSTM 建立多因子模型，最后再进行回测分析。\n4.2 实验数据 本文采用的行情数据来源于 Tushare。具体数据集的特征如下表所示。\n4.2.1 股票数据集包含的特征 名称 类型 描述 ts_code str 股票代码 trade_date str 交易日期 open float 开盘价 high float 最高价 low float 最低价 close float 收盘价 pre_close float 昨收价 change float 涨跌额 pct_chg float 涨跌幅（未复权） vol float 成交量（手） amount float 成交额（千元） A股全市场日线数据集包含5,872,309行数据，即包含5,872,309个样本。如表4.2.1所示，A股全市场日线数据集数据集有以下11个特征，分别依次为股票代码（ts_code）、交易日期（trade_date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、昨收价（pre_close）、涨跌额（change）、换手率（turnover_rate）、交易金额（amount）、总市值（total_mv）和复权因子（adj_factor）。\n4.2.2 A股全市场日线数据集部分展示 ts_code trade_date open high low close pre_close change vol amount 600613.SH 20120104 8.20 8.20 7.84 7.86 8.16 -0.30 4762.98 3854.1000 600690.SH 20120104 9.00 9.17 8.78 8.78 8.93 -0.15 142288.41 127992.6050 300277.SZ 20120104 22.90 22.98 20.81 20.88 22.68 -1.80 12212.39 26797.1370 002403.SZ 20120104 8.87 8.90 8.40 8.40 8.84 -0.441 10331.97 9013.4317 300179.SZ 20120104 19.99 20.32 19.20 19.50 19.96 -0.46 1532.31 3008.0594 600000.SH 20120104 8.54 8.56 8.39 8.41 8.49 -0.08 342013.79 290229.5510 300282.SZ 20120104 22.90 23.33 21.02 21.02 23.35 -2.33 38408.60 86216.2356 002319.SZ 20120104 9.74 9.95 9.38 9.41 9.73 -0.32 4809.74 4671.4803 601991.SH 20120104 5.17 5.39 5.12 5.25 5.16 0.09 145268.38 76547.7490 000780.SZ 20120104 10.42 10.49 10.00 10.00 10.30 -0.30 20362.30 20830.1761 [5872309 rows x 11 columns]\n中证全指日线数据集包含5,057行数据，即包含5,057个样本。如表4.2.2所示，中证全指日线数据集有以下7个特征，分别依次为交易日期（trade_date）、开盘价（open）、最高价（high）、最低价（low）、收盘价（close）、交易量（volume）和昨收价（pre_close）。\n4.2.3 中证全指日线数据集部分展示 trade_date open high low close volume pre_close 2006-11-24 1564.3560 1579.3470 1549.9790 1576.1530 7.521819e+09 1567.0910 2006-11-27 1574.1130 1598.7440 1574.1130 1598.7440 7.212786e+09 1581.1530 2006-11-28 1597.7200 1604.7190 1585.3620 1596.8400 7.025637e+09 1598.7440 2006-11-29 1575.3030 1620.2870 1575.3030 1617.9880 7.250354e+09 1596.8400 2006-11-30 1621.4280 1657.3230 1621.4280 1657.3230 9.656888e+09 1617.9880 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 2020-11-11 5477.8870 5493.5867 5422.9110 5425.8017 5.604086e+10 5494.1042 2020-11-12 5439.2296 5454.3452 5413.9659 5435.1379 4.594251e+10 5425.8017 2020-11-13 5418.2953 5418.3523 5364.2031 5402.7702 4.688916e+10 5435.1379 2020-11-16 5422.3565 5456.7264 5391.9232 5456.7264 5.593672e+10 5402.7702 2020-11-17 5454.0696 5454.0696 5395.6052 5428.0765 5.857009e+10 5456.7264 [5057 rows x 7 columns]\n下表4.2.4是原始的因子部分数据。依次经过上述因子缺失值填充、因子去极值、因子标准化和因子中性化这 4 个因子清洗的步骤后，得到如表4.2.5展示的经过因子清洗后的因子部分数据。\n4.2.4 原始的因子数据 trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 4.2.5 清洗后的因子数据 sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.2.6 因子数据 价量因子构建 本文使用如下方式构建价量因子，构建价量因子的基础要素有两点：首先是基础字段，其次是算子。如表4.2.1 所示，基础字段包括日频的最高价（high），最低价（low），开盘价（open），收盘价（close），上一日收盘价（pre_close），成交量（vol），涨跌（pct_chg），换手率（turnover_rate），交易金额（amount），总市值（total_mv）和复权因子（adj_factor）。\n4.2.7 基础字段表 编号 字段名 意义 high 最高价 当日成交订单中最高的价格 low 最低价 当日成交订单中最低的价格 open 开盘价 当日集合竞价成交的价格 close 收盘价 当日最后一笔成交订单的价格 pre_close 上一日收盘价 上一日最后一笔成交订单的价格 vol 成交 全天成交的股票数 pct_chg 涨跌 本日证券涨跌 turnover_rate 换手率 本日证券的换手率 amount 成交金额 全天成交的金额 total_mv 总市值 总股本数乘以当时股价得出的股票总价值 adj_factor 复权因子 权息修复比例 本文通过 gplearn提供的基础算子集和自己定义的一些特殊算子，得到如表所示的算子列表。\n4.2.8 算子列表 算子 名称 定义 add(x, y) 和 \\( x + y\\)；点运算 \\(\\operatorname{div}(x, y)\\) 除 \\( x / y\\)；点运算 \\(\\operatorname{mul}(x, y)\\) 乘 \\( x \\cdot y\\)；点运算 \\(\\operatorname{sub}(x, y)\\) 减 \\( x - y\\)；点运算 neg(x) 负 \\(-x\\)；点运算 \\(\\log(x)\\) 对数 \\(\\log(x)\\)；点运算 max(x, y) 最大值 \\(x, y\\) 中数值较大的数；点运算 \\(\\min(x, y)\\) 最小值 \\(x, y\\) 中数值较小的数；点运算 delta_d(x) d 日差值 当日的 \\(x\\) 值减去 d 日前的 \\(x\\) 值；时序运算 delay_d(x) d 日延时 d 日前的 \\(x\\) 值；时序运算 Corr_d(x, y) d 日相关性 d 日 \\(x\\) 值和 d 日 \\(y\\) 值的相关性；时序运算 Max_d(x) d 日最大值 d 日 \\(x\\) 值的最大值；时序运算 Min_d(x) d 日最小值 d 日 \\(x\\) 值的最小值；时序运算 sort_d(x) d 日排序位置 d 日 \\(x\\) 值的排序值；时序运算 Argsortmin_d(x) d 日最小值位置 d 日 \\(x\\) 值的最小值的位置；时序运算 Argsortmax_d(x) d 日最大值位置 d 日 \\(x\\) 值的最大值的位置；时序运算 \\(\\operatorname{inv}(x)\\) 倒数 \\( 1 / x\\)；点运算 Std_d(x) d 日方差 d 日 \\(x\\) 值的方差；时序运算 abs(x) 绝对值 \\(\\lvert x\\rvert\\)；点运算 4.2.9 遗传规划 遗传规划（Genetic Programming, GP）的核心思想是使用进化算法在算子（operators）与基础字段（terminals）组合而成的巨大搜索空间中，自动“进化”出具有较强预测能力的因子表达式。对于本文中的因子挖掘来说，GP 的主要目标是从表4.2.7中的基础字段和表4.2.8中的算子所能组合成的所有可能表达式中，搜索并找到那些能对下一期股票收益有较好预测效果的因子。GP 的核心流程可分为以下几个步骤：\n初始化（Initialization） 定义算子集与基础字段\n算子集（operators）如表4.2.8所示，包括加、减、乘、除、对数、绝对值、延时、移动最大/最小值、移动相关系数等运算。 基础字段（terminals）如表4.2.7所示，包括开盘价、收盘价、最高价、最低价、成交量、复权因子等。\n这些算子和基础字段可以视作因子表达式树中的“节点”，其中基础字段为叶子节点（终端节点），算子为内部节点。 随机生成初始种群\n在初始化阶段，根据给定的算子集与字段集，随机“拼接”生成一系列因子表达式（可表示为若干语法树或表达式树），形成初始种群。 例如，可能随机产生\n\\[ \\text{因子1}: \\mathrm{Max\\_5}\\bigl(\\mathrm{add}(\\mathrm{vol}, \\mathrm{close})\\bigr), \\quad \\text{因子2}: \\mathrm{sub}\\bigl(\\mathrm{adj\\_factor}, \\mathrm{neg}(\\mathrm{turnover\\_rate})\\bigr), \\dots \\] 每个因子表达式都将对应一个个体（individual）。 适应度函数（Fitness Function） 度量因子的预测能力\n针对每个表达式（个体），我们需要评估它对未来收益或其他目标的预测能力。具体来说，可以在下一期股票收益 \\( r^{T+1} \\) 与当前期因子暴露度 \\( x_k^T \\) 之间，计算其相关系数（IC）或更综合的指标 IR（Information Ratio）来衡量。 设定目标\n若我们希望因子具有更高的相关性（IC），则可令适应度函数为 \\(\\lvert \\rho(x_k^T, r^{T+1})\\rvert\\)； 若我们希望因子的 IR 更高，则可设定适应度函数为 IR 值。 因子 IC 或 IR 越高，该表达式的“适应度”就越高。 \\[ \\text{Fitness} \\bigl(F(x)\\bigr) \\;=\\; \\begin{cases} \\lvert \\rho(x_k^T, r^{T+1})\\rvert \\quad \u0026\\text{(IC最大化)},\\\\[6pt] \\mathrm{IR}(x_k^T) \\quad \u0026\\text{(IR最大化)}. \\end{cases} \\] 其中 \\(\\rho(\\cdot)\\) 表示相关系数，\\(\\mathrm{IR}(\\cdot)\\) 为 IR 指标。\n选择（Selection）、交叉（Crossover）与变异（Mutation） 选择（Selection）\n根据适应度函数的结果，将因子适应度高的表达式“保留”或“繁衍”，适应度较低的表达式则被淘汰。 这类似于生物进化中的“优胜劣汰”。 交叉（Crossover）\n将若干适应度较高的表达式（父本）随机选取一部分“节点”进行交换，从而得到新的表达式（子本）。 在表达式树结构中，可以将子树 A 与子树 B 互换，从而产生新的后代表达式。 例如，若表达式树 \\(\\mathrm{FactorA}\\) 的某个子树与表达式树 \\(\\mathrm{FactorB}\\) 的对应子树相交换，就生成了两个新的表达式。 变异（Mutation）\n以一定概率对表达式的某些节点进行随机变更，比如： 更换节点的算子（例如将 \\(\\mathrm{add}\\) 换为 \\(\\mathrm{sub}\\)）， 替换终端节点的基础字段（例如将 \\(\\mathrm{vol}\\) 换为 \\(\\mathrm{close}\\)）， 或随机改变运算参数（如移动窗口长度、平滑因子等）。 变异可以增加群体的多样性，避免过早收敛或陷入局部最优。 迭代进化（Iteration） 循环执行\n将选择、交叉、变异的操作反复执行多代（generations）。 每一代都产生一个新的因子表达式种群，并对其进行适应度评估。 收敛与终止\n当进化达到预先设定的停止条件（如迭代次数、适应度阈值等）时，算法终止。 通常我们会选出若干个最终适应度较高的因子表达式，将它们视为进化结果。 数学表征：搜索最优因子表达式 将上述过程抽象成下式，可以简单表示因子的搜索目标：\n\\[ F(x) \\;=\\; \\mathrm{GP}\\bigl(\\{\\text{operators}\\}, \\{\\text{terminals}\\}\\bigr), \\] 表示通过 GP 算法在给定算子集（operators）和基础字段集（terminals）上搜索出一个函数 \\(F(x)\\)。从最优化的角度看，我们希望找到：\n\\[ \\max_{F} \\bigl\\lvert \\rho(F^T, r^{T+1}) \\bigr\\rvert \\quad \\text{或者} \\quad \\max_{F} \\; \\mathrm{IR}\\bigl(F\\bigr), \\] 其中\n\\(\\rho(\\cdot)\\) 表示因子与下一期收益的相关系数（IC）， \\(\\mathrm{IR}(\\cdot)\\) 表示该因子的 IR 指标。 在实际应用中，我们会给定一段回测期，对每一代的候选因子进行打分（IC/IR 评估），通过选择、交叉和变异的迭代过程不断“进化”出更优质的因子。\n通过以上步骤，我们最终能够在庞大的算子组合与基础字段组合的搜索空间中，自动挖掘到一批对未来收益有较强预测能力、且具有较好稳健性（如 IR 较高）的因子表达式。\n4.2.10 挖掘出的部分因子 因子名 定义 0 Max＿25(add(turnover_rate, vol)) 1 Max＿30(vol) 2 Max＿25(turnover_rate) 3 Max＿35(add(vol, close)) 4 Max＿30(turnover_rate) 5 sub(Min＿20(neg(pre_close)), div(vol, adj_factor)) 6 Max＿60(max(vol, adj_factor)) 7 Max＿50(amount) 8 div(vol, neg(close)) 9 min(ArgSortMin＿25(pre_close), neg(vol)) 10 neg(max(vol, turnover_rate)) 11 mul(amount, neg(turnover_rate)) 12 inv(add(ArgSortMax＿40(change), inv(pct_chg))) 13 Std＿40(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))) 14 div(log(total_mv),amount) 15 div(neg(Max＿5(amount)), Min＿20(ArgSort＿60(high))) 16 Corr＿30(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))), add(log(Max＿10(pre_close)), high)) 17 ArgSort＿60(neg(turnover_rate)) \u0026hellip; \u0026hellip; 这些因子均是通过遗传规划从算子列表（表4.2.8）与基础字段列表（表4.2.7）中组合而得，具有不同的数学表达形式。\n因子有效性检验 当我们得到挖掘的因子之后，需要对因子进行有效性检验，常见的检验指标有信息系数（Information Coefficient，IC）和信息比率（Information Ratio，IR）。\n信息系数（IC）描述的是所选股票下期收益率和本期因子暴露度的线性相关程度，可以反应该因子进行收益率预测的稳健性 **信息比率（IR）**是超额收益的均值与超额收益的标准差之比，信息比率与夏普比率类似，主要区别在于夏普比率使用无风险收益作为基准，而信息比率使用风险指数作为基准。夏普比率有助于确定投资组合的绝对收益，信息比率有助于确定投资组合的相对收益。当我们计算了 IC 之后，可以根据 IC 的值再对 IR 进行计算。当 IR 大于 0.5 时，因子稳定获取超额收益能力较强。 实际计算中，因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 值一般是指所选股票第 \\(T\\) 期的因子 \\(k\\) 上的暴露度 \\( x_k^T \\) 与所选股票第 \\(T+1\\) 期的收益率 \\( r^{T+1} \\) 的相关系数；因子 \\(k\\) 的 \\( \\mathrm{IR} \\) 值为因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 的均值除以因子 \\(k\\) 的 \\( \\mathrm{IC} \\) 的标准差，计算公式如下：\n$$ \\begin{gathered} I C=\\rho_{x_{k}^{T}, r^{T+1}}=\\frac{\\operatorname{cov}\\left(x_{k}^{T}, r^{T+1}\\right)}{\\sigma_{x_{k}^{T}} \\sigma_{r^{T+1}}}=\\frac{\\mathrm{E}\\left(x_{k}^{T} * r^{T+1}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right) \\mathrm{E}\\left(r^{T+1}\\right)}{\\sqrt{\\mathrm{E}\\left(\\left(x_{k}^{T}\\right)^{2}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right)^{2}} \\cdot \\sqrt{\\mathrm{E}\\left(\\left(r^{T+1}\\right)^{2}\\right)-\\mathrm{E}\\left(r^{T+1}\\right)^{2}}} \\\\ I R=\\frac{\\overline{I C}}{\\sigma_{I C}} \\end{gathered} $$其中：\n$x_{k}^{T}$ ：所选股票第 $T$ 期的因子 $k$ 上的暴露度 $r^{T+1}$ ：所选股票第 $T+1$ 期的收益率 $\\overline{I C}: I C$ 的均值 本文采用 IR 判断因子好坏，通过对大量不同的算子和基础数据的组合以及 IC 和 IR 的“篮选”，文章得到了本文所选用的 50 个价量因子。经过 IR 检测，按 IR 由高到低排序得到如下图所示的表格。从下表中我们可以看出来所选的 50 个价量因子的 IR 都大于 0.5 ，说明这些因子稳定获取超额收益能力较强。\n4.2.11 因子 IR 检验表 因子名 IR 因子名 IR 0 3.11 25 2.73 1 2.95 26 2.71 2 2.95 27 2.70 3 2.95 28 2.69 4 2.95 29 2.69 5 2.94 30 2.69 6 2.94 31 2.68 7 2.94 32 2.68 8 2.93 33 2.68 9 2.93 34 2.68 10 2.93 35 2.67 11 2.92 36 2.67 12 2.91 37 2.66 13 2.89 38 2.65 14 2.86 39 2.65 15 2.83 40 2.65 16 2.83 41 2.65 17 2.83 42 2.64 18 2.79 43 2.63 19 2.78 44 2.63 20 2.78 45 2.62 21 2.76 46 2.62 22 2.75 47 2.62 从该表可见，在所筛选的因子中，所有因子的 IR 均大于 0.5，具有较强且稳定的获取超额收益的能力。\n4.3 因子清洗 4.3.1 因子缺失值处理和去极值 对于因子的缺失值处理的方法有个案剔除法，均值替换法，回归替换法等方法。本文采用较为简单的均值替换法对缺损值进行处理，即利用因子的平均值来替代缺失的数据。对于因子去极值有中位数去极值，百分比去极值和 $3 \\sigma$ 去极值等方法。本文采用的是 $3 \\sigma$ 去极值法，该方法是利用统计学上的 $3 \\sigma$ 原则，将把离该因子均值三个标准差以上的极值因子转化到刚好离均值三个标准差的位置，具体计算公式如下：\n$$ X_i^{\\prime}= \\begin{cases} \\bar{X}+3 \\sigma \u0026 \\text{if } X_i \u003e \\bar{X} + 3 \\sigma \\\\ \\bar{X}-3 \\sigma \u0026 \\text{if } X_i \u003c \\bar{X} - 3 \\sigma \\\\ X_i \u0026 \\text{if } \\bar{X} - 3 \\sigma \u003c X_i \u003c \\bar{X} + 3 \\sigma \\end{cases} $$其中：\n$X_{i}$ ：因子处理之前的值 $\\bar{X}$ ：因子序列的均值 $\\sigma$ ：因子序列的标准差 $X_{i}^{\\prime}$ ：去极值后的因子的值 4.3.2 因子的标准化 本文实验选取了多个因子，并且各个因子量纲并不完全相同，为了我们方便进行比较和回归，我们还需对因子进行标准化处理。目前常用的具体的标准化方法有 Min－Max标准化，Z－score 标准化和 Decimal scaling 小数定标标准化等。本文选择 Z－score 标准化的方法。通过原始数据的均值和标准差进行数据的标准化，经过处理的数据符合标准正态分布，即均值为 0 ，标准差为 1 ，其标准化后的数值大小有正有负，得到标准正态分布曲线。\n本文采用 Z－score 标准化公式如下：\n$$ \\tilde{x}=\\frac{x_{i}-u}{\\sigma} $$其中：\n$x_{i}$ ：因子的原值 $u$ ：因子序列的均值 $\\sigma$ ：因子序列的标准差 $\\tilde{x}$ ：标准化后的因子值 4.3.3 因子的中性化 因子中性化是为了剔除其他因素对我们所选因子的影响，使我们构建量化投资策略组合所选择的股票更加分散，而不是集中在特定的行业或者市值的股票上，可以更好地分担投资组合的风险和解决因子多重共线性的问题。市值和行业是影响股票收益最主要的两种自变量，所以在进行因子清洗的过程中，还必须考虑市值和行业的影响。本文实证中我们采用仅纳入行业因子，而将市场因子包含在行业因子中的方法。针对因子的单因子回归模型见公式(31)，我们将以下回归模型的残差项作为因子中性化后的新的因子值。\n$$ \\tilde{r}_{j}^{t}=\\sum_{s=1}^{s} X_{j s}^{t} \\tilde{f}_{s}^{t}+X_{j k}^{t} \\tilde{f}_{k}^{t}+\\tilde{u}_{j}^{t} $$其中：\n$\\tilde{r}_{j}^{t}$ ：股票 $j$ 在第 $t$ 期的收益率 $X_{j s}^{t}$ ：股票 $j$ 在第 $t$ 期在行业 $s$ 上的暴露度 $\\tilde{f}_{s}^{t}$ ：行业在第 $t$ 期的收益率 $X_{j k}^{t}$ ：股票 $j$ 在第 $t$ 期在因子 $k$ 上的暴露度 $\\tilde{f}_{k}^{t}$ ：因子 $k$ 在第 $t$ 期的收益率 $\\tilde{u}_j^t$ ：一个 $0-1$ 哑变量，即如果股票 $j$ 属于行业 $s$ ，则暴露度为 1 ，否则为 0 在本文中，并不会对公司所属行业进行按照比例拆分，即股票 $j$ 只能属于一个特定的行业 $s$ ，在行业 $s$ 上的暴露度为 1 ，在其他所有行业的暴露度为 0 。本文使用申万宏源行业分类标准，具体分类情况依次为农林牧渔，采掘，化工，钢铁，有色金属，电子元器件，家用电器，食品饮料，纺织服装，轻工制造，医药生物，公用事业，交通运输，房地产，商业贸易，餐饮旅游，综合，建筑材料，建筑装饰，电器设备，国防军工，计算机，传媒，通信，银行，非银金融，汽车和机械设备这 28 类。下表为 2021 年 2 月 5 日的申万指数一级行业历史行情图。\n4.3.3.1 2021年2月5日的申万指数一级行业历史行情图 指数代码 指数名称 发布日期 开盘指数 最高指数 最低指数 收盘指数 成交量(亿殴) 成交额(亿元) 涨跌幅(%) 801010 农林牧渔 2021/2/5 0:00 4111.43 4271.09 4072.53 4081.81 15.81 307.82 -0.3 801020 采掘 2021/2/5 0:00 2344.62 2357.33 2288.97 2289.41 18.06 115.6 -2.25 801030 化工 2021/2/5 0:00 4087.77 4097.59 3910.67 3910.67 55.78 778.85 -3.95 801040 钢铁 2021/2/5 0:00 2253.78 2268.17 2243.48 2250.81 11.61 48.39 -1.02 801050 有色金属 2021/2/5 0:00 4212.1 4250.59 4035.99 4036.74 45.41 593.92 -4.43 801080 电子元器件 2021/2/5 0:00 4694.8 4694.8 4561.95 4561.95 52.67 850.79 -2.78 801110 家用电器 2021/2/5 0:00 10033.82 10171.26 9968.93 10096.83 8.55 149.18 0.83 801120 食品饮料 2021/2/5 0:00 30876.33 31545.02 30649.57 30931.69 11.32 657.11 0.47 801130 纺织服装 2021/2/5 0:00 1614.48 1633.89 1604.68 1607.63 6.28 57.47 -0.39 801140 轻工制造 2021/2/5 0:00 2782.07 2791.88 2735.48 2737.24 15.28 176.16 -1.35 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 数据来源：申银万国\n下表是原始的因子部分数据。依次经过上述因子缺失值填充、因子去极值、因子标准化和因子中性化这 4 个因子清洗的步骤后，得到如表展示的经过因子清洗后的因子部分数据。\n4.3.3.2 原始的因子数据 trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 4.3.3.3 清洗后的因子数据 sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.4 基于 LightGBM 的因子选择 4.4.1 GBDT Friedman（2001）$^{[20]}$ 提出的梯度提升决策树（Gradient Boosting Decision Tree，GBDT）是一种基于迭代的回归型决策树。其主要思想是通过逐步添加弱分类器（通常是决策树）来优化模型，使得整体模型能够最小化损失函数。GBDT 的模型可以表示为：\n$$ \\hat{y} = \\sum_{m=1}^{M} \\gamma_m h_m(\\mathbf{x}) $$其中：\n\\( M \\) 是迭代次数， \\( \\gamma_m \\) 是第 \\( m \\) 个弱分类器的权重， \\( h_m(\\mathbf{x}) \\) 是第 \\( m \\) 个决策树模型。 GBDT 的训练过程通过逐步拟合负梯度方向来最小化损失函数，具体更新公式为：\n$$ \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{N} L\\left(y_i, \\hat{y}_{i}^{(m-1)} + \\gamma h_m(\\mathbf{x}_i)\\right) $$其中，\\( L \\) 是损失函数，\\( y_i \\) 是真实值，\\( \\hat{y}_{i}^{(m-1)} \\) 是第 \\( m-1 \\) 次迭代后的预测值。\n4.4.2 LightGBM 轻量级梯度提升机（Light Gradient Boosting Machine，LightGBM)$^{[21]}$ 是一个高效实现 GBDT 算法的框架，最初由 Microsoft 开发，作为一个免费开源的分布式梯度提升框架。LightGBM 基于决策树算法，广泛应用于排名、分类及其他机器学习任务，开发重点在于性能和可伸缩性。其主要优势包括高效率的并行训练、更快的训练速度、更低的内存消耗、更好的准确率，以及支持分布式计算和快速处理海量数据$^{[22]}$。\nLightGBM 的核心算法基于以下优化目标：\n$$ L = \\sum_{i=1}^{N} l(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(h_m) $$其中，\\( l \\) 是损失函数，\\( \\Omega \\) 是正则化项，用于控制模型复杂度，通常表示为：\n$$ \\Omega(h_m) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 $$这里，\\( T \\) 是树的叶子数，\\( w_j \\) 是第 \\( j \\) 个叶子的权重，\\( \\gamma \\) 和 \\( \\lambda \\) 是正则化参数。\nLightGBM 采用基于梯度的单边采样（Gradient-based One-Side Sampling，GOSS）和互斥特征捆绑（Exclusive Feature Bundling，EFB）等技术，显著提升了训练效率和模型性能。\n在本研究中，训练过程中使用的损失函数为均方误差（Mean Squared Error，MSE），其定义为：\n$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$其中，\\( y \\) 为真实收益率，\\( \\hat{y} \\) 为模型预测的收益率，\\( N \\) 为样本数量。\n4.4.3 算法流程 本小节算法的具体流程如下：\n数据准备：使用一年的每只股票的 50 个因子数据（A 股全市场数据）和历史未来一个月的收益率作为特征。\n模型训练：利用网格搜索法（Grid Search）优化 LightGBM 模型的超参数，训练模型以预测未来一个月的收益率。模型训练流程如图4.12所示。\n$$ \\text{参数优化：} \\quad \\theta^* = \\arg\\min_\\theta \\sum_{i=1}^{N} L(y_i, \\hat{y}_i(\\theta)) $$其中，\\( \\theta \\) 表示模型的超参数集合，\\( \\theta^* \\) 为最优参数。\n因子重要性计算：使用 LightGBM 的 feature_importances_ 方法计算各因子的特征重要性。特征重要性主要通过两个指标衡量：\nSplit：该特征在所有树中被用于分裂的次数。 Gain：该特征在所有分裂中带来的总增益（即损失函数的减少量）。 因子的特征重要性可以表示为：\n$$ \\text{Importance}_{\\text{split}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\mathbb{I}(f \\text{ 被用于第 } j \\text{ 个叶节点的分裂}) $$$$ \\text{Importance}_{\\text{gain}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\Delta L_{m,j} \\cdot \\mathbb{I}(f \\text{ 被用于第 } j \\text{ 个叶节点的分裂}) $$其中，\\( \\mathbb{I} \\) 是指示函数，\\( \\Delta L_{m,j} \\) 是因子 \\( f \\) 在第 \\( m \\) 棵树的第 \\( j \\) 个分裂中带来的损失减少量。\n因子筛选：根据模型计算的因子重要性进行排序，选择前十个重要性最高的因子作为本横截面分析中使用的因子。所选因子的重要性如表4.4.4所示。\n4.4.4 部分所选因子重要性排序 importance feature_name trade_date 35 factor_35 2010-08-11 27 factor_27 2010-08-11 33 factor_33 2010-08-11 20 factor_20 2010-08-11 24 factor_24 2010-08-11 45 factor_45 2010-08-11 37 factor_37 2010-08-11 49 factor_49 2010-08-11 19 factor_19 2010-08-11 47 factor_47 2010-08-11 22 factor_22 2010-09-09 20 factor_20 2010-09-09 30 factor_30 2010-09-09 24 factor_24 2010-09-09 4.4.5 代码实现片段 以下是训练过程所使用的部分代码，用于因子选择。\nfeature_choice def feature_choice( self, days=21, is_local=False ): if is_local: feature_info = pd.read_hdf(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;)) else: factors = self.get_env().query_data(Factors_Data) factors = factors[ factors[COM_DATE] \u0026gt;= \u0026#39;2010-01-01\u0026#39; ] trade_list = list(set(factors[COM_DATE])) trade_list.sort() if len(trade_list) % days == 0: n = int(len(trade_list) / days) - 7 else: n = int(len(trade_list) / days) - 6 feature_info = pd.DataFrame() begin_index = 147 feature = list(factors.columns) feature.remove(COM_SEC) feature.remove(COM_DATE) feature.remove(Ret) for i in range(n): end_date = days * i + begin_index - 21 begin_date = days * i trade_date = days * i + begin_index print(trade_list[trade_date]) train_data = factors[ (factors[COM_DATE] \u0026lt;= trade_list[end_date]) \u0026amp; (factors[COM_DATE] \u0026gt;= trade_list[begin_date]) ] model = lgb.LGBMRegressor() model.fit(train_data[feature], train_data[Ret]) feature_info_cell = pd.DataFrame(columns=Info_Fields) feature_info_cell[Importance] = model.feature_importances_ feature_info_cell[Feature_Name] = model.feature_name_ feature_info_cell = feature_info_cell.sort_values(by=Importance).tail(10) feature_info_cell[COM_DATE] = trade_list[trade_date] feature_info = pd.concat( [feature_info, feature_info_cell], axis=0 ) h = pd.HDFStore(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;), \u0026#39;w\u0026#39;) h[\u0026#39;data\u0026#39;] = feature_info h.close() self.get_env().add_data(feature_info, Feature_Info) pass 通过上述流程，利用 LightGBM 高效地筛选出对预测未来收益率最具影响力的因子，从而提升模型的预测能力和解释性。\n4.5 基于 BiLSTM 的因子组合 本小节使用 BiLSTM 进行因子组合。BiLSTM 的具体原理在第二章已经介绍了，这里不再赘述。下面先介绍一下使用模型的具体网络结构，本文通过大量反复试验最终 BiLSTM 设置的网络结构如表4.5.1所示。其中层与层之间使用循环神经网络默认的 tanh 和 linear 作为激活函数。并且为了防止过拟合加入 Dropout，但是如果 Dropout 使用过大的丢弃比例会出现欠拟合的现象，因此 Dropout 的丢弃比例取值为 0.01。最终模型的 BiLSTM 循环层的神经元个数为 100，采用一层 BiLSTM 层和三层全连接层，其中 BiLSTM 层和第一个全连接层之间设置了一个 Dropout。\n4.5.1 BiLSTM的网络结构 Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 100) 24400 dropout_1 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 256) 25856 dropout_2 (Dropout) (None, 256) 0 dense_2 (Dense) (None, 64) 16448 dense_3 (Dense) (None, 1) 0 Total params：66,769\nTrainable params：66,769\nNon-trainable params：0\n因为本次实验使用数据的数据量较大，因此选用 epochs=400，batch_size=1024。模型的损失函数采用均方误差（Mean Square Error，MSE）。其中优化器采用随机梯度下降（Stochastic Gradient Descent，SGD）。随机梯度下降相对于梯度下降（Gradient Descent，GD）有在信息冗余的情况下更能有效地利用信息，前期迭代效果卓越，适合处理大样本的数据这三个优势 $^{[23]}$。由于本实验训练数据量较大，使用 SGD 的话每次仅用一个样本来迭代，训练的速度很快，可以大大减少我们训练所花费的时间。使用其 keras 包中的默认值，即 lr=0.01、momentum=0.0、decay=0.0 和 nesterov=False。\n参数解释：\nlr：学习率 momentum：动量参数 decay：每次更新后的学习率衰减值 nesterov：确定是否使用 Nesterov 动量 4.5.2 算法流程 本小节算法的具体流程如下：\n使用一年的每只股票的 10 个因子（LightGBM 选出来的因子）和历史未来一个月的收益率的 A 股全市场数据作为特征。 以一年每支股票未来一个月的收益率为预测目标，利用 BiLSTM 进行训练，如 Fig. 12 所示。 Fig. 12. Rolling Window\n一个月的样本外数据的实时因子数据通过训练好的 BiLSTM 模型，得到实时的未来一月的每只股票预期收益率。收益率如表4.11所示。 4.5.3 部分股票预测收益率表 sec_code trade_date y_hat 000001.SZ 2011/5/26 0.0424621 000002.SZ 2011/5/26 -0.1632174 000004.SZ 2011/5/26 -0.0642319 000005.SZ 2011/5/26 0.08154649 000006.SZ 2011/5/26 0.00093213 000007.SZ 2011/5/26 -0.073218 000008.SZ 2011/5/26 -0.0464256 000009.SZ 2011/5/26 -0.091549 000010.SZ 2011/5/26 0.08154649 000011.SZ 2011/5/26 -0.1219943 000012.SZ 2011/5/26 -0.1448984 000014.SZ 2011/5/26 0.09038845 000016.SZ 2011/5/26 -0.11225 4.5.4 代码实现片段 以下是训练过程所使用的部分代码，用于构建BiLSTM训练网络。\nbuild_net_blstm def build_net_blstm(self): model = ks.Sequential() model.add( ks.layers.Bidirectional(ks.layers.LSTM( 50 ),input_shape=(11,10)) ) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(256)) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(64)) model.add(ks.layers.Dense(1)) model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;mse\u0026#39;) model.summary() self.set_model(model) 4.6 量化策略和策略回测 4.6.1 回测指标 下面先对策略的一些常见回测指标进行介绍。评价指标包括累计收益率（Total Rate of Return）、年化收益率（Annualized Rate of Return）、年化波动率（Annualized volatility）、夏普比率（Sharpe ratio）、最大回撤率 (Maximum Drawdown，MDD)、年化换手率（Annualized turnover rate）和年化交易成本率（Annualized transaction cost rate），其中假定一年股市开盘252天，无风险利率默认为0.035，手续费默认0.002。\n累计收益率（Total Rate of Return）：在其他指标相同的情况下，累计收益率越大说明该策略越好，越能带来更大的收益。公式如下： $$ \\text{Total Rate of Return} = r_{p} = \\frac{P_{1} - P_{0}}{P_{0}} $$$P_{1}$ ：最终股票和现金的总价值\n$P_{0}$ ：初始股票和现金的总价值\n年化收益率（Annualized Rate of Return）：是将累计总收益率换算成以年为单位的几何平均收益率。在其他指标相同的情况下，年化收益率越大，代表该策略越好。公式如下： $$ \\text{Annualized Rate of Return} = R_{p} = \\left(1 + r_{p}\\right)^{\\frac{252}{t}} - 1 $$$r_{p}$ ：累计收益率\n$t$ ：投资策略执行的天数\n年化波动率（Annualized volatility）：定义为对象资产的年回报率的对数值的标准差。年化波动率用来衡量策略的风险性，波动率越大代表策略的风险越高。公式如下： $$ \\begin{aligned} \\text{Annualized volatility} = \\sigma_{p} \u0026= \\sqrt{\\frac{252}{t-1} \\sum_{i=1}^{t}\\left(r_{d} - \\bar{r}_{d}\\right)^{2}} \\\\ \\bar{r}_{d} \u0026= \\frac{1}{t} \\sum_{i=1}^{t} r_{d_{i}} \\end{aligned} $$$r_{d_{i}}$ ：第 $i$ 天日收益率\n$\\bar{r}_{d}$ ：日平均收益率\n$t$ ：投资策略执行的天数\n夏普比率（Sharpe ratio）：是由 Sharpe（1966）$^{[24]}$ 提出的。它代表投资者额外承受一单位风险，所获得的超额收益$^{[25]}$。这里给出年化的夏普比率计算公式： $$ S = \\frac{R_{p} - R_{f}}{\\sigma_{p}} $$$R_{p}$ ：年化收益率\n$R_{f}$ ：无风险收益率\n$\\sigma_{p}$ ：年化波动率\n最大回撤率 (Maximum Drawdown，MDD)：表示表示我们策略运行期间股票和现金的总价值走到最低点时的收益率回撤幅度的最大值。最大回测率用来策略最极端可能的亏损情况。 $$ MDD = \\frac{\\max \\left(V_{x} - V_{y}\\right)}{V_{x}} $$$V_{x}$ 和 $V_{y}$ 分别为策略组合在第 $x$ 天和第 $y$ 天的股票和现金的总价值，且 $x \u0026lt; y$。\n年化换手率（Annualized turnover rate）：用来衡量对投资组合里面的股票买卖的频繁程度。越大说明改投资组合换仓越频繁，交易成本也会越大。 $$ \\text{change} = \\frac{N \\times 252}{t} $$$t$ ：投资策略执行的天数\n$N$ ：总共买进和卖出的次数\n年化交易成本率（Annualized transaction cost rate）：用来衡量投资组合策略的交易成本，越大说明交易成本越高。 $$ c = \\left(1 + \\text{commison}\\right)^{\\text{change}} - 1 $$change：年化换手率\ncommison：手续费\n4.6.2 策略及回测结果 本文量化交易策略采用每隔一个月进行换仓（即调仓周期为28个交易日），每次换仓采取等额持股的方式买入 BiLSTM 预测出的预期收益率最高的25支股票，卖出原本所持有的股票。本文的回测时间和规则如下：\n回测时间：从 2012 年 1 月到 2020 年 10 月。 回测股票池：全 A 股，剔除特别处理（Special treatment，ST）股票。 交易手续费：买入时支付给券商交易佣金千分之二，卖出时支付给券商交易佣金千分之二，其中单笔交易佣金不满5元券商按5元收取交易佣金。 买卖规则：当天开盘涨停股票不能买入，跌停股票不能卖出。 4.6.2.1 策略回测结果 累计收益率 年化收益率 年化波动率 夏普比率 最大回撤 年化换手率 年化交易成本率 策略 701.00% 29.18% 33.44% 0.77 51.10% 51.10% 11.35% 基准 110.40% 9.70% 26.01% 0.24 58.49% 58.49% 0.00% Fig. 22. Net Profit Curve\n回测结果如上表和 Fig.22 所示。我的策略采用的是本章所介绍的 LightGBM-BiLSTM 量化策略。基准采用的中证全指（000985）。由上面的结果可以看到，本策略累计收益率为701.00%，远高于基准110.40%；年化收益率为29.18%，远高于基准9.70%；夏普率为0.77，高于基准0.24。这三项回测指标说明 LightGBM-BiLSTM 量化策略确实能够给投资者带来更大的收益。本策略年化波动率为33.44%大于基准26.01%，最大回撤为51.10%小于基准58.49%，这两项回测指标说明 LightGBM-BiLSTM 量化策略存在一定的风险，特别是很难抵御系统性风险的冲击。年化换手率为11.35%，年化交易成本率为2.29%，说明我们策略不是高频交易策略，交易成本较小。从收益曲线图可以看出 LightGBM-BiLSTM 量化策略在前两年的收益率和基准相差不大，并没有特别的优势。但从2015年4月左右开始 LightGBM-BiLSTM 量化策略的收益率明显好于基准的收益率。总体而言，该 LightGBM-BiLSTM 量化策略的收益率十分可观，但仍然存在一定的风险。\n第五章 总结与展望 5.1 总结 本文首先介绍了基于深度学习的股票价格预测和量化策略研究的研究背景和研究意义，然后分别介绍了股票价格预测和量化投资策略国内外的研究现状，之后说明了本文的创新点和研究框架。接着本论文在相关理论基础章节大致介绍了本文用到的深度学习模型和量化投资的发展历程。重点介绍了 LSTM，GRU，BiLSTM 这三个模型的基本结构，基本原理和特点。\n随后，本文利用浦发银行和 IBM 的日频数据，通过一系列的数据处理过程和特征提取来对数据进行预处理。然后介绍了 LSTM，GRU，BiLSTM 这三个模型的具体网络结构以及超参数的设定。紧接着我们使用 LSTM，GRU，BiLSTM 分别进行两只股票收盘价的预测和模型评估比较。实验结果表明对于两只股票而言都是 BiLSTM 预测效果更加准确。\n最后，本论文为了进一步说明 BiLSTM 在金融上的运用价值，构建了基于 LightGBM－BiLSTM 的量化投资模型。选取 A 股全市场的股票和多个因子依次进行因子清洗，基于 LightGBM 的因子选择和基于 LSTM 的因子组合等过程。接着，我们构建一定的投资策略并通过累计收益率，年化收益率，年化波动率和夏普比率等评估指标与基准的持有中证全指进行对比。通过对比发现 LightGBM－BiLSTM 量化投资模型能带来更好的收益，说明了利用深度学习构建量化投资策略的有效性。\n5.2 展望 本文虽然分别对比 LSTM，GRU，BiLSTM 这三个模型预测股票收盘价的效果和基于 LightGBM－BiLSTM 量化投资策略取得了一定的成果，但本文研究仍有一些不足之处。结合本文的研究成果，可以进一步进行以下研究和改进：\n预测目标多样化：本文在预测股票价格方面，选取的股票收盘价作为预测目标，虽然这一结果最直观，但 Bachelier（1900）$^{[26]}$ 提出的随机游走假说（Random Walk Hypothesis，RWH）认为股票的价格服从随机漫步，是不可预测的。虽然之后有许多行为经济学家证明这一观点不完全正确，但这也同时说明单纯预测股票的收盘价难度和可解释性不那么强 $^{[27][28]}$。因此可以选择股票波动率预测，股票涨跌判断和股票收益率预测等作为未来的研究的方向。 模型多样化对比：本文在预测股票价格方面，对比了 LSTM，GRU 和 BiLSTM 这三种循环神经网络模型并且说明了 BiLSTM 预测效果比较好，但仍然缺少和其他更多不同模型的对比研究。因此未来可以深入研究与 Autoregressive Integrated Moving Average (ARIMA)，卷积神经网络（Convolutional Neural Networks，CNN），深度神经网络（Deep Neural Networks，DNN）, CNN－LSTM, Transformer 和 TimeGPT 等单一或复合模型之间的对比。 因子多样化：本文在构建量化投资策略方面使用的因子都是技术面的价量因子，因子的种类单一。未来可以选择财务因子，情绪因子，成长因子等不同种类的因子，从而提高策略的性能。同时未来研究还可以适当的加入择时策略，在预测大盘上涨时增加仓位，在预测大盘下跌时减少仓位，赚取贝塔（beta，$\\beta$）的钱。 投资组合优化：本文的因子组合过程仍然不完善，未来可以利用二次规划的方法对投资组合进行优化。 高频交易策略研究：本文的量化投资策略方法采取的是低频交易的策略，未来可以利用股票的 tick 数据来研究高频策略和超高频策略。 参考文献 [1] White, H. “Economic prediction using neural networks: The case of IBM daily stock returns.” Proc. of ICNN. 1988, 2: 451-458.\n[2] Kimoto, T., Asakawa, K., Yoda, M., et al. “Stock market prediction system with modular neural networks.” Proc. of 1990 IJCNN International Joint Conference on Neural Networks. IEEE, 1990: 1-6.\n[3] Zhang, G. P. “Time series forecasting using a hybrid ARIMA and neural network model.” Neurocomputing. 2003, 50: 159-175.\n[4] Akita, R., Yoshihara, A., Matsubara, T., et al. “Deep learning for stock prediction using numerical and textual information.” Proc. of 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS). IEEE, 2016: 1-6.\n[5] 宮崎邦洋, 松尾豊. “Deep Learning を用いた株価予測の分析.” 人工知能学会全国大会論文集 第31回全国大会. 一般社団法人 人工知能学会, 2017: 2D3OS19a3-2D3OS19a3.\n[6] Kim, T., Kim, H. Y. “Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data.” PLoS ONE. 2019, 14(2): e0212320.\n[7] Hochreiter, S., Schmidhuber, J. “Long short-term memory.” Neural Computation. 1997, 9(8): 1735-1780.\n[8] Cho, K., Van Merriënboer, B., Gulcehre, C., et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078. 2014.\n[9] Chung, J., Gulcehre, C., Cho, K. H., et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” arXiv preprint arXiv:1412.3555. 2014.\n[10] Gruber, N., Jockisch, A. “Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?” Frontiers in Artificial Intelligence. 2020, 3(40): 1-6.\n[11] Markowitz, H. “Portfolio Selection.” The Journal of Finance. 1952, 7(1): 77-91. doi:10.2307/2975974.\n[12] Merton, R. C. “An analytic derivation of the efficient portfolio frontier.” Journal of Financial and Quantitative Analysis. 1972: 1851-1872.\n[13] Sharpe, W. F. “Capital asset prices: A theory of market equilibrium under conditions of risk.” The Journal of Finance. 1964, 19(3): 425-442.\n[14] Lintner, J. “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.” Review of Economics and Statistics. 1965, 47(1): 13-37.\n[15] Mossin, J. “Equilibrium in a capital asset market.” Econometrica: Journal of the Econometric Society. 1966: 768-783.\n[16] Ross, S. A. “The arbitrage theory of capital asset pricing.” Journal of Economic Theory. 1976, 13(3): 341-60.\n[17] Fama, E. F., French, K. R. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics. 1993, 33(1): 3-56.\n[18] Fama, E. F., French, K. R. “A five-factor asset pricing model.” Journal of Financial Economics. 2015, 116(1): 1-22.\n[19] Kingma, D. P., Ba, J. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980. 2014.\n[20] Friedman, J. H. “Greedy function approximation: A gradient boosting machine.” Annals of Statistics. 2001: 1189-1232.\n[21] Kopitar, L., Kocbek, P., Cilar, L., et al. “Early detection of type 2 diabetes mellitus using machine learning-based prediction models.” Scientific Reports. 2020, 10(1): 1-12.\n[22] Ke, G., Meng, Q., Finley, T., et al. “Lightgbm: A highly efficient gradient boosting decision tree.” Advances in Neural Information Processing Systems. 2017, 30: 3146-3154.\n[23] Bottou, L., Curtis, F. E., Nocedal, J. “Optimization methods for large-scale machine learning.” SIAM Review. 2018, 60(2): 223-311.\n[24] Sharpe, W. F. “Mutual fund performance.” The Journal of Business. 1966, 39(1): 119-138.\n[25] Sharpe, W. F. “The sharpe ratio.” Journal of Portfolio Management. 1994, 21(1): 49-58.\n[26] Bachelier, L. “Théorie de la spéculation.” Annales Scientifiques de l\u0026rsquo;École Normale Supérieure. 1900, 17: 21-86.\n[27] Fromlet, H. “Behavioral finance-theory and practical application: Systematic analysis of departures from the homo oeconomicus paradigm are essential for realistic financial research and analysis.” Business Economics. 2001: 63-69.\n[28] Lo, A. W. “The adaptive markets hypothesis.” The Journal of Portfolio Management. 2004, 30(5): 15-29.\n参考博客 Colah\u0026rsquo;s Blog. (2015, August 27). Understanding LSTM Networks. 引用 引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Apr 2021). 基于深度学习的股票价格预测和量化策略.\nhttps://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\nOr\n@article{syhya2021stockprediction, title = \u0026#34;基于深度学习的股票价格预测和量化策略\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2021\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\u0026#34; } ","permalink":"https://syhya.github.io/zh/posts/2021-04-21-deep-learning-stock-prediction/","summary":"\u003ch2 id=\"摘要\"\u003e摘要\u003c/h2\u003e\n\u003cp\u003e股票市场是金融市场的重要组成部分，近些年来，股票市场蓬勃发展，股票价格预测和量化投资策略研究吸引了许多领域的研究学者。其中最近几年随着人工智能和机器学习的发展，学者们从传统的统计学模型迁移到了人工智能算法，尤其是在深度学习热潮掀起后，神经网络在股票价格预测和量化投资策略研究中取得了不错的效果。深度学习的目标是学习多层次的特征，通过组合低级特征构建抽象的高级特征，从而挖掘数据的分布式特征表示，基于此进行复杂的非线性建模，从而实现预测任务。其中 RNN 被人们广泛地应用在序列数据上面，如自然语言和语音。股票每天的股价，交易信息都是序列数据，因此之前有很多研究者，基于 RNN 来预测股票价格。由于基础的循环神经网络在层数过多的情况下，会出现梯度消失的问题，而 LSTM 的诞生，解决了此问题，之后出现了诸如 GRU，Peephole LSTM，BiLSTM 等 LSTM 的变体。但传统的股票预测模型有些并未考虑时间因素，有些仅考虑时间上的单向关系。因此，文中使用 BiLSTM 模型进行股票价格预测。从模型原理上来说，BiLSTM 模型充分利用了时间序列上向前，向后两个时间方向的上下文关系，并且避免了长时间序列上的梯度消失和梯度爆炸问题，能够更好地学习到对时间有长期依赖性的信息。\u003c/p\u003e","title":"基于深度学习的股票价格预测和量化策略"}]