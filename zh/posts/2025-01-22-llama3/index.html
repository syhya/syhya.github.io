<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Llama系列（长期更新中） | Yue Shui 博客</title>
<meta name=keywords content="AI,NLP,LLM,Pre-training,Post-training,Llama"><meta name=description content="
注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。
引言
本篇文章将系统梳理LLaMA系列模型从LLaMA1到LLaMA3的发展历程，深入解析其模型架构、训练数据和训练方法，并通过对比表格揭示各版本的核心差异。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2025-01-22-llama3/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-01-22-llama3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2025-01-22-llama3/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="Llama系列（长期更新中）"><meta property="og:description" content=" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。
引言 本篇文章将系统梳理LLaMA系列模型从LLaMA1到LLaMA3的发展历程，深入解析其模型架构、训练数据和训练方法，并通过对比表格揭示各版本的核心差异。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-22T12:00:00+08:00"><meta property="article:modified_time" content="2025-01-22T12:00:00+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Pre-Training"><meta property="article:tag" content="Post-Training"><meta property="article:tag" content="Llama"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="Llama系列（长期更新中）"><meta name=twitter:description content="
注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。
引言
本篇文章将系统梳理LLaMA系列模型从LLaMA1到LLaMA3的发展历程，深入解析其模型架构、训练数据和训练方法，并通过对比表格揭示各版本的核心差异。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"Llama系列（长期更新中）","item":"https://syhya.github.io/zh/posts/2025-01-22-llama3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Llama系列（长期更新中）","name":"Llama系列（长期更新中）","description":" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。\n引言 本篇文章将系统梳理LLaMA系列模型从LLaMA1到LLaMA3的发展历程，深入解析其模型架构、训练数据和训练方法，并通过对比表格揭示各版本的核心差异。\n","keywords":["AI","NLP","LLM","Pre-training","Post-training","Llama"],"articleBody":" 注意: 本文正在更新中，内容只是草稿版本，并不完善，后续会有变动。请随时关注最新版本。\n引言 本篇文章将系统梳理LLaMA系列模型从LLaMA1到LLaMA3的发展历程，深入解析其模型架构、训练数据和训练方法，并通过对比表格揭示各版本的核心差异。\nLLaMA模型演进 大语言模型（LLM）在近年来取得了重大进展，Meta 在 2023 年相继推出了多代 LLaMA 模型系列，每一代在模型规模、训练数据以及应用场景上都有新的提升。本节将依次介绍 LLaMA1、LLaMA2、Code Llama、Llama Guard 和 LLaMA3 模型的主要特性与技术创新。\nLLaMA1 概述 LLaMA1（Touvron et al., 2023）于 2023 年 2 月发布，作为当时性能优异的开源模型，在学术界和工业界迅速引起广泛关注。\n特点 参数规模：提供 7B、13B、30B 和 65B 四个版本。 训练数据量：超过 1.4 万亿个 token。 训练资源：以 65B 模型为例，在 2048 张 A100 80GB GPU 上训练约 21 天。 性能优势：在大多数基准测试中，65B 模型超越了当时流行的 175B 参数 GPT-3。 技术细节 Pre-normalization \u0026 RMSNorm：采用预归一化（Pre-normalization）方案，使用 RMSNorm 替换传统的 LayerNorm，训练更稳定且速度更快。 FFN_SWiGLU：在前馈网络中引入 SWiGLU 结构，激活函数由 ReLU 更换为 SiLU，并对隐藏单元数量进行优化配置。 Rotary Embeddings (RoPE)：在每一层动态注入旋转位置嵌入信息，有利于长序列的建模能力。 应用场景 研究与开发：作为通用大模型的基础，用于各种 NLP 任务研究。 商业应用：可在客户服务、内容生成等场景中提升自动化水平。 教育与培训：为学术机构和教育平台提供新的教学和实验工具。 LLaMA2 概述 LLaMA2（Touvron et al., 2023）是 LLaMA 系列的第二代版本，于 2023 年中发布，相较于第一代在规模和性能上均有大幅提升。\n特点 参数规模：覆盖 7B、13B、34B、70B 四个不同规模。 训练数据量：从 1.4 万亿 token 扩展到 2 万亿 token，增幅约 40%。 上下文长度：支持 4096 个 token，是 LLaMA1 的两倍。 注意力机制：引入分组查询注意力（Group Query Attention, GQA）以提高推理效率和内存利用率。 技术细节 GQA（Group Query Attention）：对查询进行分组，可减少自注意力计算的开销，降低显存占用。 KV Cache：推理阶段采用 KV 缓存，提升解码速度，缩短推理时延。 应用场景 对话系统：更自然、准确地生成对话回复，改善用户交互体验。 内容生成：可应用于新闻、营销文案等高质量文本的自动化生成。 Code Llama Fig. 1. The Code Llama specialization pipeline. (Image source: [Rozière et al., 2023)\n概述 Code Llama（Rozière et al., 2023）基于 LLaMA2 进行额外训练与微调，专门面向代码生成、补全以及指令跟随任务，涵盖多种编程语言。\n特点 多种参数规模：提供 7B、13B、34B、70B 四类版本，可根据算力和应用需求选择。 训练规模： 7B、13B、34B 三个型号在约 5000 亿（500B）标记的代码数据基础上训练； 70B 型号在约 1 万亿（1T）标记的同源数据上训练。 支持长上下文：通过“长上下文微调”（LCFT）过程，能稳定处理多达 16k 乃至 100k tokens 的大型代码文件。 技术细节 架构继承：延续 LLaMA2 的 Transformer 架构，并针对代码领域做专门的目标任务优化。 数据构成：主要来源于公开可用的开源代码库，辅以少量与代码相关的自然语言内容，以保持通用理解能力。 填充中间（Fill-in-the-Middle, FIM）：7B、13B、70B 参数的基础模型直接支持在已有代码任意位置插入补全，方便 IDE 中的实时代码片段生成。 长上下文微调（LCFT）：在已训练的模型上使用更长序列（16k tokens）再次微调，重置 RoPE 参数，使模型在处理超过训练长度的输入时更稳定。 指令微调（Instruct Fine Tuning）：结合 Llama 2 安全指令数据与自监督生成的单元测试筛选数据，以提升模型对自然语言指令的理解和合规性。 应用场景 开发者工具：集成在 IDE 中用于智能补全、调试建议与文档注释，提升开发效率。 教育/培训：为初学者或教学平台提供示例代码、解题思路和习题解析。 商业化软件：与版本控制、CI/CD 等平台集成，为企业级开发提供自动化支持。 研究探索：在自动化测试、代码生成等领域带来新的算法与应用思路。 Llama Guard Fig. 2. Example task instructions for the Llama Guard prompt and response classification tasks. (Image source: Inan et al., 2023)\nFig. 3. Llama Guard 3 Vision classifies harmful content in the response classification task. (Image source: Chi et al., 2024)\n概述 Llama Guard（Inan et al., 2023）是 Meta 为 LLaMA2 及后续版本（如 LLaMA3）开发的安全增强模块，主要面向内容安全的评估与过滤，确保模型输出符合相关安全标准。\n特点 版本： Llama Guard 3 1B：面向基础文本内容的安全评估。 Llama Guard 3 8B：可处理更复杂的文本安全场景，专注于代码解释器滥用（S14）检测。 Llama Guard 3 Vision（Chi et al., 2024）：增强多模态处理能力，支持图像和文本的综合安全评估。 技术细节 多模态评估：通过特殊 \u003c|image|\u003e token 将图像信息与文本输入相结合，进行统一的安全审查。 安全类别：基于 ML Commons consortium 定义的 13 个安全类别（S1-S13），在 3.2 版本中新增针对“代码解释器滥用（S14）”的安全检测。 评估流程：用户将安全类别和对话内容作为输入提示，模型给出判定结果（安全或不安全）及违规类别。 应用场景 内容审核：自动化检测并过滤违反平台或法律规定的内容。 安全监控：在生产环境中实时监控信息流，防范有害或敏感内容传播。 多模态审核：对含图文混合的输入执行更加全面的安全审查。 LLaMA3 Fig. 4. The architecute of Llama 2 and 3. (Image source: Umar Jamil)\n概述 LLaMA3（Grattafiori et al., 2024）是 LLaMA 系列的第三代模型，在多语言、多模态、以及边缘设备部署方面均有提升，拥有从 1B 到 405B 等多种规模。\n特点 参数规模：1B、3B、11B、70B、90B 和 405B 六种版本，覆盖从轻量级到超大规模的多种需求。 训练数据量：累计 15 万亿 token，约为 LLaMA2 的 7.5 倍。 Tokenizer 更新：采用更高效的 tiktoken，词表从 32k 扩大至 128k。 上下文长度：可处理多达 128k tokens 的上下文。 多语言支持：覆盖 8 种语言，全面升级在跨语言环境下的适配能力。 多模态支持：11B 与 90B 版本提供视觉语言模型，可处理与图像结合的任务。 轻量级版本：1B 与 3B 通过剪枝和知识蒸馏技术，适合边缘与移动端部署。 技术细节 全面采用 GQA（Grouped Query Attention）：优化自注意力计算效率与内存使用。 训练方法多样化：结合监督微调(SFT)、拒绝采样(RS)、直接策略优化(DPO)等，以进一步提升模型推理与编码能力。 多模态模型：同时支持图像、视频和语音的多模态处理。 应用场景 高级对话系统：面对更广泛、更复杂的对话需求，提供自然、上下文一致的回复。 跨语言场景：为全球化应用提供多语言支持，覆盖更多人群和市场。 多模态任务：在图像理解、视觉问答等场景中发挥出色的多模态生成与推理能力。 边缘计算：1B 和 3B 版本可在算力有限的设备上运行，为 IoT 或移动端场景提供支持。 LLaMA 系列模型特性对比 特性 LLaMA1 LLaMA2 LLaMA3 发布时间 2023年2月 2023年7月 2024年4月 模型规模 7B、13B、30B、65B 7B、13B、34B、70B 1B、3B、11B、70B、90B、405B 训练数据量 1.4 万亿+ tokens 2 万亿 +tokens 15 万亿+ tokens 上下文长度 2048 tokens 4096 tokens 128k tokens Tokenizer SentencePiece BPE，32k 词汇表 SentencePiece BPE，32k 词汇表 tiktoken BPE，128k 词汇表 位置编码 RoPE RoPE RoPE 注意力机制和推理优化 Multi-Head Attention (MHA) Grouped Query Attention (GQA)+ kv cache Grouped Query Attention (GQA) + kv cache 归一化方法 RMSNorm RMSNorm RMSNorm 激活函数 SwiGLU SwiGLU SwiGLU 训练资源 2048 * A100 80GB 3.3M GPU hours on A100-80GB 16K H100 80GB 应用场景 通用语言理解与生成 通用语言理解与生成，推理效率进一步提升 多模态应用（图像、语音）、轻量级部署、边缘设备适配 关键技术解析 LLaMA3作为系列最新版本，集成了LLaMA1和LLaMA2的核心技术，并在此基础上进行了多项创新和优化。以下是LLaMA3所采用的所有关键技术的详细解析，包括数学公式和相关说明。\nRMS Normalization 在深度学习中，归一化技术在加速训练、提升模型性能和稳定性方面起着至关重要的作用。RMS Normalization (Zhang, et al., 2019) 是一种简化的归一化方法，通过仅计算输入向量的均方根（RMS）进行归一化，从而减少计算开销。其数学表达式如下：\n$$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$其中：\n\\( x \\) 为输入向量。 \\( d \\) 为特征维度的大小。 \\( \\epsilon \\) 为一个极小的常数，用于防止分母为零。 \\( \\gamma \\) 为可学习的缩放参数。 优势：\n计算效率高：相比 LayerNorm 需要计算均值和方差，RMSNorm 仅需计算均方根，减少了计算开销。 训练稳定性：通过归一化输入，提升了模型的训练稳定性，使其在更大的学习率下仍能稳定训练。 资源优化：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。 缺点：\n信息损失：仅使用均方根进行归一化，可能丢失部分信息，如均值信息。 适用性有限：在某些任务中，可能不如 BatchNorm 或 LayerNorm 表现优异。 LLaMA3 选择 RMSNorm 作为其归一化方法，主要基于以下考虑：\n计算效率：RMSNorm 相比 LayerNorm、BatchNorm 和WeightNorm 计算量更低，仅计算输入向量的均方根，适合 LLM 的高效训练。 训练稳定性：RMSNorm 在保持训练稳定性的同时，能够适应更大的学习率，促进模型的快速收敛。 资源优化：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。 简化实现：RMSNorm 的实现相对简单，便于在复杂模型中集成和优化。 通过集成和优化 RMSNorm，LLaMA3 在多种任务中展现出卓越的性能和高效的计算表现。\nFFN_SwiGLU Swish-Gated Linear Unit (Shazeer, 2020) 是 LLaMA 中用于增强前馈网络（Feed-Forward Network, FFN）非线性表达能力的关键技术。SwiGLU 结合了 Swish 激活函数和门控机制，显著提升了模型的表现力和性能。此外，与 PaLM (Chowdhery, 2022) 中使用的$4 d$隐藏维度不同，LLaMA 采用了 $\\frac{2}{3}d$ 的隐藏维度，从而在保持参数量和计算量不变的情况下，实现了更高的参数效率。\n$$ \\operatorname{FFN}_{\\mathrm{SwiGLU}}\\left(x, W_1, W_3, W_2\\right)=\\left(\\operatorname{Swish}\\left(x W_1\\right) \\otimes x W_3\\right) W_2 $$ 其中：\n\\( \\text{Swish}(x) = x \\cdot \\sigma(x) \\)（Swish 激活函数）。 \\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)（Sigmoid 函数）。 \\( \\otimes \\) 表示逐元素相乘。 \\( W_1, W_2, W_3 \\) 为线性变换矩阵。 优势：\n增强非线性表达：SwiGLU 通过结合 Swish 激活函数与门控机制，能够更有效地捕捉复杂的模式和关系，提升 FFN 层的表达能力。 参数效率：采用 $\\frac{2}{3}d$ 的隐藏维度，在引入额外的线性变换矩阵的同时，保持了总参数量不变，实现了参数的高效利用。 性能提升：在多项基准测试中，FFN_SwiGLU 显著提升了模型的性能，尤其在处理复杂任务和长文本时表现尤为出色。例如，在文本生成和理解任务中，SwiGLU 帮助模型更好地理解上下文和长距离依赖关系。 实现细节：\n权重矩阵调整：为了保持与传统 FFN 层相同的参数量和计算量，SwiGLU 通过减少隐藏层的维度（例如，将隐藏层大小从 4d 调整为 $\\frac{2}{3}d$），在引入额外的线性变换矩阵的同时，确保整体模型的效率不受影响。 兼容性：SwiGLU 作为 GLU 家族的一员，能够无缝集成到现有的 Transformer 架构中，替代传统的 ReLU 或 GELU 激活函数，提升模型的整体性能。 Rotary Positional Embeddings (RoPE) Rotary Positional Embeddings (RoPE) 是LLaMA3中用于表示序列中位置关系的技术，通过对Query和Key向量应用旋转变换，增强了模型对相对位置信息的感知能力。\n$$ \\text{RoPE}(x) = x \\cdot e^{i\\theta} $$ 其中：\n\\( x \\) 为输入向量。 \\( \\theta \\) 为与位置和维度相关的旋转角度。 优势：\n相对位置感知：RoPE能够自然地捕捉词汇之间的相对位置关系，提升了长距离依赖的建模效果。 计算效率高：无需额外的计算，位置编码与词向量的结合在计算上是高效的，适用于大规模模型。 适应不同长度的序列：RoPE可以灵活处理不同长度的输入序列，不受固定位置编码的限制。 Grouped Query Attention (GQA) Grouped Query Attention (GQA) (Ainslie, 2023) 是 LLaMA3 中用于优化自注意力计算的关键技术。在大规模语言模型的推理过程中，每个注意力头（head）拥有独立的键（Key）和值（Value）参数会导致巨大的内存消耗。Grouped Query Attention (GQA) 旨在通过将多个查询（Query）头分组，并让每组共享一组键值头，从而在模型性能与推理效率之间取得更优的平衡。GQA 是 Multi-Head Attention (MHA) 和 Multi-Query Attention (MQA) 之间的一种折中方案：\nMHA：每个注意力头都有独立的 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)。 MQA：所有注意力头共享一组 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)。 GQA：将 \\(H\\) 个查询头划分为 \\(G\\) 组，每组共享一组 \\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\)（其中 \\(1 \u003c G \u003c H\\)）。 1. 投影 (Projections) 给定输入序列 \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\)，首先通过线性变换投影得到查询、键和值矩阵：\n$$ \\mathbf{Q} = \\mathbf{X} W_Q, \\quad \\mathbf{K} = \\mathbf{X} W_K, \\quad \\mathbf{V} = \\mathbf{X} W_V, $$其中，\\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) 为可学习的投影矩阵。\n2. 头与分组 (Heads and Grouping) 头的切分：将 \\(\\mathbf{Q}\\)、\\(\\mathbf{K}\\) 和 \\(\\mathbf{V}\\) 分割成 \\(H\\) 个头，每个头的向量维度为 \\(d_{\\text{head}} = \\frac{d}{H}\\)。 $$ \\mathbf{Q} = [\\mathbf{Q}_1; \\mathbf{Q}_2; \\dots; \\mathbf{Q}_H], \\quad \\mathbf{K} = [\\mathbf{K}_1; \\mathbf{K}_2; \\dots; \\mathbf{K}_H], \\quad \\mathbf{V} = [\\mathbf{V}_1; \\mathbf{V}_2; \\dots; \\mathbf{V}_H] $$ 分组：将这 \\(H\\) 个查询头进一步划分为 \\(G\\) 组（\\(1 \u003c G \u003c H\\)）。对于第 \\(g\\) 组，包含 \\(\\frac{H}{G}\\) 个查询头，并共享一组键值头 \\(\\mathbf{K}^g\\) 和 \\(\\mathbf{V}^g\\)。 $$ \\mathcal{G} = \\left\\{ \\mathcal{G}_1, \\mathcal{G}_2, \\dots, \\mathcal{G}_G \\right\\}, \\quad |\\mathcal{G}_g| = \\frac{H}{G} \\quad \\forall g \\in \\{1, 2, \\dots, G\\} $$下图展示了 GQA 与传统 MHA 和 MQA 的对比，可见在 GQA 中，每组查询头公用一组键值头。\nFig. 5. Overview of Grouped Query Attention (GQA). (Image source: Ainslie et al., 2023)\n3. 组内注意力 (Intra-Group Attention) 对于第 \\(g\\) 组，令该组的查询向量为 \\(\\{\\mathbf{Q}_i\\}_{i \\in \\mathcal{G}_g}\\)，共享的键值向量为 \\(\\mathbf{K}^g\\) 和 \\(\\mathbf{V}^g\\)。组内注意力的计算公式为：\n$$ \\text{Attention}_g(\\mathbf{Q}_i, \\mathbf{K}^g, \\mathbf{V}^g) = \\text{softmax}\\left( \\frac{\\mathbf{Q}_i (\\mathbf{K}^g)^\\top}{\\sqrt{d_{\\text{head}}}} \\right) \\mathbf{V}^g $$其中，\\(\\sqrt{d_{\\text{head}}}\\) 为缩放因子，用于稳定梯度和数值计算。\n4. 拼接输出 (Concatenate \u0026 Output) 将所有组的注意力结果在通道维度上拼接，得到矩阵 \\(\\mathbf{O}\\)，然后通过线性变换矩阵 \\(W_O \\in \\mathbb{R}^{d \\times d}\\) 得到最终输出：\n$$ \\mathbf{O} = \\text{Concat}\\left( \\text{Attention}_1, \\text{Attention}_2, \\dots, \\text{Attention}_G \\right) W_O $$其中，\\(\\text{Concat}\\) 表示在通道维度上的拼接操作。\n更多关于注意力机制在 MHA、MQA 和 GQA 之间的详细对比及代码示例，可参考我之前的技术博客：Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA。\nBPE tiktoken tokenizer是LLaMA3采用的新一代分词器，相较于LLaMA2使用的SentencePiece BPE，tiktoken在以下方面有所改进：\n词汇表扩展：词汇表从32k扩展至128k，覆盖更多语言和专业术语，减少了分词次数，提升了生成质量。 编码效率：优化了编码算法，减少了分词时间，提高了处理速度。 生成质量：通过更细粒度的词汇表示，提升了模型生成文本的连贯性和准确性。 $$ \\text{Tokenize}(w) = \\text{BPE}(w) \\quad \\text{vs} \\quad \\text{Tokenize}(w) = \\text{tiktoken\\_BPE}(w) $$ 其中，\\( w \\) 为输入词汇，tiktoken_BPE通过更大词汇表减少了分词次数。 优势：\n减少分词次数：更大的词汇表使得更多词汇能作为单一token处理，减少了分词次数，提高了生成效率和质量。 提升生成质量：更细粒度的词汇表示，使模型在生成文本时能够更准确地表达复杂语义。 编码速度快：优化的编码算法提升了分词速度，适用于大规模模型的高效训练和推理。 轻量级模型 为了适应边缘设备和移动设备的需求，LLaMA3推出了1B和3B参数量的轻量级模型，采用以下技术：\n剪枝技术：通过系统性地移除网络中的冗余参数，减小模型规模，同时保持核心性能。 知识蒸馏：让小模型从大模型中学习，提升其在特定任务上的表现。 优化部署：针对移动设备的硬件架构进行优化，如针对Arm处理器的性能调优，确保模型在资源受限环境中的高效运行。 $$ \\text{Pruned\\_Model} = \\text{Prune}(\\text{Original\\_Model}, \\text{Pruning\\_Rate}) $$$$ \\text{Distilled\\_Model} = \\text{Distill}(\\text{Large\\_Model}, \\text{Small\\_Model}) $$ 其中，Prune表示剪枝操作，Distill表示知识蒸馏过程。 优势：\n适应资源受限设备：减小模型规模，使其适用于边缘设备和移动设备，推动了大语言模型的普及。 保持性能：通过剪枝和知识蒸馏技术，保持了模型的核心性能和表现。 高效运行：优化的模型结构和权重格式（如BFloat16）提升了计算效率，确保在移动设备上的高效运行。 训练方法 LLaMA3在训练数据和方法上进行了全面升级，采用了更大规模的数据和更先进的训练技术：\n预训练阶段：\n大规模数据扩展：训练数据量达到15万亿token，覆盖更多语言、专业领域和多模态数据，提升了模型的泛化能力和多语言支持。 扩展法则（Scaling Laws）： 根据Chinchilla扩展法则，优化模型的训练数据量和参数规模平衡，确保模型在关键任务上的最佳性能。 数学表达式： $$ \\text{Optimal Data} \\propto \\text{Model Size}^{4/3} $$ 这一公式指导了数据和模型规模的平衡，确保随着模型规模的增加，训练数据量也按比例增长，避免模型过拟合或欠拟合。 并行训练策略：\n数据并行：将训练数据分布到多个GPU上，提升数据处理速度。 模型并行：将模型的不同部分分布到多个GPU上，支持更大规模的模型训练。 流水并行：分阶段处理模型的不同部分，提高训练效率。 $$ \\text{Total Throughput} = \\text{Data Parallelism} \\times \\text{Model Parallelism} \\times \\text{Pipeline Parallelism} $$ 其中，总吞吐量（Total Throughput）是数据并行、模型并行和流水并行的乘积，显著提升了训练效率。 硬件优化：\n高效利用GPU：在16K GPU上实现每GPU超过400 TFLOPS的计算利用率，通过定制的24K GPU集群进行训练，确保训练过程的高效性和稳定性。 错误处理与存储优化： 自动错误检测与处理：确保训练过程的连续性和高效性。 可扩展存储系统：减少检查点和回滚的开销，提高数据存储效率。 微调阶段：\n多轮对齐步骤： 监督微调（SFT）：使用高质量的标注数据进一步优化模型性能。 拒绝采样（Rejection Sampling）：通过拒绝低质量内容，提升生成文本的质量。 近端策略优化（Proximal Policy Optimization, PPO）和直接策略优化（Direct Policy Optimization, DPO）：结合两者的优势，优化模型的生成策略，使其更符合人类偏好。 $$ \\mathcal{L}_{\\text{RLHF}} = \\mathbb{E}_{\\theta \\sim \\pi_{\\theta}} \\left[ r(s, a) \\right] $$ 其中，\\( \\mathcal{L}_{\\text{RLHF}} \\)为RLHF的损失函数，\\( \\pi_{\\theta} \\)为策略分布，\\( r(s, a) \\)为奖励函数。 多模态训练：\n视觉语言模型：结合图像和文本数据，提升模型在多模态任务中的表现。 代码数据扩展：增加代码token数量，提升模型在编程任务中的表现。 模型安全与质量控制：\n数据过滤pipeline： 启发式过滤器：基于规则的过滤，提高数据质量。 NSFW过滤器：去除不适内容，确保数据的安全性。 语义重复数据删除：使用语义分析技术，删除内容高度相似的数据。 文本分类器：预测数据质量，进一步优化数据集。 优化训练堆栈：\n高级训练堆栈：自动检测和处理训练过程中的错误，提升硬件可靠性。 性能调优：针对不同硬件平台进行优化，确保训练过程的高效性。 LLaMA3通过这些先进的训练方法和优化策略，显著提升了模型的性能和适应性，成为开源大语言模型领域的领先者。\n总结 LLaMA系列模型从LLaMA1到LLaMA3，体现了大规模预训练语言模型的技术进化与产业影响。通过不断扩展训练数据量、优化模型架构和引入先进的训练方法，LLaMA系列在性能、多语言支持和多模态能力等方面取得了显著的提升。其开源策略不仅推动了全球AI社区的创新和发展，也为产业应用提供了强大的技术支持。\n参考资料 Hendrycks and Gimpel, 2016 GLU Variants Improve Transformer LLaMA: Open and Efficient Foundation Language Models LLaMA2: Open Foundation and Fine-Tuned Chat Models meta-llama repo ","wordCount":"7449","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2025-01-22T12:00:00+08:00","dateModified":"2025-01-22T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2025-01-22-llama3/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Llama系列（长期更新中）</h1><div class=post-meta><span title='2025-01-22 12:00:00 +0800 +0800'>2025-01-22</span>&nbsp;·&nbsp;15 分钟&nbsp;·&nbsp;7449 字&nbsp;·&nbsp;Yue Shui</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#引言>引言</a></li><li><a href=#llama模型演进>LLaMA模型演进</a><ul><li><a href=#llama1>LLaMA1</a><ul><li><a href=#概述>概述</a></li><li><a href=#特点>特点</a></li><li><a href=#技术细节>技术细节</a></li><li><a href=#应用场景>应用场景</a></li></ul></li><li><a href=#llama2>LLaMA2</a><ul><li><a href=#概述-1>概述</a></li><li><a href=#特点-1>特点</a></li><li><a href=#技术细节-1>技术细节</a></li><li><a href=#应用场景-1>应用场景</a></li></ul></li><li><a href=#code-llama>Code Llama</a><ul><li><a href=#概述-2>概述</a></li><li><a href=#特点-2>特点</a></li><li><a href=#技术细节-2>技术细节</a></li><li><a href=#应用场景-2>应用场景</a></li></ul></li><li><a href=#llama-guard>Llama Guard</a><ul><li><a href=#概述-3>概述</a></li><li><a href=#特点-3>特点</a></li><li><a href=#技术细节-3>技术细节</a></li><li><a href=#应用场景-3>应用场景</a></li></ul></li><li><a href=#llama3>LLaMA3</a><ul><li><a href=#概述-4>概述</a></li><li><a href=#特点-4>特点</a></li><li><a href=#技术细节-4>技术细节</a></li><li><a href=#应用场景-4>应用场景</a></li></ul></li><li><a href=#llama-系列模型特性对比>LLaMA 系列模型特性对比</a></li></ul></li><li><a href=#关键技术解析>关键技术解析</a><ul><li><a href=#rms-normalization>RMS Normalization</a></li><li><a href=#ffn_swiglu>FFN_SwiGLU</a><ul><li><a href=#rotary-positional-embeddings-rope>Rotary Positional Embeddings (RoPE)</a></li></ul></li><li><a href=#grouped-query-attention-gqa>Grouped Query Attention (GQA)</a><ul><li><a href=#1-投影-projections>1. 投影 (Projections)</a></li><li><a href=#2-头与分组-heads-and-grouping>2. 头与分组 (Heads and Grouping)</a></li><li><a href=#3-组内注意力-intra-group-attention>3. 组内注意力 (Intra-Group Attention)</a></li><li><a href=#4-拼接输出-concatenate--output>4. 拼接输出 (Concatenate & Output)</a></li><li><a href=#bpe>BPE</a></li><li><a href=#轻量级模型>轻量级模型</a></li><li><a href=#训练方法>训练方法</a></li></ul></li></ul></li><li><a href=#总结>总结</a></li><li><a href=#参考资料>参考资料</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p><strong>注意</strong>: 本文<strong>正在更新中</strong>，内容只是<strong>草稿版本</strong>，并不完善，后续会有变动。请随时关注最新版本。</p></blockquote><h2 id=引言>引言<a hidden class=anchor aria-hidden=true href=#引言>#</a></h2><p>本篇文章将系统梳理LLaMA系列模型从LLaMA1到LLaMA3的发展历程，深入解析其模型架构、训练数据和训练方法，并通过对比表格揭示各版本的核心差异。</p><h2 id=llama模型演进>LLaMA模型演进<a hidden class=anchor aria-hidden=true href=#llama模型演进>#</a></h2><p>大语言模型（LLM）在近年来取得了重大进展，Meta 在 2023 年相继推出了多代 LLaMA 模型系列，每一代在模型规模、训练数据以及应用场景上都有新的提升。本节将依次介绍 LLaMA1、LLaMA2、Code Llama、Llama Guard 和 LLaMA3 模型的主要特性与技术创新。</p><hr><h3 id=llama1>LLaMA1<a hidden class=anchor aria-hidden=true href=#llama1>#</a></h3><h4 id=概述>概述<a hidden class=anchor aria-hidden=true href=#概述>#</a></h4><p><strong>LLaMA1</strong>（<a href=https://arxiv.org/abs/2302.13971>Touvron et al., 2023</a>）于 2023 年 2 月发布，作为当时性能优异的开源模型，在学术界和工业界迅速引起广泛关注。</p><h4 id=特点>特点<a hidden class=anchor aria-hidden=true href=#特点>#</a></h4><ul><li><strong>参数规模</strong>：提供 7B、13B、30B 和 65B 四个版本。</li><li><strong>训练数据量</strong>：超过 1.4 万亿个 token。</li><li><strong>训练资源</strong>：以 65B 模型为例，在 2048 张 A100 80GB GPU 上训练约 21 天。</li><li><strong>性能优势</strong>：在大多数基准测试中，65B 模型超越了当时流行的 175B 参数 GPT-3。</li></ul><h4 id=技术细节>技术细节<a hidden class=anchor aria-hidden=true href=#技术细节>#</a></h4><ul><li><strong>Pre-normalization & RMSNorm</strong>：采用预归一化（Pre-normalization）方案，使用 RMSNorm 替换传统的 LayerNorm，训练更稳定且速度更快。</li><li><strong>FFN_SWiGLU</strong>：在前馈网络中引入 SWiGLU 结构，激活函数由 ReLU 更换为 SiLU，并对隐藏单元数量进行优化配置。</li><li><strong>Rotary Embeddings (RoPE)</strong>：在每一层动态注入旋转位置嵌入信息，有利于长序列的建模能力。</li></ul><h4 id=应用场景>应用场景<a hidden class=anchor aria-hidden=true href=#应用场景>#</a></h4><ul><li><strong>研究与开发</strong>：作为通用大模型的基础，用于各种 NLP 任务研究。</li><li><strong>商业应用</strong>：可在客户服务、内容生成等场景中提升自动化水平。</li><li><strong>教育与培训</strong>：为学术机构和教育平台提供新的教学和实验工具。</li></ul><hr><h3 id=llama2>LLaMA2<a hidden class=anchor aria-hidden=true href=#llama2>#</a></h3><h4 id=概述-1>概述<a hidden class=anchor aria-hidden=true href=#概述-1>#</a></h4><p><strong>LLaMA2</strong>（<a href=https://arxiv.org/abs/2307.09288>Touvron et al., 2023</a>）是 LLaMA 系列的第二代版本，于 2023 年中发布，相较于第一代在规模和性能上均有大幅提升。</p><h4 id=特点-1>特点<a hidden class=anchor aria-hidden=true href=#特点-1>#</a></h4><ul><li><strong>参数规模</strong>：覆盖 7B、13B、34B、70B 四个不同规模。</li><li><strong>训练数据量</strong>：从 1.4 万亿 token 扩展到 2 万亿 token，增幅约 40%。</li><li><strong>上下文长度</strong>：支持 4096 个 token，是 LLaMA1 的两倍。</li><li><strong>注意力机制</strong>：引入分组查询注意力（Group Query Attention, GQA）以提高推理效率和内存利用率。</li></ul><h4 id=技术细节-1>技术细节<a hidden class=anchor aria-hidden=true href=#技术细节-1>#</a></h4><ul><li><strong>GQA（Group Query Attention）</strong>：对查询进行分组，可减少自注意力计算的开销，降低显存占用。</li><li><strong>KV Cache</strong>：推理阶段采用 KV 缓存，提升解码速度，缩短推理时延。</li></ul><h4 id=应用场景-1>应用场景<a hidden class=anchor aria-hidden=true href=#应用场景-1>#</a></h4><ul><li><strong>对话系统</strong>：更自然、准确地生成对话回复，改善用户交互体验。</li><li><strong>内容生成</strong>：可应用于新闻、营销文案等高质量文本的自动化生成。</li></ul><hr><h3 id=code-llama>Code Llama<a hidden class=anchor aria-hidden=true href=#code-llama>#</a></h3><figure class=align-center><img loading=lazy src=codellama.png#center alt="Fig. 1. The Code Llama specialization pipeline. (Image source: [Rozière et al., 2023)" width=100%><figcaption><p>Fig. 1. The Code Llama specialization pipeline. (Image source: [<a href=https://arxiv.org/abs/2308.12950>Rozière et al., 2023</a>)</p></figcaption></figure><h4 id=概述-2>概述<a hidden class=anchor aria-hidden=true href=#概述-2>#</a></h4><p><strong>Code Llama</strong>（<a href=https://arxiv.org/abs/2308.12950>Rozière et al., 2023</a>）基于 LLaMA2 进行额外训练与微调，专门面向代码生成、补全以及指令跟随任务，涵盖多种编程语言。</p><h4 id=特点-2>特点<a hidden class=anchor aria-hidden=true href=#特点-2>#</a></h4><ol><li><strong>多种参数规模</strong>：提供 7B、13B、34B、70B 四类版本，可根据算力和应用需求选择。</li><li><strong>训练规模</strong>：<ul><li>7B、13B、34B 三个型号在约 5000 亿（500B）标记的代码数据基础上训练；</li><li>70B 型号在约 1 万亿（1T）标记的同源数据上训练。</li></ul></li><li><strong>支持长上下文</strong>：通过“长上下文微调”（LCFT）过程，能稳定处理多达 16k 乃至 100k tokens 的大型代码文件。</li></ol><h4 id=技术细节-2>技术细节<a hidden class=anchor aria-hidden=true href=#技术细节-2>#</a></h4><ul><li><strong>架构继承</strong>：延续 LLaMA2 的 Transformer 架构，并针对代码领域做专门的目标任务优化。</li><li><strong>数据构成</strong>：主要来源于公开可用的开源代码库，辅以少量与代码相关的自然语言内容，以保持通用理解能力。</li><li><strong>填充中间（Fill-in-the-Middle, FIM）</strong>：7B、13B、70B 参数的基础模型直接支持在已有代码任意位置插入补全，方便 IDE 中的实时代码片段生成。</li><li><strong>长上下文微调（LCFT）</strong>：在已训练的模型上使用更长序列（16k tokens）再次微调，重置 RoPE 参数，使模型在处理超过训练长度的输入时更稳定。</li><li><strong>指令微调（Instruct Fine Tuning）</strong>：结合 Llama 2 安全指令数据与自监督生成的单元测试筛选数据，以提升模型对自然语言指令的理解和合规性。</li></ul><h4 id=应用场景-2>应用场景<a hidden class=anchor aria-hidden=true href=#应用场景-2>#</a></h4><ol><li><strong>开发者工具</strong>：集成在 IDE 中用于智能补全、调试建议与文档注释，提升开发效率。</li><li><strong>教育/培训</strong>：为初学者或教学平台提供示例代码、解题思路和习题解析。</li><li><strong>商业化软件</strong>：与版本控制、CI/CD 等平台集成，为企业级开发提供自动化支持。</li><li><strong>研究探索</strong>：在自动化测试、代码生成等领域带来新的算法与应用思路。</li></ol><h3 id=llama-guard>Llama Guard<a hidden class=anchor aria-hidden=true href=#llama-guard>#</a></h3><figure class=align-center><img loading=lazy src=llama_guard.png#center alt="Fig. 2. Example task instructions for the Llama Guard prompt and response classification tasks. (Image source: Inan et al., 2023)" width=100%><figcaption><p>Fig. 2. Example task instructions for the Llama Guard prompt and response classification tasks. (Image source: <a href=https://arxiv.org/abs/2312.06674>Inan et al., 2023</a>)</p></figcaption></figure><figure class=align-center><img loading=lazy src=llama_guard_vision.png#center alt="Fig. 3. Llama Guard 3 Vision classifies harmful content in the response classification task. (Image source: Chi et al., 2024)" width=100%><figcaption><p>Fig. 3. Llama Guard 3 Vision classifies harmful content in the response classification task. (Image source: <a href=https://arxiv.org/abs/2411.10414>Chi et al., 2024</a>)</p></figcaption></figure><h4 id=概述-3>概述<a hidden class=anchor aria-hidden=true href=#概述-3>#</a></h4><p><strong>Llama Guard</strong>（<a href=https://arxiv.org/abs/2312.06674>Inan et al., 2023</a>）是 Meta 为 LLaMA2 及后续版本（如 LLaMA3）开发的安全增强模块，主要面向内容安全的评估与过滤，确保模型输出符合相关安全标准。</p><h4 id=特点-3>特点<a hidden class=anchor aria-hidden=true href=#特点-3>#</a></h4><ul><li><strong>版本</strong>：<ol><li><strong>Llama Guard 3 1B</strong>：面向基础文本内容的安全评估。</li><li><strong>Llama Guard 3 8B</strong>：可处理更复杂的文本安全场景，专注于代码解释器滥用（S14）检测。</li><li><strong>Llama Guard 3 Vision</strong>（<a href=https://arxiv.org/abs/2411.10414>Chi et al., 2024</a>）：增强多模态处理能力，支持图像和文本的综合安全评估。</li></ol></li></ul><h4 id=技术细节-3>技术细节<a hidden class=anchor aria-hidden=true href=#技术细节-3>#</a></h4><ul><li><strong>多模态评估</strong>：通过特殊 <code>&lt;|image|></code> token 将图像信息与文本输入相结合，进行统一的安全审查。</li><li><strong>安全类别</strong>：基于 ML Commons consortium 定义的 13 个安全类别（S1-S13），在 3.2 版本中新增针对“代码解释器滥用（S14）”的安全检测。</li><li><strong>评估流程</strong>：用户将安全类别和对话内容作为输入提示，模型给出判定结果（安全或不安全）及违规类别。</li></ul><h4 id=应用场景-3>应用场景<a hidden class=anchor aria-hidden=true href=#应用场景-3>#</a></h4><ul><li><strong>内容审核</strong>：自动化检测并过滤违反平台或法律规定的内容。</li><li><strong>安全监控</strong>：在生产环境中实时监控信息流，防范有害或敏感内容传播。</li><li><strong>多模态审核</strong>：对含图文混合的输入执行更加全面的安全审查。</li></ul><h3 id=llama3>LLaMA3<a hidden class=anchor aria-hidden=true href=#llama3>#</a></h3><figure class=align-center><img loading=lazy src=llama3_architecture.png#center alt="Fig. 4. The architecute of Llama 2 and 3. (Image source: Umar Jamil)" width=80%><figcaption><p>Fig. 4. The architecute of Llama 2 and 3. (Image source: <a href=https://github.com/hkproj/pytorch-llama/blob/main/Slides.pdf>Umar Jamil</a>)</p></figcaption></figure><h4 id=概述-4>概述<a hidden class=anchor aria-hidden=true href=#概述-4>#</a></h4><p><strong>LLaMA3</strong>（<a href=https://arxiv.org/abs/2411.10414>Grattafiori et al., 2024</a>）是 LLaMA 系列的第三代模型，在多语言、多模态、以及边缘设备部署方面均有提升，拥有从 1B 到 405B 等多种规模。</p><h4 id=特点-4>特点<a hidden class=anchor aria-hidden=true href=#特点-4>#</a></h4><ul><li><strong>参数规模</strong>：1B、3B、11B、70B、90B 和 405B 六种版本，覆盖从轻量级到超大规模的多种需求。</li><li><strong>训练数据量</strong>：累计 15 万亿 token，约为 LLaMA2 的 7.5 倍。</li><li><strong>Tokenizer 更新</strong>：采用更高效的 <code>tiktoken</code>，词表从 32k 扩大至 128k。</li><li><strong>上下文长度</strong>：可处理多达 128k tokens 的上下文。</li><li><strong>多语言支持</strong>：覆盖 8 种语言，全面升级在跨语言环境下的适配能力。</li><li><strong>多模态支持</strong>：11B 与 90B 版本提供视觉语言模型，可处理与图像结合的任务。</li><li><strong>轻量级版本</strong>：1B 与 3B 通过剪枝和知识蒸馏技术，适合边缘与移动端部署。</li></ul><h4 id=技术细节-4>技术细节<a hidden class=anchor aria-hidden=true href=#技术细节-4>#</a></h4><ul><li><strong>全面采用 GQA（Grouped Query Attention）</strong>：优化自注意力计算效率与内存使用。</li><li><strong>训练方法多样化</strong>：结合监督微调(SFT)、拒绝采样(RS)、直接策略优化(DPO)等，以进一步提升模型推理与编码能力。</li><li><strong>多模态模型</strong>：同时支持图像、视频和语音的多模态处理。</li></ul><h4 id=应用场景-4>应用场景<a hidden class=anchor aria-hidden=true href=#应用场景-4>#</a></h4><ul><li><strong>高级对话系统</strong>：面对更广泛、更复杂的对话需求，提供自然、上下文一致的回复。</li><li><strong>跨语言场景</strong>：为全球化应用提供多语言支持，覆盖更多人群和市场。</li><li><strong>多模态任务</strong>：在图像理解、视觉问答等场景中发挥出色的多模态生成与推理能力。</li><li><strong>边缘计算</strong>：1B 和 3B 版本可在算力有限的设备上运行，为 IoT 或移动端场景提供支持。</li></ul><h3 id=llama-系列模型特性对比>LLaMA 系列模型特性对比<a hidden class=anchor aria-hidden=true href=#llama-系列模型特性对比>#</a></h3><table><thead><tr><th>特性</th><th>LLaMA1</th><th>LLaMA2</th><th>LLaMA3</th></tr></thead><tbody><tr><td><strong>发布时间</strong></td><td>2023年2月</td><td>2023年7月</td><td>2024年4月</td></tr><tr><td><strong>模型规模</strong></td><td>7B、13B、30B、65B</td><td>7B、13B、34B、70B</td><td>1B、3B、11B、70B、90B、405B</td></tr><tr><td><strong>训练数据量</strong></td><td>1.4 万亿+ tokens</td><td>2 万亿 +tokens</td><td>15 万亿+ tokens</td></tr><tr><td><strong>上下文长度</strong></td><td>2048 tokens</td><td>4096 tokens</td><td>128k tokens</td></tr><tr><td><strong>Tokenizer</strong></td><td>SentencePiece BPE，32k 词汇表</td><td>SentencePiece BPE，32k 词汇表</td><td>tiktoken BPE，128k 词汇表</td></tr><tr><td><strong>位置编码</strong></td><td>RoPE</td><td>RoPE</td><td>RoPE</td></tr><tr><td><strong>注意力机制和推理优化</strong></td><td>Multi-Head Attention (MHA)</td><td>Grouped Query Attention (GQA)+ kv cache</td><td>Grouped Query Attention (GQA) + kv cache</td></tr><tr><td><strong>归一化方法</strong></td><td>RMSNorm</td><td>RMSNorm</td><td>RMSNorm</td></tr><tr><td><strong>激活函数</strong></td><td>SwiGLU</td><td>SwiGLU</td><td>SwiGLU</td></tr><tr><td><strong>训练资源</strong></td><td>2048 * A100 80GB</td><td>3.3M GPU hours on A100-80GB</td><td>16K H100 80GB</td></tr><tr><td><strong>应用场景</strong></td><td>通用语言理解与生成</td><td>通用语言理解与生成，推理效率进一步提升</td><td>多模态应用（图像、语音）、轻量级部署、边缘设备适配</td></tr></tbody></table><h2 id=关键技术解析>关键技术解析<a hidden class=anchor aria-hidden=true href=#关键技术解析>#</a></h2><p>LLaMA3作为系列最新版本，集成了LLaMA1和LLaMA2的核心技术，并在此基础上进行了多项创新和优化。以下是LLaMA3所采用的所有关键技术的详细解析，包括数学公式和相关说明。</p><h3 id=rms-normalization>RMS Normalization<a hidden class=anchor aria-hidden=true href=#rms-normalization>#</a></h3><p>在深度学习中，归一化技术在加速训练、提升模型性能和稳定性方面起着至关重要的作用。RMS Normalization (<a href=https://arxiv.org/abs/1910.07467>Zhang, et al., 2019</a>) 是一种简化的归一化方法，通过仅计算输入向量的均方根（RMS）进行归一化，从而减少计算开销。其数学表达式如下：</p>$$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2 + \epsilon}} \cdot \gamma
$$<p>其中：</p><ul><li>\( x \) 为输入向量。</li><li>\( d \) 为特征维度的大小。</li><li>\( \epsilon \) 为一个极小的常数，用于防止分母为零。</li><li>\( \gamma \) 为可学习的缩放参数。</li></ul><p><strong>优势：</strong></p><ul><li><strong>计算效率高</strong>：相比 LayerNorm 需要计算均值和方差，RMSNorm 仅需计算均方根，减少了计算开销。</li><li><strong>训练稳定性</strong>：通过归一化输入，提升了模型的训练稳定性，使其在更大的学习率下仍能稳定训练。</li><li><strong>资源优化</strong>：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>信息损失</strong>：仅使用均方根进行归一化，可能丢失部分信息，如均值信息。</li><li><strong>适用性有限</strong>：在某些任务中，可能不如 BatchNorm 或 LayerNorm 表现优异。</li></ul><p>LLaMA3 选择 <strong>RMSNorm</strong> 作为其归一化方法，主要基于以下考虑：</p><ul><li><strong>计算效率</strong>：RMSNorm 相比 LayerNorm、BatchNorm 和WeightNorm 计算量更低，仅计算输入向量的均方根，适合 LLM 的高效训练。</li><li><strong>训练稳定性</strong>：RMSNorm 在保持训练稳定性的同时，能够适应更大的学习率，促进模型的快速收敛。</li><li><strong>资源优化</strong>：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。</li><li><strong>简化实现</strong>：RMSNorm 的实现相对简单，便于在复杂模型中集成和优化。</li></ul><p>通过集成和优化 RMSNorm，LLaMA3 在多种任务中展现出卓越的性能和高效的计算表现。</p><h3 id=ffn_swiglu>FFN_SwiGLU<a hidden class=anchor aria-hidden=true href=#ffn_swiglu>#</a></h3><p>Swish-Gated Linear Unit (<a href=https://arxiv.org/abs/2002.05202v1>Shazeer, 2020</a>) 是 LLaMA 中用于增强前馈网络（Feed-Forward Network, FFN）非线性表达能力的关键技术。SwiGLU 结合了 Swish 激活函数和门控机制，显著提升了模型的表现力和性能。此外，与 PaLM (<a href=https://arxiv.org/abs/2204.02311>Chowdhery, 2022</a>) 中使用的$4 d$隐藏维度不同，LLaMA 采用了 $\frac{2}{3}d$ 的隐藏维度，从而在保持参数量和计算量不变的情况下，实现了更高的参数效率。</p>$$
\operatorname{FFN}_{\mathrm{SwiGLU}}\left(x, W_1, W_3, W_2\right)=\left(\operatorname{Swish}\left(x W_1\right) \otimes x W_3\right) W_2
$$<p>其中：</p><ul><li>\( \text{Swish}(x) = x \cdot \sigma(x) \)（Swish 激活函数）。</li><li>\( \sigma(x) = \frac{1}{1 + e^{-x}} \)（Sigmoid 函数）。</li><li>\( \otimes \) 表示逐元素相乘。</li><li>\( W_1, W_2, W_3 \) 为线性变换矩阵。</li></ul><p><strong>优势</strong>：</p><ul><li><strong>增强非线性表达</strong>：SwiGLU 通过结合 Swish 激活函数与门控机制，能够更有效地捕捉复杂的模式和关系，提升 FFN 层的表达能力。</li><li><strong>参数效率</strong>：采用 $\frac{2}{3}d$ 的隐藏维度，在引入额外的线性变换矩阵的同时，保持了总参数量不变，实现了参数的高效利用。</li><li><strong>性能提升</strong>：在多项基准测试中，FFN_SwiGLU 显著提升了模型的性能，尤其在处理复杂任务和长文本时表现尤为出色。例如，在文本生成和理解任务中，SwiGLU 帮助模型更好地理解上下文和长距离依赖关系。</li></ul><p><strong>实现细节</strong>：</p><ul><li><strong>权重矩阵调整</strong>：为了保持与传统 FFN 层相同的参数量和计算量，SwiGLU 通过减少隐藏层的维度（例如，将隐藏层大小从 4d 调整为 $\frac{2}{3}d$），在引入额外的线性变换矩阵的同时，确保整体模型的效率不受影响。</li><li><strong>兼容性</strong>：SwiGLU 作为 GLU 家族的一员，能够无缝集成到现有的 Transformer 架构中，替代传统的 ReLU 或 GELU 激活函数，提升模型的整体性能。</li></ul><h4 id=rotary-positional-embeddings-rope>Rotary Positional Embeddings (RoPE)<a hidden class=anchor aria-hidden=true href=#rotary-positional-embeddings-rope>#</a></h4><p><strong>Rotary Positional Embeddings (RoPE)</strong> 是LLaMA3中用于表示序列中位置关系的技术，通过对Query和Key向量应用旋转变换，增强了模型对相对位置信息的感知能力。</p>$$
\text{RoPE}(x) = x \cdot e^{i\theta}
$$<p>其中：</p><ul><li>\( x \) 为输入向量。</li><li>\( \theta \) 为与位置和维度相关的旋转角度。</li></ul><p><strong>优势</strong>：</p><ul><li><strong>相对位置感知</strong>：RoPE能够自然地捕捉词汇之间的相对位置关系，提升了长距离依赖的建模效果。</li><li><strong>计算效率高</strong>：无需额外的计算，位置编码与词向量的结合在计算上是高效的，适用于大规模模型。</li><li><strong>适应不同长度的序列</strong>：RoPE可以灵活处理不同长度的输入序列，不受固定位置编码的限制。</li></ul><h3 id=grouped-query-attention-gqa>Grouped Query Attention (GQA)<a hidden class=anchor aria-hidden=true href=#grouped-query-attention-gqa>#</a></h3><p>Grouped Query Attention (GQA) (<a href=https://arxiv.org/pdf/2305.13245>Ainslie, 2023</a>) 是 LLaMA3 中用于优化自注意力计算的关键技术。在大规模语言模型的推理过程中，每个注意力头（head）拥有独立的键（Key）和值（Value）参数会导致巨大的内存消耗。<strong>Grouped Query Attention (GQA)</strong> 旨在通过将多个查询（Query）头分组，并让每组共享一组键值头，从而在模型性能与推理效率之间取得更优的平衡。GQA 是 <strong>Multi-Head Attention (MHA)</strong> 和 <strong>Multi-Query Attention (MQA)</strong> 之间的一种折中方案：</p><ul><li><strong>MHA</strong>：每个注意力头都有独立的 \(\mathbf{K}\) 和 \(\mathbf{V}\)。</li><li><strong>MQA</strong>：所有注意力头共享一组 \(\mathbf{K}\) 和 \(\mathbf{V}\)。</li><li><strong>GQA</strong>：将 \(H\) 个查询头划分为 \(G\) 组，每组共享一组 \(\mathbf{K}\) 和 \(\mathbf{V}\)（其中 \(1 < G < H\)）。</li></ul><h4 id=1-投影-projections>1. 投影 (Projections)<a hidden class=anchor aria-hidden=true href=#1-投影-projections>#</a></h4><p>给定输入序列 \(\mathbf{X} \in \mathbb{R}^{B \times S \times d}\)，首先通过线性变换投影得到查询、键和值矩阵：</p>$$
\mathbf{Q} = \mathbf{X} W_Q, \quad
\mathbf{K} = \mathbf{X} W_K, \quad
\mathbf{V} = \mathbf{X} W_V,
$$<p>其中，\(W_Q, W_K, W_V \in \mathbb{R}^{d \times d}\) 为可学习的投影矩阵。</p><h4 id=2-头与分组-heads-and-grouping>2. 头与分组 (Heads and Grouping)<a hidden class=anchor aria-hidden=true href=#2-头与分组-heads-and-grouping>#</a></h4><ul><li><strong>头的切分</strong>：将 \(\mathbf{Q}\)、\(\mathbf{K}\) 和 \(\mathbf{V}\) 分割成 \(H\) 个头，每个头的向量维度为 \(d_{\text{head}} = \frac{d}{H}\)。</li></ul>$$
\mathbf{Q} = [\mathbf{Q}_1; \mathbf{Q}_2; \dots; \mathbf{Q}_H], \quad
\mathbf{K} = [\mathbf{K}_1; \mathbf{K}_2; \dots; \mathbf{K}_H], \quad
\mathbf{V} = [\mathbf{V}_1; \mathbf{V}_2; \dots; \mathbf{V}_H]
$$<ul><li><strong>分组</strong>：将这 \(H\) 个查询头进一步划分为 \(G\) 组（\(1 < G < H\)）。对于第 \(g\) 组，包含 \(\frac{H}{G}\) 个查询头，并共享一组键值头 \(\mathbf{K}^g\) 和 \(\mathbf{V}^g\)。</li></ul>$$
\mathcal{G} = \left\{ \mathcal{G}_1, \mathcal{G}_2, \dots, \mathcal{G}_G \right\}, \quad |\mathcal{G}_g| = \frac{H}{G} \quad \forall g \in \{1, 2, \dots, G\}
$$<p>下图展示了 GQA 与传统 MHA 和 MQA 的对比，可见在 GQA 中，<strong>每组查询头公用一组键值头</strong>。</p><figure class=align-center><img loading=lazy src=attention_comparison.png#center alt="Fig. 5. Overview of Grouped Query Attention (GQA). (Image source: Ainslie et al., 2023)" width=100%><figcaption><p>Fig. 5. Overview of Grouped Query Attention (GQA). (Image source: <a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>)</p></figcaption></figure><h4 id=3-组内注意力-intra-group-attention>3. 组内注意力 (Intra-Group Attention)<a hidden class=anchor aria-hidden=true href=#3-组内注意力-intra-group-attention>#</a></h4><p>对于第 \(g\) 组，令该组的查询向量为 \(\{\mathbf{Q}_i\}_{i \in \mathcal{G}_g}\)，共享的键值向量为 \(\mathbf{K}^g\) 和 \(\mathbf{V}^g\)。组内注意力的计算公式为：</p>$$
\text{Attention}_g(\mathbf{Q}_i, \mathbf{K}^g, \mathbf{V}^g) = \text{softmax}\left( \frac{\mathbf{Q}_i (\mathbf{K}^g)^\top}{\sqrt{d_{\text{head}}}} \right) \mathbf{V}^g
$$<p>其中，\(\sqrt{d_{\text{head}}}\) 为缩放因子，用于稳定梯度和数值计算。</p><h4 id=4-拼接输出-concatenate--output>4. 拼接输出 (Concatenate & Output)<a hidden class=anchor aria-hidden=true href=#4-拼接输出-concatenate--output>#</a></h4><p>将所有组的注意力结果在通道维度上拼接，得到矩阵 \(\mathbf{O}\)，然后通过线性变换矩阵 \(W_O \in \mathbb{R}^{d \times d}\) 得到最终输出：</p>$$
\mathbf{O} = \text{Concat}\left( \text{Attention}_1, \text{Attention}_2, \dots, \text{Attention}_G \right) W_O
$$<p>其中，\(\text{Concat}\) 表示在通道维度上的拼接操作。</p><blockquote><p>更多关于注意力机制在 <strong>MHA</strong>、<strong>MQA</strong> 和 <strong>GQA</strong> 之间的详细对比及代码示例，可参考我之前的技术博客：<a href=https://syhya.github.io/posts/2025-01-16-group-query-attention/#grouped-query-attention-gqa>Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA</a>。</p></blockquote><h4 id=bpe>BPE<a hidden class=anchor aria-hidden=true href=#bpe>#</a></h4><p><strong>tiktoken</strong> tokenizer是LLaMA3采用的新一代分词器，相较于LLaMA2使用的SentencePiece BPE，tiktoken在以下方面有所改进：</p><ol><li><strong>词汇表扩展</strong>：词汇表从32k扩展至128k，覆盖更多语言和专业术语，减少了分词次数，提升了生成质量。</li><li><strong>编码效率</strong>：优化了编码算法，减少了分词时间，提高了处理速度。</li><li><strong>生成质量</strong>：通过更细粒度的词汇表示，提升了模型生成文本的连贯性和准确性。</li></ol>$$
\text{Tokenize}(w) = \text{BPE}(w) \quad \text{vs} \quad \text{Tokenize}(w) = \text{tiktoken\_BPE}(w)
$$<ul><li>其中，\( w \) 为输入词汇，tiktoken_BPE通过更大词汇表减少了分词次数。</li></ul><p><strong>优势</strong>：</p><ul><li><strong>减少分词次数</strong>：更大的词汇表使得更多词汇能作为单一token处理，减少了分词次数，提高了生成效率和质量。</li><li><strong>提升生成质量</strong>：更细粒度的词汇表示，使模型在生成文本时能够更准确地表达复杂语义。</li><li><strong>编码速度快</strong>：优化的编码算法提升了分词速度，适用于大规模模型的高效训练和推理。</li></ul><h4 id=轻量级模型>轻量级模型<a hidden class=anchor aria-hidden=true href=#轻量级模型>#</a></h4><p>为了适应边缘设备和移动设备的需求，LLaMA3推出了<strong>1B和3B参数量的轻量级模型</strong>，采用以下技术：</p><ol><li><strong>剪枝技术</strong>：通过系统性地移除网络中的冗余参数，减小模型规模，同时保持核心性能。</li><li><strong>知识蒸馏</strong>：让小模型从大模型中学习，提升其在特定任务上的表现。</li><li><strong>优化部署</strong>：针对移动设备的硬件架构进行优化，如针对Arm处理器的性能调优，确保模型在资源受限环境中的高效运行。</li></ol>$$
\text{Pruned\_Model} = \text{Prune}(\text{Original\_Model}, \text{Pruning\_Rate})
$$$$
\text{Distilled\_Model} = \text{Distill}(\text{Large\_Model}, \text{Small\_Model})
$$<ul><li>其中，Prune表示剪枝操作，Distill表示知识蒸馏过程。</li></ul><p><strong>优势</strong>：</p><ul><li><strong>适应资源受限设备</strong>：减小模型规模，使其适用于边缘设备和移动设备，推动了大语言模型的普及。</li><li><strong>保持性能</strong>：通过剪枝和知识蒸馏技术，保持了模型的核心性能和表现。</li><li><strong>高效运行</strong>：优化的模型结构和权重格式（如BFloat16）提升了计算效率，确保在移动设备上的高效运行。</li></ul><h4 id=训练方法>训练方法<a hidden class=anchor aria-hidden=true href=#训练方法>#</a></h4><p><strong>LLaMA3</strong>在训练数据和方法上进行了全面升级，采用了更大规模的数据和更先进的训练技术：</p><ol><li><p><strong>预训练阶段</strong>：</p><ul><li><strong>大规模数据扩展</strong>：训练数据量达到15万亿token，覆盖更多语言、专业领域和多模态数据，提升了模型的泛化能力和多语言支持。</li><li><strong>扩展法则（Scaling Laws）</strong>：<ul><li>根据Chinchilla扩展法则，优化模型的训练数据量和参数规模平衡，确保模型在关键任务上的最佳性能。</li><li>数学表达式：
$$
\text{Optimal Data} \propto \text{Model Size}^{4/3}
$$
这一公式指导了数据和模型规模的平衡，确保随着模型规模的增加，训练数据量也按比例增长，避免模型过拟合或欠拟合。</li></ul></li></ul></li><li><p><strong>并行训练策略</strong>：</p><ul><li><strong>数据并行</strong>：将训练数据分布到多个GPU上，提升数据处理速度。</li><li><strong>模型并行</strong>：将模型的不同部分分布到多个GPU上，支持更大规模的模型训练。</li><li><strong>流水并行</strong>：分阶段处理模型的不同部分，提高训练效率。</li></ul>$$
\text{Total Throughput} = \text{Data Parallelism} \times \text{Model Parallelism} \times \text{Pipeline Parallelism}
$$<ul><li>其中，总吞吐量（Total Throughput）是数据并行、模型并行和流水并行的乘积，显著提升了训练效率。</li></ul></li><li><p><strong>硬件优化</strong>：</p><ul><li><strong>高效利用GPU</strong>：在16K GPU上实现每GPU超过400 TFLOPS的计算利用率，通过定制的24K GPU集群进行训练，确保训练过程的高效性和稳定性。</li><li><strong>错误处理与存储优化</strong>：<ul><li><strong>自动错误检测与处理</strong>：确保训练过程的连续性和高效性。</li><li><strong>可扩展存储系统</strong>：减少检查点和回滚的开销，提高数据存储效率。</li></ul></li></ul></li><li><p><strong>微调阶段</strong>：</p><ul><li><strong>多轮对齐步骤</strong>：<ul><li><strong>监督微调（SFT）</strong>：使用高质量的标注数据进一步优化模型性能。</li><li><strong>拒绝采样（Rejection Sampling）</strong>：通过拒绝低质量内容，提升生成文本的质量。</li><li><strong>近端策略优化（Proximal Policy Optimization, PPO）和直接策略优化（Direct Policy Optimization, DPO）</strong>：结合两者的优势，优化模型的生成策略，使其更符合人类偏好。</li></ul></li></ul>$$
\mathcal{L}_{\text{RLHF}} = \mathbb{E}_{\theta \sim \pi_{\theta}} \left[ r(s, a) \right]
$$<ul><li>其中，\( \mathcal{L}_{\text{RLHF}} \)为RLHF的损失函数，\( \pi_{\theta} \)为策略分布，\( r(s, a) \)为奖励函数。</li></ul></li><li><p><strong>多模态训练</strong>：</p><ul><li><strong>视觉语言模型</strong>：结合图像和文本数据，提升模型在多模态任务中的表现。</li><li><strong>代码数据扩展</strong>：增加代码token数量，提升模型在编程任务中的表现。</li></ul></li><li><p><strong>模型安全与质量控制</strong>：</p><ul><li><strong>数据过滤pipeline</strong>：<ul><li><strong>启发式过滤器</strong>：基于规则的过滤，提高数据质量。</li><li><strong>NSFW过滤器</strong>：去除不适内容，确保数据的安全性。</li><li><strong>语义重复数据删除</strong>：使用语义分析技术，删除内容高度相似的数据。</li><li><strong>文本分类器</strong>：预测数据质量，进一步优化数据集。</li></ul></li></ul></li><li><p><strong>优化训练堆栈</strong>：</p><ul><li><strong>高级训练堆栈</strong>：自动检测和处理训练过程中的错误，提升硬件可靠性。</li><li><strong>性能调优</strong>：针对不同硬件平台进行优化，确保训练过程的高效性。</li></ul></li></ol><p><strong>LLaMA3</strong>通过这些先进的训练方法和优化策略，显著提升了模型的性能和适应性，成为开源大语言模型领域的领先者。</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p><strong>LLaMA</strong>系列模型从LLaMA1到LLaMA3，体现了大规模预训练语言模型的技术进化与产业影响。通过不断扩展训练数据量、优化模型架构和引入先进的训练方法，LLaMA系列在性能、多语言支持和多模态能力等方面取得了显著的提升。其开源策略不仅推动了全球AI社区的创新和发展，也为产业应用提供了强大的技术支持。</p><h2 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h2><ol><li><a href=https://arxiv.org/pdf/1606.08415.pdf>Hendrycks and Gimpel, 2016</a></li><li><a href=https://arxiv.org/pdf/2002.05202.pdf>GLU Variants Improve Transformer</a></li><li><a href=https://arxiv.org/abs/2302.13971>LLaMA: Open and Efficient Foundation Language Models</a></li><li><a href=https://arxiv.org/pdf/2307.09288>LLaMA2: Open Foundation and Fine-Tuned Chat Models</a></li><li><a href=https://github.com/meta-llama/llama/blob/main/llama/model.py>meta-llama repo</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/zh/tags/nlp/>NLP</a></li><li><a href=https://syhya.github.io/zh/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/zh/tags/pre-training/>Pre-Training</a></li><li><a href=https://syhya.github.io/zh/tags/post-training/>Post-Training</a></li><li><a href=https://syhya.github.io/zh/tags/llama/>Llama</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/zh/posts/2025-01-27-deepseek-r1/><span class=title>« 上一页</span><br><span>OpenAI o1复现进展：DeepSeek-R1</span>
</a><a class=next href=https://syhya.github.io/zh/posts/2025-01-16-group-query-attention/><span class=title>下一页 »</span><br><span>Transformer注意力机制：MHA、MQA与GQA的对比</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Llama系列（长期更新中） on x" href="https://x.com/intent/tweet/?text=Llama%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llama3%2f&amp;hashtags=AI%2cNLP%2cLLM%2cPre-training%2cPost-training%2cLlama"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Llama系列（长期更新中） on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llama3%2f&amp;title=Llama%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89&amp;summary=Llama%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llama3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Llama系列（长期更新中） on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llama3%2f&title=Llama%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Llama系列（长期更新中） on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llama3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Llama系列（长期更新中） on whatsapp" href="https://api.whatsapp.com/send?text=Llama%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llama3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Llama系列（长期更新中） on telegram" href="https://telegram.me/share/url?text=Llama%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llama3%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Llama系列（长期更新中） on ycombinator" href="https://news.ycombinator.com/submitlink?t=Llama%e7%b3%bb%e5%88%97%ef%bc%88%e9%95%bf%e6%9c%9f%e6%9b%b4%e6%96%b0%e4%b8%ad%ef%bc%89&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-01-22-llama3%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>