<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.147.1"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Yue Shui Blog</title>
<meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="ExampleSite description"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/json href=https://syhya.github.io/index.json><link rel=alternate hreflang=en href=https://syhya.github.io/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Yue Shui Blog"><meta property="og:description" content="ExampleSite description"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Yue Shui Blog"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Yue Shui Blog","url":"https://syhya.github.io/","description":"ExampleSite description","logo":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E","sameAs":["https://www.linkedin.com/in/yue-shui/","https://github.com/syhya","mailto:syhya925666582@gmail.com"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=ç®€ä½“ä¸­æ–‡ aria-label=ç®€ä½“ä¸­æ–‡>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>ðŸ‘‹ Welcome to Yue&rsquo;s blog</h1></header><div class=entry-content>Hi, this is Yue Shui, an LLM Algorithm Engineer at PwC. My work focuses on researching and applying LLMs in areas like finance, audit, and code generation. This blog serves as a space to document and share insights from my work and learning journey. The grammar mistakes in the posts might give you a hint about ChatGPT&rsquo;s involvement ðŸ˜‰â€”let me know if you spot any! My interests include model training, RAG and Agent. Recently, I&rsquo;ve been learning how to utilize RL to train reasoning models. Feel free to connect!</div><footer class=entry-footer><div class=social-icons><a href=https://www.linkedin.com/in/yue-shui/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a href=https://github.com/syhya target=_blank rel="noopener noreferrer me" title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=mailto:syhya925666582@gmail.com target=_blank rel="noopener noreferrer me" title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></footer></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Multimodal Large Language Models</h2></header><div class=entry-content><p>Humans interact with the world through multiple senses (vision, hearing, touch, etc.), with each sensory channel offering unique advantages in representing and communicating specific concepts. This multimodal interaction fosters our deep understanding of the world. One of the core goals in the field of artificial intelligence is to develop general-purpose assistants that can effectively follow multimodal instructions (such as visual and linguistic ones), enabling them to perform various real-world tasks like humans. In recent years, with the release of models like GPT-4o (OpenAI, 2024), Gemini 2.5 Pro (DeepMind, 2025), and o3/o4-mini (OpenAI, 2025), Multimodal Large Language Models (MLLMs) have made significant progress. They can not only understand information from multiple modalities like images, videos, and audio but also perform complex reasoning and generation.
...</p></div><footer class=entry-footer><span title='2025-05-04 12:00:00 +0800 +0800'>2025-05-04</span>&nbsp;Â·&nbsp;48 min&nbsp;Â·&nbsp;10208 words&nbsp;Â·&nbsp;Yue Shui</footer><a class=entry-link aria-label="post link to Multimodal Large Language Models" href=https://syhya.github.io/posts/2025-05-04-multimodal-llm/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>DeepSeek-V2 vs V3</h2></header><div class=entry-content><p>DeepSeek AI successively released DeepSeek-V2 (DeepSeek-AI, 2024) and DeepSeek-V3 (DeepSeek-AI, 2024), two powerful Mixture-of-Experts (MoE) language models that significantly optimize training costs and inference efficiency while maintaining state-of-the-art performance. DeepSeek-V2 has a total of 236B parameters, activating 21B per token, while DeepSeek-V3 further expands to 671B total parameters, activating 37B per token. Both support a 128K context length.
The core innovations of these two models lie in the adoption of Multi-head Latent Attention (MLA) and the DeepSeekMoE architecture (Dai et al., 2024). MLA drastically reduces GPU memory usage during inference by compressing the Key-Value (KV) cache into low-dimensional latent vectors, improving efficiency. DeepSeekMoE achieves stronger expert specialization capabilities and more economical training costs through fine-grained expert segmentation and shared expert isolation. Building upon V2, DeepSeek-V3 further introduces an Auxiliary-Loss-Free Load Balancing strategy (Wang et al., 2024) and the Multi-Token Prediction (MTP) (Gloeckle et al., 2024) training objective, further enhancing model performance and training efficiency.
...</p></div><footer class=entry-footer><span title='2025-04-18 12:00:00 +0800 +0800'>2025-04-18</span>&nbsp;Â·&nbsp;63 min&nbsp;Â·&nbsp;13242 words&nbsp;Â·&nbsp;Yue Shui</footer><a class=entry-link aria-label="post link to DeepSeek-V2 vs V3" href=https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>The LLaMA Herd</h2></header><div class=entry-content><p>LLaMA The LLaMA series of open-source models released by Meta AI has become one of the cornerstones of the large language model community, profoundly impacting the advancement of open research and applications. From the pioneering LLaMA released in early 2023, to the significantly improved LLaMA 2 later that year, to derivative models targeting specific domains (like code, safety), and the subsequent new generations LLaMA 3 and LLaMA 4 launched in 2024 and 2025 respectively, Meta has continuously committed to enhancing the performance of open-source models, gradually bringing them closer to state-of-the-art closed-source models. Below, we will introduce the key technical details of each major model in sequence.
...</p></div><footer class=entry-footer><span title='2025-04-06 12:00:00 +0800 +0800'>2025-04-06</span>&nbsp;Â·&nbsp;33 min&nbsp;Â·&nbsp;6862 words&nbsp;Â·&nbsp;Yue Shui</footer><a class=entry-link aria-label="post link to The LLaMA Herd" href=https://syhya.github.io/posts/2025-04-06-llama/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Large Language Model Agents</h2></header><div class=entry-content><p>Agents Since OpenAI released ChatGPT in October 2022, and with the subsequent emergence of projects such as AutoGPT and AgentGPT, LLM-related agents have gradually become a research hotspot and a promising direction for practical applications in AI in recent years. This article will introduce the basic concepts of agents, their core technologies, and the latest advances in their applications.
Large Language Model Agents Large Language Model Agents (LLM agents) utilize LLMs as the systemâ€™s brain, combined with modules such as planning, memory, and external tools, to achieve automated execution of complex tasks.
...</p></div><footer class=entry-footer><span title='2025-03-27 10:00:00 +0000 UTC'>2025-03-27</span>&nbsp;Â·&nbsp;32 min&nbsp;Â·&nbsp;6788 words&nbsp;Â·&nbsp;Yue Shui</footer><a class=entry-link aria-label="post link to Large Language Model Agents" href=https://syhya.github.io/posts/2025-03-27-llm-agent/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Parallelism and Memory Optimization Techniques for Training Large Models</h2></header><div class=entry-content><p>Background Recently, the number of parameters in large models has been continuously increasing, from the initial billions to todayâ€™s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models.
...</p></div><footer class=entry-footer><span title='2025-03-01 12:00:00 +0800 +0800'>2025-03-01</span>&nbsp;Â·&nbsp;61 min&nbsp;Â·&nbsp;12817 words&nbsp;Â·&nbsp;Yue Shui</footer><a class=entry-link aria-label="post link to Parallelism and Memory Optimization Techniques for Training Large Models" href=https://syhya.github.io/posts/2025-03-01-train-llm/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://syhya.github.io/page/2/>Next&nbsp;&nbsp;Â»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>