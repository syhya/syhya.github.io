<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>vLLM：高吞吐、有效内存的LLM服务引擎 | Yue Shui 博客</title><meta name=keywords content="PagedAttention,LLM Serving,Inference,KV Cache,Memory Optimization,LLMs,AI Infrastructure"><meta name=description content="随着大语言模型 (Large Language Models, LLMs) 参数不断增大，实际部署和提供这些模型的服务也面临挑战。vLLM 是一个开源库，旨在实现快速、便捷且经济高效的 LLM 推理和在线服务。其核心是利用 PagedAttention 算法高效地管理注意力机制中的键和值的缓存（KV Cache）。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2025-05-17-vllm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-05-17-vllm/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-05-17-vllm/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2025-05-17-vllm/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="vLLM：高吞吐、有效内存的LLM服务引擎"><meta property="og:description" content="随着大语言模型 (Large Language Models, LLMs) 参数不断增大，实际部署和提供这些模型的服务也面临挑战。vLLM 是一个开源库，旨在实现快速、便捷且经济高效的 LLM 推理和在线服务。其核心是利用 PagedAttention 算法高效地管理注意力机制中的键和值的缓存（KV Cache）。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-17T10:00:00+08:00"><meta property="article:modified_time" content="2025-06-29T21:43:56+08:00"><meta property="article:tag" content="PagedAttention"><meta property="article:tag" content="LLM Serving"><meta property="article:tag" content="Inference"><meta property="article:tag" content="KV Cache"><meta property="article:tag" content="Memory Optimization"><meta property="article:tag" content="LLMs"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="vLLM：高吞吐、有效内存的LLM服务引擎"><meta name=twitter:description content="随着大语言模型 (Large Language Models, LLMs) 参数不断增大，实际部署和提供这些模型的服务也面临挑战。vLLM 是一个开源库，旨在实现快速、便捷且经济高效的 LLM 推理和在线服务。其核心是利用 PagedAttention 算法高效地管理注意力机制中的键和值的缓存（KV Cache）。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"vLLM：高吞吐、有效内存的LLM服务引擎","item":"https://syhya.github.io/zh/posts/2025-05-17-vllm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"vLLM：高吞吐、有效内存的LLM服务引擎","name":"vLLM：高吞吐、有效内存的LLM服务引擎","description":"随着大语言模型 (Large Language Models, LLMs) 参数不断增大，实际部署和提供这些模型的服务也面临挑战。vLLM 是一个开源库，旨在实现快速、便捷且经济高效的 LLM 推理和在线服务。其核心是利用 PagedAttention 算法高效地管理注意力机制中的键和值的缓存（KV Cache）。\n","keywords":["PagedAttention","LLM Serving","Inference","KV Cache","Memory Optimization","LLMs","AI Infrastructure"],"articleBody":"随着大语言模型 (Large Language Models, LLMs) 参数不断增大，实际部署和提供这些模型的服务也面临挑战。vLLM 是一个开源库，旨在实现快速、便捷且经济高效的 LLM 推理和在线服务。其核心是利用 PagedAttention 算法高效地管理注意力机制中的键和值的缓存（KV Cache）。\n评价指标 为了评估 LLM 推理与服务引擎的性能，我们主要关注以下几个指标：\n首 token 生成时间 首 token 生成时间（Time To First Token, TTFT） 是指模型从接收到用户输入到生成第一个输出 token 所花费的时间。TTFT 越短，用户等待响应的时间就越少，这对于实时交互场景尤为重要；而在离线场景中，TTFT 的重要性相对较低。\n每个输出 token 的生成时间 每个输出 token 的生成时间（Time Per Output Token, TPOT） 指模型平均生成一个新 token 所需的时间，它直接决定了用户感知到的响应“速度”。为提升体验，实际应用中通常采用 Streaming 方式。例如，如果 TPOT 为 0.1 秒/token，意味着模型每秒可生成约 10 个 token，折合每分钟约 450 个单词，已超过多数人的阅读速度。\n总体延迟 总体延迟（Latency） 指模型为用户生成完整响应所需的总时间。总体延迟可由 TTFT 和 TPOT 计算得出，公式如下：\n$$ \\text{Latency} = \\text{TTFT} + \\text{TPOT} \\times (\\text{Number of Output Tokens}) $$吞吐量 吞吐量（Throughput） 衡量模型推理服务器单位时间能为所有用户请求生成的总 token 数量（包括输入与输出 token），体现了服务器的处理效率与并发能力。具体计算公式如下：\n$$ \\text{Throughput} = \\frac{\\text{Batch Size} \\times (\\text{Number of Input Tokens} + \\text{Number of Output Tokens})}{\\text{End-to-End Latency}} $$Token 间延迟 Token 间延迟（Inter Token Latency, ITL） 表示生成连续 token 时每两个 token 间的平均时间间隔。它体现了模型在生成首个 token 后，每个后续 token 的生成速度，计算公式为：\n$$ \\text{ITL} = \\frac{\\text{End-to-End Latency} - \\text{TTFT}}{\\text{Batch Size} \\times (\\text{Number of Output Tokens} - 1)} $$这些指标反映了推理引擎的响应速度、处理效率和并发能力，是评估和优化 LLM 推理性能的重要依据。\nvLLM V0 自 2023 年 6 月首次发布以来，配备 PagedAttention 的 vLLM 显著提升了 LLM 服务的性能标杆，相较于 HuggingFace Transformers (HF) 和 Text Generation Inference (TGI) 具有显著的吞吐量优势, 且无需修改任何模型架构。\nFig. 1. Throughput comparison (single output completion) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)\n单输出推理：图中显示了 vLLM 吞吐量比 HF 高 14x-24x，比 TGI 高 2.2x-2.5x。 Fig. 2. Throughput comparison (three parallel output completions) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)\n三路并行推理：图中显示了 vLLM 吞吐量比 HF 高 8.5x-15x，比 TGI 高 3.3x-3.5x。 Batching 传统的动态批处理 (Dynamic Batching) 会等待一批请求全部完成后再处理下一批，如果某些请求提前结束，会导致 GPU 空闲，资源利用率降低。\n而 vLLM 采用的 连续批处理 (Continuous Batching) 则允许在批次执行过程中动态插入新的请求序列，一旦某个序列完成，就可以立即用新的序列替换，从而显著提高 GPU 利用率和吞吐量。\nFig. 3. Dynamic Batching vs Continuous Batching. (Image source: NYC vLLM Meetup, 2025)\nDynamic Batching: 如图左侧所示，T1-T4 时刻，S₁-S₄ 四个序列并行处理。在 T5 时刻，S₁ 和 S₃ 提前完成，但由于 S₂ 和 S₄ 仍在运行，新的序列无法立即加入，导致 GPU 部分空闲。直到 T6 时刻 S₂ 结束、T7 时刻 S₄ 结束后，新的序列才能开始。\nContinuous Batching: 如图右侧所示，T1-T4 时刻与动态批处理类似。但在 T5 时刻，S₁ 和 S₃ 完成后，新的序列 S₅ 和 S₆ 可以立即加入并开始处理，而 S₂ 和 S₄ 继续运行。当 S₂ 在 T6 结束时，S₇ 可以即时加入。这种方式使得 GPU 几乎总是满负荷运行，极大提高了效率。\nKV 缓存 LLM 服务性能的主要瓶颈在于内存管理，在自回归解码过程中，LLM 为输入序列中的每个 token 生成注意力键和值张量，这些 KV 缓存必须保留在 GPU 内存中以生成后续的 token。KV 缓存具有以下特点：\n占用空间大: 对于 LLaMA-13B 模型，单个序列的 KV 缓存可能高达 1.7 GB。 动态性: KV 缓存的大小取决于序列长度，而序列长度是高度可变且不可预测的。 管理效率低下: 现有推理框架比如 FasterTransformer, Orca (Yu et al. 2022) 通常将 KV 缓存存储在连续的内存块中。为了应对动态性，它们需要预先分配足够容纳最大可能序列长度的内存块。这导致了严重的内存浪费： 内部碎片: 预留空间远大于实际需要。 外部碎片: 不同大小的预留块导致内存空间难以有效利用。 过度预留: 为未来 token 预留的空间在当前无法被其他请求利用。 下图展示了现有推理系统中 KV 缓存管理导致的内存浪费类型：\nFig. 4. KV cache memory management in existing systems, showing reserved waste, internal fragmentation, and external fragmentation. (Image source: Kwon et al. 2023)\n以下左图展示了在 NVIDIA A100 GPU 上运行 13B 参数 LLM 的内存分布：约 65% 的内存用于静态模型权重（灰色），约 30% 的内存按需动态分配给 KV 缓存（红色），用于存储前序 token 的注意力上下文，而少量内存（黄色）则用于临时激活计算；右图则表明 vLLM 通过平滑 KV 缓存内存使用的快速增长，有效缓解了内存瓶颈，从而大幅提升了批量请求处理能力和整体服务吞吐量。\nFig. 5. Left: Memory layout for a 13B LLM on an NVIDIA A100—gray is persistent parameters, red is per-request KV cache, and yellow is temporary activation memory. Right: vLLM limits rapid KV cache growth, improving throughput. (Image source: Kwon et al. 2023)\nPagedAttention PagedAttention (Kwon et al. 2023) 的灵感来源于操作系统中的虚拟内存 (Virtual Memory) 和 分页 (Paging)。它允许将逻辑上连续的 KV Cache 存储在物理上非连续的显存空间中。\n具体来说，PagedAttention 将每个序列的 KV Cache 分割成固定大小的 块 (Blocks)。每个块包含固定数量 token 的 Key 和 Value 向量。系统维护一个 块表 (Block Table)，用于记录每个序列的逻辑块到物理块的映射关系。\nFig. 6. Illustration of the PagedAttention algorithm, where KV vectors are stored in non-contiguous blocks. (Image source: Kwon et al. 2023)\nPagedAttention 的核心思想借鉴了操作系统的虚拟内存和分页机制来管理 KV 缓存。\n具体来说，PagedAttention 的设计理念可以总结为以下几点：\n类比关系:\nKV 缓存的 块 (Blocks) 类比于操作系统内存管理的 页 (Pages)。 Token 类比于 字节 (Bytes)。 序列 (Sequences) 类比于 进程 (Processes)。 映射机制: PagedAttention 使用 块表 来维护从序列的连续 逻辑块 到 物理块 的映射。这些物理块在内存中可以是非连续的，就像操作系统的页表将虚拟地址映射到物理页帧一样。\n按需分配: 最关键的一点是，物理块 不是预先为整个序列最大长度分配好的，而是在需要存储新的 Key-Value（即生成新 Token）时 按需分配。\n这种按需、非连续的内存管理方式，使得 PagedAttention 能更有效地利用内存，避免了因预分配大量连续空间而造成的浪费和内部碎片，从而提高了 GPU 内存的利用率。\n数学上，PagedAttention 将注意力计算转化为块级计算。设块大小为 $B$，第 $j$ 个 Key 块为 $K_{j}=\\left(k_{(j-1) B+1}, \\ldots, k_{j B}\\right)$，Value 块为 $V_{j}=\\left(v_{(j-1) B+1}, \\ldots, v_{j B}\\right)$。对于查询向量 $q_i$，注意力计算变为：\n\\[ A_{i j}=\\frac{\\exp \\left(q_{i}^{\\top} K_{j} / \\sqrt{d}\\right)}{\\sum_{t=1}^{\\lceil i / B\\rceil} \\exp \\left(q_{i}^{\\top} K_{t} \\mathbf{1} / \\sqrt{d}\\right)}, \\quad o_{i}=\\sum_{j=1}^{\\lceil i / B\\rceil} V_{j} A_{i j}^{\\top} \\]其中 $A_{i j}=\\left(a_{i,(j-1) B+1}, \\ldots, a_{i, j B}\\right)$ 是第 $i$ 个查询对第 $j$ 个 KV 块的注意力得分行向量。在计算过程中，PagedAttention 内核会高效地识别并获取所需的物理块。\nKV 缓存管理器 vLLM 的内存管理器借鉴了操作系统的虚拟内存机制：\n逻辑块与物理块: 每个请求的 KV 缓存被表示为一系列逻辑块。GPU 工作节点上的块引擎分配物理内存并将其划分为物理块。 块表: 维护每个请求的逻辑块到物理块的映射。每个条目记录物理块地址和块内已填充的 token 数量。 动态分配: 物理块按需分配，无需预先保留最大长度的空间，从而消除了大部分内存浪费。 Fig. 7. Block table translation in vLLM. Logical blocks are mapped to non-contiguous physical blocks. (Image source: Kwon et al. 2023)\n结合上图7的例子：\n预填充阶段: 输入 prompt 有 7 个 token。假设块大小为 4。vLLM 分配 2 个物理块（例如物理块 7 和 1）并更新块表，将逻辑块 0 映射到物理块 7，逻辑块 1 映射到物理块 1。计算 prompt 的 KV 缓存并填充到这两个物理块中。逻辑块 0 填满 4 个 token，逻辑块 1 填充 3 个 token，剩余 1 个 slot 备用。 解码阶段 : 第 1 步: 使用 PagedAttention 计算下一个 token。由于逻辑块 1 还有空位，新的 KV 缓存直接存入物理块 1，并更新块表中逻辑块 1 的填充计数。 第 2 步: 逻辑块 1 已满。vLLM 分配一个新的物理块（例如物理块 3），更新块表将新的逻辑块 2 映射到物理块 3，并将新生成的 KV 缓存存入物理块 3。 这种按需分配的方式将内存浪费限制在每个序列的最后一个块内，实现了接近最优的内存利用率（浪费低于 4%），从而可以批处理更多请求，提高吞吐量。\n图8中展示了 vLLM 如何管理两个序列的内存空间。两个序列的逻辑块被映射到 GPU 工作节点（GPU worker）上由区块引擎预留的不同物理块中。这意味着，即使在逻辑层面相邻的块在物理 GPU 内存中也无需连续，从而两个序列可以有效地共享和利用物理内存空间。\nFig. 8. Storing the KV cache of two requests concurrently in vLLM using paged memory. (Image source: Kwon et al. 2023)\n内存共享 PagedAttention 的另一个关键优势是高效的内存共享，尤其适用于复杂的解码策略。\n并行采样 当一个请求需要从同一个 prompt 生成多个输出序列时（例如代码补全建议），prompt 部分的 KV 缓存可以共享。\nFig. 9. Parallel sampling example. Logical blocks for the shared prompt map to the same physical blocks. (Image source: Kwon et al. 2023)\nvLLM 通过块表实现共享：\n共享映射: 不同序列的逻辑块可以映射到同一个物理块。 引用计数: 每个物理块维护一个引用计数。 写时复制 (Copy-on-Write, CoW): 当一个共享块（引用计数 \u003e 1）需要被写入时，vLLM 会分配一个新物理块，复制原块内容，更新写入序列的块表映射，并将原物理块的引用计数减 1。后续对该物理块的写入（当引用计数为 1 时）则直接进行。 这种机制显著减少了并行采样 (Parallel Sampling) 的内存开销，论文实验显示可节省高达 55% 的内存。\n束搜索 束搜索 (Beam Search) 在解码过程中，不同的候选序列（beam）不仅共享 prompt 部分，还可能共享后续生成的 token 的 KV 缓存，且共享模式是动态变化的。\nFig. 10. Beam search example ($k=4$). Blocks are dynamically shared and freed based on candidate survival. (Image source: Kwon et al. 2023)\nvLLM 通过引用计数和 CoW 机制，高效地管理这种动态共享，避免了传统实现中频繁且昂贵的内存拷贝操作。大部分块可以共享，只有当新生成的 token 落入旧的共享块时才需要 CoW（仅拷贝一个块）。\n共享前缀 对于许多 prompt 共享相同前缀（如系统指令、few-shot 示例）的应用场景，vLLM 可以预先计算并缓存这些 共享前缀 (Shared Prefix) 的 KV 缓存到一组物理块中。\nFig. 11. Shared prompt example for machine translation using few-shot examples. (Image source: Kwon et al. 2023)\n当处理包含该前缀的请求时，只需将其逻辑块映射到缓存的物理块（最后一个块标记为 CoW），从而避免了对前缀部分的重复计算。\n调度与抢占 vLLM 采用 FCFS 调度策略。当 GPU 内存不足以容纳新生成的 KV 缓存时，需要进行抢占：\n抢占单位: 以序列组 为单位进行抢占（例如，一个 beam search 请求的所有候选序列）。确保最早到达的请求优先服务，最晚到达的请求优先被抢占。 恢复机制: 换出: 将被抢占序列的 KV 块拷贝到 CPU 内存。当资源可用时再换回 GPU。适用于 PCIe 带宽较高且块较大的情况。 重计算: 丢弃被抢占序列的 KV 缓存。当资源可用时，将原始 prompt 和已生成的 token 拼接起来，通过一次高效的 prompt phase 重新计算 KV 缓存。适用于 PCIe 带宽较低或块较小的情况。 分布式执行 vLLM 支持 Megatron-LM 风格的张量模型并行。\nFig. 12. vLLM system overview showing centralized scheduler and distributed workers. (Image source: Kwon et al. 2023)\n集中式调度器: 包含 KV 缓存管理器，维护全局的逻辑块到物理块的映射。 共享映射: 所有 GPU worker 共享块表。 本地存储: 每个 worker 只存储其负责的注意力头对应的 KV 缓存部分。 执行流程: 调度器广播输入 token ID 和块表给所有 worker -\u003e worker 执行模型计算（包括 PagedAttention）-\u003e worker 间通过 All-Reduce 同步中间结果 -\u003e worker 将采样结果返回给调度器。内存管理信息在每步开始时一次性广播，无需 worker 间同步。 内核优化 为了高效实现 PagedAttention，vLLM 开发了定制 CUDA 核：\n融合 Reshape 和块写入: 将新 KV 缓存分块、重塑布局、按块表写入融合为单个核。 融合块读取和注意力计算: 修改 FasterTransformer 的注意力核，使其能根据块表读取非连续块并即时计算注意力，优化内存访问模式。 融合块拷贝: 将 CoW 触发的多个小块拷贝操作批量化到单个核中执行。 vLLM V1 2025 年 1 月，vLLM 团队发布了 vLLM V1 的 alpha 版本，这是对其核心架构的一次重大升级。基于过去一年半的开发经验，V1 版本重新审视了关键设计决策，整合了各种特性，并简化了代码库。\n基于 vLLM V0 的成功和经验教训，vLLM V1 对核心架构进行了重大升级，旨在提供更简洁、模块化、易于扩展且性能更高的代码库。\nV1 的动机与目标 V0 的挑战: 随着功能和硬件支持的扩展，V0 的代码复杂度增加，特性难以有效组合，技术债务累积。 V1 的目标: 简洁、模块化、易于修改的代码库。 接近零 CPU 开销的高性能。 将关键优化统一到架构中。 默认启用优化，实现零配置。 优化的执行循环与 API 服务器 Fig. 13. vLLM V1’s multiprocessing architecture with an isolated EngineCore. (Image source: vLLM Blog, 2025)\n随着 GPU 计算速度加快（例如 H100 上 Llama-8B 推理时间仅约 5ms），CPU 开销（API 服务、调度、输入准备、解码、流式响应）成为瓶颈。V1 采用了多进程架构：\n隔离的 EngineCore: 将调度器和模型执行器隔离在核心引擎循环中。 CPU 任务卸载: 将 Tokenization、多模态输入处理、Detokenization、流式传输等 CPU 密集型任务移至独立进程，与 EngineCore 并行执行，最大化模型吞吐量。 简洁灵活的调度器 Fig. 14. vLLM V1’s scheduler treats prompt and generated tokens uniformly, enabling features like chunked prefill. (Image source: vLLM Blog, 2025)\n统一处理: 不再区分 “prefill” 和 “decode” 阶段，统一处理用户输入 token 和模型生成 token。 简单表示: 调度决策用字典表示，如 {request_id: num_tokens}，指定每步为每个请求处理多少 token。 通用性: 这种表示足以支持块状预填充 (Chunked Prefills)、前缀缓存 (Prefix Caching)、投机解码 (Speculative Decoding) 等特性。例如，块状预填充只需在固定 token 预算下动态分配各请求的处理数量。 零开销前缀缓存 Fig. 15. Performance comparison of prefix caching in vLLM V0 and V1. V1 achieves near-zero overhead even at 0% hit rate. (Image source: vLLM Blog, 2025)\nV1 优化了前缀缓存（基于哈希匹配和 LRU 驱逐）的实现：\n优化数据结构: 实现常数时间缓存驱逐。 减少 Python 对象开销: 最小化对象创建。 结果: 即使缓存命中率为 0%，性能下降也小于 1%。而在高命中率时，性能提升数倍。因此，V1 默认启用前缀缓存。 清晰的张量并行推理架构 Fig. 16. vLLM V1’s symmetric tensor-parallel architecture using diff-based updates. (Image source: vLLM Blog, 2025)\nV1 解决了 V0 中调度器和 Worker 0 耦合导致的非对称架构问题：\nWorker 端状态缓存: 请求状态缓存在 Worker 端。 增量更新: 每步只传输状态的增量变化 (diffs)，极大减少了进程间通信。 对称架构: 调度器和 Worker 0 可以运行在不同进程中，架构更清晰、对称。 抽象分布式逻辑: Worker 在单 GPU 和多 GPU 设置下行为一致。 高效的输入准备 Fig. 17. vLLM V1 uses Persistent Batch to cache input tensors and apply diffs. (Image source: vLLM Blog, 2025)\nV0 每步重新创建模型输入张量和元数据，CPU 开销大。V1 采用 Persistent Batch 技术：\n缓存输入张量: 缓存输入张量。 应用 Diffs: 每步只应用增量变化。 Numpy 优化: 大量使用 Numpy 操作替代 Python 原生操作，减少更新张量的 CPU 开销。 综合优化 torch.compile 与分段 CUDA 图\n集成 torch.compile: V1 充分利用 vLLM 的 torch.compile 集成功能，自动优化模型，支持多种模型的高效运行，显著减少了手动编写 CUDA 核的需求。 分段 CUDA 图 (Piecewise CUDA Graphs): 通过引入分段 CUDA 图，成功克服了原生 CUDA 图的局限性，提高了模型的灵活性和性能。 增强的多模态 LLM 支持\nV1 针对多模态大语言模型 (MLLMs) 推出多项关键改进： 优化预处理: 将图像解码、裁剪、转换等 CPU 密集型的预处理任务移至非阻塞的独立进程，防止阻塞 GPU 工作。同时引入预处理缓存，以便缓存已处理的输入，供之后的请求复用，尤其适用于相同的多模态输入。 多模态前缀缓存: 除了使用 token ID 的哈希，V1 还引入图像哈希来标识包含图像输入的 KV 缓存。此改进在包含图像输入的多轮对话场景中尤其有利。 编码器缓存: 针对需要视觉编码器输出的应用，V1 临时缓存视觉嵌入，允许调度器将文本输入分块处理，避免在每一步都重新计算视觉嵌入，从而支持 MLLM 的块状填充调度。 FlashAttention 3 集成\n由于 V1 的高度动态性（如在同一批处理内结合预填充和解码），需要一种灵活且高性能的注意力核。FlashAttention 3 完美符合这一需求，提供了强大的功能支持，同时在各种使用场景中保持优异的性能表现。 性能对比 得益于架构改进和 CPU 开销的大幅降低，V1 相比 V0（未开启多步调度）实现了高达 1.7 倍的吞吐量提升，同时在多模态模型上的性能提升显著。\nFig. 18. Performance comparison between vLLM V0 and V1 on Llama 3.1 8B \u0026 Llama 3.3 70B (1xH100). (Image source: vLLM Blog, 2025)\nFig. 19. Performance comparison between vLLM V0 and V1 on Qwen2-VL 7B (1xH100). (Image source: vLLM Blog, 2025)\n表格对比:\n特性 vLLM V0 vLLM V1 改进点 核心技术 PagedAttention PagedAttention + 全面架构重构 保持 PagedAttention 优势，优化整体架构 内存效率 极高 (浪费 \u003c 4%) 极高 (浪费 \u003c 4%) 维持高内存效率 内存共享 支持 (CoW) 支持 (CoW) 维持高效共享 CPU 开销 相对较高，尤其在复杂场景或低命中率前缀缓存时 显著降低，接近零开销 多进程、Persistent Batch、优化数据结构等 执行循环 单进程，API 服务器与引擎耦合较紧 多进程，API 服务器与 EngineCore 解耦，高度并行 提升 CPU/GPU 并行度，减少阻塞 调度器 区分 Prefill/Decode 统一处理 Token，字典式调度表示 更简洁、灵活，易于支持高级特性 前缀缓存 默认禁用 (低命中率时有开销) 默认启用 (零开销设计) 优化后无惧低命中率，默认开启提升易用性 张量并行 不对称架构 (Scheduler+Worker0 同进程) 对称架构 (Scheduler 与 Worker 分离) 架构更清晰，IPC 开销通过状态缓存和 Diffs 传输控制 多模态支持 基本支持 增强支持 (非阻塞预处理、图像前缀缓存、编码器缓存等) 提升 VLM 性能和易用性 编译器集成 有限 集成 torch.compile 自动化模型优化，减少手写 Kernel Attention Kernel 定制 Kernel (基于 FasterTransformer) 集成 FlashAttention 3 拥抱业界标准，获得更好的性能和特性支持 性能 (vs V0) 基线 吞吐量提升高达 1.7x (文本), 多模态模型提升更显著 全面优化 CPU 开销带来的提升 代码复杂度 随功能增加而提高 更简洁、模块化 降低维护成本，方便社区贡献和二次开发 其他推理框架 LightLLM：一个基于 Python 的轻量级推理与服务框架，以轻量级设计、可扩展性和高速性能著称，汲取了 vLLM 等其他开源项目的优势。 LMDeploy：用于压缩、部署和服务 LLM 的工具包，内置 TurboMind 推理引擎，强调高请求吞吐量和高效量化。 SGLang：通过前端语言与后端执行引擎协同设计，高效执行复杂（尤其涉及结构化生成）的 LLM 程序的框架。 TGI：Hugging Face 的生产级 LLM 服务方案，广泛应用并支持多种硬件后端，借助 vLLM 的 PagedAttention 内核，提供高并发、低延迟的推理服务。 TensorRT-LLM：NVIDIA 推出的开源库，用于在 NVIDIA GPU 上优化并加速 LLM 推理，利用 TensorRT 的提前编译和深度硬件优化能力。 总结 vLLM 通过其核心技术 PagedAttention，极大地缓解了 LLM 服务中 KV 缓存管理带来的内存瓶颈，显著提高了内存利用率和吞吐量。PagedAttention 借鉴操作系统分页机制，实现了 KV 缓存的非连续存储、动态分配和高效共享（支持并行采样、束搜索、共享前缀等）。\nvLLM V1 在 V0 的基础上，对核心架构进行了全面重构和优化，通过多进程架构、灵活调度器、零开销前缀缓存、对称张量并行架构、高效输入准备、torch.compile 集成、增强 MLLMs 支持以及 FlashAttention 3 集成等一系列改进，进一步降低了 CPU 开销，提升了系统整体性能、灵活性和可扩展性，为未来快速迭代新功能奠定了坚实基础。\n参考文献 [1] Kwon, Woosuk, et al. “Efficient memory management for large language model serving with pagedattention.” Proceedings of the 29th Symposium on Operating Systems Principles. 2023.\n[2] vLLM Team. “vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention.” vLLM Blog, June 20, 2023.\n[3] vLLM Team. “vLLM V1: A Major Upgrade to vLLM’s Core Architecture.” vLLM Blog, Jan 27, 2025.\n[4] NVIDIA. “FasterTransformer.” GitHub Repository, 2023.\n[5] Yu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. “Orca: A Distributed Serving System for Transformer-Based Generative Models.” In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 2022.\n[6] OpenAI. “API Reference - Streaming.” OpenAI Platform Documentation, 2025.\n[7] Wolf, Thomas, et al. “Transformers: State-of-the-Art Natural Language Processing.” In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.\n[8] Hugging Face. “Text Generation Inference.” GitHub Repository, 2025.\n[9] Shoeybi, Mohammad, et al. “Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.” arXiv preprint arXiv:1909.08053 (2019).\n[10] InternLM Team. “LMDeploy.” GitHub Repository, 2025.\n[12] Shah, Jay, et al. “Flashattention-3: Fast and accurate attention with asynchrony and low-precision.” Advances in Neural Information Processing Systems 37 (2024): 68658-68685.\n[13] ModelTC. “LightLLM.” GitHub Repository, 2025.\n[14] Zheng, Lianmin, et al. “Sglang: Efficient execution of structured language model programs.” Advances in Neural Information Processing Systems 37 (2024): 62557-62583.\n[15] NVIDIA. “TensorRT-LLM.” GitHub Repository, 2025.\n[16] vLLM Team. “NYC vLLM Meetup Presentation.” Google Slides, 2025.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (May 2025). vLLM：高吞吐、有效内存的LLM服务引擎. https://syhya.github.io/zh/posts/2025-05-17-vllm\nOr\n@article{syhya2025vllm, title = \"vLLM：高吞吐、有效内存的LLM服务引擎\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"May\", url = \"https://syhya.github.io/zh/posts/2025-05-17-vllm\" } ","wordCount":"7455","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2025-05-17T10:00:00+08:00","dateModified":"2025-06-29T21:43:56+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2025-05-17-vllm/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">vLLM：高吞吐、有效内存的LLM服务引擎</h1><div class=post-meta><span title='2025-05-17 10:00:00 +0800 +0800'>2025-05-17</span>&nbsp;·&nbsp;15 分钟&nbsp;·&nbsp;7455 字&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;翻译:<ul class=i18n_list><li><a href=https://syhya.github.io/posts/2025-05-17-vllm/>En</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#评价指标>评价指标</a><ul><li><a href=#首-token-生成时间>首 token 生成时间</a></li><li><a href=#每个输出-token-的生成时间>每个输出 token 的生成时间</a></li><li><a href=#总体延迟>总体延迟</a></li><li><a href=#吞吐量>吞吐量</a></li><li><a href=#token-间延迟>Token 间延迟</a></li></ul></li><li><a href=#vllm-v0>vLLM V0</a><ul><li><a href=#batching>Batching</a></li><li><a href=#kv-缓存>KV 缓存</a></li><li><a href=#pagedattention>PagedAttention</a></li><li><a href=#kv-缓存管理器>KV 缓存管理器</a></li><li><a href=#内存共享>内存共享</a><ul><li><a href=#并行采样>并行采样</a></li><li><a href=#束搜索>束搜索</a></li><li><a href=#共享前缀>共享前缀</a></li></ul></li><li><a href=#调度与抢占>调度与抢占</a></li><li><a href=#分布式执行>分布式执行</a></li><li><a href=#内核优化>内核优化</a></li></ul></li><li><a href=#vllm-v1>vLLM V1</a><ul><li><a href=#v1-的动机与目标>V1 的动机与目标</a></li><li><a href=#优化的执行循环与-api-服务器>优化的执行循环与 API 服务器</a></li><li><a href=#简洁灵活的调度器>简洁灵活的调度器</a></li><li><a href=#零开销前缀缓存>零开销前缀缓存</a></li><li><a href=#清晰的张量并行推理架构>清晰的张量并行推理架构</a></li><li><a href=#高效的输入准备>高效的输入准备</a></li><li><a href=#综合优化>综合优化</a></li><li><a href=#性能对比>性能对比</a></li></ul></li><li><a href=#其他推理框架>其他推理框架</a></li><li><a href=#总结>总结</a></li><li><a href=#参考文献>参考文献</a></li><li><a href=#引用>引用</a></li></ul></nav></div></details></div><div class=post-content><p>随着大语言模型 (Large Language Models, LLMs) 参数不断增大，实际部署和提供这些模型的服务也面临挑战。<a href=https://github.com/vllm-project/vllm>vLLM</a> 是一个开源库，旨在实现快速、便捷且经济高效的 LLM 推理和在线服务。其核心是利用 <strong>PagedAttention</strong> 算法高效地管理注意力机制中的键和值的缓存（KV Cache）。</p><h2 id=评价指标>评价指标<a hidden class=anchor aria-hidden=true href=#评价指标>#</a></h2><p>为了评估 LLM 推理与服务引擎的性能，我们主要关注以下几个指标：</p><h3 id=首-token-生成时间>首 token 生成时间<a hidden class=anchor aria-hidden=true href=#首-token-生成时间>#</a></h3><p><strong>首 token 生成时间（Time To First Token, TTFT）</strong> 是指模型从接收到用户输入到生成第一个输出 token 所花费的时间。TTFT 越短，用户等待响应的时间就越少，这对于实时交互场景尤为重要；而在离线场景中，TTFT 的重要性相对较低。</p><h3 id=每个输出-token-的生成时间>每个输出 token 的生成时间<a hidden class=anchor aria-hidden=true href=#每个输出-token-的生成时间>#</a></h3><p><strong>每个输出 token 的生成时间（Time Per Output Token, TPOT）</strong> 指模型平均生成一个新 token 所需的时间，它直接决定了用户感知到的响应“速度”。为提升体验，实际应用中通常采用 <a href="https://platform.openai.com/docs/guides/streaming-responses?api-mode=responses">Streaming</a> 方式。例如，如果 TPOT 为 0.1 秒/token，意味着模型每秒可生成约 10 个 token，折合每分钟约 450 个单词，已超过多数人的阅读速度。</p><h3 id=总体延迟>总体延迟<a hidden class=anchor aria-hidden=true href=#总体延迟>#</a></h3><p><strong>总体延迟（Latency）</strong> 指模型为用户生成完整响应所需的总时间。总体延迟可由 TTFT 和 TPOT 计算得出，公式如下：</p>$$
\text{Latency} = \text{TTFT} + \text{TPOT} \times (\text{Number of Output Tokens})
$$<h3 id=吞吐量>吞吐量<a hidden class=anchor aria-hidden=true href=#吞吐量>#</a></h3><p><strong>吞吐量（Throughput）</strong> 衡量模型推理服务器单位时间能为所有用户请求生成的总 token 数量（包括输入与输出 token），体现了服务器的处理效率与并发能力。具体计算公式如下：</p>$$
\text{Throughput} = \frac{\text{Batch Size} \times (\text{Number of Input Tokens} + \text{Number of Output Tokens})}{\text{End-to-End Latency}}
$$<h3 id=token-间延迟>Token 间延迟<a hidden class=anchor aria-hidden=true href=#token-间延迟>#</a></h3><p><strong>Token 间延迟（Inter Token Latency, ITL）</strong> 表示生成连续 token 时每两个 token 间的平均时间间隔。它体现了模型在生成首个 token 后，每个后续 token 的生成速度，计算公式为：</p>$$
\text{ITL} = \frac{\text{End-to-End Latency} - \text{TTFT}}{\text{Batch Size} \times (\text{Number of Output Tokens} - 1)}
$$<p>这些指标反映了推理引擎的响应速度、处理效率和并发能力，是评估和优化 LLM 推理性能的重要依据。</p><h2 id=vllm-v0>vLLM V0<a hidden class=anchor aria-hidden=true href=#vllm-v0>#</a></h2><p>自 2023 年 6 月首次发布以来，配备 PagedAttention 的 vLLM 显著提升了 LLM 服务的性能标杆，相较于 <a href=https://huggingface.co/docs/transformers/main_classes/text_generation>HuggingFace Transformers (HF)</a> 和 <a href=https://github.com/huggingface/text-generation-inference>Text Generation Inference (TGI)</a> 具有显著的吞吐量优势, 且无需修改任何模型架构。</p><figure class=align-center><img loading=lazy src=vllm_v0_throughput1.png#center alt="Fig. 1. Throughput comparison (single output completion) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)" width=80%><figcaption><p>Fig. 1. Throughput comparison (single output completion) on LLaMA models. vLLM vs. HF and TGI. (Image source: <a href=https://vllm.ai/blog/2023/06/20/vllm.html>vLLM Blog, 2023</a>)</p></figcaption></figure><ul><li>单输出推理：图中显示了 vLLM 吞吐量比 HF 高 14x-24x，比 TGI 高 2.2x-2.5x。</li></ul><figure class=align-center><img loading=lazy src=vllm_v0_throughput2.png#center alt="Fig. 2. Throughput comparison (three parallel output completions) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)" width=80%><figcaption><p>Fig. 2. Throughput comparison (three parallel output completions) on LLaMA models. vLLM vs. HF and TGI. (Image source: <a href=https://vllm.ai/blog/2023/06/20/vllm.html>vLLM Blog, 2023</a>)</p></figcaption></figure><ul><li>三路并行推理：图中显示了 vLLM 吞吐量比 HF 高 8.5x-15x，比 TGI 高 3.3x-3.5x。</li></ul><h3 id=batching>Batching<a hidden class=anchor aria-hidden=true href=#batching>#</a></h3><p>传统的<strong>动态批处理 (Dynamic Batching)</strong> 会<strong>等待一批请求全部完成后再处理下一批</strong>，如果某些请求提前结束，会导致 GPU 空闲，资源利用率降低。</p><p>而 vLLM 采用的 <strong>连续批处理 (Continuous Batching)</strong> 则允许在批次执行过程中动态插入新的请求序列，<strong>一旦某个序列完成，就可以立即用新的序列替换</strong>，从而显著提高 GPU 利用率和吞吐量。</p><figure class=align-center><img loading=lazy src=batching.png#center alt="Fig. 3. Dynamic Batching vs Continuous Batching. (Image source: NYC vLLM Meetup, 2025)" width=100%><figcaption><p>Fig. 3. Dynamic Batching vs Continuous Batching. (Image source: <a href="https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?slide=id.g31441846c39_0_0#slide=id.g31441846c39_0_0">NYC vLLM Meetup, 2025</a>)</p></figcaption></figure><ul><li><p><strong>Dynamic Batching</strong>: 如图左侧所示，T1-T4 时刻，S₁-S₄ 四个序列并行处理。在 T5 时刻，S₁ 和 S₃ 提前完成，但由于 S₂ 和 S₄ 仍在运行，新的序列无法立即加入，导致 GPU 部分空闲。直到 T6 时刻 S₂ 结束、T7 时刻 S₄ 结束后，新的序列才能开始。</p></li><li><p><strong>Continuous Batching</strong>: 如图右侧所示，T1-T4 时刻与动态批处理类似。但在 T5 时刻，S₁ 和 S₃ 完成后，新的序列 S₅ 和 S₆ 可以立即加入并开始处理，而 S₂ 和 S₄ 继续运行。当 S₂ 在 T6 结束时，S₇ 可以即时加入。这种方式使得 GPU 几乎总是满负荷运行，极大提高了效率。</p></li></ul><h3 id=kv-缓存>KV 缓存<a hidden class=anchor aria-hidden=true href=#kv-缓存>#</a></h3><p>LLM 服务性能的主要瓶颈在于内存管理，在自回归解码过程中，LLM 为输入序列中的每个 token 生成注意力键和值张量，这些 KV 缓存必须保留在 GPU 内存中以生成后续的 token。KV 缓存具有以下特点：</p><ol><li><strong>占用空间大:</strong> 对于 LLaMA-13B 模型，单个序列的 KV 缓存可能高达 1.7 GB。</li><li><strong>动态性:</strong> KV 缓存的大小取决于序列长度，而序列长度是高度可变且不可预测的。</li><li><strong>管理效率低下:</strong> 现有推理框架比如 <a href="https://github.com/NVIDIA/FasterTransformer?tab=readme-ov-file">FasterTransformer</a>, Orca (<a href=https://www.usenix.org/system/files/osdi22-yu.pdf>Yu et al. 2022</a>) 通常将 KV 缓存存储在连续的内存块中。为了应对动态性，它们需要预先分配足够容纳最大可能序列长度的内存块。这导致了严重的内存浪费：<ul><li><strong>内部碎片:</strong> 预留空间远大于实际需要。</li><li><strong>外部碎片:</strong> 不同大小的预留块导致内存空间难以有效利用。</li><li><strong>过度预留:</strong> 为未来 token 预留的空间在当前无法被其他请求利用。</li></ul></li></ol><p>下图展示了现有推理系统中 KV 缓存管理导致的内存浪费类型：</p><figure class=align-center><img loading=lazy src=kv_cache_existing_system.png#center alt="Fig. 4. KV cache memory management in existing systems, showing reserved waste, internal fragmentation, and external fragmentation. (Image source: Kwon et al. 2023)" width=100%><figcaption><p>Fig. 4. KV cache memory management in existing systems, showing reserved waste, internal fragmentation, and external fragmentation. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>以下左图展示了在 NVIDIA A100 GPU 上运行 13B 参数 LLM 的内存分布：<strong>约 65% 的内存用于静态模型权重（灰色），约 30% 的内存按需动态分配给 KV 缓存（红色）</strong>，用于存储前序 token 的注意力上下文，而少量内存（黄色）则用于临时激活计算；右图则表明 vLLM 通过平滑 KV 缓存内存使用的快速增长，有效缓解了内存瓶颈，从而大幅提升了批量请求处理能力和整体服务吞吐量。</p><figure class=align-center><img loading=lazy src=memory_layout.png#center alt="Fig. 5. Left: Memory layout for a 13B LLM on an NVIDIA A100—gray is persistent parameters, red is per-request KV cache, and yellow is temporary activation memory. Right: vLLM limits rapid KV cache growth, improving throughput. (Image source: Kwon et al. 2023)" width=80%><figcaption><p>Fig. 5. Left: Memory layout for a 13B LLM on an NVIDIA A100—gray is persistent parameters, red is per-request KV cache, and yellow is temporary activation memory. Right: vLLM limits rapid KV cache growth, improving throughput. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><h3 id=pagedattention>PagedAttention<a hidden class=anchor aria-hidden=true href=#pagedattention>#</a></h3><p><strong>PagedAttention</strong> (<a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>) 的灵感来源于操作系统中的<strong>虚拟内存 (Virtual Memory)</strong> 和 <strong>分页 (Paging)</strong>。它允许将<strong>逻辑上连续的 KV Cache 存储在物理上非连续的显存空间中</strong>。</p><p>具体来说，PagedAttention 将每个序列的 KV Cache 分割成固定大小的 <strong>块 (Blocks)</strong>。每个块包含固定数量 token 的 Key 和 Value 向量。系统维护一个 <strong>块表 (Block Table)</strong>，用于记录每个序列的逻辑块到物理块的映射关系。</p><figure class=align-center><img loading=lazy src=PagedAttention.png#center alt="Fig. 6. Illustration of the PagedAttention algorithm, where KV vectors are stored in non-contiguous blocks. (Image source: Kwon et al. 2023)" width=70%><figcaption><p>Fig. 6. Illustration of the PagedAttention algorithm, where KV vectors are stored in non-contiguous blocks. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>PagedAttention 的核心思想借鉴了操作系统的虚拟内存和分页机制来管理 KV 缓存。</p><p>具体来说，PagedAttention 的设计理念可以总结为以下几点：</p><ol><li><p><strong>类比关系</strong>:</p><ul><li>KV 缓存的 <strong>块 (Blocks)</strong> 类比于操作系统内存管理的 <strong>页 (Pages)</strong>。</li><li><strong>Token</strong> 类比于 <strong>字节 (Bytes)</strong>。</li><li><strong>序列 (Sequences)</strong> 类比于 <strong>进程 (Processes)</strong>。</li></ul></li><li><p><strong>映射机制</strong>: PagedAttention 使用 <strong>块表</strong> 来维护从序列的连续 <strong>逻辑块</strong> 到 <strong>物理块</strong> 的映射。这些物理块在内存中可以是非连续的，就像操作系统的页表将虚拟地址映射到物理页帧一样。</p></li><li><p><strong>按需分配</strong>: 最关键的一点是，<strong>物理块</strong> 不是预先为整个序列最大长度分配好的，而是在需要存储新的 Key-Value（即生成新 Token）时 <strong>按需分配</strong>。</p></li></ol><p>这种按需、非连续的内存管理方式，使得 PagedAttention 能更有效地利用内存，避免了因预分配大量连续空间而造成的浪费和内部碎片，从而提高了 GPU 内存的利用率。</p><p>数学上，PagedAttention 将注意力计算转化为块级计算。设块大小为 $B$，第 $j$ 个 Key 块为 $K_{j}=\left(k_{(j-1) B+1}, \ldots, k_{j B}\right)$，Value 块为 $V_{j}=\left(v_{(j-1) B+1}, \ldots, v_{j B}\right)$。对于查询向量 $q_i$，注意力计算变为：</p>\[
A_{i j}=\frac{\exp \left(q_{i}^{\top} K_{j} / \sqrt{d}\right)}{\sum_{t=1}^{\lceil i / B\rceil} \exp \left(q_{i}^{\top} K_{t} \mathbf{1} / \sqrt{d}\right)}, \quad o_{i}=\sum_{j=1}^{\lceil i / B\rceil} V_{j} A_{i j}^{\top}
\]<p>其中 $A_{i j}=\left(a_{i,(j-1) B+1}, \ldots, a_{i, j B}\right)$ 是第 $i$ 个查询对第 $j$ 个 KV 块的注意力得分行向量。在计算过程中，PagedAttention 内核会高效地识别并获取所需的物理块。</p><h3 id=kv-缓存管理器>KV 缓存管理器<a hidden class=anchor aria-hidden=true href=#kv-缓存管理器>#</a></h3><p>vLLM 的内存管理器借鉴了操作系统的虚拟内存机制：</p><ol><li><strong>逻辑块与物理块:</strong> 每个请求的 KV 缓存被表示为一系列逻辑块。GPU 工作节点上的块引擎分配物理内存并将其划分为物理块。</li><li><strong>块表:</strong> 维护每个请求的逻辑块到物理块的映射。每个条目记录物理块地址和块内已填充的 token 数量。</li><li><strong>动态分配:</strong> 物理块按需分配，无需预先保留最大长度的空间，从而消除了大部分内存浪费。</li></ol><figure class=align-center><img loading=lazy src=block_table.png#center alt="Fig. 7. Block table translation in vLLM. Logical blocks are mapped to non-contiguous physical blocks. (Image source: Kwon et al. 2023)" width=80%><figcaption><p>Fig. 7. Block table translation in vLLM. Logical blocks are mapped to non-contiguous physical blocks. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>结合上图7的例子：</p><ol><li><strong>预填充阶段:</strong> 输入 prompt 有 7 个 token。假设块大小为 4。vLLM 分配 2 个物理块（例如物理块 7 和 1）并更新块表，将逻辑块 0 映射到物理块 7，逻辑块 1 映射到物理块 1。计算 prompt 的 KV 缓存并填充到这两个物理块中。逻辑块 0 填满 4 个 token，逻辑块 1 填充 3 个 token，剩余 1 个 slot 备用。</li><li><strong>解码阶段 :</strong><ul><li><strong>第 1 步:</strong> 使用 PagedAttention 计算下一个 token。由于逻辑块 1 还有空位，新的 KV 缓存直接存入物理块 1，并更新块表中逻辑块 1 的填充计数。</li><li><strong>第 2 步:</strong> 逻辑块 1 已满。vLLM 分配一个新的物理块（例如物理块 3），更新块表将新的逻辑块 2 映射到物理块 3，并将新生成的 KV 缓存存入物理块 3。</li></ul></li></ol><p>这种按需分配的方式将内存浪费限制在每个序列的最后一个块内，实现了接近最优的内存利用率（浪费低于 4%），从而可以批处理更多请求，提高吞吐量。</p><p>图8中展示了 vLLM 如何管理两个序列的内存空间。两个序列的逻辑块被映射到 GPU 工作节点（GPU worker）上由区块引擎预留的不同物理块中。这意味着，即使在逻辑层面相邻的块在物理 GPU 内存中也无需连续，从而两个序列可以有效地共享和利用物理内存空间。</p><figure class=align-center><img loading=lazy src=two_requests_vllm.png#center alt="Fig. 8. Storing the KV cache of two requests concurrently in vLLM using paged memory. (Image source: Kwon et al. 2023)" width=80%><figcaption><p>Fig. 8. Storing the KV cache of two requests concurrently in vLLM using paged memory. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><h3 id=内存共享>内存共享<a hidden class=anchor aria-hidden=true href=#内存共享>#</a></h3><p>PagedAttention 的另一个关键优势是高效的内存共享，尤其适用于复杂的解码策略。</p><h4 id=并行采样>并行采样<a hidden class=anchor aria-hidden=true href=#并行采样>#</a></h4><p>当一个请求需要从同一个 prompt 生成多个输出序列时（例如代码补全建议），prompt 部分的 KV 缓存可以共享。</p><figure class=align-center><img loading=lazy src=parallel_sampling.png#center alt="Fig. 9. Parallel sampling example. Logical blocks for the shared prompt map to the same physical blocks. (Image source: Kwon et al. 2023)" width=80%><figcaption><p>Fig. 9. Parallel sampling example. Logical blocks for the shared prompt map to the same physical blocks. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>vLLM 通过块表实现共享：</p><ol><li><strong>共享映射:</strong> 不同序列的逻辑块可以映射到同一个物理块。</li><li><strong>引用计数:</strong> 每个物理块维护一个引用计数。</li><li><strong>写时复制 (Copy-on-Write, CoW):</strong> 当一个共享块（引用计数 > 1）需要被写入时，vLLM 会分配一个新物理块，复制原块内容，更新写入序列的块表映射，并将原物理块的引用计数减 1。后续对该物理块的写入（当引用计数为 1 时）则直接进行。</li></ol><p>这种机制显著减少了<strong>并行采样 (Parallel Sampling)</strong> 的内存开销，论文实验显示可节省高达 55% 的内存。</p><h4 id=束搜索>束搜索<a hidden class=anchor aria-hidden=true href=#束搜索>#</a></h4><p><strong>束搜索 (Beam Search)</strong> 在解码过程中，不同的候选序列（beam）不仅共享 prompt 部分，还可能共享后续生成的 token 的 KV 缓存，且共享模式是动态变化的。</p><figure class=align-center><img loading=lazy src=beam_search.png#center alt="Fig. 10. Beam search example ($k=4$). Blocks are dynamically shared and freed based on candidate survival. (Image source: Kwon et al. 2023)" width=70%><figcaption><p>Fig. 10. Beam search example ($k=4$). Blocks are dynamically shared and freed based on candidate survival. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>vLLM 通过引用计数和 CoW 机制，高效地管理这种动态共享，避免了传统实现中频繁且昂贵的内存拷贝操作。大部分块可以共享，只有当新生成的 token 落入旧的共享块时才需要 CoW（仅拷贝一个块）。</p><h4 id=共享前缀>共享前缀<a hidden class=anchor aria-hidden=true href=#共享前缀>#</a></h4><p>对于许多 prompt 共享相同前缀（如系统指令、few-shot 示例）的应用场景，vLLM 可以预先计算并缓存这些 <strong>共享前缀 (Shared Prefix)</strong> 的 KV 缓存到一组物理块中。</p><figure class=align-center><img loading=lazy src=shared_prefix.png#center alt="Fig. 11. Shared prompt example for machine translation using few-shot examples. (Image source: Kwon et al. 2023)" width=70%><figcaption><p>Fig. 11. Shared prompt example for machine translation using few-shot examples. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>当处理包含该前缀的请求时，只需将其逻辑块映射到缓存的物理块（最后一个块标记为 CoW），从而避免了对前缀部分的重复计算。</p><h3 id=调度与抢占>调度与抢占<a hidden class=anchor aria-hidden=true href=#调度与抢占>#</a></h3><p>vLLM 采用 FCFS 调度策略。当 GPU 内存不足以容纳新生成的 KV 缓存时，需要进行抢占：</p><ol><li><strong>抢占单位:</strong> 以<strong>序列组</strong> 为单位进行抢占（例如，一个 beam search 请求的所有候选序列）。确保最早到达的请求优先服务，最晚到达的请求优先被抢占。</li><li><strong>恢复机制:</strong><ul><li><strong>换出:</strong> 将被抢占序列的 KV 块拷贝到 CPU 内存。当资源可用时再换回 GPU。适用于 PCIe 带宽较高且块较大的情况。</li><li><strong>重计算:</strong> 丢弃被抢占序列的 KV 缓存。当资源可用时，将原始 prompt 和已生成的 token 拼接起来，通过一次高效的 prompt phase 重新计算 KV 缓存。适用于 PCIe 带宽较低或块较小的情况。</li></ul></li></ol><h3 id=分布式执行>分布式执行<a hidden class=anchor aria-hidden=true href=#分布式执行>#</a></h3><p>vLLM 支持 <a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a> 风格的张量模型并行。</p><figure class=align-center><img loading=lazy src=vllm_system_overview.png#center alt="Fig. 12. vLLM system overview showing centralized scheduler and distributed workers. (Image source: Kwon et al. 2023)" width=70%><figcaption><p>Fig. 12. vLLM system overview showing centralized scheduler and distributed workers. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><ul><li><strong>集中式调度器:</strong> 包含 KV 缓存管理器，维护全局的逻辑块到物理块的映射。</li><li><strong>共享映射:</strong> 所有 GPU worker 共享块表。</li><li><strong>本地存储:</strong> 每个 worker 只存储其负责的注意力头对应的 KV 缓存部分。</li><li><strong>执行流程:</strong> 调度器广播输入 token ID 和块表给所有 worker -> worker 执行模型计算（包括 PagedAttention）-> worker 间通过 All-Reduce 同步中间结果 -> worker 将采样结果返回给调度器。内存管理信息在每步开始时一次性广播，无需 worker 间同步。</li></ul><h3 id=内核优化>内核优化<a hidden class=anchor aria-hidden=true href=#内核优化>#</a></h3><p>为了高效实现 PagedAttention，vLLM 开发了定制 CUDA 核：</p><ul><li><strong>融合 Reshape 和块写入:</strong> 将新 KV 缓存分块、重塑布局、按块表写入融合为单个核。</li><li><strong>融合块读取和注意力计算:</strong> 修改 FasterTransformer 的注意力核，使其能根据块表读取非连续块并即时计算注意力，优化内存访问模式。</li><li><strong>融合块拷贝:</strong> 将 CoW 触发的多个小块拷贝操作批量化到单个核中执行。</li></ul><h2 id=vllm-v1>vLLM V1<a hidden class=anchor aria-hidden=true href=#vllm-v1>#</a></h2><p>2025 年 1 月，vLLM 团队发布了 <strong>vLLM V1</strong> 的 alpha 版本，这是对其核心架构的一次重大升级。基于过去一年半的开发经验，V1 版本重新审视了关键设计决策，整合了各种特性，并简化了代码库。</p><p>基于 vLLM V0 的成功和经验教训，vLLM V1 对核心架构进行了重大升级，旨在提供更简洁、模块化、易于扩展且性能更高的代码库。</p><h3 id=v1-的动机与目标>V1 的动机与目标<a hidden class=anchor aria-hidden=true href=#v1-的动机与目标>#</a></h3><ul><li><strong>V0 的挑战:</strong> 随着功能和硬件支持的扩展，V0 的代码复杂度增加，特性难以有效组合，技术债务累积。</li><li><strong>V1 的目标:</strong><ul><li>简洁、模块化、易于修改的代码库。</li><li>接近零 CPU 开销的高性能。</li><li>将关键优化统一到架构中。</li><li>默认启用优化，实现零配置。</li></ul></li></ul><h3 id=优化的执行循环与-api-服务器>优化的执行循环与 API 服务器<a hidden class=anchor aria-hidden=true href=#优化的执行循环与-api-服务器>#</a></h3><figure class=align-center><img loading=lazy src=vllm_v1_architecture.png#center alt="Fig. 13. vLLM V1&rsquo;s multiprocessing architecture with an isolated EngineCore. (Image source: vLLM Blog, 2025)" width=80%><figcaption><p>Fig. 13. vLLM V1&rsquo;s multiprocessing architecture with an isolated EngineCore. (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><p>随着 GPU 计算速度加快（例如 H100 上 Llama-8B 推理时间仅约 5ms），CPU 开销（API 服务、调度、输入准备、解码、流式响应）成为瓶颈。V1 采用了<strong>多进程架构</strong>：</p><ul><li><strong>隔离的 EngineCore:</strong> 将调度器和模型执行器隔离在核心引擎循环中。</li><li><strong>CPU 任务卸载:</strong> 将 Tokenization、多模态输入处理、Detokenization、流式传输等 CPU 密集型任务移至独立进程，与 EngineCore 并行执行，最大化模型吞吐量。</li></ul><h3 id=简洁灵活的调度器>简洁灵活的调度器<a hidden class=anchor aria-hidden=true href=#简洁灵活的调度器>#</a></h3><figure class=align-center><img loading=lazy src=v1_scheduler.png#center alt="Fig. 14. vLLM V1&rsquo;s scheduler treats prompt and generated tokens uniformly, enabling features like chunked prefill. (Image source: vLLM Blog, 2025)" width=80%><figcaption><p>Fig. 14. vLLM V1&rsquo;s scheduler treats prompt and generated tokens uniformly, enabling features like chunked prefill. (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><ul><li><strong>统一处理:</strong> 不再区分 &ldquo;prefill&rdquo; 和 &ldquo;decode&rdquo; 阶段，统一处理用户输入 token 和模型生成 token。</li><li><strong>简单表示:</strong> 调度决策用字典表示，如 <code>{request_id: num_tokens}</code>，指定每步为每个请求处理多少 token。</li><li><strong>通用性:</strong> 这种表示足以支持块状预填充 (Chunked Prefills)、前缀缓存 (Prefix Caching)、投机解码 (Speculative Decoding) 等特性。例如，块状预填充只需在固定 token 预算下动态分配各请求的处理数量。</li></ul><h3 id=零开销前缀缓存>零开销前缀缓存<a hidden class=anchor aria-hidden=true href=#零开销前缀缓存>#</a></h3><figure class=align-center><img loading=lazy src=prefix_caching_benchmark.png#center alt="Fig. 15. Performance comparison of prefix caching in vLLM V0 and V1. V1 achieves near-zero overhead even at 0% hit rate. (Image source: vLLM Blog, 2025)" width=100%><figcaption><p>Fig. 15. Performance comparison of prefix caching in vLLM V0 and V1. V1 achieves near-zero overhead even at 0% hit rate. (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><p>V1 优化了前缀缓存（基于哈希匹配和 LRU 驱逐）的实现：</p><ul><li><strong>优化数据结构:</strong> 实现常数时间缓存驱逐。</li><li><strong>减少 Python 对象开销:</strong> 最小化对象创建。</li><li><strong>结果:</strong> 即使缓存命中率为 0%，性能下降也小于 1%。而在高命中率时，性能提升数倍。因此，V1 默认启用前缀缓存。</li></ul><h3 id=清晰的张量并行推理架构>清晰的张量并行推理架构<a hidden class=anchor aria-hidden=true href=#清晰的张量并行推理架构>#</a></h3><figure class=align-center><img loading=lazy src=v1_tp_architecture.png#center alt="Fig. 16. vLLM V1&rsquo;s symmetric tensor-parallel architecture using diff-based updates. (Image source: vLLM Blog, 2025)" width=80%><figcaption><p>Fig. 16. vLLM V1&rsquo;s symmetric tensor-parallel architecture using diff-based updates. (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><p>V1 解决了 V0 中调度器和 Worker 0 耦合导致的非对称架构问题：</p><ul><li><strong>Worker 端状态缓存:</strong> 请求状态缓存在 Worker 端。</li><li><strong>增量更新:</strong> 每步只传输状态的<strong>增量变化 (diffs)</strong>，极大减少了进程间通信。</li><li><strong>对称架构:</strong> 调度器和 Worker 0 可以运行在不同进程中，架构更清晰、对称。</li><li><strong>抽象分布式逻辑:</strong> Worker 在单 GPU 和多 GPU 设置下行为一致。</li></ul><h3 id=高效的输入准备>高效的输入准备<a hidden class=anchor aria-hidden=true href=#高效的输入准备>#</a></h3><figure class=align-center><img loading=lazy src=persistent_batch.png#center alt="Fig. 17. vLLM V1 uses Persistent Batch to cache input tensors and apply diffs. (Image source: vLLM Blog, 2025)" width=70%><figcaption><p>Fig. 17. vLLM V1 uses Persistent Batch to cache input tensors and apply diffs. (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><p>V0 每步重新创建模型输入张量和元数据，CPU 开销大。V1 采用 <a href="https://github.com/InternLM/lmdeploy?tab=readme-ov-file">Persistent Batch</a> 技术：</p><ul><li><strong>缓存输入张量:</strong> 缓存输入张量。</li><li><strong>应用 Diffs:</strong> 每步只应用增量变化。</li><li><strong>Numpy 优化:</strong> 大量使用 Numpy 操作替代 Python 原生操作，减少更新张量的 CPU 开销。</li></ul><h3 id=综合优化>综合优化<a hidden class=anchor aria-hidden=true href=#综合优化>#</a></h3><ol><li><p><strong>torch.compile 与分段 CUDA 图</strong></p><ul><li><strong>集成 <code>torch.compile</code>:</strong> V1 充分利用 vLLM 的 <code>torch.compile</code> 集成功能，自动优化模型，支持多种模型的高效运行，显著减少了手动编写 CUDA 核的需求。</li><li><strong>分段 CUDA 图 (Piecewise CUDA Graphs):</strong> 通过引入分段 CUDA 图，成功克服了原生 CUDA 图的局限性，提高了模型的灵活性和性能。</li></ul></li><li><p><strong>增强的多模态 LLM 支持</strong></p><ul><li>V1 针对多模态大语言模型 (MLLMs) 推出多项关键改进：<ul><li><strong>优化预处理:</strong> 将图像解码、裁剪、转换等 CPU 密集型的预处理任务移至非阻塞的独立进程，防止阻塞 GPU 工作。同时引入预处理缓存，以便缓存已处理的输入，供之后的请求复用，尤其适用于相同的多模态输入。</li><li><strong>多模态前缀缓存:</strong> 除了使用 token ID 的哈希，V1 还引入图像哈希来标识包含图像输入的 KV 缓存。此改进在包含图像输入的多轮对话场景中尤其有利。</li><li><strong>编码器缓存:</strong> 针对需要视觉编码器输出的应用，V1 临时缓存视觉嵌入，允许调度器将文本输入分块处理，避免在每一步都重新计算视觉嵌入，从而支持 MLLM 的块状填充调度。</li></ul></li></ul></li><li><p><strong>FlashAttention 3 集成</strong></p><ul><li>由于 V1 的高度动态性（如在同一批处理内结合预填充和解码），需要一种灵活且高性能的注意力核。<a href=https://arxiv.org/abs/2407.08608>FlashAttention 3</a> 完美符合这一需求，提供了强大的功能支持，同时在各种使用场景中保持优异的性能表现。</li></ul></li></ol><h3 id=性能对比>性能对比<a hidden class=anchor aria-hidden=true href=#性能对比>#</a></h3><p>得益于架构改进和 CPU 开销的大幅降低，V1 相比 V0（未开启多步调度）实现了高达 1.7 倍的吞吐量提升，同时在多模态模型上的性能提升显著。</p><figure class=align-center><img loading=lazy src=vllm_v1_llama3.png#center alt="Fig. 18. Performance comparison between vLLM V0 and V1 on Llama 3.1 8B & Llama 3.3 70B (1xH100). (Image source: vLLM Blog, 2025)" width=100%><figcaption><p>Fig. 18. Performance comparison between vLLM V0 and V1 on Llama 3.1 8B & Llama 3.3 70B (1xH100). (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><figure class=align-center><img loading=lazy src=v1_qwen2vl.png#center alt="Fig. 19. Performance comparison between vLLM V0 and V1 on Qwen2-VL 7B (1xH100). (Image source: vLLM Blog, 2025)" width=60%><figcaption><p>Fig. 19. Performance comparison between vLLM V0 and V1 on Qwen2-VL 7B (1xH100). (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><p><strong>表格对比:</strong></p><table><thead><tr><th style=text-align:left>特性</th><th style=text-align:left>vLLM V0</th><th style=text-align:left>vLLM V1</th><th style=text-align:left>改进点</th></tr></thead><tbody><tr><td style=text-align:left><strong>核心技术</strong></td><td style=text-align:left>PagedAttention</td><td style=text-align:left>PagedAttention + 全面架构重构</td><td style=text-align:left>保持 PagedAttention 优势，优化整体架构</td></tr><tr><td style=text-align:left><strong>内存效率</strong></td><td style=text-align:left>极高 (浪费 &lt; 4%)</td><td style=text-align:left>极高 (浪费 &lt; 4%)</td><td style=text-align:left>维持高内存效率</td></tr><tr><td style=text-align:left><strong>内存共享</strong></td><td style=text-align:left>支持 (CoW)</td><td style=text-align:left>支持 (CoW)</td><td style=text-align:left>维持高效共享</td></tr><tr><td style=text-align:left><strong>CPU 开销</strong></td><td style=text-align:left>相对较高，尤其在复杂场景或低命中率前缀缓存时</td><td style=text-align:left>显著降低，接近零开销</td><td style=text-align:left>多进程、Persistent Batch、优化数据结构等</td></tr><tr><td style=text-align:left><strong>执行循环</strong></td><td style=text-align:left>单进程，API 服务器与引擎耦合较紧</td><td style=text-align:left>多进程，API 服务器与 EngineCore 解耦，高度并行</td><td style=text-align:left>提升 CPU/GPU 并行度，减少阻塞</td></tr><tr><td style=text-align:left><strong>调度器</strong></td><td style=text-align:left>区分 Prefill/Decode</td><td style=text-align:left>统一处理 Token，字典式调度表示</td><td style=text-align:left>更简洁、灵活，易于支持高级特性</td></tr><tr><td style=text-align:left><strong>前缀缓存</strong></td><td style=text-align:left>默认禁用 (低命中率时有开销)</td><td style=text-align:left>默认启用 (零开销设计)</td><td style=text-align:left>优化后无惧低命中率，默认开启提升易用性</td></tr><tr><td style=text-align:left><strong>张量并行</strong></td><td style=text-align:left>不对称架构 (Scheduler+Worker0 同进程)</td><td style=text-align:left>对称架构 (Scheduler 与 Worker 分离)</td><td style=text-align:left>架构更清晰，IPC 开销通过状态缓存和 Diffs 传输控制</td></tr><tr><td style=text-align:left><strong>多模态支持</strong></td><td style=text-align:left>基本支持</td><td style=text-align:left>增强支持 (非阻塞预处理、图像前缀缓存、编码器缓存等)</td><td style=text-align:left>提升 VLM 性能和易用性</td></tr><tr><td style=text-align:left><strong>编译器集成</strong></td><td style=text-align:left>有限</td><td style=text-align:left>集成 <code>torch.compile</code></td><td style=text-align:left>自动化模型优化，减少手写 Kernel</td></tr><tr><td style=text-align:left><strong>Attention Kernel</strong></td><td style=text-align:left>定制 Kernel (基于 FasterTransformer)</td><td style=text-align:left>集成 FlashAttention 3</td><td style=text-align:left>拥抱业界标准，获得更好的性能和特性支持</td></tr><tr><td style=text-align:left><strong>性能 (vs V0)</strong></td><td style=text-align:left>基线</td><td style=text-align:left>吞吐量提升高达 1.7x (文本), 多模态模型提升更显著</td><td style=text-align:left>全面优化 CPU 开销带来的提升</td></tr><tr><td style=text-align:left><strong>代码复杂度</strong></td><td style=text-align:left>随功能增加而提高</td><td style=text-align:left>更简洁、模块化</td><td style=text-align:left>降低维护成本，方便社区贡献和二次开发</td></tr></tbody></table><h2 id=其他推理框架>其他推理框架<a hidden class=anchor aria-hidden=true href=#其他推理框架>#</a></h2><ul><li><a href=https://github.com/ModelTC/lightllm>LightLLM</a>：一个基于 Python 的轻量级推理与服务框架，以轻量级设计、可扩展性和高速性能著称，汲取了 vLLM 等其他开源项目的优势。</li><li><a href=https://github.com/InternLM/lmdeploy>LMDeploy</a>：用于压缩、部署和服务 LLM 的工具包，内置 TurboMind 推理引擎，强调高请求吞吐量和高效量化。</li><li><a href=https://github.com/sgl-project/sglang>SGLang</a>：通过前端语言与后端执行引擎协同设计，高效执行复杂（尤其涉及结构化生成）的 LLM 程序的框架。</li><li><a href=https://github.com/huggingface/text-generation-inference>TGI</a>：Hugging Face 的生产级 LLM 服务方案，广泛应用并支持多种硬件后端，借助 vLLM 的 PagedAttention 内核，提供高并发、低延迟的推理服务。</li><li><a href=https://github.com/NVIDIA/TensorRT-LLM>TensorRT-LLM</a>：NVIDIA 推出的开源库，用于在 NVIDIA GPU 上优化并加速 LLM 推理，利用 TensorRT 的提前编译和深度硬件优化能力。</li></ul><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>vLLM 通过其核心技术 PagedAttention，极大地缓解了 LLM 服务中 KV 缓存管理带来的内存瓶颈，显著提高了内存利用率和吞吐量。PagedAttention 借鉴操作系统分页机制，实现了 KV 缓存的非连续存储、动态分配和高效共享（支持并行采样、束搜索、共享前缀等）。</p><p>vLLM V1 在 V0 的基础上，对核心架构进行了全面重构和优化，通过多进程架构、灵活调度器、零开销前缀缓存、对称张量并行架构、高效输入准备、<code>torch.compile</code> 集成、增强 MLLMs 支持以及 FlashAttention 3 集成等一系列改进，进一步降低了 CPU 开销，提升了系统整体性能、灵活性和可扩展性，为未来快速迭代新功能奠定了坚实基础。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><p>[1] Kwon, Woosuk, et al. <a href=https://arxiv.org/abs/2309.06180>&ldquo;Efficient memory management for large language model serving with pagedattention.&rdquo;</a> Proceedings of the 29th Symposium on Operating Systems Principles. 2023.</p><p>[2] vLLM Team. <a href=https://vllm.ai/blog/2023/06/20/vllm.html>&ldquo;vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention.&rdquo;</a> vLLM Blog, June 20, 2023.</p><p>[3] vLLM Team. <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>&ldquo;vLLM V1: A Major Upgrade to vLLM&rsquo;s Core Architecture.&rdquo;</a> vLLM Blog, Jan 27, 2025.</p><p>[4] NVIDIA. <a href=https://github.com/NVIDIA/FasterTransformer>&ldquo;FasterTransformer.&rdquo;</a> GitHub Repository, 2023.</p><p>[5] Yu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. <a href=https://www.usenix.org/conference/osdi22/presentation/yu>&ldquo;Orca: A Distributed Serving System for Transformer-Based Generative Models.&rdquo;</a> In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 2022.</p><p>[6] OpenAI. <a href="https://platform.openai.com/docs/guides/streaming-responses?api-mode=responses">&ldquo;API Reference - Streaming.&rdquo;</a> OpenAI Platform Documentation, 2025.</p><p>[7] Wolf, Thomas, et al. <a href=https://www.aclweb.org/anthology/2020.emnlp-demos.6>&ldquo;Transformers: State-of-the-Art Natural Language Processing.&rdquo;</a> In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.</p><p>[8] Hugging Face. <a href=https://github.com/huggingface/text-generation-inference>&ldquo;Text Generation Inference.&rdquo;</a> GitHub Repository, 2025.</p><p>[9] Shoeybi, Mohammad, et al. <a href=https://arxiv.org/abs/1909.08053>&ldquo;Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.&rdquo;</a> arXiv preprint arXiv:1909.08053 (2019).</p><p>[10] InternLM Team. <a href=https://github.com/InternLM/lmdeploy>&ldquo;LMDeploy.&rdquo;</a> GitHub Repository, 2025.</p><p>[12] Shah, Jay, et al. <a href=https://arxiv.org/abs/2407.08608>&ldquo;Flashattention-3: Fast and accurate attention with asynchrony and low-precision.&rdquo;</a> Advances in Neural Information Processing Systems 37 (2024): 68658-68685.</p><p>[13] ModelTC. <a href=https://github.com/ModelTC/lightllm>&ldquo;LightLLM.&rdquo;</a> GitHub Repository, 2025.</p><p>[14] Zheng, Lianmin, et al. <a href=https://arxiv.org/abs/2312.07104>&ldquo;Sglang: Efficient execution of structured language model programs.&rdquo;</a> Advances in Neural Information Processing Systems 37 (2024): 62557-62583.</p><p>[15] NVIDIA. <a href=https://github.com/NVIDIA/TensorRT-LLM>&ldquo;TensorRT-LLM.&rdquo;</a> GitHub Repository, 2025.</p><p>[16] vLLM Team. <a href="https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?slide=id.g31441846c39_0_0#slide=id.g31441846c39_0_0">&ldquo;NYC vLLM Meetup Presentation.&rdquo;</a> Google Slides, 2025.</p><h2 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h2><blockquote><p><strong>引用</strong>：转载或引用本文内容时，请注明原作者和来源。</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (May 2025). vLLM：高吞吐、有效内存的LLM服务引擎.
<a href=https://syhya.github.io/zh/posts/2025-05-17-vllm>https://syhya.github.io/zh/posts/2025-05-17-vllm</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025vllm</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;vLLM：高吞吐、有效内存的LLM服务引擎&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;May&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/zh/posts/2025-05-17-vllm&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/pagedattention/>PagedAttention</a></li><li><a href=https://syhya.github.io/zh/tags/llm-serving/>LLM Serving</a></li><li><a href=https://syhya.github.io/zh/tags/inference/>Inference</a></li><li><a href=https://syhya.github.io/zh/tags/kv-cache/>KV Cache</a></li><li><a href=https://syhya.github.io/zh/tags/memory-optimization/>Memory Optimization</a></li><li><a href=https://syhya.github.io/zh/tags/llms/>LLMs</a></li><li><a href=https://syhya.github.io/zh/tags/ai-infrastructure/>AI Infrastructure</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/zh/posts/2025-06-29-llm-inference/><span class=title>« 上一页</span><br><span>大语言模型推理</span>
</a><a class=next href=https://syhya.github.io/zh/posts/2025-05-04-multimodal-llm/><span class=title>下一页 »</span><br><span>多模态大语言模型</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM：高吞吐、有效内存的LLM服务引擎 on x" href="https://x.com/intent/tweet/?text=vLLM%ef%bc%9a%e9%ab%98%e5%90%9e%e5%90%90%e3%80%81%e6%9c%89%e6%95%88%e5%86%85%e5%ad%98%e7%9a%84LLM%e6%9c%8d%e5%8a%a1%e5%bc%95%e6%93%8e&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-05-17-vllm%2f&amp;hashtags=PagedAttention%2cLLMServing%2cInference%2cKVCache%2cMemoryOptimization%2cLLMs%2cAIInfrastructure"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM：高吞吐、有效内存的LLM服务引擎 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-05-17-vllm%2f&amp;title=vLLM%ef%bc%9a%e9%ab%98%e5%90%9e%e5%90%90%e3%80%81%e6%9c%89%e6%95%88%e5%86%85%e5%ad%98%e7%9a%84LLM%e6%9c%8d%e5%8a%a1%e5%bc%95%e6%93%8e&amp;summary=vLLM%ef%bc%9a%e9%ab%98%e5%90%9e%e5%90%90%e3%80%81%e6%9c%89%e6%95%88%e5%86%85%e5%ad%98%e7%9a%84LLM%e6%9c%8d%e5%8a%a1%e5%bc%95%e6%93%8e&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-05-17-vllm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM：高吞吐、有效内存的LLM服务引擎 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-05-17-vllm%2f&title=vLLM%ef%bc%9a%e9%ab%98%e5%90%9e%e5%90%90%e3%80%81%e6%9c%89%e6%95%88%e5%86%85%e5%ad%98%e7%9a%84LLM%e6%9c%8d%e5%8a%a1%e5%bc%95%e6%93%8e"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM：高吞吐、有效内存的LLM服务引擎 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-05-17-vllm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM：高吞吐、有效内存的LLM服务引擎 on whatsapp" href="https://api.whatsapp.com/send?text=vLLM%ef%bc%9a%e9%ab%98%e5%90%9e%e5%90%90%e3%80%81%e6%9c%89%e6%95%88%e5%86%85%e5%ad%98%e7%9a%84LLM%e6%9c%8d%e5%8a%a1%e5%bc%95%e6%93%8e%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-05-17-vllm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM：高吞吐、有效内存的LLM服务引擎 on telegram" href="https://telegram.me/share/url?text=vLLM%ef%bc%9a%e9%ab%98%e5%90%9e%e5%90%90%e3%80%81%e6%9c%89%e6%95%88%e5%86%85%e5%ad%98%e7%9a%84LLM%e6%9c%8d%e5%8a%a1%e5%bc%95%e6%93%8e&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-05-17-vllm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM：高吞吐、有效内存的LLM服务引擎 on ycombinator" href="https://news.ycombinator.com/submitlink?t=vLLM%ef%bc%9a%e9%ab%98%e5%90%9e%e5%90%90%e3%80%81%e6%9c%89%e6%95%88%e5%86%85%e5%ad%98%e7%9a%84LLM%e6%9c%8d%e5%8a%a1%e5%bc%95%e6%93%8e&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-05-17-vllm%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>