[{"content":"Background With the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.\nBased on my work experience, this article summarizes how to build LLMs equipped with specific domain knowledge by leveraging data preparation, model training, deployment, evaluation, and continuous iteration on top of existing general models.\nWhy Inject Domain Knowledge into the Foundational LLMs? Challenge 1: Limited Domain Knowledge Existing pre-trained models (such as GPT-4 and Llama 3) are primarily trained on general-purpose corpora, lacking in-depth understanding of niche languages or proprietary domains. This deficiency leads to subpar performance when the models handle programming code.\nChallenge 2: Data Security and Compliance When enterprises handle sensitive data, they must adhere to strict data sovereignty and compliance requirements. Uploading proprietary business data to third-party cloud services poses security risks, necessitating data processing and model training within local environments.\nChallenge 3: Limitations of OpenAI Fine-Tuning Mainstream commercial APIs for fine-tuning are typically basic and struggle to achieve deep alignment and optimization. For highly customized domain models, such approaches often fail to meet the required specifications.\nTwo Approaches of Injecting Knowledge In practical projects, the common methods for injecting domain knowledge into base models include Fine-Tuning and Retrieval-Augmented Generation (RAG). The following sections provide a detailed comparison of these methods to aid in selecting the most suitable strategy.\nMethod Comparison Fine-Tuning Core Concept\nThrough continued pre-training, supervised fine-tuning, and preference alignment, directly update the model parameters to enable it to master domain-specific knowledge and task patterns.\nTechnical Details\nContinued Pre-Training (CPT): Continue pre-training the base model on a large volume of domain-specific unsupervised data. Supervised Fine-Tuning (SFT): Perform supervised fine-tuning using high-quality labeled data. Preference Alignment (DPO): Optimize model outputs based on user feedback. Parameter Tuning Methods: Utilize full-parameter fine-tuning or combine with PEFT methods like LoRA to freeze some parameters and add adapters. Advantages\nDeep Customization: Updating the internal weights of the model enables a profound understanding of domain knowledge. No External Retrieval Dependency: Inference does not require additional knowledge bases, reducing latency and total token consumption. Enhanced Overall Performance: Significantly outperforms general models in domain-specific tasks. Disadvantages\nHigh Computational Cost: Requires substantial computational resources for training, especially during the CPT phase. Long Training Cycles: From data preparation to model training and optimization, the process is time-consuming. Catastrophic Forgetting: The model may forget its original general capabilities while learning new knowledge. Retrieval-Augmented Generation (RAG) Core Concept\nBuild a domain-specific knowledge base and retrieve relevant documents during inference to assist the model in generating more accurate responses without directly altering model parameters.\nTechnical Details\nData Processing: Preprocess domain documents by chunking them based on size and overlap. Vectorization: Embedding text chunks as vectors using embedding models and storing them in a Vector Store for retrieval. Retrieval: During inference, retrieve relevant documents through similarity search to provide contextual information or few-shot examples to the base model. Advantages\nPreserves General Capabilities: Model parameters remain unchanged, retaining general language abilities. Quick Updates: The knowledge base can be dynamically updated without retraining the model. Computational Efficiency: Avoids large-scale training, saving computational resources. Disadvantages\nDependence on Knowledge Base Quality: The quality of retrieved documents directly impacts response quality. Inference Speed: The retrieval process may increase inference latency and require more tokens. Limited Knowledge Coverage: The model’s internal knowledge is still restricted by the base model’s pre-training data. Models and Training Resources Base Models Taking the Llama 3 series as an example, it features the following characteristics:\nParameter Scale\nThe Llama 3 series includes models ranging from 1B to 405B parameters, widely supporting multilingual processing, code generation, reasoning, as well as visual and textual tasks. Smaller models (1B and 3B) are specially optimized for edge and mobile devices, supporting up to 128K context windows, efficiently handling local tasks such as summary generation, instruction execution, and text rewriting.\nMultimodal Capabilities\nLlama 3\u0026rsquo;s visual models (11B and 90B parameters) outperform many closed models in image understanding tasks and support multimodal processing of images, videos, and audio. All models support fine-tuning, facilitating customized development for specific domains.\nOpen Source and Community Support\nLlama 3 series models and their weights are released in open-source form and can be accessed via llama.com and the Hugging Face platform, providing convenient access and application support for developers.\nDataset Restrictions\nAlthough the Llama 3 models are released as open-source, the datasets used for their training are not open-sourced. Therefore, strictly speaking, Llama 3 is not entirely open-source. This limitation may pose challenges in addressing catastrophic forgetting, as obtaining data sets identical to the original training data is difficult.\nTraining Resources Training large language models requires robust computational resources and efficient distributed training frameworks.\nHardware Resources\nGPU Clusters: NVIDIA A100 or H100 GPUs are recommended, with configurations of 4 or 8 GPUs connected via NVLink or InfiniBand to enhance communication bandwidth. Storage Resources: High-performance SSDs (e.g., NVMe) to support fast data read and write operations. Software Frameworks\nDistributed Training Frameworks: DeepSpeed, Megatron-LM, among others, support large-scale model training. Inference Frameworks: vLLM, ollama, etc., optimize inference speed and resource utilization. Parallel Strategies\nData Parallelism (DP): Suitable when the model fits on a single GPU, implemented via DeepSpeed\u0026rsquo;s ZeRO Stage 0. Model Parallelism (MP), Pipeline Parallelism (PP), and Tensor Parallelism (TP): When the model cannot fit on a single GPU, optimize using ZeRO Stage 1, 2, or 3, or employ ZeRO-Infinity to offload parts of parameters and optimizer states to CPU or NVMe. DeepSpeed ZeRO Sharding Strategies Comparison ZeRO Stage Sharding Strategies ZeRO Stage Description GPU Memory Usage Training Speed ZeRO-0 Pure data parallelism without any sharding. All optimizer states, gradients, and parameters are fully replicated on each GPU. Highest Fastest ZeRO-1 Shards optimizer states (e.g., momentum and second moments), reducing GPU memory usage, but gradients and parameters remain data parallel. High Slightly slower than ZeRO-0 ZeRO-2 Shards optimizer states and gradients, further reducing GPU memory usage based on ZeRO-1. Medium Slower than ZeRO-1 ZeRO-3 Shards optimizer states, gradients, and model parameters, achieving the lowest GPU memory usage, suitable for extremely large models. Requires parameter broadcasting (All-Gather/All-Reduce) during forward/backward passes, significantly increasing communication overhead. Low Significantly slower than ZeRO-2, depends on model size and network bandwidth Offload Strategies Offload Type Description GPU Memory Usage Training Speed ZeRO-1 + CPU Offload Extends ZeRO-1 by offloading optimizer states to CPU memory, further reducing GPU memory usage but necessitating CPU-GPU data transfer, relying on PCIe bandwidth, and occupying CPU memory. Medium-low Slower than ZeRO-1, affected by CPU performance and PCIe bandwidth ZeRO-2 + CPU Offload Extends ZeRO-2 by offloading optimizer states to CPU memory, further reducing GPU memory usage for larger models but increasing CPU-GPU data transfer overhead. Lower Slower than ZeRO-2, affected by CPU performance and PCIe bandwidth ZeRO-3 + CPU Offload Extends ZeRO-3 by offloading optimizer states and model parameters to CPU, achieving minimal GPU memory usage but with extremely high CPU-GPU communication volume and CPU bandwidth significantly lower than GPU-GPU communication. Extremely Low Very Slow ZeRO-Infinity (NVMe Offload) Based on ZeRO-3, offloads optimizer states, gradients, and parameters to NVMe, breaking CPU memory limits and suitable for ultra-large-scale models; performance highly depends on NVMe parallel read/write speeds. Extremely LowRequires NVMe support Slower than ZeRO-3 but generally faster than ZeRO-3 + CPU Offload, can achieve better throughput if NVMe bandwidth is sufficient Communication Volume and Performance Impact ZeRO-0/1/2:\nCommunication is primarily gradient synchronization using All-Reduce operations, resulting in relatively low communication volume.\nZeRO-3:\nRequires All-Gather/All-Reduce operations for model parameters, significantly increasing communication volume. Network bandwidth becomes a critical bottleneck, and parameter broadcasting during forward/backward passes further exacerbates communication load.\nCPU Offload (ZeRO-1/2/3 + CPU):\nOffloads optimizer states or parameters to CPU, reducing GPU memory usage. Communication volume mainly arises from CPU \u0026lt;-\u0026gt; GPU data transfers, which have much lower bandwidth compared to GPU-GPU communication, easily causing performance bottlenecks, especially in ZeRO-3 scenarios. NVMe Offload (ZeRO-Infinity):\nFurther offloads to NVMe based on ZeRO-3, overcoming CPU memory limitations to support ultra-large-scale models. Performance heavily relies on NVMe I/O bandwidth and parallelism. If NVMe speed is sufficiently high, it typically outperforms CPU Offload; however, performance may suffer in scenarios with weak I/O performance or high latency. Hardware and Configuration Impact Hardware Constraints:\nPCIe Bandwidth, Network Bandwidth, NVMe I/O, etc., significantly impact Offload performance. Optimal strategies should be selected based on the hardware environment. Additional Notes:\nCPU Offload utilizes CPU memory and transfers data via PCIe; NVMe Offload saves states on NVMe devices. NVMe Offload generally outperforms CPU Offload when NVMe I/O performance is adequate, but care must be taken to avoid performance bottlenecks caused by insufficient I/O performance. Reference to Official Documentation:\nIt is recommended to consult the DeepSpeed official documentation for the latest and most accurate configuration parameters and performance tuning advice. Data Preparation: The Core of Training Success Data quality directly determines model performance. Data preparation includes data collection, cleaning, deduplication, categorization and balancing, anonymization, and other steps.\nPre-Training Data Data Sources Public Datasets: Such as the-stack-v2, Common Crawl, etc. Enterprise Proprietary Data: Internal documents, code repositories, business logs, etc. Web Crawlers: Collect domain-relevant web content using crawling technologies. Data Scale It is recommended to use at least hundreds of millions to billions of tokens to ensure the model can thoroughly learn domain knowledge. When data volume is insufficient, model performance may be limited. Data augmentation methods are suggested to supplement the data. Data Processing Data Preprocessing\nUniform Formatting: Process large volumes of unlabeled corpora from multiple data sources to ensure consistent formatting. It is recommended to use efficient storage formats like Parquet to improve data reading and processing efficiency. Data Deduplication\nDetection Methods: Use algorithms such as MinHash, SimHash, or cosine similarity for approximate duplicate detection. Granularity of Processing: Choose to deduplicate at the sentence, paragraph, or document level, adjusting flexibly based on task requirements. Similarity Threshold: Set a reasonable similarity threshold (e.g., 0.9) to remove texts with duplication above the threshold, ensuring data diversity. Data Cleaning\nText Filtering: Remove garbled text, spelling errors, and low-quality text by combining rule-based methods and model scorers (e.g., BERT/RoBERTa). Formatting Processing: Prefer using JSON format to handle data, ensuring the accuracy of special formats like code, Markdown, and LaTeX. Data Anonymization\nPrivacy Protection: Anonymize or remove sensitive information such as names, phone numbers, emails, passwords, etc., to ensure data compliance. Filtering Non-Compliant Content: Remove data blocks containing illegal, pornographic, or racially discriminatory content. Data Mixing and Balancing\nProportion Control: For example, combine 70% domain-specific data with 30% general data to prevent the model from forgetting general capabilities. Task Types: Ensure the data includes various task types such as code generation, Q\u0026amp;A dialogue, document summarization, multi-turn conversations, and mathematical reasoning. Data Sequencing\nProgressive Guidance: Use Curriculum Learning to start training with simple, clean data and gradually introduce more complex or noisy data, optimizing the model\u0026rsquo;s learning efficiency and convergence path. Semantic Coherence: Utilize In-Context Pretraining techniques to concatenate semantically similar documents, enhancing contextual consistency and improving the model\u0026rsquo;s depth of semantic understanding and generalization ability. Supervised Fine-Tuning Data Data Format Adopt Alpaca or Vicuna styles, such as single-turn and multi-turn dialogues structured as [instruction, input, output].\nScale: From thousands to hundreds of thousands, depending on project requirements and computational resources. Quality: Ensure high-quality and diverse data to prevent the model from learning errors or biases. Data Construction During the data construction process, we first collect daily business data and collaboratively build foundational questions with business experts. Subsequently, we use large language models for data augmentation to enhance data diversity and robustness. The specific data augmentation strategies are as follows:\nData Augmentation Strategies Diverse Expressions\nRewrite existing data using large language models through synonym replacement and syntactic transformations to increase data diversity.\nRobustness Enhancement\nCreate prompts containing spelling errors, mixed languages, and other input variations to simulate real-world scenarios while ensuring high-quality generated answers.\nKnowledge Distillation\nUtilize large language models like GPT-4 and Claude for knowledge distillation to generate Q\u0026amp;A pairs that meet requirements.\nComplex Task Design\nManually design high-quality data for complex scenarios (e.g., multi-turn dialogues, logical reasoning) to cover the model\u0026rsquo;s capability boundaries.\nData Generation Pipeline\nBuild an automated data generation pipeline that integrates data generation, filtering, formatting, and validation to improve overall efficiency.\nKey Points Task Type Annotation: Clearly annotate each data entry with its task type to facilitate subsequent fine-grained analysis and tuning. Multi-Turn Dialogues and Topic Switching: Construct data that captures contextual coherence and topic transitions in multi-turn dialogues to ensure the model learns the ability to handle topic switching and maintain contextual relevance. Chain of Thought (CoT) Strategy: For classification and reasoning tasks, generate procedural answers using CoT to improve accuracy. Data Flywheel: Continuously collect real user queries after deployment, iterating data based on real needs; regularly clean the data to ensure quality and diversity. Preference Data Data Format Triple Structure: [prompt, chosen answer, rejected answer] Annotation Details: Multi-Model Sampling: Generate answers using multiple models at different training stages or with different data ratios to increase data diversity. Editing and Optimization: Annotators can make slight modifications to the chosen answers to ensure answer quality. Sampling Strategies Multi-Model Sampling: Deploy multiple versions of the model to generate diverse answers for the same prompt. Comparative Annotation: Use manual or automated systems to compare generated answers and select superior answer pairs. Key Points Data Diversity and Coverage: Ensure preference data covers various scenarios and tasks to prevent the model from underperforming in specific contexts. High-Quality Annotation: The quality of preference data directly affects the model\u0026rsquo;s alignment, requiring accurate and consistent annotations. Training Process A complete training process for a domain-specific large language model typically includes Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO) as the three main steps, ultimately achieving model deployment and continuous optimization.\nComparison of Three Methods Training Method Overview Training Method Main Objective Data Requirements Typical Application Scenarios Continued Pre-Training (CPT) Continue pre-training on large-scale unsupervised corpora to inject new domain knowledge Large amounts of unlabeled text (at least hundreds of millions to billions of tokens) Supplementing domain knowledge, such as specialized texts in law, medicine, finance, etc. Supervised Fine-Tuning (SFT) Fine-tune on supervised labeled data to strengthen specific tasks and instruction execution capabilities Customized labeled data (instruction/dialog pairs), ranging from thousands to hundreds of thousands Various specific tasks, such as code generation, Q\u0026amp;A, text rewriting, complex instruction execution, etc. Direct Preference Optimization (DPO) Optimize model outputs to align with human preferences using preference data (chosen vs. rejected) Preference data: [prompt, chosen, rejected](relatively smaller scale) Aligning with human feedback, such as response style, compliance, safety, etc. Advantages and Challenges Continued Pre-Training (CPT) Advantages:\nBetter domain coverage, comprehensively enhancing the model\u0026rsquo;s understanding and generation capabilities in specific domains. No need for additional manual annotation. Challenges/Limitations:\nRequires a large volume of high-quality domain data. High training costs, necessitating massive computational power and time. May introduce domain biases, necessitating careful handling of data quality and distribution. Supervised Fine-Tuning (SFT) Advantages:\nQuickly acquires task execution capabilities. Significantly improves accuracy in specific scenarios. Challenges/Limitations:\nHigh data annotation costs. Requires careful selection of labeled data to avoid overfitting. Fine-tuning may weaken the model\u0026rsquo;s generality. Direct Preference Optimization (DPO) Advantages:\nNo need to train a separate Reward Model. Requires less data and computational resources to achieve similar or better results compared to PPO. Challenges/Limitations:\nRequires reliable preference annotations. Continues to need more preference data for complex and diverse scenarios. Easily constrained by the distribution of preference data. General Training Tips and Technical Details When performing CPT, SFT, and DPO, there are numerous general training tips and technical details. The following sections uniformly describe these general aspects for better understanding and application.\nData Processing and Preparation Data Quality: Regardless of CPT, SFT, or DPO, data quality is crucial. Ensure data accuracy, unambiguity, and diversity. Data Formatting: Consistent data formats simplify the training process. For example, using JSON or other structured formats to store training data. Data Augmentation: Increase data diversity and improve the model\u0026rsquo;s generalization ability through methods like LLM rewriting and optimization. Learning Rate and Optimization Learning Rate Settings: Typically use a smaller learning rate than during pre-training, such as reducing from 3e-4 to 3e-5, depending on the task and data volume. Learning Rate Scheduling: Use warm-up strategies (e.g., linearly increasing for the first 10% of steps), followed by linear decay or cosine annealing to ensure a smooth training process. Optimizer Selection: Choose suitable optimizers based on model size and hardware resources, such as AdamW. Training Strategies Full-Parameter Fine-Tuning: When resources permit, prioritize full-parameter fine-tuning to ensure the model fully captures new knowledge. Parameter-Efficient Fine-Tuning (PEFT): Methods like LoRA are suitable for scenarios with limited computational resources by freezing some parameters and adding adapters for efficient fine-tuning. Mixed Precision Training: Use bf16 or fp16 on supported GPUs to reduce memory usage and increase training speed. Training Stability: Employ techniques such as gradient clipping, regularization, dropout, and weight decay to prevent gradient explosion and model overfitting. Flash Attention: Utilize Flash Attention to optimize the computation efficiency of the attention mechanism, enhancing training speed and reducing memory usage. Monitoring and Tuning Convergence Monitoring: Continuously monitor loss curves on training and validation sets to ensure the model is converging properly. Adjust learning rates and other hyperparameters as needed. Checkpoint: Regularly save checkpoints to prevent loss of all training progress due to unexpected interruptions. Early Stopping: Prevent model overfitting by stopping training at an appropriate time and saving the best model state. Model Evaluation: Conduct periodic evaluations during training to ensure model performance meets expectations. Continued Pre-Training (CPT) Objective Inject new domain knowledge into the base model by continuing pre-training on a large volume of domain-specific unsupervised data, enhancing the model\u0026rsquo;s understanding and generation capabilities in the specific domain.\nTraining Tips Streaming Data Loading\nImplement streaming data loading to dynamically read data during training, preventing memory overflows and training interruptions. Full-Parameter Fine-Tuning\nTypically, update all model parameters during training to ensure comprehensive knowledge acquisition. Compared to parameter-efficient fine-tuning methods (e.g., LoRA), full-parameter fine-tuning offers better domain knowledge injection, especially when computational resources are abundant. It is recommended to prioritize full-parameter fine-tuning under such conditions. Supervised Fine-Tuning (SFT) Objective Enhance the model\u0026rsquo;s practicality and accuracy by training it on high-quality labeled data to perform specific tasks such as code generation, code repair, and complex instruction execution.\nTraining Tips Number of Epochs\nTypically, 1 to 4 epochs are sufficient to observe significant effects when data volume is adequate. If data volume is insufficient, consider increasing the number of epochs while being mindful of overfitting risks. Data augmentation is recommended in such cases. Data Augmentation and Diversity\nEnsure training data covers a variety of task types and instruction expressions to improve the model\u0026rsquo;s generalization ability. Include multi-turn dialogues and robustness data to enhance the model\u0026rsquo;s capability to handle real user scenarios. Direct Preference Optimization (DPO) Objective Optimize model outputs to better align with human expectations and needs, including response style, safety, and readability, by leveraging user feedback and preference data.\nCharacteristics of DPO Direct Optimization\nDoes not require training a separate Reward Model. Instead, directly performs contrastive learning on (chosen, rejected) data pairs.\nEfficiency\nCompared to PPO, DPO requires less data and computational resources to achieve similar or better results.\nDynamic Adaptation\nThe model can immediately adapt whenever new data is available without the need to retrain a Reward Model.\nTraining Tips Collecting Preference Data\nDeploy multiple models at different training stages or with different data ratios to generate diverse responses. Annotate chosen and rejected answer pairs through manual or automated means to ensure data diversity and quality. Contrastive Learning\nOptimize model parameters by maximizing the probability of chosen answers and minimizing the probability of rejected answers. Iterative Optimization\nContinuously collect user feedback, generate new preference data, and perform iterative training to gradually enhance model performance. Implement a data flywheel mechanism to achieve ongoing model evolution and optimization. Common Issues and Solutions Repetitive Outputs\nIssue: The model generates repetitive content, continuously printing without stopping.\nSolutions:\nData Deduplication and Cleaning: Ensure training data does not contain a large amount of repetitive content. Check EOT (End-of-Token) Settings: Prevent the model from continuously generating without stopping. Align via SFT/DPO: Optimize model output quality. Adjust Decoding Strategies: Increase parameters like top_k, repetition penalty, and temperature. Catastrophic Forgetting\nIssue: The model forgets its original general capabilities during fine-tuning, effectively overfitting to the new dataset and causing excessive changes to the original model parameter space.\nSolutions:\nMix in Some General Data: Maintain the model’s general capabilities. Lower the Learning Rate: Reduce the impact on existing knowledge. Increase Dropout Rate and Weight Decay: Prevent overfitting. Use Parameter-Efficient Fine-Tuning Methods like LoRA: Avoid large-scale parameter updates. Utilize RAG Assistance: Combine with external knowledge bases to enhance model performance. Chat Vector: Quickly inject conversational and general capabilities into the model through simple arithmetic operations on model weights. Insufficient Understanding of Entity Relationships and Reasoning Paths\nIssue: The model struggles to correctly understand complex entity relationships and reasoning paths.\nSolutions:\nIntroduce Chain-of-Thought (CoT) Data and Enhanced Reasoning Training: Improve the model\u0026rsquo;s capabilities through step-by-step reasoning training, combined with Reinforcement Fine-Tuning and o1/o3 training methods. Expand Training Data Coverage: Incorporate more diverse scenarios containing complex entity relationships and reasoning paths. Combine with Knowledge Graph Modeling: Use GraphRAG to strengthen the model\u0026rsquo;s understanding and reasoning abilities regarding entity relationships. Model Deployment and Evaluation Deployment Inference Frameworks\nollama: Local inference deployment based on llama.cpp, enabling quick startups. vLLM: Optimized for high concurrency and inference throughput in multi-user scenarios. Quantization: Quantize the model to 8-bit or 4-bit to further reduce inference costs and improve deployment efficiency. Integrate RAG \u0026amp; Agents\nRAG: Combine with a vector knowledge base to retrieve relevant documents or code snippets in real-time, assisting the model in generating more accurate responses. Agents: Utilize Function Calls or multi-turn dialogue mechanisms to enable the model to invoke external tools or perform complex reasoning, enhancing interactivity and practicality. Langgraph: Encapsulate RAG and multi-agent workflows to build customized dialogue systems or automated code generation platforms. Evaluation Evaluation Metrics\nCPT Phase: Use domain-specific test sets to evaluate perplexity (PPL) or cross-entropy, measuring the model\u0026rsquo;s mastery of new knowledge. SFT/DPO Phase: Human or Model Evaluation: Assess the accuracy, coherence, readability, and safety of responses through human ratings or automated tools. Code Generation: Build a large-scale unit test set to evaluate the pass@k metric, measuring the correctness rate of code generation. General Capabilities: Test the model on common benchmarks (e.g., MMLU, CMMLU) to ensure minimal performance degradation on general tasks. Decoding Hyperparameters\nConsistency: Maintain consistent decoding parameters such as top_k, top_p, temperature, and max_new_tokens during evaluation to ensure comparability of results. Grid Search: When computational resources permit, evaluate different combinations of decoding parameters to select the optimal configuration. Data Flywheel and Continuous Iteration Data Flywheel Mechanism\nReal-Time Collection of User Logs\nCollect real user prompts and generated responses online, covering diverse usage scenarios and task types. Automated or Manual Annotation\nAnnotate collected user prompts and responses with preferences, generating new (chosen, rejected) data pairs. Iterative Training\nIncorporate newly generated preference data into the next round of SFT/DPO training to continuously optimize response quality and user experience. Robustness Data\nInclude data with spelling errors, mixed languages, vague instructions, etc., to enhance the model’s robustness and ability to handle real-world scenarios. Continuous Optimization\nFeedback Loop: Utilize user feedback to continuously improve training data and model performance, achieving self-optimization and evolution of the model. Multi-Model Collaboration: Deploy multiple versions of the model to generate diverse responses, enhancing the model\u0026rsquo;s comprehensive capabilities through contrastive learning. Integrating Intent Recognition and Multi-Agent Reasoning Use an intent classification model to allow the large model to determine the category of user input intent. Based on the mapping between intent categories and context types, supervise the reasoning path, and then perform multi-way retrieval based on the reasoning path. Provide this information to the trained model to generate the final result.\nConclusion Through the combination of Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO), it is possible to effectively inject domain-specific knowledge into base large models, constructing closed-source LLMs capable of efficiently solving business problems. The key steps are as follows:\nData Preparation\nHigh-quality data collection, cleaning, deduplication, and categorization to ensure data diversity and accuracy. Implement data anonymization strategies to protect privacy and ensure compliance. Model Training\nUse CPT to inject domain knowledge, SFT to learn specific task patterns, and DPO to optimize model outputs to align with human preferences and safety. Leverage efficient parallel training frameworks and hyperparameter tuning techniques to enhance training efficiency and resource utilization. Deployment and Evaluation\nEmploy efficient inference frameworks, integrating RAG and Agents for knowledge enhancement and functional extension. Conduct multi-dimensional evaluations to ensure the model performs as expected at each stage. Continuous Iteration\nBuild a data flywheel to continuously collect user feedback and optimize training data and model performance. Integrate RAG and Agents to achieve ongoing improvement and expansion of model capabilities. Ultimately, through a systematic process and technical measures, it is possible to construct an AI system with not only profound domain knowledge but also the flexibility to handle complex business requirements over its lifecycle.\nReferences DeepSpeed Megatron-LM ollama vLLM GraphRAG The Llama 3 Herd of Models ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages Evaluating Large Language Models Trained on Code Direct Preference Optimization: Your Language Model is Secretly a Reward Model Citation Citation: To reproduce or cite the content of this article, please acknowledge the original author and source.\nCited as:\nYue Shui. (Jan 2025). Building Domain-Specific LLMs.\nhttps://syhya.github.io/posts/2025-01-05-build-domain-llm\nOr\n@article{syhya2024domainllm, title = \u0026#34;Building Domain-Specific LLMs\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-05-build-domain-llm/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-01-05-domain-llm-training/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eWith the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.\u003c/p\u003e","title":"Building Domain-Specific LLMs"},{"content":"Rent a GPU or Buy Your Own? Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\nAdvantages of Renting GPUs:\nNo high upfront hardware costs Elastic scalability according to project needs Cloud vendors typically provide data compliance and security assurances, reducing hardware maintenance concerns Advantages of Buying GPUs:\nLower total cost if used extensively over the long term Higher privacy and control for in-house data and models Hardware can be upgraded or adjusted at any time, offering more flexible deployment Personal Suggestions\nIf you have a limited budget or are just starting out, use Colab, Kaggle, or cloud-based GPU solutions first. Once computing needs and privacy requirements increase, consider building your own multi-GPU server or renting a multi-node, multi-GPU cluster. Background In September 2023, to continue my exploration and research on Large Language Models (LLMs) outside of work, I assembled a dual RTX 4090 personal AI server. It has been running for nearly a year, and here are some observations:\nNoise: Placed under my desk, the fans can get quite loud under full load. However, noise levels are acceptable during inference or moderate loads. Inference Performance: With a total of 48GB of VRAM, and by using 4-bit quantization, it can handle 70B-level models (e.g., Llama 70B, Qwen 72B). Training Performance: By using DeepSpeed with distributed and offload techniques (ZeRO-3 + CPU offload), I can finetune around 34B parameter models (e.g., CodeLlama 34B). Cost-Effectiveness: For personal or small-team experiments and small-to-medium scale model training, this configuration is quite practical. However, full-scale training of very large models (hundreds of billions of parameters) still requires more professional GPUs (e.g., multi-GPU A100 or H100 clusters). Below is an illustration of VRAM requirements for various model sizes and training approaches (referenced from LLaMA-Factory):\nAssembly Strategy \u0026amp; Configuration Details The total budget is around 40,000 RMB (~6,000 USD). The final build list is as follows (for reference only):\nComponent Model Price (RMB) GPU RTX 4090 * 2 25098 Motherboard + CPU AMD R9 7900X + MSI MPG X670E CARBON 5157.55 Memory Corsair 48GB * 2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + Samsung 990PRO 4TB 4587 PSU Corsair AX1600i 2699 Fans Phanteks T30 120mm P * 6 1066.76 CPU Cooler Thermalright FC140 BLACK 419 Chassis Phanteks 620PC Full Tower 897.99 GPU Riser Cable Phanteks FL60 PCI-E4.0 *16 399 Total: ~ 42,723.3 RMB\nGPU Selection For large-scale model research, floating-point performance (TFLOPS) and VRAM capacity are the most critical metrics. Professional GPUs (A100, H100, etc.) boast higher VRAM and NVLink support but can easily cost hundreds of thousands of RMB, which is prohibitive for most individual users. According to Tim Dettmers, the RTX 4090 offers excellent performance-per-dollar and supports features like BF16 and Flash Attention, making it a high-value option.\nCooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling Cooling Method Advantages Disadvantages Best For Blower Fan Compact form factor; good for multi-GPU setups Loud noise, generally weaker cooling Server racks, dense multi-GPU deployments Air-Cooling Good balance of performance and noise; easy upkeep Cards are often large, require space Home or personal research (with enough space) Liquid-Cooling Excellent cooling, quieter under full load Risk of leaks, higher cost Extreme quiet needs or heavy overclocking Home Setup Recommendation: Air-cooled GPUs are typically the best balance of cooling efficiency, noise, and maintenance. They are generally more user-friendly compared to blower or liquid-cooled alternatives.\nCPU \u0026amp; Motherboard In deep learning scenarios, the CPU handles data preprocessing, pipeline scheduling, and multi-process/thread management, ensuring high-throughput, low-latency data delivery to the GPUs. Thus, the CPU’s main requirements include sufficient PCIe lanes and robust multi-threaded performance.\nIntel: 13th/14th Generation i9 (e.g., 13900K) offers 20 primary PCIe lanes, enough for dual GPUs at x8 + x8. AMD: The Ryzen 7000/9000 series (e.g., 7950X) provides 28 (24 usable) PCIe lanes and also supports dual GPUs at x8 + x8, with enough bandwidth for multiple M.2 SSDs. MSI MPG X670E CARBON Motherboard Expandability: Supports PCIe 5.0 and DDR5 memory, offering ample future-proofing. Stability: High-quality power delivery, ensuring stable operation for both CPU and multiple GPUs. Rich I/O: Supports multiple M.2 SSDs and USB4 for various usage scenarios. AMD Ryzen 9 7900X Highlights Cores \u0026amp; Threads: 12 cores, 24 threads, excellent for data preprocessing and multitasking in deep learning workloads. PCIe Bandwidth: Provides 28 (24 usable) PCIe 5.0 lanes to support dual GPUs at x8 + x8 while also powering high-speed M.2 SSDs. Power Efficiency: Built on the Zen 4 architecture, delivering outstanding performance-to-power ratio for high-performance computing tasks. Key Motherboard Considerations Physical Layout RTX 4090 cards are huge, often occupying multiple slots. Confirm the board can hold two such cards simultaneously. If space or thermal conflicts arise, use a riser cable for vertical placement of the second GPU. PCIe Lane Splitting Ensure the motherboard can run two PCIe 4.0 x8 + x8 slots. Avoid a setup ending up as x16 + x2, which severely limits the second GPU’s bandwidth and can lead to a significant performance bottleneck, especially critical in large model training. Expandability With two GPUs installed, you still need enough M.2 slots and external ports for additional storage or peripherals. After evaluating expandability, performance, and cost-effectiveness, I chose the AMD Ryzen 9 7900X paired with the MSI MPG X670E CARBON. A GPU riser cable resolved the slot conflicts caused by the thickness of dual RTX 4090s.\nBIOS Setup Recommendations Memory Optimization Enable XMP/EXPO (Intel/AMD) to boost memory clock speeds and bandwidth. Overclocking If additional performance is needed, enable PBO (Precision Boost Overdrive) or Intel Performance Tuning and monitor system stability. Thermals \u0026amp; Stability Avoid extreme overclocking and keep temperatures under control to maintain system stability. Memory During deep learning training, large amounts of system memory are used for data loading and optimizer states (especially in multi-GPU scenarios with Zero-stage parallelism). Aim for at least 2× the total GPU VRAM capacity. This build uses 48GB * 2 (96GB in total), sufficient for everyday multitasking and distributed training without frequent swapping.\nStorage Prefer M.2 NVMe SSDs: They offer superior read/write performance, critical for loading large model weights, caching intermediate files, and storing training logs. Capacity ≥ 2TB: Model files continue to grow in size. 2TB can fill up quickly; consider 4TB or more depending on your needs. SSD Brands: Samsung, SK Hynix, and Western Digital have reliable high-end product lines. Power Supply Dual RTX 4090s can draw 900W–1000W under full load, and you also need overhead for the CPU, motherboard, and storage. Generally, 1,500W+ Platinum or Titanium PSUs are recommended to ensure stable power delivery and mitigate voltage fluctuations.\nI opted for a Corsair AX1600i (digital PSU), which provides real-time power monitoring and sufficient headroom.\nCooling \u0026amp; Fans I chose an air-cooling setup:\nCPU Cooler: Thermalright FC140, a dual-tower air cooler offering solid thermal performance with relatively low noise. Case Fans: Phanteks T30 120mm * 6, maintaining slightly positive internal pressure to ensure proper airflow across the GPUs and power delivery modules. For prolonged high-load training (such as distributed training of large models), chassis airflow management and adequate fans are crucial. Monitor CPU, GPU, and VRM temperatures regularly and adjust fan curves as needed.\nAdvanced Cooling\nFor even quieter operation, consider a Hybrid or partial liquid-cooling solution, along with finely tuned fan curves. Regularly cleaning dust filters, using dust guards, and refreshing thermal compound can also help improve thermals and stability. Chassis Because the RTX 4090 is massive, and two of them stacked requires ample internal space and airflow, a full-tower chassis is recommended for better cable routing and thermal design. I went with the Phanteks 620PC, which offers ample space and built-in cable management.\nBelow is a sample photo of the completed build:\nSystem \u0026amp; Software Environment Operating System: Linux (e.g., Ubuntu 22.04 LTS) is highly recommended due to better compatibility and support for CUDA, NVIDIA drivers, and popular deep learning frameworks. The general setup flow:\nOS Installation: Ubuntu or another Linux distribution. NVIDIA Driver Installation: Make sure nvidia-smi detects both 4090 GPUs correctly:\nCUDA Toolkit: Verify via nvcc -V:\ncuDNN: Ensure your deep learning frameworks can leverage GPU-accelerated kernels for convolution, RNN, etc. Framework Testing: Use PyTorch, TensorFlow, or JAX to confirm basic inference and training functionality. Docker Containerization: With nvidia-container-toolkit, containers can directly access GPU resources, eliminating host-environment conflicts. For multi-node, multi-GPU setups, consider Kubernetes, Ray, or Slurm for cluster scheduling and resource management. Recommended Tools \u0026amp; Frameworks Training Frameworks\nLLaMA-Factory: Offers user-friendly packaging for large language model training and inference. Great for beginners. DeepSpeed: Provides distributed training for large models, with multiple parallelization strategies and optimizations. Megatron-LM: NVIDIA’s official large-scale language model training framework, ideal for multi-node, multi-GPU scenarios. Monitoring \u0026amp; Visualization\nWeights \u0026amp; Biases or TensorBoard: Real-time monitoring of loss, learning rate, etc., with remote-friendly UI. Inference Tools\nollama: Based on llama.cpp, easy local inference setup. vLLM: Optimized for high-concurrency, multi-user scenarios, delivering superior throughput. Framework ollama vLLM Function Simple local LLM deployment High-concurrency / high-throughput LLM Concurrent Performance drops as concurrency increases Handles higher concurrency with better TPS 16 Threads ~17s/req ~9s/req Throughput Slower token generation speeds ~2× faster token generation Max Concur. Performance deteriorates over 32 threads Remains stable under large concurrency Use Cases Personal or low-traffic apps Enterprise or multi-user high concurrency WebUI\nOpen-WebUI: A user-friendly, web-based solution that integrates multiple AI backends (ollama, OpenAI API, etc.), handy for rapid prototyping and visualization. Advanced Suggestions Development \u0026amp; Debugging Efficiency\nUse SSH for remote development, and create custom Docker images to reduce setup overhead. Quantization \u0026amp; Pruning\nTechniques like 4-bit or 8-bit quantization and pruning can reduce model size and VRAM usage, thereby improving inference performance. Mixed-Precision Training\nSwitch to BF16 or FP16 training to accelerate training speed, combined with gradient scaling (GradScaler) to maintain numerical stability. CPU Coordination\nEnhance data loading using multi-threading, multi-processing, or RAM disks for streaming large pretraining datasets. Multi-Node Cluster Deployment\nConnect nodes via InfiniBand or high-speed Ethernet and use Kubernetes for efficient resource scheduling. Summary With the above configuration and methodology, I successfully built a dual RTX 4090 deep learning workstation. It excels at inference and small to medium scale fine-tuning scenarios—ideal for individuals or small teams focusing on LLM research or application development. This setup strikes a balance between cost-effectiveness and flexibility. However, if you plan to train massive models (hundreds of billions of parameters) in a full-parameter regime, you will still need more GPUs (e.g., multi-GPU A100 or H100 clusters).\nFrom personal experience, dual 4090 GPUs provide sufficient performance within a reasonable budget, meeting the majority of small-to-medium-scale R\u0026amp;D needs—a solid option for qualified individuals or teams to consider.\nReferences Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe lane specs AMD R5 7600X PCIe lane specs MSI MPG X670E CARBON Specifications nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI Copyright \u0026amp; Citation Disclaimer: The configurations, prices, and recommendations in this article are for technical discussion and research purposes only. Actual purchases and deployments should be carefully evaluated based on individual budgets and requirements. The author assumes no liability for any direct or indirect consequences resulting from following or adapting the information provided herein.\nCitation: When reposting or referencing this content, please credit the original author and source.\nCited as:\nYue Shui. (Dec 2024). Building a Home Deep Learning Rig with Dual RTX 4090 GPUs. https://syhya.github.io/posts/2024-12-21-build-gpu-server\nOr\n@article{syhya2024build, title = \u0026#34;Building a Home Deep Learning Rig with Dual RTX 4090 GPUs\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Dec\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2024-12-21-build-gpu-server/\u0026#34; ","permalink":"https://syhya.github.io/posts/2024-12-21-build-gpu-server/","summary":"\u003ch2 id=\"rent-a-gpu-or-buy-your-own\"\u003eRent a GPU or Buy Your Own?\u003c/h2\u003e\n\u003cp\u003eBefore setting up a deep learning environment, consider \u003cstrong\u003eusage duration\u003c/strong\u003e, \u003cstrong\u003ebudget\u003c/strong\u003e, \u003cstrong\u003edata privacy\u003c/strong\u003e, and \u003cstrong\u003emaintenance overhead\u003c/strong\u003e. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\u003c/p\u003e","title":"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs"}]