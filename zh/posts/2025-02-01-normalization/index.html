<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>深度学习中的归一化 | Yue Shui 博客</title><meta name=keywords content="AI,NLP,Deep Learning,Normalization,Residual Connection,ResNet,Batch Normalization,Layer Normalization,Weight Normalization,RMS Normalization,Pre-Norm,Post-Norm,LLM"><meta name=description content="引言
在深度学习中，网络架构的设计对模型的性能和训练效率有着至关重要的影响。随着模型深度的增加，训练深层神经网络面临诸多挑战，如梯度消失和梯度爆炸问题。为了应对这些挑战，残差连接和各种归一化方法被引入并广泛应用于现代深度学习模型中。本文将首先介绍残差连接和两种架构，分别是 pre-norm 和 post-norm。随后介绍四种常见的方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析为何当前主流的大模型倾向于采用 RMSNorm 与 Pre-Norm 结合的架构。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2025-02-01-normalization/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-02-01-normalization/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-02-01-normalization/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2025-02-01-normalization/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="深度学习中的归一化"><meta property="og:description" content="引言 在深度学习中，网络架构的设计对模型的性能和训练效率有着至关重要的影响。随着模型深度的增加，训练深层神经网络面临诸多挑战，如梯度消失和梯度爆炸问题。为了应对这些挑战，残差连接和各种归一化方法被引入并广泛应用于现代深度学习模型中。本文将首先介绍残差连接和两种架构，分别是 pre-norm 和 post-norm。随后介绍四种常见的方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析为何当前主流的大模型倾向于采用 RMSNorm 与 Pre-Norm 结合的架构。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-01T12:00:00+08:00"><meta property="article:modified_time" content="2025-08-25T17:41:19+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Normalization"><meta property="article:tag" content="Residual Connection"><meta property="article:tag" content="ResNet"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="深度学习中的归一化"><meta name=twitter:description content="引言
在深度学习中，网络架构的设计对模型的性能和训练效率有着至关重要的影响。随着模型深度的增加，训练深层神经网络面临诸多挑战，如梯度消失和梯度爆炸问题。为了应对这些挑战，残差连接和各种归一化方法被引入并广泛应用于现代深度学习模型中。本文将首先介绍残差连接和两种架构，分别是 pre-norm 和 post-norm。随后介绍四种常见的方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析为何当前主流的大模型倾向于采用 RMSNorm 与 Pre-Norm 结合的架构。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"深度学习中的归一化","item":"https://syhya.github.io/zh/posts/2025-02-01-normalization/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"深度学习中的归一化","name":"深度学习中的归一化","description":"引言 在深度学习中，网络架构的设计对模型的性能和训练效率有着至关重要的影响。随着模型深度的增加，训练深层神经网络面临诸多挑战，如梯度消失和梯度爆炸问题。为了应对这些挑战，残差连接和各种归一化方法被引入并广泛应用于现代深度学习模型中。本文将首先介绍残差连接和两种架构，分别是 pre-norm 和 post-norm。随后介绍四种常见的方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析为何当前主流的大模型倾向于采用 RMSNorm 与 Pre-Norm 结合的架构。\n","keywords":["AI","NLP","Deep Learning","Normalization","Residual Connection","ResNet","Batch Normalization","Layer Normalization","Weight Normalization","RMS Normalization","Pre-Norm","Post-Norm","LLM"],"articleBody":"引言 在深度学习中，网络架构的设计对模型的性能和训练效率有着至关重要的影响。随着模型深度的增加，训练深层神经网络面临诸多挑战，如梯度消失和梯度爆炸问题。为了应对这些挑战，残差连接和各种归一化方法被引入并广泛应用于现代深度学习模型中。本文将首先介绍残差连接和两种架构，分别是 pre-norm 和 post-norm。随后介绍四种常见的方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析为何当前主流的大模型倾向于采用 RMSNorm 与 Pre-Norm 结合的架构。\n残差连接 残差连接（Residual Connection） 是深度神经网络中的一项关键创新，它构成了残差网络（ResNet）(He, et al., 2015) 的核心。残差连接是一种重要的架构设计，目的是缓解深层网络训练中的梯度消失问题，并促进信息在网络中的流动。它通过引入快捷路径（Shortcut/Skip Connection），允许信息直接从浅层传递到深层，从而增强模型的表达能力和训练稳定性。\nFig. 1. Residual learning: a building block. (Image source: He, et al., 2015)\n在标准的残差连接中，输入 $x_l$ 经过一系列变换函数 $\\text{F}(\\cdot)$ 后，与原始输入 $x_l$ 相加，形成输出 $x_{l+1}$：\n$$ x_{l+1} = x_l + \\text{F}(x_l) $$其中：\n$x_l$ 是第 $l$ 层的输入。 $\\text{F}(x_l)$ 表示由一系列非线性变换（例如卷积层、全连接层、激活函数等）组成的残差函数。 $x_{l+1}$ 是第 $l+1$ 层的输出。 使用残差连接的结构有以下几个优势：\n缓解梯度消失： 通过快捷路径直接传递梯度，有效减少梯度在深层网络中的衰减，从而更容易训练更深的模型。 促进信息流动： 快捷路径允许信息更自由地在网络层之间流动，有助于网络学习更复杂的特征表示。 优化学习过程： 残差连接使得损失函数曲面更加平滑，优化模型的学习过程，使其更容易收敛到较好的解。 提升模型性能： 在图像识别、自然语言处理等多种深度学习任务中，使用残差连接的模型通常表现出更优越的性能。 Pre-Norm 与 Post-Norm 在讨论归一化方法时，Pre-Norm 和 Post-Norm 是两个关键的架构设计选择，尤其在 Transformer 模型中表现突出。以下将详细探讨两者的定义、区别及其对模型训练的影响。\n定义 Fig. 2. (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer. (Image source: Xiong, et al., 2020)\n从上图我可以直观看到，Post-Norm 和 Pre-Norm 的主要区别在于归一化层的位置：\nPost-Norm：传统的 Transformer 架构中，归一化层（如 LayerNorm）通常位于残差连接之后。\n$$ \\text{Post-Norm}: \\quad x_{l+1} = \\text{Norm}(x_l + \\text{F}(x_l)) $$ Pre-Norm：将归一化层放在残差连接之前。\n$$ \\text{Pre-Norm}: \\quad x_{l+1} = x_l + \\text{F}(\\text{Norm}(x_l)) $$ 对比分析 特性 Post-Norm Pre-Norm 归一化位置 残差连接之后 残差连接之前 梯度流动 可能导致梯度消失或爆炸，尤其在深层模型中 梯度更稳定，有助于训练深层模型 训练稳定性 难以训练深层模型，需要复杂的优化技巧 更容易训练深层模型，减少对学习率调度的依赖 信息传递 保留了原始输入的特性，有助于信息传递 可能导致输入特征的信息被压缩或丢失 模型性能 在浅层模型或需要强正则化效果时表现更优 在深层模型中表现更好，提升训练稳定性和收敛速度 实现复杂度 实现较为直接，但训练过程可能需要更多调优 实现简单，训练过程更稳定 Pre-Norm 和 Post-Norm 在模型训练中的差异可以从梯度反向传播的角度理解：\nPre-Norm：归一化操作在前，梯度在反向传播时能够更直接地传递到前面的层，减少了梯度消失的风险。但这也可能导致每一层的实际贡献被弱化，降低模型的实际有效深度。\nPost-Norm：归一化操作在后，有助于保持每一层的输出稳定，但在深层模型中，梯度可能会逐层衰减，导致训练困难。\nDeepNet (Wang, et al., 2022) 论文表明 Pre-Norm 在极深的 Transformer 模型中能够有效训练，而 Post-Norm 难以扩展到如此深度。\n归一化方法 在深度学习中，归一化方法种类繁多，不同的方法在不同的应用场景下表现各异。下面将详细介绍四种常见的归一化方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析它们的优劣势及适用场景。\nBatch Normalization Batch Normalization (Ioffe, et al., 2015) 旨在通过标准化每一批次的数据，使其均值为0，方差为1，从而缓解内部协变量偏移（Internal Covariate Shift）的问题。其数学表达式如下：\n$$ \\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta $$其中：\n$x_i$ 为输入向量中的第 $i$ 个样本。 $\\mu_{\\text{B}}$ 为当前批次的均值： $$ \\mu_{\\text{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i $$ 其中 $m$ 为批次大小。 $\\sigma_{\\text{B}}^2$ 为当前批次的方差： $$ \\sigma_{\\text{B}}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\text{B}})^2 $$ $\\epsilon$ 为一个极小的常数，用于防止分母为零。 $\\gamma$ 和 $\\beta$ 为可学习的缩放和平移参数。 优势：\n加速训练：通过标准化加速模型的收敛速度。 正则化效果：在一定程度上减少过拟合，降低了对 Dropout 等正则化技术的依赖。 减轻梯度消失问题：有助于缓解梯度消失，提高深层网络的训练效果。 缺点：\n对小批次不友好：在批次大小较小时，均值和方差的估计可能不稳定，影响归一化效果。 依赖批次大小：需要较大的批次才能获得良好的统计量估计，限制了在某些应用场景中的使用。 在某些网络结构中应用复杂：如循环神经网络（RNN），需要特殊处理以适应时间步的依赖性。 Layer Normalization Layer Normalization (Ba, et al., 2016) 通过在特征维度上进行归一化，使得每个样本的特征具有相同的均值和方差。其数学表达式如下：\n$$ \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta $$其中：\n$x$ 为输入向量。 $\\mu_{\\text{L}}$ 为特征维度的均值： $$ \\mu_{\\text{L}} = \\frac{1}{d} \\sum_{i=1}^{d} x_i $$ 其中 $d$ 为特征维度的大小。 $\\sigma_{\\text{L}}^2$ 为特征维度的方差： $$ \\sigma_{\\text{L}}^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu_{\\text{L}})^2 $$ $\\epsilon$ 为一个极小的常数，用于防止分母为零。 $\\gamma$ 和 $\\beta$ 为可学习的缩放和平移参数。 优势：\n对批次大小不敏感：适用于小批次或动态批次大小的场景，尤其在序列模型中表现优异。 适用于多种网络结构：在循环神经网络（RNN）和 Transformer 等模型中表现良好。 简化实现：无需依赖批次统计量，简化了在分布式训练中的实现。 缺点：\n计算量较大：相比 BatchNorm，计算均值和方差的开销稍高。 可能不如 BatchNorm 提升训练速度：在某些情况下，LayerNorm 的效果可能不如 BatchNorm 显著。 Weight Normalization Weight Normalization (Salimans, et al., 2016) 通过重新参数化神经网络中的权重向量来解耦其模长（norm）和方向（direction），从而简化优化过程并在一定程度上加速训练。其数学表达式如下：\n$$ w = \\frac{g}{\\lVert v \\rVert} \\cdot v $$$$ \\text{WeightNorm}(x) = w^T x + b $$其中：\n$w$ 是重新参数化后的权重向量。 $g$ 为可学习的标量缩放参数。 $v$ 为可学习的方向向量（与原始 $w$ 维度相同）。 $\\lVert v \\rVert$ 表示 $v$ 的欧几里得范数。 $x$ 为输入向量。 $b$ 为偏置项。 优势：\n简化优化目标：单独控制权重的模长与方向，有助于加速收敛。 稳定训练过程：在某些情况下，可减少梯度爆炸或消失问题。 实现不依赖批次大小：与输入数据的批次无关，适用性更广。 缺点：\n实现复杂度：需要对网络层进行重新参数化，可能带来额外的实现成本。 与其他归一化方法结合时需谨慎：如与 BatchNorm、LayerNorm 等同用时，需要调试和实验来确定最佳组合。 RMS Normalization RMS Normalization (Zhang, et al., 2019) 是一种简化的归一化方法，通过仅计算输入向量的均方根（RMS）进行归一化，从而减少计算开销。其数学表达式如下：\n$$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$其中：\n$x$ 为输入向量。 $d$ 为特征维度的大小。 $\\epsilon$ 为一个极小的常数，用于防止分母为零。 $\\gamma$ 为可学习的缩放参数。 优势：\n计算效率高：相比 LayerNorm 需要计算均值和方差，RMSNorm 仅需计算均方根，减少了计算开销。 训练稳定性：通过归一化输入，提升了模型的训练稳定性，使其在更大的学习率下仍能稳定训练。 资源优化：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。 简化实现：RMSNorm 的实现相对简单，便于在复杂模型中集成和优化，减少了工程实现的复杂性。 缺点：\n信息损失：仅使用均方根进行归一化，可能丢失部分信息，如均值信息。 适用性有限：在某些任务中，可能不如 BatchNorm 或 LayerNorm 表现优异。 代码示例 可以参考normalization.py\n归一化方法对比 以下两个表格对比了 BatchNorm、LayerNorm、WeightNorm 和 RMSNorm 四种归一化方法的主要特性：\nBatchNorm vs. LayerNorm 特性 BatchNorm (BN) LayerNorm (LN) 计算的统计量 批量的均值和方差 每个样本的均值和方差 操作维度 对批量数据的所有样本进行归一化 对每个样本的所有特征进行归一化 适用场景 适用于大批量数据，卷积神经网络 (CNN) 适用于小批量或序列数据，RNN 或 Transformer 是否依赖批量大小 强依赖批量大小 不依赖批量大小，适用于小批量或单样本任务 可学习的参数 缩放参数 $\\gamma$ 和平移参数 $\\beta$ 缩放参数 $\\gamma$ 和平移参数 $\\beta$ 公式 $\\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta$ $\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta$ 计算复杂度 需要计算批量的均值和方差 需要计算每个样本的均值和方差 使用示例 CNN, Vision Transformers RNN, Transformer, NLP WeightNorm vs. RMSNorm 特性 WeightNorm (WN) RMSNorm (RMS) 计算的统计量 分解权重向量的模长和方向 每个样本的均方根 (RMS) 操作维度 针对权重向量的维度进行重新参数化 对每个样本的所有特征进行归一化 适用场景 适用于需要更灵活的权重控制或加速收敛的场景 适用于需要高效计算的任务，如 RNN 或 Transformer 是否依赖批量大小 不依赖批量大小，与输入数据的维度无关 不依赖批量大小，适用于小批量或单样本任务 可学习的参数 标量缩放 $g$ 和方向向量 $v$ 缩放参数 $\\gamma$ 公式 $\\text{WeightNorm}(x) = w^T x + b$ $\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma$ 计算复杂度 参数重新分解和更新，额外开销较小，但需修改网络层实现 只需计算每个样本的均方根，计算较为高效 使用示例 深度网络的全连接层、卷积层等，提升训练稳定性和收敛速度 Transformer, NLP, 高效序列任务 通过上述对比，可以看出四种归一化方法各有优劣：\nBatchNorm 在大批量数据和卷积神经网络中表现优异，但对小批量敏感。 LayerNorm 适用于各种批量大小，尤其是在 RNN 和 Transformer 中效果显著。 WeightNorm 通过重新参数化权重向量，在一定程度上简化了优化过程并加速收敛。 RMSNorm 则在需要高效计算的场景下提供了一种轻量级的替代方案。 为什么当前主流 LLM 都使用 Pre-Norm 和 RMSNorm？ 近年来，随着大规模语言模型（LLM）如 GPT、LLaMA 和 Qwen 系列等的兴起，RMSNorm 和 Pre-Norm 已成为这些模型的标准选择。\nRMSNorm 的优势 Fig. 3. RMSNorm vs. LayerNorm: A Comparison of Time Consumption (Image source: Zhang, et al., 2019)\n计算效率更高\n减少运算量：只需计算输入向量的均方根（RMS），无需计算均值和方差。 加快训练速度：实际测试中，RMSNorm 显著缩短了训练时间（如上图由 665s 降至 501s），在大规模模型训练中尤其明显。 训练更稳定\n适应更大学习率：在保持稳定性的同时，能够使用更大学习率，加速模型收敛。 保持表达能力：通过适当的缩放参数 $\\gamma$ 简化归一化过程的同时，仍能维持模型的表现。 节省资源\n降低硬件需求：更少的计算开销既能提升速度，也能减少对硬件资源的占用，适合在资源受限环境中部署。 Pre-Norm 的优势 更易训练深层模型\n稳定梯度传播：在残差连接之前进行归一化，可有效缓解梯度消失或爆炸。 减少对复杂优化技巧的依赖：即使模型很深，训练过程依然稳定。 加速模型收敛\n高效的梯度流动：Pre-Norm 使梯度更容易传递到前面的层，整体收敛速度更快。 结论 残差连接和归一化方法在深度学习模型中扮演着至关重要的角色，不同的归一化方法和网络架构设计各有其适用场景和优缺点。通过引入残差连接，ResNet 成功地训练了极深的网络，显著提升了模型的表达能力和训练效率。同时，归一化方法如 BatchNorm、LayerNorm、WeightNorm 和 RMSNorm 各自提供了不同的优势，适应了不同的应用需求。\n随着模型规模的不断扩大，选择合适的归一化方法和网络架构设计变得尤为重要。RMSNorm 由于其高效的计算和良好的训练稳定性，结合 Pre-Norm 的架构设计，成为当前主流 LLM 的首选。这种组合不仅提升了模型的训练效率，还确保了在大规模参数下的训练稳定性和性能表现。\n参考文献 [1] He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[2] Xiong, Ruibin, et al. “On layer normalization in the transformer architecture.” International Conference on Machine Learning. PMLR, 2020.\n[3] Wang, Hongyu, et al. “Deepnet: Scaling transformers to 1,000 layers.” IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).\n[4] Ioffe, Sergey. “Batch normalization: Accelerating deep network training by reducing internal covariate shift.” arXiv preprint arXiv:1502.03167 (2015).\n[5] Ba, Jimmy Lei. “Layer normalization.” arXiv preprint arXiv:1607.06450 (2016).\n[6] Salimans, Tim, and Durk P. Kingma. “Weight normalization: A simple reparameterization to accelerate training of deep neural networks.” Advances in neural information processing systems 29 (2016).\n[7] Zhang, Biao, and Rico Sennrich. “Root mean square layer normalization.” Advances in Neural Information Processing Systems 32 (2019).\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Feb 2025). 深度学习中的归一化.\nhttps://syhya.github.io/posts/2025-02-01-normalization\nOr\n@article{syhya2025normalization, title = \"深度学习中的归一化\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Feb\", url = \"https://syhya.github.io/posts/2025-02-01-normalization\" } ","wordCount":"4690","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2025-02-01T12:00:00+08:00","dateModified":"2025-08-25T17:41:19+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2025-02-01-normalization/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">深度学习中的归一化</h1><div class=post-meta><span title='2025-02-01 12:00:00 +0800 +0800'>2025-02-01</span>&nbsp;·&nbsp;10 分钟&nbsp;·&nbsp;4690 字&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;翻译:<ul class=i18n_list><li><a href=https://syhya.github.io/posts/2025-02-01-normalization/>En</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#引言>引言</a></li><li><a href=#残差连接>残差连接</a></li><li><a href=#pre-norm-与-post-norm>Pre-Norm 与 Post-Norm</a><ul><li><a href=#定义>定义</a></li><li><a href=#对比分析>对比分析</a></li></ul></li><li><a href=#归一化方法>归一化方法</a><ul><li><a href=#batch-normalization>Batch Normalization</a></li><li><a href=#layer-normalization>Layer Normalization</a></li><li><a href=#weight-normalization>Weight Normalization</a></li><li><a href=#rms-normalization>RMS Normalization</a></li><li><a href=#代码示例>代码示例</a></li><li><a href=#归一化方法对比>归一化方法对比</a><ul><li><a href=#batchnorm-vs-layernorm>BatchNorm vs. LayerNorm</a></li><li><a href=#weightnorm-vs-rmsnorm>WeightNorm vs. RMSNorm</a></li></ul></li></ul></li><li><a href=#为什么当前主流-llm-都使用-pre-norm-和-rmsnorm>为什么当前主流 LLM 都使用 Pre-Norm 和 RMSNorm？</a><ul><li><a href=#rmsnorm-的优势>RMSNorm 的优势</a></li><li><a href=#pre-norm-的优势>Pre-Norm 的优势</a></li></ul></li><li><a href=#结论>结论</a></li><li><a href=#参考文献>参考文献</a></li><li><a href=#引用>引用</a></li></ul></nav></div></details></div><div class=post-content><h2 id=引言>引言<a hidden class=anchor aria-hidden=true href=#引言>#</a></h2><p>在深度学习中，网络架构的设计对模型的性能和训练效率有着至关重要的影响。随着模型深度的增加，训练深层神经网络面临诸多挑战，如梯度消失和梯度爆炸问题。为了应对这些挑战，残差连接和各种归一化方法被引入并广泛应用于现代深度学习模型中。本文将首先介绍残差连接和两种架构，分别是 pre-norm 和 post-norm。随后介绍四种常见的方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析为何当前主流的大模型倾向于采用 <strong>RMSNorm</strong> 与 <strong>Pre-Norm</strong> 结合的架构。</p><h2 id=残差连接>残差连接<a hidden class=anchor aria-hidden=true href=#残差连接>#</a></h2><p><strong>残差连接（Residual Connection）</strong> 是深度神经网络中的一项关键创新，它构成了残差网络（ResNet）(<a href=https://arxiv.org/abs/1512.03385>He, et al., 2015</a>) 的核心。残差连接是一种重要的架构设计，目的是缓解深层网络训练中的梯度消失问题，并促进信息在网络中的流动。它通过引入快捷路径（Shortcut/Skip Connection），允许信息直接从浅层传递到深层，从而增强模型的表达能力和训练稳定性。</p><figure class=align-center><img loading=lazy src=residual_connection.png#center alt="Fig. 1. Residual learning: a building block. (Image source: He, et al., 2015)" width=70%><figcaption><p>Fig. 1. Residual learning: a building block. (Image source: <a href=https://arxiv.org/abs/1502.03167>He, et al., 2015</a>)</p></figcaption></figure><p>在标准的残差连接中，输入 $x_l$ 经过一系列变换函数 $\text{F}(\cdot)$ 后，与原始输入 $x_l$ 相加，形成输出 $x_{l+1}$：</p>$$
x_{l+1} = x_l + \text{F}(x_l)
$$<p>其中：</p><ul><li>$x_l$ 是第 $l$ 层的输入。</li><li>$\text{F}(x_l)$ 表示由一系列非线性变换（例如卷积层、全连接层、激活函数等）组成的残差函数。</li><li>$x_{l+1}$ 是第 $l+1$ 层的输出。</li></ul><p>使用残差连接的结构有以下几个优势：</p><ul><li><strong>缓解梯度消失</strong>： 通过快捷路径直接传递梯度，有效减少梯度在深层网络中的衰减，从而更容易训练更深的模型。</li><li><strong>促进信息流动</strong>： 快捷路径允许信息更自由地在网络层之间流动，有助于网络学习更复杂的特征表示。</li><li><strong>优化学习过程</strong>： 残差连接使得损失函数曲面更加平滑，优化模型的学习过程，使其更容易收敛到较好的解。</li><li><strong>提升模型性能</strong>： 在图像识别、自然语言处理等多种深度学习任务中，使用残差连接的模型通常表现出更优越的性能。</li></ul><h2 id=pre-norm-与-post-norm>Pre-Norm 与 Post-Norm<a hidden class=anchor aria-hidden=true href=#pre-norm-与-post-norm>#</a></h2><p>在讨论归一化方法时，<strong>Pre-Norm</strong> 和 <strong>Post-Norm</strong> 是两个关键的架构设计选择，尤其在 Transformer 模型中表现突出。以下将详细探讨两者的定义、区别及其对模型训练的影响。</p><h3 id=定义>定义<a hidden class=anchor aria-hidden=true href=#定义>#</a></h3><figure class=align-center><img loading=lazy src=pre_post_norm_comparison.png#center alt="Fig. 2. (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer. (Image source: Xiong, et al., 2020)" width=50%><figcaption><p>Fig. 2. (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer. (Image source: <a href=https://arxiv.org/abs/2002.04745>Xiong, et al., 2020</a>)</p></figcaption></figure><p>从上图我可以直观看到，Post-Norm 和 Pre-Norm 的主要区别在于归一化层的位置：</p><ul><li><p><strong>Post-Norm</strong>：传统的 Transformer 架构中，归一化层（如 LayerNorm）通常位于残差连接之后。</p>$$
\text{Post-Norm}: \quad x_{l+1} = \text{Norm}(x_l + \text{F}(x_l))
$$</li><li><p><strong>Pre-Norm</strong>：将归一化层放在残差连接之前。</p>$$
\text{Pre-Norm}: \quad x_{l+1} = x_l + \text{F}(\text{Norm}(x_l))
$$</li></ul><h3 id=对比分析>对比分析<a hidden class=anchor aria-hidden=true href=#对比分析>#</a></h3><table><thead><tr><th>特性</th><th>Post-Norm</th><th>Pre-Norm</th></tr></thead><tbody><tr><td><strong>归一化位置</strong></td><td>残差连接之后</td><td>残差连接之前</td></tr><tr><td><strong>梯度流动</strong></td><td>可能导致梯度消失或爆炸，尤其在深层模型中</td><td>梯度更稳定，有助于训练深层模型</td></tr><tr><td><strong>训练稳定性</strong></td><td>难以训练深层模型，需要复杂的优化技巧</td><td>更容易训练深层模型，减少对学习率调度的依赖</td></tr><tr><td><strong>信息传递</strong></td><td>保留了原始输入的特性，有助于信息传递</td><td>可能导致输入特征的信息被压缩或丢失</td></tr><tr><td><strong>模型性能</strong></td><td>在浅层模型或需要强正则化效果时表现更优</td><td>在深层模型中表现更好，提升训练稳定性和收敛速度</td></tr><tr><td><strong>实现复杂度</strong></td><td>实现较为直接，但训练过程可能需要更多调优</td><td>实现简单，训练过程更稳定</td></tr></tbody></table><p>Pre-Norm 和 Post-Norm 在模型训练中的差异可以从梯度反向传播的角度理解：</p><ul><li><p><strong>Pre-Norm</strong>：归一化操作在前，梯度在反向传播时能够更直接地传递到前面的层，减少了梯度消失的风险。但这也可能导致每一层的实际贡献被弱化，降低模型的实际有效深度。</p></li><li><p><strong>Post-Norm</strong>：归一化操作在后，有助于保持每一层的输出稳定，但在深层模型中，梯度可能会逐层衰减，导致训练困难。</p></li></ul><p><strong>DeepNet</strong> (<a href=https://arxiv.org/abs/2203.00555>Wang, et al., 2022</a>) 论文表明 Pre-Norm 在极深的 Transformer 模型中能够有效训练，而 Post-Norm 难以扩展到如此深度。</p><h2 id=归一化方法>归一化方法<a hidden class=anchor aria-hidden=true href=#归一化方法>#</a></h2><p>在深度学习中，归一化方法种类繁多，不同的方法在不同的应用场景下表现各异。下面将详细介绍四种常见的归一化方法：Batch Normalization、Layer Normalization、Weight Normalization 和 RMS Normalization，并分析它们的优劣势及适用场景。</p><h3 id=batch-normalization>Batch Normalization<a hidden class=anchor aria-hidden=true href=#batch-normalization>#</a></h3><p>Batch Normalization (<a href=https://arxiv.org/abs/1502.03167>Ioffe, et al., 2015</a>) 旨在通过标准化每一批次的数据，使其均值为0，方差为1，从而缓解内部协变量偏移（Internal Covariate Shift）的问题。其数学表达式如下：</p>$$
\text{BatchNorm}(x_i) = \gamma \cdot \frac{x_i - \mu_{\text{B}}}{\sqrt{\sigma_{\text{B}}^2 + \epsilon}} + \beta
$$<p>其中：</p><ul><li>$x_i$ 为输入向量中的第 $i$ 个样本。</li><li>$\mu_{\text{B}}$ 为当前批次的均值：
$$
\mu_{\text{B}} = \frac{1}{m} \sum_{i=1}^{m} x_i
$$
其中 $m$ 为批次大小。</li><li>$\sigma_{\text{B}}^2$ 为当前批次的方差：
$$
\sigma_{\text{B}}^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_{\text{B}})^2
$$</li><li>$\epsilon$ 为一个极小的常数，用于防止分母为零。</li><li>$\gamma$ 和 $\beta$ 为可学习的缩放和平移参数。</li></ul><p><strong>优势：</strong></p><ul><li><strong>加速训练</strong>：通过标准化加速模型的收敛速度。</li><li><strong>正则化效果</strong>：在一定程度上减少过拟合，降低了对 Dropout 等正则化技术的依赖。</li><li><strong>减轻梯度消失问题</strong>：有助于缓解梯度消失，提高深层网络的训练效果。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>对小批次不友好</strong>：在批次大小较小时，均值和方差的估计可能不稳定，影响归一化效果。</li><li><strong>依赖批次大小</strong>：需要较大的批次才能获得良好的统计量估计，限制了在某些应用场景中的使用。</li><li><strong>在某些网络结构中应用复杂</strong>：如循环神经网络（RNN），需要特殊处理以适应时间步的依赖性。</li></ul><h3 id=layer-normalization>Layer Normalization<a hidden class=anchor aria-hidden=true href=#layer-normalization>#</a></h3><p>Layer Normalization (<a href=https://arxiv.org/abs/1607.06450>Ba, et al., 2016</a>) 通过在特征维度上进行归一化，使得每个样本的特征具有相同的均值和方差。其数学表达式如下：</p>$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu_{\text{L}}}{\sqrt{\sigma_{\text{L}}^2 + \epsilon}} + \beta
$$<p>其中：</p><ul><li>$x$ 为输入向量。</li><li>$\mu_{\text{L}}$ 为特征维度的均值：
$$
\mu_{\text{L}} = \frac{1}{d} \sum_{i=1}^{d} x_i
$$
其中 $d$ 为特征维度的大小。</li><li>$\sigma_{\text{L}}^2$ 为特征维度的方差：
$$
\sigma_{\text{L}}^2 = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu_{\text{L}})^2
$$</li><li>$\epsilon$ 为一个极小的常数，用于防止分母为零。</li><li>$\gamma$ 和 $\beta$ 为可学习的缩放和平移参数。</li></ul><p><strong>优势：</strong></p><ul><li><strong>对批次大小不敏感</strong>：适用于小批次或动态批次大小的场景，尤其在序列模型中表现优异。</li><li><strong>适用于多种网络结构</strong>：在循环神经网络（RNN）和 Transformer 等模型中表现良好。</li><li><strong>简化实现</strong>：无需依赖批次统计量，简化了在分布式训练中的实现。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>计算量较大</strong>：相比 BatchNorm，计算均值和方差的开销稍高。</li><li><strong>可能不如 BatchNorm 提升训练速度</strong>：在某些情况下，LayerNorm 的效果可能不如 BatchNorm 显著。</li></ul><h3 id=weight-normalization>Weight Normalization<a hidden class=anchor aria-hidden=true href=#weight-normalization>#</a></h3><p>Weight Normalization (<a href=https://arxiv.org/abs/1602.07868>Salimans, et al., 2016</a>) 通过重新参数化神经网络中的权重向量来解耦其模长（norm）和方向（direction），从而简化优化过程并在一定程度上加速训练。其数学表达式如下：</p>$$
w = \frac{g}{\lVert v \rVert} \cdot v
$$$$
\text{WeightNorm}(x) = w^T x + b
$$<p>其中：</p><ul><li>$w$ 是重新参数化后的权重向量。</li><li>$g$ 为可学习的标量缩放参数。</li><li>$v$ 为可学习的方向向量（与原始 $w$ 维度相同）。</li><li>$\lVert v \rVert$ 表示 $v$ 的欧几里得范数。</li><li>$x$ 为输入向量。</li><li>$b$ 为偏置项。</li></ul><p><strong>优势：</strong></p><ul><li><strong>简化优化目标</strong>：单独控制权重的模长与方向，有助于加速收敛。</li><li><strong>稳定训练过程</strong>：在某些情况下，可减少梯度爆炸或消失问题。</li><li><strong>实现不依赖批次大小</strong>：与输入数据的批次无关，适用性更广。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>实现复杂度</strong>：需要对网络层进行重新参数化，可能带来额外的实现成本。</li><li><strong>与其他归一化方法结合时需谨慎</strong>：如与 BatchNorm、LayerNorm 等同用时，需要调试和实验来确定最佳组合。</li></ul><h3 id=rms-normalization>RMS Normalization<a hidden class=anchor aria-hidden=true href=#rms-normalization>#</a></h3><p>RMS Normalization (<a href=https://arxiv.org/abs/1910.07467>Zhang, et al., 2019</a>) 是一种简化的归一化方法，通过仅计算输入向量的均方根（RMS）进行归一化，从而减少计算开销。其数学表达式如下：</p>$$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2 + \epsilon}} \cdot \gamma
$$<p>其中：</p><ul><li>$x$ 为输入向量。</li><li>$d$ 为特征维度的大小。</li><li>$\epsilon$ 为一个极小的常数，用于防止分母为零。</li><li>$\gamma$ 为可学习的缩放参数。</li></ul><p><strong>优势：</strong></p><ul><li><strong>计算效率高</strong>：相比 LayerNorm 需要计算均值和方差，RMSNorm 仅需计算均方根，减少了计算开销。</li><li><strong>训练稳定性</strong>：通过归一化输入，提升了模型的训练稳定性，使其在更大的学习率下仍能稳定训练。</li><li><strong>资源优化</strong>：减少计算开销有助于在资源受限的环境中部署模型，提高训练和推理的效率。</li><li><strong>简化实现</strong>：RMSNorm 的实现相对简单，便于在复杂模型中集成和优化，减少了工程实现的复杂性。</li></ul><p><strong>缺点：</strong></p><ul><li><strong>信息损失</strong>：仅使用均方根进行归一化，可能丢失部分信息，如均值信息。</li><li><strong>适用性有限</strong>：在某些任务中，可能不如 BatchNorm 或 LayerNorm 表现优异。</li></ul><h3 id=代码示例>代码示例<a hidden class=anchor aria-hidden=true href=#代码示例>#</a></h3><p>可以参考<a href=https://github.com/syhya/syhya.github.io/blob/main/content/en/posts/2025-02-01-normalization/normalization.py>normalization.py</a></p><h3 id=归一化方法对比>归一化方法对比<a hidden class=anchor aria-hidden=true href=#归一化方法对比>#</a></h3><p>以下两个表格对比了 BatchNorm、LayerNorm、WeightNorm 和 RMSNorm 四种归一化方法的主要特性：</p><h4 id=batchnorm-vs-layernorm>BatchNorm vs. LayerNorm<a hidden class=anchor aria-hidden=true href=#batchnorm-vs-layernorm>#</a></h4><table><thead><tr><th>特性</th><th>BatchNorm (BN)</th><th>LayerNorm (LN)</th></tr></thead><tbody><tr><td><strong>计算的统计量</strong></td><td>批量的均值和方差</td><td>每个样本的均值和方差</td></tr><tr><td><strong>操作维度</strong></td><td>对批量数据的所有样本进行归一化</td><td>对每个样本的所有特征进行归一化</td></tr><tr><td><strong>适用场景</strong></td><td>适用于大批量数据，卷积神经网络 (CNN)</td><td>适用于小批量或序列数据，RNN 或 Transformer</td></tr><tr><td><strong>是否依赖批量大小</strong></td><td>强依赖批量大小</td><td>不依赖批量大小，适用于小批量或单样本任务</td></tr><tr><td><strong>可学习的参数</strong></td><td>缩放参数 $\gamma$ 和平移参数 $\beta$</td><td>缩放参数 $\gamma$ 和平移参数 $\beta$</td></tr><tr><td><strong>公式</strong></td><td>$\text{BatchNorm}(x_i) = \gamma \cdot \frac{x_i - \mu_{\text{B}}}{\sqrt{\sigma_{\text{B}}^2 + \epsilon}} + \beta$</td><td>$\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu_{\text{L}}}{\sqrt{\sigma_{\text{L}}^2 + \epsilon}} + \beta$</td></tr><tr><td><strong>计算复杂度</strong></td><td>需要计算批量的均值和方差</td><td>需要计算每个样本的均值和方差</td></tr><tr><td><strong>使用示例</strong></td><td>CNN, Vision Transformers</td><td>RNN, Transformer, NLP</td></tr></tbody></table><h4 id=weightnorm-vs-rmsnorm>WeightNorm vs. RMSNorm<a hidden class=anchor aria-hidden=true href=#weightnorm-vs-rmsnorm>#</a></h4><table><thead><tr><th>特性</th><th>WeightNorm (WN)</th><th>RMSNorm (RMS)</th></tr></thead><tbody><tr><td><strong>计算的统计量</strong></td><td>分解权重向量的模长和方向</td><td>每个样本的均方根 (RMS)</td></tr><tr><td><strong>操作维度</strong></td><td>针对权重向量的维度进行重新参数化</td><td>对每个样本的所有特征进行归一化</td></tr><tr><td><strong>适用场景</strong></td><td>适用于需要更灵活的权重控制或加速收敛的场景</td><td>适用于需要高效计算的任务，如 RNN 或 Transformer</td></tr><tr><td><strong>是否依赖批量大小</strong></td><td>不依赖批量大小，与输入数据的维度无关</td><td>不依赖批量大小，适用于小批量或单样本任务</td></tr><tr><td><strong>可学习的参数</strong></td><td>标量缩放 $g$ 和方向向量 $v$</td><td>缩放参数 $\gamma$</td></tr><tr><td><strong>公式</strong></td><td>$\text{WeightNorm}(x) = w^T x + b$</td><td>$\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d} \sum_{i=1}^{d} x_i^2 + \epsilon}} \cdot \gamma$</td></tr><tr><td><strong>计算复杂度</strong></td><td>参数重新分解和更新，额外开销较小，但需修改网络层实现</td><td>只需计算每个样本的均方根，计算较为高效</td></tr><tr><td><strong>使用示例</strong></td><td>深度网络的全连接层、卷积层等，提升训练稳定性和收敛速度</td><td>Transformer, NLP, 高效序列任务</td></tr></tbody></table><p>通过上述对比，可以看出四种归一化方法各有优劣：</p><ul><li><strong>BatchNorm</strong> 在大批量数据和卷积神经网络中表现优异，但对小批量敏感。</li><li><strong>LayerNorm</strong> 适用于各种批量大小，尤其是在 RNN 和 Transformer 中效果显著。</li><li><strong>WeightNorm</strong> 通过重新参数化权重向量，在一定程度上简化了优化过程并加速收敛。</li><li><strong>RMSNorm</strong> 则在需要高效计算的场景下提供了一种轻量级的替代方案。</li></ul><h2 id=为什么当前主流-llm-都使用-pre-norm-和-rmsnorm>为什么当前主流 LLM 都使用 Pre-Norm 和 RMSNorm？<a hidden class=anchor aria-hidden=true href=#为什么当前主流-llm-都使用-pre-norm-和-rmsnorm>#</a></h2><p>近年来，随着大规模语言模型（LLM）如 GPT、LLaMA 和 Qwen 系列等的兴起，<strong>RMSNorm</strong> 和 <strong>Pre-Norm</strong> 已成为这些模型的标准选择。</p><h3 id=rmsnorm-的优势>RMSNorm 的优势<a hidden class=anchor aria-hidden=true href=#rmsnorm-的优势>#</a></h3><figure class=align-center><img loading=lazy src=rms_norm_time_benchmark.png#center alt="Fig. 3. RMSNorm vs. LayerNorm: A Comparison of Time Consumption (Image source: Zhang, et al., 2019)" width=80%><figcaption><p>Fig. 3. RMSNorm vs. LayerNorm: A Comparison of Time Consumption (Image source: <a href=https://arxiv.org/abs/1910.07467>Zhang, et al., 2019</a>)</p></figcaption></figure><ol><li><p><strong>计算效率更高</strong></p><ul><li><strong>减少运算量</strong>：只需计算输入向量的均方根（RMS），无需计算均值和方差。</li><li><strong>加快训练速度</strong>：实际测试中，RMSNorm 显著缩短了训练时间（如上图由 <strong>665s</strong> 降至 <strong>501s</strong>），在大规模模型训练中尤其明显。</li></ul></li><li><p><strong>训练更稳定</strong></p><ul><li><strong>适应更大学习率</strong>：在保持稳定性的同时，能够使用更大学习率，加速模型收敛。</li><li><strong>保持表达能力</strong>：通过适当的缩放参数 $\gamma$ 简化归一化过程的同时，仍能维持模型的表现。</li></ul></li><li><p><strong>节省资源</strong></p><ul><li><strong>降低硬件需求</strong>：更少的计算开销既能提升速度，也能减少对硬件资源的占用，适合在资源受限环境中部署。</li></ul></li></ol><h3 id=pre-norm-的优势>Pre-Norm 的优势<a hidden class=anchor aria-hidden=true href=#pre-norm-的优势>#</a></h3><ol><li><p><strong>更易训练深层模型</strong></p><ul><li><strong>稳定梯度传播</strong>：在残差连接之前进行归一化，可有效缓解梯度消失或爆炸。</li><li><strong>减少对复杂优化技巧的依赖</strong>：即使模型很深，训练过程依然稳定。</li></ul></li><li><p><strong>加速模型收敛</strong></p><ul><li><strong>高效的梯度流动</strong>：Pre-Norm 使梯度更容易传递到前面的层，整体收敛速度更快。</li></ul></li></ol><h2 id=结论>结论<a hidden class=anchor aria-hidden=true href=#结论>#</a></h2><p>残差连接和归一化方法在深度学习模型中扮演着至关重要的角色，不同的归一化方法和网络架构设计各有其适用场景和优缺点。通过引入残差连接，ResNet 成功地训练了极深的网络，显著提升了模型的表达能力和训练效率。同时，归一化方法如 BatchNorm、LayerNorm、WeightNorm 和 RMSNorm 各自提供了不同的优势，适应了不同的应用需求。</p><p>随着模型规模的不断扩大，选择合适的归一化方法和网络架构设计变得尤为重要。<strong>RMSNorm</strong> 由于其高效的计算和良好的训练稳定性，结合 <strong>Pre-Norm</strong> 的架构设计，成为当前主流 LLM 的首选。这种组合不仅提升了模型的训练效率，还确保了在大规模参数下的训练稳定性和性能表现。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><p>[1] He, Kaiming, et al. <a href=https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf>&ldquo;Deep residual learning for image recognition.&rdquo;</a> Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</p><p>[2] Xiong, Ruibin, et al. <a href=https://arxiv.org/abs/2002.04745>&ldquo;On layer normalization in the transformer architecture.&rdquo;</a> International Conference on Machine Learning. PMLR, 2020.</p><p>[3] Wang, Hongyu, et al. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10496231">&ldquo;Deepnet: Scaling transformers to 1,000 layers.&rdquo;</a> IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).</p><p>[4] Ioffe, Sergey. <a href=https://arxiv.org/abs/1502.03167>&ldquo;Batch normalization: Accelerating deep network training by reducing internal covariate shift.&rdquo;</a> arXiv preprint arXiv:1502.03167 (2015).</p><p>[5] Ba, Jimmy Lei. <a href=https://arxiv.org/abs/1607.06450>&ldquo;Layer normalization.&rdquo;</a> arXiv preprint arXiv:1607.06450 (2016).</p><p>[6] Salimans, Tim, and Durk P. Kingma. <a href=https://proceedings.neurips.cc/paper_files/paper/2016/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf>&ldquo;Weight normalization: A simple reparameterization to accelerate training of deep neural networks.&rdquo;</a> Advances in neural information processing systems 29 (2016).</p><p>[7] Zhang, Biao, and Rico Sennrich. <a href=https://arxiv.org/abs/1910.07467>&ldquo;Root mean square layer normalization.&rdquo;</a> Advances in Neural Information Processing Systems 32 (2019).</p><h2 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h2><blockquote><p><strong>引用</strong>：转载或引用本文内容时，请注明原作者和来源。</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Feb 2025). 深度学习中的归一化.<br><a href=https://syhya.github.io/posts/2025-02-01-normalization>https://syhya.github.io/posts/2025-02-01-normalization</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025normalization</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;深度学习中的归一化&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Feb&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-02-01-normalization&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/zh/tags/nlp/>NLP</a></li><li><a href=https://syhya.github.io/zh/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/zh/tags/normalization/>Normalization</a></li><li><a href=https://syhya.github.io/zh/tags/residual-connection/>Residual Connection</a></li><li><a href=https://syhya.github.io/zh/tags/resnet/>ResNet</a></li><li><a href=https://syhya.github.io/zh/tags/batch-normalization/>Batch Normalization</a></li><li><a href=https://syhya.github.io/zh/tags/layer-normalization/>Layer Normalization</a></li><li><a href=https://syhya.github.io/zh/tags/weight-normalization/>Weight Normalization</a></li><li><a href=https://syhya.github.io/zh/tags/rms-normalization/>RMS Normalization</a></li><li><a href=https://syhya.github.io/zh/tags/pre-norm/>Pre-Norm</a></li><li><a href=https://syhya.github.io/zh/tags/post-norm/>Post-Norm</a></li><li><a href=https://syhya.github.io/zh/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/zh/posts/2025-02-03-rag/><span class=title>« 上一页</span><br><span>检索增强生成 (RAG) 技术综述（长期更新中）</span>
</a><a class=next href=https://syhya.github.io/zh/posts/2025-01-27-deepseek-r1/><span class=title>下一页 »</span><br><span>OpenAI o1复现进展：DeepSeek-R1</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习中的归一化 on x" href="https://x.com/intent/tweet/?text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%bd%92%e4%b8%80%e5%8c%96&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-01-normalization%2f&amp;hashtags=AI%2cNLP%2cDeepLearning%2cNormalization%2cResidualConnection%2cResNet%2cBatchNormalization%2cLayerNormalization%2cWeightNormalization%2cRMSNormalization%2cPre-Norm%2cPost-Norm%2cLLM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习中的归一化 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-01-normalization%2f&amp;title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%bd%92%e4%b8%80%e5%8c%96&amp;summary=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%bd%92%e4%b8%80%e5%8c%96&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-01-normalization%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习中的归一化 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-01-normalization%2f&title=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%bd%92%e4%b8%80%e5%8c%96"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习中的归一化 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-01-normalization%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习中的归一化 on whatsapp" href="https://api.whatsapp.com/send?text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%bd%92%e4%b8%80%e5%8c%96%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-01-normalization%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习中的归一化 on telegram" href="https://telegram.me/share/url?text=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%bd%92%e4%b8%80%e5%8c%96&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-01-normalization%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 深度学习中的归一化 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%ad%e7%9a%84%e5%bd%92%e4%b8%80%e5%8c%96&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-02-01-normalization%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>