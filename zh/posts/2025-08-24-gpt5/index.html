<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>gpt-oss & GPT-5 | Yue Shui 博客</title><meta name=keywords content="gpt-oss,GPT-5,MoE,Reasoning,Tool Use,OpenAI,LLM Architecture"><meta name=description content="2025 年 8 月，AI 领域迎来了 OpenAI 的密集发布期。继 2019 年 GPT-2 (OpenAI, 2019) 之后，OpenAI 再次向开源社区贡献了其首个开放权重的大型语言模型系列 gpt-oss (OpenAI, 2025)，包含 120B 和 20B 两种规模。紧随其后，备受瞩目的下一代旗舰模型 GPT-5 (OpenAI, 2025) 也正式发布。这一系列发布不仅标志着开源模型在推理和智能体能力上达到了新的高度，也揭示了 OpenAI 在模型架构、训练方法论以及安全对齐方面的最新进展。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2025-08-24-gpt5/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-08-24-gpt5/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-08-24-gpt5/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2025-08-24-gpt5/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="gpt-oss & GPT-5"><meta property="og:description" content="2025 年 8 月，AI 领域迎来了 OpenAI 的密集发布期。继 2019 年 GPT-2 (OpenAI, 2019) 之后，OpenAI 再次向开源社区贡献了其首个开放权重的大型语言模型系列 gpt-oss (OpenAI, 2025)，包含 120B 和 20B 两种规模。紧随其后，备受瞩目的下一代旗舰模型 GPT-5 (OpenAI, 2025) 也正式发布。这一系列发布不仅标志着开源模型在推理和智能体能力上达到了新的高度，也揭示了 OpenAI 在模型架构、训练方法论以及安全对齐方面的最新进展。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-24T12:00:00+08:00"><meta property="article:modified_time" content="2025-08-24T12:00:00+08:00"><meta property="article:tag" content="Gpt-Oss"><meta property="article:tag" content="GPT-5"><meta property="article:tag" content="MoE"><meta property="article:tag" content="Reasoning"><meta property="article:tag" content="Tool Use"><meta property="article:tag" content="OpenAI"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="gpt-oss & GPT-5"><meta name=twitter:description content="2025 年 8 月，AI 领域迎来了 OpenAI 的密集发布期。继 2019 年 GPT-2 (OpenAI, 2019) 之后，OpenAI 再次向开源社区贡献了其首个开放权重的大型语言模型系列 gpt-oss (OpenAI, 2025)，包含 120B 和 20B 两种规模。紧随其后，备受瞩目的下一代旗舰模型 GPT-5 (OpenAI, 2025) 也正式发布。这一系列发布不仅标志着开源模型在推理和智能体能力上达到了新的高度，也揭示了 OpenAI 在模型架构、训练方法论以及安全对齐方面的最新进展。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"gpt-oss \u0026 GPT-5","item":"https://syhya.github.io/zh/posts/2025-08-24-gpt5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"gpt-oss \u0026 GPT-5","name":"gpt-oss \u0026 GPT-5","description":"2025 年 8 月，AI 领域迎来了 OpenAI 的密集发布期。继 2019 年 GPT-2 (OpenAI, 2019) 之后，OpenAI 再次向开源社区贡献了其首个开放权重的大型语言模型系列 gpt-oss (OpenAI, 2025)，包含 120B 和 20B 两种规模。紧随其后，备受瞩目的下一代旗舰模型 GPT-5 (OpenAI, 2025) 也正式发布。这一系列发布不仅标志着开源模型在推理和智能体能力上达到了新的高度，也揭示了 OpenAI 在模型架构、训练方法论以及安全对齐方面的最新进展。\n","keywords":["gpt-oss","GPT-5","MoE","Reasoning","Tool Use","OpenAI","LLM Architecture"],"articleBody":"2025 年 8 月，AI 领域迎来了 OpenAI 的密集发布期。继 2019 年 GPT-2 (OpenAI, 2019) 之后，OpenAI 再次向开源社区贡献了其首个开放权重的大型语言模型系列 gpt-oss (OpenAI, 2025)，包含 120B 和 20B 两种规模。紧随其后，备受瞩目的下一代旗舰模型 GPT-5 (OpenAI, 2025) 也正式发布。这一系列发布不仅标志着开源模型在推理和智能体能力上达到了新的高度，也揭示了 OpenAI 在模型架构、训练方法论以及安全对齐方面的最新进展。\ngpt-oss gpt-oss (OpenAI, 2025) 是 OpenAI 自 GPT-2 以来首次发布的开放权重语言模型，旨在为开源社区提供强大的推理和工具使用能力。该系列包含 gpt-oss-120b 和 gpt-oss-20b 两个版本，均在 Apache 2.0 许可下发布。\n架构概览 Fig. 1. A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B. (Image source: Raschka, 2025)\ngpt-oss 建立在 GPT 系列架构之上，并融合了近年来多项主流技术包括 RMSNorm、SwiGLU、GQA、RoPE、YaRN 和 MoE 等。\n下表直观对比了 GPT-OSS 20B vs GPT-2 XL 1.5B 模型差异。\n特性 GPT-OSS 20B (2025) GPT-2 XL 1.5B (2019) 发布时间 2025年 2019年 模型大小 20B 参数 1.5B 参数 活跃参数 3.5B (每次推理) 1.5B (全部激活) 词汇表大小 200k tokens 50k tokens 嵌入维度 2,880 1,600 Transformer层数 24层 48层 注意力头数 64个 25个 支持上下文长度 131k tokens 1,024 tokens 位置编码 RoPE (旋转位置编码) 绝对位置嵌入 注意力机制 Grouped Query Attention Multi-head Attention 前馈网络 SwiGLU激活 + MoE GELU激活 MoE架构 32个专家，4个激活 无 归一化方法 RMSNorm (2处) LayerNorm (2处) Dropout 无 有 滑动窗口注意力 每隔一层使用(窗口128 tokens) 无 训练特点 包含监督微调+强化学习 仅预训练 量化支持 MXFP4 (可在单GPU运行) 无特殊量化 许可证 Apache 2.0 MIT 高效注意力机制 为了在支持 128k 长上下文的同时保持高效率，gpt-oss 采用了多种先进的注意力机制。\n分组查询注意力 (Grouped-Query Attention, GQA): gpt-oss 中有 64 个查询头和 8 个键值头，意味着每 8 个查询头共享一对 K/V，这显著减少了 KV 缓存的大小和内存带宽需求，从而在几乎不损失模型性能的情况下，大幅提升了推理吞吐量。\n滑动窗口注意力 (Sliding Window Attention): 为了进一步降低计算复杂度，gpt-oss 借鉴了 Longformer(Jiang et al., 2020) 和 Mistral(Jiang et al., 2023)的思想，采用了交替的注意力模式。其 Transformer 层在全注意力 (Dense Attention)和局部带状稀疏注意力 (Locally Banded Sparse Attention)之间交替。后者即滑动窗口注意力，它将每个 Token 的注意力范围限制在一个固定大小的局部窗口内。\nFig. 2. Comparison between regular attention (left) and sliding window attention (right). (Image source: Jiang et al., 2023)\n在 gpt-oss 中，这个窗口大小被设置为 128 个 Token。这意味着，在一个局部注意力层中，一个 Token 只能关注其前面 128 个 Token，而不是整个上下文。这种设计使得注意力的计算复杂度从 \\( O(L^2) \\) 降低到 \\( O(L \\cdot W) \\)，其中 \\( L \\) 是序列长度，\\( W \\) 是窗口大小。通过与全注意力层交替，模型既能高效处理局部信息，又能通过全注意力层整合全局信息。\n注意力池: 模型引入了注意力池（Attention Sinks） (Xiao et al., 2023)，通过学习一个附加到注意力分数上的偏置 \\( \\mathbf{s}_h \\)，使得初始 Token 能够被持续关注，这有助于在长上下文场景下稳定注意力分布，防止信息丢失。 \\[ \\text{Attention}(Q, K, V)_h = \\text{softmax}\\left(\\frac{Q_h K_h^T}{\\sqrt{d_k}} + \\mathbf{s}_h\\right)V_h \\] Fig. 3. Illustration of StreamingLLM vs existing methods. (Image source: Xiao et al., 2023)\n上图比较了 StreamingLLM 与三种常见长文本处理方法在性能和效率上的差异。假设语言模型在预训练时只见过长度为 $L$ 的文本，而推理时需要预测第 $T$ 个 token（其中 $T \\gg L$）：\n密集注意力（Dense Attention）：保留所有历史 token 的键值（KV）并计算全量注意力，时间复杂度为 $O(T^2)$，缓存规模持续增长。当输入长度超过预训练长度 $L$ 时，性能明显下降。 窗口注意力（Window Attention）：只缓存最近 $L$ 个 token 的 KV，推理效率高，但一旦早期 token 的信息被替换，性能会急剧下降。 滑动窗口重计算（Sliding Window with Re-computation）：在每次生成新 token 时，从最近 $L$ 个 token 重建 KV 状态。尽管在长文本上性能较好，但因重计算涉及二次注意力，时间复杂度高达 $O(TL^2)$，推理速度很慢。 这种方法在计算时结合了注意力池和最近的 token，不仅保持了推理效率，还能在长文本场景下维持稳定的注意力分布和较低的困惑度。\nMXFP4 量化 Fig. 4. Faster MXFP4 Backpropagation via Stochastic Rounding and Hadamard Transform. (Image source: Tseng et al., 2025)\n为了让大模型能在消费级硬件上运行，gpt-oss 采用了 MXFP4 (Tseng et al., 2025) 格式对 MoE 权重进行量化。MXFP4 是一种微缩放浮点格式，可以将权重有效量化到约 4.25 bit。由于 MoE 权重占模型总参数的 90% 以上，此方法极大地压缩了模型大小，使得 120B 模型能装入单个 80GB GPU，20B 模型能在 16GB 显存的设备上运行。\n训练 预训练: 模型在数万亿 Token 的文本数据集上进行训练，数据侧重于 STEM、编码和通用知识。为了提升安全性，预训练数据复用了 GPT-4o 的 CBRN 内容过滤器。\n后训练 (推理与工具使用): 预训练后，模型采用与 OpenAI o3 类似的 CoT RL 技术进行后训练。这一阶段的目标是教会模型：\n推理: 生成详细的思维链（Chain-of-Thought, CoT）来解决复杂问题。 工具使用: 学会调用外部工具（如网页搜索、代码执行）来增强自身能力。 为了实现这些高级智能体功能，OpenAI 设计了Harmony Chat Format。该格式引入了“通道（channels）”概念（如 analysis 用于 CoT，commentary 用于工具调用，final 用于最终答案），并建立了严格的指令层级（System \u003e Developer \u003e User \u003e Assistant \u003e Tool），确保模型行为的可控性。\n此外，模型还支持可变努力推理（Variable Effort Reasoning）。用户可以在系统提示中设置 Reasoning: low/medium/high，从而在延迟和性能之间进行权衡。高努力度会生成更长的 CoT，通常带来更高的准确率，但延迟也相应增加。\nFig. 5. Accuracy vs. average CoT length for different reasoning levels on AIME and GPQA benchmarks. (Image source: OpenAI, 2025)\n评估 Fig. 6. Main capabilities evaluations for gpt-oss series. (Image source: OpenAI, 2025)\n评测基准结果显示 gpt-oss-120b 的准确率已超过 OpenAI 的 o3-mini，并接近 o4-mini；而规模仅为其六分之一的 gpt-oss-20b 也展现出一定竞争力。\nGPT-5 GPT-5 (OpenAI, 2025) 并非单个模型，而是一个统一的智能系统。它并非一个单一的庞大模型，而是一个由多个专业模型和智能路由机制协同工作的复杂系统，旨在平衡性能、速度与成本。\n系统架构 Fig. 7. GPT-5 Unified System Architecture. (Image source: Latent Space, 2025)\nGPT-5 系统由以三个核心部分组成：\ngpt-5-main: 作为系统的默认模型，它快速、高效，负责处理绝大多数用户查询。可视为 GPT-4o 的继任者。 gpt-5-thinking: 用于处理更复杂、需要深度思考的问题。当用户明确要求（如“think hard about this”）或系统判断任务需要时，该模型会被激活。可视为 OpenAI o3 的继任者。 实时路由器 (Real-time Router): 这是一个持续训练的决策模型，它能根据多种信号快速判断应将用户请求分配给哪个模型处理。其决策依据包括： 对话类型 (Conversation Type): 是闲聊、问答还是任务导向型对话。 复杂性 (Complexity): 问题的难度和所需的推理深度。 工具需求 (Tool Needs): 是否需要调用网页搜索、代码解释器等外部工具。 用户意图 (Explicit Intent): 用户可以通过明确的指令（如“think hard about this”）来引导路由器选择深度推理模型。 该路由器通过持续学习真实用户信号（如用户切换模型的行为、响应的偏好率、答案的实测正确率）来不断优化其决策能力。\n安全补全 传统的安全训练范式是硬性拒绝 (Hard Refusals)，即模型根据用户意图的二元分类（安全或不安全）来决定是完全回答还是直接拒绝。这种方法在处理意图明确的恶意提示时有效，但在面对意图模糊或涉及双重用途 (Dual-use)（如生物、网络安全）的领域时，显得非常脆弱，常常导致过度拒绝。\n安全补全 (Safe Completions)(Baker et al., 2025)不再对用户意图进行二元分类，而是致力于在遵守安全策略的前提下，最大化模型的帮助性。\n对于明确有害的请求：模型依然会拒绝。 对于双重用途的请求（如生物、化学、网络安全领域）：模型会提供安全的、高层次的、不包含可直接执行细节的回答，而不是完全拒绝。 对于模糊意图的请求：模型会尝试以安全的方式完成任务，或提供安全的替代方案。 这种方法显著提升了模型在双重用途领域的安全性和实用性，并减少了不必要的过度拒绝。\nFig. 8. Left: Overall structure of the safe-completion training stack. Right: Details of the safecompletion reward design. (Image source: OpenAI, 2025)\n思维链监控 OpenAI 采用 思维链监控 (CoT Monitoring)(Baker et al., 2025)技术来确保推理模型的可靠性、安全性以及避免出现 reward hacking。与一些试图通过 SFT 来优化 CoT 的做法不同，GPT-5 的 CoT 训练没有施加直接对齐。这使得 CoT 能够更真实地反映模型的“思考”过程，成为一个有效的窗口，用于检测模型的错误行为、欺骗意图或潜在的风险。\nFig. 9. Monitoring Frontier Reasoning Models for Reward Hacking. (Image source: Baker et al., 2025)\n通过 CoT 监控，OpenAI 发现 gpt-5-thinking 的欺骗行为发生率从 o3 的 4.8% 降低到了 2.1%。这项技术对于理解和缓解高级 AI 系统的风险至关重要。\n评估 GPT-5 在多个基准测试中表现出色，尤其在推理、编码和多模态能力上树立了新的标杆。与前代模型相比，它不仅在准确率上实现了飞跃，还在效率上取得了显著进步，通常能用 50-80% 更少的输出来达到甚至超越 o3 的性能。\nFig. 10. GPT-5 performance in SWE-bench Verified Software Engineering. (Image source: OpenAI, 2025)\n🧠 智慧 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano AIME ’25 94.6% 91.1% 85.2% 88.9% 92.7% 46.4% 40.2% - FrontierMath (with python tool only) 26.3% 22.1% 9.6% 15.8% 15.4% - - - GPQA diamond 85.7% 82.3% 71.2% 83.3% 81.4% 66.3% 65.0% 50.3% HLE 24.8% 16.7% 8.7% 20.2% 14.7% 5.4% 3.7% - HMMT 2025 93.3% 87.8% 75.6% 81.7% 85.0% 28.9% 35.0% - 🖼️ 多模态 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano MMMU 84.2% 81.6% 75.6% 82.9% 81.6% 74.8% 72.7% 55.4% MMMU-Pro (avg across standard and vision sets) 78.4% 74.1% 62.6% 76.4% 73.4% 60.3% 58.9% 33.0% CharXiv reasoning (python enabled) 81.1% 75.5% 62.7% 78.6% 72.0% 56.7% 56.8% 40.5% VideoMMMU (max frame 256) 84.6% 82.5% 66.8% 83.3% 79.4% 60.9% 55.1% 30.2% ERQA 65.7% 62.9% 50.1% 64.0% 56.5% 44.3% 42.3% 26.5% 💻 编码 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano SWE-Lancer: IC SWE Diamond Freelance Coding Tasks $112K $75K $49K $86K $66K $34K $31K $9K SWE-bench Verified 74.9% 71.0% 54.7% 69.1% 68.1% 54.6% 23.6% - Aider polyglot (diff) 88.0% 71.6% 48.4% 79.6% 58.2% 52.9% 31.6% 6.2% 📋 指令遵循 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano Scale multichallenge (o3-mini grader) 69.6% 62.3% 54.9% 60.4% 57.5% 46.2% 42.2% 31.1% Internal API instruction following eval (hard) 64.0% 65.8% 56.1% 47.4% 44.7% 49.1% 45.1% 31.6% COLLIE 99.0% 98.5% 96.9% 98.4% 96.1% 65.8% 54.6% 42.5% 🔧 工具调用 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano Tau²-bench airline 62.6% 60.0% 41.0% 64.8% 60.2% 56.0% 51.0% 14.0% Tau²-bench retail 81.1% 78.3% 62.3% 80.2% 70.5% 74.0% 66.0% 21.5% Tau²-bench telecom 96.7% 74.1% 35.5% 58.2% 40.5% 34.0% 44.0% 12.1% 📚 长上下文 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano OpenAI-MRCR: 2 needle 128k 95.2% 84.3% 43.2% 55.0% 56.4% 57.2% 47.2% 36.6% OpenAI-MRCR: 2 needle 256k 86.8% 58.8% 34.9% - - 56.2% 45.5% 22.6% Graphwalks bfs \u003c128k 78.3% 73.4% 64.0% 77.3% 62.3% 61.7% 61.7% 25.0% Graphwalks parents \u003c128k 73.3% 64.3% 43.8% 72.9% 51.1% 58.0% 60.5% 9.4% BrowseComp Long Context 128k 90.0% 89.4% 80.4% 88.3% 80.0% 85.9% 89.0% 89.4% BrowseComp Long Context 256k 88.8% 86.0% 68.4% - - 75.5% 81.6% 19.1% VideoMME (long, with subtitle category) 86.7% 78.5% 65.7% 84.9% 79.5% 78.7% 68.4% 55.2% 🚨 幻觉 Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini LongFact-Concepts hallucination rate (no tools) 1.0% 0.7% 1.0% 5.2% 3.0% 0.7% 1.1% LongFact-Objects hallucination rate (no tools) 1.2% 1.3% 2.8% 6.8% 8.9% 1.1% 1.8% FActScore hallucination rate (no tools) 2.8% 3.5% 7.3% 23.5% 38.7% 6.7% 10.9% 这些结果表明，GPT-5 在需要深度推理的复杂任务（如 GPQA、AIME）和需要与外部环境交互的智能体任务（如 SWE-bench、τ²-bench）上取得了较大提升。同时，事实准确性的大幅提升（幻觉率降低近 8 倍）也使其在实际应用中更加可靠。\n参考文献 [1] Raschka, S. (2025). “From GPT-2 to gpt-oss: Analyzing the Architectural Advances.” Ahead of AI.\n[2] Radford, Alec, et al. “Language models are unsupervised multitask learners.” OpenAI blog 1.8 (2019): 9.\n[3] OpenAI. (2025). “Introducing gpt-oss.” OpenAI Blog.\n[4] OpenAI. (2025). “Introducing GPT-5.” OpenAI Blog.\n[5] OpenAI. (2025). “gpt-oss-120b \u0026 gpt-oss-20b Model Card.”\n[6] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. “Longformer: The long-document transformer.” arXiv preprint arXiv:2004.05150 (2020).\n[7] Jiang, Dongsheng, et al. “Mistral 7B.” arXiv preprint arXiv:2310.08825 (2023).\n[8] Xiao, G., et al. (2023). “Efficient Streaming Language Models with Attention Sinks.” arXiv preprint arXiv:2309.17453.\n[9] Tseng, Albert, Tao Yu, and Youngsuk Park. “Training llms with mxfp4.” arXiv preprint arXiv:2502.20586 (2025).\n[10] Yuan, Yuan, et al. “From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training.” arXiv preprint arXiv:2508.09224 (2025).\n[11] B. Baker, J. Huizinga, L. Gao, Z. Dou, M. Y. Guan, A. Madry, W. Zaremba, J. Pachocki, and D. Farhi, “Monitoring reasoning models for misbehavior and the risks of promoting obfuscation.” arXiv preprint arXiv:2503.11926, 2025. Submitted on 14 March 2025.\n[12] OpenAI. (2025). “GPT-5 System Card.”\n[13] OpenAI. (2025). “Introducing GPT-5 for developers.” OpenAI Blog.\n引用 引用：转载或引用本文内容时，请注明原作者和来源。\nCited as:\nYue Shui. (Aug 2025). gpt-oss \u0026 GPT-5. https://syhya.github.io/zh/posts/2025-08-24-gpt5\nOr\n@article{yue_shui_gpt5_2025 title = \"gpt-oss \u0026 GPT-5\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Aug\", url = \"https://syhya.github.io/zh/posts/2025-08-24-gpt5\" } ","wordCount":"4305","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2025-08-24T12:00:00+08:00","dateModified":"2025-08-24T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2025-08-24-gpt5/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">gpt-oss & GPT-5</h1><div class=post-meta><span title='2025-08-24 12:00:00 +0800 +0800'>Created:&nbsp;2025-08-24</span>&nbsp;·&nbsp;Updated:&nbsp;2025-08-24&nbsp;·&nbsp;9 分钟&nbsp;·&nbsp;4305 字&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;翻译:<ul class=i18n_list><li><a href=https://syhya.github.io/posts/2025-08-24-gpt5/>En</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#gpt-oss>gpt-oss</a><ul><li><a href=#架构概览>架构概览</a></li><li><a href=#高效注意力机制>高效注意力机制</a></li><li><a href=#mxfp4-量化>MXFP4 量化</a></li><li><a href=#训练>训练</a></li><li><a href=#评估>评估</a></li></ul></li><li><a href=#gpt-5>GPT-5</a><ul><li><a href=#系统架构>系统架构</a></li><li><a href=#安全补全>安全补全</a></li><li><a href=#思维链监控>思维链监控</a></li><li><a href=#评估-1>评估</a><ul><li><a href=#-智慧>🧠 智慧</a></li><li><a href=#-多模态>🖼️ 多模态</a></li><li><a href=#-编码>💻 编码</a></li><li><a href=#-指令遵循>📋 指令遵循</a></li><li><a href=#-工具调用>🔧 工具调用</a></li><li><a href=#-长上下文>📚 长上下文</a></li><li><a href=#-幻觉>🚨 幻觉</a></li></ul></li></ul></li><li><a href=#参考文献>参考文献</a></li><li><a href=#引用>引用</a></li></ul></nav></div></details></div><div class=post-content><p>2025 年 8 月，AI 领域迎来了 OpenAI 的密集发布期。继 2019 年 <strong>GPT-2</strong> (<a href=https://openai.com/index/better-language-models/>OpenAI, 2019</a>) 之后，OpenAI 再次向开源社区贡献了其首个开放权重的大型语言模型系列 <strong>gpt-oss</strong> (<a href=https://openai.com/index/introducing-gpt-oss/>OpenAI, 2025</a>)，包含 120B 和 20B 两种规模。紧随其后，备受瞩目的下一代旗舰模型 <strong>GPT-5</strong> (<a href=https://openai.com/index/introducing-gpt-5/>OpenAI, 2025</a>) 也正式发布。这一系列发布不仅标志着开源模型在推理和智能体能力上达到了新的高度，也揭示了 OpenAI 在模型架构、训练方法论以及安全对齐方面的最新进展。</p><h2 id=gpt-oss>gpt-oss<a hidden class=anchor aria-hidden=true href=#gpt-oss>#</a></h2><p><strong>gpt-oss</strong> (<a href=https://openai.com/index/introducing-gpt-oss/>OpenAI, 2025</a>) 是 OpenAI 自 GPT-2 以来首次发布的开放权重语言模型，旨在为开源社区提供强大的推理和工具使用能力。该系列包含 <code>gpt-oss-120b</code> 和 <code>gpt-oss-20b</code> 两个版本，均在 Apache 2.0 许可下发布。</p><h3 id=架构概览>架构概览<a hidden class=anchor aria-hidden=true href=#架构概览>#</a></h3><figure class=align-center><img loading=lazy src=gpt_oss_vs_gpt2.png#center alt="Fig. 1. A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B. (Image source: Raschka, 2025)" width=100%><figcaption><p>Fig. 1. A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B. (Image source: <a href=https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the>Raschka, 2025</a>)</p></figcaption></figure><p>gpt-oss 建立在 GPT 系列架构之上，并融合了近年来多项主流技术包括 <a href=https://syhya.github.io/posts/2025-02-01-normalization/#rms-normalization>RMSNorm</a>、<a href=https://syhya.github.io/posts/2025-04-06-llama/#ffn_swiglu>SwiGLU</a>、<a href=https://syhya.github.io/posts/2025-01-16-group-query-attention/#grouped-query-attention-gqa>GQA</a>、<a href=https://syhya.github.io/posts/2025-04-06-llama/#rotary-positional-embeddings-rope>RoPE</a>、<a href=https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/>YaRN</a> 和 <a href=https://syhya.github.io/posts/2025-03-01-train-llm/#mixture-of-experts-model>MoE</a> 等。</p><p>下表直观对比了 GPT-OSS 20B vs GPT-2 XL 1.5B 模型差异。</p><table><thead><tr><th><strong>特性</strong></th><th><strong>GPT-OSS 20B (2025)</strong></th><th><strong>GPT-2 XL 1.5B (2019)</strong></th></tr></thead><tbody><tr><td><strong>发布时间</strong></td><td>2025年</td><td>2019年</td></tr><tr><td><strong>模型大小</strong></td><td><strong>20B 参数</strong></td><td>1.5B 参数</td></tr><tr><td><strong>活跃参数</strong></td><td><strong>3.5B</strong> (每次推理)</td><td>1.5B (全部激活)</td></tr><tr><td><strong>词汇表大小</strong></td><td><strong>200k tokens</strong></td><td>50k tokens</td></tr><tr><td><strong>嵌入维度</strong></td><td><strong>2,880</strong></td><td>1,600</td></tr><tr><td><strong>Transformer层数</strong></td><td>24层</td><td><strong>48层</strong></td></tr><tr><td><strong>注意力头数</strong></td><td><strong>64个</strong></td><td>25个</td></tr><tr><td><strong>支持上下文长度</strong></td><td><strong>131k tokens</strong></td><td>1,024 tokens</td></tr><tr><td><strong>位置编码</strong></td><td><strong>RoPE</strong> (旋转位置编码)</td><td>绝对位置嵌入</td></tr><tr><td><strong>注意力机制</strong></td><td><strong>Grouped Query Attention</strong></td><td>Multi-head Attention</td></tr><tr><td><strong>前馈网络</strong></td><td><strong>SwiGLU激活 + MoE</strong></td><td>GELU激活</td></tr><tr><td><strong>MoE架构</strong></td><td><strong>32个专家，4个激活</strong></td><td>无</td></tr><tr><td><strong>归一化方法</strong></td><td><strong>RMSNorm</strong> (2处)</td><td>LayerNorm (2处)</td></tr><tr><td><strong>Dropout</strong></td><td><strong>无</strong></td><td>有</td></tr><tr><td><strong>滑动窗口注意力</strong></td><td><strong>每隔一层使用</strong>(窗口128 tokens)</td><td>无</td></tr><tr><td><strong>训练特点</strong></td><td><strong>包含监督微调+强化学习</strong></td><td>仅预训练</td></tr><tr><td><strong>量化支持</strong></td><td><strong>MXFP4</strong> (可在单GPU运行)</td><td>无特殊量化</td></tr><tr><td><strong>许可证</strong></td><td>Apache 2.0</td><td>MIT</td></tr></tbody></table><h3 id=高效注意力机制>高效注意力机制<a hidden class=anchor aria-hidden=true href=#高效注意力机制>#</a></h3><p>为了在支持 128k 长上下文的同时保持高效率，gpt-oss 采用了多种先进的注意力机制。</p><ul><li><p><strong>分组查询注意力 (Grouped-Query Attention, GQA)</strong>: gpt-oss 中有 64 个查询头和 8 个键值头，意味着每 8 个查询头共享一对 K/V，这显著减少了 KV 缓存的大小和内存带宽需求，从而在几乎不损失模型性能的情况下，大幅提升了推理吞吐量。</p></li><li><p><strong>滑动窗口注意力 (Sliding Window Attention)</strong>: 为了进一步降低计算复杂度，gpt-oss 借鉴了 <strong>Longformer</strong>(<a href=https://arxiv.org/abs/2004.05150>Jiang et al., 2020</a>) 和 <strong>Mistral</strong>(<a href=https://arxiv.org/abs/2310.06825>Jiang et al., 2023</a>)的思想，采用了<strong>交替的注意力模式</strong>。其 Transformer 层在全注意力 (Dense Attention)和局部带状稀疏注意力 (Locally Banded Sparse Attention)之间交替。后者即<strong>滑动窗口注意力</strong>，它将每个 Token 的注意力范围限制在一个固定大小的局部窗口内。</p></li></ul><figure class=align-center><img loading=lazy src=sliding_window_attention.png#center alt="Fig. 2. Comparison between regular attention (left) and sliding window attention (right). (Image source: Jiang et al., 2023)" width=80%><figcaption><p>Fig. 2. Comparison between regular attention (left) and sliding window attention (right). (Image source: <a href=https://arxiv.org/abs/2310.06825>Jiang et al., 2023</a>)</p></figcaption></figure><p>在 gpt-oss 中，这个窗口大小被设置为 128 个 Token。这意味着，在一个局部注意力层中，一个 Token 只能关注其前面 128 个 Token，而不是整个上下文。这种设计使得注意力的计算复杂度从 \( O(L^2) \) 降低到 \( O(L \cdot W) \)，其中 \( L \) 是序列长度，\( W \) 是窗口大小。通过与全注意力层交替，模型既能高效处理局部信息，又能通过全注意力层整合全局信息。</p><ul><li><strong>注意力池</strong>: 模型引入了<strong>注意力池（Attention Sinks）</strong> (<a href=https://arxiv.org/abs/2309.17453>Xiao et al., 2023</a>)，通过学习一个附加到注意力分数上的偏置 \( \mathbf{s}_h \)，使得初始 Token 能够被持续关注，这有助于在长上下文场景下稳定注意力分布，防止信息丢失。</li></ul>\[ \text{Attention}(Q, K, V)_h = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_k}} + \mathbf{s}_h\right)V_h \]<figure class=align-center><img loading=lazy src=StreamingLLM.png#center alt="Fig. 3. Illustration of StreamingLLM vs existing methods. (Image source: Xiao et al., 2023)" width=100%><figcaption><p>Fig. 3. Illustration of StreamingLLM vs existing methods. (Image source: <a href=https://arxiv.org/abs/2309.17453>Xiao et al., 2023</a>)</p></figcaption></figure><p>上图比较了 <strong>StreamingLLM</strong> 与三种常见长文本处理方法在性能和效率上的差异。假设语言模型在预训练时只见过长度为 $L$ 的文本，而推理时需要预测第 $T$ 个 token（其中 $T \gg L$）：</p><ol><li><strong>密集注意力（Dense Attention）</strong>：保留所有历史 token 的键值（KV）并计算全量注意力，时间复杂度为 $O(T^2)$，缓存规模持续增长。当输入长度超过预训练长度 $L$ 时，性能明显下降。</li><li><strong>窗口注意力（Window Attention）</strong>：只缓存最近 $L$ 个 token 的 KV，推理效率高，但一旦早期 token 的信息被替换，性能会急剧下降。</li><li><strong>滑动窗口重计算（Sliding Window with Re-computation）</strong>：在每次生成新 token 时，从最近 $L$ 个 token 重建 KV 状态。尽管在长文本上性能较好，但因重计算涉及二次注意力，时间复杂度高达 $O(TL^2)$，推理速度很慢。</li></ol><p>这种方法在计算时结合了注意力池和最近的 token，不仅保持了推理效率，还能在长文本场景下维持稳定的注意力分布和较低的困惑度。</p><h3 id=mxfp4-量化>MXFP4 量化<a hidden class=anchor aria-hidden=true href=#mxfp4-量化>#</a></h3><figure class=align-center><img loading=lazy src=mxfp4.png#center alt="Fig. 4. Faster MXFP4 Backpropagation via Stochastic Rounding and Hadamard Transform. (Image source: Tseng et al., 2025)" width=60%><figcaption><p>Fig. 4. Faster MXFP4 Backpropagation via Stochastic Rounding and Hadamard Transform. (Image source: <a href=https://arxiv.org/abs/2502.20586>Tseng et al., 2025</a>)</p></figcaption></figure><p>为了让大模型能在消费级硬件上运行，gpt-oss 采用了 <strong>MXFP4</strong> (<a href=https://arxiv.org/abs/2502.20586>Tseng et al., 2025</a>) 格式对 MoE 权重进行量化。MXFP4 是一种微缩放浮点格式，可以将权重有效量化到约 4.25 bit。由于 MoE 权重占模型总参数的 90% 以上，此方法极大地压缩了模型大小，使得 120B 模型能装入单个 80GB GPU，20B 模型能在 16GB 显存的设备上运行。</p><h3 id=训练>训练<a hidden class=anchor aria-hidden=true href=#训练>#</a></h3><ul><li><p><strong>预训练</strong>: 模型在数万亿 Token 的文本数据集上进行训练，数据侧重于 STEM、编码和通用知识。为了提升安全性，预训练数据复用了 GPT-4o 的 CBRN 内容过滤器。</p></li><li><p><strong>后训练 (推理与工具使用)</strong>: 预训练后，模型采用与 OpenAI o3 类似的 <strong>CoT RL</strong> 技术进行后训练。这一阶段的目标是教会模型：</p><ol><li><strong>推理</strong>: 生成详细的思维链（Chain-of-Thought, CoT）来解决复杂问题。</li><li><strong>工具使用</strong>: 学会调用外部工具（如网页搜索、代码执行）来增强自身能力。</li></ol></li></ul><p>为了实现这些高级智能体功能，OpenAI 设计了<a href=https://github.com/openai/harmony>Harmony Chat Format</a>。该格式引入了“通道（channels）”概念（如 <code>analysis</code> 用于 CoT，<code>commentary</code> 用于工具调用，<code>final</code> 用于最终答案），并建立了严格的指令层级（System > Developer > User > Assistant > Tool），确保模型行为的可控性。</p><p>此外，模型还支持<strong>可变努力推理（Variable Effort Reasoning）</strong>。用户可以在系统提示中设置 <code>Reasoning: low/medium/high</code>，从而在延迟和性能之间进行权衡。高努力度会生成更长的 CoT，通常带来更高的准确率，但延迟也相应增加。</p><figure class=align-center><img loading=lazy src=reasoning_efforts.png#center alt="Fig. 5. Accuracy vs. average CoT length for different reasoning levels on AIME and GPQA benchmarks. (Image source: OpenAI, 2025)" width=100%><figcaption><p>Fig. 5. Accuracy vs. average CoT length for different reasoning levels on AIME and GPQA benchmarks. (Image source: <a href=https://openai.com/index/gpt-oss-model-card/>OpenAI, 2025</a>)</p></figcaption></figure><h3 id=评估>评估<a hidden class=anchor aria-hidden=true href=#评估>#</a></h3><figure class=align-center><img loading=lazy src=gpt_oss_eval.png#center alt="Fig. 6. Main capabilities evaluations for gpt-oss series. (Image source: OpenAI, 2025)" width=100%><figcaption><p>Fig. 6. Main capabilities evaluations for gpt-oss series. (Image source: <a href=https://openai.com/index/introducing-gpt-oss/>OpenAI, 2025</a>)</p></figcaption></figure><p>评测基准结果显示 gpt-oss-120b 的准确率已超过 OpenAI 的 o3-mini，并接近 o4-mini；而规模仅为其六分之一的 gpt-oss-20b 也展现出一定竞争力。</p><h2 id=gpt-5>GPT-5<a hidden class=anchor aria-hidden=true href=#gpt-5>#</a></h2><p><strong>GPT-5</strong> (<a href=https://openai.com/index/introducing-gpt-5/>OpenAI, 2025</a>) 并非单个模型，而是一个<strong>统一的智能系统</strong>。它并非一个单一的庞大模型，而是一个由多个专业模型和智能路由机制协同工作的复杂系统，旨在平衡性能、速度与成本。</p><h3 id=系统架构>系统架构<a hidden class=anchor aria-hidden=true href=#系统架构>#</a></h3><figure class=align-center><img loading=lazy src=gpt5_system_arch.png#center alt="Fig. 7. GPT-5 Unified System Architecture. (Image source: Latent Space, 2025)" width=100%><figcaption><p>Fig. 7. GPT-5 Unified System Architecture. (Image source: <a href=https://www.latent.space/p/gpt5-router>Latent Space, 2025</a>)</p></figcaption></figure><p>GPT-5 系统由以三个核心部分组成：</p><ol><li><strong>gpt-5-main</strong>: 作为系统的默认模型，它快速、高效，负责处理绝大多数用户查询。可视为 GPT-4o 的继任者。</li><li><strong>gpt-5-thinking</strong>: 用于处理更复杂、需要深度思考的问题。当用户明确要求（如“think hard about this”）或系统判断任务需要时，该模型会被激活。可视为 OpenAI o3 的继任者。</li><li><strong>实时路由器 (Real-time Router)</strong>: 这是一个持续训练的决策模型，它能根据多种信号快速判断应将用户请求分配给哪个模型处理。其决策依据包括：</li></ol><ul><li><strong>对话类型 (Conversation Type):</strong> 是闲聊、问答还是任务导向型对话。</li><li><strong>复杂性 (Complexity):</strong> 问题的难度和所需的推理深度。</li><li><strong>工具需求 (Tool Needs):</strong> 是否需要调用网页搜索、代码解释器等外部工具。</li><li><strong>用户意图 (Explicit Intent):</strong> 用户可以通过明确的指令（如“think hard about this”）来引导路由器选择深度推理模型。</li></ul><p>该路由器通过持续学习真实用户信号（如用户切换模型的行为、响应的偏好率、答案的实测正确率）来不断优化其决策能力。</p><h3 id=安全补全>安全补全<a hidden class=anchor aria-hidden=true href=#安全补全>#</a></h3><p>传统的安全训练范式是<strong>硬性拒绝 (Hard Refusals)</strong>，即模型根据用户意图的二元分类（安全或不安全）来决定是完全回答还是直接拒绝。这种方法在处理意图明确的恶意提示时有效，但在面对意图模糊或涉及<strong>双重用途 (Dual-use)</strong>（如生物、网络安全）的领域时，显得非常脆弱，常常导致过度拒绝。</p><p><strong>安全补全 (Safe Completions)</strong>(<a href=https://openai.com/index/gpt-5-safe-completions/>Baker et al., 2025</a>)不再对用户意图进行二元分类，而是致力于在遵守安全策略的前提下，最大化模型的帮助性。</p><ul><li><strong>对于明确有害的请求</strong>：模型依然会拒绝。</li><li><strong>对于双重用途的请求</strong>（如生物、化学、网络安全领域）：模型会提供安全的、高层次的、不包含可直接执行细节的回答，而不是完全拒绝。</li><li><strong>对于模糊意图的请求</strong>：模型会尝试以安全的方式完成任务，或提供安全的替代方案。</li></ul><p>这种方法显著提升了模型在双重用途领域的安全性和实用性，并减少了不必要的过度拒绝。</p><figure class=align-center><img loading=lazy src=safe_completions.png#center alt="Fig. 8. Left: Overall structure of the safe-completion training stack. Right: Details of the safecompletion reward design. (Image source: OpenAI, 2025)" width=100%><figcaption><p>Fig. 8. Left: Overall structure of the safe-completion training stack. Right: Details of the safecompletion reward design. (Image source: <a href=https://openai.com/index/gpt-5-safe-completions/>OpenAI, 2025</a>)</p></figcaption></figure><h3 id=思维链监控>思维链监控<a hidden class=anchor aria-hidden=true href=#思维链监控>#</a></h3><p>OpenAI 采用 <strong>思维链监控 (CoT Monitoring)</strong>(<a href=https://arxiv.org/abs/2503.11926>Baker et al., 2025</a>)技术来确保推理模型的可靠性、安全性以及避免出现 <a href=https://lilianweng.github.io/posts/2024-11-28-reward-hacking/>reward hacking</a>。与一些试图通过 SFT 来优化 CoT 的做法不同，GPT-5 的 CoT 训练没有施加直接对齐。这使得 CoT 能够更真实地反映模型的“思考”过程，成为一个有效的窗口，用于检测模型的错误行为、欺骗意图或潜在的风险。</p><figure class=align-center><img loading=lazy src=monitor_frontier_reasoning.png#center alt="Fig. 9. Monitoring Frontier Reasoning Models for Reward Hacking. (Image source: Baker et al., 2025)" width=100%><figcaption><p>Fig. 9. Monitoring Frontier Reasoning Models for Reward Hacking. (Image source: <a href=https://arxiv.org/abs/2503.11926>Baker et al., 2025</a>)</p></figcaption></figure><p>通过 CoT 监控，OpenAI 发现 <code>gpt-5-thinking</code> 的欺骗行为发生率从 o3 的 4.8% 降低到了 2.1%。这项技术对于理解和缓解高级 AI 系统的风险至关重要。</p><h3 id=评估-1>评估<a hidden class=anchor aria-hidden=true href=#评估-1>#</a></h3><p>GPT-5 在多个基准测试中表现出色，尤其在推理、编码和多模态能力上树立了新的标杆。与前代模型相比，它不仅在准确率上实现了飞跃，还在效率上取得了显著进步，通常能用 50-80% 更少的输出来达到甚至超越 o3 的性能。</p><figure class=align-center><img loading=lazy src=gpt5_swe_bench.svg#center alt="Fig. 10. GPT-5 performance in SWE-bench Verified Software Engineering. (Image source: OpenAI, 2025)" width=100%><figcaption><p>Fig. 10. GPT-5 performance in SWE-bench Verified Software Engineering. (Image source: <a href=https://openai.com/index/introducing-gpt-5/>OpenAI, 2025</a>)</p></figcaption></figure><h4 id=-智慧>🧠 智慧<a hidden class=anchor aria-hidden=true href=#-智慧>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>AIME ’25</td><td><strong>94.6%</strong></td><td>91.1%</td><td>85.2%</td><td>88.9%</td><td>92.7%</td><td>46.4%</td><td>40.2%</td><td>-</td></tr><tr><td>FrontierMath (with python tool only)</td><td><strong>26.3%</strong></td><td>22.1%</td><td>9.6%</td><td>15.8%</td><td>15.4%</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GPQA diamond</td><td><strong>85.7%</strong></td><td>82.3%</td><td>71.2%</td><td>83.3%</td><td>81.4%</td><td>66.3%</td><td>65.0%</td><td>50.3%</td></tr><tr><td>HLE</td><td><strong>24.8%</strong></td><td>16.7%</td><td>8.7%</td><td>20.2%</td><td>14.7%</td><td>5.4%</td><td>3.7%</td><td>-</td></tr><tr><td>HMMT 2025</td><td><strong>93.3%</strong></td><td>87.8%</td><td>75.6%</td><td>81.7%</td><td>85.0%</td><td>28.9%</td><td>35.0%</td><td>-</td></tr></tbody></table><h4 id=-多模态>🖼️ 多模态<a hidden class=anchor aria-hidden=true href=#-多模态>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>MMMU</td><td><strong>84.2%</strong></td><td>81.6%</td><td>75.6%</td><td>82.9%</td><td>81.6%</td><td>74.8%</td><td>72.7%</td><td>55.4%</td></tr><tr><td>MMMU-Pro (avg across standard and vision sets)</td><td><strong>78.4%</strong></td><td>74.1%</td><td>62.6%</td><td>76.4%</td><td>73.4%</td><td>60.3%</td><td>58.9%</td><td>33.0%</td></tr><tr><td>CharXiv reasoning (python enabled)</td><td><strong>81.1%</strong></td><td>75.5%</td><td>62.7%</td><td>78.6%</td><td>72.0%</td><td>56.7%</td><td>56.8%</td><td>40.5%</td></tr><tr><td>VideoMMMU (max frame 256)</td><td><strong>84.6%</strong></td><td>82.5%</td><td>66.8%</td><td>83.3%</td><td>79.4%</td><td>60.9%</td><td>55.1%</td><td>30.2%</td></tr><tr><td>ERQA</td><td><strong>65.7%</strong></td><td>62.9%</td><td>50.1%</td><td>64.0%</td><td>56.5%</td><td>44.3%</td><td>42.3%</td><td>26.5%</td></tr></tbody></table><h4 id=-编码>💻 编码<a hidden class=anchor aria-hidden=true href=#-编码>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>SWE-Lancer: IC SWE Diamond Freelance Coding Tasks</td><td><strong>$112K</strong></td><td>$75K</td><td>$49K</td><td>$86K</td><td>$66K</td><td>$34K</td><td>$31K</td><td>$9K</td></tr><tr><td>SWE-bench Verified</td><td><strong>74.9%</strong></td><td>71.0%</td><td>54.7%</td><td>69.1%</td><td>68.1%</td><td>54.6%</td><td>23.6%</td><td>-</td></tr><tr><td>Aider polyglot (diff)</td><td><strong>88.0%</strong></td><td>71.6%</td><td>48.4%</td><td>79.6%</td><td>58.2%</td><td>52.9%</td><td>31.6%</td><td>6.2%</td></tr></tbody></table><h4 id=-指令遵循>📋 指令遵循<a hidden class=anchor aria-hidden=true href=#-指令遵循>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>Scale multichallenge (o3-mini grader)</td><td><strong>69.6%</strong></td><td>62.3%</td><td>54.9%</td><td>60.4%</td><td>57.5%</td><td>46.2%</td><td>42.2%</td><td>31.1%</td></tr><tr><td>Internal API instruction following eval (hard)</td><td>64.0%</td><td><strong>65.8%</strong></td><td>56.1%</td><td>47.4%</td><td>44.7%</td><td>49.1%</td><td>45.1%</td><td>31.6%</td></tr><tr><td>COLLIE</td><td><strong>99.0%</strong></td><td>98.5%</td><td>96.9%</td><td>98.4%</td><td>96.1%</td><td>65.8%</td><td>54.6%</td><td>42.5%</td></tr></tbody></table><h4 id=-工具调用>🔧 工具调用<a hidden class=anchor aria-hidden=true href=#-工具调用>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>Tau²-bench airline</td><td>62.6%</td><td>60.0%</td><td>41.0%</td><td><strong>64.8%</strong></td><td>60.2%</td><td>56.0%</td><td>51.0%</td><td>14.0%</td></tr><tr><td>Tau²-bench retail</td><td><strong>81.1%</strong></td><td>78.3%</td><td>62.3%</td><td>80.2%</td><td>70.5%</td><td>74.0%</td><td>66.0%</td><td>21.5%</td></tr><tr><td>Tau²-bench telecom</td><td><strong>96.7%</strong></td><td>74.1%</td><td>35.5%</td><td>58.2%</td><td>40.5%</td><td>34.0%</td><td>44.0%</td><td>12.1%</td></tr></tbody></table><h4 id=-长上下文>📚 长上下文<a hidden class=anchor aria-hidden=true href=#-长上下文>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th><th>GPT-4.1 nano</th></tr></thead><tbody><tr><td>OpenAI-MRCR: 2 needle 128k</td><td><strong>95.2%</strong></td><td>84.3%</td><td>43.2%</td><td>55.0%</td><td>56.4%</td><td>57.2%</td><td>47.2%</td><td>36.6%</td></tr><tr><td>OpenAI-MRCR: 2 needle 256k</td><td><strong>86.8%</strong></td><td>58.8%</td><td>34.9%</td><td>-</td><td>-</td><td>56.2%</td><td>45.5%</td><td>22.6%</td></tr><tr><td>Graphwalks bfs &lt;128k</td><td><strong>78.3%</strong></td><td>73.4%</td><td>64.0%</td><td>77.3%</td><td>62.3%</td><td>61.7%</td><td>61.7%</td><td>25.0%</td></tr><tr><td>Graphwalks parents &lt;128k</td><td><strong>73.3%</strong></td><td>64.3%</td><td>43.8%</td><td>72.9%</td><td>51.1%</td><td>58.0%</td><td>60.5%</td><td>9.4%</td></tr><tr><td>BrowseComp Long Context 128k</td><td><strong>90.0%</strong></td><td>89.4%</td><td>80.4%</td><td>88.3%</td><td>80.0%</td><td>85.9%</td><td>89.0%</td><td>89.4%</td></tr><tr><td>BrowseComp Long Context 256k</td><td><strong>88.8%</strong></td><td>86.0%</td><td>68.4%</td><td>-</td><td>-</td><td>75.5%</td><td>81.6%</td><td>19.1%</td></tr><tr><td>VideoMME (long, with subtitle category)</td><td><strong>86.7%</strong></td><td>78.5%</td><td>65.7%</td><td>84.9%</td><td>79.5%</td><td>78.7%</td><td>68.4%</td><td>55.2%</td></tr></tbody></table><h4 id=-幻觉>🚨 幻觉<a hidden class=anchor aria-hidden=true href=#-幻觉>#</a></h4><table><thead><tr><th>Benchmark</th><th>GPT-5 (high)</th><th>GPT-5 mini (high)</th><th>GPT-5 nano (high)</th><th>OpenAI o3 (high)</th><th>OpenAI o4-mini (high)</th><th>GPT-4.1</th><th>GPT-4.1 mini</th></tr></thead><tbody><tr><td>LongFact-Concepts hallucination rate (no tools)</td><td>1.0%</td><td><strong>0.7%</strong></td><td>1.0%</td><td>5.2%</td><td>3.0%</td><td><strong>0.7%</strong></td><td>1.1%</td></tr><tr><td>LongFact-Objects hallucination rate (no tools)</td><td>1.2%</td><td>1.3%</td><td>2.8%</td><td>6.8%</td><td>8.9%</td><td><strong>1.1%</strong></td><td>1.8%</td></tr><tr><td>FActScore hallucination rate (no tools)</td><td><strong>2.8%</strong></td><td>3.5%</td><td>7.3%</td><td>23.5%</td><td>38.7%</td><td>6.7%</td><td>10.9%</td></tr></tbody></table><p>这些结果表明，GPT-5 在需要深度推理的复杂任务（如 GPQA、AIME）和需要与外部环境交互的智能体任务（如 SWE-bench、τ²-bench）上取得了较大提升。同时，事实准确性的大幅提升（幻觉率降低近 8 倍）也使其在实际应用中更加可靠。</p><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><p>[1] Raschka, S. (2025). <a href=https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the>&ldquo;From GPT-2 to gpt-oss: Analyzing the Architectural Advances.&rdquo;</a> Ahead of AI.</p><p>[2] Radford, Alec, et al. <a href=https://openai.com/index/better-language-models/>&ldquo;Language models are unsupervised multitask learners.&rdquo;</a> OpenAI blog 1.8 (2019): 9.</p><p>[3] OpenAI. (2025). <a href=https://openai.com/index/introducing-gpt-oss/>&ldquo;Introducing gpt-oss.&rdquo;</a> OpenAI Blog.</p><p>[4] OpenAI. (2025). <a href=https://openai.com/index/introducing-gpt-5/>&ldquo;Introducing GPT-5.&rdquo;</a> OpenAI Blog.</p><p>[5] OpenAI. (2025). <a href=https://openai.com/index/gpt-oss-model-card/>&ldquo;gpt-oss-120b & gpt-oss-20b Model Card.&rdquo;</a></p><p>[6] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. <a href=https://arxiv.org/abs/2004.05150>&ldquo;Longformer: The long-document transformer.&rdquo;</a> arXiv preprint arXiv:2004.05150 (2020).</p><p>[7] Jiang, Dongsheng, et al. <a href=https://arxiv.org/abs/2310.06825>&ldquo;Mistral 7B.&rdquo;</a> arXiv preprint arXiv:2310.08825 (2023).</p><p>[8] Xiao, G., et al. (2023). <a href=https://arxiv.org/abs/2309.17453>&ldquo;Efficient Streaming Language Models with Attention Sinks.&rdquo;</a> arXiv preprint arXiv:2309.17453.</p><p>[9] Tseng, Albert, Tao Yu, and Youngsuk Park. <a href=https://arxiv.org/abs/2502.20586>&ldquo;Training llms with mxfp4.&rdquo;</a> arXiv preprint arXiv:2502.20586 (2025).</p><p>[10] Yuan, Yuan, et al. <a href=https://www.arxiv.org/abs/2508.09224>&ldquo;From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training.&rdquo;</a> arXiv preprint arXiv:2508.09224 (2025).</p><p>[11] B. Baker, J. Huizinga, L. Gao, Z. Dou, M. Y. Guan, A. Madry, W. Zaremba, J. Pachocki, and D. Farhi, <a href=https://arxiv.org/abs/2503.11926>&ldquo;Monitoring reasoning models for misbehavior and the risks of promoting obfuscation.&rdquo;</a> arXiv preprint arXiv:2503.11926, 2025. Submitted on 14 March 2025.</p><p>[12] OpenAI. (2025). <a href=https://openai.com/index/gpt-5-system-card/>&ldquo;GPT-5 System Card.&rdquo;</a></p><p>[13] OpenAI. (2025). <a href=https://openai.com/index/introducing-gpt-5-for-developers/>&ldquo;Introducing GPT-5 for developers.&rdquo;</a> OpenAI Blog.</p><h2 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h2><blockquote><p><strong>引用</strong>：转载或引用本文内容时，请注明原作者和来源。</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Aug 2025). gpt-oss & GPT-5.
<a href=https://syhya.github.io/zh/posts/2025-08-24-gpt5>https://syhya.github.io/zh/posts/2025-08-24-gpt5</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>yue_shui_gpt5_2025</span>
</span></span><span class=line><span class=cl>  <span class=err>title</span>   <span class=err>=</span> <span class=err>&#34;gpt-oss</span> <span class=err>&amp;</span> <span class=err>GPT-5&#34;,</span>
</span></span><span class=line><span class=cl>  <span class=err>author</span>  <span class=err>=</span> <span class=err>&#34;Yue</span> <span class=err>Shui&#34;,</span>
</span></span><span class=line><span class=cl>  <span class=err>journal</span> <span class=err>=</span> <span class=err>&#34;syhya.github.io&#34;,</span>
</span></span><span class=line><span class=cl>  <span class=err>year</span>    <span class=err>=</span> <span class=err>&#34;2025&#34;,</span>
</span></span><span class=line><span class=cl>  <span class=err>month</span>   <span class=err>=</span> <span class=err>&#34;Aug&#34;,</span>
</span></span><span class=line><span class=cl>  <span class=err>url</span>     <span class=err>=</span> <span class=err>&#34;https://syhya.github.io/zh/posts/2025-08-24-gpt5&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/gpt-oss/>Gpt-Oss</a></li><li><a href=https://syhya.github.io/zh/tags/gpt-5/>GPT-5</a></li><li><a href=https://syhya.github.io/zh/tags/moe/>MoE</a></li><li><a href=https://syhya.github.io/zh/tags/reasoning/>Reasoning</a></li><li><a href=https://syhya.github.io/zh/tags/tool-use/>Tool Use</a></li><li><a href=https://syhya.github.io/zh/tags/openai/>OpenAI</a></li><li><a href=https://syhya.github.io/zh/tags/llm-architecture/>LLM Architecture</a></li></ul><nav class=paginav><a class=next href=https://syhya.github.io/zh/posts/2025-06-29-llm-inference/><span class=title>下一页 »</span><br><span>大语言模型推理</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on x" href="https://x.com/intent/tweet/?text=gpt-oss%20%26%20GPT-5&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-08-24-gpt5%2f&amp;hashtags=gpt-oss%2cGPT-5%2cMoE%2cReasoning%2cToolUse%2cOpenAI%2cLLMArchitecture"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-08-24-gpt5%2f&amp;title=gpt-oss%20%26%20GPT-5&amp;summary=gpt-oss%20%26%20GPT-5&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-08-24-gpt5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-08-24-gpt5%2f&title=gpt-oss%20%26%20GPT-5"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-08-24-gpt5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on whatsapp" href="https://api.whatsapp.com/send?text=gpt-oss%20%26%20GPT-5%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-08-24-gpt5%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on telegram" href="https://telegram.me/share/url?text=gpt-oss%20%26%20GPT-5&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-08-24-gpt5%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share gpt-oss & GPT-5 on ycombinator" href="https://news.ycombinator.com/submitlink?t=gpt-oss%20%26%20GPT-5&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-08-24-gpt5%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>