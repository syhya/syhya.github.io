<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scaling Laws | Yue Shui 博客</title><meta name=keywords content="Deep Learning,AI,LLM,Scaling Laws,Test-Time Compute,Reinforcement Learning,Reward Model,Compute-Optimal"><meta name=description content="从 GPT 系列的演进中，研究者逐渐意识到：只要持续扩大模型参数、训练数据和计算资源，大模型性能便会沿着稳定且可预测的路径不断提升。这种可预测性正是由 Scaling Laws 所刻画，它为成本高昂的预训练提供了理论基础与实践信心。随着模型规模、对齐技术以及推理阶段的计算不断协同演化，AI 的能力边界正在系统性地被推高。它不仅是构建下一代模型的基础，也是在算力约束下持续提升模型能力的关键方法论。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2025-11-19-scaling-law/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-11-19-scaling-law/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-11-19-scaling-law/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2025-11-19-scaling-law/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="Scaling Laws"><meta property="og:description" content="从 GPT 系列的演进中，研究者逐渐意识到：只要持续扩大模型参数、训练数据和计算资源，大模型性能便会沿着稳定且可预测的路径不断提升。这种可预测性正是由 Scaling Laws 所刻画，它为成本高昂的预训练提供了理论基础与实践信心。随着模型规模、对齐技术以及推理阶段的计算不断协同演化，AI 的能力边界正在系统性地被推高。它不仅是构建下一代模型的基础，也是在算力约束下持续提升模型能力的关键方法论。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-19T12:00:00+08:00"><meta property="article:modified_time" content="2025-11-19T12:00:00+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Scaling Laws"><meta property="article:tag" content="Test-Time Compute"><meta property="article:tag" content="Reinforcement Learning"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="Scaling Laws"><meta name=twitter:description content="从 GPT 系列的演进中，研究者逐渐意识到：只要持续扩大模型参数、训练数据和计算资源，大模型性能便会沿着稳定且可预测的路径不断提升。这种可预测性正是由 Scaling Laws 所刻画，它为成本高昂的预训练提供了理论基础与实践信心。随着模型规模、对齐技术以及推理阶段的计算不断协同演化，AI 的能力边界正在系统性地被推高。它不仅是构建下一代模型的基础，也是在算力约束下持续提升模型能力的关键方法论。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"Scaling Laws","item":"https://syhya.github.io/zh/posts/2025-11-19-scaling-law/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scaling Laws","name":"Scaling Laws","description":"从 GPT 系列的演进中，研究者逐渐意识到：只要持续扩大模型参数、训练数据和计算资源，大模型性能便会沿着稳定且可预测的路径不断提升。这种可预测性正是由 Scaling Laws 所刻画，它为成本高昂的预训练提供了理论基础与实践信心。随着模型规模、对齐技术以及推理阶段的计算不断协同演化，AI 的能力边界正在系统性地被推高。它不仅是构建下一代模型的基础，也是在算力约束下持续提升模型能力的关键方法论。\n","keywords":["Deep Learning","AI","LLM","Scaling Laws","Test-Time Compute","Reinforcement Learning","Reward Model","Compute-Optimal"],"articleBody":"从 GPT 系列的演进中，研究者逐渐意识到：只要持续扩大模型参数、训练数据和计算资源，大模型性能便会沿着稳定且可预测的路径不断提升。这种可预测性正是由 Scaling Laws 所刻画，它为成本高昂的预训练提供了理论基础与实践信心。随着模型规模、对齐技术以及推理阶段的计算不断协同演化，AI 的能力边界正在系统性地被推高。它不仅是构建下一代模型的基础，也是在算力约束下持续提升模型能力的关键方法论。\nScaling Laws Scaling Laws (Kaplan et al., 2020) 是理解大模型性能如何随资源投入而提升的基础。其首次系统地揭示了模型性能与三个核心要素——模型参数量 $N$、数据集大小 $D$ 和训练计算量 $C$ 之间的 幂律(Power-Law)。一个基本的逆幂律关系可以表示为：\n$$ y = a \\left( \\frac{1}{x} \\right)^p $$在对数坐标系下，这个关系呈现为一条直线，这正是在多数尺度定律论文中看到的标志性图表。对于 LLM，y 通常是模型的测试损失，而 x 则是关注的某个尺度变量（如模型参数量）。\n基本定律 Fig. 1. Language modeling performance improves smoothly with model size, dataset size, and training compute. (Image source: Kaplan et al., 2020)\n当其他两个因素不受限制时，模型的测试损失 $L$ 与模型参数量 $N$、数据集大小 $D$ 和计算量 $C$ 分别呈现出幂律关系，这些关系可以被精确地数学建模：\n模型大小 (N)：在数据量充足的情况下，损失随模型非嵌入参数量 $N$ 的增加而降低。 $$ L(N)=\\left(N_{\\mathrm{c}} / N\\right)^{\\alpha_N} ; \\alpha_N \\sim 0.076, \\quad N_{\\mathrm{c}} \\sim 8.8 \\times 10^{13} \\text { (non-embedding parameters) } $$ 数据集大小 (D)：对于 LLM，在训练至收敛（通过早停）时，损失随数据集大小 $D$ 的增加而降低。 $$ L(D)=\\left(D_{\\mathrm{c}} / D\\right)^{\\alpha_D} ; \\alpha_D \\sim 0.095, \\quad D_{\\mathrm{c}} \\sim 5.4 \\times 10^{13} \\text { (tokens) } $$ 训练计算量 (C)：在最优模型大小和充足数据下，损失随最小计算量 $C_{\\min}$ 的增加而降低。 $$ L\\left(C_{\\min }\\right)=\\left(C_{\\mathrm{c}}^{\\min } / C_{\\min }\\right)^{\\alpha_C^{\\min }} ; \\alpha_C^{\\min } \\sim 0.050, \\quad C_{\\mathrm{c}}^{\\min } \\sim 3.1 \\times 10^8 \\text { (PF-days) } $$其中，$N_c, D_c, C_c^{\\min}$ 是常数，而 $\\alpha_N, \\alpha_D, \\alpha_C^{\\min}$ 是尺度指数，决定了性能提升的速度。这项工作还发现了一些重要结论：\n更大的模型更具样本效率：在达到相同的损失水平时，更大的模型需要的训练样本更少。在计算预算增加时，应优先增加模型参数量 $N$，而非数据量 $D$（建议 $N \\propto C^{0.73}, D \\propto C^{0.27}$）。 Fig. 2. Optimal parameters for compute-efficient training. (Image source: Kaplan et al., 2020)\n模型形状影响甚微：在参数总量固定的前提下，模型的深度和宽度等具体架构设计对性能的影响远小于尺度本身。 C ≈ 6ND：在 Decoder-only Transformer 模型中，训练计算量（FLOPs）可以通过一个简洁的公式估算。模型拥有 $N$ 个参数时，前向传播处理一个 token 大约需要 $2N$ 次浮点运算，而反向传播大约是前向的两倍，因此训练每个 token 的成本约为 $6N$ FLOPs。若训练语料包含 $D$ 个 token，则总训练计算量近似为 $6ND$。 多模态 Scaling Laws for Autoregressive Generative Modeling (Henighan et al., 2020) 验证了 scaling law 并非语言模型独有，也适用于多模态模型。自回归 Transformer 在图像、视频及多模态任务中，其性能均随规模增加呈现可预测的幂律提升。\nFig. 3. Smooth scaling of reducible loss across different domains (Image, Video, Multimodal). (Image source: Henighan et al., 2020)\nChinchilla Chinchilla (Hoffmann et al., 2022) 修正了 OpenAI 的结论。通过训练超过 400 个模型，他们发现 OpenAI 由于使用了固定的学习率调度策略，导致并未找到真正的最优解。\nChinchilla 定律指出为了实现计算最优（Compute-Optimal），模型参数量 $N$ 和训练数据量 $D$ 应该等比例扩展。即： $$ N \\propto C^{0.5}, \\quad D \\propto C^{0.5} $$这意味着，在给定计算预算下，大多数现有的大模型（如 GPT-3, Gopher）都属于 undertrained。遵循此定律训练出的 Chinchilla (70B) 在使用与 Gopher (280B) 相同计算量的情况下（数据量扩大 4 倍），性能全面超越后者。\nFig. 4. Training curve envelope and optimal model size/token count projections. (Image source: Hoffmann et al., 2022)\nGPT-4 OpenAI 在 GPT-4 (OpenAI, 2023) 的技术报告中展示了 Scaling Law 的重要应用通过小模型预测大模型的最终性能。他们利用计算量仅为 GPT-4 万分之一的小模型，精确预测了 GPT-4 的最终 loss。其拟合公式引入了不可约损失(irreducible loss) $c$：\n$$ L(C)=a C^b+c $$参数定义：\n$C$：训练计算量。 $L(C)$：在计算量 $C$ 下的模型损失。 $a$：尺度因子 (Scale Coefficient)，控制损失随计算量增加下降的整体幅度。 $b$：幂律指数 (Scaling Exponent)，决定损失下降的速率；指数越大，损失下降越快。 $c$：不可约损失 (Irreducible Loss)，反映数据固有熵，即无论投入多少计算量也无法继续降低的误差下限。 Fig. 5. GPT-4’s final loss on an internal code-token dataset aligns with a power-law trend extrapolated from smaller models when training compute is normalized. (Image source: OpenAI, 2023)\n在以归一化训练计算量为横轴、bits-per-word 为纵轴的图中，小模型训练得到的多个 loss 点呈现出高度稳定的幂律直线关系，而由这些点拟合出的幂律曲线（未使用任何 GPT-4 数据）却几乎精确地落在 GPT-4 的最终损失位置上。\nRM Scaling Laws 在 RLHF 流程中，奖励模型（RM）充当人类偏好的代理。然而，RM 并非完美的判别器。Goodhart’s law 指出：当一个指标成为目标时，它就不再是一个好的指标。在 LLM 训练中过度优化这个代理 RM 会导致在真实目标上的性能下降。\nScaling Laws for Reward Model Overoptimization (Gao et al., 2023) 采用了一种合成数据的设置， 将一个 60 亿参数的 Gold RM 作为真实奖励 $R$，并使用参数量从 300 万到 30 亿的不同代理 RM 作为优化目标。\nFig. 6. Scaling laws for reward model overoptimization for Best-of-N and RL. (Image source: Gao et al., 2023)\n从初始策略到优化后策略的 KL 散度定义为：\n$$ \\mathrm{KL}:=D_{\\mathrm{KL}}\\left(\\pi \\| \\pi_{\\mathrm{init}}\\right) $$并定义距离函数:\n$$ d:=\\sqrt{D_{\\mathrm{KL}}\\left(\\pi \\| \\pi_{\\mathrm{init}}\\right)} $$Gold RM 分数 $R$ 的变化遵循不同的函数形式，具体取决于优化方法:\n$$ R_{\\mathrm{BoN}}(d) = d(\\alpha_{\\mathrm{BoN}} - \\beta_{\\mathrm{BoN}} d) $$Best-of-N (BoN) 拒绝采样 呈现出先增后减的二次方衰减关系。在优化初期真实奖励提升，但超过某一最优点后，奖励随优化加深而显著下降。\n$$ R_{\\mathrm{RL}}(d) = d(\\alpha_{\\mathrm{RL}} - \\beta_{\\mathrm{RL}} \\log d) $$RL 的衰减项为 $\\log d$，下降速度比 BoN 显著更慢，呈现出对数衰减关系。\nFig. 7. The values of $\\alpha_{\\text {bon }}$, $\\beta_{\\text {bon }}$ and $\\beta_{\\mathrm{RL}}$ in the BoN and RL overoptimization scaling laws for both proxy (dashed line) and gold (solid line) rewards as they scale with parameter count. (Image source: Gao et al., 2023)\n上述公式中的系数 $\\alpha$ 和 $\\beta$ 描述了初始优化效率和过优化的严重程度，并且可以得出以下结论：\nRM 参数量至关重要：增加 RM 的参数量可以有效地提升 $\\alpha$ 并降低 $\\beta$，从而缓解过优化问题，获得更高的真实奖励峰值。 RL 比 BoN 更抗过优化：以 KL 散度为度量，RL 在优化和过优化上都比 BoN 慢。 Test-time Scaling 除了在训练阶段投入算力，还可以在推理阶段通过增加计算量来提升性能。这被称为 Test-Time Scaling (Snell et al., 2024)。主要策略包括并行采样（Parallel Sampling）和串行修正（Sequential Refinement）。\n并行采样（Parallel Sampling）：例如 Best-of-N 或多数投票（Majority Voting）。模型针对同一问题生成多个独立样本，然后通过验证器或投票选出最佳答案。 串行修正（Sequential Refinement）：如自我修正（Self-Correction），模型基于上一次输出进行迭代改进。 效率权衡 Fig. 8. Compute-Optimal Scaling for Iterative Self-Refinement and Search: Evaluating Efficiency Against Best-of-N Baselines and Analyzing the Trade-offs Between Test-Time Compute and Pretraining Scaling (Image source: Snell et al., 2024)\n从上图可以得出以下结论：\n最佳策略的选择由问题难度决定，且存在收益边界: 虽然计算最优策略总体上比 Best-of-N 基线更高效（节省约 4 倍算力），但其具体效能高度依赖于任务的复杂度。 简单问题： 模型的初始回答通常在大方向上正确，仅需局部微调。此时，串行修正是最高效的策略，能以极小的算力代价快速修复细节。 困难问题： 尽管并行搜索能探索更多路径，但随着问题难度提升，单纯增加测试时推理（无论是搜索还是修正）的收益会迅速边际递减。对于极难问题，测试时算力很难突破模型能力的天花板。 测试时算力无法完全替代预训练，基座能力依然是根本: 测试时算力（Test-time Compute）与预训练算力（Pretraining Compute）并非 1:1 等价交换。 交换的先决条件（$Y \\ll X$）： 只有当推理端的 Token 预算（$Y$）远小于预训练端的 Token 预算（$X$），且面对的是中低难度问题时，增加推理时间才是划算的。 基座模型的不可替代性： 一旦问题变难或推理需求过高，小模型的推理技巧（搜索/修正）无法填补巨大的能力鸿沟。扩大预训练规模提供的广泛知识与强推理能力，依然是解决复杂问题的决定性根基。 Simple test-time scaling s1: Simple test-time scaling (Muennighoff et al., 2025) 收集了包含 1000 个配有推理过程的问题数据集 s1K，之后以 Qwen2.5-32B-Instruct 为基座模型进行监督微调训练，其中采用了Budget Forcing 的方法来控制输出 token 的长度以此研究 test-time scaling。\nFig. 9. Budget forcing in s1-32B: suppressing the end-of-thinking delimiter prompts the model to continue after “…is 2.”, triggering a self-correction via “Wait”. (Image source: Muennighoff et al., 2025)\n延长思考（Lengthening）：当模型试图结束思考时，抑制结束标识符，并强制追加 Wait 等词语，鼓励模型进行反思和二次检查。 缩短思考（Shortening）：当思考过程超过预设 Token 长度时，强制追加结束标识符（如 或 \"Final Answer:\"），促使模型立即输出结论。 Fig. 10. Sequential and parallel test-time scaling. (Image source: Muennighoff et al., 2025)\n上述结果显示，并行扩展和顺序扩展方法通过 Budget Forcing 延长的思考时间（Token 数）与下游任务准确率呈显著的正相关。\nScale RL ScaleRL（Khatri et al., 2025）提出了用于分析和预测 LLM 中 RL 扩展的系统性方法。与预训练中常见的幂律关系不同，RL 训练的性能（如验证集上的预期奖励 $R_C$）随计算量 $C$ 的增长更符合一个S型（Sigmoidal）饱和曲线。该曲线可以用以下公式描述：\n$$ \\overbrace{R_C-R_0}^{\\text {Reward Gain }}=\\overbrace{\\left(A-R_0\\right)}^{\\text {Asymptotic Reward Gain }} \\times \\frac{1}{\\underbrace{1+\\left(C_{\\text {mid }} / C\\right)^B}_{\\text {Compute Efficiency }}} $$参数定义：\n$R_C$: 在计算量 $C$ 下的预期奖励（或 Pass Rate）。 $R_0$: 初始模型的性能。 $A$: 渐近性能上限 (Asymptotic Performance)。代表在无限计算量下模型能达到的理论最高性能。 $B$: 计算效率 (Scaling Exponent)。控制曲线的陡峭程度，值越大代表模型能越快达到性能上限。 $C_{mid}$: 达到总收益一半时所需的计算量。 Fig. 11. Interpretation of the sigmoidal scaling curve for RL. (Image source: Khatri et al., 2025)\n上图展示了这个框架的价值: 它允许研究人员通过在较小计算规模下的早期训练数据来拟合曲线，从而预测一个 RL 配方在更大计算预算下的最终性能和效率，降低了算法探索的成本。\n通过对多种 RL 设计选择（如损失函数、优势归一化、数据课程等）进行大规模消融实验，总结出几条关键原则：\n性能天花板并非普适：不同的 RL 算法（如 GRPO, DAPO, CISPO）具有不同的渐进性能上限 $A$。选择正确的损失函数至关重要。 效率与上限的分离：许多常见的 RL 技巧，如优势归一化、数据课程、长度惩罚等，主要影响计算效率（$B$ 和 $C_{\\text{mid}}$），而对性能上限 $A$ 的影响不大。 稳定可预测的扩展：一个精心设计的、稳定的 RL 配方能够遵循可预测的 S 型曲线轨迹，即使扩展到十万 GPU 小时的级别，其表现也与早期拟合的曲线高度一致。 Fig. 12. ScaleRL demonstrates more scalable performance compared to other prevalent RL methods. (Image source: Khatri et al., 2025)\n参考文献 [1] Kaplan, Jared, et al. “Scaling laws for neural language models.” arXiv preprint arXiv:2001.08361 (2020).\n[2] Henighan, Tom, et al. “Scaling laws for autoregressive generative modeling.” arXiv preprint arXiv:2010.14701 (2020).\n[3] Hoffmann, Jordan, et al. “Training compute-optimal large language models.” arXiv preprint arXiv:2203.15556 (2022).\n[4] Achiam, Josh, et al. “Gpt-4 technical report.” arXiv preprint arXiv:2303.08774 (2023).\n[5] Gao, Leo, John Schulman, and Jacob Hilton. “Scaling laws for reward model overoptimization.” International Conference on Machine Learning. PMLR, 2023.\n[6] Snell, Charlie, et al. “Scaling llm test-time compute optimally can be more effective than scaling model parameters.” arXiv preprint arXiv:2408.03314 (2024).\n[7] Muennighoff, Niklas, et al. “s1: Simple test-time scaling.” Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing. 2025.\n[8] Khatri, Devvrit, et al. “The art of scaling reinforcement learning compute for llms.” arXiv preprint arXiv:2510.13786 (2025).\n引用 引用声明：转载或引用本文内容时，请注明原作者与来源。\nCited as:\nYue Shui. (Nov 2025). Scaling Laws. https://syhya.github.io/zh/posts/2025-11-19-scaling-law/\nOr\n@article{yue_shui_scaling_laws_2025, title = {Scaling Laws}, author = {Yue Shui}, journal = {syhya.github.io}, year = {2025}, month = {November}, url = {https://syhya.github.io/zh/posts/2025-11-19-scaling-law/} } ","wordCount":"4214","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2025-11-19T12:00:00+08:00","dateModified":"2025-11-19T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2025-11-19-scaling-law/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Scaling Laws</h1><div class=post-meta><span title='2025-11-19 12:00:00 +0800 +0800'>Created:&nbsp;2025-11-19</span>&nbsp;·&nbsp;Updated:&nbsp;2025-11-19&nbsp;·&nbsp;9 分钟&nbsp;·&nbsp;4214 字&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;翻译:<ul class=i18n_list><li><a href=https://syhya.github.io/posts/2025-11-19-scaling-law/>En</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#scaling-laws>Scaling Laws</a><ul><li><a href=#基本定律>基本定律</a></li><li><a href=#多模态>多模态</a></li><li><a href=#chinchilla>Chinchilla</a></li><li><a href=#gpt-4>GPT-4</a></li></ul></li><li><a href=#rm-scaling-laws>RM Scaling Laws</a></li><li><a href=#test-time-scaling>Test-time Scaling</a><ul><li><a href=#效率权衡>效率权衡</a></li><li><a href=#simple-test-time-scaling>Simple test-time scaling</a></li></ul></li><li><a href=#scale-rl>Scale RL</a></li><li><a href=#参考文献>参考文献</a></li><li><a href=#引用>引用</a></li></ul></nav></div></details></div><div class=post-content><p>从 GPT 系列的演进中，研究者逐渐意识到：只要持续扩大模型参数、训练数据和计算资源，大模型性能便会沿着稳定且可预测的路径不断提升。这种可预测性正是由 <strong>Scaling Laws</strong> 所刻画，它为成本高昂的预训练提供了理论基础与实践信心。随着模型规模、对齐技术以及推理阶段的计算不断协同演化，AI 的能力边界正在系统性地被推高。它不仅是构建下一代模型的基础，也是在算力约束下持续提升模型能力的关键方法论。</p><h2 id=scaling-laws>Scaling Laws<a hidden class=anchor aria-hidden=true href=#scaling-laws>#</a></h2><p><strong>Scaling Laws</strong> (<a href=https://arxiv.org/abs/2001.08361>Kaplan et al., 2020</a>) 是理解大模型性能如何随资源投入而提升的基础。其首次系统地揭示了模型性能与三个核心要素——模型参数量 $N$、数据集大小 $D$ 和训练计算量 $C$ 之间的 <a href=https://en.wikipedia.org/wiki/Power_law>幂律(Power-Law)</a>。一个基本的逆幂律关系可以表示为：</p>$$
y = a \left( \frac{1}{x} \right)^p
$$<p>在对数坐标系下，这个关系呈现为一条直线，这正是在多数尺度定律论文中看到的标志性图表。对于 LLM，<code>y</code> 通常是模型的测试损失，而 <code>x</code> 则是关注的某个尺度变量（如模型参数量）。</p><h3 id=基本定律>基本定律<a hidden class=anchor aria-hidden=true href=#基本定律>#</a></h3><figure class=align-center><a href=scaling_laws.png data-fancybox=gallery><img loading=lazy src=scaling_laws.png#center alt="Fig. 1. Language modeling performance improves smoothly with model size, dataset size, and training compute. (Image source: Kaplan et al., 2020)" width=100%></a><figcaption><p>Fig. 1. Language modeling performance improves smoothly with model size, dataset size, and training compute. (Image source: <a href=https://arxiv.org/abs/2001.08361>Kaplan et al., 2020</a>)</p></figcaption></figure><p>当其他两个因素不受限制时，模型的测试损失 $L$ 与模型参数量 $N$、数据集大小 $D$ 和计算量 $C$ 分别呈现出幂律关系，这些关系可以被精确地数学建模：</p><ol><li><strong>模型大小 (N)</strong>：在数据量充足的情况下，损失随模型非嵌入参数量 $N$ 的增加而降低。</li></ol>$$
L(N)=\left(N_{\mathrm{c}} / N\right)^{\alpha_N} ; \alpha_N \sim 0.076, \quad N_{\mathrm{c}} \sim 8.8 \times 10^{13} \text { (non-embedding parameters) }
$$<ol start=2><li><strong>数据集大小 (D)</strong>：对于 LLM，在训练至收敛（通过早停）时，损失随数据集大小 $D$ 的增加而降低。</li></ol>$$
L(D)=\left(D_{\mathrm{c}} / D\right)^{\alpha_D} ; \alpha_D \sim 0.095, \quad D_{\mathrm{c}} \sim 5.4 \times 10^{13} \text { (tokens) }
$$<ol start=3><li><strong>训练计算量 (C)</strong>：在最优模型大小和充足数据下，损失随最小计算量 $C_{\min}$ 的增加而降低。</li></ol>$$
L\left(C_{\min }\right)=\left(C_{\mathrm{c}}^{\min } / C_{\min }\right)^{\alpha_C^{\min }} ; \alpha_C^{\min } \sim 0.050, \quad C_{\mathrm{c}}^{\min } \sim 3.1 \times 10^8 \text { (PF-days) }
$$<p>其中，$N_c, D_c, C_c^{\min}$ 是常数，而 $\alpha_N, \alpha_D, \alpha_C^{\min}$ 是尺度指数，决定了性能提升的速度。这项工作还发现了一些重要结论：</p><ul><li><strong>更大的模型更具样本效率</strong>：在达到相同的损失水平时，更大的模型需要的训练样本更少。在计算预算增加时，应优先增加模型参数量 $N$，而非数据量 $D$（建议 $N \propto C^{0.73}, D \propto C^{0.27}$）。</li></ul><figure class=align-center><a href=scaling_law_optimal_parameters.png data-fancybox=gallery><img loading=lazy src=scaling_law_optimal_parameters.png#center alt="Fig. 2. Optimal parameters for compute-efficient training. (Image source: Kaplan et al., 2020)" width=100%></a><figcaption><p>Fig. 2. Optimal parameters for compute-efficient training. (Image source: <a href=https://arxiv.org/abs/2001.08361>Kaplan et al., 2020</a>)</p></figcaption></figure><ul><li><strong>模型形状影响甚微</strong>：在参数总量固定的前提下，模型的深度和宽度等具体架构设计对性能的影响远小于尺度本身。</li><li><strong>C ≈ 6ND</strong>：在 Decoder-only Transformer 模型中，训练计算量（FLOPs）可以通过一个简洁的公式估算。模型拥有 $N$ 个参数时，前向传播处理一个 token 大约需要 $2N$ 次浮点运算，而反向传播大约是前向的两倍，因此训练每个 token 的成本约为 $6N$ FLOPs。若训练语料包含 $D$ 个 token，则总训练计算量近似为 $6ND$。</li></ul><h3 id=多模态>多模态<a hidden class=anchor aria-hidden=true href=#多模态>#</a></h3><p><strong>Scaling Laws for Autoregressive Generative Modeling</strong> (<a href=https://arxiv.org/abs/2010.14701>Henighan et al., 2020</a>) 验证了 scaling law 并非语言模型独有，也适用于<strong>多模态模型</strong>。自回归 Transformer 在图像、视频及多模态任务中，其性能均随规模增加呈现可预测的幂律提升。</p><figure class=align-center><a href=autoregressive_scaling_law.png data-fancybox=gallery><img loading=lazy src=autoregressive_scaling_law.png#center alt="Fig. 3. Smooth scaling of reducible loss across different domains (Image, Video, Multimodal). (Image source: Henighan et al., 2020)" width=100%></a><figcaption><p>Fig. 3. Smooth scaling of reducible loss across different domains (Image, Video, Multimodal). (Image source: <a href=https://arxiv.org/abs/2010.14701>Henighan et al., 2020</a>)</p></figcaption></figure><h3 id=chinchilla>Chinchilla<a hidden class=anchor aria-hidden=true href=#chinchilla>#</a></h3><p><strong>Chinchilla</strong> (<a href=https://arxiv.org/abs/2203.15556>Hoffmann et al., 2022</a>) 修正了 OpenAI 的结论。通过训练超过 400 个模型，他们发现 OpenAI 由于使用了固定的学习率调度策略，导致并未找到真正的最优解。</p><p>Chinchilla 定律指出<strong>为了实现计算最优（Compute-Optimal），模型参数量 $N$ 和训练数据量 $D$ 应该等比例扩展</strong>。即：</p>$$ N \propto C^{0.5}, \quad D \propto C^{0.5} $$<p>这意味着，在给定计算预算下，大多数现有的大模型（如 GPT-3, Gopher）都属于 <em>undertrained</em>。遵循此定律训练出的 Chinchilla (70B) 在使用与 Gopher (280B) 相同计算量的情况下（数据量扩大 4 倍），性能全面超越后者。</p><figure class=align-center><a href=Chinchilla.png data-fancybox=gallery><img loading=lazy src=Chinchilla.png#center alt="Fig. 4. Training curve envelope and optimal model size/token count projections. (Image source: Hoffmann et al., 2022)" width=100%></a><figcaption><p>Fig. 4. Training curve envelope and optimal model size/token count projections. (Image source: <a href=https://arxiv.org/abs/2203.15556>Hoffmann et al., 2022</a>)</p></figcaption></figure><h3 id=gpt-4>GPT-4<a hidden class=anchor aria-hidden=true href=#gpt-4>#</a></h3><p>OpenAI 在 <strong>GPT-4</strong> (<a href=https://arxiv.org/abs/2303.08774>OpenAI, 2023</a>) 的技术报告中展示了 Scaling Law 的重要应用<strong>通过小模型预测大模型的最终性能</strong>。他们利用计算量仅为 GPT-4 万分之一的小模型，精确预测了 GPT-4 的最终 loss。其拟合公式引入了<strong>不可约损失(irreducible loss)</strong> $c$：</p>$$
L(C)=a C^b+c
$$<p><strong>参数定义：</strong></p><ul><li>$C$：训练计算量。</li><li>$L(C)$：在计算量 $C$ 下的模型损失。</li><li>$a$：<strong>尺度因子 (Scale Coefficient)</strong>，控制损失随计算量增加下降的整体幅度。</li><li>$b$：<strong>幂律指数 (Scaling Exponent)</strong>，决定损失下降的速率；指数越大，损失下降越快。</li><li>$c$：<strong>不可约损失 (Irreducible Loss)</strong>，反映数据固有熵，即无论投入多少计算量也无法继续降低的误差下限。</li></ul><figure class=align-center><a href=gpt4_loss.png data-fancybox=gallery><img loading=lazy src=gpt4_loss.png#center alt="Fig. 5. GPT-4’s final loss on an internal code-token dataset aligns with a power-law trend extrapolated from smaller models when training compute is normalized. (Image source: OpenAI, 2023)" width=100%></a><figcaption><p>Fig. 5. GPT-4’s final loss on an internal code-token dataset aligns with a power-law trend extrapolated from smaller models when training compute is normalized. (Image source: <a href=https://arxiv.org/abs/2303.08774>OpenAI, 2023</a>)</p></figcaption></figure><p>在以归一化训练计算量为横轴、bits-per-word 为纵轴的图中，小模型训练得到的多个 loss 点呈现出高度稳定的幂律直线关系，而由这些点拟合出的幂律曲线（未使用任何 GPT-4 数据）却几乎精确地落在 GPT-4 的最终损失位置上。</p><h2 id=rm-scaling-laws>RM Scaling Laws<a hidden class=anchor aria-hidden=true href=#rm-scaling-laws>#</a></h2><p>在 RLHF 流程中，奖励模型（RM）充当人类偏好的代理。然而，RM 并非完美的判别器。<a href=https://en.wikipedia.org/wiki/Goodhart%27s_law>Goodhart&rsquo;s law</a> 指出：<strong>当一个指标成为目标时，它就不再是一个好的指标</strong>。在 LLM 训练中过度优化这个代理 RM 会导致在真实目标上的性能下降。</p><p><strong>Scaling Laws for Reward Model Overoptimization</strong> (<a href=https://arxiv.org/abs/2210.10760>Gao et al., 2023</a>) 采用了一种合成数据的设置， 将一个 60 亿参数的 Gold RM 作为真实奖励 $R$，并使用参数量从 300 万到 30 亿的不同代理 RM 作为优化目标。</p><figure class=align-center><a href=rm_scaling_laws.png data-fancybox=gallery><img loading=lazy src=rm_scaling_laws.png#center alt="Fig. 6. Scaling laws for reward model overoptimization for Best-of-N and RL. (Image source: Gao et al., 2023)" width=100%></a><figcaption><p>Fig. 6. Scaling laws for reward model overoptimization for Best-of-N and RL. (Image source: <a href=https://arxiv.org/abs/2210.10760>Gao et al., 2023</a>)</p></figcaption></figure><p>从初始策略到优化后策略的 KL 散度定义为：</p>$$
\mathrm{KL}:=D_{\mathrm{KL}}\left(\pi \| \pi_{\mathrm{init}}\right)
$$<p>并定义距离函数:</p>$$
d:=\sqrt{D_{\mathrm{KL}}\left(\pi \| \pi_{\mathrm{init}}\right)}
$$<p>Gold RM 分数 $R$ 的变化遵循不同的函数形式，具体取决于优化方法:</p>$$
R_{\mathrm{BoN}}(d) = d(\alpha_{\mathrm{BoN}} - \beta_{\mathrm{BoN}} d)
$$<p><strong>Best-of-N (BoN) 拒绝采样</strong> 呈现出先增后减的<strong>二次方衰减</strong>关系。在优化初期真实奖励提升，但超过某一最优点后，奖励随优化加深而显著下降。</p>$$
R_{\mathrm{RL}}(d) = d(\alpha_{\mathrm{RL}} - \beta_{\mathrm{RL}} \log d)
$$<p><strong>RL</strong> 的衰减项为 $\log d$，下降速度比 BoN 显著更慢，呈现出<strong>对数衰减</strong>关系。</p><figure class=align-center><a href=rm_size.png data-fancybox=gallery><img loading=lazy src=rm_size.png#center alt="Fig. 7. The values of $\alpha_{\text {bon }}$, $\beta_{\text {bon }}$ and $\beta_{\mathrm{RL}}$ in the BoN and RL overoptimization scaling laws for both proxy (dashed line) and gold (solid line) rewards as they scale with parameter count. (Image source: Gao et al., 2023)" width=100%></a><figcaption><p>Fig. 7. The values of $\alpha_{\text {bon }}$, $\beta_{\text {bon }}$ and $\beta_{\mathrm{RL}}$ in the BoN and RL overoptimization scaling laws for both proxy (dashed line) and gold (solid line) rewards as they scale with parameter count. (Image source: <a href=https://arxiv.org/abs/2210.10760>Gao et al., 2023</a>)</p></figcaption></figure><p>上述公式中的系数 $\alpha$ 和 $\beta$ 描述了初始优化效率和过优化的严重程度，并且可以得出以下结论：</p><ol><li><strong>RM 参数量至关重要</strong>：增加 RM 的参数量可以有效地提升 $\alpha$ 并降低 $\beta$，从而缓解过优化问题，获得更高的真实奖励峰值。</li><li><strong>RL 比 BoN 更抗过优化</strong>：以 KL 散度为度量，RL 在优化和过优化上都比 BoN 慢。</li></ol><h2 id=test-time-scaling>Test-time Scaling<a hidden class=anchor aria-hidden=true href=#test-time-scaling>#</a></h2><p>除了在训练阶段投入算力，还可以在推理阶段通过增加计算量来提升性能。这被称为 <strong>Test-Time Scaling</strong> (<a href=https://arxiv.org/abs/2408.03314>Snell et al., 2024</a>)。主要策略包括并行采样（Parallel Sampling）和串行修正（Sequential Refinement）。</p><ul><li><strong>并行采样（Parallel Sampling）</strong>：例如 Best-of-N 或多数投票（Majority Voting）。模型针对同一问题生成多个独立样本，然后通过验证器或投票选出最佳答案。</li><li><strong>串行修正（Sequential Refinement）</strong>：如自我修正（Self-Correction），模型基于上一次输出进行迭代改进。</li></ul><h3 id=效率权衡>效率权衡<a hidden class=anchor aria-hidden=true href=#效率权衡>#</a></h3><figure class=align-center><a href=test_time_scaling.png data-fancybox=gallery><img loading=lazy src=test_time_scaling.png#center alt="Fig. 8. Compute-Optimal Scaling for Iterative Self-Refinement and Search: Evaluating Efficiency Against Best-of-N Baselines and Analyzing the Trade-offs Between Test-Time Compute and Pretraining Scaling (Image source: Snell et al., 2024)" width=100%></a><figcaption><p>Fig. 8. Compute-Optimal Scaling for Iterative Self-Refinement and Search: Evaluating Efficiency Against Best-of-N Baselines and Analyzing the Trade-offs Between Test-Time Compute and Pretraining Scaling (Image source: <a href=https://arxiv.org/abs/2408.03314>Snell et al., 2024</a>)</p></figcaption></figure><p>从上图可以得出以下结论：</p><ol><li><strong>最佳策略的选择由问题难度决定，且存在收益边界</strong>: 虽然计算最优策略总体上比 Best-of-N 基线更高效（节省约 4 倍算力），但其具体效能高度依赖于任务的复杂度。</li></ol><ul><li><strong>简单问题：</strong> 模型的初始回答通常在大方向上正确，仅需局部微调。此时，串行修正是最高效的策略，能以极小的算力代价快速修复细节。</li><li><strong>困难问题：</strong> 尽管并行搜索能探索更多路径，但随着问题难度提升，单纯增加测试时推理（无论是搜索还是修正）的收益会迅速<strong>边际递减</strong>。对于极难问题，测试时算力很难突破模型能力的天花板。</li></ul><ol start=2><li><strong>测试时算力无法完全替代预训练，基座能力依然是根本</strong>: 测试时算力（Test-time Compute）与预训练算力（Pretraining Compute）并非 1:1 等价交换。</li></ol><ul><li><strong>交换的先决条件（$Y \ll X$）：</strong> 只有当推理端的 Token 预算（$Y$）远小于预训练端的 Token 预算（$X$），且面对的是中低难度问题时，增加推理时间才是划算的。</li><li><strong>基座模型的不可替代性：</strong> 一旦问题变难或推理需求过高，小模型的推理技巧（搜索/修正）无法填补巨大的能力鸿沟。扩大预训练规模提供的广泛知识与强推理能力，依然是解决复杂问题的决定性根基。</li></ul><h3 id=simple-test-time-scaling>Simple test-time scaling<a hidden class=anchor aria-hidden=true href=#simple-test-time-scaling>#</a></h3><p><strong>s1: Simple test-time scaling</strong> (<a href=https://arxiv.org/abs/2501.19393>Muennighoff et al., 2025</a>) 收集了包含 1000 个配有推理过程的问题数据集 <a href=https://huggingface.co/datasets/simplescaling/s1K>s1K</a>，之后以 Qwen2.5-32B-Instruct 为基座模型进行监督微调训练，其中采用了<strong>Budget Forcing</strong> 的方法来控制输出 token 的长度以此研究 test-time scaling。</p><figure class=align-center><a href=s1_sample.png data-fancybox=gallery><img loading=lazy src=s1_sample.png#center alt="Fig. 9. Budget forcing in s1-32B: suppressing the end-of-thinking delimiter prompts the model to continue after “&mldr;is 2.”, triggering a self-correction via “Wait”. (Image source: Muennighoff et al., 2025)" width=100%></a><figcaption><p>Fig. 9. Budget forcing in s1-32B: suppressing the end-of-thinking delimiter prompts the model to continue after “&mldr;is 2.”, triggering a self-correction via “Wait”. (Image source: <a href=https://arxiv.org/abs/2501.19393>Muennighoff et al., 2025</a>)</p></figcaption></figure><ul><li><strong>延长思考（Lengthening）</strong>：当模型试图结束思考时，抑制结束标识符，并强制追加 <code>Wait</code> 等词语，鼓励模型进行反思和二次检查。</li><li><strong>缩短思考（Shortening）</strong>：当思考过程超过预设 Token 长度时，强制追加结束标识符（如 <code>&lt;/think></code> 或 <code>"Final Answer:"</code>），促使模型立即输出结论。</li></ul><figure class=align-center><a href=s1_result.png data-fancybox=gallery><img loading=lazy src=s1_result.png#center alt="Fig. 10. Sequential and parallel test-time scaling. (Image source: Muennighoff et al., 2025)" width=100%></a><figcaption><p>Fig. 10. Sequential and parallel test-time scaling. (Image source: <a href=https://arxiv.org/abs/2501.19393>Muennighoff et al., 2025</a>)</p></figcaption></figure><p>上述结果显示，并行扩展和顺序扩展方法通过 Budget Forcing 延长的思考时间（Token 数）与下游任务准确率呈显著的正相关。</p><h2 id=scale-rl>Scale RL<a hidden class=anchor aria-hidden=true href=#scale-rl>#</a></h2><p><strong>ScaleRL</strong>（<a href=https://arxiv.org/abs/2510.13786>Khatri et al., 2025</a>）提出了用于分析和预测 LLM 中 RL 扩展的系统性方法。与预训练中常见的幂律关系不同，RL 训练的性能（如验证集上的预期奖励 $R_C$）随计算量 $C$ 的增长更符合一个<strong>S型（Sigmoidal）饱和曲线</strong>。该曲线可以用以下公式描述：</p>$$
\overbrace{R_C-R_0}^{\text {Reward Gain }}=\overbrace{\left(A-R_0\right)}^{\text {Asymptotic Reward Gain }} \times \frac{1}{\underbrace{1+\left(C_{\text {mid }} / C\right)^B}_{\text {Compute Efficiency }}}
$$<p><strong>参数定义：</strong></p><ul><li>$R_C$: 在计算量 $C$ 下的预期奖励（或 Pass Rate）。</li><li>$R_0$: 初始模型的性能。</li><li>$A$: <strong>渐近性能上限 (Asymptotic Performance)</strong>。代表在无限计算量下模型能达到的理论最高性能。</li><li>$B$: <strong>计算效率 (Scaling Exponent)</strong>。控制曲线的陡峭程度，值越大代表模型能越快达到性能上限。</li><li>$C_{mid}$: 达到总收益一半时所需的计算量。</li></ul><figure class=align-center><a href=rl_scaling_fit.png data-fancybox=gallery><img loading=lazy src=rl_scaling_fit.png#center alt="Fig. 11. Interpretation of the sigmoidal scaling curve for RL. (Image source: Khatri et al., 2025)" width=100%></a><figcaption><p>Fig. 11. Interpretation of the sigmoidal scaling curve for RL. (Image source: <a href=https://arxiv.org/abs/2510.13786>Khatri et al., 2025</a>)</p></figcaption></figure><p>上图展示了这个框架的价值: 它允许研究人员通过在较小计算规模下的早期训练数据来拟合曲线，从而<strong>预测</strong>一个 RL 配方在更大计算预算下的最终性能和效率，降低了算法探索的成本。</p><p>通过对多种 RL 设计选择（如损失函数、优势归一化、数据课程等）进行大规模消融实验，总结出几条关键原则：</p><ol><li><strong>性能天花板并非普适</strong>：不同的 RL 算法（如 GRPO, DAPO, CISPO）具有不同的渐进性能上限 $A$。选择正确的损失函数至关重要。</li><li><strong>效率与上限的分离</strong>：许多常见的 RL 技巧，如优势归一化、数据课程、长度惩罚等，主要影响计算效率（$B$ 和 $C_{\text{mid}}$），而对性能上限 $A$ 的影响不大。</li><li><strong>稳定可预测的扩展</strong>：一个精心设计的、稳定的 RL 配方能够遵循可预测的 S 型曲线轨迹，即使扩展到十万 GPU 小时的级别，其表现也与早期拟合的曲线高度一致。</li></ol><figure class=align-center><a href=scale_rl_result.png data-fancybox=gallery><img loading=lazy src=scale_rl_result.png#center alt="Fig. 12. ScaleRL demonstrates more scalable performance compared to other prevalent RL methods. (Image source: Khatri et al., 2025)" width=90%></a><figcaption><p>Fig. 12. ScaleRL demonstrates more scalable performance compared to other prevalent RL methods. (Image source: <a href=https://arxiv.org/abs/2510.13786>Khatri et al., 2025</a>)</p></figcaption></figure><h2 id=参考文献>参考文献<a hidden class=anchor aria-hidden=true href=#参考文献>#</a></h2><p>[1] Kaplan, Jared, et al. <a href=https://arxiv.org/abs/2001.08361>&ldquo;Scaling laws for neural language models.&rdquo;</a> arXiv preprint arXiv:2001.08361 (2020).</p><p>[2] Henighan, Tom, et al. <a href=https://arxiv.org/abs/2010.14701>&ldquo;Scaling laws for autoregressive generative modeling.&rdquo;</a> arXiv preprint arXiv:2010.14701 (2020).</p><p>[3] Hoffmann, Jordan, et al. <a href=https://arxiv.org/abs/2203.15556>&ldquo;Training compute-optimal large language models.&rdquo;</a> arXiv preprint arXiv:2203.15556 (2022).</p><p>[4] Achiam, Josh, et al. <a href=https://arxiv.org/abs/2303.08774>&ldquo;Gpt-4 technical report.&rdquo;</a> arXiv preprint arXiv:2303.08774 (2023).</p><p>[5] Gao, Leo, John Schulman, and Jacob Hilton. <a href=https://arxiv.org/abs/2210.10760>&ldquo;Scaling laws for reward model overoptimization.&rdquo;</a> International Conference on Machine Learning. PMLR, 2023.</p><p>[6] Snell, Charlie, et al. <a href=https://arxiv.org/abs/2408.03314>&ldquo;Scaling llm test-time compute optimally can be more effective than scaling model parameters.&rdquo;</a> arXiv preprint arXiv:2408.03314 (2024).</p><p>[7] Muennighoff, Niklas, et al. <a href=https://arxiv.org/abs/2501.19393>&ldquo;s1: Simple test-time scaling.&rdquo;</a> Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing. 2025.</p><p>[8] Khatri, Devvrit, et al. <a href=https://arxiv.org/abs/2510.13786>&ldquo;The art of scaling reinforcement learning compute for llms.&rdquo;</a> arXiv preprint arXiv:2510.13786 (2025).</p><h2 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h2><blockquote><p><strong>引用声明</strong>：转载或引用本文内容时，请注明原作者与来源。</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Nov 2025). Scaling Laws.
<a href=https://syhya.github.io/zh/posts/2025-11-19-scaling-law/>https://syhya.github.io/zh/posts/2025-11-19-scaling-law/</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>yue_shui_scaling_laws_2025</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>{Scaling Laws}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>{Yue Shui}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>{syhya.github.io}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>{November}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>{https://syhya.github.io/zh/posts/2025-11-19-scaling-law/}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/zh/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/zh/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/zh/tags/scaling-laws/>Scaling Laws</a></li><li><a href=https://syhya.github.io/zh/tags/test-time-compute/>Test-Time Compute</a></li><li><a href=https://syhya.github.io/zh/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://syhya.github.io/zh/tags/reward-model/>Reward Model</a></li><li><a href=https://syhya.github.io/zh/tags/compute-optimal/>Compute-Optimal</a></li></ul><nav class=paginav><a class=next href=https://syhya.github.io/zh/posts/2025-09-30-agentic-rl/><span class=title>下一页 »</span><br><span>Agentic RL</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on x" href="https://x.com/intent/tweet/?text=Scaling%20Laws&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-11-19-scaling-law%2f&amp;hashtags=DeepLearning%2cAI%2cLLM%2cScalingLaws%2cTest-TimeCompute%2cReinforcementLearning%2cRewardModel%2cCompute-Optimal"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-11-19-scaling-law%2f&amp;title=Scaling%20Laws&amp;summary=Scaling%20Laws&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-11-19-scaling-law%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-11-19-scaling-law%2f&title=Scaling%20Laws"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-11-19-scaling-law%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on whatsapp" href="https://api.whatsapp.com/send?text=Scaling%20Laws%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-11-19-scaling-law%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on telegram" href="https://telegram.me/share/url?text=Scaling%20Laws&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-11-19-scaling-law%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Scaling Laws on ycombinator" href="https://news.ycombinator.com/submitlink?t=Scaling%20Laws&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2025-11-19-scaling-law%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>