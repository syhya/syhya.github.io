<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLMs Alignment: DPO | Yue Shui Blog</title><meta name=keywords content="AI,NLP,LLMs,Post-training,DPO,RLHF,Alignment,Bradley–Terry Model"><meta name=description content="This blog post introduces a streamlined alternative to RLHF called DPO. Like RLHF, DPO is designed to align model outputs with human preferences, but it stands apart with its simplicity and lower resource demands. In scenarios where project resources are limited, DPO emerges as a highly attractive and practical solution worth exploring.
Notations

  
      
          Symbol
          
      
  
  
      
          \( x \)
          User input (Prompt): the question the model needs to answer
      
      
          \( y \)
          Model-generated response (Response / Completion): the text output by the model
      
      
          \( \pi_\theta(y \mid x) \)
          Actor model: The trainable policy used to generate response \(y\); parameterized by \(\theta\)
      
      
          \( \pi_{\mathrm{ref}}(y \mid x) \)
          Reference model: The frozen SFT (Supervised Fine-Tuning) model, serving as the alignment baseline
      
      
          \( r_\phi(x,y) \)
          Reward model: A reward function (with parameter \(\phi\)) used to evaluate the quality of response \(y\)
      
      
          \( V_\psi(x) \)
          Critic model: A value function (with parameter \(\psi\)) used to estimate the future cumulative reward given \(x\)
      
      
          \( \pi^*(y \mid x) \)
          Optimal policy distribution, determined via the reference model and reward function
      
      
          \( r_\theta(x,y) \)
          Reward derived from the Actor model, constructed from \(\pi_\theta\) and \(\pi_{\mathrm{ref}}\)
      
      
          \(\beta\)
          Hyperparameter that controls the weight of the KL penalty or the log-ratio difference term
      
      
          \(\mathbb{D}_{\mathrm{KL}}[P \| Q]\)
          KL divergence, a measure of the difference between probability distributions \(P\) and \(Q\)
      
      
          \(\sigma(z)\)
          Sigmoid function, defined as: \(\sigma(z)=\frac{1}{1+e^{-z}}\)
      
      
          \(\log\)
          Logarithm function
      
      
          \(\mathbb{E}\)
          Expectation operator, used to compute the average value of a random variable
      
      
          \( (y_w, y_l) \)
          A pair of preference data where \( y_w \) is the preferred (better quality) response and \( y_l \) is the lesser one
      
      
          \( P\left(y_w \succ y_l \mid x\right) \)
          The probability that response \( y_w \) is preferred over \( y_l \) given input \(x\)
      
      
          \( Z(x) \)
          Partition function, which normalizes the probability distribution over all responses \(y\)
      
      
          \( \mathcal{L}_{\mathrm{DPO}} \)
          The loss function of DPO
      
  

From RLHF to DPO
RLHF
OpenAI primarily leverages Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) to train InstructGPT (Ouyang et al., 2022), which forms the basis for LLMs (such as ChatGPT, Llama, etc.). The entire training process generally comprises the following three main steps:"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-02-08-dpo/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-02-08-dpo/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-02-08-dpo/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-02-08-dpo/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="LLMs Alignment: DPO"><meta property="og:description" content="This blog post introduces a streamlined alternative to RLHF called DPO. Like RLHF, DPO is designed to align model outputs with human preferences, but it stands apart with its simplicity and lower resource demands. In scenarios where project resources are limited, DPO emerges as a highly attractive and practical solution worth exploring.
Notations Symbol \( x \) User input (Prompt): the question the model needs to answer \( y \) Model-generated response (Response / Completion): the text output by the model \( \pi_\theta(y \mid x) \) Actor model: The trainable policy used to generate response \(y\); parameterized by \(\theta\) \( \pi_{\mathrm{ref}}(y \mid x) \) Reference model: The frozen SFT (Supervised Fine-Tuning) model, serving as the alignment baseline \( r_\phi(x,y) \) Reward model: A reward function (with parameter \(\phi\)) used to evaluate the quality of response \(y\) \( V_\psi(x) \) Critic model: A value function (with parameter \(\psi\)) used to estimate the future cumulative reward given \(x\) \( \pi^*(y \mid x) \) Optimal policy distribution, determined via the reference model and reward function \( r_\theta(x,y) \) Reward derived from the Actor model, constructed from \(\pi_\theta\) and \(\pi_{\mathrm{ref}}\) \(\beta\) Hyperparameter that controls the weight of the KL penalty or the log-ratio difference term \(\mathbb{D}_{\mathrm{KL}}[P \| Q]\) KL divergence, a measure of the difference between probability distributions \(P\) and \(Q\) \(\sigma(z)\) Sigmoid function, defined as: \(\sigma(z)=\frac{1}{1+e^{-z}}\) \(\log\) Logarithm function \(\mathbb{E}\) Expectation operator, used to compute the average value of a random variable \( (y_w, y_l) \) A pair of preference data where \( y_w \) is the preferred (better quality) response and \( y_l \) is the lesser one \( P\left(y_w \succ y_l \mid x\right) \) The probability that response \( y_w \) is preferred over \( y_l \) given input \(x\) \( Z(x) \) Partition function, which normalizes the probability distribution over all responses \(y\) \( \mathcal{L}_{\mathrm{DPO}} \) The loss function of DPO From RLHF to DPO RLHF OpenAI primarily leverages Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) to train InstructGPT (Ouyang et al., 2022), which forms the basis for LLMs (such as ChatGPT, Llama, etc.). The entire training process generally comprises the following three main steps:"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-08T12:00:00+08:00"><meta property="article:modified_time" content="2025-06-16T18:31:58+08:00"><meta property="article:tag" content="AI"><meta property="article:tag" content="NLP"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="Post-Training"><meta property="article:tag" content="DPO"><meta property="article:tag" content="RLHF"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="LLMs Alignment: DPO"><meta name=twitter:description content="This blog post introduces a streamlined alternative to RLHF called DPO. Like RLHF, DPO is designed to align model outputs with human preferences, but it stands apart with its simplicity and lower resource demands. In scenarios where project resources are limited, DPO emerges as a highly attractive and practical solution worth exploring.
Notations

  
      
          Symbol
          
      
  
  
      
          \( x \)
          User input (Prompt): the question the model needs to answer
      
      
          \( y \)
          Model-generated response (Response / Completion): the text output by the model
      
      
          \( \pi_\theta(y \mid x) \)
          Actor model: The trainable policy used to generate response \(y\); parameterized by \(\theta\)
      
      
          \( \pi_{\mathrm{ref}}(y \mid x) \)
          Reference model: The frozen SFT (Supervised Fine-Tuning) model, serving as the alignment baseline
      
      
          \( r_\phi(x,y) \)
          Reward model: A reward function (with parameter \(\phi\)) used to evaluate the quality of response \(y\)
      
      
          \( V_\psi(x) \)
          Critic model: A value function (with parameter \(\psi\)) used to estimate the future cumulative reward given \(x\)
      
      
          \( \pi^*(y \mid x) \)
          Optimal policy distribution, determined via the reference model and reward function
      
      
          \( r_\theta(x,y) \)
          Reward derived from the Actor model, constructed from \(\pi_\theta\) and \(\pi_{\mathrm{ref}}\)
      
      
          \(\beta\)
          Hyperparameter that controls the weight of the KL penalty or the log-ratio difference term
      
      
          \(\mathbb{D}_{\mathrm{KL}}[P \| Q]\)
          KL divergence, a measure of the difference between probability distributions \(P\) and \(Q\)
      
      
          \(\sigma(z)\)
          Sigmoid function, defined as: \(\sigma(z)=\frac{1}{1+e^{-z}}\)
      
      
          \(\log\)
          Logarithm function
      
      
          \(\mathbb{E}\)
          Expectation operator, used to compute the average value of a random variable
      
      
          \( (y_w, y_l) \)
          A pair of preference data where \( y_w \) is the preferred (better quality) response and \( y_l \) is the lesser one
      
      
          \( P\left(y_w \succ y_l \mid x\right) \)
          The probability that response \( y_w \) is preferred over \( y_l \) given input \(x\)
      
      
          \( Z(x) \)
          Partition function, which normalizes the probability distribution over all responses \(y\)
      
      
          \( \mathcal{L}_{\mathrm{DPO}} \)
          The loss function of DPO
      
  

From RLHF to DPO
RLHF
OpenAI primarily leverages Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) to train InstructGPT (Ouyang et al., 2022), which forms the basis for LLMs (such as ChatGPT, Llama, etc.). The entire training process generally comprises the following three main steps:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"LLMs Alignment: DPO","item":"https://syhya.github.io/posts/2025-02-08-dpo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLMs Alignment: DPO","name":"LLMs Alignment: DPO","description":"This blog post introduces a streamlined alternative to RLHF called DPO. Like RLHF, DPO is designed to align model outputs with human preferences, but it stands apart with its simplicity and lower resource demands. In scenarios where project resources are limited, DPO emerges as a highly attractive and practical solution worth exploring.\nNotations Symbol \\( x \\) User input (Prompt): the question the model needs to answer \\( y \\) Model-generated response (Response / Completion): the text output by the model \\( \\pi_\\theta(y \\mid x) \\) Actor model: The trainable policy used to generate response \\(y\\); parameterized by \\(\\theta\\) \\( \\pi_{\\mathrm{ref}}(y \\mid x) \\) Reference model: The frozen SFT (Supervised Fine-Tuning) model, serving as the alignment baseline \\( r_\\phi(x,y) \\) Reward model: A reward function (with parameter \\(\\phi\\)) used to evaluate the quality of response \\(y\\) \\( V_\\psi(x) \\) Critic model: A value function (with parameter \\(\\psi\\)) used to estimate the future cumulative reward given \\(x\\) \\( \\pi^*(y \\mid x) \\) Optimal policy distribution, determined via the reference model and reward function \\( r_\\theta(x,y) \\) Reward derived from the Actor model, constructed from \\(\\pi_\\theta\\) and \\(\\pi_{\\mathrm{ref}}\\) \\(\\beta\\) Hyperparameter that controls the weight of the KL penalty or the log-ratio difference term \\(\\mathbb{D}_{\\mathrm{KL}}[P \\| Q]\\) KL divergence, a measure of the difference between probability distributions \\(P\\) and \\(Q\\) \\(\\sigma(z)\\) Sigmoid function, defined as: \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\) \\(\\log\\) Logarithm function \\(\\mathbb{E}\\) Expectation operator, used to compute the average value of a random variable \\( (y_w, y_l) \\) A pair of preference data where \\( y_w \\) is the preferred (better quality) response and \\( y_l \\) is the lesser one \\( P\\left(y_w \\succ y_l \\mid x\\right) \\) The probability that response \\( y_w \\) is preferred over \\( y_l \\) given input \\(x\\) \\( Z(x) \\) Partition function, which normalizes the probability distribution over all responses \\(y\\) \\( \\mathcal{L}_{\\mathrm{DPO}} \\) The loss function of DPO From RLHF to DPO RLHF OpenAI primarily leverages Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) to train InstructGPT (Ouyang et al., 2022), which forms the basis for LLMs (such as ChatGPT, Llama, etc.). The entire training process generally comprises the following three main steps:\n","keywords":["AI","NLP","LLMs","Post-training","DPO","RLHF","Alignment","Bradley–Terry Model"],"articleBody":"This blog post introduces a streamlined alternative to RLHF called DPO. Like RLHF, DPO is designed to align model outputs with human preferences, but it stands apart with its simplicity and lower resource demands. In scenarios where project resources are limited, DPO emerges as a highly attractive and practical solution worth exploring.\nNotations Symbol \\( x \\) User input (Prompt): the question the model needs to answer \\( y \\) Model-generated response (Response / Completion): the text output by the model \\( \\pi_\\theta(y \\mid x) \\) Actor model: The trainable policy used to generate response \\(y\\); parameterized by \\(\\theta\\) \\( \\pi_{\\mathrm{ref}}(y \\mid x) \\) Reference model: The frozen SFT (Supervised Fine-Tuning) model, serving as the alignment baseline \\( r_\\phi(x,y) \\) Reward model: A reward function (with parameter \\(\\phi\\)) used to evaluate the quality of response \\(y\\) \\( V_\\psi(x) \\) Critic model: A value function (with parameter \\(\\psi\\)) used to estimate the future cumulative reward given \\(x\\) \\( \\pi^*(y \\mid x) \\) Optimal policy distribution, determined via the reference model and reward function \\( r_\\theta(x,y) \\) Reward derived from the Actor model, constructed from \\(\\pi_\\theta\\) and \\(\\pi_{\\mathrm{ref}}\\) \\(\\beta\\) Hyperparameter that controls the weight of the KL penalty or the log-ratio difference term \\(\\mathbb{D}_{\\mathrm{KL}}[P \\| Q]\\) KL divergence, a measure of the difference between probability distributions \\(P\\) and \\(Q\\) \\(\\sigma(z)\\) Sigmoid function, defined as: \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\) \\(\\log\\) Logarithm function \\(\\mathbb{E}\\) Expectation operator, used to compute the average value of a random variable \\( (y_w, y_l) \\) A pair of preference data where \\( y_w \\) is the preferred (better quality) response and \\( y_l \\) is the lesser one \\( P\\left(y_w \\succ y_l \\mid x\\right) \\) The probability that response \\( y_w \\) is preferred over \\( y_l \\) given input \\(x\\) \\( Z(x) \\) Partition function, which normalizes the probability distribution over all responses \\(y\\) \\( \\mathcal{L}_{\\mathrm{DPO}} \\) The loss function of DPO From RLHF to DPO RLHF OpenAI primarily leverages Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) to train InstructGPT (Ouyang et al., 2022), which forms the basis for LLMs (such as ChatGPT, Llama, etc.). The entire training process generally comprises the following three main steps:\nFig. 1. A diagram illustrating the three steps of InstructGPT. (Image source: Ouyang et al., 2022)\nSupervised Fine-Tuning (SFT)\nA pre-trained model is fine-tuned using a large volume of human-annotated examples, resulting in an initial model capable of understanding instructions and generating reasonable responses. This model is referred to as the reference model, \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\).\nReward Model Training\nFor simplicity, assume that for each input \\(x\\), two distinct responses are generated. In practice, multiple responses can be ranked. Two responses \\(y_w\\) (better) and \\(y_l\\) (worse) are generated for the same input \\(x\\), and human ranking provides the preference data. A reward model \\(r_\\phi(x, y)\\) is then trained on this data to predict which response aligns better with human preferences.\nPPO-Based Reinforcement Learning\nUsing feedback from the reward model \\(r_\\phi\\), the Actor model \\(\\pi_\\theta\\) is optimized via the Proximal Policy Optimization (PPO) algorithm to improve response quality. To prevent the model from deviating too far from \\(\\pi_{\\mathrm{ref}}\\), a KL penalty is added during optimization. This stage typically involves the following four models:\n\\(\\pi_\\theta\\): The model (after SFT) that is updated. \\(\\pi_{\\mathrm{ref}}\\): The frozen SFT model used as the alignment baseline. \\(r_\\phi\\): The fixed reward model for evaluating response quality. \\(V_\\psi\\): The critic model that estimates future rewards to assist the update of the Actor model. Limitations of RLHF While RLHF leverages human preference data to enhance model alignment, it comes with several inherent limitations:\nMulti-Model Training: In addition to the Actor model \\(\\pi_\\theta\\), extra models such as the reward model \\(r_\\phi\\) and the Critic model \\(V_\\psi\\) must be trained, making the overall process complex and resource-intensive. High Sampling Cost: LLMs require significant computational resources to generate text. The extensive online sampling during reinforcement learning further increases computational costs; insufficient sampling may lead to suboptimal optimization directions. Training Instability and Hyperparameter Sensitivity: PPO involves numerous hyperparameters (e.g., learning rate, sampling batch size), making tuning complex and the training process prone to instability. Alignment Tax Effect: While improving model alignment, the performance on other tasks may suffer. Introduction to DPO Fig. 2. DPO optimizes for human preferences while avoiding reinforcement learning. (Image source: Rafailov et al., 2023)\nDirect Preference Optimization (DPO) (Rafailov et al., 2023) algorithm was developed to address the above issues of RLHF. Its core idea is to convert the RLHF objective into a contrastive learning task akin to supervised fine-tuning, thereby achieving the following:\nEliminating Reward Model Training: Directly optimize the Actor model \\(\\pi_\\theta\\) using human preference data, without training a separate \\(r_\\phi\\). Removing Reinforcement Learning Sampling: Replace PPO with a contrastive loss function, reducing sampling and computational overhead. Enhancing Training Stability: The supervised learning approach is less sensitive to hyperparameters, leading to a more stable training process. Although DPO might have a lower performance ceiling compared to RLHF in terms of ultimate LLM performance improvements, it offers advantages in resource utilization, reduced implementation complexity, and training stability.\nMethod Comparison Method Training Steps Models Involved Training Approach Advantages Disadvantages RLHF Train a reward model first, then use PPO to optimize the policy \\(\\pi_\\theta\\), \\(\\pi_{\\mathrm{ref}}\\), \\(r_\\phi\\), \\(V_\\psi\\) Reinforcement learning with online sampling Fully leverages human preferences; higher performance potential Resource intensive; unstable training; hyperparameter sensitive DPO Directly train the Actor model using preference data \\(\\pi_\\theta\\), \\(\\pi_{\\mathrm{ref}}\\) Supervised-learning-like approach Simplified process; stable training; lower resource cost Performance ceiling may be lower than RLHF Mathematical Derivation of DPO RLHF Objective and the Optimal Policy Distribution In the alignment of large language models, our goal is to use RLHF to optimize model outputs. Let the input \\( x \\) be drawn from a dataset \\(\\mathcal{D}\\), and let the model generate a response \\( y \\). Denote the trainable model as \\(\\pi_\\theta(y \\mid x)\\) and the reference model as \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\) (typically the SFT model). We also introduce a reward function \\( r(x,y) \\) to measure the quality of a response. The RLHF objective can be written as\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\Big[ r(x,y) \\Big] \\;-\\; \\beta\\, \\mathbb{D}_{\\mathrm{KL}}\\Big[ \\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x) \\Big], \\tag{1} \\]where \\(\\beta\\) is a hyperparameter that balances the reward and the deviation from the reference model. Using the definition of KL divergence,\n\\[ \\mathbb{D}_{\\mathrm{KL}} \\Big[\\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\Big] = \\mathbb{E}_{y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right], \\tag{2} \\]we can rewrite Equation (1) as\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ r(x,y) - \\beta \\, \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right]. \\tag{3} \\]Converting (3) to a minimization problem and dividing by \\(\\beta\\) yields\n\\[ \\min_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} - \\frac{1}{\\beta} r(x,y) \\right]. \\tag{4} \\]Assuming there exists an optimal policy distribution \\(\\pi^*(y \\mid x)\\) that globally minimizes (4), we set\n\\[ \\pi^*(y \\mid x) \\;=\\; \\frac{1}{Z(x)} \\,\\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big), \\tag{5} \\]where the partition function \\( Z(x) \\) is defined as\n\\[ Z(x) = \\sum_{y}\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big). \\tag{6} \\] \\(Z(x)\\) sums over all possible \\(y\\) to normalize the distribution, ensuring that \\(\\pi^*(y \\mid x)\\) is a valid probability distribution. \\(Z(x)\\) is a function of \\(x\\) and is independent of the trainable Actor model \\(\\pi_\\theta\\). Taking the logarithm of (5) gives\n\\[ \\log \\pi^*(y \\mid x) = \\log \\pi_{\\mathrm{ref}}(y \\mid x) + \\frac{1}{\\beta}\\, r(x,y) - \\log Z(x), \\tag{7} \\]which can be rearranged to obtain\n\\[ r(x,y) = \\beta \\left[\\log \\frac{\\pi^*(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{8} \\]The Bradley–Terry Model To leverage pairwise preference data \\((x, y_w, y_l)\\) for training, we aim for the model to favor the higher-quality response \\( y_w \\) over the lower-quality response \\( y_l \\) for the same input \\( x \\).\nThe Bradley–Terry model is used to predict the outcomes of paired comparisons. For any two items \\( i \\) and \\( j \\), if we assign each a positive score \\( p_i \\) and \\( p_j \\), then the probability that item \\( i \\) is preferred over item \\( j \\) is\n\\[ \\Pr(i \u003e j) = \\frac{p_i}{p_i + p_j}. \\tag{9} \\]In our scenario, we set the strength parameter for each response \\( y \\) as \\( p_{y} = \\exp\\big(r(x,y)\\big) \\) (ensuring positivity). Therefore, given input \\( x \\), the probability that response \\( y_w \\) is preferred over \\( y_l \\) becomes\n\\[ P\\left(y_w \\succ y_l \\mid x\\right)=\\frac{\\exp \\big[r(x,y_w)\\big]}{\\exp \\big[r(x,y_w)\\big]+\\exp \\big[r(x,y_l)\\big]}. \\tag{10} \\]To maximize the probability that the higher-quality response \\( y_w \\) wins in every preference pair \\((x, y_w, y_l)\\) in the dataset, we design the reward model’s training objective to maximize this probability or, equivalently, to minimize the negative log-likelihood:\n\\[ L_{R}\\left(r_{\\phi}, D\\right) = -\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\left[\\log P\\left(y_w \\succ y_l \\mid x\\right)\\right], \\tag{11} \\]where the dataset is defined as\n\\[ D=\\{(x^i, y_w^i, y_l^i)\\}_{i=1}^{N}. \\tag{12} \\]Using Equations (10) and (11) along with the identity\n\\[ \\log \\frac{e^a}{e^a+e^b} = \\log\\frac{1}{1+e^{b-a}} = \\log \\sigma(a-b), \\tag{13} \\]with the Sigmoid function defined as\n\\[ \\sigma(z)=\\frac{1}{1+e^{-z}}, \\tag{14} \\]we have\n\\[ \\log P\\left(y_w \\succ y_l \\mid x\\right) = \\log \\sigma\\Big(r(x,y_w)-r(x,y_l)\\Big). \\tag{15} \\]Direct Preference Optimization Notice from Equation (8) that the reward \\( r(x,y) \\) is related to the log-ratio of the optimal policy. To avoid explicitly training a separate reward model \\(r_\\phi\\), DPO directly substitutes the trainable Actor model \\(\\pi_\\theta\\) in place of the optimal policy \\(\\pi^*\\) and represents the reward as\n\\[ r_\\theta(x,y) \\;=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{16} \\]In pairwise comparisons, for the same input \\( x \\), both responses \\( y_w \\) and \\( y_l \\) contain the same \\(\\log Z(x)\\) term; therefore, when computing the difference, this term cancels out:\n\\[ \\begin{aligned} r_\\theta(x,y_w)-r_\\theta(x,y_l) \u0026=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} + \\log Z(x)\\right] - \\beta \\left[\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} + \\log Z(x)\\right] \\\\ \u0026=\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)}. \\end{aligned} \\tag{17} \\]Substituting Equation (17) into (15) and combining with (11), we obtain the final DPO loss function:\n\\[ \\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}}) = - \\mathbb{E}_{(x,y_w,y_l) \\sim D} \\left[ \\log \\sigma\\Big( \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} -\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big) \\right]. \\tag{18} \\]This loss function is designed for the trainable Actor model \\(\\pi_\\theta\\). It distinguishes between good and bad responses by comparing the log-probability ratios (relative to the reference model \\(\\pi_{\\mathrm{ref}}\\)) for the high-quality response \\(y_w\\) and the low-quality response \\(y_l\\): when \\(\\pi_\\theta\\) assigns a much higher relative probability to \\(y_w\\) compared to \\(y_l\\), the sigmoid output approaches 1 and the loss decreases; conversely, if the relative probability for \\(y_l\\) is too high, the loss increases. The parameter \\(\\beta\\) scales the difference between these log-ratios, thereby adjusting the model’s sensitivity in distinguishing good from bad responses.\nTraining Process Data Preparation Preference Data Collection\nCollect a large amount of preference data. Each sample includes an input \\( x \\) and a pair of responses \\((y_w, y_l)\\), where \\( y_w \\) is the chosen (preferred) response and \\( y_l \\) is the rejected (lower quality) response.\nDataset Example: dpo_en_demo.json\n{ \"conversations\": [ { \"from\": \"human\", \"value\": \"what does abstraction mean on oop\" } ], \"chosen\": { \"from\": \"gpt\", \"value\": \"Abstraction in Object-Oriented Programming (OOP) is the process of exposing only the necessary information to the outside world while hiding the implementation details. It's a way to simplify complex systems by focusing only on the essential aspects and encapsulating the implementation details.\\n\\nIn other words, abstraction is about hiding the internal workings of an object or system and only exposing the necessary interfaces or APIs to interact with it. This helps to reduce complexity, improve modularity, and make the system easier to understand and maintain.\\n\\nFor example, consider a car. A car has many components like engine, transmission, brakes, etc. However, when you drive a car, you don't need to know how the engine works or how the transmission changes gears. All you need to know is how to operate the car's controls like accelerator, brakes, and steering wheel. The car's controls are an abstraction of the underlying complex systems, allowing you to interact with the car without needing to know the details of how it works.\\n\\nIn programming, abstraction is achieved by defining interfaces or abstract classes that define the essential functions or behaviors of an object without specifying how they are implemented. This allows developers to create different implementations of the same interface or abstract class, which can be swapped out without affecting the rest of the system.\\n\\nAbstraction is a fundamental concept in OOP and is used to create modular, maintainable, and flexible software systems. It helps to reduce code duplication, improve code reuse, and make software systems more understandable and easier to evolve over time.\" }, \"rejected\": { \"from\": \"gpt\", \"value\": \"Abstraction is a principle in object-oriented programming that refers to the process of focusing on the essential features of an object or concept without emphasizing the details that are not important for its functionality. It enables programmers to create classes that represent the objects they want to work with, and specifies only the behavior of those objects, leaving out unnecessary details. Abstraction helps to make the code more maintainable, modular, and scalable. It also improves the productivity of developers by reducing the amount of code they need to write.\" } } Output Precomputation\nTo reduce computational overhead, pre-compute and cache the outputs of the reference model \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\).\nModel Training Training Objective\nDirectly optimize the Actor model \\(\\pi_\\theta\\) by minimizing the DPO loss \\(\\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}})\\) so that its generated responses better align with human preferences.\nTraining Steps\nSample a batch of data \\((x, y_w, y_l)\\) from the dataset.\nCompute the output probabilities of the Actor model \\(\\pi_\\theta(y \\mid x)\\).\nCalculate the loss using:\n\\[ \\mathcal{L}_{\\mathrm{DPO}} = - \\log \\sigma\\Big( \\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big). \\] Update the Actor model parameters \\(\\theta\\) via backpropagation.\nModel Inference Once training is complete, the resulting Actor model \\(\\pi_\\theta\\) can be used directly for inference. Given an input \\( x \\), the model generates responses based on the learned probability distribution. Since human preferences have been incorporated during training and the model is constrained by the reference model \\(\\pi_{\\mathrm{ref}}\\), the generated responses are not only aligned with expectations but also maintain stability in the generated text.\nSummary DPO simplifies the RLHF process into a direct supervised learning task, saving resources, enhancing training stability, and reducing implementation complexity. It serves as an efficient alternative for LLM alignment training. In practical applications, one can choose between RLHF and DPO methods based on the specific business scenario to achieve the best training results.\nReferences [1] Christiano, Paul F., et al. “Deep reinforcement learning from human preferences.” Advances in neural information processing systems 30 (2017).\n[2] Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” Advances in neural information processing systems 35 (2022): 27730-27744.\n[3] Rafailov, Rafael, et al. “Direct preference optimization: Your language model is secretly a reward model.” Advances in Neural Information Processing Systems 36 (2024).\nCitation Citation: When reprinting or citing the contents of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Feb 2025). LLMs Alignment: DPO.\nhttps://syhya.github.io/posts/2025-02-08-dpo\nOr\n@article{syhya2025dpo, title = \"LLMs Alignment: DPO\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Feb\", url = \"https://syhya.github.io/posts/2025-02-08-dpo\" } ","wordCount":"2577","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-02-08T12:00:00+08:00","dateModified":"2025-06-16T18:31:58+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-02-08-dpo/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">LLMs Alignment: DPO</h1><div class=post-meta><span title='2025-02-08 12:00:00 +0800 +0800'>2025-02-08</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;2577 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-02-08-dpo/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#notations>Notations</a></li><li><a href=#from-rlhf-to-dpo>From RLHF to DPO</a><ul><li><a href=#rlhf>RLHF</a></li><li><a href=#limitations-of-rlhf>Limitations of RLHF</a></li><li><a href=#introduction-to-dpo>Introduction to DPO</a></li><li><a href=#method-comparison>Method Comparison</a></li></ul></li><li><a href=#mathematical-derivation-of-dpo>Mathematical Derivation of DPO</a><ul><li><a href=#rlhf-objective-and-the-optimal-policy-distribution>RLHF Objective and the Optimal Policy Distribution</a></li><li><a href=#the-bradleyterry-model>The Bradley–Terry Model</a></li><li><a href=#direct-preference-optimization>Direct Preference Optimization</a></li></ul></li><li><a href=#training-process>Training Process</a><ul><li><a href=#data-preparation>Data Preparation</a></li><li><a href=#model-training>Model Training</a></li><li><a href=#model-inference>Model Inference</a></li></ul></li><li><a href=#summary>Summary</a></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><p>This blog post introduces a streamlined alternative to RLHF called DPO. Like RLHF, DPO is designed to align model outputs with human preferences, but it stands apart with its simplicity and lower resource demands. In scenarios where project resources are limited, DPO emerges as a highly attractive and practical solution worth exploring.</p><h2 id=notations>Notations<a hidden class=anchor aria-hidden=true href=#notations>#</a></h2><table><thead><tr><th>Symbol</th><th></th></tr></thead><tbody><tr><td>\( x \)</td><td>User input (Prompt): the question the model needs to answer</td></tr><tr><td>\( y \)</td><td>Model-generated response (Response / Completion): the text output by the model</td></tr><tr><td>\( \pi_\theta(y \mid x) \)</td><td>Actor model: The trainable policy used to generate response \(y\); parameterized by \(\theta\)</td></tr><tr><td>\( \pi_{\mathrm{ref}}(y \mid x) \)</td><td>Reference model: The frozen SFT (Supervised Fine-Tuning) model, serving as the alignment baseline</td></tr><tr><td>\( r_\phi(x,y) \)</td><td>Reward model: A reward function (with parameter \(\phi\)) used to evaluate the quality of response \(y\)</td></tr><tr><td>\( V_\psi(x) \)</td><td>Critic model: A value function (with parameter \(\psi\)) used to estimate the future cumulative reward given \(x\)</td></tr><tr><td>\( \pi^*(y \mid x) \)</td><td>Optimal policy distribution, determined via the reference model and reward function</td></tr><tr><td>\( r_\theta(x,y) \)</td><td>Reward derived from the Actor model, constructed from \(\pi_\theta\) and \(\pi_{\mathrm{ref}}\)</td></tr><tr><td>\(\beta\)</td><td>Hyperparameter that controls the weight of the KL penalty or the log-ratio difference term</td></tr><tr><td>\(\mathbb{D}_{\mathrm{KL}}[P \| Q]\)</td><td>KL divergence, a measure of the difference between probability distributions \(P\) and \(Q\)</td></tr><tr><td>\(\sigma(z)\)</td><td>Sigmoid function, defined as: \(\sigma(z)=\frac{1}{1+e^{-z}}\)</td></tr><tr><td>\(\log\)</td><td>Logarithm function</td></tr><tr><td>\(\mathbb{E}\)</td><td>Expectation operator, used to compute the average value of a random variable</td></tr><tr><td>\( (y_w, y_l) \)</td><td>A pair of preference data where \( y_w \) is the preferred (better quality) response and \( y_l \) is the lesser one</td></tr><tr><td>\( P\left(y_w \succ y_l \mid x\right) \)</td><td>The probability that response \( y_w \) is preferred over \( y_l \) given input \(x\)</td></tr><tr><td>\( Z(x) \)</td><td>Partition function, which normalizes the probability distribution over all responses \(y\)</td></tr><tr><td>\( \mathcal{L}_{\mathrm{DPO}} \)</td><td>The loss function of DPO</td></tr></tbody></table><h2 id=from-rlhf-to-dpo>From RLHF to DPO<a hidden class=anchor aria-hidden=true href=#from-rlhf-to-dpo>#</a></h2><h3 id=rlhf>RLHF<a hidden class=anchor aria-hidden=true href=#rlhf>#</a></h3><p>OpenAI primarily leverages <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> (<a href=https://arxiv.org/abs/1706.03741>Christiano et al., 2017</a>) to train InstructGPT (<a href=https://arxiv.org/abs/2203.02155>Ouyang et al., 2022</a>), which forms the basis for LLMs (such as ChatGPT, Llama, etc.). The entire training process generally comprises the following three main steps:</p><figure class=align-center><img loading=lazy src=InstructGPT.png#center alt="Fig. 1. A diagram illustrating the three steps of InstructGPT. (Image source: Ouyang et al., 2022)" width=100%><figcaption><p>Fig. 1. A diagram illustrating the three steps of InstructGPT. (Image source: <a href=https://arxiv.org/abs/2203.02155>Ouyang et al., 2022</a>)</p></figcaption></figure><ol><li><p><strong>Supervised Fine-Tuning (SFT)</strong><br>A pre-trained model is fine-tuned using a large volume of human-annotated examples, resulting in an initial model capable of understanding instructions and generating reasonable responses. This model is referred to as the reference model, \(\pi_{\mathrm{ref}}(y \mid x)\).</p></li><li><p><strong>Reward Model Training</strong><br>For simplicity, assume that for each input \(x\), two distinct responses are generated. In practice, multiple responses can be ranked. Two responses \(y_w\) (better) and \(y_l\) (worse) are generated for the same input \(x\), and human ranking provides the preference data. A reward model \(r_\phi(x, y)\) is then trained on this data to predict which response aligns better with human preferences.</p></li><li><p><strong>PPO-Based Reinforcement Learning</strong><br>Using feedback from the reward model \(r_\phi\), the Actor model \(\pi_\theta\) is optimized via the Proximal Policy Optimization (PPO) algorithm to improve response quality. To prevent the model from deviating too far from \(\pi_{\mathrm{ref}}\), a KL penalty is added during optimization. This stage typically involves the following four models:</p><ul><li>\(\pi_\theta\): The model (after SFT) that is updated.</li><li>\(\pi_{\mathrm{ref}}\): The frozen SFT model used as the alignment baseline.</li><li>\(r_\phi\): The fixed reward model for evaluating response quality.</li><li>\(V_\psi\): The critic model that estimates future rewards to assist the update of the Actor model.</li></ul></li></ol><h3 id=limitations-of-rlhf>Limitations of RLHF<a hidden class=anchor aria-hidden=true href=#limitations-of-rlhf>#</a></h3><p>While RLHF leverages human preference data to enhance model alignment, it comes with several inherent limitations:</p><ul><li><strong>Multi-Model Training</strong>: In addition to the Actor model \(\pi_\theta\), extra models such as the reward model \(r_\phi\) and the Critic model \(V_\psi\) must be trained, making the overall process complex and resource-intensive.</li><li><strong>High Sampling Cost</strong>: LLMs require significant computational resources to generate text. The extensive online sampling during reinforcement learning further increases computational costs; insufficient sampling may lead to suboptimal optimization directions.</li><li><strong>Training Instability and Hyperparameter Sensitivity</strong>: PPO involves numerous hyperparameters (e.g., learning rate, sampling batch size), making tuning complex and the training process prone to instability.</li><li><strong>Alignment Tax Effect</strong>: While improving model alignment, the performance on other tasks may suffer.</li></ul><h3 id=introduction-to-dpo>Introduction to DPO<a hidden class=anchor aria-hidden=true href=#introduction-to-dpo>#</a></h3><figure class=align-center><img loading=lazy src=rlhf_dpo.png#center alt="Fig. 2. DPO optimizes for human preferences while avoiding reinforcement learning. (Image source: Rafailov et al., 2023)" width=100%><figcaption><p>Fig. 2. DPO optimizes for human preferences while avoiding reinforcement learning. (Image source: <a href=https://arxiv.org/abs/2305.18290>Rafailov et al., 2023</a>)</p></figcaption></figure><p><strong>Direct Preference Optimization (DPO)</strong> (<a href=https://arxiv.org/abs/2305.18290>Rafailov et al., 2023</a>) algorithm was developed to address the above issues of RLHF. Its core idea is to convert the RLHF objective into a contrastive learning task akin to supervised fine-tuning, thereby achieving the following:</p><ul><li><strong>Eliminating Reward Model Training</strong>: Directly optimize the Actor model \(\pi_\theta\) using human preference data, without training a separate \(r_\phi\).</li><li><strong>Removing Reinforcement Learning Sampling</strong>: Replace PPO with a contrastive loss function, reducing sampling and computational overhead.</li><li><strong>Enhancing Training Stability</strong>: The supervised learning approach is less sensitive to hyperparameters, leading to a more stable training process.</li></ul><p>Although DPO might have a lower performance ceiling compared to RLHF in terms of ultimate LLM performance improvements, it offers advantages in resource utilization, reduced implementation complexity, and training stability.</p><h3 id=method-comparison>Method Comparison<a hidden class=anchor aria-hidden=true href=#method-comparison>#</a></h3><table><thead><tr><th>Method</th><th>Training Steps</th><th>Models Involved</th><th>Training Approach</th><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody><tr><td><strong>RLHF</strong></td><td>Train a reward model first, then use PPO to optimize the policy</td><td>\(\pi_\theta\), \(\pi_{\mathrm{ref}}\), \(r_\phi\), \(V_\psi\)</td><td>Reinforcement learning with online sampling</td><td>Fully leverages human preferences; higher performance potential</td><td>Resource intensive; unstable training; hyperparameter sensitive</td></tr><tr><td><strong>DPO</strong></td><td>Directly train the Actor model using preference data</td><td>\(\pi_\theta\), \(\pi_{\mathrm{ref}}\)</td><td>Supervised-learning-like approach</td><td>Simplified process; stable training; lower resource cost</td><td>Performance ceiling may be lower than RLHF</td></tr></tbody></table><h2 id=mathematical-derivation-of-dpo>Mathematical Derivation of DPO<a hidden class=anchor aria-hidden=true href=#mathematical-derivation-of-dpo>#</a></h2><h3 id=rlhf-objective-and-the-optimal-policy-distribution>RLHF Objective and the Optimal Policy Distribution<a hidden class=anchor aria-hidden=true href=#rlhf-objective-and-the-optimal-policy-distribution>#</a></h3><p>In the alignment of large language models, our goal is to use RLHF to optimize model outputs. Let the input \( x \) be drawn from a dataset \(\mathcal{D}\), and let the model generate a response \( y \). Denote the trainable model as \(\pi_\theta(y \mid x)\) and the reference model as \(\pi_{\mathrm{ref}}(y \mid x)\) (typically the SFT model). We also introduce a reward function \( r(x,y) \) to measure the quality of a response. The RLHF objective can be written as</p>\[
\max_{\pi} \; \mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi(y \mid x)} \Big[ r(x,y) \Big] \;-\; \beta\, \mathbb{D}_{\mathrm{KL}}\Big[ \pi(y \mid x) \,\|\, \pi_{\mathrm{ref}}(y \mid x) \Big],
\tag{1}
\]<p>where \(\beta\) is a hyperparameter that balances the reward and the deviation from the reference model. Using the definition of KL divergence,</p>\[
\mathbb{D}_{\mathrm{KL}} \Big[\pi(y \mid x) \,\|\, \pi_{\mathrm{ref}}(y \mid x)\Big] = \mathbb{E}_{y \sim \pi(y \mid x)} \left[ \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \right],
\tag{2}
\]<p>we can rewrite Equation (1) as</p>\[
\max_{\pi} \; \mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi(y \mid x)} \left[ r(x,y) - \beta \, \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} \right].
\tag{3}
\]<p>Converting (3) to a minimization problem and dividing by \(\beta\) yields</p>\[
\min_{\pi} \; \mathbb{E}_{x \sim \mathcal{D},\, y \sim \pi(y \mid x)} \left[ \log \frac{\pi(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} - \frac{1}{\beta} r(x,y) \right].
\tag{4}
\]<p>Assuming there exists an optimal policy distribution \(\pi^*(y \mid x)\) that globally minimizes (4), we set</p>\[
\pi^*(y \mid x) \;=\; \frac{1}{Z(x)} \,\pi_{\mathrm{ref}}(y \mid x)\, \exp\!\Big(\frac{1}{\beta} \, r(x,y)\Big),
\tag{5}
\]<p>where the partition function \( Z(x) \) is defined as</p>\[
Z(x) = \sum_{y}\, \pi_{\mathrm{ref}}(y \mid x)\, \exp\!\Big(\frac{1}{\beta} \, r(x,y)\Big).
\tag{6}
\]<ul><li>\(Z(x)\) sums over all possible \(y\) to normalize the distribution, ensuring that \(\pi^*(y \mid x)\) is a valid probability distribution.</li><li>\(Z(x)\) is a function of \(x\) and is independent of the trainable Actor model \(\pi_\theta\).</li></ul><p>Taking the logarithm of (5) gives</p>\[
\log \pi^*(y \mid x) = \log \pi_{\mathrm{ref}}(y \mid x) + \frac{1}{\beta}\, r(x,y) - \log Z(x),
\tag{7}
\]<p>which can be rearranged to obtain</p>\[
r(x,y) = \beta \left[\log \frac{\pi^*(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} + \log Z(x)\right].
\tag{8}
\]<h3 id=the-bradleyterry-model>The Bradley–Terry Model<a hidden class=anchor aria-hidden=true href=#the-bradleyterry-model>#</a></h3><p>To leverage pairwise preference data \((x, y_w, y_l)\) for training, we aim for the model to favor the higher-quality response \( y_w \) over the lower-quality response \( y_l \) for the same input \( x \).</p><p>The <a href=https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model>Bradley–Terry model</a> is used to predict the outcomes of paired comparisons. For any two items \( i \) and \( j \), if we assign each a positive score \( p_i \) and \( p_j \), then the probability that item \( i \) is preferred over item \( j \) is</p>\[
\Pr(i > j) = \frac{p_i}{p_i + p_j}.
\tag{9}
\]<p>In our scenario, we set the strength parameter for each response \( y \) as \( p_{y} = \exp\big(r(x,y)\big) \) (ensuring positivity). Therefore, given input \( x \), the probability that response \( y_w \) is preferred over \( y_l \) becomes</p>\[
P\left(y_w \succ y_l \mid x\right)=\frac{\exp \big[r(x,y_w)\big]}{\exp \big[r(x,y_w)\big]+\exp \big[r(x,y_l)\big]}.
\tag{10}
\]<p>To maximize the probability that the higher-quality response \( y_w \) wins in every preference pair \((x, y_w, y_l)\) in the dataset, we design the reward model’s training objective to maximize this probability or, equivalently, to minimize the negative log-likelihood:</p>\[
L_{R}\left(r_{\phi}, D\right) = -\mathbb{E}_{(x,y_w,y_l) \sim D}\left[\log P\left(y_w \succ y_l \mid x\right)\right],
\tag{11}
\]<p>where the dataset is defined as</p>\[
D=\{(x^i, y_w^i, y_l^i)\}_{i=1}^{N}.
\tag{12}
\]<p>Using Equations (10) and (11) along with the identity</p>\[
\log \frac{e^a}{e^a+e^b} = \log\frac{1}{1+e^{b-a}} = \log \sigma(a-b),
\tag{13}
\]<p>with the Sigmoid function defined as</p>\[
\sigma(z)=\frac{1}{1+e^{-z}},
\tag{14}
\]<p>we have</p>\[
\log P\left(y_w \succ y_l \mid x\right) = \log \sigma\Big(r(x,y_w)-r(x,y_l)\Big).
\tag{15}
\]<h3 id=direct-preference-optimization>Direct Preference Optimization<a hidden class=anchor aria-hidden=true href=#direct-preference-optimization>#</a></h3><p>Notice from Equation (8) that the reward \( r(x,y) \) is related to the log-ratio of the optimal policy. To avoid explicitly training a separate reward model \(r_\phi\), DPO directly substitutes the trainable Actor model \(\pi_\theta\) in place of the optimal policy \(\pi^*\) and represents the reward as</p>\[
r_\theta(x,y) \;=\; \beta \left[\log \frac{\pi_\theta(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)} + \log Z(x)\right].
\tag{16}
\]<p>In pairwise comparisons, for the same input \( x \), both responses \( y_w \) and \( y_l \) contain the same \(\log Z(x)\) term; therefore, when computing the difference, this term cancels out:</p>\[
\begin{aligned}
r_\theta(x,y_w)-r_\theta(x,y_l)
&=\; \beta \left[\log \frac{\pi_\theta(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)} + \log Z(x)\right] - \beta \left[\log \frac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)} + \log Z(x)\right] \\
&=\; \beta \,\log \frac{\pi_\theta(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)} - \beta \,\log \frac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)}.
\end{aligned}
\tag{17}
\]<p>Substituting Equation (17) into (15) and combining with (11), we obtain the final DPO loss function:</p>\[
\mathcal{L}_{\mathrm{DPO}}(\pi_\theta; \pi_{\mathrm{ref}})
= - \mathbb{E}_{(x,y_w,y_l) \sim D} \left[ \log \sigma\Big(
\beta \,\log \frac{\pi_\theta(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)}
-\; \beta \,\log \frac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)}
\Big) \right].
\tag{18}
\]<p>This loss function is designed for the trainable Actor model \(\pi_\theta\). It distinguishes between good and bad responses by comparing the log-probability ratios (relative to the reference model \(\pi_{\mathrm{ref}}\)) for the high-quality response \(y_w\) and the low-quality response \(y_l\): when \(\pi_\theta\) assigns a much higher relative probability to \(y_w\) compared to \(y_l\), the sigmoid output approaches 1 and the loss decreases; conversely, if the relative probability for \(y_l\) is too high, the loss increases. The parameter \(\beta\) scales the difference between these log-ratios, thereby adjusting the model’s sensitivity in distinguishing good from bad responses.</p><h2 id=training-process>Training Process<a hidden class=anchor aria-hidden=true href=#training-process>#</a></h2><h3 id=data-preparation>Data Preparation<a hidden class=anchor aria-hidden=true href=#data-preparation>#</a></h3><ul><li><p><strong>Preference Data Collection</strong><br>Collect a large amount of preference data. Each sample includes an input \( x \) and a pair of responses \((y_w, y_l)\), where \( y_w \) is the chosen (preferred) response and \( y_l \) is the rejected (lower quality) response.</p><p><strong>Dataset Example</strong>: <a href=https://github.com/hiyouga/LLaMA-Factory/blob/main/data/dpo_en_demo.json>dpo_en_demo.json</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;conversations&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;from&#34;</span><span class=p>:</span> <span class=s2>&#34;human&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;what does abstraction mean on oop&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;chosen&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;from&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;Abstraction in Object-Oriented Programming (OOP) is the process of exposing only the necessary information to the outside world while hiding the implementation details. It&#39;s a way to simplify complex systems by focusing only on the essential aspects and encapsulating the implementation details.\n\nIn other words, abstraction is about hiding the internal workings of an object or system and only exposing the necessary interfaces or APIs to interact with it. This helps to reduce complexity, improve modularity, and make the system easier to understand and maintain.\n\nFor example, consider a car. A car has many components like engine, transmission, brakes, etc. However, when you drive a car, you don&#39;t need to know how the engine works or how the transmission changes gears. All you need to know is how to operate the car&#39;s controls like accelerator, brakes, and steering wheel. The car&#39;s controls are an abstraction of the underlying complex systems, allowing you to interact with the car without needing to know the details of how it works.\n\nIn programming, abstraction is achieved by defining interfaces or abstract classes that define the essential functions or behaviors of an object without specifying how they are implemented. This allows developers to create different implementations of the same interface or abstract class, which can be swapped out without affecting the rest of the system.\n\nAbstraction is a fundamental concept in OOP and is used to create modular, maintainable, and flexible software systems. It helps to reduce code duplication, improve code reuse, and make software systems more understandable and easier to evolve over time.&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;rejected&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;from&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;Abstraction is a principle in object-oriented programming that refers to the process of focusing on the essential features of an object or concept without emphasizing the details that are not important for its functionality. It enables programmers to create classes that represent the objects they want to work with, and specifies only the behavior of those objects, leaving out unnecessary details. Abstraction helps to make the code more maintainable, modular, and scalable. It also improves the productivity of developers by reducing the amount of code they need to write.&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></li><li><p><strong>Output Precomputation</strong><br>To reduce computational overhead, pre-compute and cache the outputs of the reference model \(\pi_{\mathrm{ref}}(y \mid x)\).</p></li></ul><h3 id=model-training>Model Training<a hidden class=anchor aria-hidden=true href=#model-training>#</a></h3><ul><li><p><strong>Training Objective</strong><br>Directly optimize the Actor model \(\pi_\theta\) by minimizing the DPO loss \(\mathcal{L}_{\mathrm{DPO}}(\pi_\theta; \pi_{\mathrm{ref}})\) so that its generated responses better align with human preferences.</p></li><li><p><strong>Training Steps</strong></p><ol><li><p>Sample a batch of data \((x, y_w, y_l)\) from the dataset.</p></li><li><p>Compute the output probabilities of the Actor model \(\pi_\theta(y \mid x)\).</p></li><li><p>Calculate the loss using:</p>\[
\mathcal{L}_{\mathrm{DPO}} = - \log \sigma\Big( \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)} \Big).
\]</li><li><p>Update the Actor model parameters \(\theta\) via backpropagation.</p></li></ol></li></ul><h3 id=model-inference>Model Inference<a hidden class=anchor aria-hidden=true href=#model-inference>#</a></h3><p>Once training is complete, the resulting Actor model \(\pi_\theta\) can be used directly for inference. Given an input \( x \), the model generates responses based on the learned probability distribution. Since human preferences have been incorporated during training and the model is constrained by the reference model \(\pi_{\mathrm{ref}}\), the generated responses are not only aligned with expectations but also maintain stability in the generated text.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>DPO simplifies the RLHF process into a direct supervised learning task, saving resources, enhancing training stability, and reducing implementation complexity. It serves as an efficient alternative for LLM alignment training. In practical applications, one can choose between RLHF and DPO methods based on the specific business scenario to achieve the best training results.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Christiano, Paul F., et al. <a href=https://arxiv.org/abs/1706.03741>&ldquo;Deep reinforcement learning from human preferences.&rdquo;</a> Advances in neural information processing systems 30 (2017).</p><p>[2] Ouyang, Long, et al. <a href=https://arxiv.org/abs/2203.02155>&ldquo;Training language models to follow instructions with human feedback.&rdquo;</a> Advances in neural information processing systems 35 (2022): 27730-27744.</p><p>[3] Rafailov, Rafael, et al. <a href=https://arxiv.org/abs/1706.03741>&ldquo;Direct preference optimization: Your language model is secretly a reward model.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2024).</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reprinting or citing the contents of this article, please indicate the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Feb 2025). LLMs Alignment: DPO.<br><a href=https://syhya.github.io/posts/2025-02-08-dpo>https://syhya.github.io/posts/2025-02-08-dpo</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025dpo</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;LLMs Alignment: DPO&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Feb&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-02-08-dpo&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/nlp/>NLP</a></li><li><a href=https://syhya.github.io/tags/llms/>LLMs</a></li><li><a href=https://syhya.github.io/tags/post-training/>Post-Training</a></li><li><a href=https://syhya.github.io/tags/dpo/>DPO</a></li><li><a href=https://syhya.github.io/tags/rlhf/>RLHF</a></li><li><a href=https://syhya.github.io/tags/alignment/>Alignment</a></li><li><a href=https://syhya.github.io/tags/bradleyterry-model/>Bradley–Terry Model</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-03-01-train-llm/><span class=title>« Prev</span><br><span>Parallelism and Memory Optimization Techniques for Training Large Models</span>
</a><a class=next href=https://syhya.github.io/posts/2025-02-01-normalization/><span class=title>Next »</span><br><span>Normalization in Deep Learning</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share LLMs Alignment: DPO on x" href="https://x.com/intent/tweet/?text=LLMs%20Alignment%3a%20DPO&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-08-dpo%2f&amp;hashtags=AI%2cNLP%2cLLMs%2cPost-training%2cDPO%2cRLHF%2cAlignment%2cBradley%e2%80%93TerryModel"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLMs Alignment: DPO on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-08-dpo%2f&amp;title=LLMs%20Alignment%3a%20DPO&amp;summary=LLMs%20Alignment%3a%20DPO&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-08-dpo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLMs Alignment: DPO on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-08-dpo%2f&title=LLMs%20Alignment%3a%20DPO"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLMs Alignment: DPO on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-08-dpo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLMs Alignment: DPO on whatsapp" href="https://api.whatsapp.com/send?text=LLMs%20Alignment%3a%20DPO%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-08-dpo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLMs Alignment: DPO on telegram" href="https://telegram.me/share/url?text=LLMs%20Alignment%3a%20DPO&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-08-dpo%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLMs Alignment: DPO on ycombinator" href="https://news.ycombinator.com/submitlink?t=LLMs%20Alignment%3a%20DPO&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-02-08-dpo%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>