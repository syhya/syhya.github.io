<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DeepSeek-V2 vs V3 | Yue Shui Blog</title><meta name=keywords content="Deep Learning,AI,LLM,DeepSeek-V2,DeepSeek-V3,MoE,Transformer,MLA,DeepSeekMoE,MTP,FP8 Training,GRPO,SFT,RL,KV Cache"><meta name=description content="DeepSeek AI successively released DeepSeek-V2 (DeepSeek-AI, 2024) and DeepSeek-V3 (DeepSeek-AI, 2024), two powerful Mixture-of-Experts (MoE) language models that significantly optimize training costs and inference efficiency while maintaining state-of-the-art performance. DeepSeek-V2 has a total of 236B parameters, activating 21B per token, while DeepSeek-V3 further expands to 671B total parameters, activating 37B per token. Both support a 128K context length.
The core innovations of these two models lie in the adoption of Multi-head Latent Attention (MLA) and the DeepSeekMoE architecture (Dai et al., 2024). MLA drastically reduces GPU memory usage during inference by compressing the Key-Value (KV) cache into low-dimensional latent vectors, improving efficiency. DeepSeekMoE achieves stronger expert specialization capabilities and more economical training costs through fine-grained expert segmentation and shared expert isolation. Building upon V2, DeepSeek-V3 further introduces an Auxiliary-Loss-Free Load Balancing strategy (Wang et al., 2024) and the Multi-Token Prediction (MTP) (Gloeckle et al., 2024) training objective, further enhancing model performance and training efficiency."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="DeepSeek-V2 vs V3"><meta property="og:description" content="DeepSeek AI successively released DeepSeek-V2 (DeepSeek-AI, 2024) and DeepSeek-V3 (DeepSeek-AI, 2024), two powerful Mixture-of-Experts (MoE) language models that significantly optimize training costs and inference efficiency while maintaining state-of-the-art performance. DeepSeek-V2 has a total of 236B parameters, activating 21B per token, while DeepSeek-V3 further expands to 671B total parameters, activating 37B per token. Both support a 128K context length.
The core innovations of these two models lie in the adoption of Multi-head Latent Attention (MLA) and the DeepSeekMoE architecture (Dai et al., 2024). MLA drastically reduces GPU memory usage during inference by compressing the Key-Value (KV) cache into low-dimensional latent vectors, improving efficiency. DeepSeekMoE achieves stronger expert specialization capabilities and more economical training costs through fine-grained expert segmentation and shared expert isolation. Building upon V2, DeepSeek-V3 further introduces an Auxiliary-Loss-Free Load Balancing strategy (Wang et al., 2024) and the Multi-Token Prediction (MTP) (Gloeckle et al., 2024) training objective, further enhancing model performance and training efficiency."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-18T12:00:00+08:00"><meta property="article:modified_time" content="2025-09-02T22:00:47+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="article:tag" content="DeepSeek-V2"><meta property="article:tag" content="DeepSeek-V3"><meta property="article:tag" content="MoE"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="DeepSeek-V2 vs V3"><meta name=twitter:description content="DeepSeek AI successively released DeepSeek-V2 (DeepSeek-AI, 2024) and DeepSeek-V3 (DeepSeek-AI, 2024), two powerful Mixture-of-Experts (MoE) language models that significantly optimize training costs and inference efficiency while maintaining state-of-the-art performance. DeepSeek-V2 has a total of 236B parameters, activating 21B per token, while DeepSeek-V3 further expands to 671B total parameters, activating 37B per token. Both support a 128K context length.
The core innovations of these two models lie in the adoption of Multi-head Latent Attention (MLA) and the DeepSeekMoE architecture (Dai et al., 2024). MLA drastically reduces GPU memory usage during inference by compressing the Key-Value (KV) cache into low-dimensional latent vectors, improving efficiency. DeepSeekMoE achieves stronger expert specialization capabilities and more economical training costs through fine-grained expert segmentation and shared expert isolation. Building upon V2, DeepSeek-V3 further introduces an Auxiliary-Loss-Free Load Balancing strategy (Wang et al., 2024) and the Multi-Token Prediction (MTP) (Gloeckle et al., 2024) training objective, further enhancing model performance and training efficiency."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"DeepSeek-V2 vs V3","item":"https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DeepSeek-V2 vs V3","name":"DeepSeek-V2 vs V3","description":"DeepSeek AI successively released DeepSeek-V2 (DeepSeek-AI, 2024) and DeepSeek-V3 (DeepSeek-AI, 2024), two powerful Mixture-of-Experts (MoE) language models that significantly optimize training costs and inference efficiency while maintaining state-of-the-art performance. DeepSeek-V2 has a total of 236B parameters, activating 21B per token, while DeepSeek-V3 further expands to 671B total parameters, activating 37B per token. Both support a 128K context length.\nThe core innovations of these two models lie in the adoption of Multi-head Latent Attention (MLA) and the DeepSeekMoE architecture (Dai et al., 2024). MLA drastically reduces GPU memory usage during inference by compressing the Key-Value (KV) cache into low-dimensional latent vectors, improving efficiency. DeepSeekMoE achieves stronger expert specialization capabilities and more economical training costs through fine-grained expert segmentation and shared expert isolation. Building upon V2, DeepSeek-V3 further introduces an Auxiliary-Loss-Free Load Balancing strategy (Wang et al., 2024) and the Multi-Token Prediction (MTP) (Gloeckle et al., 2024) training objective, further enhancing model performance and training efficiency.\n","keywords":["Deep Learning","AI","LLM","DeepSeek-V2","DeepSeek-V3","MoE","Transformer","MLA","DeepSeekMoE","MTP","FP8 Training","GRPO","SFT","RL","KV Cache"],"articleBody":"DeepSeek AI successively released DeepSeek-V2 (DeepSeek-AI, 2024) and DeepSeek-V3 (DeepSeek-AI, 2024), two powerful Mixture-of-Experts (MoE) language models that significantly optimize training costs and inference efficiency while maintaining state-of-the-art performance. DeepSeek-V2 has a total of 236B parameters, activating 21B per token, while DeepSeek-V3 further expands to 671B total parameters, activating 37B per token. Both support a 128K context length.\nThe core innovations of these two models lie in the adoption of Multi-head Latent Attention (MLA) and the DeepSeekMoE architecture (Dai et al., 2024). MLA drastically reduces GPU memory usage during inference by compressing the Key-Value (KV) cache into low-dimensional latent vectors, improving efficiency. DeepSeekMoE achieves stronger expert specialization capabilities and more economical training costs through fine-grained expert segmentation and shared expert isolation. Building upon V2, DeepSeek-V3 further introduces an Auxiliary-Loss-Free Load Balancing strategy (Wang et al., 2024) and the Multi-Token Prediction (MTP) (Gloeckle et al., 2024) training objective, further enhancing model performance and training efficiency.\nDeepSeek-V2 was pre-trained on 8.1T tokens, while DeepSeek-V3 was trained on a larger scale of 14.8T tokens. Both underwent Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages to fully unlock their potential. Evaluation results show that both DeepSeek-V2 and V3 achieved top-tier performance among open-source models across numerous benchmarks. DeepSeek-V3, in particular, has become one of the strongest open-source base models currently available, with performance comparable to top closed-source models.\nFig. 1. (a) MMLU accuracy vs. activated parameters, among different open-source models. (b) Training costs and inference efficiency of DeepSeek 67B (Dense) and DeepSeek-V2. (Image source: DeepSeek-AI, 2024)\nFig. 2. Benchmark performance of DeepSeek-V3 and its counterparts. (Image source: DeepSeek-AI, 2024)\nThis article will delve into the key technologies of DeepSeek-V2 and DeepSeek-V3, including their innovative model architectures, efficient training infrastructure, pre-training, and alignment processes.\nNotations The following table lists the mathematical notations used in this article to help you read more easily.\nSymbol Meaning \\( d \\) Embedding dimension \\( n_h \\) Number of attention heads \\( d_h \\) Dimension per attention head \\( \\mathbf{h}_t \\in \\mathbb{R}^d \\) Input to the attention layer for the \\( t \\)-th token \\( \\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t \\) Query, Key, Value vectors \\( W^Q, W^K, W^V, W^O \\) Projection matrices for Query, Key, Value, Output \\( \\mathbf{q}_{t,i}, \\mathbf{k}_{t,i}, \\mathbf{v}_{t,i} \\) Query, Key, Value vectors for the \\( i \\)-th attention head \\( \\mathbf{o}_{t,i} \\) Output of the \\( i \\)-th attention head \\( \\mathbf{u}_t \\) Final output of the attention layer \\( l \\) Number of model layers \\( \\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c} \\) Compressed latent vector for key-value \\( d_c \\) KV compression dimension \\( W^{DKV}, W^{UK}, W^{UV} \\) Down-projection matrix for KV, Up-projection matrix for Key, Up-projection matrix for Value \\( \\mathbf{k}_t^C, \\mathbf{v}_t^C \\) Key and Value recovered from the latent vector via up-projection \\( \\mathbf{c}_t^Q \\in \\mathbb{R}^{d_c'} \\) Compressed latent vector for query \\( d_c' \\) Query compression dimension \\( W^{DQ}, W^{UQ} \\) Down-projection matrix for Query, Up-projection matrix for Query \\( \\mathbf{q}_t^C \\) Query recovered from the latent vector via up-projection \\( \\mathbf{q}_{t,i}^R, \\mathbf{k}_t^R \\) Decoupled RoPE query and key \\( d_h^R \\) Head dimension for decoupled RoPE query/key \\( W^{QR}, W^{KR} \\) Generation matrices for decoupled RoPE query/key \\( \\operatorname{RoPE}(\\cdot) \\) Operation applying Rotary Position Embedding \\( [\\cdot ; \\cdot] \\) Concatenation operation \\( n_g \\) Number of groups in GQA \\( n \\) Total number of experts in MoE \\( E_i \\) The \\( i \\)-th expert network \\( G(\\cdot) \\) Gating network function \\( p_i \\) The \\( i \\)-th probability output by the gating network \\( H^{(i)}(x) \\) Gating score for expert \\( i \\) in Noisy Top-k Gating \\( W_g, W_{\\text{noise}} \\) Weight matrices for MoE gating network and noise network \\( \\epsilon \\) Standard Gaussian noise \\( \\text{softplus}(\\cdot) \\) Softplus activation function \\( k \\) Number of experts selected per token in MoE \\( \\text{topk}(\\cdot, k) \\) Function selecting the top k largest values \\( \\mathcal{L}_{\\text{aux}} \\) MoE auxiliary loss \\( w_{\\text{aux}} \\) Auxiliary loss weight \\( \\text{CV}(\\cdot) \\) Coefficient of Variation \\( N_s, N_r \\) Number of shared and routing experts in DeepSeekMoE \\( \\operatorname{FFN}_i^{(s)}(\\cdot), \\operatorname{FFN}_i^{(r)}(\\cdot) \\) The \\( i \\)-th shared expert and routing expert function \\( K_r \\) Number of activated routing experts in DeepSeekMoE \\( g_{i,t} \\) Gating value of the \\( i \\)-th expert for the \\( t \\)-th token \\( g_{i,t}' \\) Raw gating value after TopK selection (V3) \\( s_{i,t} \\) Affinity score of the \\( t \\)-th token for the \\( i \\)-th expert \\( \\mathbf{e}_i \\) Center vector for the \\( i \\)-th routing expert \\( M \\) Device/Node limit for routing \\( \\mathcal{L}_{\\text{ExpBal}}, \\mathcal{L}_{\\text{DevBal}}, \\mathcal{L}_{\\text{CommBal}} \\) Expert-level, Device-level, Communication-level load balancing losses \\( f_i, P_i \\) Load score and average affinity for expert \\( i \\) \\( \\alpha_1, \\alpha_2, \\alpha_3 \\) Hyperparameters for load balancing losses \\( T \\) Number of tokens in the sequence \\( D \\) Number of device/node groups \\( \\mathcal{E}_i \\) Set of experts on the \\( i \\)-th device/node \\( f_i', P_i' \\) Average load score and total affinity for device group \\( i \\) \\( f_i'', P_i'' \\) Proportion of tokens sent to device \\( i \\) and total affinity for device group \\( i \\) \\( b_i \\) Bias term for the \\( i \\)-th expert (aux-loss-free balancing) \\( \\gamma \\) Bias term update rate \\( \\mathcal{L}_{\\text{Bal}} \\) Sequence-level load balancing loss \\( \\alpha \\) Hyperparameter for sequence-level load balancing loss \\( D_{MTP} \\) MTP prediction depth \\( \\operatorname{Emb}(\\cdot), \\operatorname{OutHead}(\\cdot) \\) Shared embedding layer and output head (MTP) \\( \\operatorname{TRM}_k(\\cdot) \\) Transformer block for the \\( k \\)-th MTP module \\( M_k \\) Projection matrix for the \\( k \\)-th MTP module \\( \\mathbf{h}_i^k \\) Representation of the \\( i \\)-th token at the \\( k \\)-th MTP depth \\( \\mathbf{h}_i^{\\prime k} \\) Input to the Transformer block of the \\( k \\)-th MTP module \\( P_{i+k+1}^k \\) Predicted probability distribution for the \\( i+k+1 \\)-th token by the \\( k \\)-th MTP module \\( V \\) Vocabulary size \\( \\mathcal{L}_{\\text{MTP}}^k \\) Cross-entropy loss for the \\( k \\)-th MTP depth \\( \\mathcal{L}_{\\text{MTP}} \\) Total MTP loss \\( \\lambda \\) Weight factor for MTP loss \\( \\mathcal{J}_{GRPO}(\\theta) \\) GRPO objective function \\( A_i \\) Relative advantage value (GRPO) \\( \\varepsilon \\) Clipping hyperparameter in PPO/GRPO \\( \\beta \\) Coefficient for KL divergence penalty term \\( \\mathbb{D}_{KL}(\\pi_\\theta \\| \\pi_{ref}) \\) KL divergence \\( \\pi_\\theta, \\pi_{\\theta_{old}}, \\pi_{ref} \\) Current policy, old policy, reference policy models \\( r_i \\) Reward value for the \\( i \\)-th output \\( \\mathbb{1}(\\cdot) \\) Indicator function Core Architecture Both DeepSeek-V2 and V3 are based on the Transformer architecture, but employ innovative designs in the attention and feed-forward network (FFN) parts, such as MLA and DeepSeekMoE, to balance performance, training cost, and inference efficiency. The figure below illustrates the architecture of DeepSeek-V2 and V3.\nFig. 3. Illustration of the architecture of DeepSeek-V2 and DeepSeek-V3. MLA ensures efficient inference by significantly reducing the KV cache for generation, and DeepSeekMoE enables training strong models at an economical cost through the sparse architecture. (Image source: DeepSeek-AI, 2024)\nMulti-head Latent Attention (MLA) Traditional Transformer models typically use Multi-Head Attention (MHA) (Vaswani et al., 2017), but during generation, their large KV cache becomes a bottleneck limiting inference efficiency. To address this, researchers proposed Multi-Query Attention (MQA) (Shazeer, 2019) and Grouped-Query Attention (GQA) (Ainslie et al., 2023). While these methods reduce the KV cache, they often come at the cost of model performance.\nDeepSeek-V2 and V3 adopt the innovative Multi-head Latent Attention (MLA) mechanism. The core idea of MLA is Low-Rank Key-Value Joint Compression.\nFig. 4. Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. (Image source: DeepSeek-AI, 2024)\nMHA Recap Standard MHA first transforms the input \\(\\mathbf{h}_t \\in \\mathbb{R}^d\\) into query \\(\\mathbf{q}_t\\), key \\(\\mathbf{k}_t\\), and value \\(\\mathbf{v}_t \\in \\mathbb{R}^{d_h n_h}\\) using three projection matrices \\(W^Q, W^K, W^V \\in \\mathbb{R}^{d_h n_h \\times d}\\): \\[ \\begin{aligned} \\mathbf{q}_{t} \u0026= W^{Q} \\mathbf{h}_{t}, \\\\ \\mathbf{k}_{t} \u0026= W^{K} \\mathbf{h}_{t}, \\\\ \\mathbf{v}_{t} \u0026= W^{V} \\mathbf{h}_{t}. \\end{aligned} \\] Then, \\(\\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t\\) are split into \\(n_h\\) heads for multi-head attention computation: \\[ \\begin{aligned} \u0026 [\\mathbf{q}_{t, 1} ; \\mathbf{q}_{t, 2} ; \\ldots ; \\mathbf{q}_{t, n_{h}}] = \\mathbf{q}_{t}, \\\\ \u0026 [\\mathbf{k}_{t, 1} ; \\mathbf{k}_{t, 2} ; \\ldots ; \\mathbf{k}_{t, n_{h}}] = \\mathbf{k}_{t}, \\\\ \u0026 [\\mathbf{v}_{t, 1} ; \\mathbf{v}_{t, 2} ; \\ldots ; \\mathbf{v}_{t, n_{h}}] = \\mathbf{v}_{t}, \\\\ \u0026 \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} \\operatorname{Softmax}_{j}\\left(\\frac{\\mathbf{q}_{t, i}^{T} \\mathbf{k}_{j, i}}{\\sqrt{d_{h}}}\\right) \\mathbf{v}_{j, i}, \\\\ \u0026 \\mathbf{u}_{t} = W^{O}\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right], \\end{aligned} \\] where \\(\\mathbf{q}_{t, i}, \\mathbf{k}_{t, i}, \\mathbf{v}_{t, i} \\in \\mathbb{R}^{d_h}\\) are the query, key, and value for the \\(i\\)-th head, respectively, and \\(W^O \\in \\mathbb{R}^{d \\times d_h n_h}\\) is the output projection matrix. During inference, keys and values for all \\(t\\) steps need to be cached, requiring \\(2 n_h d_h l\\) elements per token (\\(l\\) being the number of layers), which constitutes a huge KV cache overhead.\nLow-Rank Key-Value Joint Compression MLA introduces a low-dimensional latent vector \\(\\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c}\\) to jointly compress keys and values, where \\(d_c \\ll d_h n_h\\): \\[ \\begin{aligned} \\boxed{\\mathbf{c}_{t}^{K V}} \u0026= W^{D K V} \\mathbf{h}_{t}, \\\\ \\mathbf{k}_{t}^{C} \u0026= W^{U K} \\mathbf{c}_{t}^{K V}, \\\\ \\mathbf{v}_{t}^{C} \u0026= W^{U V} \\mathbf{c}_{t}^{K V}. \\end{aligned} \\] Here, \\(W^{DKV} \\in \\mathbb{R}^{d_c \\times d}\\) is the down-projection matrix, and \\(W^{UK}, W^{UV} \\in \\mathbb{R}^{d_h n_h \\times d_c}\\) are the up-projection matrices for keys and values, respectively. During inference, MLA only needs to cache the compressed latent vector \\(\\mathbf{c}_t^{KV}\\) (and the decoupled RoPE key \\(\\mathbf{k}_t^R\\) mentioned later), greatly reducing the KV cache size.\nTo reduce activation memory during training, MLA also applies similar low-rank compression to the query: \\[ \\begin{aligned} \\mathbf{c}_{t}^{Q} \u0026= W^{D Q} \\mathbf{h}_{t}, \\\\ \\mathbf{q}_{t}^{C} \u0026= W^{U Q} \\mathbf{c}_{t}^{Q}, \\end{aligned} \\] where \\(\\mathbf{c}_t^Q \\in \\mathbb{R}^{d_c'}\\) is the compressed latent vector for the query, \\(d_c' \\ll d_h n_h\\), and \\(W^{DQ} \\in \\mathbb{R}^{d_c' \\times d}\\) and \\(W^{UQ} \\in \\mathbb{R}^{d_h n_h \\times d_c'}\\) are the down-projection and up-projection matrices for the query, respectively.\nDecoupled Rotary Position Embedding Standard Rotary Position Embedding (RoPE) (Su et al., 2024) is applied directly to keys and queries, but this is incompatible with MLA’s low-rank KV compression. If RoPE were applied to the compressed key \\(\\mathbf{k}_t^C\\), the up-projection matrix \\(W^{UK}\\) would couple with the position-dependent RoPE matrix. This would prevent absorbing \\(W^{UK}\\) into \\(W^Q\\) during inference, requiring recomputation of keys for all prefix tokens, severely impacting efficiency.\nTo solve this, MLA proposes the Decoupled RoPE strategy. It introduces an additional multi-head query \\(\\mathbf{q}_{t, i}^R \\in \\mathbb{R}^{d_h^R}\\) and a shared key \\(\\mathbf{k}_t^R \\in \\mathbb{R}^{d_h^R}\\) specifically to carry the RoPE information: \\[ \\begin{aligned} \\left[\\mathbf{q}_{t,1}^R;\\,\\mathbf{q}_{t,2}^R;\\,\\dots;\\,\\mathbf{q}_{t,n_h}^R\\right] = \\mathbf{q}_t^R \u0026= \\operatorname{RoPE}\\bigl(W^{Q R}\\,\\mathbf{c}_t^Q\\bigr),\\\\ \\boxed{\\mathbf{k}_t^R} \u0026= \\operatorname{RoPE}\\bigl(W^{K R}\\,\\mathbf{h}_t\\bigr). \\end{aligned} \\] Here, \\(W^{QR} \\in \\mathbb{R}^{d_h^R n_h \\times d_c'}\\) and \\(W^{KR} \\in \\mathbb{R}^{d_h^R \\times d}\\) are matrices generating the decoupled query and key. The compressed key/query parts (\\(C\\)) are then concatenated with the decoupled RoPE parts (\\(R\\)) to form the final keys and queries: \\[ \\begin{aligned} \\mathbf{q}_{t, i} \u0026= [\\mathbf{q}_{t, i}^{C} ; \\mathbf{q}_{t, i}^{R}], \\\\ \\mathbf{k}_{t, i} \u0026= [\\mathbf{k}_{t, i}^{C} ; \\mathbf{k}_{t}^{R}]. \\end{aligned} \\] The final attention computation becomes: \\[ \\begin{aligned} \\mathbf{o}_{t, i} \u0026= \\sum_{j=1}^{t} \\operatorname{Softmax}_{j}\\left(\\frac{\\mathbf{q}_{t, i}^{T} \\mathbf{k}_{j, i}}{\\sqrt{d_{h}+d_{h}^{R}}}\\right) \\mathbf{v}_{j, i}^{C}, \\\\ \\mathbf{u}_{t} \u0026= W^{O}\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right]. \\end{aligned} \\] During inference, besides caching \\(\\mathbf{c}_t^{KV}\\), the decoupled RoPE key \\(\\mathbf{k}_t^R\\) also needs to be cached. Therefore, DeepSeek-V2/V3 require caching a total of \\((d_c + d_h^R)l\\) elements per token.\nMatrix Absorption in MLA Inference A key advantage of MLA is the improvement in inference efficiency, partly due to the associative property of matrix multiplication allowing the up-projection matrices \\(W^{UK}\\) and \\(W^{UV}\\) to be “absorbed,” avoiding the explicit computation of the full keys \\(\\mathbf{k}_t^C\\) and values \\(\\mathbf{v}_t^C\\).\n1. Absorbing \\(W^{UK}\\) (Optimizing Attention Score Calculation):\nThe core of attention score calculation is the dot product of query and key \\(\\mathbf{q}_{t,i}^T \\mathbf{k}_{j,i}\\). Focusing on the \\(C\\) part generated from the compressed vector: \\[ (\\mathbf{q}_{t,i}^C)^T \\mathbf{k}_{j,i}^C \\] Substitute \\(\\mathbf{k}_{j,i}^C = W^{UK} \\mathbf{c}_j^{KV}\\): \\[ (\\mathbf{q}_{t,i}^C)^T (W^{UK} \\mathbf{c}_j^{KV}) \\] Using matrix multiplication associativity \\((AB)C = A(BC)\\) and transpose property \\((AB)^T = B^T A^T\\), the expression can be rewritten as: \\[ (\\mathbf{q}_{t,i}^C)^T (W^{UK} \\mathbf{c}_j^{KV}) = ((W^{UK})^T \\mathbf{q}_{t,i}^C)^T \\mathbf{c}_j^{KV} \\] The significance of this transformation is: we no longer need to apply \\(W^{UK}\\) to the cached \\(\\mathbf{c}_j^{KV}\\) to get \\(\\mathbf{k}_{j,i}^C\\). Instead, we can first compute an “effective query” \\(\\tilde{\\mathbf{q}}_{t,i}^C = (W^{UK})^T \\mathbf{q}_{t,i}^C\\), and then directly compute the dot product of this effective query with the cached latent vector \\(\\mathbf{c}_j^{KV}\\).\nThe original query \\(\\mathbf{q}_{t,i}^C\\) is computed from \\(\\mathbf{h}_t\\) via \\(W^{UQ}\\) and \\(W^{DQ}\\) (\\(\\mathbf{q}_{t,i}^C = (W^{UQ} W^{DQ} \\mathbf{h}_t)_i\\)). Thus, the entire computation from \\(\\mathbf{h}_t\\) to the effective query \\(\\tilde{\\mathbf{q}}_{t,i}^C\\) can be viewed as a new, effective query projection operation that incorporates \\(W^{UK}\\). In practice, this means after computing \\(\\mathbf{q}_{t,i}^C\\), one can left-multiply by \\((W^{UK})^T\\), or more efficiently, merge \\((W^{UK})^T\\) into the original query generation matrix \\(W^Q\\) (or \\(W^{UQ}W^{DQ}\\)) to form a new query projection matrix \\(\\tilde{W}^Q = (W^{UK})^T W^{UQ} W^{DQ}\\).\nCrucially, the computation involving \\(W^{UK}\\) is moved to the query side and performed once before calculating attention scores, eliminating the need to recover \\(\\mathbf{k}_{j,i}^C\\) from the cached \\(\\mathbf{c}_j^{KV}\\) using \\(W^{UK}\\) for every query.\n2. Absorbing \\(W^{UV}\\) (Optimizing Weighted Sum):\nThe output of an attention head \\(\\mathbf{o}_{t,i}\\) is the weighted sum of attention weights (denoted \\(w_{ij}\\)) and values \\(\\mathbf{v}_{j,i}^C\\): \\[ \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} w_{ij} \\cdot \\mathbf{v}_{j, i}^{C} \\] Substitute \\(\\mathbf{v}_{j,i}^C = (W^{UV} \\mathbf{c}_j^{KV})_i\\) (where \\((\\cdot)_i\\) denotes the part belonging to the \\(i\\)-th head): \\[ \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} w_{ij} \\cdot (W^{UV} \\mathbf{c}_j^{KV})_i \\] The final attention layer output \\(\\mathbf{u}_t\\) is obtained by concatenating the outputs of all heads \\(\\mathbf{o}_{t,i}\\) and projecting through the output matrix \\(W^O\\): \\[ \\mathbf{u}_{t} = W^{O}\\left[\\mathbf{o}_{t, 1} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right] = W^{O} \\begin{bmatrix} \\sum_{j} w_{1j} (W^{UV} \\mathbf{c}_j^{KV})_1 \\\\ \\vdots \\\\ \\sum_{j} w_{n_h j} (W^{UV} \\mathbf{c}_j^{KV})_{n_h} \\end{bmatrix} \\] Due to the linearity of matrix multiplication (\\(A(B+C) = AB + AC\\) and \\(A(cB) = c(AB)\\)), \\(W^{UV}\\) can be “factored out” of the summation (this is for intuitive understanding; the actual operation is at the matrix level): \\[ \\mathbf{u}_{t} \\approx W^{O} W^{UV} \\left( \\sum_{j=1}^{t} \\begin{bmatrix} w_{1j} (\\mathbf{c}_j^{KV})_1 \\\\ \\vdots \\\\ w_{n_h j} (\\mathbf{c}_j^{KV})_{n_h} \\end{bmatrix} \\right) \\] (Note: \\((\\mathbf{c}_j^{KV})_i\\) here is illustrative; in practice, operations are performed directly on the complete \\(\\mathbf{c}_j^{KV}\\), but the principle is the same: first perform the weighted sum on \\(\\mathbf{c}_j^{KV}\\), then apply \\(W^{UV}\\) and \\(W^O\\)).\nLet the effective output matrix be \\(\\tilde{W}^O = W^O W^{UV}\\). This means we can first compute the weighted sum of attention weights and the latent vectors \\(\\mathbf{c}_j^{KV}\\) (yielding an intermediate result \\(\\tilde{\\mathbf{o}}_t = \\sum_j w_{ij} \\mathbf{c}_j^{KV}\\) of dimension \\(d_c\\)), and then directly use this merged effective output matrix \\(\\tilde{W}^O\\) for the final projection to get \\(\\mathbf{u}_t\\). Similarly, the computation involving \\(W^{UV}\\) is merged into the final output projection step, eliminating the need to recover \\(\\mathbf{v}_{j,i}^C\\) from \\(\\mathbf{c}_j^{KV}\\) during the weighted sum calculation.\nSummary: Through matrix absorption, MLA avoids repeatedly computing the high-dimensional keys \\(\\mathbf{k}_{j,i}^C\\) and values \\(\\mathbf{v}_{j,i}^C\\) from the cached low-dimensional latent vectors \\(\\mathbf{c}_j^{KV}\\) during inference, significantly improving computational efficiency. Only \\(\\mathbf{c}_t^{KV}\\) and \\(\\mathbf{k}_t^R\\) are actually cached.\nKV Cache Comparison The table below compares the per-token KV cache size for different attention mechanisms. \\(n_h\\) is the number of attention heads, \\(d_h\\) is the dimension per head, \\(l\\) is the number of layers, \\(n_g\\) is the number of GQA groups, and \\(d_c\\) and \\(d_h^R\\) are MLA’s KV compression dimension and decoupled RoPE dimension. For DeepSeek-V2, \\(d_c = 4d_h\\), \\(d_h^R = d_h/2\\), making its KV cache equivalent to GQA with \\(n_g=2.25\\), but with performance superior to MHA. DeepSeek-V3 uses a similar configuration.\nAttention Mechanism Per-Token KV Cache Size (# elements) Capability Multi-Head Attention (MHA) \\(2 n_{h} d_{h} l\\) Strong Grouped-Query Attention (GQA) \\(2 n_{g} d_{h} l\\) Medium Multi-Query Attention (MQA) \\(2 d_{h} l\\) Weak Multi-head Latent Attention (MLA) \\(\\bigl(d_{c} + d_{h}^{R}\\bigr) l \\approx \\tfrac{9}{2} \\, d_{h} \\, l\\) Stronger The figure below shows that MLA not only significantly reduces the KV cache but also achieves performance superior to standard MHA.\nFig. 5. Comparison between MLA and MHA on hard benchmarks. DeepSeek-V2 shows better performance than MHA, but requires a significantly smaller amount of KV cache. (Image source: DeepSeek-AI, 2024)\nMixture-of-Experts Models Before diving into DeepSeekMoE, let’s review the basics of Mixture-of-Experts (MoE) models.\nMixture-of-Experts (MoE) (Shazeer et al. 2017) is a sparsely activated model that significantly increases model parameter count and performance without substantially increasing computational cost by combining multiple independent “expert” networks and a gating network. The core idea of MoE is Sparse Activation, meaning that for each input sample, only a subset of expert networks is activated, rather than the entire model. This approach enhances both computational efficiency and the model’s expressive power, leading to excellent performance in LLMs.\nMoE design is inspired by Ensemble learning, a technique that decomposes complex tasks into multiple subtasks handled collaboratively by different models. In MoE, these “subtasks” are processed by multiple independent expert networks, while a gating network dynamically selects the most suitable experts based on the input sample’s features. This division of labor resembles expert teams in human society: specialists from different fields provide expertise on specific problems, and their insights are combined to reach a final result.\nFig. 6. Illustration of a mixture-of-experts (MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)\nCore MoE Components A typical MoE layer includes the following components:\nExperts: A set of independent neural networks ${E_1, E_2, …, E_n}$. Each expert network $E_i$ can be any type of neural network, such as an FFN, CNN, RNN, etc. The number of experts $n$ can be large, e.g., tens, hundreds, or even thousands. Gating Network: A trainable neural network $G$ that learns a probability distribution based on the input sample $x$ to decide which experts to activate. The gating network takes the input sample $x$ and outputs an $n$-dimensional probability vector $p = G(x) = [p_1, p_2, …, p_n]$, where $p_i$ represents the probability of activating expert $E_i$. Expert Output Aggregation: Based on the probability distribution from the gating network, the outputs of the activated expert networks are weighted and summed to produce the final output $y$ of the MoE layer. Noisy Top-k Gating To achieve sparse activation and ensure balanced expert utilization, MoE typically employs Noisy Top-k Gating as the gating mechanism. This method introduces noise and top-k selection to ensure computational efficiency while preventing uneven expert load. Here’s the detailed workflow:\nGating Score Calculation:\nFor an input sample $x$, the gating network first computes a gating score $H^{(i)}(x)$ for each expert. This score consists of a linear transformation and a noise term, formulated as:\n$$ H^{(i)}(x) =(x W_g)^{(i)} + \\epsilon \\cdot \\text{softplus}\\left((x W_{\\text{noise}})^{(i)} \\right), \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$ Parameters: $W_g \\in \\mathbb{R}^{d \\times n}$: Trainable weight matrix of the gating network, where $d$ is the input feature dimension and $n$ is the number of experts. $W_{\\text{noise}} \\in \\mathbb{R}^{d \\times n}$: Weight matrix used to generate noise. $\\epsilon \\sim \\mathcal{N}(0, 1)$: Standard Gaussian noise, adding randomness to the gating. $\\text{softplus}(x) = \\log(1 + e^x)$: Smooth activation function ensuring non-negative noise. The introduction of noise prevents the gating network from always selecting the same experts, enhancing the model’s robustness and diversity.\nTop-k Selection:\nAfter computing the gating score vector $H(x) = [H^{(1)}(x), H^{(2)}(x), \\dots, H^{(n)}(x)]$, the gating network selects the top $k$ experts with the highest scores (usually $k \\ll n$). This step is implemented using the $\\text{topk}(v, k)$ function:\n$$ \\text{topk}^{(i)}(v, k) = \\begin{cases} v^{(i)} \u0026 \\text{if } v^{(i)} \\text{ is in the top } k \\text{ elements of } v \\\\ -\\infty \u0026 \\text{otherwise} \\end{cases} $$Setting the scores of non-top-k experts to $-\\infty$ ensures their probabilities become 0 after the subsequent softmax operation, achieving sparsity.\nSoftmax Normalization:\nThe gating scores of the top-k experts are normalized using softmax to obtain a sparse probability distribution $G(x)$:\n$$ G(x) = \\text{softmax}\\left( \\text{topk}(H(x), k) \\right) $$Only the top-k experts have non-zero probabilities; the rest are 0. For example, if $n=100, k=2$, then 98 experts will have a probability of 0.\nWeighted Sum:\nThe outputs of the top-k experts are weighted by their probabilities and summed to get the MoE layer’s output:\n$$ y = \\sum_{i=1}^{n} G^{(i)}(x) E_i(x) $$Since only $k$ experts are activated, the computational load is much lower than activating all $n$ experts.\nAuxiliary Loss To prevent the gating network from overly favoring a few experts, MoE introduces an Auxiliary Loss (Shazeer et al. 2017) to encourage uniform usage of all experts. A common method is based on the square of the Coefficient of Variation (CV) of expert usage:\n$$ \\mathcal{L}_{\\text{aux}} = w_{\\text{aux}} \\cdot \\text{CV}\\left( \\sum_{x \\in X} G(x) \\right)^2 $$ Parameters:\n$X$: A mini-batch of input samples. $\\sum_{x \\in X} G(x)$: Counts the number of times each expert is activated within the mini-batch. $\\text{CV}$: The ratio of the standard deviation to the mean, measuring the uniformity of expert usage distribution. $w_{\\text{aux}}$: Weight of the auxiliary loss, needs manual tuning. Purpose: By minimizing $\\mathcal{L}_{\\text{aux}}$, the model optimizes the balance of expert selection, preventing some experts from being overused while others remain idle.\nGShard GShard (Lepikhin et al. 2020) primarily focuses on sharding the MoE layer, distributing the expert networks ${E_1, E_2, …, E_n}$ across multiple TPU devices. For instance, with $P$ TPU devices, the experts can be divided into $P$ groups, each assigned to one TPU device. Other layers of the Transformer model (e.g., self-attention, LayerNorm) are replicated across all TPU devices.\nGShard’s Improved Gating Mechanism:\nGShard builds upon Noisy Top-k Gating with several improvements to enhance performance and stability:\nExpert Capacity: To prevent expert overload, GShard introduces expert capacity limits. Each expert network has a maximum capacity, indicating the maximum number of tokens it can process. If a token is routed to an expert that has reached its capacity limit, the token is marked as “overflowed,” and its gating output is set to a zero vector, meaning it won’t be routed to any expert.\nLocal Group Dispatching: To improve gating efficiency, GShard groups tokens and enforces expert capacity limits at the group level. For example, tokens in a mini-batch are divided into multiple local groups, each containing a certain number of tokens. The gating network selects top-k experts for each local group, ensuring that the number of tokens processed by each expert within a group does not exceed its capacity limit.\nAuxiliary Loss: GShard also uses an auxiliary loss function to balance expert load. Unlike the original MoE model’s auxiliary loss, GShard’s loss aims to minimize the mean squared error of the proportion of data routed to each expert, more directly measuring expert load balance.\nRandom Routing: To increase routing randomness, GShard introduces a random routing mechanism when selecting the top-k experts. Besides selecting the best top-k experts, GShard also randomly selects sub-optimal experts with a certain probability, increasing expert diversity and improving the model’s generalization ability.\nBelow is the core algorithm flow of GShard:\nFig. 7. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)\nSwitch Transformer Switch Transformer (Fedus et al. 2021) is a trillion-parameter MoE model proposed by Google. Its core innovation is replacing the dense feed-forward network (FFN) layers in the Transformer model with sparse Switch FFN layers. Unlike GShard’s Top-2 Gating, Switch Transformer routes each input token to only one expert network, achieving higher sparsity and further reducing computational costs, making it possible to train trillion-parameter models. It encourages more balanced token routing among the $N$ experts. Switch Transformer’s auxiliary loss is based on the product sum of the actual routing fraction and the predicted routing probability, formulated as:\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ Parameters: $N$: Total number of experts. $f_i$: Fraction of tokens routed to the $i$-th expert, defined as: $$ f_i = \\frac{1}{T} \\sum_{x \\in B} \\mathbb{1}\\{\\text{argmax } p(x) = i\\} $$ $P_i$: Routing probability for the $i$-th expert predicted by the gating network, defined as: $$ P_i = \\frac{1}{T} \\sum_{x \\in B} p_i(x) $$ $T$: Total number of tokens in batch $B$. $\\alpha$: Weight hyperparameter for the auxiliary loss, typically set to $10^{-2}$. By minimizing this loss, the model encourages the actual routing fraction $f_i$ to align with the predicted probability $P_i$, indirectly promoting load balance among experts and preventing some from being idle.\nFig. 8. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)\nSwitch Router Mechanism:\nRouting Prediction: For an input token $x$, the Switch Router predicts the routing probability $p_i = G^{(i)}(x)$ for each expert network, where $i = 1, 2, …, n$, and n is the number of expert networks.\nExpert Selection: Select the expert network with the highest routing probability as the best expert. Switch Transformer uses a Top-1 routing strategy, meaning each token is routed only to the expert with the highest probability.\nToken Routing: Route the input token $x$ to the selected best expert network for processing.\nSwitch Transformer Training Stability Optimizations:\nTo improve the training stability of Switch Transformer, the paper proposes the following optimization strategies:\nSelective Precision: Using FP32 precision inside the router function improves training stability without the overhead of FP32 tensor communication. Specifically, the Switch Router computations are performed entirely in FP32, and the final result is converted back to FP16 to balance efficiency and precision.\nSmaller Initialization: It is recommended to adjust the Transformer weight initialization scale parameter $s$ from 1.0 to 0.1. A smaller initialization scale helps mitigate the risk of gradient explosion early in training, thereby improving overall training stability. This is implemented by sampling from a truncated normal distribution with mean 0 and standard deviation $\\sqrt{s/n}$ (where $n$ is the number of input units).\nHigher Expert Dropout: Using a higher dropout rate (e.g., 0.4) in the expert FFN layers while maintaining a lower dropout rate (e.g., 0.1) in non-expert layers effectively prevents overfitting and enhances the model’s generalization ability. The experimental results in the figure below show that the model performs best on tasks like GLUE, CNNDM, SQuAD, and SuperGLUE when the expert layer dropout rate is set to 0.4.\nFig. 9. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set (higher numbers are better). (Image source: Fedus et al. 2021)\nThe Switch Transformers paper uses the following figure to intuitively illustrate how different parallelism techniques partition model weights and data:\nFig. 10. An illustration of various parallelism strategies on how (Top) model weights and (Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)\nExpert Choice Expert Choice (EC) (Zhou et al. 2022) is a routing strategy opposite to token choice routing (like GShard’s top-2 or Switch Transformer’s top-1). In token choice routing, each token selects top-k experts from all available experts. In expert choice routing, each expert selects top-k tokens from all available tokens to process. This approach aims to address the load imbalance and token dropping issues of token choice routing while significantly improving training efficiency. Here is the specific computation process:\nCompute token-to-expert affinity scores:\nFor an input matrix $X \\in \\mathbb{R}^{n \\times d}$, the token-to-expert affinity score matrix $S \\in \\mathbb{R}^{n \\times e}$ is computed as:\n$$ S = \\text{softmax}(X \\cdot W_g), \\quad \\text{where } W_g \\in \\mathbb{R}^{d \\times e}. $$ Here, $W_g$ is the gating weight matrix, and $e$ is the number of experts.\nExperts select tokens:\nEach expert selects the top-k tokens from all tokens to process. This is done by performing top-k selection on $S^T$:\n$$ G, I = \\text{top-}k(S^T, k), $$This yields:\nGating matrix $G \\in \\mathbb{R}^{e \\times k}$: Records the routing weights corresponding to the tokens selected by the experts, where $G[i, j]$ is the weight for the $j$-th token selected by expert $i$. Token index matrix $I \\in \\mathbb{R}^{e \\times k}$: Indicates the indices of the tokens selected by each expert in the input. One-hot encoding:\nConvert the token index matrix $I$ into a one-hot encoded matrix $P \\in \\mathbb{R}^{e \\times k \\times n}$ for subsequent calculations:\n$$ P = \\operatorname{one}-\\operatorname{hot}(I) $$ Construct input for Gated FFN layer:\nFor each expert $i$, the input to its gated FFN layer is:\n$$ (P \\cdot X) \\in \\mathbb{R}^{e \\times k \\times d}. $$ EC controls model sparsity by regularizing the number of experts each token is routed to. A common regularization objective is:\n$$ \\begin{aligned} \u0026 \\max_{A} \\langle S^{\\top}, A \\rangle + \\lambda H(A) \\\\ \u0026 \\text{s.t. } \\forall i: \\sum_{j'} A[i, j'] = k, \\quad \\forall j: \\sum_{i'} A[i', j] \\leq b, \\quad \\forall i,j: 0 \\leq A[i, j] \\leq 1, \\end{aligned} $$The optimization problem defines a matrix $A$ where the element at row $i$, column $j$ indicates whether expert $i$ selected token $j$ (value 0 or 1). Since solving this optimization problem is complex, the paper uses Dijkstra’s algorithm (obtaining an approximate solution through multiple iterations).\nThe parameter $b$ is typically determined by the total number of tokens $n$ in the batch and a capacity factor, which represents the average number of experts used per token. Most experiments use a high capacity factor. Experimental results show that even with reduced capacity, EC generally outperforms traditional top-1 token choice routing, although capped expert choice slightly degrades fine-tuning performance.\nThe advantages of EC are mainly twofold:\nPerfect Load Balancing: Each expert processes a fixed $k$ tokens, avoiding the issue of some experts being overloaded while others are idle, achieving ideal load balance. Higher Training Efficiency: Experiments show that EC can improve training convergence speed by about 2x, making it more efficient than traditional token choice routing. However, EC also has limitations:\nBatch Size Requirement: EC requires a relatively large batch size, making it unsuitable for scenarios with smaller batch sizes. Autoregressive Generation Limitation: In autoregressive text generation tasks, EC’s top-k selection cannot be implemented because future tokens are unknown, making it unsuitable for such tasks. DeepSeekMoE Mixture-of-Experts (MoE) models enhance efficiency and performance by routing computation to specific “expert” subnetworks. DeepSeek-V2 and V3 employ an architecture named DeepSeekMoE (Dai et al., 2024) in their FFN (Feed-Forward Network) layers. Compared to traditional MoE architectures like GShard, the core ideas of DeepSeekMoE are:\nFine-grained Expert Segmentation: Splitting expert networks into smaller units. This aims for higher expert specialization and more precise knowledge acquisition, as each expert can focus on a narrower domain. Shared Expert Isolation: The architecture includes a set of “shared experts” processed by all tokens, intended to handle general knowledge. This reduces knowledge redundancy among the “routing experts” that need to be selected, allowing them to focus more on specific knowledge. Basic Architecture For an input token representation \\(\\mathbf{u}_t\\) to the FFN layer, the output \\(\\mathbf{h}_t'\\) of DeepSeekMoE is computed by combining the outputs of shared experts and selected routing experts: \\[ \\mathbf{h}_{t}^{\\prime} = \\mathbf{u}_{t} + \\sum_{i=1}^{N_{s}} \\operatorname{FFN}_{i}^{(s)}(\\mathbf{u}_{t}) + \\sum_{i=1}^{N_{r}} g_{i, t} \\operatorname{FFN}_{i}^{(r)}(\\mathbf{u}_{t}), \\] where:\n\\(N_s\\) is the number of shared experts. \\(N_r\\) is the number of routing experts. \\(\\operatorname{FFN}_i^{(s)}\\) is the \\(i\\)-th shared expert network. \\(\\operatorname{FFN}_i^{(r)}\\) is the \\(i\\)-th routing expert network. \\(g_{i, t}\\) is the gating value (weight) assigned to the \\(i\\)-th routing expert for the \\(t\\)-th token. The calculation of the gating value \\(g_{i,t}\\), based on token-to-expert affinity scores \\(s_{i,t}\\) and selected via a Top-K routing mechanism, is one of the key differences between DeepSeek-V2 and V3.\nV2 vs V3 Gating Mechanism and Load Balancing Comparison A core challenge in MoE models is load balancing: ensuring all experts are effectively utilized, avoiding situations where some experts are overloaded while others are idle, which affects training stability and computational efficiency. DeepSeek-V2 and V3 adopt different approaches to gating mechanisms and load balancing strategies.\n1. Affinity Calculation (\\(s_{i,t}\\)) and Top-K Selection:\nDeepSeek-V2: Uses the Softmax function to compute the affinity score of each token for each routing expert. Top-K selection is directly based on these affinity scores \\(s_{i,t}\\). \\[ s_{i, t} = \\operatorname{Softmax}_{i}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i}) \\] where \\(\\mathbf{e}_i\\) is the learnable center vector for the \\(i\\)-th routing expert. The \\(K_r\\) experts with the highest \\(s_{i,t}\\) are selected.\nDeepSeek-V3: Uses the Sigmoid function to compute affinity scores. More importantly, it introduces a learnable bias term \\(b_i\\) for each routing expert. Top-K selection is based on the bias-adjusted affinity \\(s_{i,t} + b_i\\). \\[ s_{i, t} = \\operatorname{Sigmoid}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i}) \\] Selection is based on the \\(K_r\\) experts with the highest \\(s_{i,t} + b_i\\) values.\n2. Gating Value Calculation (\\(g_{i,t}\\)):\nDeepSeek-V2: For experts selected by Top-K, their gating value \\(g_{i,t}\\) is directly equal to their original affinity score \\(s_{i,t}\\). For unselected experts, \\(g_{i,t} = 0\\). \\[ g_{i, t}^{\\prime} = \\begin{cases} s_{i, t}, \u0026 s_{i, t} \\in \\operatorname{Topk}(\\{s_{j, t}\\}, K_{r}), \\\\ 0, \u0026 \\text{otherwise}, \\end{cases} \\] \\[ g_{i, t} = g_{i, t}^{\\prime} \\quad (\\text{No additional normalization in V2}) \\] DeepSeek-V3: For experts selected based on \\(s_{i,t} + b_i\\), their gating value \\(g_{i,t}\\) is obtained by normalizing the original affinity scores \\(s_{i,t}\\) of these selected experts. The bias \\(b_i\\) is only used for routing selection and does not affect the final weighted sum. \\[ g_{i, t}^{\\prime}= \\begin{cases} s_{i, t}, \u0026 s_{i, t}+b_{i} \\in \\operatorname{Topk}\\left(\\left\\{s_{j, t}+b_{j} \\mid 1 \\leqslant j \\leqslant N_{r}\\right\\}, K_{r}\\right) \\\\ 0, \u0026 \\text{otherwise.} \\end{cases} \\] \\[ g_{i, t} = \\frac{g_{i, t}^{\\prime}}{\\sum_{j=1}^{N_{r}} g_{j, t}^{\\prime}} \\quad (\\text{Normalize affinities of selected experts}) \\] 3. Load Balancing Strategy:\nDeepSeek-V2:\nPrimary Strategy: Auxiliary Losses V2 introduces multiple auxiliary loss terms to explicitly encourage load balancing: Expert-level Balancing Loss (\\(\\mathcal{L}_{\\text{ExpBal}}\\)): Encourages each expert to process roughly the same number of tokens. \\[ \\begin{aligned} \\mathcal{L}_{\\text{ExpBal}} \u0026= \\alpha_{1} \\sum_{i=1}^{N_{r}} f_{i} P_{i} \\\\ f_{i} \u0026= \\frac{N_{r}}{K_{r} T} \\sum_{t=1}^{T} \\mathbb{1}(\\text{Token } t \\text{ selects Expert } i) \\\\ P_{i} \u0026= \\frac{1}{T} \\sum_{t=1}^{T} s_{i, t} \\end{aligned} \\] where \\(T\\) is the total number of tokens in the batch, \\(f_i\\) is the fraction of tokens routed to expert \\(i\\) (relative to the ideal balanced state), \\(P_i\\) is the average affinity score for expert \\(i\\), and \\(\\alpha_1\\) is a hyperparameter. Device-level Balancing Loss (\\(\\mathcal{L}_{\\text{DevBal}}\\)): Encourages uniform distribution of computational load across different device groups (assuming experts are distributed across \\(D\\) device groups \\(\\{\\mathcal{E}_1, \\dots, \\mathcal{E}_D\\}\\)). \\[ \\begin{aligned} \\mathcal{L}_{\\text{DevBal}} \u0026= \\alpha_{2} \\sum_{i=1}^{D} f_{i}^{\\prime} P_{i}^{\\prime} \\\\ f_{i}^{\\prime} \u0026= \\frac{1}{|\\mathcal{E}_{i}|} \\sum_{j \\in \\mathcal{E}_{i}} f_{j} \\\\ P_{i}^{\\prime} \u0026= \\sum_{j \\in \\mathcal{E}_{i}} P_{j} \\end{aligned} \\] where \\(f_i'\\) is the average load score for device group \\(i\\), \\(P_i'\\) is the total affinity for device group \\(i\\), and \\(\\alpha_2\\) is a hyperparameter. Communication Balancing Loss (\\(\\mathcal{L}_{\\text{CommBal}}\\)): Encourages roughly equal numbers of tokens sent to each device to balance All-to-All communication load. \\[ \\begin{aligned} \\mathcal{L}_{\\text{CommBal}} \u0026= \\alpha_{3} \\sum_{i=1}^{D} f_{i}^{\\prime \\prime} P_{i}^{\\prime \\prime} \\\\ f_{i}^{\\prime \\prime} \u0026= \\frac{D}{M T} \\sum_{t=1}^{T} \\mathbb{1}(\\text{Token } t \\text{ is sent to Device } i) \\\\ P_{i}^{\\prime \\prime} \u0026= \\sum_{j \\in \\mathcal{E}_{i}} P_{j} \\end{aligned} \\] where \\(f_i''\\) is the fraction of tokens sent to device \\(i\\) (relative to the ideal balanced state), \\(P_i''\\) is the total affinity for device group \\(i\\), and \\(\\alpha_3\\) is a hyperparameter. Routing Restriction: Device-Limited Routing Limits each token to route to experts distributed on at most \\(M\\) different devices. In V2, \\(M=3\\). Token Dropping: During training, if a device receives more tokens than a preset capacity factor (usually slightly above the average), some tokens with the lowest routing weights (affinities) are dropped to avoid wasting computational resources. However, tokens from about 10% of sequences are preserved from dropping. DeepSeek-V3:\nPrimary Strategy: Auxiliary-Loss-Free Load Balancing V3 posits that auxiliary losses can harm model performance and thus adopts an innovative Auxiliary-Loss-Free Load Balancing (Wang et al., 2024). It achieves load balancing by dynamically adjusting the aforementioned learnable bias terms \\(b_i\\): Bias Update: After each training step, monitor the number of tokens processed by each expert \\(i\\) in the current batch. If expert \\(i\\) is overloaded (processed tokens \u003e Total batch tokens / \\(N_r\\)), decrease its bias: \\(b_i \\leftarrow b_i - \\gamma\\). If expert \\(i\\) is underloaded (processed tokens \u003c Total batch tokens / \\(N_r\\)), increase its bias: \\(b_i \\leftarrow b_i + \\gamma\\). \\(\\gamma\\) is a small positive step size (bias update rate hyperparameter). This way, highly loaded experts become less likely to be selected in subsequent routing, while lowly loaded experts become more likely, dynamically balancing the load at the batch level. Supplementary Strategy: Sequence-Level Auxiliary Loss (\\(\\mathcal{L}_{\\text{Bal}}\\)) V3 still retains an auxiliary loss with an extremely small weight (\\(\\alpha=0.0001\\)), but it acts on the expert selection balance within individual sequences, rather than the entire batch. This is mainly to prevent extreme imbalance within a single sequence. \\[ \\begin{gathered} \\mathcal{L}_{\\text{Bal}} = \\alpha \\sum_{i=1}^{N_{r}} f_{i} P_{i}, \\\\ f_{i} = \\frac{N_{r}}{K_{r} T_{seq}} \\sum_{t=1}^{T_{seq}} \\mathbb{1}\\left(s_{i, t} \\in \\operatorname{Topk}\\left(\\left\\{s_{j, t} \\mid 1 \\leqslant j \\leqslant N_{r}\\right\\}, K_{r}\\right)\\right), \\\\ s_{i, t}^{\\prime} = \\frac{s_{i, t}}{\\sum_{j=1}^{N_{r}} s_{j, t}}, \\quad P_{i} = \\frac{1}{T_{seq}} \\sum_{t=1}^{T_{seq}} s_{i, t}^{\\prime} \\end{gathered} \\] Note that \\(f_i, P_i\\) here are computed over a single sequence (length \\(T_{seq}\\)), and \\(s_{i,t}'\\) is the value of original \\(s_{i,t}\\) normalized within the sequence. Routing Restriction: Node-Limited Routing Similar to V2’s device limit, but applied at the node level. In V3, \\(M=4\\). No Token Dropping: Due to the effectiveness of bias-adjustment-based load balancing, V3 does not drop any tokens during training or inference. Advantages of V3’s Strategy: V3’s auxiliary-loss-free strategy aims to minimize the negative impact of the load balancing mechanism on the final model performance. By dynamically adjusting bias terms for batch-level load balancing, the constraints are looser compared to V2’s sequence-level balancing based on auxiliary losses. This allows experts to exhibit stronger specialization patterns across different domains, as routing decisions do not need to strictly follow a balanced distribution within each sequence. The figure below shows experimental results indicating this strategy outperforms auxiliary-loss-based methods on multiple benchmarks.\nFig. 11. Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. (Image source: DeepSeek-AI, 2024)\nThe key difference between auxiliary-loss-free load balancing and sequence-level auxiliary loss lies in their balancing scope: batch-level versus sequence-level. Compared to sequence-level auxiliary loss, batch-level balancing imposes more flexible constraints as it does not enforce domain balance within each sequence. This flexibility allows experts to specialize better across different domains. To validate this, the figure records and analyzes the expert load on different domains of the Pile test set for a 16B baseline model with auxiliary loss and a 16B model without auxiliary loss. It can be observed that the auxiliary-loss-free model exhibits more pronounced expert specialization patterns, as expected.\nFig. 12. Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. (Image source: DeepSeek-AI, 2024)\nDeepSeekMoE V2 vs V3 Comparison Summary Table Feature DeepSeek-V2 DeepSeek-V3 Affinity Calculation \\(s_{i,t}\\) \\(\\operatorname{Softmax}_{i}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i})\\) \\(\\operatorname{Sigmoid}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i})\\) TopK Selection Basis Original affinity \\(s_{i,t}\\) Bias-adjusted affinity \\(s_{i,t} + b_i\\) Gating Value Calc. \\(g_{i,t}\\) For selected experts, \\(g_{i,t} = s_{i,t}\\) (Usually no extra normalization) For selected experts, normalize based on original affinity \\(s_{i,t}\\): \\(g_{i, t} = \\frac{s_{i, t}}{\\sum_{j \\in \\text{Selected}} s_{j, t}}\\) Primary Load Balancing Auxiliary Losses: - \\(\\mathcal{L}_{\\text{ExpBal}}\\) (Expert-level) - \\(\\mathcal{L}_{\\text{DevBal}}\\) (Device-level) - \\(\\mathcal{L}_{\\text{CommBal}}\\) (Comm-level) Auxiliary-Loss-Free: - Dynamic adjustment of learnable bias \\(b_i\\) (step \\(\\gamma\\)) for batch-level balancing Supplementary Balancing No explicit supplementary strategy Sequence-Level Aux Loss \\(\\mathcal{L}_{\\text{Bal}}\\) (Weight \\(\\alpha\\) minimal, e.g., 0.0001), prevents extreme imbalance within single sequences Routing Restriction Device Limit: Each token routes to experts on at most \\(M=3\\) devices Node Limit: Each token routes to experts on at most \\(M=4\\) nodes Token Dropping Yes: During training, tokens exceeding device capacity with lowest affinity are dropped (preserving ~10% sequences) to mitigate bottlenecks No: No tokens dropped during training or inference Balancing Granularity Primarily enforced at sequence/batch level via auxiliary losses Primarily balanced dynamically at batch level via bias adjustment, looser constraints Impact on Performance Auxiliary losses might negatively impact model performance Designed to minimize negative impact of balancing strategy on performance, allowing better expert specialization Multi-Token Prediction (MTP) To further enhance model performance and data efficiency, DeepSeek-V3 introduces the Multi-Token Prediction (MTP) training objective (inspired by Gloeckle et al., 2024). Standard language models only predict the next token, whereas MTP makes the model predict multiple future tokens (in V3, \\(D_{MTP}=1\\), i.e., predicting the token after the next) at each position.\nMTP Implementation MTP is implemented through \\(D_{MTP}\\) sequential modules. The \\(k\\)-th MTP module (\\(k=1, \\dots, D_{MTP}\\)) contains:\nA shared embedding layer \\(\\operatorname{Emb}(\\cdot)\\) A shared output head \\(\\operatorname{OutHead}(\\cdot)\\) Independent Transformer blocks \\(\\operatorname{TRM}_k(\\cdot)\\) Independent projection matrices \\(M_k \\in \\mathbb{R}^{d \\times 2d}\\) For the \\(i\\)-th token \\(t_i\\) in the input sequence, at the \\(k\\)-th prediction depth:\nConcatenate the representation of the \\(i\\)-th token at depth \\(k-1\\), \\(\\mathbf{h}_i^{k-1}\\) (which is the main model output when \\(k=1\\)), with the embedding of the \\((i+k)\\)-th token, \\(\\operatorname{Emb}(t_{i+k})\\). Project this concatenation through matrix \\(M_k\\) to get the combined representation \\(\\mathbf{h}_i^{\\prime k}\\): \\[ \\mathbf{h}_{i}^{\\prime k} = M_{k}[\\operatorname{RMSNorm}(\\mathbf{h}_{i}^{k-1}) ; \\operatorname{RMSNorm}(\\operatorname{Emb}(t_{i+k}))] \\] Input the combined representation into the \\(k\\)-th Transformer block to get the output representation \\(\\mathbf{h}_i^k\\) for the current depth: \\[ \\mathbf{h}_{1: T-k}^{k} = \\operatorname{TRM}_{k}(\\mathbf{h}_{1: T-k}^{\\prime k}) \\] Use the shared output head to predict the probability distribution \\(P_{i+k+1}^k \\in \\mathbb{R}^V\\) for the \\((i+k+1)\\)-th token: \\[ P_{i+k+1}^{k} = \\operatorname{OutHead}(\\mathbf{h}_{i}^{k}) \\] Crucially, this implementation maintains the complete causal chain for each prediction depth, differing from methods that predict multiple tokens in parallel.\nFig. 13. Illustration of our Multi-Token Prediction (MTP) implementation. They keep the complete causal chain for the prediction of each token at each depth. (Image source: DeepSeek-AI, 2024)\nMTP Training Objective Compute the cross-entropy loss \\(\\mathcal{L}_{\\text{MTP}}^k\\) for each prediction depth \\(k\\): \\[ \\mathcal{L}_{\\text{MTP}}^{k} = \\operatorname{CrossEntropy}(P_{2+k: T+1}^{k}, t_{2+k: T+1}) = -\\frac{1}{T} \\sum_{i=2+k}^{T+1} \\log P_{i}^{k}[t_{i}] \\] The total MTP loss is the weighted average of losses across all depths: \\[ \\mathcal{L}_{\\text{MTP}} = \\frac{\\lambda}{D_{MTP}} \\sum_{k=1}^{D_{MTP}} \\mathcal{L}_{\\text{MTP}}^{k} \\] where \\(\\lambda\\) is a weighting factor (0.3 initially, 0.1 later in V3 training). This loss is added to the main model’s standard next-token prediction loss.\nMTP Inference MTP is primarily used to enhance the main model’s performance. During inference, the MTP modules can be simply discarded, and the main model works independently. Alternatively, the MTP modules can be utilized for speculative decoding (Leviathan et al., 2023; Xia et al., 2023) to accelerate the generation process. V3 experiments show an acceptance rate of 85%-90% for the second token, potentially speeding up decoding by about 1.8x.\nInfrastructure and Training Efficiency The efficient training and deployment of DeepSeek-V3 benefit from the synergistic design of algorithms, frameworks, and hardware.\nCompute Cluster DeepSeek-V3 was trained on a cluster equipped with 2048 NVIDIA H800 GPUs.\nIntra-node: Each node contains 8 H800 GPUs interconnected via high-speed NVLink and NVSwitch. Inter-node: Different nodes communicate using the InfiniBand (IB) network. Training Framework DeepSeek-V3 training is based on the self-developed, efficient, and lightweight framework HAI-LLM. Overall, it employs:\n16-way Pipeline Parallelism (PP) (Qi et al., 2023) 64-way Expert Parallelism (EP) (across 8 nodes) (Lepikhin et al., 2021) ZeRO-1 Data Parallelism (DP) (Rajbhandari et al., 2020) To achieve efficient training, Deepseek performed meticulous engineering optimizations:\nDesigned the DualPipe algorithm for efficient pipeline parallelism, reducing bubbles and overlapping computation with communication, addressing the heavy communication overhead introduced by cross-node expert parallelism. Developed efficient cross-node All-to-all communication Kernels that fully utilize IB and NVLink bandwidth while saving SM resources for communication. Carefully optimized memory usage during training, enabling DeepSeek-V3 to be trained without using Tensor Parallelism (TP). DualPipe and Computation-Communication Overlap Challenge: Cross-node expert parallelism leads to a computation-to-communication ratio close to 1:1, which is inefficient. Fig. 17. Overlapping strategy for a pair of forward and backward chunks with misaligned transformer block boundaries. Orange: forward, green: backward for input, blue: backward for weights, purple: PP communication, red: barriers. Both all-to-all and PP communications are fully hidden. (Image source: DeepSeek-AI, 2024)\nCore Idea: Overlap computation and communication within a pair of independent forward and backward chunks. Each chunk is decomposed into four components: Attention, All-to-all Dispatch, MLP, All-to-all Combine (backward Attention and MLP are further divided into backward for input and backward for weights, similar to ZeroBubble (Qi et al., 2023). By rearranging these components and manually adjusting the GPU SM ratio for communication versus computation, both All-to-all and PP communication can be fully hidden. Scheduling: Adopts a bidirectional pipeline schedule, feeding micro-batches from both ends of the pipeline simultaneously, allowing most communication to be completely overlapped. Fig. 18. Example DualPipe scheduling with 8 PP ranks and 20 micro-batches in both directions. The reverse-direction micro-batches mirror the forward ones, so their batch IDs are omitted for simplicity. Two cells within a shared black border represent mutually overlapped computation and communication. (Image source: DeepSeek-AI, 2024)\nAdvantages: Efficient even in general scenarios without heavy communication burden. Compared to ZB1P (Qi et al., 2023) and 1F1B (Harlap et al., 2018), significantly reduces pipeline bubbles, only increasing peak activation memory by a factor of \\(\\frac{1}{PP}\\). Although requiring two copies of model parameters, the memory increase is not significant due to the large EP size used in training. Compared to Chimera (Li and Hoefler, 2021), has looser requirements on the number of micro-batches (only needs to be divisible by 2), and bubble/activation memory does not grow with the number of micro-batches. Method (Method) Bubble (Bubble) Parameter (Parameter) Activation (Activation) 1F1B \\((PP-1)(F+B)\\) \\(1 \\times\\) \\(PP\\) ZB1P \\((PP-1)(F+B-2W)\\) \\(1 \\times\\) \\(PP\\) DualPipe (Deepseek V3) \\(\\left(\\frac{PP}{2}-1\\right)(F\\\u0026B+B-3W)\\) \\(2 \\times\\) \\(PP+1\\) The table above compares pipeline bubble and memory usage for different pipeline parallelism methods. \\(F\\): Forward chunk execution time; \\(B\\): Full backward chunk execution time; \\(W\\): “Backward for weights” chunk execution time; \\(F\\\u0026B\\): Execution time of two mutually overlapped forward and backward chunks.\nEfficient Cross-Node All-to-All Communication Implementation Goal: Provide sufficient computational performance for DualPipe by customizing efficient cross-node All-to-all communication Kernels (dispatching \u0026 combining), saving SMs dedicated to communication. Strategy: Combine MoE gating algorithm with cluster network topology (fully connected IB between nodes, NVLink within nodes). Bandwidth Utilization: NVLink bandwidth (\\(160 \\mathrm{~GB} / \\mathrm{s}\\)) is about 3.2 times IB bandwidth (\\(50 \\mathrm{~GB} / \\mathrm{s}\\)). Limit each token to be dispatched to at most 4 nodes to reduce IB traffic. Transmission Path: After token routing is determined, tokens are first transmitted via IB to the GPU with the same intra-node index on the target node. Upon arrival at the target node, they are immediately forwarded via NVLink to the specific GPU hosting the target expert, avoiding blockage by subsequently arriving tokens. Effect: IB and NVLink communication are fully overlapped. Each token can efficiently select an average of 3.2 experts/node without additional NVLink overhead. This means V3 actually selects 8 routing experts, but could theoretically scale up to 13 experts (4 nodes × 3.2 experts/node) with no increase in communication cost. Implementation: Use Warp Specialization (Bauer et al., 2014) technology to divide 20 SMs into 10 communication channels. Dispatch process: IB send, IB-to-NVLink forward, NVLink receive are handled by respective warps, with warp counts dynamically adjusted based on load. Combine process: NVLink send, NVLink-to-IB forward \u0026 accumulate, IB receive \u0026 accumulate are also handled by dynamically adjusted warps. Optimization: Dispatch and Combine Kernels overlap with computation streams. Use custom PTX instructions and automatically adjust communication chunk sizes to significantly reduce L2 cache usage and interference with other SM computation Kernels. Result: Only 20 SMs are required to fully utilize IB and NVLink bandwidth. Extreme Memory Optimization and Minimal Overhead To reduce training memory footprint, the following techniques were employed:\nRecomputation: Recompute all RMSNorm operations and MLA up-projections during backpropagation, avoiding storage of their output activations. Significantly reduces activation memory demand at minimal overhead. CPU Storage for EMA: Store the Exponential Moving Average (EMA) of model parameters in CPU memory and update asynchronously after each training step. Maintains EMA parameters without additional GPU memory or time overhead. Shared Embedding and Output Head: Leverage the DualPipe strategy to deploy the shallowest layers (including Embedding) and deepest layers (including Output Head) on the same PP rank. This allows MTP modules and the main model to physically share parameters and gradients for Embedding and Output Head, further enhancing memory efficiency. Effect: These optimizations enable DeepSeek-V3 to be trained without using expensive Tensor Parallelism (TP). FP8 Training To accelerate training and reduce GPU memory usage, DeepSeek-V3 employs an FP8 mixed-precision training framework (Dettmers et al., 2022; Noune et al., 2022; Peng et al., 2023), validating its effectiveness on ultra-large-scale models for the first time.\nMixed Precision Framework Core Computation (GEMM): Most GEMM operations (forward, activation gradient backward, weight gradient backward) use FP8 inputs, outputting BF16 or FP32, theoretically doubling computation speed. High Precision Retention: Parts sensitive to precision or with low computational overhead (e.g., Embedding, Output Head, MoE Gating, Normalization, Attention) retain BF16/FP32 precision. High Precision Storage: Main weights, weight gradients, and optimizer states (partially BF16) use higher precision, with ZeRO-1 sharding reducing GPU memory pressure. Fig. 14. The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. (Image source: DeepSeek-AI, 2024)\nPrecision Enhancement Strategies Fine-grained Quantization: To address FP8’s limited dynamic range and sensitivity to outliers (Fishman et al., 2024; He et al., 2024; Sun et al., 2024), adopt finer-grained quantization: Activations: Scaled in groups of \\(1 \\times 128\\) tiles. Weights: Scaled in groups of \\(128 \\times 128\\) blocks. This method allows scaling factors to better adapt to the range of local data, reducing quantization error. Improved Accumulation Precision: H800 Tensor Cores have limited accumulation precision (approx. 14 bits) for FP8 GEMM. To solve this, employ the Promotion to CUDA Cores strategy (Thakkar et al., 2023): Tensor Cores compute partial sums (e.g., every \\(N_C=128\\) elements), then transfer the results to CUDA Core FP32 registers for full-precision accumulation. Scaling factors from fine-grained quantization can also be efficiently applied on CUDA Cores. With concurrent execution of WGMMA operations, this method improves precision with minimal impact on computational efficiency. E4M3 Format: V3 uniformly uses the E4M3 format (4 exponent bits, 3 mantissa bits) for all tensors, rather than mixing with E5M2 (NVIDIA, 2024; Peng et al., 2023; Sun et al., 2019b). The fine-grained quantization strategy effectively mitigates the smaller dynamic range issue of E4M3. Online Quantization: Compute scaling factors based on the maximum absolute value of each tile/block in real-time, rather than relying on historical values (NVIDIA, 2024; Peng et al., 2023), ensuring quantization accuracy. Fig. 15. (a) Fine-grained quantization method to mitigate quantization errors. (b) Improved FP8 GEMM precision by promoting to CUDA Cores for high-precision accumulation. (Image source: DeepSeek-AI, 2024)\nLow-Precision Storage and Communication Optimizer States: First and second moments of AdamW (Loshchilov and Hutter, 2017) are stored in BF16. Main weights and gradient accumulation remain FP32. Activation Caching: Since Wgrad operations use FP8 inputs, activations can be cached as FP8. For specific sensitive operations (e.g., input to Linear after Attention), a custom E5M6 format with round scaling is used. Inputs to SwiGLU in MoE are also cached as FP8. Communication: Activations before MoE up-projection are quantized to FP8 for dispatch; activation gradients before MoE down-projection are also quantized to FP8. Combine operations retain BF16 precision. The figure below shows experiments demonstrating that the relative error of FP8 training loss compared to BF16 is less than 0.25%, which is within an acceptable range.\nFig. 16. Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9 DeepSeek-AI, 2024)\nInference and Deployment DeepSeek-V3 is deployed on an H800 cluster (NVLink intra-node, fully connected IB inter-node). To guarantee both SLO (Service Level Objective) for online services and high throughput, a deployment strategy separating Prefilling and Decoding stages is adopted.\nPrefilling Stage Minimum Deployment Unit: 4 nodes, 32 GPUs. Parallelism Strategy: Attention part: TP4 (Tensor Parallelism) + SP (Sequence Parallelism) combined with DP8 (Data Parallelism). The smaller TP size (4) limits TP communication overhead. MoE part: EP32 (Expert Parallelism), ensuring each expert processes a sufficiently large batch for computational efficiency. Shallow Dense MLP: Uses TP1 to save TP communication. MoE All-to-All Communication: Uses a method similar to training: first transmit tokens across nodes via IB, then forward within the node between GPUs via NVLink. Load Balancing: Employs a redundant expert deployment strategy. Based on statistics collected from online deployment, periodically (e.g., every 10 minutes) detect high-load experts and replicate them. After determining the redundant expert set, carefully reshuffle experts among GPUs within nodes based on observed load, balancing GPU load as much as possible without increasing cross-node All-to-all communication overhead. In DeepSeek-V3 deployment, the Prefilling stage sets up 32 redundant experts. Each GPU hosts its original 8 experts plus 1 additional redundant expert. Efficiency Optimization: To improve throughput and hide All-to-all and TP communication overhead, process two micro-batches with similar computational load concurrently, overlapping the Attention and MoE of one micro-batch with the Dispatch and Combine of the other. Exploration Direction: Dynamic redundancy strategy, where each GPU hosts more experts (e.g., 16), but only activates 9 per inference step. Dynamically compute the globally optimal routing scheme before each layer’s All-to-all operation begins. Since Prefilling is computationally intensive, the overhead of computing the routing scheme is almost negligible. Decoding Stage Expert Perspective: Treat the shared expert as one routing target. From this perspective, each token selects 9 experts during routing (the shared expert is considered a high-load expert that is always selected). Minimum Deployment Unit: 40 nodes, 320 GPUs. Parallelism Strategy: Attention part: TP4 + SP combined with DP80. MoE part: EP320. Each GPU hosts only one expert, with 64 GPUs responsible for hosting redundant and shared experts. All-to-All Communication: Dispatch and Combine parts use direct IB point-to-point transmission for low latency. Utilize IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency. Load Balancing: Similar to Prefilling, periodically determine the redundant expert set based on online service’s statistical expert load. However, since each GPU hosts only one expert, reshuffling is not needed. Exploration Directions: Dynamic redundancy strategy: Requires more careful optimization of the algorithm for computing the globally optimal routing scheme and fusion with the Dispatch Kernel to reduce overhead. Processing two micro-batches concurrently: Unlike Prefilling, the Attention phase takes a larger proportion of time in Decoding. Therefore, overlap the Attention of one micro-batch with the Dispatch+MoE+Combine of another. In the Decoding stage, the batch size per expert is relatively small (typically \u003c 256 tokens), making memory access the bottleneck rather than computation. Since the MoE part only needs to load parameters for one expert, memory access overhead is small, and using fewer SMs does not significantly impact overall performance. Thus, only a small portion of SMs can be allocated to Dispatch+MoE+Combine without affecting the computation speed of the Attention part. Suggestions for Hardware Design Based on the implementation of All-to-all communication and the FP8 training scheme, the DeepSeek team proposes the following chip design suggestions to AI hardware vendors.\nCommunication Hardware Current State: Communication latency is hidden through computation/communication overlap, significantly reducing dependency on communication bandwidth. However, the current communication implementation relies on expensive SMs (e.g., 20 out of 132 SMs on H800 allocated for this purpose), limiting computational throughput. Furthermore, using SMs for communication leaves Tensor Cores completely idle, which is inefficient. Primary SM Tasks: Forwarding data between IB and NVLink domains, while aggregating IB traffic destined for multiple GPUs within the same node from a single GPU. Transferring data between RDMA buffers and input/output buffers. Performing Reduce operations for All-to-all Combine. Managing fine-grained memory layout when transmitting data in chunks to multiple experts across IB and NVLink domains. Expectation: Future vendors should develop hardware to offload these communication tasks from valuable computational units (SMs), potentially as GPU co-processors or network co-processors similar to NVIDIA SHARP (Graham et al., 2016). To reduce application programming complexity, this hardware should ideally unify IB (scale-out) and NVLink (scale-up) networks from the perspective of the computational units. Through this unified interface, computational units could easily perform operations like read, write, multicast, and reduce across the entire unified IB-NVLink domain by submitting communication requests based on simple primitives. Compute Hardware Higher Precision FP8 GEMM Accumulation in Tensor Cores: Problem: In the current NVIDIA Hopper architecture Tensor Core implementation, FP8 GEMM uses fixed-point accumulation, aligning mantissa products via right shifts before addition. Experiments show that after sign-extended right shifts, only the top 14 bits of each mantissa product are used, with bits beyond this range truncated. However, achieving an accurate FP32 result from accumulating, say, 32 FP8×FP8 products requires at least 34 bits of precision. Suggestion: Future chip designs should increase the accumulation precision within Tensor Cores to support full-precision accumulation, or select an appropriate accumulation bit-width based on the precision requirements of training and inference algorithms. This approach maintains computational efficiency while ensuring errors remain within acceptable limits. Support for Tile and Block Level Quantization: Problem: Current GPUs only support per-tensor quantization, lacking native support for finer-grained quantization like tile-wise and block-wise. In the current implementation, when the \\(N_C\\) interval is reached, partial results must be copied from Tensor Cores to CUDA Cores, multiplied by scaling factors, and then added to CUDA Core FP32 registers. Although combining this with the exact FP32 accumulation strategy significantly mitigates dequantization overhead, frequent data movement between Tensor Cores and CUDA Cores still limits computational efficiency. Suggestion: Future chips should support fine-grained quantization by enabling Tensor Cores to receive scaling factors and implement MMA with grouped scaling. This would allow the entire partial sum accumulation and dequantization to be performed directly within the Tensor Cores until the final result is produced, avoiding frequent data movement. Support for Online Quantization: Problem: Current implementations struggle to efficiently support online quantization, despite its proven effectiveness in research. The existing workflow requires reading 128 BF16 activation values (output from a previous computation) from HBM for quantization, writing the quantized FP8 values back to HBM, and then reading them again for MMA. Suggestion: Future chips should fuse FP8 type conversion and TMA (Tensor Memory Accelerator) access into a single operation. This would allow quantization to occur as activations are transferred from global memory to shared memory, avoiding frequent memory reads and writes. Recommend supporting warp-level cast instructions for acceleration, further promoting better fusion of Layer Normalization and FP8 cast. Alternatively, adopt near-memory computing approaches, placing computation logic near HBM. This way, BF16 elements could be directly converted to FP8 as they are read from HBM into the GPU, reducing off-chip memory access by about 50%. Support for Transposed GEMM Operations: Problem: Fusing matrix transpose with GEMM operations is cumbersome in the current architecture. In the workflow, activations from the forward pass are quantized into \\(1 \\times 128\\) FP8 tiles and stored. During backpropagation, the matrix needs to be read, dequantized, transposed, re-quantized into \\(128 \\times 1\\) tiles, and stored back into HBM. Suggestion: Future chips should support reading transposed matrices directly from shared memory before MMA operations (for the precisions required by training and inference). Combined with the fusion of FP8 format conversion and TMA access, this enhancement would significantly simplify the quantization workflow. Training Cost and Efficiency DeepSeek-V2: Compared to DeepSeek 67B (Dense), achieved 42.5% savings in training cost, 93.3% reduction in KV cache, and a 5.76x increase in maximum throughput. DeepSeek-V3: Extremely high training efficiency, requiring only 180K H800 GPU hours per 1T tokens trained. The total training cost (pre-training + context extension + post-training) was only 2.788M H800 GPU hours (approx. $5.58 million, assuming $2/hour). Pre-training took less than 2 months on the 2048 H800 GPU cluster. Training Stage H800 GPU Hours Estimated Cost (USD) Pre-training 2664 K $5.328 M Context Extension 119 K $0.238 M Post-training 5 K $0.01 M Total 2788 K $5.576 M Pre-training Data Construction Compared to DeepSeek-V2 (based on a 67B model, using a 100K vocabulary Byte-level BPE Tokenizer, 8.1T tokens), DeepSeek-V3 achieved larger scale and higher quality data construction during the pre-training phase through the following strategies:\nCorpus Expansion and Refinement\nDomain Focus: Significantly increased the proportion of text related to mathematics and programming, strengthening the model’s understanding and generation capabilities in technical domains. Multilingual Coverage: Added corpora in multiple languages beyond English and Chinese, improving cross-lingual generalization performance. Deduplication and Diversity: Employed efficient data deduplication and filtering processes to minimize redundancy while ensuring content diversity. Scale Increase: Ultimately constructed approximately 14.8T high-quality tokens, an increase of nearly 83% compared to V2. Training Strategy and Technical Innovation\nDocument Packing Combined the Document Packing (Ding et al., 2024) method, packing coherent texts into longer segments to improve GPU utilization and context integrity; did not use cross-sample attention masks to maintain implementation simplicity. Fill-in-Middle (FIM) Strategy Motivation: Inspired by DeepSeekCoder-V2 (DeepSeek-AI, 2024), aimed at enhancing the model’s ability to fill in missing information in the middle. Framework: Introduced the Prefix-Suffix-Middle (PSM) structure, with examples like: \u003c|fim_begin|\u003e f_pre \u003c|fim_hole|\u003e f_suf \u003c|fim_end|\u003e f_middle \u003c|eos_token|\u003e Application Ratio: Inserted FIM before document-level pre-packing, accounting for 10%, balancing generation and prediction tasks. Tokenizer Optimization\nBBPE Vocabulary Expansion: Adopted Byte-level BPE, expanding the vocabulary from 100K to 128K, improving coverage of rare words and proper nouns. Pre-tokenizer Improvement: Adjusted tokenization rules for multilingual scenarios, enhancing compression efficiency and encoding consistency. Boundary Bias Mitigation: Referenced Lundberg, 2023’s method to reduce bias from punctuation + newline combination tokens in few-shot scenarios by introducing a random splitting mechanism, exposing the model to more boundary variations. Hyperparameters Parameter DeepSeek-V2 DeepSeek-V3 Transformer Layers 60 61 Hidden Dimension \\(d\\) 5120 7168 Initialization Stddev 0.006 0.006 MLA Parameters Attention Heads \\(n_h\\) 128 128 Dim per Head \\(d_h\\) 128 128 KV Compression Dim \\(d_c\\) 512 (\\(4d_h\\)) 512 (\\(4d_h\\)) Query Compression Dim \\(d_c'\\) 1536 1536 Decoupled RoPE Dim \\(d_h^R\\) 64 (\\(d_h/2\\)) 64 (\\(d_h/2\\)) DeepSeekMoE Parameters MoE Layer Position All except layer 1 All except first 3 layers Shared Experts \\(N_s\\) 2 1 Routing Experts \\(N_r\\) 160 256 Expert Intermediate Dim 1536 2048 Activated Experts \\(K_r\\) 6 8 Device/Node Limit \\(M\\) 3 (Device) 4 (Node) Load Balancing Strategy Aux Losses (\\(\\alpha_1=0.003, \\alpha_2=0.05, \\alpha_3=0.02\\)) + Token Dropping Aux-Loss-Free (\\(\\gamma=0.001\\)) + Seq Loss (\\(\\alpha=0.0001\\)) MTP Parameters (V3 only) Prediction Depth \\(D_{MTP}\\) N/A 1 MTP Loss Weight \\(\\lambda\\) N/A 0.3 (first 10T) / 0.1 (last 4.8T) Training Parameters Optimizer AdamW (\\(\\beta_1=0.9, \\beta_2=0.95, wd=0.1\\)) AdamW (\\(\\beta_1=0.9, \\beta_2=0.95, wd=0.1\\)) Max Sequence Length 4K 4K Training Tokens 8.1T 14.8T Learning Rate Warmup + Step Decay (Max \\(2.4 \\times 10^{-4}\\)) Warmup + Cosine Decay + Constant (Max \\(2.2 \\times 10^{-4}\\)) Batch Size 2304 -\u003e 9216 3072 -\u003e 15360 Gradient Clipping 1.0 1.0 Precision BF16 FP8 Mixed Precision Long Context Extension Both models use the YaRN (Peng et al., 2023) technique to extend the context window.\nDeepSeek-V2: Extended from 4K to 128K. Used YaRN (scale \\(s=40, \\alpha=1, \\beta=32\\)), trained for 1000 steps on 32K sequence length. Adjusted length scaling factor \\(\\sqrt{t}=0.0707 \\ln s+1\\). DeepSeek-V3: Extended from 4K to 32K, then to 128K in two stages. Each stage trained for 1000 steps. YaRN parameters same as V2, length scaling factor \\(\\sqrt{t}=0.1 \\ln s+1\\). First stage sequence length 32K, second stage 128K. Both models demonstrated good long context capabilities in the NIAH test.\nFig. 19. Evaluation results on the ‘Needle In A Haystack’ (NIAH) tests for DeepSeek-V2. (Image source: DeepSeek-AI, 2024)\nFig. 20. Evaluation results on the ‘Needle In A Haystack’ (NIAH) tests for DeepSeek-V3. (Image source: DeepSeek-AI, 2024)\nEvaluation DeepSeek-V2 Evaluation Results:\nComparison of DeepSeek-V2 with representative open-source models (partial results). DeepSeek-V2 achieved state-of-the-art performance at the time with 21B activated parameters.\nBenchmark (Metric) # Shots DeepSeek 67B Qwen1.5 72B Mixtral 8x22B LLaMA 3 70B DeepSeek-V2 # Activated Params - 67B 72B 39B 70B 21B English MMLU (Hendrycks et al., 2020) (Acc.) 5-shot 71.3 77.2 77.6 78.9 78.5 Code HumanEval (Chen et al., 2021) (Pass@1) 0-shot 45.1 43.9 53.1 48.2 48.8 Math GSM8K (Cobbe et al., 2021) (EM) 8-shot 63.4 77.9 80.3 83.0 79.2 Chinese C-Eval (Huang et al., 2023) (Acc.) 5-shot 66.1 83.7 59.6 67.5 81.7 DeepSeek-V3 Evaluation Results:\nComparison of DeepSeek-V3-Base with representative open-source models (partial results). DeepSeek-V3-Base became the strongest open-source model on most benchmarks, especially in code and math.\nBenchmark (Metric) # Shots DeepSeek-V2 Base Qwen2.5 72B Base LLaMA-3.1 405B Base DeepSeek-V3 Base # Activated Params - 21B 72B 405B 37B English MMLU (Hendrycks et al., 2020) (EM) 5-shot 78.4 85.0 84.4 87.1 MMLU-Pro (Wang et al., 2024) (em) 5-shot 51.4 58.3 52.8 64.4 Code HumanEval (Chen et al., 2021) (Pass@1) 0-shot 43.3 53.0 54.9 65.2 LiveCodeBench-Base (Jain et al., 2024) (Pass@1) 3-shot 11.6 12.9 15.5 19.4 Math GSM8K (Cobbe et al., 2021) (Em) 8-shot 81.6 88.3 83.5 89.3 MATH (Hendrycks et al., 2021) (EM) 4-shot 43.4 54.4 49.0 61.6 Chinese C-Eval (Huang et al., 2023) (EM) 5-shot 81.4 89.2 72.5 90.1 Multilingual MMMLU-non-English (OpenAI, 2024) (em) 5-shot 64.0 74.8 73.8 79.4 Summary: DeepSeek-V3-Base, leveraging its architectural innovations, larger training dataset, and efficient training methods, comprehensively surpassed DeepSeek-V2-Base and other top open-source models (including LLaMA-3.1 405B, whose total parameter count far exceeds V3’s activated parameters).\nAlignment To enable the models to better understand instructions, follow human preferences, and enhance specific capabilities (like reasoning), both DeepSeek-V2 and V3 underwent Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL).\nSupervised Fine-Tuning DeepSeek-V2: Used approximately 1.5M instruction data points, including 1.2M helpfulness data and 0.3M safety data, focusing on improving data quality to reduce hallucinations and enhance writing ability. DeepSeek-V3: Reasoning Data: Utilized the internal DeepSeek-R1 model (Guo et al., 2025) to generate reasoning processes (math, code, logic, etc.). Since R1 outputs could be overly long or poorly formatted, V3 adopted a knowledge distillation approach: Train domain expert models (e.g., code expert): Combined original SFT data with R1-generated long CoT data (with system prompts guiding reflection/verification) for SFT+RL training. Use expert models to generate SFT data: The expert models learned during RL to blend R1’s reasoning patterns with the conciseness of regular SFT data. Rejection sampling: Filtered high-quality SFT data for the final V3 SFT. Non-Reasoning Data: Generated using DeepSeek-V2.5 and verified by human annotators. SFT Setup: Fine-tuned for 2 epochs, with learning rate cosine decayed from \\(5 \\times 10^{-6}\\) to \\(1 \\times 10^{-6}\\). Employed sample packing and mask isolation. Reinforcement Learning Both models used the Group Relative Policy Optimization (GRPO) algorithm (Shao et al., 2024) for RL. GRPO is an Actor-Only method that estimates advantage \\(A_i\\) by comparing the relative quality of a group (\\(G\\)) of candidate outputs. This avoids training a Critic model of the same size as the policy model, saving costs.\nGRPO objective function: \\[ \\begin{gathered} \\mathcal{J}_{G R P O}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_{i}\\right\\}_{i=1}^{G} \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\ \\frac{1}{G} \\sum_{i=1}^{G}\\left(\\min \\left(\\frac{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i} \\mid q\\right)} A_{i}, \\operatorname{clip}\\left(\\frac{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i} \\mid q\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A_{i}\\right)-\\beta \\mathbb{D}_{K L}\\left(\\pi_{\\theta}| | \\pi_{r e f}\\right)\\right), \\end{gathered} \\] where the advantage \\(A_i\\) is obtained by standardizing the intra-group rewards \\(r_i\\): \\[ A_{i}=\\frac{r_{i}-\\operatorname{mean}\\left(\\left\\{r_{1}, r_{2}, \\cdots, r_{G}\\right\\}\\right)}{\\operatorname{std}\\left(\\left\\{r_{1}, r_{2}, \\cdots, r_{G}\\right\\}\\right)}. \\] The KL divergence penalty term uses the Schulman unbiased estimator: \\[ \\mathbb{D}_{K L}\\left(\\pi_{\\theta}| | \\pi_{r e f}\\right)=\\frac{\\pi_{r e f}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}-1. \\]Reward Model (RM):\nDeepSeek-V2: Employed a two-stage RL strategy. Reasoning Alignment: Used a specially trained \\(RM_{\\text{reasoning}}\\) to optimize for code and math reasoning tasks. Human Preference Alignment: Used a multi-reward framework combining \\(RM_{\\text{helpful}}\\), \\(RM_{\\text{safety}}\\), and rule-based \\(RM_{\\text{rule}}\\). DeepSeek-V3: Rule-based RM: For verifiable tasks (e.g., math answer format, LeetCode test cases), used rules to provide reliable rewards. Model-based RM: For free-form answers or tasks without standard answers (e.g., creative writing), used an RM initialized from the V3 SFT Checkpoint. This RM was trained on preference data with CoT to enhance reliability and reduce reward hacking risks. Self-Reward: V3 explored using the model’s own judgment capabilities (enhanced via voting) as a feedback source, especially in general scenarios, combined with ideas from Constitutional AI (Bai et al., 2022) for optimization. RL Training Optimization (V2/V3): Addressed the high resource demands of large-model RL through engineering optimizations like a hybrid engine (different parallelism strategies for training/inference), using vLLM (Kwon et al., 2023) for accelerated sampling, CPU offloading scheduling, etc.\nEvaluation DeepSeek-V2 Chat Evaluation:\nComparison of DeepSeek-V2 Chat (SFT/RL) with representative open-source Chat models on open-ended generation tasks. V2 Chat (RL) performed exceptionally well on AlpacaEval 2.0 and AlignBench.\nModel MT-Bench (Zheng et al., 2023) AlpacaEval 2.0 (Dubois et al., 2024) (LC Win Rate) AlignBench (Liu et al., 2023) (Chinese) DeepSeek 67B Chat 8.35 16.6 6.43 Mistral 8x22B Instruct 8.66 30.9 - Qwen1.5 72B Chat 8.61 36.6 7.19 LLaMA3 70B Instruct 8.95 34.4 - DeepSeek-V2 Chat (SFT) 8.62 30.0 7.74 DeepSeek-V2 Chat (RL) 8.97 38.9 7.91 DeepSeek-V3 Chat Evaluation:\nComparison of DeepSeek-V3 Chat with representative open-source and closed-source Chat models (partial results). V3 leads open-source models on most benchmarks and is comparable to top closed-source models in code, math, Chinese, and open-ended generation tasks.\nBenchmark (Metric) DeepSeek V2.5-0905 Qwen2.5 72B-Inst. LLaMA-3.1 405B-Inst. Claude-3.5- Sonnet-1022 GPT-4o 0513 DeepSeek V3 English MMLU (Hendrycks et al., 2020) (EM) 80.6 85.3 88.6 88.3 87.2 88.5 MMLU-Pro (Wang et al., 2024) (EM) 66.2 71.6 73.3 78.0 72.6 75.9 GPQA-Diamond (Rein et al., 2023) (Pass@1) 41.3 49.0 51.1 65.0 49.9 59.1 SimpleQA (OpenAI, 2024c) (Correct) 10.2 9.1 17.1 28.4 38.2 24.9 Code HumanEval-Mul (Pass@1) 77.4 77.3 77.2 81.7 80.5 82.6 LiveCodeBench (Jain et al., 2024) (Pass@1-COT) 29.2 31.1 28.4 36.3 33.4 40.5 SWE Verified (OpenAI, 2024d) (Resolved) 22.6 23.8 24.5 50.8 38.8 42.0 Math AIME 2024 (MAA, 2024 (Pass@1) 16.7 23.3 23.3 16.0 9.3 39.2 MATH-500 (Hendrycks et al., 2021) (ЕМ) 74.7 80.0 73.8 78.3 74.6 90.2 Chinese C-Eval (Huang et al., 2023) (EM) 79.5 86.1 61.5 76.7 76.0 86.5 C-SimpleQA (He et al., 2024) (Correct) 54.1 48.4 50.4 51.3 59.3 64.8 Open-Ended Arena-Hard (Li et al., 2024) 76.2 81.2 69.3 85.2 80.4 85.5 AlpacaEval 2.0 (Dubois et al., 2024) (LC Win Rate) 50.5 49.1 40.5 52.0 51.1 70.0 Summary:\nDeepSeek-V2 Chat (RL) was already a top-tier open-source chat model at its release, particularly excelling on AlpacaEval and the Chinese AlignBench. DeepSeek-V3 Chat further boosted performance, becoming the current strongest open-source chat model. It shows extremely strong performance in code, math, Chinese knowledge, and open-ended evaluations like Arena-Hard (Li et al., 2024) and AlpacaEval, reaching levels comparable to GPT-4o and Claude-3.5-Sonnet. V3’s R1 distillation significantly improved reasoning capabilities but might also increase response length, requiring a trade-off between accuracy and efficiency. V3’s self-reward capability (strong performance on RewardBench (Lambert et al., 2024)) provides an effective pathway for continuous alignment. Discussion Load Balancing Strategy Evolution: The shift from V2’s auxiliary losses to V3’s auxiliary-loss-free + bias adjustment reflects a trend towards minimizing interference with model performance while ensuring load balance. Batch-level balancing, compared to sequence-level, better facilitates expert specialization. Effectiveness of MTP: V3’s experiments demonstrate that multi-token prediction as an auxiliary training objective indeed improves model performance on standard evaluation tasks, while also offering potential for inference acceleration (speculative decoding). R1 Distillation: V3 successfully distilled the long-chain reasoning capabilities of DeepSeek-R1 into a standard LLM, significantly boosting math and code abilities. This is an important technical direction, though controlling generation length needs attention. Self-Reward: V3’s strong judgment capability (evidenced by RewardBench results (Lambert et al., 2024)) enables effective self-feedback and self-alignment. This is crucial for reducing reliance on human annotation and achieving continuous model self-improvement. SFT Data Quantity: While LIMA (Zhou et al., 2024) suggested that a small amount of high-quality SFT data can achieve good results, sufficient high-quality data is still necessary for specific skills (like instruction following, IFEval) to reach satisfactory performance. Alignment Tax: OpenAI noted in InstructGPT (Ouyang et al., 2022) that RL alignment, while improving open-ended generation capabilities, might sacrifice performance on some standard benchmarks. Both V2 and V3 made efforts in data processing and training strategies to mitigate this issue and achieve an acceptable balance. Conclusion, Limitations \u0026 Future Directions Conclusion DeepSeek-V2 and DeepSeek-V3 are two powerful, economical, and efficient MoE language models. Through innovations like the MLA and DeepSeekMoE architectures, along with V3’s introduction of auxiliary-loss-free load balancing, MTP, FP8 training, and R1 distillation, they have achieved breakthroughs in performance, training cost, and inference efficiency. DeepSeek-V3 has become one of the strongest open-source models currently available, with performance competitive with top closed-source models.\nLimitations General LLM Limitations: Such as knowledge cutoffs, hallucinations, factual errors, etc. Language Coverage: Primarily focused on Chinese and English, with limited capabilities in other languages (V2). V3 expanded multilingual support but remains predominantly focused on Chinese and English. Deployment Threshold (V3): Efficient inference requires relatively large deployment units (multi-node), which might be challenging for smaller teams. Inference Efficiency: Although V3’s inference efficiency improved over V2, there is still room for optimization. Future Directions Architectural Innovation: Continue optimizing MoE architectures, exploring new architectures supporting infinite context and overcoming Transformer limitations. Data Expansion: Improve the quantity, quality, and dimensionality (multimodality, etc.) of training data. Deeper Reasoning: Enhance the model’s reasoning length and depth, increasing intelligence levels. Evaluation Methods: Develop more comprehensive, multi-dimensional evaluation methods to avoid overfitting to specific benchmarks. Alignment and Safety: Continuously improve alignment techniques (e.g., self-reward) to ensure models are helpful, honest, harmless, and aligned with human values. References [1] Liu, Aixin, et al. “Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.” arXiv preprint arXiv:2405.04434 (2024).\n[2] Liu, Aixin, et al. “Deepseek-v3 technical report.” arXiv preprint arXiv:2412.19437 (2024).\n[3] Dai, Damai, et al. “Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.” arXiv preprint arXiv:2401.06066 (2024).\n[4] Wang, Lean, et al. “Auxiliary-loss-free load balancing strategy for mixture-of-experts.” arXiv preprint arXiv:2408.15664 (2024).\n[5] Gloeckle, Fabian, et al. “Better \u0026 faster large language models via multi-token prediction.” Proceedings of the 41st International Conference on Machine Learning. PMLR 235:16821-16841 (2024).\n[6] Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems 30 (2017).\n[7] Shazeer, Noam. “Fast transformer decoding: One write-head is all you need.” arXiv preprint arXiv:1911.02150 (2019).\n[8] Ainslie, Joshua, et al. “GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.” Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4895-4901 (2023).\n[9] Su, Jianlin, et al. “Roformer: Enhanced transformer with rotary position embedding.” Neurocomputing 568 (2024): 127063.\n[10] Shazeer, Noam, et al. “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.” arXiv preprint arXiv:1701.06538 (2017).\n[11] Lepikhin, Dmitry, et al. “Gshard: Scaling giant models with conditional computation and automatic sharding.” arXiv preprint arXiv:2006.16668 (2020).\n[12] Fedus, William, Barret Zoph, and Noam Shazeer. “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.” The Journal of Machine Learning Research 23.1: 5232-5270 (2022).\n[13] Zhou, Zexuan, et al. “Mixture-of-experts with expert choice routing.” Advances in Neural Information Processing Systems 35: 7103-7114 (2022).\n[14] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. “Fast inference from transformers via speculative decoding.” Proceedings of the 40th International Conference on Machine Learning. PMLR 202:19274-19286 (2023).\n[15] Xia, Yichao, et al. “Accelerating large language model decoding with speculative sampling.” arXiv preprint arXiv:2302.01318 (2023).\n[16] Qi, Hai, et al. “ZeroBubble: A High-Performance Framework for Training Mixture-of-Experts Models.” arXiv preprint arXiv:2401.10241 (2024).\n[17] Rajbhandari, Samyam, et al. “Zero: Memory optimizations toward training trillion parameter models.” SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE (2020).\n[18] Harlap, Aaron, et al. “Pipedream: Fast and efficient pipeline parallel dnn training.” arXiv preprint arXiv:1806.03377 (2018).\n[19] Li, Shigang, and Torsten Hoefler. “Chimera: Efficiently training large-scale neural networks with bidirectional pipelines.” Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.\n[20] Bauer, Michael, Sean Treichler, and Alex Aiken. “Singe: Leveraging warp specialization for high performance on gpus.” Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming. 2014.\n[21] Dettmers, Tim, et al. “Llm. int8 (): 8-bit matrix multiplication for transformers at scale.” Advances in Neural Information Processing Systems 35: 34138-34151 (2022).\n[22] Noune, Badreddine, et al. “8-bit numerical formats for deep neural networks.” arXiv preprint arXiv:2206.02915 (2022).\n[23] Peng, Houwen, et al. “FP8-LM: Training FP8 Large Language Models.” arXiv preprint arXiv:2310.18313 (2023).\n[24] Fishman, Maxim, et al. “Scaling FP8 training to trillion-token LLMs.”) arXiv preprint arXiv:2409.12517 (2024).\n[25] He, Bobby, et al. “Understanding and minimising outlier features in neural network training.” arXiv preprint arXiv:2405.19279 (2024).\n[26] Sun, Xiao, et al. “Massive activations in large language models.” arXiv preprint arXiv:2402.17762 (2024).\n[27] NVIDIA. “Transformer Engine.” GitHub Repository (Accessed 2024).\n[28] Sun, Xiao, et al. “Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks.” Advances in neural information processing systems 32 (2019).\n[29] Loshchilov, Ilya, and Frank Hutter. “Decoupled weight decay regularization.” arXiv preprint arXiv:1711.05101 (2017).\n[30] NVIDIA. “GPUDirect Storage: A Direct Path Between Storage and GPU Memory.” NVIDIA Developer Blog (2022).\n[31] Graham, Richard L., et al. “Scalable hierarchical aggregation protocol (SHArP): A hardware architecture for efficient data reduction.” 2016 First International Workshop on Communication Optimizations in HPC (COMHPC). IEEE, 2016.\n[32] Ding, Yiran, et al. “Longrope: Extending llm context window beyond 2 million tokens.” arXiv preprint arXiv:2402.13753 (2024).\n[33] Zhu, Qihao, et al. “DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence.” arXiv preprint arXiv:2406.11931 (2024).\n[34] Lundberg, Scott M. “Guidance: Prompt Boundaries and Token Healing.” GitHub Notebook (2023).\n[35] Peng, Bowen, et al. “YaRN: Efficient Context Window Extension of Large Language Models.” arXiv preprint arXiv:2309.00071 (2023).\n[36] Hendrycks, Dan, et al. “Measuring massive multitask language understanding.” arXiv preprint arXiv:2009.03300 (2020).\n[37] Chen, Mark, et al. “Evaluating large language models trained on code.” arXiv preprint arXiv:2107.03374 (2021).\n[38] Cobbe, Karl, et al. “Training verifiers to solve math word problems.” arXiv preprint arXiv:2110.14168 (2021).\n[39] Huang, Yuzhen, et al. “C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models.” Advances in Neural Information Processing Systems 36 (2023): 62991-63010.\n[40] Wang, Yubo, et al. “Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.” The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024.\n[41] Jain, Naman, et al. “LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code.” arXiv preprint arXiv:2403.07974 (2024).\n[42] Hendrycks, Dan, et al. “Measuring mathematical problem solving with the math dataset.” arXiv preprint arXiv:2103.03874 (2021).\n[43] OpenAI. “MMMLU Dataset.” Hugging Face Datasets (Accessed 2024).\n[44] Guo, Daya, et al. “Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.” arXiv preprint arXiv:2501.12948 (2025).\n[45] Shao, Zhihong, et al. “Deepseekmath: Pushing the limits of mathematical reasoning in open language models.” arXiv preprint arXiv:2402.03300 (2024).\n[46] Bai, Yuntao, et al. “Constitutional ai: Harmlessness from ai feedback.” arXiv preprint arXiv:2212.08073 (2022).\n[47] Kwon, Woosuk, et al. “Efficient memory management for large language model serving with pagedattention.” Proceedings of the 29th Symposium on Operating Systems Principles. 2023.\n[48] Zheng, Lianmin, et al. “Judging llm-as-a-judge with mt-bench and chatbot arena.” Advances in Neural Information Processing Systems 36 (2023): 46595-46623.\n[49] Dubois, Yann, et al. “Length-controlled alpacaeval: A simple way to debias automatic evaluators.” arXiv preprint arXiv:2404.04475 (2024).\n[50] Liu, Xiao, et al. “Alignbench: Benchmarking chinese alignment of large language models.” arXiv preprint arXiv:2311.18743 (2023).\n[51] Rein, David, et al. “GPQA: A Graduate-Level Google-Proof Q\u0026A Benchmark.” First Conference on Language Modeling. 2024.\n[52] OpenAI. “Introducing SimpleQA” OpenAI Blog (2024).\n[53] OpenAI. “Introducing SWE-bench Verified” OpenAI Blog (2024).\n[54] Mathematical Association of America (MAA). “2024 AIME I Problems.” Art of Problem Solving Wiki (2024).\n[55] Li, Tianle, et al. “From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.” arXiv preprint arXiv:2406.11939 (2024).\n[56] Lambert, Nathan, et al. “RewardBench: Evaluating Reward Models for Language Modeling.” arXiv preprint arXiv:2403.13787 (2024).\n[57] Zhou, Chunting, et al. “Lima: Less is more for alignment.” Advances in Neural Information Processing Systems 36 (2023): 55006-55021.\n[58] Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” Advances in neural information processing systems 35 (2022): 27730-27744.\nCitation Citation: When reproducing or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Apr 2025). DeepSeek-V2 vs V3. https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3\nOr\n@article{syhya2025deepseekv2v3, title = \"DeepSeek-V2 vs V3\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Apr\", url = \"https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3\" } ","wordCount":"13242","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-04-18T12:00:00+08:00","dateModified":"2025-09-02T22:00:47+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">DeepSeek-V2 vs V3</h1><div class=post-meta><span title='2025-04-18 12:00:00 +0800 +0800'>Created:&nbsp;2025-04-18</span>&nbsp;·&nbsp;Updated:&nbsp;2025-09-02&nbsp;·&nbsp;63 min&nbsp;·&nbsp;13242 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-04-18-deepseek-v2-v3/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#notations>Notations</a></li><li><a href=#core-architecture>Core Architecture</a><ul><li><a href=#multi-head-latent-attention-mla>Multi-head Latent Attention (MLA)</a><ul><li><a href=#mha-recap>MHA Recap</a></li><li><a href=#low-rank-key-value-joint-compression>Low-Rank Key-Value Joint Compression</a></li><li><a href=#decoupled-rotary-position-embedding>Decoupled Rotary Position Embedding</a></li><li><a href=#matrix-absorption-in-mla-inference>Matrix Absorption in MLA Inference</a></li><li><a href=#kv-cache-comparison>KV Cache Comparison</a></li></ul></li><li><a href=#mixture-of-experts-models>Mixture-of-Experts Models</a></li><li><a href=#core-moe-components>Core MoE Components</a></li><li><a href=#noisy-top-k-gating>Noisy Top-k Gating</a></li><li><a href=#auxiliary-loss>Auxiliary Loss</a></li><li><a href=#gshard>GShard</a></li><li><a href=#switch-transformer>Switch Transformer</a></li><li><a href=#expert-choice>Expert Choice</a></li><li><a href=#deepseekmoe>DeepSeekMoE</a><ul><li><a href=#basic-architecture>Basic Architecture</a></li><li><a href=#v2-vs-v3-gating-mechanism-and-load-balancing-comparison>V2 vs V3 Gating Mechanism and Load Balancing Comparison</a></li><li><a href=#deepseekmoe-v2-vs-v3-comparison-summary-table>DeepSeekMoE V2 vs V3 Comparison Summary Table</a></li></ul></li><li><a href=#multi-token-prediction-mtp>Multi-Token Prediction (MTP)</a><ul><li><a href=#mtp-implementation>MTP Implementation</a></li><li><a href=#mtp-training-objective>MTP Training Objective</a></li><li><a href=#mtp-inference>MTP Inference</a></li></ul></li></ul></li><li><a href=#infrastructure-and-training-efficiency>Infrastructure and Training Efficiency</a><ul><li><a href=#compute-cluster>Compute Cluster</a></li><li><a href=#training-framework>Training Framework</a><ul><li><a href=#dualpipe-and-computation-communication-overlap>DualPipe and Computation-Communication Overlap</a></li><li><a href=#efficient-cross-node-all-to-all-communication-implementation>Efficient Cross-Node All-to-All Communication Implementation</a></li><li><a href=#extreme-memory-optimization-and-minimal-overhead>Extreme Memory Optimization and Minimal Overhead</a></li></ul></li><li><a href=#fp8-training>FP8 Training</a><ul><li><a href=#mixed-precision-framework>Mixed Precision Framework</a></li><li><a href=#precision-enhancement-strategies>Precision Enhancement Strategies</a></li><li><a href=#low-precision-storage-and-communication>Low-Precision Storage and Communication</a></li></ul></li><li><a href=#inference-and-deployment>Inference and Deployment</a><ul><li><a href=#prefilling-stage>Prefilling Stage</a></li><li><a href=#decoding-stage>Decoding Stage</a></li></ul></li><li><a href=#suggestions-for-hardware-design>Suggestions for Hardware Design</a><ul><li><a href=#communication-hardware>Communication Hardware</a></li><li><a href=#compute-hardware>Compute Hardware</a></li></ul></li><li><a href=#training-cost-and-efficiency>Training Cost and Efficiency</a></li></ul></li><li><a href=#pre-training>Pre-training</a><ul><li><a href=#data-construction>Data Construction</a></li><li><a href=#hyperparameters>Hyperparameters</a></li><li><a href=#long-context-extension>Long Context Extension</a></li><li><a href=#evaluation>Evaluation</a></li></ul></li><li><a href=#alignment>Alignment</a><ul><li><a href=#supervised-fine-tuning>Supervised Fine-Tuning</a></li><li><a href=#reinforcement-learning>Reinforcement Learning</a></li><li><a href=#evaluation-1>Evaluation</a></li></ul></li><li><a href=#discussion>Discussion</a></li><li><a href=#conclusion-limitations--future-directions>Conclusion, Limitations & Future Directions</a><ul><li><a href=#conclusion>Conclusion</a></li><li><a href=#limitations>Limitations</a></li><li><a href=#future-directions>Future Directions</a></li></ul></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><p>DeepSeek AI successively released <strong>DeepSeek-V2</strong> (<a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>) and <strong>DeepSeek-V3</strong> (<a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>), two powerful Mixture-of-Experts (MoE) language models that significantly optimize training costs and inference efficiency while maintaining state-of-the-art performance. DeepSeek-V2 has a total of 236B parameters, activating 21B per token, while DeepSeek-V3 further expands to 671B total parameters, activating 37B per token. Both support a 128K context length.</p><p>The core innovations of these two models lie in the adoption of <strong>Multi-head Latent Attention (MLA)</strong> and the <strong>DeepSeekMoE</strong> architecture (<a href=https://arxiv.org/abs/2401.06066>Dai et al., 2024</a>). MLA drastically reduces GPU memory usage during inference by compressing the Key-Value (KV) cache into low-dimensional latent vectors, improving efficiency. DeepSeekMoE achieves stronger expert specialization capabilities and more economical training costs through fine-grained expert segmentation and shared expert isolation. Building upon V2, DeepSeek-V3 further introduces an <strong>Auxiliary-Loss-Free Load Balancing</strong> strategy (<a href=https://arxiv.org/abs/2408.15664>Wang et al., 2024</a>) and the <strong>Multi-Token Prediction (MTP)</strong> (<a href=https://arxiv.org/abs/2404.19737>Gloeckle et al., 2024</a>) training objective, further enhancing model performance and training efficiency.</p><p>DeepSeek-V2 was pre-trained on 8.1T tokens, while DeepSeek-V3 was trained on a larger scale of 14.8T tokens. Both underwent Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages to fully unlock their potential. Evaluation results show that both DeepSeek-V2 and V3 achieved top-tier performance among open-source models across numerous benchmarks. DeepSeek-V3, in particular, has become one of the strongest open-source base models currently available, with performance comparable to top closed-source models.</p><figure class=align-center><img loading=lazy src=deepseek_v2_benchmark.png#center alt="Fig. 1. (a) MMLU accuracy vs. activated parameters, among different open-source models. (b) Training costs and inference efficiency of DeepSeek 67B (Dense) and DeepSeek-V2. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 1. (a) MMLU accuracy vs. activated parameters, among different open-source models. (b) Training costs and inference efficiency of DeepSeek 67B (Dense) and DeepSeek-V2. (Image source: <a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>)</p></figcaption></figure><figure class=align-center><img loading=lazy src=deepseek_v3_benchmark.png#center alt="Fig. 2. Benchmark performance of DeepSeek-V3 and its counterparts. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 2. Benchmark performance of DeepSeek-V3 and its counterparts. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><p>This article will delve into the key technologies of DeepSeek-V2 and DeepSeek-V3, including their innovative model architectures, efficient training infrastructure, pre-training, and alignment processes.</p><h2 id=notations>Notations<a hidden class=anchor aria-hidden=true href=#notations>#</a></h2><p>The following table lists the mathematical notations used in this article to help you read more easily.</p><table><thead><tr><th style=text-align:left>Symbol</th><th style=text-align:left>Meaning</th></tr></thead><tbody><tr><td style=text-align:left>\( d \)</td><td style=text-align:left>Embedding dimension</td></tr><tr><td style=text-align:left>\( n_h \)</td><td style=text-align:left>Number of attention heads</td></tr><tr><td style=text-align:left>\( d_h \)</td><td style=text-align:left>Dimension per attention head</td></tr><tr><td style=text-align:left>\( \mathbf{h}_t \in \mathbb{R}^d \)</td><td style=text-align:left>Input to the attention layer for the \( t \)-th token</td></tr><tr><td style=text-align:left>\( \mathbf{q}_t, \mathbf{k}_t, \mathbf{v}_t \)</td><td style=text-align:left>Query, Key, Value vectors</td></tr><tr><td style=text-align:left>\( W^Q, W^K, W^V, W^O \)</td><td style=text-align:left>Projection matrices for Query, Key, Value, Output</td></tr><tr><td style=text-align:left>\( \mathbf{q}_{t,i}, \mathbf{k}_{t,i}, \mathbf{v}_{t,i} \)</td><td style=text-align:left>Query, Key, Value vectors for the \( i \)-th attention head</td></tr><tr><td style=text-align:left>\( \mathbf{o}_{t,i} \)</td><td style=text-align:left>Output of the \( i \)-th attention head</td></tr><tr><td style=text-align:left>\( \mathbf{u}_t \)</td><td style=text-align:left>Final output of the attention layer</td></tr><tr><td style=text-align:left>\( l \)</td><td style=text-align:left>Number of model layers</td></tr><tr><td style=text-align:left>\( \mathbf{c}_t^{KV} \in \mathbb{R}^{d_c} \)</td><td style=text-align:left>Compressed latent vector for key-value</td></tr><tr><td style=text-align:left>\( d_c \)</td><td style=text-align:left>KV compression dimension</td></tr><tr><td style=text-align:left>\( W^{DKV}, W^{UK}, W^{UV} \)</td><td style=text-align:left>Down-projection matrix for KV, Up-projection matrix for Key, Up-projection matrix for Value</td></tr><tr><td style=text-align:left>\( \mathbf{k}_t^C, \mathbf{v}_t^C \)</td><td style=text-align:left>Key and Value recovered from the latent vector via up-projection</td></tr><tr><td style=text-align:left>\( \mathbf{c}_t^Q \in \mathbb{R}^{d_c'} \)</td><td style=text-align:left>Compressed latent vector for query</td></tr><tr><td style=text-align:left>\( d_c' \)</td><td style=text-align:left>Query compression dimension</td></tr><tr><td style=text-align:left>\( W^{DQ}, W^{UQ} \)</td><td style=text-align:left>Down-projection matrix for Query, Up-projection matrix for Query</td></tr><tr><td style=text-align:left>\( \mathbf{q}_t^C \)</td><td style=text-align:left>Query recovered from the latent vector via up-projection</td></tr><tr><td style=text-align:left>\( \mathbf{q}_{t,i}^R, \mathbf{k}_t^R \)</td><td style=text-align:left>Decoupled RoPE query and key</td></tr><tr><td style=text-align:left>\( d_h^R \)</td><td style=text-align:left>Head dimension for decoupled RoPE query/key</td></tr><tr><td style=text-align:left>\( W^{QR}, W^{KR} \)</td><td style=text-align:left>Generation matrices for decoupled RoPE query/key</td></tr><tr><td style=text-align:left>\( \operatorname{RoPE}(\cdot) \)</td><td style=text-align:left>Operation applying Rotary Position Embedding</td></tr><tr><td style=text-align:left>\( [\cdot ; \cdot] \)</td><td style=text-align:left>Concatenation operation</td></tr><tr><td style=text-align:left>\( n_g \)</td><td style=text-align:left>Number of groups in GQA</td></tr><tr><td style=text-align:left>\( n \)</td><td style=text-align:left>Total number of experts in MoE</td></tr><tr><td style=text-align:left>\( E_i \)</td><td style=text-align:left>The \( i \)-th expert network</td></tr><tr><td style=text-align:left>\( G(\cdot) \)</td><td style=text-align:left>Gating network function</td></tr><tr><td style=text-align:left>\( p_i \)</td><td style=text-align:left>The \( i \)-th probability output by the gating network</td></tr><tr><td style=text-align:left>\( H^{(i)}(x) \)</td><td style=text-align:left>Gating score for expert \( i \) in Noisy Top-k Gating</td></tr><tr><td style=text-align:left>\( W_g, W_{\text{noise}} \)</td><td style=text-align:left>Weight matrices for MoE gating network and noise network</td></tr><tr><td style=text-align:left>\( \epsilon \)</td><td style=text-align:left>Standard Gaussian noise</td></tr><tr><td style=text-align:left>\( \text{softplus}(\cdot) \)</td><td style=text-align:left>Softplus activation function</td></tr><tr><td style=text-align:left>\( k \)</td><td style=text-align:left>Number of experts selected per token in MoE</td></tr><tr><td style=text-align:left>\( \text{topk}(\cdot, k) \)</td><td style=text-align:left>Function selecting the top k largest values</td></tr><tr><td style=text-align:left>\( \mathcal{L}_{\text{aux}} \)</td><td style=text-align:left>MoE auxiliary loss</td></tr><tr><td style=text-align:left>\( w_{\text{aux}} \)</td><td style=text-align:left>Auxiliary loss weight</td></tr><tr><td style=text-align:left>\( \text{CV}(\cdot) \)</td><td style=text-align:left>Coefficient of Variation</td></tr><tr><td style=text-align:left>\( N_s, N_r \)</td><td style=text-align:left>Number of shared and routing experts in DeepSeekMoE</td></tr><tr><td style=text-align:left>\( \operatorname{FFN}_i^{(s)}(\cdot), \operatorname{FFN}_i^{(r)}(\cdot) \)</td><td style=text-align:left>The \( i \)-th shared expert and routing expert function</td></tr><tr><td style=text-align:left>\( K_r \)</td><td style=text-align:left>Number of activated routing experts in DeepSeekMoE</td></tr><tr><td style=text-align:left>\( g_{i,t} \)</td><td style=text-align:left>Gating value of the \( i \)-th expert for the \( t \)-th token</td></tr><tr><td style=text-align:left>\( g_{i,t}' \)</td><td style=text-align:left>Raw gating value after TopK selection (V3)</td></tr><tr><td style=text-align:left>\( s_{i,t} \)</td><td style=text-align:left>Affinity score of the \( t \)-th token for the \( i \)-th expert</td></tr><tr><td style=text-align:left>\( \mathbf{e}_i \)</td><td style=text-align:left>Center vector for the \( i \)-th routing expert</td></tr><tr><td style=text-align:left>\( M \)</td><td style=text-align:left>Device/Node limit for routing</td></tr><tr><td style=text-align:left>\( \mathcal{L}_{\text{ExpBal}}, \mathcal{L}_{\text{DevBal}}, \mathcal{L}_{\text{CommBal}} \)</td><td style=text-align:left>Expert-level, Device-level, Communication-level load balancing losses</td></tr><tr><td style=text-align:left>\( f_i, P_i \)</td><td style=text-align:left>Load score and average affinity for expert \( i \)</td></tr><tr><td style=text-align:left>\( \alpha_1, \alpha_2, \alpha_3 \)</td><td style=text-align:left>Hyperparameters for load balancing losses</td></tr><tr><td style=text-align:left>\( T \)</td><td style=text-align:left>Number of tokens in the sequence</td></tr><tr><td style=text-align:left>\( D \)</td><td style=text-align:left>Number of device/node groups</td></tr><tr><td style=text-align:left>\( \mathcal{E}_i \)</td><td style=text-align:left>Set of experts on the \( i \)-th device/node</td></tr><tr><td style=text-align:left>\( f_i', P_i' \)</td><td style=text-align:left>Average load score and total affinity for device group \( i \)</td></tr><tr><td style=text-align:left>\( f_i'', P_i'' \)</td><td style=text-align:left>Proportion of tokens sent to device \( i \) and total affinity for device group \( i \)</td></tr><tr><td style=text-align:left>\( b_i \)</td><td style=text-align:left>Bias term for the \( i \)-th expert (aux-loss-free balancing)</td></tr><tr><td style=text-align:left>\( \gamma \)</td><td style=text-align:left>Bias term update rate</td></tr><tr><td style=text-align:left>\( \mathcal{L}_{\text{Bal}} \)</td><td style=text-align:left>Sequence-level load balancing loss</td></tr><tr><td style=text-align:left>\( \alpha \)</td><td style=text-align:left>Hyperparameter for sequence-level load balancing loss</td></tr><tr><td style=text-align:left>\( D_{MTP} \)</td><td style=text-align:left>MTP prediction depth</td></tr><tr><td style=text-align:left>\( \operatorname{Emb}(\cdot), \operatorname{OutHead}(\cdot) \)</td><td style=text-align:left>Shared embedding layer and output head (MTP)</td></tr><tr><td style=text-align:left>\( \operatorname{TRM}_k(\cdot) \)</td><td style=text-align:left>Transformer block for the \( k \)-th MTP module</td></tr><tr><td style=text-align:left>\( M_k \)</td><td style=text-align:left>Projection matrix for the \( k \)-th MTP module</td></tr><tr><td style=text-align:left>\( \mathbf{h}_i^k \)</td><td style=text-align:left>Representation of the \( i \)-th token at the \( k \)-th MTP depth</td></tr><tr><td style=text-align:left>\( \mathbf{h}_i^{\prime k} \)</td><td style=text-align:left>Input to the Transformer block of the \( k \)-th MTP module</td></tr><tr><td style=text-align:left>\( P_{i+k+1}^k \)</td><td style=text-align:left>Predicted probability distribution for the \( i+k+1 \)-th token by the \( k \)-th MTP module</td></tr><tr><td style=text-align:left>\( V \)</td><td style=text-align:left>Vocabulary size</td></tr><tr><td style=text-align:left>\( \mathcal{L}_{\text{MTP}}^k \)</td><td style=text-align:left>Cross-entropy loss for the \( k \)-th MTP depth</td></tr><tr><td style=text-align:left>\( \mathcal{L}_{\text{MTP}} \)</td><td style=text-align:left>Total MTP loss</td></tr><tr><td style=text-align:left>\( \lambda \)</td><td style=text-align:left>Weight factor for MTP loss</td></tr><tr><td style=text-align:left>\( \mathcal{J}_{GRPO}(\theta) \)</td><td style=text-align:left>GRPO objective function</td></tr><tr><td style=text-align:left>\( A_i \)</td><td style=text-align:left>Relative advantage value (GRPO)</td></tr><tr><td style=text-align:left>\( \varepsilon \)</td><td style=text-align:left>Clipping hyperparameter in PPO/GRPO</td></tr><tr><td style=text-align:left>\( \beta \)</td><td style=text-align:left>Coefficient for KL divergence penalty term</td></tr><tr><td style=text-align:left>\( \mathbb{D}_{KL}(\pi_\theta \| \pi_{ref}) \)</td><td style=text-align:left>KL divergence</td></tr><tr><td style=text-align:left>\( \pi_\theta, \pi_{\theta_{old}}, \pi_{ref} \)</td><td style=text-align:left>Current policy, old policy, reference policy models</td></tr><tr><td style=text-align:left>\( r_i \)</td><td style=text-align:left>Reward value for the \( i \)-th output</td></tr><tr><td style=text-align:left>\( \mathbb{1}(\cdot) \)</td><td style=text-align:left>Indicator function</td></tr></tbody></table><h2 id=core-architecture>Core Architecture<a hidden class=anchor aria-hidden=true href=#core-architecture>#</a></h2><p>Both DeepSeek-V2 and V3 are based on the Transformer architecture, but employ innovative designs in the attention and feed-forward network (FFN) parts, such as MLA and DeepSeekMoE, to balance performance, training cost, and inference efficiency. The figure below illustrates the architecture of DeepSeek-V2 and V3.</p><figure class=align-center><img loading=lazy src=deepseek_architecture.png#center alt="Fig. 3. Illustration of the architecture of DeepSeek-V2 and DeepSeek-V3. MLA ensures efficient inference by significantly reducing the KV cache for generation, and DeepSeekMoE enables training strong models at an economical cost through the sparse architecture. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 3. Illustration of the architecture of DeepSeek-V2 and DeepSeek-V3. MLA ensures efficient inference by significantly reducing the KV cache for generation, and DeepSeekMoE enables training strong models at an economical cost through the sparse architecture. (Image source: <a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h3 id=multi-head-latent-attention-mla>Multi-head Latent Attention (MLA)<a hidden class=anchor aria-hidden=true href=#multi-head-latent-attention-mla>#</a></h3><p>Traditional Transformer models typically use <strong>Multi-Head Attention (MHA)</strong> (<a href=https://arxiv.org/abs/1706.03762>Vaswani et al., 2017</a>), but during generation, their large KV cache becomes a bottleneck limiting inference efficiency. To address this, researchers proposed <strong>Multi-Query Attention (MQA)</strong> (<a href=https://arxiv.org/abs/1911.02150>Shazeer, 2019</a>) and <strong>Grouped-Query Attention (GQA)</strong> (<a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>). While these methods reduce the KV cache, they often come at the cost of model performance.</p><p>DeepSeek-V2 and V3 adopt the innovative <strong>Multi-head Latent Attention (MLA)</strong> mechanism. The core idea of MLA is <strong>Low-Rank Key-Value Joint Compression</strong>.</p><figure class=align-center><img loading=lazy src=mla.png#center alt="Fig. 4. Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 4. Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. (Image source: <a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h4 id=mha-recap>MHA Recap<a hidden class=anchor aria-hidden=true href=#mha-recap>#</a></h4><p>Standard MHA first transforms the input \(\mathbf{h}_t \in \mathbb{R}^d\) into query \(\mathbf{q}_t\), key \(\mathbf{k}_t\), and value \(\mathbf{v}_t \in \mathbb{R}^{d_h n_h}\) using three projection matrices \(W^Q, W^K, W^V \in \mathbb{R}^{d_h n_h \times d}\):</p>\[
\begin{aligned}
\mathbf{q}_{t} &= W^{Q} \mathbf{h}_{t}, \\
\mathbf{k}_{t} &= W^{K} \mathbf{h}_{t}, \\
\mathbf{v}_{t} &= W^{V} \mathbf{h}_{t}.
\end{aligned}
\]<p>Then, \(\mathbf{q}_t, \mathbf{k}_t, \mathbf{v}_t\) are split into \(n_h\) heads for multi-head attention computation:</p>\[
\begin{aligned}
& [\mathbf{q}_{t, 1} ; \mathbf{q}_{t, 2} ; \ldots ; \mathbf{q}_{t, n_{h}}] = \mathbf{q}_{t}, \\
& [\mathbf{k}_{t, 1} ; \mathbf{k}_{t, 2} ; \ldots ; \mathbf{k}_{t, n_{h}}] = \mathbf{k}_{t}, \\
& [\mathbf{v}_{t, 1} ; \mathbf{v}_{t, 2} ; \ldots ; \mathbf{v}_{t, n_{h}}] = \mathbf{v}_{t}, \\
& \mathbf{o}_{t, i} = \sum_{j=1}^{t} \operatorname{Softmax}_{j}\left(\frac{\mathbf{q}_{t, i}^{T} \mathbf{k}_{j, i}}{\sqrt{d_{h}}}\right) \mathbf{v}_{j, i}, \\
& \mathbf{u}_{t} = W^{O}\left[\mathbf{o}_{t, 1} ; \mathbf{o}_{t, 2} ; \ldots ; \mathbf{o}_{t, n_{h}}\right],
\end{aligned}
\]<p>where \(\mathbf{q}_{t, i}, \mathbf{k}_{t, i}, \mathbf{v}_{t, i} \in \mathbb{R}^{d_h}\) are the query, key, and value for the \(i\)-th head, respectively, and \(W^O \in \mathbb{R}^{d \times d_h n_h}\) is the output projection matrix. During inference, keys and values for all \(t\) steps need to be cached, requiring \(2 n_h d_h l\) elements per token (\(l\) being the number of layers), which constitutes a huge KV cache overhead.</p><h4 id=low-rank-key-value-joint-compression>Low-Rank Key-Value Joint Compression<a hidden class=anchor aria-hidden=true href=#low-rank-key-value-joint-compression>#</a></h4><p>MLA introduces a low-dimensional latent vector \(\mathbf{c}_t^{KV} \in \mathbb{R}^{d_c}\) to jointly compress keys and values, where \(d_c \ll d_h n_h\):</p>\[
\begin{aligned}
\boxed{\mathbf{c}_{t}^{K V}} &= W^{D K V} \mathbf{h}_{t}, \\
\mathbf{k}_{t}^{C} &= W^{U K} \mathbf{c}_{t}^{K V}, \\
\mathbf{v}_{t}^{C} &= W^{U V} \mathbf{c}_{t}^{K V}.
\end{aligned}
\]<p>Here, \(W^{DKV} \in \mathbb{R}^{d_c \times d}\) is the down-projection matrix, and \(W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}\) are the up-projection matrices for keys and values, respectively. During inference, MLA <strong>only needs to cache the compressed latent vector \(\mathbf{c}_t^{KV}\)</strong> (and the decoupled RoPE key \(\mathbf{k}_t^R\) mentioned later), greatly reducing the KV cache size.</p><p>To reduce activation memory during training, MLA also applies similar low-rank compression to the query:</p>\[
\begin{aligned}
\mathbf{c}_{t}^{Q} &= W^{D Q} \mathbf{h}_{t}, \\
\mathbf{q}_{t}^{C} &= W^{U Q} \mathbf{c}_{t}^{Q},
\end{aligned}
\]<p>where \(\mathbf{c}_t^Q \in \mathbb{R}^{d_c'}\) is the compressed latent vector for the query, \(d_c' \ll d_h n_h\), and \(W^{DQ} \in \mathbb{R}^{d_c' \times d}\) and \(W^{UQ} \in \mathbb{R}^{d_h n_h \times d_c'}\) are the down-projection and up-projection matrices for the query, respectively.</p><h4 id=decoupled-rotary-position-embedding>Decoupled Rotary Position Embedding<a hidden class=anchor aria-hidden=true href=#decoupled-rotary-position-embedding>#</a></h4><p>Standard <strong>Rotary Position Embedding (RoPE)</strong> (<a href=https://arxiv.org/abs/2104.09864>Su et al., 2024</a>) is applied directly to keys and queries, but this is incompatible with MLA&rsquo;s low-rank KV compression. If RoPE were applied to the compressed key \(\mathbf{k}_t^C\), the up-projection matrix \(W^{UK}\) would couple with the position-dependent RoPE matrix. This would prevent absorbing \(W^{UK}\) into \(W^Q\) during inference, requiring recomputation of keys for all prefix tokens, severely impacting efficiency.</p><p>To solve this, MLA proposes the <strong>Decoupled RoPE</strong> strategy. It introduces an additional multi-head query \(\mathbf{q}_{t, i}^R \in \mathbb{R}^{d_h^R}\) and a shared key \(\mathbf{k}_t^R \in \mathbb{R}^{d_h^R}\) specifically to carry the RoPE information:</p>\[
\begin{aligned}
\left[\mathbf{q}_{t,1}^R;\,\mathbf{q}_{t,2}^R;\,\dots;\,\mathbf{q}_{t,n_h}^R\right]
= \mathbf{q}_t^R
&= \operatorname{RoPE}\bigl(W^{Q R}\,\mathbf{c}_t^Q\bigr),\\
\boxed{\mathbf{k}_t^R}
&= \operatorname{RoPE}\bigl(W^{K R}\,\mathbf{h}_t\bigr).
\end{aligned}
\]<p>Here, \(W^{QR} \in \mathbb{R}^{d_h^R n_h \times d_c'}\) and \(W^{KR} \in \mathbb{R}^{d_h^R \times d}\) are matrices generating the decoupled query and key.
The compressed key/query parts (\(C\)) are then concatenated with the decoupled RoPE parts (\(R\)) to form the final keys and queries:</p>\[
\begin{aligned}
\mathbf{q}_{t, i} &= [\mathbf{q}_{t, i}^{C} ; \mathbf{q}_{t, i}^{R}], \\
\mathbf{k}_{t, i} &= [\mathbf{k}_{t, i}^{C} ; \mathbf{k}_{t}^{R}].
\end{aligned}
\]<p>The final attention computation becomes:</p>\[
\begin{aligned}
\mathbf{o}_{t, i} &= \sum_{j=1}^{t} \operatorname{Softmax}_{j}\left(\frac{\mathbf{q}_{t, i}^{T} \mathbf{k}_{j, i}}{\sqrt{d_{h}+d_{h}^{R}}}\right) \mathbf{v}_{j, i}^{C}, \\
\mathbf{u}_{t} &= W^{O}\left[\mathbf{o}_{t, 1} ; \mathbf{o}_{t, 2} ; \ldots ; \mathbf{o}_{t, n_{h}}\right].
\end{aligned}
\]<p>During inference, besides caching \(\mathbf{c}_t^{KV}\), the decoupled RoPE key \(\mathbf{k}_t^R\) also needs to be cached. Therefore, DeepSeek-V2/V3 require caching a total of \((d_c + d_h^R)l\) elements per token.</p><h4 id=matrix-absorption-in-mla-inference>Matrix Absorption in MLA Inference<a hidden class=anchor aria-hidden=true href=#matrix-absorption-in-mla-inference>#</a></h4><p>A key advantage of MLA is the improvement in inference efficiency, partly due to the associative property of matrix multiplication allowing the up-projection matrices \(W^{UK}\) and \(W^{UV}\) to be &ldquo;absorbed,&rdquo; avoiding the explicit computation of the full keys \(\mathbf{k}_t^C\) and values \(\mathbf{v}_t^C\).</p><p><strong>1. Absorbing \(W^{UK}\) (Optimizing Attention Score Calculation):</strong></p><p>The core of attention score calculation is the dot product of query and key \(\mathbf{q}_{t,i}^T \mathbf{k}_{j,i}\). Focusing on the \(C\) part generated from the compressed vector:</p>\[
(\mathbf{q}_{t,i}^C)^T \mathbf{k}_{j,i}^C
\]<p>Substitute \(\mathbf{k}_{j,i}^C = W^{UK} \mathbf{c}_j^{KV}\):</p>\[
(\mathbf{q}_{t,i}^C)^T (W^{UK} \mathbf{c}_j^{KV})
\]<p>Using matrix multiplication associativity \((AB)C = A(BC)\) and transpose property \((AB)^T = B^T A^T\), the expression can be rewritten as:</p>\[
(\mathbf{q}_{t,i}^C)^T (W^{UK} \mathbf{c}_j^{KV}) = ((W^{UK})^T \mathbf{q}_{t,i}^C)^T \mathbf{c}_j^{KV}
\]<p>The significance of this transformation is: we no longer need to apply \(W^{UK}\) to the cached \(\mathbf{c}_j^{KV}\) to get \(\mathbf{k}_{j,i}^C\). Instead, we can first compute an &ldquo;effective query&rdquo; \(\tilde{\mathbf{q}}_{t,i}^C = (W^{UK})^T \mathbf{q}_{t,i}^C\), and then directly compute the dot product of this effective query with the cached latent vector \(\mathbf{c}_j^{KV}\).</p><p>The original query \(\mathbf{q}_{t,i}^C\) is computed from \(\mathbf{h}_t\) via \(W^{UQ}\) and \(W^{DQ}\) (\(\mathbf{q}_{t,i}^C = (W^{UQ} W^{DQ} \mathbf{h}_t)_i\)). Thus, the entire computation from \(\mathbf{h}_t\) to the effective query \(\tilde{\mathbf{q}}_{t,i}^C\) can be viewed as a new, effective query projection operation that incorporates \(W^{UK}\). In practice, this means after computing \(\mathbf{q}_{t,i}^C\), one can left-multiply by \((W^{UK})^T\), or more efficiently, merge \((W^{UK})^T\) into the original query generation matrix \(W^Q\) (or \(W^{UQ}W^{DQ}\)) to form a new query projection matrix \(\tilde{W}^Q = (W^{UK})^T W^{UQ} W^{DQ}\).</p><p>Crucially, the computation involving \(W^{UK}\) is moved to the query side and performed once before calculating attention scores, eliminating the need to recover \(\mathbf{k}_{j,i}^C\) from the cached \(\mathbf{c}_j^{KV}\) using \(W^{UK}\) for every query.</p><p><strong>2. Absorbing \(W^{UV}\) (Optimizing Weighted Sum):</strong></p><p>The output of an attention head \(\mathbf{o}_{t,i}\) is the weighted sum of attention weights (denoted \(w_{ij}\)) and values \(\mathbf{v}_{j,i}^C\):</p>\[
\mathbf{o}_{t, i} = \sum_{j=1}^{t} w_{ij} \cdot \mathbf{v}_{j, i}^{C}
\]<p>Substitute \(\mathbf{v}_{j,i}^C = (W^{UV} \mathbf{c}_j^{KV})_i\) (where \((\cdot)_i\) denotes the part belonging to the \(i\)-th head):</p>\[
\mathbf{o}_{t, i} = \sum_{j=1}^{t} w_{ij} \cdot (W^{UV} \mathbf{c}_j^{KV})_i
\]<p>The final attention layer output \(\mathbf{u}_t\) is obtained by concatenating the outputs of all heads \(\mathbf{o}_{t,i}\) and projecting through the output matrix \(W^O\):</p>\[
\mathbf{u}_{t} = W^{O}\left[\mathbf{o}_{t, 1} ; \ldots ; \mathbf{o}_{t, n_{h}}\right] = W^{O} \begin{bmatrix} \sum_{j} w_{1j} (W^{UV} \mathbf{c}_j^{KV})_1 \\ \vdots \\ \sum_{j} w_{n_h j} (W^{UV} \mathbf{c}_j^{KV})_{n_h} \end{bmatrix}
\]<p>Due to the linearity of matrix multiplication (\(A(B+C) = AB + AC\) and \(A(cB) = c(AB)\)), \(W^{UV}\) can be &ldquo;factored out&rdquo; of the summation (this is for intuitive understanding; the actual operation is at the matrix level):</p>\[
\mathbf{u}_{t} \approx W^{O} W^{UV} \left( \sum_{j=1}^{t} \begin{bmatrix} w_{1j} (\mathbf{c}_j^{KV})_1 \\ \vdots \\ w_{n_h j} (\mathbf{c}_j^{KV})_{n_h} \end{bmatrix} \right)
\]<p>(Note: \((\mathbf{c}_j^{KV})_i\) here is illustrative; in practice, operations are performed directly on the complete \(\mathbf{c}_j^{KV}\), but the principle is the same: first perform the weighted sum on \(\mathbf{c}_j^{KV}\), then apply \(W^{UV}\) and \(W^O\)).</p><p>Let the effective output matrix be \(\tilde{W}^O = W^O W^{UV}\). This means we can first compute the weighted sum of attention weights and the latent vectors \(\mathbf{c}_j^{KV}\) (yielding an intermediate result \(\tilde{\mathbf{o}}_t = \sum_j w_{ij} \mathbf{c}_j^{KV}\) of dimension \(d_c\)), and then directly use this merged effective output matrix \(\tilde{W}^O\) for the final projection to get \(\mathbf{u}_t\). Similarly, the computation involving \(W^{UV}\) is merged into the final output projection step, eliminating the need to recover \(\mathbf{v}_{j,i}^C\) from \(\mathbf{c}_j^{KV}\) during the weighted sum calculation.</p><p><strong>Summary:</strong> Through matrix absorption, MLA avoids repeatedly computing the high-dimensional keys \(\mathbf{k}_{j,i}^C\) and values \(\mathbf{v}_{j,i}^C\) from the cached low-dimensional latent vectors \(\mathbf{c}_j^{KV}\) during inference, significantly improving computational efficiency. Only \(\mathbf{c}_t^{KV}\) and \(\mathbf{k}_t^R\) are actually cached.</p><h4 id=kv-cache-comparison>KV Cache Comparison<a hidden class=anchor aria-hidden=true href=#kv-cache-comparison>#</a></h4><p>The table below compares the per-token KV cache size for different attention mechanisms. \(n_h\) is the number of attention heads, \(d_h\) is the dimension per head, \(l\) is the number of layers, \(n_g\) is the number of GQA groups, and \(d_c\) and \(d_h^R\) are MLA&rsquo;s KV compression dimension and decoupled RoPE dimension. For DeepSeek-V2, \(d_c = 4d_h\), \(d_h^R = d_h/2\), making its KV cache equivalent to GQA with \(n_g=2.25\), but with performance superior to MHA. DeepSeek-V3 uses a similar configuration.</p><table><thead><tr><th style=text-align:left>Attention Mechanism</th><th style=text-align:center>Per-Token KV Cache Size (# elements)</th><th style=text-align:center>Capability</th></tr></thead><tbody><tr><td style=text-align:left>Multi-Head Attention (MHA)</td><td style=text-align:center>\(2 n_{h} d_{h} l\)</td><td style=text-align:center>Strong</td></tr><tr><td style=text-align:left>Grouped-Query Attention (GQA)</td><td style=text-align:center>\(2 n_{g} d_{h} l\)</td><td style=text-align:center>Medium</td></tr><tr><td style=text-align:left>Multi-Query Attention (MQA)</td><td style=text-align:center>\(2 d_{h} l\)</td><td style=text-align:center>Weak</td></tr><tr><td style=text-align:left>Multi-head Latent Attention (MLA)</td><td style=text-align:center>\(\bigl(d_{c} + d_{h}^{R}\bigr) l \approx \tfrac{9}{2} \, d_{h} \, l\)</td><td style=text-align:center>Stronger</td></tr></tbody></table><p>The figure below shows that MLA not only significantly reduces the KV cache but also achieves performance superior to standard MHA.</p><figure class=align-center><img loading=lazy src=mla_vs_mha.png#center alt="Fig. 5. Comparison between MLA and MHA on hard benchmarks. DeepSeek-V2 shows better performance than MHA, but requires a significantly smaller amount of KV cache. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 5. Comparison between MLA and MHA on hard benchmarks. DeepSeek-V2 shows better performance than MHA, but requires a significantly smaller amount of KV cache. (Image source: <a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h3 id=mixture-of-experts-models>Mixture-of-Experts Models<a hidden class=anchor aria-hidden=true href=#mixture-of-experts-models>#</a></h3><p>Before diving into DeepSeekMoE, let&rsquo;s review the basics of Mixture-of-Experts (MoE) models.</p><p><strong>Mixture-of-Experts (MoE)</strong> (<a href=https://arxiv.org/abs/1701.06538>Shazeer et al. 2017</a>) is a sparsely activated model that significantly increases model parameter count and performance without substantially increasing computational cost by combining multiple independent &ldquo;expert&rdquo; networks and a gating network. The core idea of MoE is <strong>Sparse Activation</strong>, meaning that for each input sample, only a subset of expert networks is activated, rather than the entire model. This approach enhances both computational efficiency and the model&rsquo;s expressive power, leading to excellent performance in LLMs.</p><p>MoE design is inspired by <a href=https://en.wikipedia.org/wiki/Ensemble_learning>Ensemble learning</a>, a technique that decomposes complex tasks into multiple subtasks handled collaboratively by different models. In MoE, these &ldquo;subtasks&rdquo; are processed by multiple independent expert networks, while a gating network dynamically selects the most suitable experts based on the input sample&rsquo;s features. This division of labor resembles expert teams in human society: specialists from different fields provide expertise on specific problems, and their insights are combined to reach a final result.</p><figure class=align-center><img loading=lazy src=moe.png#center alt="Fig. 6. Illustration of a mixture-of-experts (MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)" width=100%><figcaption><p>Fig. 6. Illustration of a mixture-of-experts (MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: <a href=https://arxiv.org/abs/1701.06538>Shazeer et al. 2017</a>)</p></figcaption></figure><h3 id=core-moe-components>Core MoE Components<a hidden class=anchor aria-hidden=true href=#core-moe-components>#</a></h3><p>A typical MoE layer includes the following components:</p><ul><li><strong>Experts:</strong> A set of independent neural networks ${E_1, E_2, &mldr;, E_n}$. Each expert network $E_i$ can be any type of neural network, such as an FFN, CNN, RNN, etc. The number of experts $n$ can be large, e.g., tens, hundreds, or even thousands.</li><li><strong>Gating Network:</strong> A trainable neural network $G$ that learns a probability distribution based on the input sample $x$ to decide which experts to activate. The gating network takes the input sample $x$ and outputs an $n$-dimensional probability vector $p = G(x) = [p_1, p_2, &mldr;, p_n]$, where $p_i$ represents the probability of activating expert $E_i$.</li><li><strong>Expert Output Aggregation:</strong> Based on the probability distribution from the gating network, the outputs of the activated expert networks are weighted and summed to produce the final output $y$ of the MoE layer.</li></ul><h3 id=noisy-top-k-gating>Noisy Top-k Gating<a hidden class=anchor aria-hidden=true href=#noisy-top-k-gating>#</a></h3><p>To achieve sparse activation and ensure balanced expert utilization, MoE typically employs <strong>Noisy Top-k Gating</strong> as the gating mechanism. This method introduces noise and top-k selection to ensure computational efficiency while preventing uneven expert load. Here&rsquo;s the detailed workflow:</p><ol><li><p><strong>Gating Score Calculation:</strong></p><p>For an input sample $x$, the gating network first computes a gating score $H^{(i)}(x)$ for each expert. This score consists of a linear transformation and a noise term, formulated as:</p>$$
H^{(i)}(x) =(x W_g)^{(i)} + \epsilon \cdot \text{softplus}\left((x W_{\text{noise}})^{(i)} \right), \quad \epsilon \sim \mathcal{N}(0, 1)
$$<ul><li><strong>Parameters:</strong><ul><li>$W_g \in \mathbb{R}^{d \times n}$: Trainable weight matrix of the gating network, where $d$ is the input feature dimension and $n$ is the number of experts.</li><li>$W_{\text{noise}} \in \mathbb{R}^{d \times n}$: Weight matrix used to generate noise.</li><li>$\epsilon \sim \mathcal{N}(0, 1)$: Standard Gaussian noise, adding randomness to the gating.</li><li>$\text{softplus}(x) = \log(1 + e^x)$: Smooth activation function ensuring non-negative noise.</li></ul></li></ul><p>The introduction of noise prevents the gating network from always selecting the same experts, enhancing the model&rsquo;s robustness and diversity.</p></li><li><p><strong>Top-k Selection:</strong></p><p>After computing the gating score vector $H(x) = [H^{(1)}(x), H^{(2)}(x), \dots, H^{(n)}(x)]$, the gating network selects the top $k$ experts with the highest scores (usually $k \ll n$). This step is implemented using the $\text{topk}(v, k)$ function:</p>$$
\text{topk}^{(i)}(v, k) =
\begin{cases}
v^{(i)} & \text{if } v^{(i)} \text{ is in the top } k \text{ elements of } v \\
-\infty & \text{otherwise}
\end{cases}
$$<p>Setting the scores of non-top-k experts to $-\infty$ ensures their probabilities become 0 after the subsequent softmax operation, achieving sparsity.</p></li><li><p><strong>Softmax Normalization:</strong></p><p>The gating scores of the top-k experts are normalized using softmax to obtain a sparse probability distribution $G(x)$:</p>$$
G(x) = \text{softmax}\left( \text{topk}(H(x), k) \right)
$$<p>Only the top-k experts have non-zero probabilities; the rest are 0. For example, if $n=100, k=2$, then 98 experts will have a probability of 0.</p></li><li><p><strong>Weighted Sum:</strong></p><p>The outputs of the top-k experts are weighted by their probabilities and summed to get the MoE layer&rsquo;s output:</p>$$
y = \sum_{i=1}^{n} G^{(i)}(x) E_i(x)
$$<p>Since only $k$ experts are activated, the computational load is much lower than activating all $n$ experts.</p></li></ol><h3 id=auxiliary-loss>Auxiliary Loss<a hidden class=anchor aria-hidden=true href=#auxiliary-loss>#</a></h3><p>To <strong>prevent the gating network from overly favoring a few experts</strong>, MoE introduces an <strong>Auxiliary Loss</strong> (<a href=https://arxiv.org/abs/1701.06538>Shazeer et al. 2017</a>) to encourage uniform usage of all experts. A common method is based on the square of the <a href=https://en.wikipedia.org/wiki/Coefficient_of_variation>Coefficient of Variation (CV)</a> of expert usage:</p>$$
\mathcal{L}_{\text{aux}} = w_{\text{aux}} \cdot \text{CV}\left( \sum_{x \in X} G(x) \right)^2
$$<ul><li><p><strong>Parameters:</strong></p><ul><li>$X$: A mini-batch of input samples.</li><li>$\sum_{x \in X} G(x)$: Counts the number of times each expert is activated within the mini-batch.</li><li>$\text{CV}$: The ratio of the standard deviation to the mean, measuring the uniformity of expert usage distribution.</li><li>$w_{\text{aux}}$: Weight of the auxiliary loss, needs manual tuning.</li></ul></li><li><p><strong>Purpose:</strong> By minimizing $\mathcal{L}_{\text{aux}}$, the model optimizes the balance of expert selection, preventing some experts from being overused while others remain idle.</p></li></ul><h3 id=gshard>GShard<a hidden class=anchor aria-hidden=true href=#gshard>#</a></h3><p><strong>GShard</strong> (<a href=https://arxiv.org/abs/2006.16668>Lepikhin et al. 2020</a>) primarily focuses on sharding the MoE layer, distributing the expert networks ${E_1, E_2, &mldr;, E_n}$ across multiple TPU devices. For instance, with $P$ TPU devices, the experts can be divided into $P$ groups, each assigned to one TPU device. Other layers of the Transformer model (e.g., self-attention, LayerNorm) are replicated across all TPU devices.</p><p><strong>GShard&rsquo;s Improved Gating Mechanism:</strong></p><p>GShard builds upon Noisy Top-k Gating with several improvements to enhance performance and stability:</p><ul><li><p><strong>Expert Capacity:</strong>
To prevent expert overload, GShard introduces expert capacity limits. Each expert network has a maximum capacity, indicating the maximum number of tokens it can process. If a token is routed to an expert that has reached its capacity limit, the token is marked as &ldquo;overflowed,&rdquo; and its gating output is set to a zero vector, meaning it won&rsquo;t be routed to any expert.</p></li><li><p><strong>Local Group Dispatching:</strong>
To improve gating efficiency, GShard groups tokens and enforces expert capacity limits at the group level. For example, tokens in a mini-batch are divided into multiple local groups, each containing a certain number of tokens. The gating network selects top-k experts for each local group, ensuring that the number of tokens processed by each expert within a group does not exceed its capacity limit.</p></li><li><p><strong>Auxiliary Loss:</strong>
GShard also uses an auxiliary loss function to balance expert load. Unlike the original MoE model&rsquo;s auxiliary loss, GShard&rsquo;s loss aims to minimize the mean squared error of the proportion of data routed to each expert, more directly measuring expert load balance.</p></li><li><p><strong>Random Routing:</strong>
To increase routing randomness, GShard introduces a random routing mechanism when selecting the top-k experts. Besides selecting the best top-k experts, GShard also randomly selects sub-optimal experts with a certain probability, increasing expert diversity and improving the model&rsquo;s generalization ability.</p></li></ul><p>Below is the core algorithm flow of GShard:</p><figure class=align-center><img loading=lazy src=gshard.png#center alt="Fig. 7. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)" width=100%><figcaption><p>Fig. 7. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: <a href=https://arxiv.org/abs/2006.16668>Lepikhin et al. 2020</a>)</p></figcaption></figure><h3 id=switch-transformer>Switch Transformer<a hidden class=anchor aria-hidden=true href=#switch-transformer>#</a></h3><p><strong>Switch Transformer</strong> (<a href=https://arxiv.org/pdf/2101.03961>Fedus et al. 2021</a>) is a <strong>trillion-parameter</strong> MoE model proposed by Google. Its core innovation is replacing the dense feed-forward network (FFN) layers in the Transformer model with sparse Switch FFN layers. Unlike GShard&rsquo;s Top-2 Gating, Switch Transformer routes each input token to only one expert network, achieving higher sparsity and further reducing computational costs, making it possible to train trillion-parameter models. It encourages more balanced token routing among the $N$ experts. Switch Transformer&rsquo;s auxiliary loss is based on the product sum of the actual routing fraction and the predicted routing probability, formulated as:</p>$$
\text{loss} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i
$$<ul><li><strong>Parameters:</strong><ul><li>$N$: Total number of experts.</li><li>$f_i$: Fraction of tokens routed to the $i$-th expert, defined as:
$$
f_i = \frac{1}{T} \sum_{x \in B} \mathbb{1}\{\text{argmax } p(x) = i\}
$$</li><li>$P_i$: Routing probability for the $i$-th expert predicted by the gating network, defined as:
$$
P_i = \frac{1}{T} \sum_{x \in B} p_i(x)
$$</li><li>$T$: Total number of tokens in batch $B$.</li><li>$\alpha$: Weight hyperparameter for the auxiliary loss, typically set to $10^{-2}$.</li></ul></li></ul><p>By minimizing this loss, the model encourages the actual routing fraction $f_i$ to align with the predicted probability $P_i$, indirectly promoting load balance among experts and preventing some from being idle.</p><figure class=align-center><img loading=lazy src=switch_transformer.png#center alt="Fig. 8. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)" width=100%><figcaption><p>Fig. 8. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: <a href=https://arxiv.org/abs/2101.03961>Fedus et al. 2021</a>)</p></figcaption></figure><p><strong>Switch Router Mechanism:</strong></p><ol><li><p><strong>Routing Prediction:</strong>
For an input token $x$, the Switch Router predicts the routing probability $p_i = G^{(i)}(x)$ for each expert network, where $i = 1, 2, &mldr;, n$, and n is the number of expert networks.</p></li><li><p><strong>Expert Selection:</strong>
Select the expert network with the highest routing probability as the best expert. Switch Transformer uses a Top-1 routing strategy, meaning each token is routed only to the expert with the highest probability.</p></li><li><p><strong>Token Routing:</strong>
Route the input token $x$ to the selected best expert network for processing.</p></li></ol><p><strong>Switch Transformer Training Stability Optimizations:</strong></p><p>To improve the training stability of Switch Transformer, the paper proposes the following optimization strategies:</p><ul><li><p><strong>Selective Precision:</strong>
Using FP32 precision inside the router function improves training stability without the overhead of FP32 tensor communication. Specifically, the Switch Router computations are performed entirely in FP32, and the final result is converted back to FP16 to balance efficiency and precision.</p></li><li><p><strong>Smaller Initialization:</strong>
It is recommended to adjust the Transformer weight initialization scale parameter $s$ from 1.0 to 0.1. A smaller initialization scale helps mitigate the risk of gradient explosion early in training, thereby improving overall training stability. This is implemented by sampling from a truncated normal distribution with mean 0 and standard deviation $\sqrt{s/n}$ (where $n$ is the number of input units).</p></li><li><p><strong>Higher Expert Dropout:</strong>
Using a higher dropout rate (e.g., 0.4) in the expert FFN layers while maintaining a lower dropout rate (e.g., 0.1) in non-expert layers effectively prevents overfitting and enhances the model&rsquo;s generalization ability. The experimental results in the figure below show that the model performs best on tasks like GLUE, CNNDM, SQuAD, and SuperGLUE when the expert layer dropout rate is set to 0.4.</p></li></ul><figure class=align-center><img loading=lazy src=switch_transformer_fine_tuning_result.png#center alt="Fig. 9. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set (higher numbers are better). (Image source: Fedus et al. 2021)" width=100%><figcaption><p>Fig. 9. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set (higher numbers are better). (Image source: <a href=https://arxiv.org/abs/2101.03961>Fedus et al. 2021</a>)</p></figcaption></figure><p>The Switch Transformers paper uses the following figure to intuitively illustrate how different parallelism techniques partition model weights and data:</p><figure class=align-center><img loading=lazy src=switch_transformer_parallelism.png#center alt="Fig. 10. An illustration of various parallelism strategies on how (Top) model weights and (Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)" width=100%><figcaption><p>Fig. 10. An illustration of various parallelism strategies on how (Top) model weights and (Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: <a href=https://arxiv.org/abs/2101.03961>Fedus et al. 2021</a>)</p></figcaption></figure><h3 id=expert-choice>Expert Choice<a hidden class=anchor aria-hidden=true href=#expert-choice>#</a></h3><p><strong>Expert Choice (EC)</strong> (<a href=https://arxiv.org/abs/2202.09368>Zhou et al. 2022</a>) is a routing strategy opposite to token choice routing (like GShard&rsquo;s top-2 or Switch Transformer&rsquo;s top-1). In token choice routing, each token selects top-k experts from all available experts. In expert choice routing, each expert selects top-k tokens from all available tokens to process. This approach aims to address the load imbalance and token dropping issues of token choice routing while significantly improving training efficiency. Here is the specific computation process:</p><ol><li><p><strong>Compute token-to-expert affinity scores:</strong></p><p>For an input matrix $X \in \mathbb{R}^{n \times d}$, the token-to-expert affinity score matrix $S \in \mathbb{R}^{n \times e}$ is computed as:</p>$$
S = \text{softmax}(X \cdot W_g), \quad \text{where } W_g \in \mathbb{R}^{d \times e}.
$$<p>Here, $W_g$ is the gating weight matrix, and $e$ is the number of experts.</p></li><li><p><strong>Experts select tokens:</strong></p><p>Each expert selects the top-k tokens from all tokens to process. This is done by performing top-k selection on $S^T$:</p>$$
G, I = \text{top-}k(S^T, k),
$$<p>This yields:</p><ul><li><strong>Gating matrix $G \in \mathbb{R}^{e \times k}$:</strong> Records the routing weights corresponding to the tokens selected by the experts, where $G[i, j]$ is the weight for the $j$-th token selected by expert $i$.</li><li><strong>Token index matrix $I \in \mathbb{R}^{e \times k}$:</strong> Indicates the indices of the tokens selected by each expert in the input.</li></ul></li><li><p><strong>One-hot encoding:</strong></p><p>Convert the token index matrix $I$ into a one-hot encoded matrix $P \in \mathbb{R}^{e \times k \times n}$ for subsequent calculations:</p>$$
P = \operatorname{one}-\operatorname{hot}(I)
$$</li><li><p><strong>Construct input for Gated FFN layer:</strong></p><p>For each expert $i$, the input to its gated FFN layer is:</p>$$
(P \cdot X) \in \mathbb{R}^{e \times k \times d}.
$$</li></ol><p>EC controls model sparsity by regularizing the number of experts each token is routed to. A common regularization objective is:</p>$$
\begin{aligned}
& \max_{A} \langle S^{\top}, A \rangle + \lambda H(A) \\
& \text{s.t. } \forall i: \sum_{j'} A[i, j'] = k, \quad \forall j: \sum_{i'} A[i', j] \leq b, \quad \forall i,j: 0 \leq A[i, j] \leq 1,
\end{aligned}
$$<p>The optimization problem defines a matrix $A$ where the element at row $i$, column $j$ indicates whether expert $i$ selected token $j$ (value 0 or 1). Since solving this optimization problem is complex, the paper uses <a href=https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm>Dijkstra&rsquo;s algorithm</a> (obtaining an approximate solution through multiple iterations).</p><p>The parameter $b$ is typically determined by the total number of tokens $n$ in the batch and a capacity factor, which represents the average number of experts used per token. Most experiments use a high capacity factor. Experimental results show that even with reduced capacity, EC generally outperforms traditional top-1 token choice routing, although capped expert choice slightly degrades fine-tuning performance.</p><p>The advantages of EC are mainly twofold:</p><ul><li><strong>Perfect Load Balancing:</strong> Each expert processes a fixed $k$ tokens, avoiding the issue of some experts being overloaded while others are idle, achieving ideal load balance.</li><li><strong>Higher Training Efficiency:</strong> Experiments show that EC can improve training convergence speed by about 2x, making it more efficient than traditional token choice routing.</li></ul><p>However, EC also has limitations:</p><ul><li><strong>Batch Size Requirement:</strong> EC requires a relatively large batch size, making it unsuitable for scenarios with smaller batch sizes.</li><li><strong>Autoregressive Generation Limitation:</strong> In autoregressive text generation tasks, EC&rsquo;s top-k selection cannot be implemented because future tokens are unknown, making it unsuitable for such tasks.</li></ul><h3 id=deepseekmoe>DeepSeekMoE<a hidden class=anchor aria-hidden=true href=#deepseekmoe>#</a></h3><p>Mixture-of-Experts (MoE) models enhance efficiency and performance by routing computation to specific &ldquo;expert&rdquo; subnetworks. DeepSeek-V2 and V3 employ an architecture named <strong>DeepSeekMoE</strong> (<a href=https://arxiv.org/abs/2401.06066>Dai et al., 2024</a>) in their FFN (Feed-Forward Network) layers. Compared to traditional MoE architectures like GShard, the core ideas of DeepSeekMoE are:</p><ol><li><strong>Fine-grained Expert Segmentation:</strong> Splitting expert networks into smaller units. This aims for higher expert specialization and more precise knowledge acquisition, as each expert can focus on a narrower domain.</li><li><strong>Shared Expert Isolation:</strong> The architecture includes a set of &ldquo;shared experts&rdquo; processed by all tokens, intended to handle general knowledge. This reduces knowledge redundancy among the &ldquo;routing experts&rdquo; that need to be selected, allowing them to focus more on specific knowledge.</li></ol><h4 id=basic-architecture>Basic Architecture<a hidden class=anchor aria-hidden=true href=#basic-architecture>#</a></h4><p>For an input token representation \(\mathbf{u}_t\) to the FFN layer, the output \(\mathbf{h}_t'\) of DeepSeekMoE is computed by combining the outputs of shared experts and selected routing experts:</p>\[
\mathbf{h}_{t}^{\prime} = \mathbf{u}_{t} + \sum_{i=1}^{N_{s}} \operatorname{FFN}_{i}^{(s)}(\mathbf{u}_{t}) + \sum_{i=1}^{N_{r}} g_{i, t} \operatorname{FFN}_{i}^{(r)}(\mathbf{u}_{t}),
\]<p>where:</p><ul><li>\(N_s\) is the number of shared experts.</li><li>\(N_r\) is the number of routing experts.</li><li>\(\operatorname{FFN}_i^{(s)}\) is the \(i\)-th shared expert network.</li><li>\(\operatorname{FFN}_i^{(r)}\) is the \(i\)-th routing expert network.</li><li>\(g_{i, t}\) is the gating value (weight) assigned to the \(i\)-th routing expert for the \(t\)-th token.</li></ul><p>The calculation of the gating value \(g_{i,t}\), based on token-to-expert affinity scores \(s_{i,t}\) and selected via a Top-K routing mechanism, is one of the key differences between DeepSeek-V2 and V3.</p><h4 id=v2-vs-v3-gating-mechanism-and-load-balancing-comparison>V2 vs V3 Gating Mechanism and Load Balancing Comparison<a hidden class=anchor aria-hidden=true href=#v2-vs-v3-gating-mechanism-and-load-balancing-comparison>#</a></h4><p>A core challenge in MoE models is load balancing: ensuring all experts are effectively utilized, avoiding situations where some experts are overloaded while others are idle, which affects training stability and computational efficiency. DeepSeek-V2 and V3 adopt different approaches to gating mechanisms and load balancing strategies.</p><p><strong>1. Affinity Calculation (\(s_{i,t}\)) and Top-K Selection:</strong></p><ul><li><p><strong>DeepSeek-V2:</strong> Uses the Softmax function to compute the affinity score of each token for each routing expert. Top-K selection is directly based on these affinity scores \(s_{i,t}\).</p>\[
s_{i, t} = \operatorname{Softmax}_{i}(\mathbf{u}_{t}^{T} \mathbf{e}_{i})
\]<p>where \(\mathbf{e}_i\) is the learnable center vector for the \(i\)-th routing expert. The \(K_r\) experts with the highest \(s_{i,t}\) are selected.</p></li><li><p><strong>DeepSeek-V3:</strong> Uses the Sigmoid function to compute affinity scores. More importantly, it introduces a learnable bias term \(b_i\) for each routing expert. Top-K selection is based on the <strong>bias-adjusted affinity</strong> \(s_{i,t} + b_i\).</p>\[
s_{i, t} = \operatorname{Sigmoid}(\mathbf{u}_{t}^{T} \mathbf{e}_{i})
\]<p>Selection is based on the \(K_r\) experts with the highest \(s_{i,t} + b_i\) values.</p></li></ul><p><strong>2. Gating Value Calculation (\(g_{i,t}\)):</strong></p><ul><li><p><strong>DeepSeek-V2:</strong> For experts selected by Top-K, their gating value \(g_{i,t}\) is directly equal to their original affinity score \(s_{i,t}\). For unselected experts, \(g_{i,t} = 0\).</p>\[
g_{i, t}^{\prime} = \begin{cases} s_{i, t}, & s_{i, t} \in \operatorname{Topk}(\{s_{j, t}\}, K_{r}), \\ 0, & \text{otherwise}, \end{cases}
\]<p></p>\[
g_{i, t} = g_{i, t}^{\prime} \quad (\text{No additional normalization in V2})
\]</li><li><p><strong>DeepSeek-V3:</strong> For experts selected based on \(s_{i,t} + b_i\), their gating value \(g_{i,t}\) is obtained by normalizing the <strong>original affinity scores</strong> \(s_{i,t}\) of these selected experts. The bias \(b_i\) is <strong>only used for routing selection</strong> and does not affect the final weighted sum.</p>\[
g_{i, t}^{\prime}= \begin{cases} s_{i, t}, & s_{i, t}+b_{i} \in \operatorname{Topk}\left(\left\{s_{j, t}+b_{j} \mid 1 \leqslant j \leqslant N_{r}\right\}, K_{r}\right) \\ 0, & \text{otherwise.} \end{cases}
\]<p></p>\[
g_{i, t} = \frac{g_{i, t}^{\prime}}{\sum_{j=1}^{N_{r}} g_{j, t}^{\prime}} \quad (\text{Normalize affinities of selected experts})
\]</li></ul><p><strong>3. Load Balancing Strategy:</strong></p><ul><li><p><strong>DeepSeek-V2:</strong></p><ul><li><strong>Primary Strategy: Auxiliary Losses</strong> V2 introduces multiple auxiliary loss terms to explicitly encourage load balancing:<ul><li><strong>Expert-level Balancing Loss (\(\mathcal{L}_{\text{ExpBal}}\)):</strong> Encourages each expert to process roughly the same number of tokens.
\[
\begin{aligned}
\mathcal{L}_{\text{ExpBal}} &= \alpha_{1} \sum_{i=1}^{N_{r}} f_{i} P_{i} \\
f_{i} &= \frac{N_{r}}{K_{r} T} \sum_{t=1}^{T} \mathbb{1}(\text{Token } t \text{ selects Expert } i) \\
P_{i} &= \frac{1}{T} \sum_{t=1}^{T} s_{i, t}
\end{aligned}
\]
where \(T\) is the total number of tokens in the batch, \(f_i\) is the fraction of tokens routed to expert \(i\) (relative to the ideal balanced state), \(P_i\) is the average affinity score for expert \(i\), and \(\alpha_1\) is a hyperparameter.</li><li><strong>Device-level Balancing Loss (\(\mathcal{L}_{\text{DevBal}}\)):</strong> Encourages uniform distribution of computational load across different device groups (assuming experts are distributed across \(D\) device groups \(\{\mathcal{E}_1, \dots, \mathcal{E}_D\}\)).
\[
\begin{aligned}
\mathcal{L}_{\text{DevBal}} &= \alpha_{2} \sum_{i=1}^{D} f_{i}^{\prime} P_{i}^{\prime} \\
f_{i}^{\prime} &= \frac{1}{|\mathcal{E}_{i}|} \sum_{j \in \mathcal{E}_{i}} f_{j} \\
P_{i}^{\prime} &= \sum_{j \in \mathcal{E}_{i}} P_{j}
\end{aligned}
\]
where \(f_i'\) is the average load score for device group \(i\), \(P_i'\) is the total affinity for device group \(i\), and \(\alpha_2\) is a hyperparameter.</li><li><strong>Communication Balancing Loss (\(\mathcal{L}_{\text{CommBal}}\)):</strong> Encourages roughly equal numbers of tokens sent to each device to balance All-to-All communication load.
\[
\begin{aligned}
\mathcal{L}_{\text{CommBal}} &= \alpha_{3} \sum_{i=1}^{D} f_{i}^{\prime \prime} P_{i}^{\prime \prime} \\
f_{i}^{\prime \prime} &= \frac{D}{M T} \sum_{t=1}^{T} \mathbb{1}(\text{Token } t \text{ is sent to Device } i) \\
P_{i}^{\prime \prime} &= \sum_{j \in \mathcal{E}_{i}} P_{j}
\end{aligned}
\]
where \(f_i''\) is the fraction of tokens sent to device \(i\) (relative to the ideal balanced state), \(P_i''\) is the total affinity for device group \(i\), and \(\alpha_3\) is a hyperparameter.</li></ul></li><li><strong>Routing Restriction: Device-Limited Routing</strong> Limits each token to route to experts distributed on at most \(M\) different devices. In V2, \(M=3\).</li><li><strong>Token Dropping:</strong> During training, if a device receives more tokens than a preset capacity factor (usually slightly above the average), some tokens with the lowest routing weights (affinities) are dropped to avoid wasting computational resources. However, tokens from about 10% of sequences are preserved from dropping.</li></ul></li><li><p><strong>DeepSeek-V3:</strong></p><ul><li><strong>Primary Strategy: Auxiliary-Loss-Free Load Balancing</strong> V3 posits that auxiliary losses can harm model performance and thus adopts an innovative <strong>Auxiliary-Loss-Free Load Balancing</strong> (<a href=https://arxiv.org/abs/2408.15664>Wang et al., 2024</a>). It achieves load balancing by dynamically adjusting the aforementioned learnable bias terms \(b_i\):<ul><li><strong>Bias Update:</strong> After each training step, monitor the number of tokens processed by each expert \(i\) in the current batch.<ul><li>If expert \(i\) is overloaded (processed tokens > Total batch tokens / \(N_r\)), decrease its bias: \(b_i \leftarrow b_i - \gamma\).</li><li>If expert \(i\) is underloaded (processed tokens &lt; Total batch tokens / \(N_r\)), increase its bias: \(b_i \leftarrow b_i + \gamma\).</li></ul></li><li>\(\gamma\) is a small positive step size (bias update rate hyperparameter). This way, highly loaded experts become less likely to be selected in subsequent routing, while lowly loaded experts become more likely, dynamically balancing the load at the batch level.</li></ul></li><li><strong>Supplementary Strategy: Sequence-Level Auxiliary Loss (\(\mathcal{L}_{\text{Bal}}\))</strong> V3 still retains an auxiliary loss with an <strong>extremely small weight</strong> (\(\alpha=0.0001\)), but it acts on the expert selection balance <strong>within individual sequences</strong>, rather than the entire batch. This is mainly to prevent extreme imbalance within a single sequence.
\[
\begin{gathered}
\mathcal{L}_{\text{Bal}} = \alpha \sum_{i=1}^{N_{r}} f_{i} P_{i}, \\
f_{i} = \frac{N_{r}}{K_{r} T_{seq}} \sum_{t=1}^{T_{seq}} \mathbb{1}\left(s_{i, t} \in \operatorname{Topk}\left(\left\{s_{j, t} \mid 1 \leqslant j \leqslant N_{r}\right\}, K_{r}\right)\right), \\
s_{i, t}^{\prime} = \frac{s_{i, t}}{\sum_{j=1}^{N_{r}} s_{j, t}}, \quad P_{i} = \frac{1}{T_{seq}} \sum_{t=1}^{T_{seq}} s_{i, t}^{\prime}
\end{gathered}
\]
Note that \(f_i, P_i\) here are computed over a single sequence (length \(T_{seq}\)), and \(s_{i,t}'\) is the value of original \(s_{i,t}\) normalized within the sequence.</li><li><strong>Routing Restriction: Node-Limited Routing</strong> Similar to V2&rsquo;s device limit, but applied at the node level. In V3, \(M=4\).</li><li><strong>No Token Dropping:</strong> Due to the effectiveness of bias-adjustment-based load balancing, V3 does not drop any tokens during training or inference.</li></ul></li></ul><p><strong>Advantages of V3&rsquo;s Strategy:</strong>
V3&rsquo;s auxiliary-loss-free strategy aims to minimize the negative impact of the load balancing mechanism on the final model performance. By dynamically adjusting bias terms for batch-level load balancing, the constraints are looser compared to V2&rsquo;s sequence-level balancing based on auxiliary losses. This allows experts to exhibit stronger specialization patterns across different domains, as routing decisions do not need to strictly follow a balanced distribution within each sequence. The figure below shows experimental results indicating this strategy outperforms auxiliary-loss-based methods on multiple benchmarks.</p><figure class=align-center><img loading=lazy src=auxiliary_loss_free_result.png#center alt="Fig. 11. Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 11. Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><p>The key difference between auxiliary-loss-free load balancing and sequence-level auxiliary loss lies in their balancing scope: batch-level versus sequence-level. Compared to sequence-level auxiliary loss, batch-level balancing imposes more flexible constraints as it does not enforce domain balance within each sequence. This flexibility allows experts to specialize better across different domains. To validate this, the figure records and analyzes the expert load on different domains of the Pile test set for a 16B baseline model with auxiliary loss and a 16B model without auxiliary loss. It can be observed that the auxiliary-loss-free model exhibits more pronounced expert specialization patterns, as expected.</p><figure class=align-center><img loading=lazy src=expert_load.png#center alt="Fig. 12. Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 12. Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><hr><h4 id=deepseekmoe-v2-vs-v3-comparison-summary-table>DeepSeekMoE V2 vs V3 Comparison Summary Table<a hidden class=anchor aria-hidden=true href=#deepseekmoe-v2-vs-v3-comparison-summary-table>#</a></h4><table><thead><tr><th style=text-align:left>Feature</th><th style=text-align:left>DeepSeek-V2</th><th style=text-align:left>DeepSeek-V3</th></tr></thead><tbody><tr><td style=text-align:left><strong>Affinity Calculation \(s_{i,t}\)</strong></td><td style=text-align:left>\(\operatorname{Softmax}_{i}(\mathbf{u}_{t}^{T} \mathbf{e}_{i})\)</td><td style=text-align:left>\(\operatorname{Sigmoid}(\mathbf{u}_{t}^{T} \mathbf{e}_{i})\)</td></tr><tr><td style=text-align:left><strong>TopK Selection Basis</strong></td><td style=text-align:left>Original affinity \(s_{i,t}\)</td><td style=text-align:left>Bias-adjusted affinity \(s_{i,t} + b_i\)</td></tr><tr><td style=text-align:left><strong>Gating Value Calc. \(g_{i,t}\)</strong></td><td style=text-align:left>For selected experts, \(g_{i,t} = s_{i,t}\) (Usually no extra normalization)</td><td style=text-align:left>For selected experts, normalize based on original affinity \(s_{i,t}\): \(g_{i, t} = \frac{s_{i, t}}{\sum_{j \in \text{Selected}} s_{j, t}}\)</td></tr><tr><td style=text-align:left><strong>Primary Load Balancing</strong></td><td style=text-align:left><strong>Auxiliary Losses:</strong> - \(\mathcal{L}_{\text{ExpBal}}\) (Expert-level) - \(\mathcal{L}_{\text{DevBal}}\) (Device-level) - \(\mathcal{L}_{\text{CommBal}}\) (Comm-level)</td><td style=text-align:left><strong>Auxiliary-Loss-Free:</strong> - Dynamic adjustment of learnable bias \(b_i\) (step \(\gamma\)) for batch-level balancing</td></tr><tr><td style=text-align:left><strong>Supplementary Balancing</strong></td><td style=text-align:left>No explicit supplementary strategy</td><td style=text-align:left><strong>Sequence-Level Aux Loss</strong> \(\mathcal{L}_{\text{Bal}}\) (Weight \(\alpha\) minimal, e.g., 0.0001), prevents extreme imbalance within single sequences</td></tr><tr><td style=text-align:left><strong>Routing Restriction</strong></td><td style=text-align:left><strong>Device Limit:</strong> Each token routes to experts on at most \(M=3\) devices</td><td style=text-align:left><strong>Node Limit:</strong> Each token routes to experts on at most \(M=4\) nodes</td></tr><tr><td style=text-align:left><strong>Token Dropping</strong></td><td style=text-align:left><strong>Yes:</strong> During training, tokens exceeding device capacity with lowest affinity are dropped (preserving ~10% sequences) to mitigate bottlenecks</td><td style=text-align:left><strong>No:</strong> No tokens dropped during training or inference</td></tr><tr><td style=text-align:left><strong>Balancing Granularity</strong></td><td style=text-align:left>Primarily enforced at sequence/batch level via auxiliary losses</td><td style=text-align:left>Primarily balanced dynamically at batch level via bias adjustment, looser constraints</td></tr><tr><td style=text-align:left><strong>Impact on Performance</strong></td><td style=text-align:left>Auxiliary losses might negatively impact model performance</td><td style=text-align:left>Designed to minimize negative impact of balancing strategy on performance, allowing better expert specialization</td></tr></tbody></table><h3 id=multi-token-prediction-mtp>Multi-Token Prediction (MTP)<a hidden class=anchor aria-hidden=true href=#multi-token-prediction-mtp>#</a></h3><p>To further enhance model performance and data efficiency, DeepSeek-V3 introduces the <strong>Multi-Token Prediction (MTP)</strong> training objective (inspired by <a href=https://arxiv.org/abs/2404.19737>Gloeckle et al., 2024</a>). Standard language models only predict the next token, whereas MTP makes the model predict multiple future tokens (in V3, \(D_{MTP}=1\), i.e., predicting the token after the next) at each position.</p><h4 id=mtp-implementation>MTP Implementation<a hidden class=anchor aria-hidden=true href=#mtp-implementation>#</a></h4><p>MTP is implemented through \(D_{MTP}\) sequential modules. The \(k\)-th MTP module (\(k=1, \dots, D_{MTP}\)) contains:</p><ul><li>A shared embedding layer \(\operatorname{Emb}(\cdot)\)</li><li>A shared output head \(\operatorname{OutHead}(\cdot)\)</li><li>Independent Transformer blocks \(\operatorname{TRM}_k(\cdot)\)</li><li>Independent projection matrices \(M_k \in \mathbb{R}^{d \times 2d}\)</li></ul><p>For the \(i\)-th token \(t_i\) in the input sequence, at the \(k\)-th prediction depth:</p><ol><li>Concatenate the representation of the \(i\)-th token at depth \(k-1\), \(\mathbf{h}_i^{k-1}\) (which is the main model output when \(k=1\)), with the embedding of the \((i+k)\)-th token, \(\operatorname{Emb}(t_{i+k})\). Project this concatenation through matrix \(M_k\) to get the combined representation \(\mathbf{h}_i^{\prime k}\):
\[
\mathbf{h}_{i}^{\prime k} = M_{k}[\operatorname{RMSNorm}(\mathbf{h}_{i}^{k-1}) ; \operatorname{RMSNorm}(\operatorname{Emb}(t_{i+k}))]
\]</li><li>Input the combined representation into the \(k\)-th Transformer block to get the output representation \(\mathbf{h}_i^k\) for the current depth:
\[
\mathbf{h}_{1: T-k}^{k} = \operatorname{TRM}_{k}(\mathbf{h}_{1: T-k}^{\prime k})
\]</li><li>Use the shared output head to predict the probability distribution \(P_{i+k+1}^k \in \mathbb{R}^V\) for the \((i+k+1)\)-th token:
\[
P_{i+k+1}^{k} = \operatorname{OutHead}(\mathbf{h}_{i}^{k})
\]</li></ol><p>Crucially, this implementation <strong>maintains the complete causal chain for each prediction depth</strong>, differing from methods that predict multiple tokens in parallel.</p><figure class=align-center><img loading=lazy src=mtp.png#center alt="Fig. 13. Illustration of our Multi-Token Prediction (MTP) implementation. They keep the complete causal chain for the prediction of each token at each depth. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 13. Illustration of our Multi-Token Prediction (MTP) implementation. They keep the complete causal chain for the prediction of each token at each depth. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h4 id=mtp-training-objective>MTP Training Objective<a hidden class=anchor aria-hidden=true href=#mtp-training-objective>#</a></h4><p>Compute the cross-entropy loss \(\mathcal{L}_{\text{MTP}}^k\) for each prediction depth \(k\):</p>\[
\mathcal{L}_{\text{MTP}}^{k} = \operatorname{CrossEntropy}(P_{2+k: T+1}^{k}, t_{2+k: T+1}) = -\frac{1}{T} \sum_{i=2+k}^{T+1} \log P_{i}^{k}[t_{i}]
\]<p>The total MTP loss is the weighted average of losses across all depths:</p>\[
\mathcal{L}_{\text{MTP}} = \frac{\lambda}{D_{MTP}} \sum_{k=1}^{D_{MTP}} \mathcal{L}_{\text{MTP}}^{k}
\]<p>where \(\lambda\) is a weighting factor (0.3 initially, 0.1 later in V3 training). This loss is added to the main model&rsquo;s standard next-token prediction loss.</p><h4 id=mtp-inference>MTP Inference<a hidden class=anchor aria-hidden=true href=#mtp-inference>#</a></h4><p>MTP is primarily used to enhance the main model&rsquo;s performance. During inference, the <strong>MTP modules can be simply discarded</strong>, and the main model works independently. Alternatively, the MTP modules can be utilized for <strong>speculative decoding</strong> (<a href=https://arxiv.org/abs/2211.17192>Leviathan et al., 2023</a>; <a href=https://arxiv.org/abs/2203.16487>Xia et al., 2023</a>) to accelerate the generation process. V3 experiments show an acceptance rate of 85%-90% for the second token, potentially speeding up decoding by about 1.8x.</p><h2 id=infrastructure-and-training-efficiency>Infrastructure and Training Efficiency<a hidden class=anchor aria-hidden=true href=#infrastructure-and-training-efficiency>#</a></h2><p>The efficient training and deployment of DeepSeek-V3 benefit from the synergistic design of algorithms, frameworks, and hardware.</p><h3 id=compute-cluster>Compute Cluster<a hidden class=anchor aria-hidden=true href=#compute-cluster>#</a></h3><p>DeepSeek-V3 was trained on a cluster equipped with <strong>2048 NVIDIA H800 GPUs</strong>.</p><ul><li><strong>Intra-node:</strong> Each node contains 8 H800 GPUs interconnected via high-speed <strong>NVLink</strong> and <strong>NVSwitch</strong>.</li><li><strong>Inter-node:</strong> Different nodes communicate using the <strong>InfiniBand (IB)</strong> network.</li></ul><h3 id=training-framework>Training Framework<a hidden class=anchor aria-hidden=true href=#training-framework>#</a></h3><p>DeepSeek-V3 training is based on the self-developed, efficient, and lightweight framework <strong>HAI-LLM</strong>. Overall, it employs:</p><ul><li><strong>16-way Pipeline Parallelism (PP)</strong> (<a href=https://arxiv.org/abs/2401.10241>Qi et al., 2023</a>)</li><li><strong>64-way Expert Parallelism (EP)</strong> (across 8 nodes) (<a href=https://arxiv.org/abs/2006.16668>Lepikhin et al., 2021</a>)</li><li><strong>ZeRO-1 Data Parallelism (DP)</strong> (<a href=https://arxiv.org/pdf/1910.02054>Rajbhandari et al., 2020</a>)</li></ul><p>To achieve efficient training, Deepseek performed meticulous engineering optimizations:</p><ol><li>Designed the <strong>DualPipe</strong> algorithm for efficient pipeline parallelism, reducing bubbles and overlapping computation with communication, addressing the heavy communication overhead introduced by cross-node expert parallelism.</li><li>Developed efficient <strong>cross-node All-to-all communication Kernels</strong> that fully utilize IB and NVLink bandwidth while saving SM resources for communication.</li><li>Carefully optimized <strong>memory usage</strong> during training, enabling DeepSeek-V3 to be trained <strong>without using Tensor Parallelism (TP)</strong>.</li></ol><h4 id=dualpipe-and-computation-communication-overlap>DualPipe and Computation-Communication Overlap<a hidden class=anchor aria-hidden=true href=#dualpipe-and-computation-communication-overlap>#</a></h4><ul><li><strong>Challenge:</strong> Cross-node expert parallelism leads to a computation-to-communication ratio close to 1:1, which is inefficient.</li></ul><figure class=align-center><img loading=lazy src=forward_backward_chucks.png#center alt="Fig. 17. Overlapping strategy for a pair of forward and backward chunks with misaligned transformer block boundaries. Orange: forward, green: backward for input, blue: backward for weights, purple: PP communication, red: barriers. Both all-to-all and PP communications are fully hidden. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 17. Overlapping strategy for a pair of forward and backward chunks with misaligned transformer block boundaries. Orange: forward, green: backward for input, blue: backward for weights, purple: PP communication, red: barriers. Both all-to-all and PP communications are fully hidden. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><ul><li><strong>Core Idea:</strong> Overlap computation and communication within a pair of independent forward and backward chunks. Each chunk is decomposed into four components: <strong>Attention</strong>, <strong>All-to-all Dispatch</strong>, <strong>MLP</strong>, <strong>All-to-all Combine</strong> (backward Attention and MLP are further divided into backward for input and backward for weights, similar to <strong>ZeroBubble</strong> (<a href=https://arxiv.org/abs/2401.10241>Qi et al., 2023</a>). By rearranging these components and manually adjusting the GPU SM ratio for communication versus computation, both All-to-all and PP communication can be fully hidden.</li><li><strong>Scheduling:</strong> Adopts a bidirectional pipeline schedule, feeding micro-batches from both ends of the pipeline simultaneously, allowing most communication to be completely overlapped.</li></ul><figure class=align-center><img loading=lazy src=dualpipe.png#center alt="Fig. 18. Example DualPipe scheduling with 8 PP ranks and 20 micro-batches in both directions. The reverse-direction micro-batches mirror the forward ones, so their batch IDs are omitted for simplicity. Two cells within a shared black border represent mutually overlapped computation and communication. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 18. Example DualPipe scheduling with 8 PP ranks and 20 micro-batches in both directions. The reverse-direction micro-batches mirror the forward ones, so their batch IDs are omitted for simplicity. Two cells within a shared black border represent mutually overlapped computation and communication. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><ul><li><strong>Advantages:</strong><ul><li>Efficient even in general scenarios without heavy communication burden.</li><li>Compared to <strong>ZB1P</strong> (<a href=https://arxiv.org/abs/2401.10241>Qi et al., 2023</a>) and <strong>1F1B</strong> (<a href=https://arxiv.org/abs/1806.03377>Harlap et al., 2018</a>), significantly reduces pipeline bubbles, only increasing peak activation memory by a factor of \(\frac{1}{PP}\).</li><li>Although requiring two copies of model parameters, the memory increase is not significant due to the large EP size used in training.</li><li>Compared to <strong>Chimera</strong> (<a href=https://dl.acm.org/doi/10.1145/3458817.3476145>Li and Hoefler, 2021</a>), has looser requirements on the number of micro-batches (only needs to be divisible by 2), and bubble/activation memory does not grow with the number of micro-batches.</li></ul></li></ul><table><thead><tr><th style=text-align:left>Method (Method)</th><th style=text-align:left>Bubble (Bubble)</th><th style=text-align:left>Parameter (Parameter)</th><th style=text-align:left>Activation (Activation)</th></tr></thead><tbody><tr><td style=text-align:left>1F1B</td><td style=text-align:left>\((PP-1)(F+B)\)</td><td style=text-align:left>\(1 \times\)</td><td style=text-align:left>\(PP\)</td></tr><tr><td style=text-align:left>ZB1P</td><td style=text-align:left>\((PP-1)(F+B-2W)\)</td><td style=text-align:left>\(1 \times\)</td><td style=text-align:left>\(PP\)</td></tr><tr><td style=text-align:left><strong>DualPipe (Deepseek V3)</strong></td><td style=text-align:left>\(\left(\frac{PP}{2}-1\right)(F\&B+B-3W)\)</td><td style=text-align:left>\(2 \times\)</td><td style=text-align:left>\(PP+1\)</td></tr></tbody></table><p>The table above compares pipeline bubble and memory usage for different pipeline parallelism methods. \(F\): Forward chunk execution time; \(B\): Full backward chunk execution time; \(W\): &ldquo;Backward for weights&rdquo; chunk execution time; \(F\&B\): Execution time of two mutually overlapped forward and backward chunks.</p><h4 id=efficient-cross-node-all-to-all-communication-implementation>Efficient Cross-Node All-to-All Communication Implementation<a hidden class=anchor aria-hidden=true href=#efficient-cross-node-all-to-all-communication-implementation>#</a></h4><ul><li><strong>Goal:</strong> Provide sufficient computational performance for DualPipe by customizing efficient cross-node All-to-all communication Kernels (dispatching & combining), saving SMs dedicated to communication.</li><li><strong>Strategy:</strong> Combine MoE gating algorithm with cluster network topology (fully connected IB between nodes, NVLink within nodes).<ul><li><strong>Bandwidth Utilization:</strong> NVLink bandwidth (\(160 \mathrm{~GB} / \mathrm{s}\)) is about 3.2 times IB bandwidth (\(50 \mathrm{~GB} / \mathrm{s}\)). Limit each token to be dispatched to at most <strong>4 nodes</strong> to reduce IB traffic.</li><li><strong>Transmission Path:</strong> After token routing is determined, tokens are first transmitted via <strong>IB</strong> to the GPU with the same intra-node index on the target node. Upon arrival at the target node, they are immediately forwarded via <strong>NVLink</strong> to the specific GPU hosting the target expert, avoiding blockage by subsequently arriving tokens.</li><li><strong>Effect:</strong> IB and NVLink communication are fully overlapped. Each token can efficiently select an average of <strong>3.2 experts/node</strong> without additional NVLink overhead. This means V3 actually selects 8 routing experts, but could theoretically scale up to <strong>13 experts</strong> (4 nodes × 3.2 experts/node) with no increase in communication cost.</li></ul></li><li><strong>Implementation:</strong><ul><li>Use <strong>Warp Specialization</strong> (<a href=https://doi.org/10.1145/2555243.2555258>Bauer et al., 2014</a>) technology to divide <strong>20 SMs</strong> into 10 communication channels.</li><li>Dispatch process: IB send, IB-to-NVLink forward, NVLink receive are handled by respective warps, with warp counts dynamically adjusted based on load.</li><li>Combine process: NVLink send, NVLink-to-IB forward & accumulate, IB receive & accumulate are also handled by dynamically adjusted warps.</li><li><strong>Optimization:</strong> Dispatch and Combine Kernels overlap with computation streams. Use custom <strong>PTX</strong> instructions and automatically adjust communication chunk sizes to significantly reduce L2 cache usage and interference with other SM computation Kernels.</li></ul></li><li><strong>Result:</strong> Only <strong>20 SMs</strong> are required to fully utilize IB and NVLink bandwidth.</li></ul><h4 id=extreme-memory-optimization-and-minimal-overhead>Extreme Memory Optimization and Minimal Overhead<a hidden class=anchor aria-hidden=true href=#extreme-memory-optimization-and-minimal-overhead>#</a></h4><p>To reduce training memory footprint, the following techniques were employed:</p><ul><li><strong>Recomputation:</strong> Recompute all <strong>RMSNorm</strong> operations and <strong>MLA up-projections</strong> during backpropagation, avoiding storage of their output activations. Significantly reduces activation memory demand at minimal overhead.</li><li><strong>CPU Storage for EMA:</strong> Store the <strong>Exponential Moving Average (EMA)</strong> of model parameters in <strong>CPU memory</strong> and update asynchronously after each training step. Maintains EMA parameters without additional GPU memory or time overhead.</li><li><strong>Shared Embedding and Output Head:</strong> Leverage the DualPipe strategy to deploy the shallowest layers (including Embedding) and deepest layers (including Output Head) on the <strong>same PP rank</strong>. This allows <strong>MTP modules</strong> and the main model to <strong>physically share</strong> parameters and gradients for Embedding and Output Head, further enhancing memory efficiency.</li><li><strong>Effect:</strong> These optimizations enable DeepSeek-V3 to be trained <strong>without using expensive Tensor Parallelism (TP)</strong>.</li></ul><h3 id=fp8-training>FP8 Training<a hidden class=anchor aria-hidden=true href=#fp8-training>#</a></h3><p>To accelerate training and reduce GPU memory usage, DeepSeek-V3 employs an <strong>FP8 mixed-precision training framework</strong> (<a href=https://arxiv.org/pdf/2208.07339>Dettmers et al., 2022</a>; <a href=https://arxiv.org/abs/2206.02915>Noune et al., 2022</a>; <a href=https://arxiv.org/abs/2310.18313>Peng et al., 2023</a>), validating its effectiveness on ultra-large-scale models for the first time.</p><h4 id=mixed-precision-framework>Mixed Precision Framework<a hidden class=anchor aria-hidden=true href=#mixed-precision-framework>#</a></h4><ul><li><strong>Core Computation (GEMM):</strong> Most GEMM operations (forward, activation gradient backward, weight gradient backward) use FP8 inputs, outputting BF16 or FP32, theoretically doubling computation speed.</li><li><strong>High Precision Retention:</strong> Parts sensitive to precision or with low computational overhead (e.g., Embedding, Output Head, MoE Gating, Normalization, Attention) retain BF16/FP32 precision.</li><li><strong>High Precision Storage:</strong> Main weights, weight gradients, and optimizer states (partially BF16) use higher precision, with ZeRO-1 sharding reducing GPU memory pressure.</li></ul><figure class=align-center><img loading=lazy src=fp8_framework.png#center alt="Fig. 14. The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 14. The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h4 id=precision-enhancement-strategies>Precision Enhancement Strategies<a hidden class=anchor aria-hidden=true href=#precision-enhancement-strategies>#</a></h4><ol><li><strong>Fine-grained Quantization:</strong> To address FP8&rsquo;s <strong>limited dynamic range and sensitivity to outliers</strong> (<a href=https://arxiv.org/abs/2409.12517>Fishman et al., 2024</a>; <a href=https://arxiv.org/abs/2405.19279>He et al., 2024</a>; <a href=https://arxiv.org/abs/2402.17762>Sun et al., 2024</a>), adopt finer-grained quantization:<ul><li><strong>Activations:</strong> Scaled in groups of \(1 \times 128\) tiles.</li><li><strong>Weights:</strong> Scaled in groups of \(128 \times 128\) blocks.
This method allows scaling factors to better adapt to the range of local data, reducing quantization error.</li></ul></li><li><strong>Improved Accumulation Precision:</strong> H800 Tensor Cores have limited accumulation precision (approx. 14 bits) for FP8 GEMM. To solve this, employ the <strong>Promotion to CUDA Cores</strong> strategy (<a href=https://github.com/NVIDIA/cutlass>Thakkar et al., 2023</a>): Tensor Cores compute partial sums (e.g., every \(N_C=128\) elements), then transfer the results to CUDA Core FP32 registers for full-precision accumulation. Scaling factors from fine-grained quantization can also be efficiently applied on CUDA Cores. With concurrent execution of WGMMA operations, this method improves precision with minimal impact on computational efficiency.</li><li><strong>E4M3 Format:</strong> V3 uniformly uses the <strong>E4M3 format</strong> (4 exponent bits, 3 mantissa bits) for all tensors, rather than mixing with <strong>E5M2</strong> (<a href=https://github.com/NVIDIA/TransformerEngine>NVIDIA, 2024</a>; <a href=https://arxiv.org/abs/2310.18313>Peng et al., 2023</a>; <a href=https://papers.nips.cc/paper_files/paper/2019/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html>Sun et al., 2019b</a>). The fine-grained quantization strategy effectively mitigates the smaller dynamic range issue of E4M3.</li><li><strong>Online Quantization:</strong> Compute scaling factors based on the <strong>maximum absolute value of each tile/block in real-time, rather than relying on historical values</strong> (<a href=https://github.com/NVIDIA/TransformerEngine>NVIDIA, 2024</a>; <a href=https://arxiv.org/abs/2310.18313>Peng et al., 2023</a>), ensuring quantization accuracy.</li></ol><figure class=align-center><img loading=lazy src=fp8_quantization_enhancement.png#center alt="Fig. 15. (a) Fine-grained quantization method to mitigate quantization errors. (b) Improved FP8 GEMM precision by promoting to CUDA Cores for high-precision accumulation. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 15. (a) Fine-grained quantization method to mitigate quantization errors. (b) Improved FP8 GEMM precision by promoting to CUDA Cores for high-precision accumulation. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h4 id=low-precision-storage-and-communication>Low-Precision Storage and Communication<a hidden class=anchor aria-hidden=true href=#low-precision-storage-and-communication>#</a></h4><ul><li><strong>Optimizer States:</strong> First and second moments of <strong>AdamW</strong> (<a href=https://arxiv.org/abs/1711.05101>Loshchilov and Hutter, 2017</a>) are stored in BF16. Main weights and gradient accumulation remain FP32.</li><li><strong>Activation Caching:</strong> Since Wgrad operations use FP8 inputs, activations can be cached as FP8. For specific sensitive operations (e.g., input to Linear after Attention), a custom E5M6 format with round scaling is used. Inputs to SwiGLU in MoE are also cached as FP8.</li><li><strong>Communication:</strong> Activations before MoE up-projection are quantized to FP8 for dispatch; activation gradients before MoE down-projection are also quantized to FP8. Combine operations retain BF16 precision.</li></ul><p>The figure below shows experiments demonstrating that the relative error of FP8 training loss compared to BF16 is less than <strong>0.25%</strong>, which is within an acceptable range.</p><figure class=align-center><img loading=lazy src=fp8_vs_bf16_loss_curves.png#center alt="Fig. 16. Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9 DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 16. Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9 <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h3 id=inference-and-deployment>Inference and Deployment<a hidden class=anchor aria-hidden=true href=#inference-and-deployment>#</a></h3><p>DeepSeek-V3 is deployed on an H800 cluster (NVLink intra-node, fully connected IB inter-node). To guarantee both <strong>SLO (Service Level Objective)</strong> for online services and high throughput, a deployment strategy separating <strong>Prefilling</strong> and <strong>Decoding</strong> stages is adopted.</p><h4 id=prefilling-stage>Prefilling Stage<a hidden class=anchor aria-hidden=true href=#prefilling-stage>#</a></h4><ul><li><strong>Minimum Deployment Unit:</strong> 4 nodes, 32 GPUs.</li><li><strong>Parallelism Strategy:</strong><ul><li>Attention part: <strong>TP4 (Tensor Parallelism) + SP (Sequence Parallelism)</strong> combined with <strong>DP8 (Data Parallelism)</strong>. The smaller TP size (4) limits TP communication overhead.</li><li>MoE part: <strong>EP32 (Expert Parallelism)</strong>, ensuring each expert processes a sufficiently large batch for computational efficiency.</li><li>Shallow Dense MLP: Uses TP1 to save TP communication.</li></ul></li><li><strong>MoE All-to-All Communication:</strong> Uses a method similar to training: first transmit tokens across nodes via IB, then forward within the node between GPUs via NVLink.</li><li><strong>Load Balancing:</strong> Employs a <strong>redundant expert</strong> deployment strategy.<ul><li>Based on statistics collected from online deployment, periodically (e.g., every 10 minutes) detect <strong>high-load experts</strong> and replicate them.</li><li>After determining the redundant expert set, carefully <strong>reshuffle experts</strong> among GPUs within nodes based on observed load, balancing GPU load as much as possible without increasing cross-node All-to-all communication overhead.</li><li>In DeepSeek-V3 deployment, the Prefilling stage sets up <strong>32 redundant experts</strong>. Each GPU hosts its original 8 experts plus 1 additional redundant expert.</li></ul></li><li><strong>Efficiency Optimization:</strong> To improve throughput and hide All-to-all and TP communication overhead, <strong>process two micro-batches with similar computational load concurrently</strong>, overlapping the Attention and MoE of one micro-batch with the Dispatch and Combine of the other.</li><li><strong>Exploration Direction:</strong> <strong>Dynamic redundancy</strong> strategy, where each GPU hosts more experts (e.g., 16), but only activates 9 per inference step. Dynamically compute the globally optimal routing scheme before each layer&rsquo;s All-to-all operation begins. Since Prefilling is computationally intensive, the overhead of computing the routing scheme is almost negligible.</li></ul><h4 id=decoding-stage>Decoding Stage<a hidden class=anchor aria-hidden=true href=#decoding-stage>#</a></h4><ul><li><strong>Expert Perspective:</strong> Treat the <strong>shared expert</strong> as one routing target. From this perspective, each token selects <strong>9 experts</strong> during routing (the shared expert is considered a high-load expert that is always selected).</li><li><strong>Minimum Deployment Unit:</strong> 40 nodes, 320 GPUs.</li><li><strong>Parallelism Strategy:</strong><ul><li>Attention part: <strong>TP4 + SP</strong> combined with <strong>DP80</strong>.</li><li>MoE part: <strong>EP320</strong>. Each GPU hosts only one expert, with 64 GPUs responsible for hosting redundant and shared experts.</li></ul></li><li><strong>All-to-All Communication:</strong> Dispatch and Combine parts use <strong>direct IB point-to-point transmission</strong> for low latency. Utilize <strong>IBGDA</strong> (<a href=https://developer.nvidia.com/blog/gpudirect-storage/>NVIDIA, 2022</a>) technology to further minimize latency and enhance communication efficiency.</li><li><strong>Load Balancing:</strong> Similar to Prefilling, periodically determine the redundant expert set based on online service&rsquo;s statistical expert load. However, since each GPU hosts only one expert, reshuffling is not needed.</li><li><strong>Exploration Directions:</strong><ul><li><strong>Dynamic redundancy strategy:</strong> Requires more careful optimization of the algorithm for computing the globally optimal routing scheme and fusion with the Dispatch Kernel to reduce overhead.</li><li><strong>Processing two micro-batches concurrently:</strong> Unlike Prefilling, the Attention phase takes a larger proportion of time in Decoding. Therefore, overlap the <strong>Attention</strong> of one micro-batch with the <strong>Dispatch+MoE+Combine</strong> of another. In the Decoding stage, the batch size per expert is relatively small (typically &lt; 256 tokens), making memory access the bottleneck rather than computation. Since the MoE part only needs to load parameters for one expert, memory access overhead is small, and using fewer SMs does not significantly impact overall performance. Thus, only a small portion of SMs can be allocated to Dispatch+MoE+Combine without affecting the computation speed of the Attention part.</li></ul></li></ul><h3 id=suggestions-for-hardware-design>Suggestions for Hardware Design<a hidden class=anchor aria-hidden=true href=#suggestions-for-hardware-design>#</a></h3><p>Based on the implementation of <strong>All-to-all communication</strong> and the <strong>FP8 training scheme</strong>, the DeepSeek team proposes the following chip design suggestions to AI hardware vendors.</p><h4 id=communication-hardware>Communication Hardware<a hidden class=anchor aria-hidden=true href=#communication-hardware>#</a></h4><ul><li><strong>Current State:</strong> Communication latency is hidden through computation/communication overlap, significantly reducing dependency on communication bandwidth. However, the current communication implementation relies on expensive <strong>SMs</strong> (e.g., 20 out of 132 SMs on H800 allocated for this purpose), limiting computational throughput. Furthermore, using SMs for communication leaves Tensor Cores completely idle, which is inefficient.</li><li><strong>Primary SM Tasks:</strong><ul><li>Forwarding data between IB and NVLink domains, while aggregating IB traffic destined for multiple GPUs within the same node from a single GPU.</li><li>Transferring data between RDMA buffers and input/output buffers.</li><li>Performing Reduce operations for All-to-all Combine.</li><li>Managing fine-grained memory layout when transmitting data in chunks to multiple experts across IB and NVLink domains.</li></ul></li><li><strong>Expectation:</strong><ul><li>Future vendors should develop hardware to <strong>offload</strong> these communication tasks from valuable computational units (SMs), potentially as GPU co-processors or network co-processors similar to NVIDIA SHARP (<a href=https://network.nvidia.com/pdf/solutions/hpc/paperieee_copyright.pdf>Graham et al., 2016</a>).</li><li>To reduce application programming complexity, this hardware should ideally <strong>unify IB (scale-out) and NVLink (scale-up) networks</strong> from the perspective of the computational units. Through this unified interface, computational units could easily perform operations like read, write, multicast, and reduce across the entire unified IB-NVLink domain by submitting communication requests based on simple primitives.</li></ul></li></ul><h4 id=compute-hardware>Compute Hardware<a hidden class=anchor aria-hidden=true href=#compute-hardware>#</a></h4><ol><li><strong>Higher Precision FP8 GEMM Accumulation in Tensor Cores:</strong><ul><li><strong>Problem:</strong> In the current NVIDIA Hopper architecture Tensor Core implementation, FP8 GEMM uses fixed-point accumulation, aligning mantissa products via right shifts before addition. Experiments show that after sign-extended right shifts, only the top 14 bits of each mantissa product are used, with bits beyond this range truncated. However, achieving an accurate FP32 result from accumulating, say, 32 FP8×FP8 products requires at least 34 bits of precision.</li><li><strong>Suggestion:</strong> Future chip designs should <strong>increase the accumulation precision within Tensor Cores</strong> to support full-precision accumulation, or select an appropriate accumulation bit-width based on the precision requirements of training and inference algorithms. This approach maintains computational efficiency while ensuring errors remain within acceptable limits.</li></ul></li><li><strong>Support for Tile and Block Level Quantization:</strong><ul><li><strong>Problem:</strong> Current GPUs only support per-tensor quantization, lacking native support for finer-grained quantization like tile-wise and block-wise. In the current implementation, when the \(N_C\) interval is reached, partial results must be copied from Tensor Cores to CUDA Cores, multiplied by scaling factors, and then added to CUDA Core FP32 registers. Although combining this with the exact FP32 accumulation strategy significantly mitigates dequantization overhead, frequent data movement between Tensor Cores and CUDA Cores still limits computational efficiency.</li><li><strong>Suggestion:</strong> Future chips should support fine-grained quantization by <strong>enabling Tensor Cores to receive scaling factors and implement MMA with grouped scaling</strong>. This would allow the entire partial sum accumulation and dequantization to be performed directly within the Tensor Cores until the final result is produced, avoiding frequent data movement.</li></ul></li><li><strong>Support for Online Quantization:</strong><ul><li><strong>Problem:</strong> Current implementations struggle to efficiently support online quantization, despite its proven effectiveness in research. The existing workflow requires reading 128 BF16 activation values (output from a previous computation) from HBM for quantization, writing the quantized FP8 values back to HBM, and then reading them again for MMA.</li><li><strong>Suggestion:</strong><ul><li>Future chips should <strong>fuse FP8 type conversion and TMA (Tensor Memory Accelerator) access into a single operation</strong>. This would allow quantization to occur as activations are transferred from global memory to shared memory, avoiding frequent memory reads and writes.</li><li>Recommend supporting <strong>warp-level cast instructions</strong> for acceleration, further promoting better fusion of Layer Normalization and FP8 cast.</li><li>Alternatively, adopt <strong>near-memory computing</strong> approaches, placing computation logic near HBM. This way, BF16 elements could be directly converted to FP8 as they are read from HBM into the GPU, reducing off-chip memory access by about 50%.</li></ul></li></ul></li><li><strong>Support for Transposed GEMM Operations:</strong><ul><li><strong>Problem:</strong> Fusing matrix transpose with GEMM operations is cumbersome in the current architecture. In the workflow, activations from the forward pass are quantized into \(1 \times 128\) FP8 tiles and stored. During backpropagation, the matrix needs to be read, dequantized, transposed, re-quantized into \(128 \times 1\) tiles, and stored back into HBM.</li><li><strong>Suggestion:</strong> Future chips should support <strong>reading transposed matrices directly from shared memory</strong> before MMA operations (for the precisions required by training and inference). Combined with the fusion of FP8 format conversion and TMA access, this enhancement would significantly simplify the quantization workflow.</li></ul></li></ol><h3 id=training-cost-and-efficiency>Training Cost and Efficiency<a hidden class=anchor aria-hidden=true href=#training-cost-and-efficiency>#</a></h3><ul><li><strong>DeepSeek-V2:</strong> Compared to DeepSeek 67B (Dense), achieved 42.5% savings in training cost, 93.3% reduction in KV cache, and a 5.76x increase in maximum throughput.</li><li><strong>DeepSeek-V3:</strong> Extremely high training efficiency, requiring only 180K H800 GPU hours per 1T tokens trained. The total training cost (pre-training + context extension + post-training) was only 2.788M H800 GPU hours (approx. $5.58 million, assuming $2/hour). Pre-training took less than 2 months on the 2048 H800 GPU cluster.</li></ul><table><thead><tr><th style=text-align:left>Training Stage</th><th style=text-align:center>H800 GPU Hours</th><th style=text-align:center>Estimated Cost (USD)</th></tr></thead><tbody><tr><td style=text-align:left>Pre-training</td><td style=text-align:center>2664 K</td><td style=text-align:center>$5.328 M</td></tr><tr><td style=text-align:left>Context Extension</td><td style=text-align:center>119 K</td><td style=text-align:center>$0.238 M</td></tr><tr><td style=text-align:left>Post-training</td><td style=text-align:center>5 K</td><td style=text-align:center>$0.01 M</td></tr><tr><td style=text-align:left><strong>Total</strong></td><td style=text-align:center><strong>2788 K</strong></td><td style=text-align:center><strong>$5.576 M</strong></td></tr></tbody></table><h2 id=pre-training>Pre-training<a hidden class=anchor aria-hidden=true href=#pre-training>#</a></h2><h3 id=data-construction>Data Construction<a hidden class=anchor aria-hidden=true href=#data-construction>#</a></h3><p>Compared to DeepSeek-V2 (based on a 67B model, using a 100K vocabulary Byte-level BPE Tokenizer, 8.1T tokens), DeepSeek-V3 achieved larger scale and higher quality data construction during the pre-training phase through the following strategies:</p><ol><li><p><strong>Corpus Expansion and Refinement</strong></p><ul><li><strong>Domain Focus:</strong> Significantly increased the proportion of text related to mathematics and programming, strengthening the model&rsquo;s understanding and generation capabilities in technical domains.</li><li><strong>Multilingual Coverage:</strong> Added corpora in multiple languages beyond English and Chinese, improving cross-lingual generalization performance.</li><li><strong>Deduplication and Diversity:</strong> Employed efficient data deduplication and filtering processes to minimize redundancy while ensuring content diversity.</li><li><strong>Scale Increase:</strong> Ultimately constructed approximately <strong>14.8T</strong> high-quality tokens, an increase of nearly 83% compared to V2.</li></ul></li><li><p><strong>Training Strategy and Technical Innovation</strong></p><ul><li><strong>Document Packing</strong>
Combined the <strong>Document Packing</strong> (<a href=https://arxiv.org/abs/2404.10830>Ding et al., 2024</a>) method, packing coherent texts into longer segments to improve GPU utilization and context integrity; did not use cross-sample attention masks to maintain implementation simplicity.</li><li><strong>Fill-in-Middle (FIM) Strategy</strong><ul><li><strong>Motivation:</strong> Inspired by DeepSeekCoder-V2 (<a href=https://arxiv.org/abs/2406.11931>DeepSeek-AI, 2024</a>), aimed at enhancing the model&rsquo;s ability to fill in missing information in the middle.</li><li><strong>Framework:</strong> Introduced the Prefix-Suffix-Middle (PSM) structure, with examples like:<pre tabindex=0><code>&lt;|fim_begin|&gt; f_pre &lt;|fim_hole|&gt; f_suf &lt;|fim_end|&gt; f_middle &lt;|eos_token|&gt;
</code></pre></li><li><strong>Application Ratio:</strong> Inserted FIM before document-level pre-packing, accounting for <strong>10%</strong>, balancing generation and prediction tasks.</li></ul></li></ul></li><li><p><strong>Tokenizer Optimization</strong></p><ul><li><strong>BBPE Vocabulary Expansion:</strong> Adopted Byte-level BPE, expanding the vocabulary from 100K to <strong>128K</strong>, improving coverage of rare words and proper nouns.</li><li><strong>Pre-tokenizer Improvement:</strong> Adjusted tokenization rules for multilingual scenarios, enhancing compression efficiency and encoding consistency.</li><li><strong>Boundary Bias Mitigation:</strong> Referenced <a href=https://github.com/guidance-ai/guidance/blob/main/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb>Lundberg, 2023</a>&rsquo;s method to reduce bias from punctuation + newline combination tokens in few-shot scenarios by introducing a random splitting mechanism, exposing the model to more boundary variations.</li></ul></li></ol><h3 id=hyperparameters>Hyperparameters<a hidden class=anchor aria-hidden=true href=#hyperparameters>#</a></h3><table><thead><tr><th style=text-align:left>Parameter</th><th style=text-align:center>DeepSeek-V2</th><th style=text-align:center>DeepSeek-V3</th></tr></thead><tbody><tr><td style=text-align:left>Transformer Layers</td><td style=text-align:center>60</td><td style=text-align:center>61</td></tr><tr><td style=text-align:left>Hidden Dimension \(d\)</td><td style=text-align:center>5120</td><td style=text-align:center>7168</td></tr><tr><td style=text-align:left>Initialization Stddev</td><td style=text-align:center>0.006</td><td style=text-align:center>0.006</td></tr><tr><td style=text-align:left><strong>MLA Parameters</strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Attention Heads \(n_h\)</td><td style=text-align:center>128</td><td style=text-align:center>128</td></tr><tr><td style=text-align:left>Dim per Head \(d_h\)</td><td style=text-align:center>128</td><td style=text-align:center>128</td></tr><tr><td style=text-align:left>KV Compression Dim \(d_c\)</td><td style=text-align:center>512 (\(4d_h\))</td><td style=text-align:center>512 (\(4d_h\))</td></tr><tr><td style=text-align:left>Query Compression Dim \(d_c'\)</td><td style=text-align:center>1536</td><td style=text-align:center>1536</td></tr><tr><td style=text-align:left>Decoupled RoPE Dim \(d_h^R\)</td><td style=text-align:center>64 (\(d_h/2\))</td><td style=text-align:center>64 (\(d_h/2\))</td></tr><tr><td style=text-align:left><strong>DeepSeekMoE Parameters</strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>MoE Layer Position</td><td style=text-align:center>All except layer 1</td><td style=text-align:center>All except first 3 layers</td></tr><tr><td style=text-align:left>Shared Experts \(N_s\)</td><td style=text-align:center>2</td><td style=text-align:center>1</td></tr><tr><td style=text-align:left>Routing Experts \(N_r\)</td><td style=text-align:center>160</td><td style=text-align:center>256</td></tr><tr><td style=text-align:left>Expert Intermediate Dim</td><td style=text-align:center>1536</td><td style=text-align:center>2048</td></tr><tr><td style=text-align:left>Activated Experts \(K_r\)</td><td style=text-align:center>6</td><td style=text-align:center>8</td></tr><tr><td style=text-align:left>Device/Node Limit \(M\)</td><td style=text-align:center>3 (Device)</td><td style=text-align:center>4 (Node)</td></tr><tr><td style=text-align:left>Load Balancing Strategy</td><td style=text-align:center>Aux Losses (\(\alpha_1=0.003, \alpha_2=0.05, \alpha_3=0.02\)) + Token Dropping</td><td style=text-align:center>Aux-Loss-Free (\(\gamma=0.001\)) + Seq Loss (\(\alpha=0.0001\))</td></tr><tr><td style=text-align:left><strong>MTP Parameters (V3 only)</strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Prediction Depth \(D_{MTP}\)</td><td style=text-align:center>N/A</td><td style=text-align:center>1</td></tr><tr><td style=text-align:left>MTP Loss Weight \(\lambda\)</td><td style=text-align:center>N/A</td><td style=text-align:center>0.3 (first 10T) / 0.1 (last 4.8T)</td></tr><tr><td style=text-align:left><strong>Training Parameters</strong></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Optimizer</td><td style=text-align:center>AdamW (\(\beta_1=0.9, \beta_2=0.95, wd=0.1\))</td><td style=text-align:center>AdamW (\(\beta_1=0.9, \beta_2=0.95, wd=0.1\))</td></tr><tr><td style=text-align:left>Max Sequence Length</td><td style=text-align:center>4K</td><td style=text-align:center>4K</td></tr><tr><td style=text-align:left>Training Tokens</td><td style=text-align:center>8.1T</td><td style=text-align:center>14.8T</td></tr><tr><td style=text-align:left>Learning Rate</td><td style=text-align:center>Warmup + Step Decay (Max \(2.4 \times 10^{-4}\))</td><td style=text-align:center>Warmup + Cosine Decay + Constant (Max \(2.2 \times 10^{-4}\))</td></tr><tr><td style=text-align:left>Batch Size</td><td style=text-align:center>2304 -> 9216</td><td style=text-align:center>3072 -> 15360</td></tr><tr><td style=text-align:left>Gradient Clipping</td><td style=text-align:center>1.0</td><td style=text-align:center>1.0</td></tr><tr><td style=text-align:left>Precision</td><td style=text-align:center>BF16</td><td style=text-align:center>FP8 Mixed Precision</td></tr></tbody></table><h3 id=long-context-extension>Long Context Extension<a hidden class=anchor aria-hidden=true href=#long-context-extension>#</a></h3><p>Both models use the <strong>YaRN</strong> (<a href=https://arxiv.org/abs/2309.00071>Peng et al., 2023</a>) technique to extend the context window.</p><ul><li><strong>DeepSeek-V2:</strong> Extended from 4K to 128K. Used YaRN (scale \(s=40, \alpha=1, \beta=32\)), trained for 1000 steps on 32K sequence length. Adjusted length scaling factor \(\sqrt{t}=0.0707 \ln s+1\).</li><li><strong>DeepSeek-V3:</strong> Extended from 4K to 32K, then to 128K in two stages. Each stage trained for 1000 steps. YaRN parameters same as V2, length scaling factor \(\sqrt{t}=0.1 \ln s+1\). First stage sequence length 32K, second stage 128K.</li></ul><p>Both models demonstrated good long context capabilities in the NIAH test.</p><figure class=align-center><img loading=lazy src=deepseek_v2_niah.png#center alt="Fig. 19. Evaluation results on the &lsquo;Needle In A Haystack&rsquo; (NIAH) tests for DeepSeek-V2. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 19. Evaluation results on the &lsquo;Needle In A Haystack&rsquo; (NIAH) tests for DeepSeek-V2. (Image source: <a href=https://arxiv.org/abs/2405.04434>DeepSeek-AI, 2024</a>)</p></figcaption></figure><figure class=align-center><img loading=lazy src=deepseek_v3_niah.png#center alt="Fig. 20. Evaluation results on the &lsquo;Needle In A Haystack&rsquo; (NIAH) tests for DeepSeek-V3. (Image source: DeepSeek-AI, 2024)" width=100%><figcaption><p>Fig. 20. Evaluation results on the &lsquo;Needle In A Haystack&rsquo; (NIAH) tests for DeepSeek-V3. (Image source: <a href=https://arxiv.org/abs/2412.19437>DeepSeek-AI, 2024</a>)</p></figcaption></figure><h3 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h3><p><strong>DeepSeek-V2 Evaluation Results:</strong></p><p>Comparison of DeepSeek-V2 with representative open-source models (partial results). DeepSeek-V2 achieved state-of-the-art performance at the time with 21B activated parameters.</p><table><thead><tr><th style=text-align:center></th><th style=text-align:center>Benchmark (Metric)</th><th style=text-align:center># Shots</th><th style=text-align:center>DeepSeek 67B</th><th style=text-align:center>Qwen1.5 72B</th><th style=text-align:center>Mixtral 8x22B</th><th style=text-align:center>LLaMA 3 70B</th><th style=text-align:center>DeepSeek-V2</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center># Activated Params</td><td style=text-align:center>-</td><td style=text-align:center>67B</td><td style=text-align:center>72B</td><td style=text-align:center>39B</td><td style=text-align:center>70B</td><td style=text-align:center><strong>21B</strong></td></tr><tr><td style=text-align:center>English</td><td style=text-align:center>MMLU (<a href=https://arxiv.org/abs/2009.03300>Hendrycks et al., 2020</a>) (Acc.)</td><td style=text-align:center>5-shot</td><td style=text-align:center>71.3</td><td style=text-align:center>77.2</td><td style=text-align:center>77.6</td><td style=text-align:center>78.9</td><td style=text-align:center><strong>78.5</strong></td></tr><tr><td style=text-align:center>Code</td><td style=text-align:center>HumanEval (<a href=https://arxiv.org/abs/2107.03374>Chen et al., 2021</a>) (Pass@1)</td><td style=text-align:center>0-shot</td><td style=text-align:center>45.1</td><td style=text-align:center>43.9</td><td style=text-align:center>53.1</td><td style=text-align:center>48.2</td><td style=text-align:center><strong>48.8</strong></td></tr><tr><td style=text-align:center>Math</td><td style=text-align:center>GSM8K (<a href=https://arxiv.org/abs/2110.14168>Cobbe et al., 2021</a>) (EM)</td><td style=text-align:center>8-shot</td><td style=text-align:center>63.4</td><td style=text-align:center>77.9</td><td style=text-align:center>80.3</td><td style=text-align:center>83.0</td><td style=text-align:center><strong>79.2</strong></td></tr><tr><td style=text-align:center>Chinese</td><td style=text-align:center>C-Eval (<a href=https://arxiv.org/abs/2305.08322>Huang et al., 2023</a>) (Acc.)</td><td style=text-align:center>5-shot</td><td style=text-align:center>66.1</td><td style=text-align:center><strong>83.7</strong></td><td style=text-align:center>59.6</td><td style=text-align:center>67.5</td><td style=text-align:center>81.7</td></tr></tbody></table><p><strong>DeepSeek-V3 Evaluation Results:</strong></p><p>Comparison of DeepSeek-V3-Base with representative open-source models (partial results). DeepSeek-V3-Base became the strongest open-source model on most benchmarks, especially in code and math.</p><table><thead><tr><th style=text-align:center></th><th style=text-align:center>Benchmark (Metric)</th><th style=text-align:center># Shots</th><th style=text-align:center>DeepSeek-V2 Base</th><th style=text-align:center>Qwen2.5 72B Base</th><th style=text-align:center>LLaMA-3.1 405B Base</th><th style=text-align:center>DeepSeek-V3 Base</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center># Activated Params</td><td style=text-align:center>-</td><td style=text-align:center>21B</td><td style=text-align:center>72B</td><td style=text-align:center>405B</td><td style=text-align:center><strong>37B</strong></td></tr><tr><td style=text-align:center>English</td><td style=text-align:center>MMLU (<a href=https://arxiv.org/abs/2009.03300>Hendrycks et al., 2020</a>) (EM)</td><td style=text-align:center>5-shot</td><td style=text-align:center>78.4</td><td style=text-align:center>85.0</td><td style=text-align:center>84.4</td><td style=text-align:center><strong>87.1</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>MMLU-Pro (<a href=https://arxiv.org/abs/2406.01574>Wang et al., 2024</a>) (em)</td><td style=text-align:center>5-shot</td><td style=text-align:center>51.4</td><td style=text-align:center>58.3</td><td style=text-align:center>52.8</td><td style=text-align:center><strong>64.4</strong></td></tr><tr><td style=text-align:center>Code</td><td style=text-align:center>HumanEval (<a href=https://arxiv.org/abs/2107.03374>Chen et al., 2021</a>) (Pass@1)</td><td style=text-align:center>0-shot</td><td style=text-align:center>43.3</td><td style=text-align:center>53.0</td><td style=text-align:center>54.9</td><td style=text-align:center><strong>65.2</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>LiveCodeBench-Base (<a href=https://arxiv.org/abs/2403.07974>Jain et al., 2024</a>) (Pass@1)</td><td style=text-align:center>3-shot</td><td style=text-align:center>11.6</td><td style=text-align:center>12.9</td><td style=text-align:center>15.5</td><td style=text-align:center><strong>19.4</strong></td></tr><tr><td style=text-align:center>Math</td><td style=text-align:center>GSM8K (<a href=https://arxiv.org/abs/2110.14168>Cobbe et al., 2021</a>) (Em)</td><td style=text-align:center>8-shot</td><td style=text-align:center>81.6</td><td style=text-align:center>88.3</td><td style=text-align:center>83.5</td><td style=text-align:center><strong>89.3</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>MATH (<a href=https://arxiv.org/abs/2103.03874>Hendrycks et al., 2021</a>) (EM)</td><td style=text-align:center>4-shot</td><td style=text-align:center>43.4</td><td style=text-align:center>54.4</td><td style=text-align:center>49.0</td><td style=text-align:center><strong>61.6</strong></td></tr><tr><td style=text-align:center>Chinese</td><td style=text-align:center>C-Eval (<a href=https://arxiv.org/abs/2305.08322>Huang et al., 2023</a>) (EM)</td><td style=text-align:center>5-shot</td><td style=text-align:center>81.4</td><td style=text-align:center>89.2</td><td style=text-align:center>72.5</td><td style=text-align:center><strong>90.1</strong></td></tr><tr><td style=text-align:center>Multilingual</td><td style=text-align:center>MMMLU-non-English (<a href=https://huggingface.co/datasets/openai/MMMLU>OpenAI, 2024</a>) (em)</td><td style=text-align:center>5-shot</td><td style=text-align:center>64.0</td><td style=text-align:center>74.8</td><td style=text-align:center>73.8</td><td style=text-align:center><strong>79.4</strong></td></tr></tbody></table><p><strong>Summary:</strong> DeepSeek-V3-Base, leveraging its architectural innovations, larger training dataset, and efficient training methods, comprehensively surpassed DeepSeek-V2-Base and other top open-source models (including LLaMA-3.1 405B, whose total parameter count far exceeds V3&rsquo;s activated parameters).</p><h2 id=alignment>Alignment<a hidden class=anchor aria-hidden=true href=#alignment>#</a></h2><p>To enable the models to better understand instructions, follow human preferences, and enhance specific capabilities (like reasoning), both DeepSeek-V2 and V3 underwent Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL).</p><h3 id=supervised-fine-tuning>Supervised Fine-Tuning<a hidden class=anchor aria-hidden=true href=#supervised-fine-tuning>#</a></h3><ul><li><strong>DeepSeek-V2:</strong> Used approximately 1.5M instruction data points, including 1.2M helpfulness data and 0.3M safety data, focusing on improving data quality to reduce hallucinations and enhance writing ability.</li><li><strong>DeepSeek-V3:</strong><ul><li><strong>Reasoning Data:</strong> Utilized the internal DeepSeek-R1 model (<a href=https://arxiv.org/abs/2501.12948>Guo et al., 2025</a>) to generate reasoning processes (math, code, logic, etc.). Since R1 outputs could be overly long or poorly formatted, V3 adopted a <strong>knowledge distillation</strong> approach:<ol><li>Train domain expert models (e.g., code expert): Combined original SFT data with R1-generated long CoT data (with system prompts guiding reflection/verification) for SFT+RL training.</li><li>Use expert models to generate SFT data: The expert models learned during RL to blend R1&rsquo;s reasoning patterns with the conciseness of regular SFT data.</li><li>Rejection sampling: Filtered high-quality SFT data for the final V3 SFT.</li></ol></li><li><strong>Non-Reasoning Data:</strong> Generated using DeepSeek-V2.5 and verified by human annotators.</li><li><strong>SFT Setup:</strong> Fine-tuned for 2 epochs, with learning rate cosine decayed from \(5 \times 10^{-6}\) to \(1 \times 10^{-6}\). Employed sample packing and mask isolation.</li></ul></li></ul><h3 id=reinforcement-learning>Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#reinforcement-learning>#</a></h3><p>Both models used the <strong>Group Relative Policy Optimization (GRPO)</strong> algorithm (<a href=https://arxiv.org/abs/2402.03300>Shao et al., 2024</a>) for RL. GRPO is an Actor-Only method that estimates advantage \(A_i\) by comparing the relative quality of a group (\(G\)) of candidate outputs. This avoids training a Critic model of the same size as the policy model, saving costs.</p><p>GRPO objective function:</p>\[
\begin{gathered}
\mathcal{J}_{G R P O}(\theta)=\mathbb{E}\left[q \sim P(Q),\left\{o_{i}\right\}_{i=1}^{G} \sim \pi_{\theta_{o l d}}(O \mid q)\right] \\
\frac{1}{G} \sum_{i=1}^{G}\left(\min \left(\frac{\pi_{\theta}\left(o_{i} \mid q\right)}{\pi_{\theta_{o l d}}\left(o_{i} \mid q\right)} A_{i}, \operatorname{clip}\left(\frac{\pi_{\theta}\left(o_{i} \mid q\right)}{\pi_{\theta_{o l d}}\left(o_{i} \mid q\right)}, 1-\varepsilon, 1+\varepsilon\right) A_{i}\right)-\beta \mathbb{D}_{K L}\left(\pi_{\theta}| | \pi_{r e f}\right)\right),
\end{gathered}
\]<p>where the advantage \(A_i\) is obtained by standardizing the intra-group rewards \(r_i\):</p>\[
A_{i}=\frac{r_{i}-\operatorname{mean}\left(\left\{r_{1}, r_{2}, \cdots, r_{G}\right\}\right)}{\operatorname{std}\left(\left\{r_{1}, r_{2}, \cdots, r_{G}\right\}\right)}.
\]<p>The KL divergence penalty term uses the Schulman unbiased estimator:</p>\[
\mathbb{D}_{K L}\left(\pi_{\theta}| | \pi_{r e f}\right)=\frac{\pi_{r e f}\left(o_{i} \mid q\right)}{\pi_{\theta}\left(o_{i} \mid q\right)}-\log \frac{\pi_{r e f}\left(o_{i} \mid q\right)}{\pi_{\theta}\left(o_{i} \mid q\right)}-1.
\]<p><strong>Reward Model (RM):</strong></p><ul><li><strong>DeepSeek-V2:</strong> Employed a two-stage RL strategy.<ol><li><strong>Reasoning Alignment:</strong> Used a specially trained \(RM_{\text{reasoning}}\) to optimize for code and math reasoning tasks.</li><li><strong>Human Preference Alignment:</strong> Used a multi-reward framework combining \(RM_{\text{helpful}}\), \(RM_{\text{safety}}\), and rule-based \(RM_{\text{rule}}\).</li></ol></li><li><strong>DeepSeek-V3:</strong><ul><li><strong>Rule-based RM:</strong> For verifiable tasks (e.g., math answer format, LeetCode test cases), used rules to provide reliable rewards.</li><li><strong>Model-based RM:</strong> For free-form answers or tasks without standard answers (e.g., creative writing), used an RM initialized from the V3 SFT Checkpoint. This RM was trained on preference data with CoT to enhance reliability and reduce reward hacking risks.</li><li><strong>Self-Reward:</strong> V3 explored using the model&rsquo;s own judgment capabilities (enhanced via voting) as a feedback source, especially in general scenarios, combined with ideas from <strong>Constitutional AI</strong> (<a href=https://arxiv.org/abs/2212.08073>Bai et al., 2022</a>) for optimization.</li></ul></li></ul><p><strong>RL Training Optimization (V2/V3):</strong> Addressed the high resource demands of large-model RL through engineering optimizations like a hybrid engine (different parallelism strategies for training/inference), using <strong>vLLM</strong> (<a href=https://arxiv.org/abs/2309.06180>Kwon et al., 2023</a>) for accelerated sampling, CPU offloading scheduling, etc.</p><h3 id=evaluation-1>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation-1>#</a></h3><p><strong>DeepSeek-V2 Chat Evaluation:</strong></p><p>Comparison of DeepSeek-V2 Chat (SFT/RL) with representative open-source Chat models on open-ended generation tasks. V2 Chat (RL) performed exceptionally well on AlpacaEval 2.0 and AlignBench.</p><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>MT-Bench (<a href=https://arxiv.org/abs/2306.05685>Zheng et al., 2023</a>)</th><th style=text-align:center>AlpacaEval 2.0 (<a href=https://arxiv.org/abs/2404.04475>Dubois et al., 2024</a>) (LC Win Rate)</th><th style=text-align:center>AlignBench (<a href=https://doi.org/10.48550/arXiv.2311.18743>Liu et al., 2023</a>) (Chinese)</th></tr></thead><tbody><tr><td style=text-align:center>DeepSeek 67B Chat</td><td style=text-align:center>8.35</td><td style=text-align:center>16.6</td><td style=text-align:center>6.43</td></tr><tr><td style=text-align:center>Mistral 8x22B Instruct</td><td style=text-align:center>8.66</td><td style=text-align:center>30.9</td><td style=text-align:center>-</td></tr><tr><td style=text-align:center>Qwen1.5 72B Chat</td><td style=text-align:center>8.61</td><td style=text-align:center>36.6</td><td style=text-align:center>7.19</td></tr><tr><td style=text-align:center>LLaMA3 70B Instruct</td><td style=text-align:center><strong>8.95</strong></td><td style=text-align:center>34.4</td><td style=text-align:center>-</td></tr><tr><td style=text-align:center>DeepSeek-V2 Chat (SFT)</td><td style=text-align:center>8.62</td><td style=text-align:center>30.0</td><td style=text-align:center>7.74</td></tr><tr><td style=text-align:center>DeepSeek-V2 Chat (RL)</td><td style=text-align:center><strong>8.97</strong></td><td style=text-align:center><strong>38.9</strong></td><td style=text-align:center><strong>7.91</strong></td></tr></tbody></table><p><strong>DeepSeek-V3 Chat Evaluation:</strong></p><p>Comparison of DeepSeek-V3 Chat with representative open-source and closed-source Chat models (partial results). V3 leads open-source models on most benchmarks and is comparable to top closed-source models in code, math, Chinese, and open-ended generation tasks.</p><table><thead><tr><th style=text-align:center></th><th style=text-align:center>Benchmark (Metric)</th><th style=text-align:center>DeepSeek V2.5-0905</th><th style=text-align:center>Qwen2.5 72B-Inst.</th><th style=text-align:center>LLaMA-3.1 405B-Inst.</th><th style=text-align:center>Claude-3.5- Sonnet-1022</th><th style=text-align:center>GPT-4o 0513</th><th style=text-align:center>DeepSeek V3</th></tr></thead><tbody><tr><td style=text-align:center>English</td><td style=text-align:center>MMLU (<a href=https://arxiv.org/abs/2009.03300>Hendrycks et al., 2020</a>) (EM)</td><td style=text-align:center>80.6</td><td style=text-align:center>85.3</td><td style=text-align:center>88.6</td><td style=text-align:center>88.3</td><td style=text-align:center>87.2</td><td style=text-align:center><strong>88.5</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>MMLU-Pro (<a href=https://arxiv.org/abs/2406.01574>Wang et al., 2024</a>) (EM)</td><td style=text-align:center>66.2</td><td style=text-align:center>71.6</td><td style=text-align:center>73.3</td><td style=text-align:center><strong>78.0</strong></td><td style=text-align:center>72.6</td><td style=text-align:center>75.9</td></tr><tr><td style=text-align:center></td><td style=text-align:center>GPQA-Diamond (<a href=https://arxiv.org/abs/2311.12022>Rein et al., 2023</a>) (Pass@1)</td><td style=text-align:center>41.3</td><td style=text-align:center>49.0</td><td style=text-align:center>51.1</td><td style=text-align:center><strong>65.0</strong></td><td style=text-align:center>49.9</td><td style=text-align:center>59.1</td></tr><tr><td style=text-align:center></td><td style=text-align:center>SimpleQA (<a href=https://openai.com/index/introducing-simpleqa/>OpenAI, 2024c</a>) (Correct)</td><td style=text-align:center>10.2</td><td style=text-align:center>9.1</td><td style=text-align:center>17.1</td><td style=text-align:center>28.4</td><td style=text-align:center><strong>38.2</strong></td><td style=text-align:center>24.9</td></tr><tr><td style=text-align:center>Code</td><td style=text-align:center>HumanEval-Mul (Pass@1)</td><td style=text-align:center>77.4</td><td style=text-align:center>77.3</td><td style=text-align:center>77.2</td><td style=text-align:center>81.7</td><td style=text-align:center>80.5</td><td style=text-align:center><strong>82.6</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>LiveCodeBench (<a href=https://arxiv.org/abs/2403.07974>Jain et al., 2024</a>) (Pass@1-COT)</td><td style=text-align:center>29.2</td><td style=text-align:center>31.1</td><td style=text-align:center>28.4</td><td style=text-align:center>36.3</td><td style=text-align:center>33.4</td><td style=text-align:center><strong>40.5</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>SWE Verified (<a href=https://openai.com/index/introducing-swe-bench-verified/>OpenAI, 2024d</a>) (Resolved)</td><td style=text-align:center>22.6</td><td style=text-align:center>23.8</td><td style=text-align:center>24.5</td><td style=text-align:center><strong>50.8</strong></td><td style=text-align:center>38.8</td><td style=text-align:center>42.0</td></tr><tr><td style=text-align:center>Math</td><td style=text-align:center>AIME 2024 (<a href="https://artofproblemsolving.com/wiki/index.php/2024_AIME_I?srsltid=AfmBOooril84-FGuAUnzl8I-zXl8XG7P00X-BAkMG9x9RIzEWcXHlwWm">MAA, 2024</a> (Pass@1)</td><td style=text-align:center>16.7</td><td style=text-align:center>23.3</td><td style=text-align:center>23.3</td><td style=text-align:center>16.0</td><td style=text-align:center>9.3</td><td style=text-align:center><strong>39.2</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>MATH-500 (<a href=https://arxiv.org/abs/2103.03874>Hendrycks et al., 2021</a>) (ЕМ)</td><td style=text-align:center>74.7</td><td style=text-align:center>80.0</td><td style=text-align:center>73.8</td><td style=text-align:center>78.3</td><td style=text-align:center>74.6</td><td style=text-align:center><strong>90.2</strong></td></tr><tr><td style=text-align:center>Chinese</td><td style=text-align:center>C-Eval (<a href=https://arxiv.org/abs/2305.08322>Huang et al., 2023</a>) (EM)</td><td style=text-align:center>79.5</td><td style=text-align:center>86.1</td><td style=text-align:center>61.5</td><td style=text-align:center>76.7</td><td style=text-align:center>76.0</td><td style=text-align:center><strong>86.5</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>C-SimpleQA (<a href=https://arxiv.org/abs/2411.07140>He et al., 2024</a>) (Correct)</td><td style=text-align:center>54.1</td><td style=text-align:center>48.4</td><td style=text-align:center>50.4</td><td style=text-align:center>51.3</td><td style=text-align:center>59.3</td><td style=text-align:center><strong>64.8</strong></td></tr><tr><td style=text-align:center>Open-Ended</td><td style=text-align:center>Arena-Hard (<a href=https://arxiv.org/abs/2406.11939>Li et al., 2024</a>)</td><td style=text-align:center>76.2</td><td style=text-align:center>81.2</td><td style=text-align:center>69.3</td><td style=text-align:center>85.2</td><td style=text-align:center>80.4</td><td style=text-align:center><strong>85.5</strong></td></tr><tr><td style=text-align:center></td><td style=text-align:center>AlpacaEval 2.0 (<a href=https://arxiv.org/abs/2404.04475>Dubois et al., 2024</a>) (LC Win Rate)</td><td style=text-align:center>50.5</td><td style=text-align:center>49.1</td><td style=text-align:center>40.5</td><td style=text-align:center>52.0</td><td style=text-align:center>51.1</td><td style=text-align:center><strong>70.0</strong></td></tr></tbody></table><p><strong>Summary:</strong></p><ul><li>DeepSeek-V2 Chat (RL) was already a top-tier open-source chat model at its release, particularly excelling on AlpacaEval and the Chinese AlignBench.</li><li>DeepSeek-V3 Chat further boosted performance, becoming the current strongest open-source chat model. It shows extremely strong performance in code, math, Chinese knowledge, and open-ended evaluations like Arena-Hard (<a href=https://arxiv.org/abs/2406.11939>Li et al., 2024</a>) and AlpacaEval, reaching levels comparable to GPT-4o and Claude-3.5-Sonnet.</li><li>V3&rsquo;s R1 distillation significantly improved reasoning capabilities but might also increase response length, requiring a trade-off between accuracy and efficiency.</li><li>V3&rsquo;s self-reward capability (strong performance on RewardBench (<a href=https://arxiv.org/abs/2403.13787>Lambert et al., 2024</a>)) provides an effective pathway for continuous alignment.</li></ul><h2 id=discussion>Discussion<a hidden class=anchor aria-hidden=true href=#discussion>#</a></h2><ul><li><strong>Load Balancing Strategy Evolution:</strong> The shift from V2&rsquo;s auxiliary losses to V3&rsquo;s auxiliary-loss-free + bias adjustment reflects a trend towards minimizing interference with model performance while ensuring load balance. Batch-level balancing, compared to sequence-level, better facilitates expert specialization.</li><li><strong>Effectiveness of MTP:</strong> V3&rsquo;s experiments demonstrate that multi-token prediction as an auxiliary training objective indeed improves model performance on standard evaluation tasks, while also offering potential for inference acceleration (speculative decoding).</li><li><strong>R1 Distillation:</strong> V3 successfully distilled the long-chain reasoning capabilities of DeepSeek-R1 into a standard LLM, significantly boosting math and code abilities. This is an important technical direction, though controlling generation length needs attention.</li><li><strong>Self-Reward:</strong> V3&rsquo;s strong judgment capability (evidenced by <strong>RewardBench</strong> results (<a href=https://arxiv.org/abs/2403.13787>Lambert et al., 2024</a>)) enables effective self-feedback and self-alignment. This is crucial for reducing reliance on human annotation and achieving continuous model self-improvement.</li><li><strong>SFT Data Quantity:</strong> While <strong>LIMA</strong> (<a href=https://arxiv.org/abs/2305.11206>Zhou et al., 2024</a>) suggested that a small amount of high-quality SFT data can achieve good results, sufficient high-quality data is still necessary for specific skills (like instruction following, IFEval) to reach satisfactory performance.</li><li><strong>Alignment Tax:</strong> OpenAI noted in <strong>InstructGPT</strong> (<a href=https://arxiv.org/pdf/2203.02155>Ouyang et al., 2022</a>) that RL alignment, while improving open-ended generation capabilities, might sacrifice performance on some standard benchmarks. Both V2 and V3 made efforts in data processing and training strategies to mitigate this issue and achieve an acceptable balance.</li></ul><h2 id=conclusion-limitations--future-directions>Conclusion, Limitations & Future Directions<a hidden class=anchor aria-hidden=true href=#conclusion-limitations--future-directions>#</a></h2><h3 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h3><p>DeepSeek-V2 and DeepSeek-V3 are two powerful, economical, and efficient MoE language models. Through innovations like the MLA and DeepSeekMoE architectures, along with V3&rsquo;s introduction of auxiliary-loss-free load balancing, MTP, FP8 training, and R1 distillation, they have achieved breakthroughs in performance, training cost, and inference efficiency. DeepSeek-V3 has become one of the strongest open-source models currently available, with performance competitive with top closed-source models.</p><h3 id=limitations>Limitations<a hidden class=anchor aria-hidden=true href=#limitations>#</a></h3><ul><li><strong>General LLM Limitations:</strong> Such as knowledge cutoffs, hallucinations, factual errors, etc.</li><li><strong>Language Coverage:</strong> Primarily focused on Chinese and English, with limited capabilities in other languages (V2). V3 expanded multilingual support but remains predominantly focused on Chinese and English.</li><li><strong>Deployment Threshold (V3):</strong> Efficient inference requires relatively large deployment units (multi-node), which might be challenging for smaller teams.</li><li><strong>Inference Efficiency:</strong> Although V3&rsquo;s inference efficiency improved over V2, there is still room for optimization.</li></ul><h3 id=future-directions>Future Directions<a hidden class=anchor aria-hidden=true href=#future-directions>#</a></h3><ul><li><strong>Architectural Innovation:</strong> Continue optimizing MoE architectures, exploring new architectures supporting infinite context and overcoming Transformer limitations.</li><li><strong>Data Expansion:</strong> Improve the quantity, quality, and dimensionality (multimodality, etc.) of training data.</li><li><strong>Deeper Reasoning:</strong> Enhance the model&rsquo;s reasoning length and depth, increasing intelligence levels.</li><li><strong>Evaluation Methods:</strong> Develop more comprehensive, multi-dimensional evaluation methods to avoid overfitting to specific benchmarks.</li><li><strong>Alignment and Safety:</strong> Continuously improve alignment techniques (e.g., self-reward) to ensure models are helpful, honest, harmless, and aligned with human values.</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Liu, Aixin, et al. <a href=https://arxiv.org/abs/2405.04434>&ldquo;Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.&rdquo;</a> arXiv preprint arXiv:2405.04434 (2024).</p><p>[2] Liu, Aixin, et al. <a href=https://arxiv.org/abs/2412.19437>&ldquo;Deepseek-v3 technical report.&rdquo;</a> arXiv preprint arXiv:2412.19437 (2024).</p><p>[3] Dai, Damai, et al. <a href=https://arxiv.org/abs/2401.06066>&ldquo;Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.&rdquo;</a> arXiv preprint arXiv:2401.06066 (2024).</p><p>[4] Wang, Lean, et al. <a href=https://arxiv.org/abs/2408.15664>&ldquo;Auxiliary-loss-free load balancing strategy for mixture-of-experts.&rdquo;</a> arXiv preprint arXiv:2408.15664 (2024).</p><p>[5] Gloeckle, Fabian, et al. <a href=https://arxiv.org/abs/2404.19737>&ldquo;Better & faster large language models via multi-token prediction.&rdquo;</a> Proceedings of the 41st International Conference on Machine Learning. PMLR 235:16821-16841 (2024).</p><p>[6] Vaswani, Ashish, et al. <a href=https://arxiv.org/abs/1706.03762>&ldquo;Attention is all you need.&rdquo;</a> Advances in neural information processing systems 30 (2017).</p><p>[7] Shazeer, Noam. <a href=https://arxiv.org/abs/1911.02150>&ldquo;Fast transformer decoding: One write-head is all you need.&rdquo;</a> arXiv preprint arXiv:1911.02150 (2019).</p><p>[8] Ainslie, Joshua, et al. <a href=https://arxiv.org/abs/2305.13245>&ldquo;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.&rdquo;</a> Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4895-4901 (2023).</p><p>[9] Su, Jianlin, et al. <a href=https://arxiv.org/abs/2104.09864>&ldquo;Roformer: Enhanced transformer with rotary position embedding.&rdquo;</a> Neurocomputing 568 (2024): 127063.</p><p>[10] Shazeer, Noam, et al. <a href=https://arxiv.org/abs/1701.06538>&ldquo;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.&rdquo;</a> arXiv preprint arXiv:1701.06538 (2017).</p><p>[11] Lepikhin, Dmitry, et al. <a href=https://arxiv.org/abs/2006.16668>&ldquo;Gshard: Scaling giant models with conditional computation and automatic sharding.&rdquo;</a> arXiv preprint arXiv:2006.16668 (2020).</p><p>[12] Fedus, William, Barret Zoph, and Noam Shazeer. <a href=https://arxiv.org/abs/2101.03961>&ldquo;Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.&rdquo;</a> The Journal of Machine Learning Research 23.1: 5232-5270 (2022).</p><p>[13] Zhou, Zexuan, et al. <a href=https://arxiv.org/abs/2202.09368>&ldquo;Mixture-of-experts with expert choice routing.&rdquo;</a> Advances in Neural Information Processing Systems 35: 7103-7114 (2022).</p><p>[14] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. <a href=https://arxiv.org/abs/2211.17192>&ldquo;Fast inference from transformers via speculative decoding.&rdquo;</a> Proceedings of the 40th International Conference on Machine Learning. PMLR 202:19274-19286 (2023).</p><p>[15] Xia, Yichao, et al. <a href=https://arxiv.org/abs/2302.01318>&ldquo;Accelerating large language model decoding with speculative sampling.&rdquo;</a> arXiv preprint arXiv:2302.01318 (2023).</p><p>[16] Qi, Hai, et al. <a href=https://arxiv.org/abs/2401.10241>&ldquo;ZeroBubble: A High-Performance Framework for Training Mixture-of-Experts Models.&rdquo;</a> arXiv preprint arXiv:2401.10241 (2024).</p><p>[17] Rajbhandari, Samyam, et al. <a href=https://arxiv.org/abs/1910.02054>&ldquo;Zero: Memory optimizations toward training trillion parameter models.&rdquo;</a> SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE (2020).</p><p>[18] Harlap, Aaron, et al. <a href=https://arxiv.org/abs/1806.03377>&ldquo;Pipedream: Fast and efficient pipeline parallel dnn training.&rdquo;</a> arXiv preprint arXiv:1806.03377 (2018).</p><p>[19] Li, Shigang, and Torsten Hoefler. <a href=https://arxiv.org/abs/2107.06925>&ldquo;Chimera: Efficiently training large-scale neural networks with bidirectional pipelines.&rdquo;</a> Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.</p><p>[20] Bauer, Michael, Sean Treichler, and Alex Aiken. <a href=https://dl.acm.org/doi/10.1145/2692916.2555258>&ldquo;Singe: Leveraging warp specialization for high performance on gpus.&rdquo;</a> Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming. 2014.</p><p>[21] Dettmers, Tim, et al. <a href=https://arxiv.org/abs/2208.07339>&ldquo;Llm. int8 (): 8-bit matrix multiplication for transformers at scale.&rdquo;</a> Advances in Neural Information Processing Systems 35: 34138-34151 (2022).</p><p>[22] Noune, Badreddine, et al. <a href=https://arxiv.org/abs/2206.02915>&ldquo;8-bit numerical formats for deep neural networks.&rdquo;</a> arXiv preprint arXiv:2206.02915 (2022).</p><p>[23] Peng, Houwen, et al. <a href=https://arxiv.org/abs/2310.18313>&ldquo;FP8-LM: Training FP8 Large Language Models.&rdquo;</a> arXiv preprint arXiv:2310.18313 (2023).</p><p>[24] Fishman, Maxim, et al. <a href=https://arxiv.org/abs/2409.12517>&ldquo;Scaling FP8 training to trillion-token LLMs.&rdquo;</a>) arXiv preprint arXiv:2409.12517 (2024).</p><p>[25] He, Bobby, et al. <a href=https://arxiv.org/abs/2405.19279>&ldquo;Understanding and minimising outlier features in neural network training.&rdquo;</a> arXiv preprint arXiv:2405.19279 (2024).</p><p>[26] Sun, Xiao, et al. <a href=https://arxiv.org/abs/2402.17762>&ldquo;Massive activations in large language models.&rdquo;</a> arXiv preprint arXiv:2402.17762 (2024).</p><p>[27] NVIDIA. <a href=https://github.com/NVIDIA/TransformerEngine>&ldquo;Transformer Engine.&rdquo;</a> GitHub Repository (Accessed 2024).</p><p>[28] Sun, Xiao, et al. <a href=https://papers.nips.cc/paper_files/paper/2019/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html>&ldquo;Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks.&rdquo;</a> Advances in neural information processing systems 32 (2019).</p><p>[29] Loshchilov, Ilya, and Frank Hutter. <a href=https://arxiv.org/abs/1711.05101>&ldquo;Decoupled weight decay regularization.&rdquo;</a> arXiv preprint arXiv:1711.05101 (2017).</p><p>[30] NVIDIA. <a href=https://developer.nvidia.com/blog/gpudirect-storage/>&ldquo;GPUDirect Storage: A Direct Path Between Storage and GPU Memory.&rdquo;</a> NVIDIA Developer Blog (2022).</p><p>[31] Graham, Richard L., et al. <a href=https://network.nvidia.com/pdf/solutions/hpc/paperieee_copyright.pdf>&ldquo;Scalable hierarchical aggregation protocol (SHArP): A hardware architecture for efficient data reduction.&rdquo;</a> 2016 First International Workshop on Communication Optimizations in HPC (COMHPC). IEEE, 2016.</p><p>[32] Ding, Yiran, et al. <a href=https://arxiv.org/abs/2402.13753>&ldquo;Longrope: Extending llm context window beyond 2 million tokens.&rdquo;</a> arXiv preprint arXiv:2402.13753 (2024).</p><p>[33] Zhu, Qihao, et al. <a href=https://arxiv.org/abs/2406.11931>&ldquo;DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence.&rdquo;</a> arXiv preprint arXiv:2406.11931 (2024).</p><p>[34] Lundberg, Scott M. <a href=https://github.com/guidance-ai/guidance/blob/main/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb>&ldquo;Guidance: Prompt Boundaries and Token Healing.&rdquo;</a> GitHub Notebook (2023).</p><p>[35] Peng, Bowen, et al. <a href=https://arxiv.org/abs/2309.00071>&ldquo;YaRN: Efficient Context Window Extension of Large Language Models.&rdquo;</a> arXiv preprint arXiv:2309.00071 (2023).</p><p>[36] Hendrycks, Dan, et al. <a href=https://arxiv.org/abs/2009.03300>&ldquo;Measuring massive multitask language understanding.&rdquo;</a> arXiv preprint arXiv:2009.03300 (2020).</p><p>[37] Chen, Mark, et al. <a href=https://arxiv.org/abs/2107.03374>&ldquo;Evaluating large language models trained on code.&rdquo;</a> arXiv preprint arXiv:2107.03374 (2021).</p><p>[38] Cobbe, Karl, et al. <a href=https://arxiv.org/abs/2110.14168>&ldquo;Training verifiers to solve math word problems.&rdquo;</a> arXiv preprint arXiv:2110.14168 (2021).</p><p>[39] Huang, Yuzhen, et al. <a href=https://arxiv.org/abs/2305.08322>&ldquo;C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 62991-63010.</p><p>[40] Wang, Yubo, et al. <a href=https://arxiv.org/abs/2406.01574>&ldquo;Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.&rdquo;</a> The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024.</p><p>[41] Jain, Naman, et al. <a href=https://arxiv.org/abs/2403.07974>&ldquo;LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code.&rdquo;</a> arXiv preprint arXiv:2403.07974 (2024).</p><p>[42] Hendrycks, Dan, et al. <a href=https://arxiv.org/abs/2103.03874>&ldquo;Measuring mathematical problem solving with the math dataset.&rdquo;</a> arXiv preprint arXiv:2103.03874 (2021).</p><p>[43] OpenAI. <a href=https://huggingface.co/datasets/openai/MMMLU>&ldquo;MMMLU Dataset.&rdquo;</a> Hugging Face Datasets (Accessed 2024).</p><p>[44] Guo, Daya, et al. <a href=https://arxiv.org/abs/2501.12948>&ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.&rdquo;</a> arXiv preprint arXiv:2501.12948 (2025).</p><p>[45] Shao, Zhihong, et al. <a href=https://arxiv.org/abs/2402.03300>&ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.&rdquo;</a> arXiv preprint arXiv:2402.03300 (2024).</p><p>[46] Bai, Yuntao, et al. <a href=https://arxiv.org/abs/2212.08073>&ldquo;Constitutional ai: Harmlessness from ai feedback.&rdquo;</a> arXiv preprint arXiv:2212.08073 (2022).</p><p>[47] Kwon, Woosuk, et al. <a href=https://arxiv.org/abs/2309.06180>&ldquo;Efficient memory management for large language model serving with pagedattention.&rdquo;</a> Proceedings of the 29th Symposium on Operating Systems Principles. 2023.</p><p>[48] Zheng, Lianmin, et al. <a href=https://arxiv.org/abs/2306.05685>&ldquo;Judging llm-as-a-judge with mt-bench and chatbot arena.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 46595-46623.</p><p>[49] Dubois, Yann, et al. <a href=https://arxiv.org/abs/2404.04475>&ldquo;Length-controlled alpacaeval: A simple way to debias automatic evaluators.&rdquo;</a> arXiv preprint arXiv:2404.04475 (2024).</p><p>[50] Liu, Xiao, et al. <a href=https://arxiv.org/abs/2311.18743>&ldquo;Alignbench: Benchmarking chinese alignment of large language models.&rdquo;</a> arXiv preprint arXiv:2311.18743 (2023).</p><p>[51] Rein, David, et al. <a href=https://arxiv.org/abs/2311.12022>&ldquo;GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark.&rdquo;</a> First Conference on Language Modeling. 2024.</p><p>[52] OpenAI. <a href=https://openai.com/index/introducing-simpleqa/>&ldquo;Introducing SimpleQA&rdquo;</a> OpenAI Blog (2024).</p><p>[53] OpenAI. <a href=https://openai.com/index/introducing-swe-bench-verified/>&ldquo;Introducing SWE-bench Verified&rdquo;</a> OpenAI Blog (2024).</p><p>[54] Mathematical Association of America (MAA). <a href=https://artofproblemsolving.com/wiki/index.php/2024_AIME_I>&ldquo;2024 AIME I Problems.&rdquo;</a> Art of Problem Solving Wiki (2024).</p><p>[55] Li, Tianle, et al. <a href=https://arxiv.org/abs/2406.11939>&ldquo;From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.&rdquo;</a> arXiv preprint arXiv:2406.11939 (2024).</p><p>[56] Lambert, Nathan, et al. <a href=https://arxiv.org/abs/2403.13787>&ldquo;RewardBench: Evaluating Reward Models for Language Modeling.&rdquo;</a> arXiv preprint arXiv:2403.13787 (2024).</p><p>[57] Zhou, Chunting, et al. <a href=https://arxiv.org/abs/2305.11206>&ldquo;Lima: Less is more for alignment.&rdquo;</a> Advances in Neural Information Processing Systems 36 (2023): 55006-55021.</p><p>[58] Ouyang, Long, et al. <a href=https://arxiv.org/abs/2203.02155>&ldquo;Training language models to follow instructions with human feedback.&rdquo;</a> Advances in neural information processing systems 35 (2022): 27730-27744.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reproducing or citing the content of this article, please indicate the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Apr 2025). DeepSeek-V2 vs V3.
<a href=https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3>https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025deepseekv2v3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;DeepSeek-V2 vs V3&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Apr&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/tags/deepseek-v2/>DeepSeek-V2</a></li><li><a href=https://syhya.github.io/tags/deepseek-v3/>DeepSeek-V3</a></li><li><a href=https://syhya.github.io/tags/moe/>MoE</a></li><li><a href=https://syhya.github.io/tags/transformer/>Transformer</a></li><li><a href=https://syhya.github.io/tags/mla/>MLA</a></li><li><a href=https://syhya.github.io/tags/deepseekmoe/>DeepSeekMoE</a></li><li><a href=https://syhya.github.io/tags/mtp/>MTP</a></li><li><a href=https://syhya.github.io/tags/fp8-training/>FP8 Training</a></li><li><a href=https://syhya.github.io/tags/grpo/>GRPO</a></li><li><a href=https://syhya.github.io/tags/sft/>SFT</a></li><li><a href=https://syhya.github.io/tags/rl/>RL</a></li><li><a href=https://syhya.github.io/tags/kv-cache/>KV Cache</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-05-04-multimodal-llm/><span class=title>« Prev</span><br><span>Multimodal Large Language Models</span>
</a><a class=next href=https://syhya.github.io/posts/2025-04-06-llama/><span class=title>Next »</span><br><span>The LLaMA Herd</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on x" href="https://x.com/intent/tweet/?text=DeepSeek-V2%20vs%20V3&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-04-18-deepseek-v2-v3%2f&amp;hashtags=DeepLearning%2cAI%2cLLM%2cDeepSeek-V2%2cDeepSeek-V3%2cMoE%2cTransformer%2cMLA%2cDeepSeekMoE%2cMTP%2cFP8Training%2cGRPO%2cSFT%2cRL%2cKVCache"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-04-18-deepseek-v2-v3%2f&amp;title=DeepSeek-V2%20vs%20V3&amp;summary=DeepSeek-V2%20vs%20V3&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-04-18-deepseek-v2-v3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-04-18-deepseek-v2-v3%2f&title=DeepSeek-V2%20vs%20V3"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-04-18-deepseek-v2-v3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on whatsapp" href="https://api.whatsapp.com/send?text=DeepSeek-V2%20vs%20V3%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-04-18-deepseek-v2-v3%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on telegram" href="https://telegram.me/share/url?text=DeepSeek-V2%20vs%20V3&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-04-18-deepseek-v2-v3%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share DeepSeek-V2 vs V3 on ycombinator" href="https://news.ycombinator.com/submitlink?t=DeepSeek-V2%20vs%20V3&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-04-18-deepseek-v2-v3%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>