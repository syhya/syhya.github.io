<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>基于双卡 RTX 4090 搭建家用深度学习主机 | Yue Shui 博客</title><meta name=keywords content="深度学习,AI,LLM,RTX 4090,AI硬件,显卡"><meta name=description content="租用 GPU 还是购买 GPU？
在构建深度学习工作环境之前，首先需要综合考虑 使用周期、预算、数据隐私 以及 维护成本。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><link rel=mask-icon href=https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2024-12-21-build-gpu-server/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/"><meta property="og:site_name" content="Yue Shui 博客"><meta property="og:title" content="基于双卡 RTX 4090 搭建家用深度学习主机"><meta property="og:description" content="租用 GPU 还是购买 GPU？ 在构建深度学习工作环境之前，首先需要综合考虑 使用周期、预算、数据隐私 以及 维护成本。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-21T12:00:00+08:00"><meta property="article:modified_time" content="2025-06-29T21:43:56+08:00"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="article:tag" content="RTX 4090"><meta property="article:tag" content="AI硬件"><meta property="article:tag" content="显卡"><meta property="og:image" content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E"><meta name=twitter:title content="基于双卡 RTX 4090 搭建家用深度学习主机"><meta name=twitter:description content="租用 GPU 还是购买 GPU？
在构建深度学习工作环境之前，首先需要综合考虑 使用周期、预算、数据隐私 以及 维护成本。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/zh/posts/"},{"@type":"ListItem","position":2,"name":"基于双卡 RTX 4090 搭建家用深度学习主机","item":"https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"基于双卡 RTX 4090 搭建家用深度学习主机","name":"基于双卡 RTX 4090 搭建家用深度学习主机","description":"租用 GPU 还是购买 GPU？ 在构建深度学习工作环境之前，首先需要综合考虑 使用周期、预算、数据隐私 以及 维护成本。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。\n","keywords":["深度学习","AI","LLM","RTX 4090","AI硬件","显卡"],"articleBody":"租用 GPU 还是购买 GPU？ 在构建深度学习工作环境之前，首先需要综合考虑 使用周期、预算、数据隐私 以及 维护成本。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。\n租用 GPU 的优点：\n无需一次性投入高额硬件成本 可根据项目需求弹性扩容 云厂商通常提供数据合规与安全保障，省去硬件运维烦恼 购买 GPU 的优点：\n长期大规模使用时，整体成本更低 对内部数据和模型有更高的隐私与可控性 硬件可随时调整、升级，部署更灵活 个人建议\n如果预算有限或只是初学阶段，可先使用 Colab、Kaggle 或云 GPU； 当算力需求和隐私需求上升时，再考虑自建多卡服务器或租用多机多卡集群。 背景 在 2023 年 9 月，为了在工作之余继续对大模型（LLM）进行探索和研究，我组装了一台 双 RTX 4090 的个人 AI 实验服务器。该服务器已运行近一年，整体体验如下：\n噪音：服务器放在脚边，满负荷训练时风扇噪音较大，但在日常推理或中等负载下可接受 推理性能：双卡共计 48GB 显存，采用 4bit 量化方案时可运行到 70B 级别的模型（如 Llama 70B、Qwen 72B） 训练性能：在使用 DeepSpeed 的分布式和 offload 技术（ZeRO-3 + CPU offload）后，可对 34B 左右的模型（如 CodeLlama 34B）进行微调 性价比：对于个人或小团队的日常实验和中小规模模型训练而言，该配置较为实用；但若进行超大规模模型的全参数训练，仍需更多专业卡（如多卡 A100 或 H100 集群） 下图展示了不同大小模型、不同训练方法对显存的需求： Fig. 1. Hardware Requirement. (Image source: LLaMA-Factory)\n搭建思路与配置详情 整机预算在 4 万元人民币（约 6000 美元） 左右，以下是我最终选用的配置清单，仅供参考：\n配件 型号 价格 (元) 显卡 RTX 4090 * 2 25098 主板 + CPU AMD R9 7900X + 微星 MPG X670E CARBON 5157.55 内存 美商海盗船(USCORSAIR) 48GB*2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + 三星 990PRO 4TB 4587 电源 美商海盗船 AX1600i 2699 风扇 追风者 T30 12cm P * 6 1066.76 散热 利民 Thermalright FC140 BLACK 419 机箱 PHANTEKS 追风者 620PC 全塔 897.99 显卡延长线 追风者 FL60 PCI-E4.0 *16 399 总计：约 42723.3 元\nGPU 选择 对于大模型研究，浮点运算性能（TFLOPS） 和 显存容量 是最核心的指标。专业卡（A100、H100 等）虽有更高显存以及 NVLink，但价格动辄数十万人民币，对个人用户并不友好。根据 Tim Dettmers 的调研，RTX 4090 在单位美元算力方面表现非常亮眼，且支持 BF16、Flash Attention 等新特性，因此成为高性价比的选择。\n散热方式：涡轮 vs 风冷 vs 水冷 散热方式 优点 缺点 适用场景 涡轮风扇 体积紧凑；适合并行多卡部署 噪音大、整体散热效率一般 企业服务器机柜、多卡密集部署 风冷散热 性能与噪音平衡佳、维护简单 显卡体型通常较大 家用或个人研究（主机摆放空间足够） 水冷散热 散热能力突出、满载噪音更低 可能会出现漏液、价格更高 对静音要求极高或极限超频场景 家用推荐：风冷卡 兼顾散热效率、噪音和维护成本；相对于涡轮卡和水冷卡更友好。\nCPU \u0026 主板 在深度学习场景中，CPU 主要负责数据预处理、管道调度以及多进程/多线程并行管理，确保数据能够以高吞吐量、低延迟的方式传递到 GPU。因此，CPU 的核心需求主要包括 充足的 PCIe 通道 和 卓越的多线程性能。\nIntel：13/14 代 i9（如 13900K）拥有 20 条 PCIe 主通道，能够满足双卡 x8 + x8 的需求 AMD：Ryzen 7000/9000 系列（如 7950X）提供 28 条（可用 24 条）PCIe 通道，支持双卡 x8 + x8，并为 M.2 SSD 提供足够带宽 微星 MPG X670E CARBON 主板 扩展性：支持 PCIe 5.0 和 DDR5 内存，具备充足的未来升级空间 稳定性：高规格供电设计，保障 CPU 与多显卡的稳定运行 接口丰富：支持多块 M.2 SSD 和 USB4，满足多样化使用需求 AMD Ryzen 9 7900X 特点 核心与线程：12 核心、24 线程，在深度学习场景中的数据预处理和多任务处理方面表现强劲 PCIe 带宽：提供 28 条（可用 24 条）PCIe 5.0 通道，可轻松支持双卡 x8 + x8，并为 M.2 SSD 提供高速带宽 能效比：基于 Zen 4 架构，性能与能耗平衡优秀，适合高性能计算需求 主板选购要点 空间布局 RTX 4090 尺寸庞大且卡槽较厚，需确认主板是否能同时容纳两张显卡；若存在空间或散热冲突，可使用显卡延长线竖放第二张卡。 PCIe 通道拆分 主板需至少支持双 PCIe 4.0 x8 + x8 配置，以避免出现 x16 + x2 的情况。x16 + x2 的带宽分配会显著限制第二块 GPU 的数据传输能力，进而影响 GPU 与 CPU 之间的数据交换效率。在大模型训练中，这种带宽瓶颈可能导致性能显著下降，严重影响整体训练效率。 扩展性 在双显卡插满的情况下，仍需确保主板具有足够的 M.2 SSD 插槽和外设接口 综合扩展性、性能与性价比等因素，我最终选择了 AMD Ryzen 9 7900X 搭配 微星 MPG X670E CARBON 主板 的组合。通过显卡延长线解决了 4090 双卡过厚导致的插槽冲突问题。\nBIOS 设置建议 内存优化 开启 XMP/EXPO（对应 Intel/AMD）以提升内存频率，增强带宽性能。 超频调整 如果需要进一步提升性能，可在 BIOS 中启用 PBO（Precision Boost Overdrive） 或 Intel Performance Tuning，并结合系统监控工具观察稳定性。 温度与稳定性 避免过度超频，注意控制温度，避免因崩溃或过热导致系统不稳定。 内存 深度学习训练中，内存会被大量占用用于数据加载、模型优化状态储存（尤其在多 GPU Zero-stage 并行场景下）。内存容量最好 ≥ 显存总容量的两倍。本配置中，使用了 48GB * 2（共 96GB），满足日常多任务和分布式训练的需求，减少内存不足导致的频繁 swap。\n硬盘 优先选用 M.2 NVMe SSD：其读写性能更优，对加载超大模型权重、缓存中间文件、训练日志等都有显著速度提升 容量建议 ≥ 2TB：随着大模型文件越来越庞大，2TB 往往很快就会被占满，建议根据自身需求选 4TB 或更多 SSD 品牌：三星、海力士或西部数据等主流大厂都拥有稳定的高端产品线 电源 双 4090 满载时整机功耗可达 900W~1000W 左右，CPU、主板和硬盘等还需额外功率余量。通常建议选择 1500W 以上 的铂金或钛金电源，以确保在高负载下电流供给稳定、降低电压波动带来的系统不稳定。\n我在此使用美商海盗船 AX1600i（数字电源），可以通过软件监控实时功耗，并提供充足冗余。\n散热与风扇 我采用 风冷 方案，包括：\nCPU 散热器：利民 FC140（双塔式气冷方案，兼顾了较高的散热效率和相对低噪音） 机箱风扇：追风者 T30 12cm * 6，保持机箱内部正压或者稍微正压的风道布局，保证显卡和供电模块的进风顺畅 在 GPU 长时间高负载训练（如分布式训练大型模型）时，机箱内的风道管理和风扇配置非常重要。建议使用监控软件及时查看 CPU、GPU、主板供电模块温度，适度调节风扇转速。\n散热进阶\n若对静音有更高要求，可考虑 Hybrid 散热（半水冷方案）或更精细的风扇调速曲线。 适度清理机箱灰尘、使用防尘网并定期更换导热硅脂也能提升散热和稳定性。 机箱 RTX 4090 体型巨大，且双卡堆叠时需要充足的内部空间和散热风道。全塔机箱能提供更好的走线空间和气流组织。我选用了 PHANTEKS 追风者 620PC，除了体型大、空间充裕外，也自带良好的线缆管理通道。\n装机完成后的图片如下：\nFig. 2. Computer\n系统与软件环境 操作系统方面强烈推荐 Linux，例如 Ubuntu 22.04 LTS，因其对 CUDA、NVIDIA 驱动以及常见深度学习框架有更好的支持和兼容性。大致流程如下：\n安装 OS：使用 Ubuntu 或其他 Linux 系统即可。 安装 NVIDIA 驱动：确保 nvidia-smi 能正确识别两张 4090:\nFig. 3. nvidia-smi Output\n安装 CUDA 工具链：通过 nvcc -V 确认版本信息:\nFig. 4. nvcc -V Output\n安装 cuDNN：确保深度学习框架可以调用 GPU 加速卷积和 RNN 等操作 测试框架：使用 PyTorch、TensorFlow 或 JAX 简单测试模型推理/训练是否正常 Docker 容器化： 利用 nvidia-container-toolkit 让容器直接访问 GPU 资源，避免主机环境污染。 在多机多卡环境下，还能结合 Kubernetes、Ray 或 Slurm 等进行集群调度与资源管理。 常用工具与框架推荐 训练框架\nLLaMA-Factory：对大语言模型训练/推理流程有较好封装，新手友好 DeepSpeed：支持大模型分布式训练、多种并行策略和优化功能 Megatron-LM：NVIDIA 官方的大规模语言模型训练框架，适合多机多卡场景 监控 \u0026 可视化\nWeights \u0026 Biases 或 TensorBoard：实时监控训练过程中的损失函数、学习率等指标，支持远程可视化 推理工具\nollama：基于 llama.cpp 的本地推理部署，可快速启动 vLLM：主打高并发、多用户场景下的推理吞吐量优化 Framework Ollama vLLM 作用 简易本地部署 LLM 高并发 / 高吞吐的 LLM 推理 多请求处理 并发数增加时，推理速度下降明显 并发数增大也能保持较高吞吐 16 路并发 ~17 秒/请求 ~9 秒/请求 吞吐对比 Token 生成速度较慢 Token 生成速度可提升约 2 倍 极限并发 32 路以上并发时，性能衰减较严重 仍能平稳处理高并发 适用场景 个人项目或低并发应用 企业级或多用户并发访问 WebUI\nOpen-WebUI：基于 Web 界面的多合一 AI 界面，支持多种后端推理（ollama、OpenAI API 等），便于快速原型和可视化 进阶建议 开发与调试效率\n使用 SSH 工具提升远程开发效率，制作自定义容器镜像减少环境配置时间。\n量化与剪枝\n通过 4bit、8bit 量化和剪枝技术，减少模型参数和显存需求，优化推理性能。\n混合精度训练\n使用 BF16 或 FP16 提升训练速度，结合 GradScaler 提高数值稳定性。\nCPU 协同优化\n使用多线程、多进程或 RAM Disk 缓存优化数据加载，支持流式加载大规模预训练数据集。\n多机集群部署\n通过 InfiniBand 或高速以太网搭建集群，使用 Kubernetes 实现高效资源调度。\n总结 通过以上配置与思路，我成功搭建了一台 双卡 RTX 4090 深度学习主机。它在 推理 和 中小规模微调 场景中表现良好，对于想要在个人或小团队环境下进行大模型（LLM）科研或应用探索的人来说，这种方案兼具 性价比 与 灵活性。当然，如果要大规模全参数训练上百亿乃至上千亿参数的大模型，依然需要更多 GPU（如多卡 A100/H100 集群）。\n就个人体验而言，双 4090 在预算范围内提供了较好的训练与推理性能，可以满足绝大部分中小规模研究与实验需求，值得有条件的个人或小团队参考。\n参考资料 Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe 通道规格 AMD R5 7600X PCIe 通道规格 MSI MPG X670E CARBON 规格 nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI 版权声明与引用 声明：本文所涉及的配置清单、价格与建议仅供技术交流与研究参考。实际购买与部署请结合个人预算和需求进行综合评估。若因参考或采纳文中信息导致任何直接或间接后果，本文作者恕不承担责任。\n引用：转载或引用本文内容，请注明原作者与出处。\nCited as:\nYue Shui. (Dec 2024). 基于双卡 RTX 4090 搭建家用深度学习主机. https://syhya.github.io/posts/2024-12-21-build-gpu-server\nOr\n@article{syhya2024build, title = \"基于双卡 RTX 4090 搭建家用深度学习主机\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2024\", month = \"Dec\", url = \"https://syhya.github.io/posts/2024-12-21-build-gpu-server/\" ","wordCount":"4095","inLanguage":"zh","image":"https://syhya.github.io/%3Copengraph%E3%80%81twitter-cards%20%E5%9B%BE%E7%89%87%E7%9A%84%E9%93%BE%E6%8E%A5%E6%88%96%E8%B7%AF%E5%BE%84%3E","datePublished":"2024-12-21T12:00:00+08:00","dateModified":"2025-06-29T21:43:56+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/"},"publisher":{"@type":"Organization","name":"Yue Shui 博客","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3C%E9%93%BE%E6%8E%A5%20/%20%E7%BB%9D%E5%AF%B9URL%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/zh/ accesskey=h title="首页 (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>首页</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/zh/archives/ title=归档><span>归档</span></a></li><li><a href=https://syhya.github.io/zh/search/ title=搜索><span>搜索</span></a></li><li><a href=https://syhya.github.io/zh/tags/ title=标签><span>标签</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/zh/>主页</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/zh/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">基于双卡 RTX 4090 搭建家用深度学习主机</h1><div class=post-meta><span title='2024-12-21 12:00:00 +0800 +0800'>2024-12-21</span>&nbsp;·&nbsp;9 分钟&nbsp;·&nbsp;4095 字&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;翻译:<ul class=i18n_list><li><a href=https://syhya.github.io/posts/2024-12-21-build-gpu-server/>En</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#租用-gpu-还是购买-gpu>租用 GPU 还是购买 GPU？</a></li><li><a href=#背景>背景</a></li><li><a href=#搭建思路与配置详情>搭建思路与配置详情</a><ul><li><a href=#gpu-选择>GPU 选择</a><ul><li><a href=#散热方式涡轮-vs-风冷-vs-水冷>散热方式：涡轮 vs 风冷 vs 水冷</a></li></ul></li><li><a href=#cpu--主板>CPU & 主板</a><ul><li><a href=#微星-mpg-x670e-carbon-主板>微星 MPG X670E CARBON 主板</a></li><li><a href=#amd-ryzen-9-7900x-特点>AMD Ryzen 9 7900X 特点</a></li><li><a href=#主板选购要点>主板选购要点</a></li><li><a href=#bios-设置建议>BIOS 设置建议</a></li></ul></li><li><a href=#内存>内存</a></li><li><a href=#硬盘>硬盘</a></li><li><a href=#电源>电源</a></li><li><a href=#散热与风扇>散热与风扇</a></li><li><a href=#机箱>机箱</a></li></ul></li><li><a href=#系统与软件环境>系统与软件环境</a></li><li><a href=#常用工具与框架推荐>常用工具与框架推荐</a></li><li><a href=#进阶建议>进阶建议</a></li><li><a href=#总结>总结</a></li><li><a href=#参考资料>参考资料</a></li><li><a href=#版权声明与引用>版权声明与引用</a></li></ul></nav></div></details></div><div class=post-content><h2 id=租用-gpu-还是购买-gpu>租用 GPU 还是购买 GPU？<a hidden class=anchor aria-hidden=true href=#租用-gpu-还是购买-gpu>#</a></h2><p>在构建深度学习工作环境之前，首先需要综合考虑 <strong>使用周期</strong>、<strong>预算</strong>、<strong>数据隐私</strong> 以及 <strong>维护成本</strong>。如果长期（例如超过一年以上）且对数据安全要求较高，自建 GPU 服务器通常能带来更低的综合成本和更可控的环境；如果只是短期项目，或对数据隐私不敏感，那么租用云上 GPU（如 Azure、AWS、GCP 等）或使用免费平台（Colab、Kaggle）则更加灵活。</p><ul><li><p><strong>租用 GPU 的优点</strong>：</p><ul><li>无需一次性投入高额硬件成本</li><li>可根据项目需求弹性扩容</li><li>云厂商通常提供数据合规与安全保障，省去硬件运维烦恼</li></ul></li><li><p><strong>购买 GPU 的优点</strong>：</p><ul><li>长期大规模使用时，整体成本更低</li><li>对内部数据和模型有更高的隐私与可控性</li><li>硬件可随时调整、升级，部署更灵活</li></ul></li></ul><blockquote><p><strong>个人建议</strong></p><ol><li>如果预算有限或只是初学阶段，可先使用 Colab、Kaggle 或云 GPU；</li><li>当算力需求和隐私需求上升时，再考虑自建多卡服务器或租用多机多卡集群。</li></ol></blockquote><hr><h2 id=背景>背景<a hidden class=anchor aria-hidden=true href=#背景>#</a></h2><p>在 2023 年 9 月，为了在工作之余继续对大模型（LLM）进行探索和研究，我组装了一台 <strong>双 RTX 4090</strong> 的个人 AI 实验服务器。该服务器已运行近一年，整体体验如下：</p><ul><li><strong>噪音</strong>：服务器放在脚边，满负荷训练时风扇噪音较大，但在日常推理或中等负载下可接受</li><li><strong>推理性能</strong>：双卡共计 48GB 显存，采用 4bit 量化方案时可运行到 70B 级别的模型（如 Llama 70B、Qwen 72B）</li><li><strong>训练性能</strong>：在使用 <a href=https://github.com/microsoft/DeepSpeed>DeepSpeed</a> 的分布式和 offload 技术（ZeRO-3 + CPU offload）后，可对 34B 左右的模型（如 CodeLlama 34B）进行微调</li><li><strong>性价比</strong>：对于个人或小团队的日常实验和中小规模模型训练而言，该配置较为实用；但若进行超大规模模型的全参数训练，仍需更多专业卡（如多卡 A100 或 H100 集群）</li></ul><p>下图展示了不同大小模型、不同训练方法对显存的需求：<figure class=align-center><img loading=lazy src=hardware_requirement.png#center alt="Fig. 1. Hardware Requirement. (Image source: LLaMA-Factory)"><figcaption><p>Fig. 1. Hardware Requirement. (Image source: <a href=https://github.com/hiyouga/LLaMA-Factory#hardware-requirement>LLaMA-Factory</a>)</p></figcaption></figure></p><hr><h2 id=搭建思路与配置详情>搭建思路与配置详情<a hidden class=anchor aria-hidden=true href=#搭建思路与配置详情>#</a></h2><p>整机预算在 <strong>4 万元人民币（约 6000 美元）</strong> 左右，以下是我最终选用的配置清单，仅供参考：</p><table><thead><tr><th style=text-align:center>配件</th><th style=text-align:left>型号</th><th style=text-align:center>价格 (元)</th></tr></thead><tbody><tr><td style=text-align:center><strong>显卡</strong></td><td style=text-align:left>RTX 4090 * 2</td><td style=text-align:center>25098</td></tr><tr><td style=text-align:center><strong>主板 + CPU</strong></td><td style=text-align:left>AMD R9 7900X + 微星 MPG X670E CARBON</td><td style=text-align:center>5157.55</td></tr><tr><td style=text-align:center><strong>内存</strong></td><td style=text-align:left>美商海盗船(USCORSAIR) 48GB*2 (DDR5 5600)</td><td style=text-align:center>2399</td></tr><tr><td style=text-align:center><strong>SSD</strong></td><td style=text-align:left>SOLIDIGM 944 PRO 2TB *2 + 三星 990PRO 4TB</td><td style=text-align:center>4587</td></tr><tr><td style=text-align:center><strong>电源</strong></td><td style=text-align:left>美商海盗船 AX1600i</td><td style=text-align:center>2699</td></tr><tr><td style=text-align:center><strong>风扇</strong></td><td style=text-align:left>追风者 T30 12cm P * 6</td><td style=text-align:center>1066.76</td></tr><tr><td style=text-align:center><strong>散热</strong></td><td style=text-align:left>利民 Thermalright FC140 BLACK</td><td style=text-align:center>419</td></tr><tr><td style=text-align:center><strong>机箱</strong></td><td style=text-align:left>PHANTEKS 追风者 620PC 全塔</td><td style=text-align:center>897.99</td></tr><tr><td style=text-align:center><strong>显卡延长线</strong></td><td style=text-align:left>追风者 FL60 PCI-E4.0 *16</td><td style=text-align:center>399</td></tr></tbody></table><p><strong>总计</strong>：约 42723.3 元</p><h3 id=gpu-选择>GPU 选择<a hidden class=anchor aria-hidden=true href=#gpu-选择>#</a></h3><p>对于大模型研究，<strong>浮点运算性能（TFLOPS）</strong> 和 <strong>显存容量</strong> 是最核心的指标。专业卡（A100、H100 等）虽有更高显存以及 NVLink，但价格动辄数十万人民币，对个人用户并不友好。根据 <a href=https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/>Tim Dettmers</a> 的调研，RTX 4090 在单位美元算力方面表现非常亮眼，且支持 BF16、Flash Attention 等新特性，因此成为高性价比的选择。</p><h4 id=散热方式涡轮-vs-风冷-vs-水冷>散热方式：涡轮 vs 风冷 vs 水冷<a hidden class=anchor aria-hidden=true href=#散热方式涡轮-vs-风冷-vs-水冷>#</a></h4><table><thead><tr><th style=text-align:center><strong>散热方式</strong></th><th style=text-align:left><strong>优点</strong></th><th style=text-align:center><strong>缺点</strong></th><th style=text-align:center><strong>适用场景</strong></th></tr></thead><tbody><tr><td style=text-align:center><strong>涡轮风扇</strong></td><td style=text-align:left>体积紧凑；适合并行多卡部署</td><td style=text-align:center>噪音大、整体散热效率一般</td><td style=text-align:center>企业服务器机柜、多卡密集部署</td></tr><tr><td style=text-align:center><strong>风冷散热</strong></td><td style=text-align:left>性能与噪音平衡佳、维护简单</td><td style=text-align:center>显卡体型通常较大</td><td style=text-align:center>家用或个人研究（主机摆放空间足够）</td></tr><tr><td style=text-align:center><strong>水冷散热</strong></td><td style=text-align:left>散热能力突出、满载噪音更低</td><td style=text-align:center>可能会出现漏液、价格更高</td><td style=text-align:center>对静音要求极高或极限超频场景</td></tr></tbody></table><blockquote><p><strong>家用推荐</strong>：<strong>风冷卡</strong> 兼顾散热效率、噪音和维护成本；相对于涡轮卡和水冷卡更友好。</p></blockquote><h3 id=cpu--主板>CPU & 主板<a hidden class=anchor aria-hidden=true href=#cpu--主板>#</a></h3><p>在深度学习场景中，CPU 主要负责数据预处理、管道调度以及多进程/多线程并行管理，确保数据能够以高吞吐量、低延迟的方式传递到 GPU。因此，CPU 的核心需求主要包括 <strong>充足的 PCIe 通道</strong> 和 <strong>卓越的多线程性能</strong>。</p><ul><li><strong>Intel</strong>：13/14 代 i9（如 13900K）拥有 20 条 PCIe 主通道，能够满足双卡 x8 + x8 的需求</li><li><strong>AMD</strong>：Ryzen 7000/9000 系列（如 7950X）提供 28 条（可用 24 条）PCIe 通道，支持双卡 x8 + x8，并为 M.2 SSD 提供足够带宽</li></ul><h4 id=微星-mpg-x670e-carbon-主板>微星 MPG X670E CARBON 主板<a hidden class=anchor aria-hidden=true href=#微星-mpg-x670e-carbon-主板>#</a></h4><ul><li><strong>扩展性</strong>：支持 PCIe 5.0 和 DDR5 内存，具备充足的未来升级空间</li><li><strong>稳定性</strong>：高规格供电设计，保障 CPU 与多显卡的稳定运行</li><li><strong>接口丰富</strong>：支持多块 M.2 SSD 和 USB4，满足多样化使用需求</li></ul><h4 id=amd-ryzen-9-7900x-特点>AMD Ryzen 9 7900X 特点<a hidden class=anchor aria-hidden=true href=#amd-ryzen-9-7900x-特点>#</a></h4><ul><li><strong>核心与线程</strong>：12 核心、24 线程，在深度学习场景中的数据预处理和多任务处理方面表现强劲</li><li><strong>PCIe 带宽</strong>：提供 28 条（可用 24 条）PCIe 5.0 通道，可轻松支持双卡 x8 + x8，并为 M.2 SSD 提供高速带宽</li><li><strong>能效比</strong>：基于 Zen 4 架构，性能与能耗平衡优秀，适合高性能计算需求</li></ul><h4 id=主板选购要点>主板选购要点<a hidden class=anchor aria-hidden=true href=#主板选购要点>#</a></h4><ol><li><strong>空间布局</strong><ul><li>RTX 4090 尺寸庞大且卡槽较厚，需确认主板是否能同时容纳两张显卡；若存在空间或散热冲突，可使用显卡延长线竖放第二张卡。</li></ul></li><li><strong>PCIe 通道拆分</strong><ul><li>主板需至少支持双 PCIe 4.0 x8 + x8 配置，以避免出现 x16 + x2 的情况。x16 + x2 的带宽分配会显著限制第二块 GPU 的数据传输能力，进而影响 GPU 与 CPU 之间的数据交换效率。在大模型训练中，这种带宽瓶颈可能导致性能显著下降，严重影响整体训练效率。</li></ul></li><li><strong>扩展性</strong><ul><li>在双显卡插满的情况下，仍需确保主板具有足够的 M.2 SSD 插槽和外设接口</li></ul></li></ol><p>综合扩展性、性能与性价比等因素，我最终选择了 <strong>AMD Ryzen 9 7900X 搭配 微星 MPG X670E CARBON 主板</strong> 的组合。通过显卡延长线解决了 4090 双卡过厚导致的插槽冲突问题。</p><h4 id=bios-设置建议>BIOS 设置建议<a hidden class=anchor aria-hidden=true href=#bios-设置建议>#</a></h4><ol><li><strong>内存优化</strong><ul><li>开启 <strong>XMP/EXPO</strong>（对应 Intel/AMD）以提升内存频率，增强带宽性能。</li></ul></li><li><strong>超频调整</strong><ul><li>如果需要进一步提升性能，可在 BIOS 中启用 <strong>PBO（Precision Boost Overdrive）</strong> 或 Intel Performance Tuning，并结合系统监控工具观察稳定性。</li></ul></li><li><strong>温度与稳定性</strong><ul><li>避免过度超频，注意控制温度，避免因崩溃或过热导致系统不稳定。</li></ul></li></ol><h3 id=内存>内存<a hidden class=anchor aria-hidden=true href=#内存>#</a></h3><p>深度学习训练中，内存会被大量占用用于数据加载、模型优化状态储存（尤其在多 GPU Zero-stage 并行场景下）。<strong>内存容量最好 ≥ 显存总容量的两倍</strong>。本配置中，使用了 48GB * 2（共 96GB），满足日常多任务和分布式训练的需求，减少内存不足导致的频繁 swap。</p><h3 id=硬盘>硬盘<a hidden class=anchor aria-hidden=true href=#硬盘>#</a></h3><ul><li><strong>优先选用 M.2 NVMe SSD</strong>：其读写性能更优，对加载超大模型权重、缓存中间文件、训练日志等都有显著速度提升</li><li><strong>容量建议 ≥ 2TB</strong>：随着大模型文件越来越庞大，2TB 往往很快就会被占满，建议根据自身需求选 4TB 或更多</li><li><strong>SSD 品牌</strong>：三星、海力士或西部数据等主流大厂都拥有稳定的高端产品线</li></ul><h3 id=电源>电源<a hidden class=anchor aria-hidden=true href=#电源>#</a></h3><p>双 4090 满载时整机功耗可达 <strong>900W~1000W 左右</strong>，CPU、主板和硬盘等还需额外功率余量。通常建议选择 <strong>1500W 以上</strong> 的铂金或钛金电源，以确保在高负载下电流供给稳定、降低电压波动带来的系统不稳定。<br>我在此使用美商海盗船 AX1600i（数字电源），可以通过软件监控实时功耗，并提供充足冗余。</p><h3 id=散热与风扇>散热与风扇<a hidden class=anchor aria-hidden=true href=#散热与风扇>#</a></h3><p>我采用 <strong>风冷</strong> 方案，包括：</p><ul><li><strong>CPU 散热器</strong>：利民 FC140（双塔式气冷方案，兼顾了较高的散热效率和相对低噪音）</li><li><strong>机箱风扇</strong>：追风者 T30 12cm * 6，保持机箱内部正压或者稍微正压的风道布局，保证显卡和供电模块的进风顺畅</li></ul><p>在 GPU 长时间高负载训练（如分布式训练大型模型）时，机箱内的风道管理和风扇配置非常重要。建议使用监控软件及时查看 CPU、GPU、主板供电模块温度，适度调节风扇转速。</p><blockquote><p><strong>散热进阶</strong></p><ul><li>若对静音有更高要求，可考虑 <em>Hybrid</em> 散热（半水冷方案）或更精细的风扇调速曲线。</li><li>适度清理机箱灰尘、使用防尘网并定期更换导热硅脂也能提升散热和稳定性。</li></ul></blockquote><h3 id=机箱>机箱<a hidden class=anchor aria-hidden=true href=#机箱>#</a></h3><p>RTX 4090 体型巨大，且双卡堆叠时需要充足的内部空间和散热风道。全塔机箱能提供更好的走线空间和气流组织。我选用了 PHANTEKS 追风者 620PC，除了体型大、空间充裕外，也自带良好的线缆管理通道。</p><p>装机完成后的图片如下：<br><figure class=align-center><img loading=lazy src=computer.jpeg#center alt="Fig. 2. Computer"><figcaption><p>Fig. 2. Computer</p></figcaption></figure></p><hr><h2 id=系统与软件环境>系统与软件环境<a hidden class=anchor aria-hidden=true href=#系统与软件环境>#</a></h2><p><strong>操作系统</strong>方面强烈推荐 <strong>Linux</strong>，例如 <strong>Ubuntu 22.04 LTS</strong>，因其对 CUDA、NVIDIA 驱动以及常见深度学习框架有更好的支持和兼容性。大致流程如下：</p><ol><li><strong>安装 OS</strong>：使用 Ubuntu 或其他 Linux 系统即可。</li><li><strong>安装 NVIDIA 驱动</strong>：确保 <code>nvidia-smi</code> 能正确识别两张 4090:<br><figure class=align-center><img loading=lazy src=nvidia_smi.png#center alt="Fig. 3. nvidia-smi Output"><figcaption><p>Fig. 3. nvidia-smi Output</p></figcaption></figure></li><li><strong>安装 CUDA 工具链</strong>：通过 <code>nvcc -V</code> 确认版本信息:<br><figure class=align-center><img loading=lazy src=nvcc.png#center alt="Fig. 4. nvcc -V Output"><figcaption><p>Fig. 4. nvcc -V Output</p></figcaption></figure></li><li><strong>安装 cuDNN</strong>：确保深度学习框架可以调用 GPU 加速卷积和 RNN 等操作</li><li><strong>测试框架</strong>：使用 <a href=https://pytorch.org/>PyTorch</a>、<a href=https://www.tensorflow.org/>TensorFlow</a> 或 <a href=https://github.com/google/jax>JAX</a> 简单测试模型推理/训练是否正常</li><li><strong>Docker 容器化</strong>：<ul><li>利用 <a href=https://github.com/NVIDIA/nvidia-container-toolkit>nvidia-container-toolkit</a> 让容器直接访问 GPU 资源，避免主机环境污染。</li><li>在多机多卡环境下，还能结合 <strong>Kubernetes</strong>、<strong>Ray</strong> 或 <strong>Slurm</strong> 等进行集群调度与资源管理。</li></ul></li></ol><hr><h2 id=常用工具与框架推荐>常用工具与框架推荐<a hidden class=anchor aria-hidden=true href=#常用工具与框架推荐>#</a></h2><ol><li><p><strong>训练框架</strong></p><ul><li><a href=https://github.com/hiyouga/LLaMA-Factory><strong>LLaMA-Factory</strong></a>：对大语言模型训练/推理流程有较好封装，新手友好</li><li><a href=https://github.com/microsoft/DeepSpeed><strong>DeepSpeed</strong></a>：支持大模型分布式训练、多种并行策略和优化功能</li><li><a href=https://github.com/NVIDIA/Megatron-LM><strong>Megatron-LM</strong></a>：NVIDIA 官方的大规模语言模型训练框架，适合多机多卡场景</li></ul></li><li><p><strong>监控 & 可视化</strong></p><ul><li><a href=https://wandb.ai/><strong>Weights & Biases</strong></a> 或 <a href=https://www.tensorflow.org/tensorboard><strong>TensorBoard</strong></a>：实时监控训练过程中的损失函数、学习率等指标，支持远程可视化</li></ul></li><li><p><strong>推理工具</strong></p><ul><li><a href=https://github.com/jmorganca/ollama><strong>ollama</strong></a>：基于 <a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a> 的本地推理部署，可快速启动</li><li><a href=https://github.com/vllm-project/vllm><strong>vLLM</strong></a>：主打高并发、多用户场景下的推理吞吐量优化</li></ul><table><thead><tr><th style=text-align:center><strong>Framework</strong></th><th style=text-align:left><strong>Ollama</strong></th><th style=text-align:center><strong>vLLM</strong></th></tr></thead><tbody><tr><td style=text-align:center><strong>作用</strong></td><td style=text-align:left>简易本地部署 LLM</td><td style=text-align:center>高并发 / 高吞吐的 LLM 推理</td></tr><tr><td style=text-align:center><strong>多请求处理</strong></td><td style=text-align:left>并发数增加时，推理速度下降明显</td><td style=text-align:center>并发数增大也能保持较高吞吐</td></tr><tr><td style=text-align:center><strong>16 路并发</strong></td><td style=text-align:left>~17 秒/请求</td><td style=text-align:center>~9 秒/请求</td></tr><tr><td style=text-align:center><strong>吞吐对比</strong></td><td style=text-align:left>Token 生成速度较慢</td><td style=text-align:center>Token 生成速度可提升约 2 倍</td></tr><tr><td style=text-align:center><strong>极限并发</strong></td><td style=text-align:left>32 路以上并发时，性能衰减较严重</td><td style=text-align:center>仍能平稳处理高并发</td></tr><tr><td style=text-align:center><strong>适用场景</strong></td><td style=text-align:left>个人项目或低并发应用</td><td style=text-align:center>企业级或多用户并发访问</td></tr></tbody></table></li><li><p><strong>WebUI</strong></p><ul><li><a href=https://github.com/open-webui/open-webui><strong>Open-WebUI</strong></a>：基于 Web 界面的多合一 AI 界面，支持多种后端推理（ollama、OpenAI API 等），便于快速原型和可视化</li></ul></li></ol><hr><h2 id=进阶建议>进阶建议<a hidden class=anchor aria-hidden=true href=#进阶建议>#</a></h2><ol><li><p><strong>开发与调试效率</strong><br>使用 SSH 工具提升远程开发效率，制作自定义容器镜像减少环境配置时间。</p></li><li><p><strong>量化与剪枝</strong><br>通过 4bit、8bit 量化和剪枝技术，减少模型参数和显存需求，优化推理性能。</p></li><li><p><strong>混合精度训练</strong><br>使用 BF16 或 FP16 提升训练速度，结合 GradScaler 提高数值稳定性。</p></li><li><p><strong>CPU 协同优化</strong><br>使用多线程、多进程或 RAM Disk 缓存优化数据加载，支持流式加载大规模预训练数据集。</p></li><li><p><strong>多机集群部署</strong><br>通过 InfiniBand 或高速以太网搭建集群，使用 Kubernetes 实现高效资源调度。</p></li></ol><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>通过以上配置与思路，我成功搭建了一台 <strong>双卡 RTX 4090</strong> 深度学习主机。它在 <strong>推理</strong> 和 <strong>中小规模微调</strong> 场景中表现良好，对于想要在个人或小团队环境下进行大模型（LLM）科研或应用探索的人来说，这种方案兼具 <strong>性价比</strong> 与 <strong>灵活性</strong>。当然，如果要大规模全参数训练上百亿乃至上千亿参数的大模型，依然需要更多 GPU（如多卡 A100/H100 集群）。</p><p>就个人体验而言，双 4090 在预算范围内提供了较好的训练与推理性能，可以满足绝大部分中小规模研究与实验需求，值得有条件的个人或小团队参考。</p><h2 id=参考资料>参考资料<a hidden class=anchor aria-hidden=true href=#参考资料>#</a></h2><ol><li><a href=https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/>Tim Dettmers: Which GPU for Deep Learning? (2023)</a></li><li><a href=https://www.intel.com/content/www/us/en/products/sku/236773/intel-core-i9-processor-14900k-36m-cache-up-to-6-00-ghz/specifications.html>Intel 14900K PCIe 通道规格</a></li><li><a href=https://www.amd.com/en/products/processors/desktops/ryzen/7000-series/amd-ryzen-5-7600.html>AMD R5 7600X PCIe 通道规格</a></li><li><a href=https://www.msi.com/Motherboard/MPG-X670E-CARBON-WIFI/Specification>MSI MPG X670E CARBON 规格</a></li><li><a href=https://github.com/NVIDIA/nvidia-container-toolkit>nvidia-container-toolkit</a></li><li><a href=https://github.com/hiyouga/LLaMA-Factory>LLaMA-Factory</a></li><li><a href=https://github.com/microsoft/DeepSpeed>DeepSpeed</a></li><li><a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a></li><li><a href=https://github.com/jmorganca/ollama>ollama</a></li><li><a href=https://github.com/vllm-project/vllm>vLLM</a></li><li><a href=https://medium.com/@naman1011/ollama-vs-vllm-which-tool-handles-ai-models-better-a93345b911e6>Ollama vs VLLM: Which Tool Handles AI Models Better?</a></li><li><a href=https://github.com/open-webui/open-webui>Open-WebUI</a></li></ol><hr><h2 id=版权声明与引用>版权声明与引用<a hidden class=anchor aria-hidden=true href=#版权声明与引用>#</a></h2><blockquote><p><strong>声明</strong>：本文所涉及的配置清单、价格与建议仅供技术交流与研究参考。实际购买与部署请结合个人预算和需求进行综合评估。若因参考或采纳文中信息导致任何直接或间接后果，本文作者恕不承担责任。<br><strong>引用</strong>：转载或引用本文内容，请注明原作者与出处。</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Dec 2024). 基于双卡 RTX 4090 搭建家用深度学习主机.
<a href=https://syhya.github.io/posts/2024-12-21-build-gpu-server>https://syhya.github.io/posts/2024-12-21-build-gpu-server</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2024build</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;基于双卡 RTX 4090 搭建家用深度学习主机&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2024&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Dec&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2024-12-21-build-gpu-server/&#34;</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/zh/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a></li><li><a href=https://syhya.github.io/zh/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/zh/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/zh/tags/rtx-4090/>RTX 4090</a></li><li><a href=https://syhya.github.io/zh/tags/ai%E7%A1%AC%E4%BB%B6/>AI硬件</a></li><li><a href=https://syhya.github.io/zh/tags/%E6%98%BE%E5%8D%A1/>显卡</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/zh/posts/2025-01-05-domain-llm-training/><span class=title>« 上一页</span><br><span>构建特定领域的大语言模型</span>
</a><a class=next href=https://syhya.github.io/zh/posts/2021-04-21-deep-learning-stock-prediction/><span class=title>下一页 »</span><br><span>基于深度学习的股票价格预测和量化策略</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于双卡 RTX 4090 搭建家用深度学习主机 on x" href="https://x.com/intent/tweet/?text=%e5%9f%ba%e4%ba%8e%e5%8f%8c%e5%8d%a1%20RTX%204090%20%e6%90%ad%e5%bb%ba%e5%ae%b6%e7%94%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%bb%e6%9c%ba&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2024-12-21-build-gpu-server%2f&amp;hashtags=%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%2cAI%2cLLM%2cRTX4090%2cAI%e7%a1%ac%e4%bb%b6%2c%e6%98%be%e5%8d%a1"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于双卡 RTX 4090 搭建家用深度学习主机 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2024-12-21-build-gpu-server%2f&amp;title=%e5%9f%ba%e4%ba%8e%e5%8f%8c%e5%8d%a1%20RTX%204090%20%e6%90%ad%e5%bb%ba%e5%ae%b6%e7%94%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%bb%e6%9c%ba&amp;summary=%e5%9f%ba%e4%ba%8e%e5%8f%8c%e5%8d%a1%20RTX%204090%20%e6%90%ad%e5%bb%ba%e5%ae%b6%e7%94%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%bb%e6%9c%ba&amp;source=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2024-12-21-build-gpu-server%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于双卡 RTX 4090 搭建家用深度学习主机 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2024-12-21-build-gpu-server%2f&title=%e5%9f%ba%e4%ba%8e%e5%8f%8c%e5%8d%a1%20RTX%204090%20%e6%90%ad%e5%bb%ba%e5%ae%b6%e7%94%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%bb%e6%9c%ba"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于双卡 RTX 4090 搭建家用深度学习主机 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2024-12-21-build-gpu-server%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于双卡 RTX 4090 搭建家用深度学习主机 on whatsapp" href="https://api.whatsapp.com/send?text=%e5%9f%ba%e4%ba%8e%e5%8f%8c%e5%8d%a1%20RTX%204090%20%e6%90%ad%e5%bb%ba%e5%ae%b6%e7%94%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%bb%e6%9c%ba%20-%20https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2024-12-21-build-gpu-server%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于双卡 RTX 4090 搭建家用深度学习主机 on telegram" href="https://telegram.me/share/url?text=%e5%9f%ba%e4%ba%8e%e5%8f%8c%e5%8d%a1%20RTX%204090%20%e6%90%ad%e5%bb%ba%e5%ae%b6%e7%94%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%bb%e6%9c%ba&amp;url=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2024-12-21-build-gpu-server%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 基于双卡 RTX 4090 搭建家用深度学习主机 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e5%9f%ba%e4%ba%8e%e5%8f%8c%e5%8d%a1%20RTX%204090%20%e6%90%ad%e5%bb%ba%e5%ae%b6%e7%94%a8%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%b8%bb%e6%9c%ba&u=https%3a%2f%2fsyhya.github.io%2fzh%2fposts%2f2024-12-21-build-gpu-server%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/zh/>Yue Shui 博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>