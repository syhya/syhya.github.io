<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>vLLM: High-Throughput, Memory-Efficient LLM Serving | Yue Shui Blog</title><meta name=keywords content="vLLM,PagedAttention,LLM Serving,Inference,KV Cache,Memory Optimization,LLMs,AI Infrastructure,Deep Learning"><meta name=description content="As the parameters of Large Language Models (LLMs) continue to grow, deploying and serving these models presents significant challenges. vLLM is an open-source library designed for fast, convenient, and cost-effective LLM inference and online serving. Its core lies in the PagedAttention algorithm, which efficiently manages the KV Cache in the attention mechanism.
Evaluation Metrics
To evaluate the performance of LLM inference and serving engines, we primarily focus on the following metrics:"><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-05-17-vllm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-05-17-vllm/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-05-17-vllm/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-05-17-vllm/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="vLLM: High-Throughput, Memory-Efficient LLM Serving"><meta property="og:description" content="As the parameters of Large Language Models (LLMs) continue to grow, deploying and serving these models presents significant challenges. vLLM is an open-source library designed for fast, convenient, and cost-effective LLM inference and online serving. Its core lies in the PagedAttention algorithm, which efficiently manages the KV Cache in the attention mechanism.
Evaluation Metrics To evaluate the performance of LLM inference and serving engines, we primarily focus on the following metrics:"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-17T10:00:00+08:00"><meta property="article:modified_time" content="2025-06-16T14:57:08+08:00"><meta property="article:tag" content="VLLM"><meta property="article:tag" content="PagedAttention"><meta property="article:tag" content="LLM Serving"><meta property="article:tag" content="Inference"><meta property="article:tag" content="KV Cache"><meta property="article:tag" content="Memory Optimization"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="vLLM: High-Throughput, Memory-Efficient LLM Serving"><meta name=twitter:description content="As the parameters of Large Language Models (LLMs) continue to grow, deploying and serving these models presents significant challenges. vLLM is an open-source library designed for fast, convenient, and cost-effective LLM inference and online serving. Its core lies in the PagedAttention algorithm, which efficiently manages the KV Cache in the attention mechanism.
Evaluation Metrics
To evaluate the performance of LLM inference and serving engines, we primarily focus on the following metrics:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"vLLM: High-Throughput, Memory-Efficient LLM Serving","item":"https://syhya.github.io/posts/2025-05-17-vllm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"vLLM: High-Throughput, Memory-Efficient LLM Serving","name":"vLLM: High-Throughput, Memory-Efficient LLM Serving","description":"As the parameters of Large Language Models (LLMs) continue to grow, deploying and serving these models presents significant challenges. vLLM is an open-source library designed for fast, convenient, and cost-effective LLM inference and online serving. Its core lies in the PagedAttention algorithm, which efficiently manages the KV Cache in the attention mechanism.\nEvaluation Metrics To evaluate the performance of LLM inference and serving engines, we primarily focus on the following metrics:\n","keywords":["vLLM","PagedAttention","LLM Serving","Inference","KV Cache","Memory Optimization","LLMs","AI Infrastructure","Deep Learning"],"articleBody":"As the parameters of Large Language Models (LLMs) continue to grow, deploying and serving these models presents significant challenges. vLLM is an open-source library designed for fast, convenient, and cost-effective LLM inference and online serving. Its core lies in the PagedAttention algorithm, which efficiently manages the KV Cache in the attention mechanism.\nEvaluation Metrics To evaluate the performance of LLM inference and serving engines, we primarily focus on the following metrics:\nTime To First Token (TTFT) Time To First Token (TTFT) refers to the time elapsed from when the model receives user input (Prompt) to when it generates the first output token. A shorter TTFT means less waiting time for the user, which is particularly important for real-time interactive scenarios; in offline scenarios, TTFT is relatively less critical.\nTime Per Output Token (TPOT) Time Per Output Token (TPOT) indicates the average time required for the model to generate one new token. It directly determines the user-perceived “speed” of the response. To enhance user experience, Streaming is commonly used in practical applications. For example, if TPOT is 0.1 seconds/token, it means the model can generate about 10 tokens per second, equivalent to approximately 450 words per minute, which exceeds the reading speed of most people.\nLatency Latency is the total time required for the model to generate a complete response for the user. It can be calculated from TTFT and TPOT using the following formula:\n$$ \\text{Latency} = \\text{TTFT} + \\text{TPOT} \\times (\\text{Number of Output Tokens}) $$Throughput Throughput measures the total number of tokens (including input and output tokens) that the model inference server can generate per unit of time for all user requests. It reflects the server’s processing efficiency and concurrency capability. The specific calculation formula is as follows:\n$$ \\text{Throughput} = \\frac{\\text{Batch Size} \\times (\\text{Number of Input Tokens} + \\text{Number of Output Tokens})}{\\text{End-to-End Latency}} $$Inter-Token Latency (ITL) Inter-Token Latency (ITL) represents the average time interval between the generation of two consecutive tokens after the first token has been generated. It reflects the speed at which each subsequent token is generated, calculated as:\n$$ \\text{ITL} = \\frac{\\text{End-to-End Latency} - \\text{TTFT}}{\\text{Batch Size} \\times (\\text{Number of Output Tokens} - 1)} $$These metrics reflect the inference engine’s response speed, processing efficiency, and concurrency capabilities, serving as important benchmarks for evaluating and optimizing LLM inference performance.\nvLLM V0 Since its initial release in June 2023, vLLM, equipped with PagedAttention, has significantly raised the performance benchmark for LLM serving. It demonstrates a notable throughput advantage over HuggingFace Transformers (HF) and Text Generation Inference (TGI), without requiring any modifications to the model architecture.\nFig. 1. Throughput comparison (single output completion) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)\nSingle output inference: The figure shows vLLM achieves 14x-24x higher throughput than HF and 2.2x-2.5x higher throughput than TGI. Fig. 2. Throughput comparison (three parallel output completions) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)\nThree-way parallel inference: The figure shows vLLM achieves 8.5x-15x higher throughput than HF and 3.3x-3.5x higher throughput than TGI. Batching Traditional Dynamic Batching waits for an entire batch of requests to complete before processing the next batch. If some requests finish early, this leads to GPU idle time and reduced resource utilization.\nIn contrast, Continuous Batching, employed by vLLM, allows new request sequences to be dynamically inserted during batch execution. Once a sequence completes, it can be immediately replaced by a new sequence, significantly improving GPU utilization and throughput.\nFig. 3. Dynamic Batching vs Continuous Batching. (Image source: NYC vLLM Meetup, 2025)\nDynamic Batching: As shown on the left, sequences S₁-S₄ are processed in parallel from T1-T4. At T5, S₁ and S₃ finish early. However, because S₂ and S₄ are still running, new sequences cannot join immediately, leading to partial GPU idleness. New sequences can only start after S₂ finishes at T6 and S₄ finishes at T7.\nContinuous Batching: As shown on the right, T1-T4 are similar to dynamic batching. However, at T5, when S₁ and S₃ complete, new sequences S₅ and S₆ can join and start processing immediately, while S₂ and S₄ continue running. When S₂ finishes at T6, S₇ can join instantly. This approach keeps the GPU almost fully utilized, greatly enhancing efficiency.\nKV Cache The primary bottleneck in LLM serving performance is memory management. During the autoregressive decoding process, LLMs generate attention Key and Value tensors for each token in the input sequence. These tensors (KV cache) must be retained in GPU memory to generate subsequent tokens. The KV cache has the following characteristics:\nLarge: For the LLaMA-13B model, the KV cache for a single sequence can be up to 1.7 GB. Dynamic: The size of the KV cache depends on the sequence length, which is highly variable and unpredictable. Inefficient Management: Existing inference frameworks like FasterTransformer and Orca (Yu et al. 2022) typically store the KV cache in contiguous memory blocks. To handle its dynamic nature, they need to pre-allocate memory blocks large enough to accommodate the maximum possible sequence length. This leads to severe memory waste: Internal Fragmentation: Reserved space is much larger than actually needed. External Fragmentation: Pre-allocated blocks of different sizes make it difficult to utilize memory space efficiently. Over-reservation: Space reserved for future tokens cannot be used by other requests currently. The figure below illustrates the types of memory waste caused by KV cache management in existing inference systems:\nFig. 4. KV cache memory management in existing systems, showing reserved waste, internal fragmentation, and external fragmentation. (Image source: Kwon et al. 2023)\nThe left image below shows the memory distribution when running a 13B parameter LLM on an NVIDIA A100 GPU: approximately 65% of the memory is used for static model weights (gray), about 30% is dynamically allocated on demand for the KV cache (red) to store the attention context of preceding tokens, and a small amount of memory (yellow) is used for temporary activation computations. The right image indicates that vLLM effectively alleviates the memory bottleneck by smoothing the rapid growth of KV cache memory usage, thereby significantly enhancing batch processing capabilities and overall service throughput.\nFig. 5. Left: Memory layout for a 13B LLM on an NVIDIA A100—gray is persistent parameters, red is per-request KV cache, and yellow is temporary activation memory. Right: vLLM limits rapid KV cache growth, improving throughput. (Image source: Kwon et al. 2023)\nPagedAttention PagedAttention (Kwon et al. 2023) is inspired by Virtual Memory and Paging concepts from operating systems. It allows logically contiguous KV Cache to be stored in physically non-contiguous GPU memory.\nSpecifically, PagedAttention divides the KV Cache of each sequence into fixed-size Blocks. Each block contains the Key and Value vectors for a fixed number of tokens. The system maintains a Block Table that records the mapping from logical blocks to physical blocks for each sequence.\nFig. 6. Illustration of the PagedAttention algorithm, where KV vectors are stored in non-contiguous blocks. (Image source: Kwon et al. 2023)\nThe core idea of PagedAttention borrows from the virtual memory and paging mechanisms of operating systems to manage the KV cache.\nSpecifically, the design philosophy of PagedAttention can be summarized as follows:\nAnalogy:\nBlocks of the KV cache are analogous to Pages in OS memory management. Tokens are analogous to Bytes. Sequences are analogous to Processes. Mapping Mechanism: PagedAttention uses a Block Table to maintain the mapping from a sequence’s contiguous Logical Blocks to Physical Blocks. These physical blocks can be non-contiguous in memory, much like an OS page table maps virtual addresses to physical page frames.\nAllocate-on-Demand: Crucially, Physical Blocks are not pre-allocated for the maximum sequence length. Instead, they are allocated on demand when new Key-Values need to be stored (i.e., when new tokens are generated).\nThis on-demand, non-contiguous memory management allows PagedAttention to utilize memory more effectively, avoiding the waste and internal fragmentation caused by pre-allocating large contiguous spaces, thereby improving GPU memory utilization.\nMathematically, PagedAttention transforms attention computation into block-wise computation. Let the block size be $B$. The $j$-th Key block is $K_{j}=\\left(k_{(j-1) B+1}, \\ldots, k_{j B}\\right)$, and the Value block is $V_{j}=\\left(v_{(j-1) B+1}, \\ldots, v_{j B}\\right)$. For a query vector $q_i$, the attention computation becomes:\n\\[ A_{i j}=\\frac{\\exp \\left(q_{i}^{\\top} K_{j} / \\sqrt{d}\\right)}{\\sum_{t=1}^{\\lceil i / B\\rceil} \\exp \\left(q_{i}^{\\top} K_{t} \\mathbf{1} / \\sqrt{d}\\right)}, \\quad o_{i}=\\sum_{j=1}^{\\lceil i / B\\rceil} V_{j} A_{i j}^{\\top} \\]where $A_{i j}=\\left(a_{i,(j-1) B+1}, \\ldots, a_{i, j B}\\right)$ is the row vector of attention scores for the $i$-th query on the $j$-th KV block. During computation, the PagedAttention kernel efficiently identifies and fetches the required physical blocks.\nKV Cache Manager vLLM’s memory manager draws inspiration from the virtual memory mechanisms of operating systems:\nLogical vs. Physical Blocks: Each request’s KV cache is represented as a series of logical blocks. The Block Engine on GPU worker nodes allocates physical memory and divides it into physical blocks. Block Table: Maintains the mapping from logical blocks to physical blocks for each request. Each entry records the physical block address and the number of tokens filled within the block. Dynamic Allocation: Physical blocks are allocated on demand, eliminating the need to pre-reserve space for the maximum length, thereby significantly reducing memory waste. Fig. 7. Block table translation in vLLM. Logical blocks are mapped to non-contiguous physical blocks. (Image source: Kwon et al. 2023)\nConsider the example in Fig. 7:\nPrefill Stage: The input prompt has 7 tokens. Assume a block size of 4. vLLM allocates 2 physical blocks (e.g., physical blocks 7 and 1) and updates the block table, mapping logical block 0 to physical block 7, and logical block 1 to physical block 1. The KV cache for the prompt is computed and filled into these two physical blocks. Logical block 0 is filled with 4 tokens, and logical block 1 is filled with 3 tokens, leaving 1 slot reserved. Decode Stage: Step 1: The next token is computed using PagedAttention. Since logical block 1 still has an empty slot, the new KV cache is stored directly in physical block 1, and the fill count for logical block 1 in the block table is updated. Step 2: Logical block 1 is now full. vLLM allocates a new physical block (e.g., physical block 3), updates the block table to map the new logical block 2 to physical block 3, and stores the newly generated KV cache in physical block 3. This on-demand allocation method limits memory waste to the last block of each sequence, achieving near-optimal memory utilization (waste below 4%). This allows for batching more requests, thereby increasing throughput.\nFig. 8 shows how vLLM manages memory for two sequences. The logical blocks of the two sequences are mapped to different physical blocks reserved by the block engine on the GPU worker. This means that even logically adjacent blocks do not need to be contiguous in physical GPU memory, allowing both sequences to effectively share and utilize the physical memory space.\nFig. 8. Storing the KV cache of two requests concurrently in vLLM using paged memory. (Image source: Kwon et al. 2023)\nMemory Sharing Another key advantage of PagedAttention is efficient memory sharing, especially for complex decoding strategies.\nParallel Sampling When a request needs to generate multiple output sequences from the same prompt (e.g., code completion suggestions), the KV cache for the prompt part can be shared.\nFig. 9. Parallel sampling example. Logical blocks for the shared prompt map to the same physical blocks. (Image source: Kwon et al. 2023)\nvLLM achieves sharing via the block table:\nShared Mapping: Logical blocks of different sequences can map to the same physical block. Reference Counting: Each physical block maintains a reference count. Copy-on-Write (CoW): When a shared block (reference count \u003e 1) needs to be written to, vLLM allocates a new physical block, copies the content of the original block, updates the block table mapping for the writing sequence, and decrements the reference count of the original physical block. Subsequent writes to this physical block (when its reference count is 1) are performed directly. This mechanism significantly reduces memory overhead for Parallel Sampling, with experiments showing memory savings of up to 55%.\nBeam Search During Beam Search, different candidate sequences (beams) not only share the prompt part but may also share the KV cache of subsequently generated tokens. The sharing pattern changes dynamically.\nFig. 10. Beam search example ($k=4$). Blocks are dynamically shared and freed based on candidate survival. (Image source: Kwon et al. 2023)\nvLLM efficiently manages this dynamic sharing using reference counting and the CoW mechanism, avoiding the frequent and costly memory copy operations found in traditional implementations. Most blocks can be shared; CoW is only needed when newly generated tokens fall into an old shared block (requiring only a single block copy).\nShared Prefix For applications where many prompts share a common prefix (e.g., system instructions, few-shot examples), vLLM can pre-compute and cache the KV cache of these Shared Prefixes into a set of physical blocks.\nFig. 11. Shared prompt example for machine translation using few-shot examples. (Image source: Kwon et al. 2023)\nWhen processing a request containing such a prefix, its logical blocks are simply mapped to the cached physical blocks (with the last block marked as CoW), thus avoiding redundant computation for the prefix part.\nScheduling and Preemption vLLM employs an FCFS scheduling policy. When GPU memory is insufficient to accommodate newly generated KV cache, preemption is necessary:\nPreemption Unit: Preemption occurs at the Sequence Group level (e.g., all candidate sequences of a beam search request). This ensures that the earliest arrived requests are served first, and the latest requests are preempted first. Recovery Mechanisms: Swapping: The KV blocks of preempted sequences are copied to CPU memory. They are swapped back to the GPU when resources become available. This is suitable for scenarios with high PCIe bandwidth and larger block sizes. Recomputation: The KV cache of preempted sequences is discarded. When resources are available, the original prompt and already generated tokens are concatenated, and the KV cache is recomputed efficiently in a single prompt phase. This is suitable for scenarios with lower PCIe bandwidth or smaller block sizes. Distributed Execution vLLM supports Megatron-LM style tensor model parallelism.\nFig. 12. vLLM system overview showing centralized scheduler and distributed workers. (Image source: Kwon et al. 2023)\nCentralized Scheduler: Contains the KV cache manager, maintaining the global mapping from logical to physical blocks. Shared Mapping: All GPU workers share the block table. Local Storage: Each worker only stores the portion of the KV cache corresponding to the attention heads it is responsible for. Execution Flow: The scheduler broadcasts input token IDs and the block table to all workers -\u003e workers execute model computation (including PagedAttention) -\u003e workers synchronize intermediate results via All-Reduce -\u003e workers return sampled results to the scheduler. Memory management information is broadcast once at the beginning of each step, requiring no synchronization between workers. Kernel Optimization To efficiently implement PagedAttention, vLLM develops custom CUDA kernels:\nFused Reshape and Block Write: Combines splitting new KV cache into blocks, reshaping the layout, and writing to the block table into a single kernel. Fused Block Read and Attention Computation: Modifies FasterTransformer’s attention kernel to read non-contiguous blocks according to the block table and compute attention on-the-fly, optimizing memory access patterns. Fused Block Copy: Batches multiple small block copy operations triggered by CoW into a single kernel launch. vLLM V1 In January 2025, the vLLM team released the alpha version of vLLM V1, a major upgrade to its core architecture. Based on development experience over the past year and a half, the V1 release revisits key design decisions, integrates various features, and simplifies the codebase.\nBuilding on the success and lessons learned from vLLM V0, vLLM V1 introduces significant upgrades to the core architecture, aiming to provide a cleaner, more modular, easily extensible, and higher-performance codebase.\nMotivation and Goals for V1 Challenges of V0: As features and hardware support expanded, V0’s code complexity increased, making it difficult to combine features effectively and accumulating technical debt. Goals of V1: A simple, modular, and easy-to-modify codebase. High performance with near-zero CPU overhead. Unify key optimizations into the architecture. Enable optimizations by default for zero-configuration. Optimized Execution Loop \u0026 API Server Fig. 13. vLLM V1’s multiprocessing architecture with an isolated EngineCore. (Image source: vLLM Blog, 2025)\nAs GPU computation speeds increase (e.g., Llama-8B inference time on H100 is only ~5ms), CPU overhead (API serving, scheduling, input preparation, decoding, streaming responses) becomes a bottleneck. V1 adopts a multiprocessing architecture:\nIsolated EngineCore: Isolates the scheduler and model executor in a core engine loop. CPU Task Offloading: Moves CPU-intensive tasks like Tokenization, multimodal input processing, Detokenization, and streaming to separate processes, executing them in parallel with the EngineCore to maximize model throughput. Simple \u0026 Flexible Scheduler Fig. 14. vLLM V1’s scheduler treats prompt and generated tokens uniformly, enabling features like chunked prefill. (Image source: vLLM Blog, 2025)\nUniform Processing: No longer distinguishes between “prefill” and “decode” phases, treating user input tokens and model-generated tokens uniformly. Simple Representation: Scheduling decisions are represented by a dictionary, e.g., {request_id: num_tokens}, specifying how many tokens to process for each request per step. Generality: This representation is sufficient to support features like Chunked Prefills, Prefix Caching, and Speculative Decoding. For example, chunked prefill is implemented by dynamically allocating the processing quantity for each request under a fixed token budget. Zero-Overhead Prefix Caching Fig. 15. Performance comparison of prefix caching in vLLM V0 and V1. V1 achieves near-zero overhead even at 0% hit rate. (Image source: vLLM Blog, 2025)\nV1 optimizes the implementation of prefix caching (based on hash matching and LRU eviction):\nOptimized Data Structures: Implements constant-time cache eviction. Reduced Python Object Overhead: Minimizes object creation. Result: Performance degradation is less than 1% even with a 0% cache hit rate. At high hit rates, performance improves severalfold. Therefore, V1 enables prefix caching by default. Clean TP Architecture (Tensor-Parallel) Fig. 16. vLLM V1’s symmetric tensor-parallel architecture using diff-based updates. (Image source: vLLM Blog, 2025)\nV1 addresses the asymmetric architecture issue in V0 caused by the coupling of the scheduler and Worker 0:\nWorker-Side State Caching: Request states are cached on the worker side. Incremental Updates: Only incremental changes (diffs) to the state are transmitted each step, greatly reducing inter-process communication. Symmetric Architecture: The scheduler and Worker 0 can run in different processes, resulting in a cleaner, symmetric architecture. Abstracted Distributed Logic: Workers behave consistently in single-GPU and multi-GPU setups. Efficient Input Preparation Fig. 17. vLLM V1 uses Persistent Batch to cache input tensors and apply diffs. (Image source: vLLM Blog, 2025)\nV0 recreates model input tensors and metadata at each step, leading to high CPU overhead. V1 adopts the Persistent Batch technique:\nCache Input Tensors: Caches input tensors. Apply Diffs: Only applies incremental changes each step. Numpy Optimization: Extensively uses Numpy operations instead of native Python operations to reduce CPU overhead in updating tensors. Comprehensive Optimizations torch.compile and Piecewise CUDA Graphs\ntorch.compile Integration: V1 fully leverages vLLM’s torch.compile integration to automatically optimize models, supporting efficient operation for various models and significantly reducing the need for manually writing CUDA kernels. Piecewise CUDA Graphs: By introducing piecewise CUDA graphs, V1 successfully overcomes the limitations of native CUDA graphs, enhancing model flexibility and performance. Enhanced Support for Multimodal LLMs\nV1 introduces several key improvements for Multimodal Large Language Models (MLLMs): Optimized Preprocessing: CPU-intensive preprocessing tasks like image decoding, cropping, and transformation are moved to non-blocking separate processes to prevent GPU work from being blocked. A preprocessing cache is also introduced to reuse processed inputs for subsequent requests, especially beneficial for identical multimodal inputs. Multimodal Prefix Caching: In addition to token ID hashes, V1 uses image hashes to identify KV cache entries containing image inputs. This improvement is particularly advantageous in multi-turn dialogue scenarios involving image inputs. Encoder Cache: For applications requiring visual encoder outputs, V1 temporarily caches visual embeddings, allowing the scheduler to process text inputs in chunks without recomputing visual embeddings at each step, thus supporting chunked-fill scheduling for MLLMs. FlashAttention 3 Integration\nDue to V1’s high dynamism (e.g., combining prefill and decode within the same batch), a flexible and high-performance attention kernel was needed. FlashAttention 3 perfectly meets this requirement, providing robust feature support while maintaining excellent performance across various use cases. Performance Comparison Thanks to architectural improvements and significantly reduced CPU overhead, V1 achieves up to 1.7x higher throughput compared to V0 (without multi-step scheduling). Performance improvements are even more pronounced for multimodal models.\nFig. 18. Performance comparison between vLLM V0 and V1 on Llama 3.1 8B \u0026 Llama 3.3 70B (1xH100). (Image source: vLLM Blog, 2025)\nFig. 19. Performance comparison between vLLM V0 and V1 on Qwen2-VL 7B (1xH100). (Image source: vLLM Blog, 2025)\nComparison Table:\nFeature vLLM V0 vLLM V1 Improvement Point Core Technology PagedAttention PagedAttention + Comprehensive Architectural Refactor Retains PagedAttention benefits, optimizes overall architecture Memory Efficiency Extremely High (Waste \u003c 4%) Extremely High (Waste \u003c 4%) Maintains high memory efficiency Memory Sharing Supported (CoW) Supported (CoW) Maintains efficient sharing CPU Overhead Relatively high, especially in complex scenarios or low-hit-rate prefix caching Significantly reduced, near-zero overhead Multiprocessing, Persistent Batch, optimized data structures, etc. Execution Loop Single process, API server tightly coupled with engine Multiprocess, API server decoupled from EngineCore, highly parallel Improves CPU/GPU parallelism, reduces blocking Scheduler Differentiates Prefill/Decode Uniform token processing, dictionary-based scheduling Simpler, more flexible, easily supports advanced features Prefix Caching Disabled by default (overhead at low hit rates) Enabled by default (zero-overhead design) Optimized for low hit rates, default enabled for ease of use Tensor Parallelism Asymmetric architecture (Scheduler+Worker0 in same process) Symmetric architecture (Scheduler \u0026 Worker separated) Cleaner architecture, IPC overhead controlled by state caching \u0026 Diffs Multimodal Support Basic support Enhanced support (non-blocking preprocessing, image prefix cache, encoder cache, etc.) Improves VLM performance and usability Compiler Integration Limited Integrated torch.compile Automated model optimization, reduces manual Kernel writing Attention Kernel Custom Kernel (based on FasterTransformer) Integrated FlashAttention 3 Adopts industry standard for better performance and feature support Performance (vs V0) Baseline Up to 1.7x throughput increase (text), MLLMs more significant Gains from comprehensive CPU overhead optimization Code Complexity Increased with features Simpler, more modular Lowers maintenance cost, facilitates community contribution \u0026 dev Other Inference Frameworks LightLLM: A Python-based lightweight inference and serving framework, known for its lightweight design, scalability, and high-speed performance, drawing on the strengths of other open-source projects like vLLM. LMDeploy: A toolkit for compressing, deploying, and serving LLMs, featuring the TurboMind inference engine, emphasizing high request throughput and efficient quantization. SGLang: A framework for efficiently executing complex LLM programs (especially those involving structured generation) through co-design of a frontend language and a backend execution engine. TGI (Text Generation Inference): Hugging Face’s production-grade LLM serving solution, widely used and supporting multiple hardware backends. It leverages vLLM’s PagedAttention kernel to provide high-concurrency, low-latency inference services. TensorRT-LLM: An open-source library from NVIDIA for optimizing and accelerating LLM inference on NVIDIA GPUs, utilizing TensorRT’s ahead-of-time compilation and deep hardware optimization capabilities. Summary vLLM, through its core technology PagedAttention, significantly alleviates the memory bottleneck in LLM serving caused by KV cache management, markedly improving memory utilization and throughput. PagedAttention, inspired by operating system paging mechanisms, enables non-contiguous storage, dynamic allocation, and efficient sharing of the KV cache (supporting parallel sampling, beam search, shared prefixes, etc.).\nBuilding on V0, vLLM V1 comprehensively refactors and optimizes the core architecture. Through a multiprocessing architecture, flexible scheduler, zero-overhead prefix caching, symmetric tensor-parallel architecture, efficient input preparation, torch.compile integration, enhanced MLLM support, and FlashAttention 3 integration, V1 further reduces CPU overhead and enhances overall system performance, flexibility, and scalability, laying a solid foundation for rapid iteration of new features in the future.\nReferences [1] Kwon, Woosuk, et al. “Efficient memory management for large language model serving with pagedattention.” Proceedings of the 29th Symposium on Operating Systems Principles. 2023.\n[2] vLLM Team. “vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention.” vLLM Blog, June 20, 2023.\n[3] vLLM Team. “vLLM V1: A Major Upgrade to vLLM’s Core Architecture.” vLLM Blog, Jan 27, 2025.\n[4] NVIDIA. “FasterTransformer.” GitHub Repository, 2023.\n[5] Yu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. “Orca: A Distributed Serving System for Transformer-Based Generative Models.” In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 2022.\n[6] OpenAI. “API Reference - Streaming.” OpenAI Platform Documentation, 2025.\n[7] Wolf, Thomas, et al. “Transformers: State-of-the-Art Natural Language Processing.” In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.\n[8] Hugging Face. “Text Generation Inference.” GitHub Repository, 2025.\n[9] Shoeybi, Mohammad, et al. “Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.” arXiv preprint arXiv:1909.08053 (2019).\n[10] InternLM Team. “LMDeploy.” GitHub Repository, 2025.\n[12] Shah, Jay, et al. “Flashattention-3: Fast and accurate attention with asynchrony and low-precision.” Advances in Neural Information Processing Systems 37 (2024): 68658-68685.\n[13] ModelTC. “LightLLM.” GitHub Repository, 2025.\n[14] Zheng, Lianmin, et al. “Sglang: Efficient execution of structured language model programs.” Advances in Neural Information Processing Systems 37 (2024): 62557-62583.\n[15] NVIDIA. “TensorRT-LLM.” GitHub Repository, 2025.\n[16] vLLM Team. “NYC vLLM Meetup Presentation.” Google Slides, 2025.\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui. (May 2025). vLLM: High-Throughput, Memory-Efficient LLM Serving. https://syhya.github.io/posts/2025-05-17-vllm\nOr\n@article{syhya2025vllm-en, title = \"vLLM: High-Throughput, Memory-Efficient LLM Serving\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"May\", url = \"https://syhya.github.io/posts/2025-05-17-vllm\" } ","wordCount":"4204","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-05-17T10:00:00+08:00","dateModified":"2025-06-16T14:57:08+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-05-17-vllm/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">vLLM: High-Throughput, Memory-Efficient LLM Serving</h1><div class=post-meta><span title='2025-05-17 10:00:00 +0800 +0800'>2025-05-17</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp;4204 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-05-17-vllm/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#evaluation-metrics>Evaluation Metrics</a><ul><li><a href=#time-to-first-token-ttft>Time To First Token (TTFT)</a></li><li><a href=#time-per-output-token-tpot>Time Per Output Token (TPOT)</a></li><li><a href=#latency>Latency</a></li><li><a href=#throughput>Throughput</a></li><li><a href=#inter-token-latency-itl>Inter-Token Latency (ITL)</a></li></ul></li><li><a href=#vllm-v0>vLLM V0</a><ul><li><a href=#batching>Batching</a></li><li><a href=#kv-cache>KV Cache</a></li><li><a href=#pagedattention>PagedAttention</a></li><li><a href=#kv-cache-manager>KV Cache Manager</a></li><li><a href=#memory-sharing>Memory Sharing</a><ul><li><a href=#parallel-sampling>Parallel Sampling</a></li><li><a href=#beam-search>Beam Search</a></li><li><a href=#shared-prefix>Shared Prefix</a></li></ul></li><li><a href=#scheduling-and-preemption>Scheduling and Preemption</a></li><li><a href=#distributed-execution>Distributed Execution</a></li><li><a href=#kernel-optimization>Kernel Optimization</a></li></ul></li><li><a href=#vllm-v1>vLLM V1</a><ul><li><a href=#motivation-and-goals-for-v1>Motivation and Goals for V1</a></li><li><a href=#optimized-execution-loop--api-server>Optimized Execution Loop & API Server</a></li><li><a href=#simple--flexible-scheduler>Simple & Flexible Scheduler</a></li><li><a href=#zero-overhead-prefix-caching>Zero-Overhead Prefix Caching</a></li><li><a href=#clean-tp-architecture-tensor-parallel>Clean TP Architecture (Tensor-Parallel)</a></li><li><a href=#efficient-input-preparation>Efficient Input Preparation</a></li><li><a href=#comprehensive-optimizations>Comprehensive Optimizations</a></li><li><a href=#performance-comparison>Performance Comparison</a></li></ul></li><li><a href=#other-inference-frameworks>Other Inference Frameworks</a></li><li><a href=#summary>Summary</a></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><p>As the parameters of Large Language Models (LLMs) continue to grow, deploying and serving these models presents significant challenges. <a href=https://github.com/vllm-project/vllm>vLLM</a> is an open-source library designed for fast, convenient, and cost-effective LLM inference and online serving. Its core lies in the <strong>PagedAttention</strong> algorithm, which efficiently manages the KV Cache in the attention mechanism.</p><h2 id=evaluation-metrics>Evaluation Metrics<a hidden class=anchor aria-hidden=true href=#evaluation-metrics>#</a></h2><p>To evaluate the performance of LLM inference and serving engines, we primarily focus on the following metrics:</p><h3 id=time-to-first-token-ttft>Time To First Token (TTFT)<a hidden class=anchor aria-hidden=true href=#time-to-first-token-ttft>#</a></h3><p><strong>Time To First Token (TTFT)</strong> refers to the time elapsed from when the model receives user input (Prompt) to when it generates the first output token. A shorter TTFT means less waiting time for the user, which is particularly important for real-time interactive scenarios; in offline scenarios, TTFT is relatively less critical.</p><h3 id=time-per-output-token-tpot>Time Per Output Token (TPOT)<a hidden class=anchor aria-hidden=true href=#time-per-output-token-tpot>#</a></h3><p><strong>Time Per Output Token (TPOT)</strong> indicates the average time required for the model to generate one new token. It directly determines the user-perceived &ldquo;speed&rdquo; of the response. To enhance user experience, <a href="https://platform.openai.com/docs/guides/streaming-responses?api-mode=responses">Streaming</a> is commonly used in practical applications. For example, if TPOT is 0.1 seconds/token, it means the model can generate about 10 tokens per second, equivalent to approximately 450 words per minute, which exceeds the reading speed of most people.</p><h3 id=latency>Latency<a hidden class=anchor aria-hidden=true href=#latency>#</a></h3><p><strong>Latency</strong> is the total time required for the model to generate a complete response for the user. It can be calculated from TTFT and TPOT using the following formula:</p>$$
\text{Latency} = \text{TTFT} + \text{TPOT} \times (\text{Number of Output Tokens})
$$<h3 id=throughput>Throughput<a hidden class=anchor aria-hidden=true href=#throughput>#</a></h3><p><strong>Throughput</strong> measures the total number of tokens (including input and output tokens) that the model inference server can generate per unit of time for all user requests. It reflects the server&rsquo;s processing efficiency and concurrency capability. The specific calculation formula is as follows:</p>$$
\text{Throughput} = \frac{\text{Batch Size} \times (\text{Number of Input Tokens} + \text{Number of Output Tokens})}{\text{End-to-End Latency}}
$$<h3 id=inter-token-latency-itl>Inter-Token Latency (ITL)<a hidden class=anchor aria-hidden=true href=#inter-token-latency-itl>#</a></h3><p><strong>Inter-Token Latency (ITL)</strong> represents the average time interval between the generation of two consecutive tokens after the first token has been generated. It reflects the speed at which each subsequent token is generated, calculated as:</p>$$
\text{ITL} = \frac{\text{End-to-End Latency} - \text{TTFT}}{\text{Batch Size} \times (\text{Number of Output Tokens} - 1)}
$$<p>These metrics reflect the inference engine&rsquo;s response speed, processing efficiency, and concurrency capabilities, serving as important benchmarks for evaluating and optimizing LLM inference performance.</p><h2 id=vllm-v0>vLLM V0<a hidden class=anchor aria-hidden=true href=#vllm-v0>#</a></h2><p>Since its initial release in June 2023, vLLM, equipped with PagedAttention, has significantly raised the performance benchmark for LLM serving. It demonstrates a notable throughput advantage over <a href=https://huggingface.co/docs/transformers/main_classes/text_generation>HuggingFace Transformers (HF)</a> and <a href=https://github.com/huggingface/text-generation-inference>Text Generation Inference (TGI)</a>, without requiring any modifications to the model architecture.</p><figure class=align-center><img loading=lazy src=vllm_v0_throughput1.png#center alt="Fig. 1. Throughput comparison (single output completion) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)" width=80%><figcaption><p>Fig. 1. Throughput comparison (single output completion) on LLaMA models. vLLM vs. HF and TGI. (Image source: <a href=https://vllm.ai/blog/2023/06/20/vllm.html>vLLM Blog, 2023</a>)</p></figcaption></figure><ul><li>Single output inference: The figure shows vLLM achieves 14x-24x higher throughput than HF and 2.2x-2.5x higher throughput than TGI.</li></ul><figure class=align-center><img loading=lazy src=vllm_v0_throughput2.png#center alt="Fig. 2. Throughput comparison (three parallel output completions) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)" width=80%><figcaption><p>Fig. 2. Throughput comparison (three parallel output completions) on LLaMA models. vLLM vs. HF and TGI. (Image source: <a href=https://vllm.ai/blog/2023/06/20/vllm.html>vLLM Blog, 2023</a>)</p></figcaption></figure><ul><li>Three-way parallel inference: The figure shows vLLM achieves 8.5x-15x higher throughput than HF and 3.3x-3.5x higher throughput than TGI.</li></ul><h3 id=batching>Batching<a hidden class=anchor aria-hidden=true href=#batching>#</a></h3><p>Traditional <strong>Dynamic Batching</strong> waits for an entire batch of requests to complete before processing the next batch. If some requests finish early, this leads to GPU idle time and reduced resource utilization.</p><p>In contrast, <strong>Continuous Batching</strong>, employed by vLLM, allows new request sequences to be dynamically inserted during batch execution. Once a sequence completes, it can be immediately replaced by a new sequence, significantly improving GPU utilization and throughput.</p><figure class=align-center><img loading=lazy src=batching.png#center alt="Fig. 3. Dynamic Batching vs Continuous Batching. (Image source: NYC vLLM Meetup, 2025)" width=100%><figcaption><p>Fig. 3. Dynamic Batching vs Continuous Batching. (Image source: <a href="https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?slide=id.g31441846c39_0_0#slide=id.g31441846c39_0_0">NYC vLLM Meetup, 2025</a>)</p></figcaption></figure><ul><li><p><strong>Dynamic Batching</strong>: As shown on the left, sequences S₁-S₄ are processed in parallel from T1-T4. At T5, S₁ and S₃ finish early. However, because S₂ and S₄ are still running, new sequences cannot join immediately, leading to partial GPU idleness. New sequences can only start after S₂ finishes at T6 and S₄ finishes at T7.</p></li><li><p><strong>Continuous Batching</strong>: As shown on the right, T1-T4 are similar to dynamic batching. However, at T5, when S₁ and S₃ complete, new sequences S₅ and S₆ can join and start processing immediately, while S₂ and S₄ continue running. When S₂ finishes at T6, S₇ can join instantly. This approach keeps the GPU almost fully utilized, greatly enhancing efficiency.</p></li></ul><h3 id=kv-cache>KV Cache<a hidden class=anchor aria-hidden=true href=#kv-cache>#</a></h3><p>The primary bottleneck in LLM serving performance is memory management. During the autoregressive decoding process, LLMs generate attention Key and Value tensors for each token in the input sequence. These tensors (KV cache) must be retained in GPU memory to generate subsequent tokens. The KV cache has the following characteristics:</p><ol><li><strong>Large:</strong> For the LLaMA-13B model, the KV cache for a single sequence can be up to 1.7 GB.</li><li><strong>Dynamic:</strong> The size of the KV cache depends on the sequence length, which is highly variable and unpredictable.</li><li><strong>Inefficient Management:</strong> Existing inference frameworks like <a href="https://github.com/NVIDIA/FasterTransformer?tab=readme-ov-file">FasterTransformer</a> and Orca (<a href=https://www.usenix.org/system/files/osdi22-yu.pdf>Yu et al. 2022</a>) typically store the KV cache in contiguous memory blocks. To handle its dynamic nature, they need to pre-allocate memory blocks large enough to accommodate the maximum possible sequence length. This leads to severe memory waste:<ul><li><strong>Internal Fragmentation:</strong> Reserved space is much larger than actually needed.</li><li><strong>External Fragmentation:</strong> Pre-allocated blocks of different sizes make it difficult to utilize memory space efficiently.</li><li><strong>Over-reservation:</strong> Space reserved for future tokens cannot be used by other requests currently.</li></ul></li></ol><p>The figure below illustrates the types of memory waste caused by KV cache management in existing inference systems:</p><figure class=align-center><img loading=lazy src=kv_cache_existing_system.png#center alt="Fig. 4. KV cache memory management in existing systems, showing reserved waste, internal fragmentation, and external fragmentation. (Image source: Kwon et al. 2023)" width=100%><figcaption><p>Fig. 4. KV cache memory management in existing systems, showing reserved waste, internal fragmentation, and external fragmentation. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>The left image below shows the memory distribution when running a 13B parameter LLM on an NVIDIA A100 GPU: <strong>approximately 65% of the memory is used for static model weights (gray), about 30% is dynamically allocated on demand for the KV cache (red)</strong> to store the attention context of preceding tokens, and a small amount of memory (yellow) is used for temporary activation computations. The right image indicates that vLLM effectively alleviates the memory bottleneck by smoothing the rapid growth of KV cache memory usage, thereby significantly enhancing batch processing capabilities and overall service throughput.</p><figure class=align-center><img loading=lazy src=memory_layout.png#center alt="Fig. 5. Left: Memory layout for a 13B LLM on an NVIDIA A100—gray is persistent parameters, red is per-request KV cache, and yellow is temporary activation memory. Right: vLLM limits rapid KV cache growth, improving throughput. (Image source: Kwon et al. 2023)" width=80%><figcaption><p>Fig. 5. Left: Memory layout for a 13B LLM on an NVIDIA A100—gray is persistent parameters, red is per-request KV cache, and yellow is temporary activation memory. Right: vLLM limits rapid KV cache growth, improving throughput. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><h3 id=pagedattention>PagedAttention<a hidden class=anchor aria-hidden=true href=#pagedattention>#</a></h3><p><strong>PagedAttention</strong> (<a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>) is inspired by <strong>Virtual Memory</strong> and <strong>Paging</strong> concepts from operating systems. It allows <strong>logically contiguous KV Cache to be stored in physically non-contiguous GPU memory</strong>.</p><p>Specifically, PagedAttention divides the KV Cache of each sequence into fixed-size <strong>Blocks</strong>. Each block contains the Key and Value vectors for a fixed number of tokens. The system maintains a <strong>Block Table</strong> that records the mapping from logical blocks to physical blocks for each sequence.</p><figure class=align-center><img loading=lazy src=PagedAttention.png#center alt="Fig. 6. Illustration of the PagedAttention algorithm, where KV vectors are stored in non-contiguous blocks. (Image source: Kwon et al. 2023)" width=70%><figcaption><p>Fig. 6. Illustration of the PagedAttention algorithm, where KV vectors are stored in non-contiguous blocks. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>The core idea of PagedAttention borrows from the virtual memory and paging mechanisms of operating systems to manage the KV cache.</p><p>Specifically, the design philosophy of PagedAttention can be summarized as follows:</p><ol><li><p><strong>Analogy</strong>:</p><ul><li><strong>Blocks</strong> of the KV cache are analogous to <strong>Pages</strong> in OS memory management.</li><li><strong>Tokens</strong> are analogous to <strong>Bytes</strong>.</li><li><strong>Sequences</strong> are analogous to <strong>Processes</strong>.</li></ul></li><li><p><strong>Mapping Mechanism</strong>: PagedAttention uses a <strong>Block Table</strong> to maintain the mapping from a sequence&rsquo;s contiguous <strong>Logical Blocks</strong> to <strong>Physical Blocks</strong>. These physical blocks can be non-contiguous in memory, much like an OS page table maps virtual addresses to physical page frames.</p></li><li><p><strong>Allocate-on-Demand</strong>: Crucially, <strong>Physical Blocks</strong> are not pre-allocated for the maximum sequence length. Instead, they are <strong>allocated on demand</strong> when new Key-Values need to be stored (i.e., when new tokens are generated).</p></li></ol><p>This on-demand, non-contiguous memory management allows PagedAttention to utilize memory more effectively, avoiding the waste and internal fragmentation caused by pre-allocating large contiguous spaces, thereby improving GPU memory utilization.</p><p>Mathematically, PagedAttention transforms attention computation into block-wise computation. Let the block size be $B$. The $j$-th Key block is $K_{j}=\left(k_{(j-1) B+1}, \ldots, k_{j B}\right)$, and the Value block is $V_{j}=\left(v_{(j-1) B+1}, \ldots, v_{j B}\right)$. For a query vector $q_i$, the attention computation becomes:</p>\[
A_{i j}=\frac{\exp \left(q_{i}^{\top} K_{j} / \sqrt{d}\right)}{\sum_{t=1}^{\lceil i / B\rceil} \exp \left(q_{i}^{\top} K_{t} \mathbf{1} / \sqrt{d}\right)}, \quad o_{i}=\sum_{j=1}^{\lceil i / B\rceil} V_{j} A_{i j}^{\top}
\]<p>where $A_{i j}=\left(a_{i,(j-1) B+1}, \ldots, a_{i, j B}\right)$ is the row vector of attention scores for the $i$-th query on the $j$-th KV block. During computation, the PagedAttention kernel efficiently identifies and fetches the required physical blocks.</p><h3 id=kv-cache-manager>KV Cache Manager<a hidden class=anchor aria-hidden=true href=#kv-cache-manager>#</a></h3><p>vLLM&rsquo;s memory manager draws inspiration from the virtual memory mechanisms of operating systems:</p><ol><li><strong>Logical vs. Physical Blocks:</strong> Each request&rsquo;s KV cache is represented as a series of logical blocks. The Block Engine on GPU worker nodes allocates physical memory and divides it into physical blocks.</li><li><strong>Block Table:</strong> Maintains the mapping from logical blocks to physical blocks for each request. Each entry records the physical block address and the number of tokens filled within the block.</li><li><strong>Dynamic Allocation:</strong> Physical blocks are allocated on demand, eliminating the need to pre-reserve space for the maximum length, thereby significantly reducing memory waste.</li></ol><figure class=align-center><img loading=lazy src=block_table.png#center alt="Fig. 7. Block table translation in vLLM. Logical blocks are mapped to non-contiguous physical blocks. (Image source: Kwon et al. 2023)" width=80%><figcaption><p>Fig. 7. Block table translation in vLLM. Logical blocks are mapped to non-contiguous physical blocks. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>Consider the example in Fig. 7:</p><ol><li><strong>Prefill Stage:</strong> The input prompt has 7 tokens. Assume a block size of 4. vLLM allocates 2 physical blocks (e.g., physical blocks 7 and 1) and updates the block table, mapping logical block 0 to physical block 7, and logical block 1 to physical block 1. The KV cache for the prompt is computed and filled into these two physical blocks. Logical block 0 is filled with 4 tokens, and logical block 1 is filled with 3 tokens, leaving 1 slot reserved.</li><li><strong>Decode Stage:</strong><ul><li><strong>Step 1:</strong> The next token is computed using PagedAttention. Since logical block 1 still has an empty slot, the new KV cache is stored directly in physical block 1, and the fill count for logical block 1 in the block table is updated.</li><li><strong>Step 2:</strong> Logical block 1 is now full. vLLM allocates a new physical block (e.g., physical block 3), updates the block table to map the new logical block 2 to physical block 3, and stores the newly generated KV cache in physical block 3.</li></ul></li></ol><p>This on-demand allocation method limits memory waste to the last block of each sequence, achieving near-optimal memory utilization (waste below 4%). This allows for batching more requests, thereby increasing throughput.</p><p>Fig. 8 shows how vLLM manages memory for two sequences. The logical blocks of the two sequences are mapped to different physical blocks reserved by the block engine on the GPU worker. This means that even logically adjacent blocks do not need to be contiguous in physical GPU memory, allowing both sequences to effectively share and utilize the physical memory space.</p><figure class=align-center><img loading=lazy src=two_requests_vllm.png#center alt="Fig. 8. Storing the KV cache of two requests concurrently in vLLM using paged memory. (Image source: Kwon et al. 2023)" width=80%><figcaption><p>Fig. 8. Storing the KV cache of two requests concurrently in vLLM using paged memory. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><h3 id=memory-sharing>Memory Sharing<a hidden class=anchor aria-hidden=true href=#memory-sharing>#</a></h3><p>Another key advantage of PagedAttention is efficient memory sharing, especially for complex decoding strategies.</p><h4 id=parallel-sampling>Parallel Sampling<a hidden class=anchor aria-hidden=true href=#parallel-sampling>#</a></h4><p>When a request needs to generate multiple output sequences from the same prompt (e.g., code completion suggestions), the KV cache for the prompt part can be shared.</p><figure class=align-center><img loading=lazy src=parallel_sampling.png#center alt="Fig. 9. Parallel sampling example. Logical blocks for the shared prompt map to the same physical blocks. (Image source: Kwon et al. 2023)" width=80%><figcaption><p>Fig. 9. Parallel sampling example. Logical blocks for the shared prompt map to the same physical blocks. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>vLLM achieves sharing via the block table:</p><ol><li><strong>Shared Mapping:</strong> Logical blocks of different sequences can map to the same physical block.</li><li><strong>Reference Counting:</strong> Each physical block maintains a reference count.</li><li><strong>Copy-on-Write (CoW):</strong> When a shared block (reference count > 1) needs to be written to, vLLM allocates a new physical block, copies the content of the original block, updates the block table mapping for the writing sequence, and decrements the reference count of the original physical block. Subsequent writes to this physical block (when its reference count is 1) are performed directly.</li></ol><p>This mechanism significantly reduces memory overhead for <strong>Parallel Sampling</strong>, with experiments showing memory savings of up to 55%.</p><h4 id=beam-search>Beam Search<a hidden class=anchor aria-hidden=true href=#beam-search>#</a></h4><p>During <strong>Beam Search</strong>, different candidate sequences (beams) not only share the prompt part but may also share the KV cache of subsequently generated tokens. The sharing pattern changes dynamically.</p><figure class=align-center><img loading=lazy src=beam_search.png#center alt="Fig. 10. Beam search example ($k=4$). Blocks are dynamically shared and freed based on candidate survival. (Image source: Kwon et al. 2023)" width=70%><figcaption><p>Fig. 10. Beam search example ($k=4$). Blocks are dynamically shared and freed based on candidate survival. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>vLLM efficiently manages this dynamic sharing using reference counting and the CoW mechanism, avoiding the frequent and costly memory copy operations found in traditional implementations. Most blocks can be shared; CoW is only needed when newly generated tokens fall into an old shared block (requiring only a single block copy).</p><h4 id=shared-prefix>Shared Prefix<a hidden class=anchor aria-hidden=true href=#shared-prefix>#</a></h4><p>For applications where many prompts share a common prefix (e.g., system instructions, few-shot examples), vLLM can pre-compute and cache the KV cache of these <strong>Shared Prefixes</strong> into a set of physical blocks.</p><figure class=align-center><img loading=lazy src=shared_prefix.png#center alt="Fig. 11. Shared prompt example for machine translation using few-shot examples. (Image source: Kwon et al. 2023)" width=70%><figcaption><p>Fig. 11. Shared prompt example for machine translation using few-shot examples. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><p>When processing a request containing such a prefix, its logical blocks are simply mapped to the cached physical blocks (with the last block marked as CoW), thus avoiding redundant computation for the prefix part.</p><h3 id=scheduling-and-preemption>Scheduling and Preemption<a hidden class=anchor aria-hidden=true href=#scheduling-and-preemption>#</a></h3><p>vLLM employs an FCFS scheduling policy. When GPU memory is insufficient to accommodate newly generated KV cache, preemption is necessary:</p><ol><li><strong>Preemption Unit:</strong> Preemption occurs at the <strong>Sequence Group</strong> level (e.g., all candidate sequences of a beam search request). This ensures that the earliest arrived requests are served first, and the latest requests are preempted first.</li><li><strong>Recovery Mechanisms:</strong><ul><li><strong>Swapping:</strong> The KV blocks of preempted sequences are copied to CPU memory. They are swapped back to the GPU when resources become available. This is suitable for scenarios with high PCIe bandwidth and larger block sizes.</li><li><strong>Recomputation:</strong> The KV cache of preempted sequences is discarded. When resources are available, the original prompt and already generated tokens are concatenated, and the KV cache is recomputed efficiently in a single prompt phase. This is suitable for scenarios with lower PCIe bandwidth or smaller block sizes.</li></ul></li></ol><h3 id=distributed-execution>Distributed Execution<a hidden class=anchor aria-hidden=true href=#distributed-execution>#</a></h3><p>vLLM supports <a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a> style tensor model parallelism.</p><figure class=align-center><img loading=lazy src=vllm_system_overview.png#center alt="Fig. 12. vLLM system overview showing centralized scheduler and distributed workers. (Image source: Kwon et al. 2023)" width=70%><figcaption><p>Fig. 12. vLLM system overview showing centralized scheduler and distributed workers. (Image source: <a href=https://arxiv.org/abs/2309.06180>Kwon et al. 2023</a>)</p></figcaption></figure><ul><li><strong>Centralized Scheduler:</strong> Contains the KV cache manager, maintaining the global mapping from logical to physical blocks.</li><li><strong>Shared Mapping:</strong> All GPU workers share the block table.</li><li><strong>Local Storage:</strong> Each worker only stores the portion of the KV cache corresponding to the attention heads it is responsible for.</li><li><strong>Execution Flow:</strong> The scheduler broadcasts input token IDs and the block table to all workers -> workers execute model computation (including PagedAttention) -> workers synchronize intermediate results via All-Reduce -> workers return sampled results to the scheduler. Memory management information is broadcast once at the beginning of each step, requiring no synchronization between workers.</li></ul><h3 id=kernel-optimization>Kernel Optimization<a hidden class=anchor aria-hidden=true href=#kernel-optimization>#</a></h3><p>To efficiently implement PagedAttention, vLLM develops custom CUDA kernels:</p><ul><li><strong>Fused Reshape and Block Write:</strong> Combines splitting new KV cache into blocks, reshaping the layout, and writing to the block table into a single kernel.</li><li><strong>Fused Block Read and Attention Computation:</strong> Modifies FasterTransformer&rsquo;s attention kernel to read non-contiguous blocks according to the block table and compute attention on-the-fly, optimizing memory access patterns.</li><li><strong>Fused Block Copy:</strong> Batches multiple small block copy operations triggered by CoW into a single kernel launch.</li></ul><h2 id=vllm-v1>vLLM V1<a hidden class=anchor aria-hidden=true href=#vllm-v1>#</a></h2><p>In January 2025, the vLLM team released the alpha version of <strong>vLLM V1</strong>, a major upgrade to its core architecture. Based on development experience over the past year and a half, the V1 release revisits key design decisions, integrates various features, and simplifies the codebase.</p><p>Building on the success and lessons learned from vLLM V0, vLLM V1 introduces significant upgrades to the core architecture, aiming to provide a cleaner, more modular, easily extensible, and higher-performance codebase.</p><h3 id=motivation-and-goals-for-v1>Motivation and Goals for V1<a hidden class=anchor aria-hidden=true href=#motivation-and-goals-for-v1>#</a></h3><ul><li><strong>Challenges of V0:</strong> As features and hardware support expanded, V0&rsquo;s code complexity increased, making it difficult to combine features effectively and accumulating technical debt.</li><li><strong>Goals of V1:</strong><ul><li>A simple, modular, and easy-to-modify codebase.</li><li>High performance with near-zero CPU overhead.</li><li>Unify key optimizations into the architecture.</li><li>Enable optimizations by default for zero-configuration.</li></ul></li></ul><h3 id=optimized-execution-loop--api-server>Optimized Execution Loop & API Server<a hidden class=anchor aria-hidden=true href=#optimized-execution-loop--api-server>#</a></h3><figure class=align-center><img loading=lazy src=vllm_v1_architecture.png#center alt="Fig. 13. vLLM V1&rsquo;s multiprocessing architecture with an isolated EngineCore. (Image source: vLLM Blog, 2025)" width=80%><figcaption><p>Fig. 13. vLLM V1&rsquo;s multiprocessing architecture with an isolated EngineCore. (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><p>As GPU computation speeds increase (e.g., Llama-8B inference time on H100 is only ~5ms), CPU overhead (API serving, scheduling, input preparation, decoding, streaming responses) becomes a bottleneck. V1 adopts a <strong>multiprocessing architecture</strong>:</p><ul><li><strong>Isolated EngineCore:</strong> Isolates the scheduler and model executor in a core engine loop.</li><li><strong>CPU Task Offloading:</strong> Moves CPU-intensive tasks like Tokenization, multimodal input processing, Detokenization, and streaming to separate processes, executing them in parallel with the EngineCore to maximize model throughput.</li></ul><h3 id=simple--flexible-scheduler>Simple & Flexible Scheduler<a hidden class=anchor aria-hidden=true href=#simple--flexible-scheduler>#</a></h3><figure class=align-center><img loading=lazy src=v1_scheduler.png#center alt="Fig. 14. vLLM V1&rsquo;s scheduler treats prompt and generated tokens uniformly, enabling features like chunked prefill. (Image source: vLLM Blog, 2025)" width=80%><figcaption><p>Fig. 14. vLLM V1&rsquo;s scheduler treats prompt and generated tokens uniformly, enabling features like chunked prefill. (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><ul><li><strong>Uniform Processing:</strong> No longer distinguishes between &ldquo;prefill&rdquo; and &ldquo;decode&rdquo; phases, treating user input tokens and model-generated tokens uniformly.</li><li><strong>Simple Representation:</strong> Scheduling decisions are represented by a dictionary, e.g., <code>{request_id: num_tokens}</code>, specifying how many tokens to process for each request per step.</li><li><strong>Generality:</strong> This representation is sufficient to support features like Chunked Prefills, Prefix Caching, and Speculative Decoding. For example, chunked prefill is implemented by dynamically allocating the processing quantity for each request under a fixed token budget.</li></ul><h3 id=zero-overhead-prefix-caching>Zero-Overhead Prefix Caching<a hidden class=anchor aria-hidden=true href=#zero-overhead-prefix-caching>#</a></h3><figure class=align-center><img loading=lazy src=prefix_caching_benchmark.png#center alt="Fig. 15. Performance comparison of prefix caching in vLLM V0 and V1. V1 achieves near-zero overhead even at 0% hit rate. (Image source: vLLM Blog, 2025)" width=100%><figcaption><p>Fig. 15. Performance comparison of prefix caching in vLLM V0 and V1. V1 achieves near-zero overhead even at 0% hit rate. (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><p>V1 optimizes the implementation of prefix caching (based on hash matching and LRU eviction):</p><ul><li><strong>Optimized Data Structures:</strong> Implements constant-time cache eviction.</li><li><strong>Reduced Python Object Overhead:</strong> Minimizes object creation.</li><li><strong>Result:</strong> Performance degradation is less than 1% even with a 0% cache hit rate. At high hit rates, performance improves severalfold. Therefore, V1 enables prefix caching by default.</li></ul><h3 id=clean-tp-architecture-tensor-parallel>Clean TP Architecture (Tensor-Parallel)<a hidden class=anchor aria-hidden=true href=#clean-tp-architecture-tensor-parallel>#</a></h3><figure class=align-center><img loading=lazy src=v1_tp_architecture.png#center alt="Fig. 16. vLLM V1&rsquo;s symmetric tensor-parallel architecture using diff-based updates. (Image source: vLLM Blog, 2025)" width=80%><figcaption><p>Fig. 16. vLLM V1&rsquo;s symmetric tensor-parallel architecture using diff-based updates. (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><p>V1 addresses the asymmetric architecture issue in V0 caused by the coupling of the scheduler and Worker 0:</p><ul><li><strong>Worker-Side State Caching:</strong> Request states are cached on the worker side.</li><li><strong>Incremental Updates:</strong> Only incremental changes (diffs) to the state are transmitted each step, greatly reducing inter-process communication.</li><li><strong>Symmetric Architecture:</strong> The scheduler and Worker 0 can run in different processes, resulting in a cleaner, symmetric architecture.</li><li><strong>Abstracted Distributed Logic:</strong> Workers behave consistently in single-GPU and multi-GPU setups.</li></ul><h3 id=efficient-input-preparation>Efficient Input Preparation<a hidden class=anchor aria-hidden=true href=#efficient-input-preparation>#</a></h3><figure class=align-center><img loading=lazy src=persistent_batch.png#center alt="Fig. 17. vLLM V1 uses Persistent Batch to cache input tensors and apply diffs. (Image source: vLLM Blog, 2025)" width=70%><figcaption><p>Fig. 17. vLLM V1 uses Persistent Batch to cache input tensors and apply diffs. (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><p>V0 recreates model input tensors and metadata at each step, leading to high CPU overhead. V1 adopts the <a href="https://github.com/InternLM/lmdeploy?tab=readme-ov-file">Persistent Batch</a> technique:</p><ul><li><strong>Cache Input Tensors:</strong> Caches input tensors.</li><li><strong>Apply Diffs:</strong> Only applies incremental changes each step.</li><li><strong>Numpy Optimization:</strong> Extensively uses Numpy operations instead of native Python operations to reduce CPU overhead in updating tensors.</li></ul><h3 id=comprehensive-optimizations>Comprehensive Optimizations<a hidden class=anchor aria-hidden=true href=#comprehensive-optimizations>#</a></h3><ol><li><p><strong>torch.compile and Piecewise CUDA Graphs</strong></p><ul><li><strong><code>torch.compile</code> Integration:</strong> V1 fully leverages vLLM&rsquo;s <code>torch.compile</code> integration to automatically optimize models, supporting efficient operation for various models and significantly reducing the need for manually writing CUDA kernels.</li><li><strong>Piecewise CUDA Graphs:</strong> By introducing piecewise CUDA graphs, V1 successfully overcomes the limitations of native CUDA graphs, enhancing model flexibility and performance.</li></ul></li><li><p><strong>Enhanced Support for Multimodal LLMs</strong></p><ul><li>V1 introduces several key improvements for Multimodal Large Language Models (MLLMs):<ul><li><strong>Optimized Preprocessing:</strong> CPU-intensive preprocessing tasks like image decoding, cropping, and transformation are moved to non-blocking separate processes to prevent GPU work from being blocked. A preprocessing cache is also introduced to reuse processed inputs for subsequent requests, especially beneficial for identical multimodal inputs.</li><li><strong>Multimodal Prefix Caching:</strong> In addition to token ID hashes, V1 uses image hashes to identify KV cache entries containing image inputs. This improvement is particularly advantageous in multi-turn dialogue scenarios involving image inputs.</li><li><strong>Encoder Cache:</strong> For applications requiring visual encoder outputs, V1 temporarily caches visual embeddings, allowing the scheduler to process text inputs in chunks without recomputing visual embeddings at each step, thus supporting chunked-fill scheduling for MLLMs.</li></ul></li></ul></li><li><p><strong>FlashAttention 3 Integration</strong></p><ul><li>Due to V1&rsquo;s high dynamism (e.g., combining prefill and decode within the same batch), a flexible and high-performance attention kernel was needed. <a href=https://arxiv.org/abs/2407.08608>FlashAttention 3</a> perfectly meets this requirement, providing robust feature support while maintaining excellent performance across various use cases.</li></ul></li></ol><h3 id=performance-comparison>Performance Comparison<a hidden class=anchor aria-hidden=true href=#performance-comparison>#</a></h3><p>Thanks to architectural improvements and significantly reduced CPU overhead, V1 achieves up to 1.7x higher throughput compared to V0 (without multi-step scheduling). Performance improvements are even more pronounced for multimodal models.</p><figure class=align-center><img loading=lazy src=vllm_v1_llama3.png#center alt="Fig. 18. Performance comparison between vLLM V0 and V1 on Llama 3.1 8B & Llama 3.3 70B (1xH100). (Image source: vLLM Blog, 2025)" width=100%><figcaption><p>Fig. 18. Performance comparison between vLLM V0 and V1 on Llama 3.1 8B & Llama 3.3 70B (1xH100). (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><figure class=align-center><img loading=lazy src=v1_qwen2vl.png#center alt="Fig. 19. Performance comparison between vLLM V0 and V1 on Qwen2-VL 7B (1xH100). (Image source: vLLM Blog, 2025)" width=60%><figcaption><p>Fig. 19. Performance comparison between vLLM V0 and V1 on Qwen2-VL 7B (1xH100). (Image source: <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>vLLM Blog, 2025</a>)</p></figcaption></figure><p><strong>Comparison Table:</strong></p><table><thead><tr><th style=text-align:left>Feature</th><th style=text-align:left>vLLM V0</th><th style=text-align:left>vLLM V1</th><th style=text-align:left>Improvement Point</th></tr></thead><tbody><tr><td style=text-align:left><strong>Core Technology</strong></td><td style=text-align:left>PagedAttention</td><td style=text-align:left>PagedAttention + Comprehensive Architectural Refactor</td><td style=text-align:left>Retains PagedAttention benefits, optimizes overall architecture</td></tr><tr><td style=text-align:left><strong>Memory Efficiency</strong></td><td style=text-align:left>Extremely High (Waste &lt; 4%)</td><td style=text-align:left>Extremely High (Waste &lt; 4%)</td><td style=text-align:left>Maintains high memory efficiency</td></tr><tr><td style=text-align:left><strong>Memory Sharing</strong></td><td style=text-align:left>Supported (CoW)</td><td style=text-align:left>Supported (CoW)</td><td style=text-align:left>Maintains efficient sharing</td></tr><tr><td style=text-align:left><strong>CPU Overhead</strong></td><td style=text-align:left>Relatively high, especially in complex scenarios or low-hit-rate prefix caching</td><td style=text-align:left>Significantly reduced, near-zero overhead</td><td style=text-align:left>Multiprocessing, Persistent Batch, optimized data structures, etc.</td></tr><tr><td style=text-align:left><strong>Execution Loop</strong></td><td style=text-align:left>Single process, API server tightly coupled with engine</td><td style=text-align:left>Multiprocess, API server decoupled from EngineCore, highly parallel</td><td style=text-align:left>Improves CPU/GPU parallelism, reduces blocking</td></tr><tr><td style=text-align:left><strong>Scheduler</strong></td><td style=text-align:left>Differentiates Prefill/Decode</td><td style=text-align:left>Uniform token processing, dictionary-based scheduling</td><td style=text-align:left>Simpler, more flexible, easily supports advanced features</td></tr><tr><td style=text-align:left><strong>Prefix Caching</strong></td><td style=text-align:left>Disabled by default (overhead at low hit rates)</td><td style=text-align:left>Enabled by default (zero-overhead design)</td><td style=text-align:left>Optimized for low hit rates, default enabled for ease of use</td></tr><tr><td style=text-align:left><strong>Tensor Parallelism</strong></td><td style=text-align:left>Asymmetric architecture (Scheduler+Worker0 in same process)</td><td style=text-align:left>Symmetric architecture (Scheduler & Worker separated)</td><td style=text-align:left>Cleaner architecture, IPC overhead controlled by state caching & Diffs</td></tr><tr><td style=text-align:left><strong>Multimodal Support</strong></td><td style=text-align:left>Basic support</td><td style=text-align:left>Enhanced support (non-blocking preprocessing, image prefix cache, encoder cache, etc.)</td><td style=text-align:left>Improves VLM performance and usability</td></tr><tr><td style=text-align:left><strong>Compiler Integration</strong></td><td style=text-align:left>Limited</td><td style=text-align:left>Integrated <code>torch.compile</code></td><td style=text-align:left>Automated model optimization, reduces manual Kernel writing</td></tr><tr><td style=text-align:left><strong>Attention Kernel</strong></td><td style=text-align:left>Custom Kernel (based on FasterTransformer)</td><td style=text-align:left>Integrated FlashAttention 3</td><td style=text-align:left>Adopts industry standard for better performance and feature support</td></tr><tr><td style=text-align:left><strong>Performance (vs V0)</strong></td><td style=text-align:left>Baseline</td><td style=text-align:left>Up to 1.7x throughput increase (text), MLLMs more significant</td><td style=text-align:left>Gains from comprehensive CPU overhead optimization</td></tr><tr><td style=text-align:left><strong>Code Complexity</strong></td><td style=text-align:left>Increased with features</td><td style=text-align:left>Simpler, more modular</td><td style=text-align:left>Lowers maintenance cost, facilitates community contribution & dev</td></tr></tbody></table><h2 id=other-inference-frameworks>Other Inference Frameworks<a hidden class=anchor aria-hidden=true href=#other-inference-frameworks>#</a></h2><ul><li><a href=https://github.com/ModelTC/lightllm>LightLLM</a>: A Python-based lightweight inference and serving framework, known for its lightweight design, scalability, and high-speed performance, drawing on the strengths of other open-source projects like vLLM.</li><li><a href=https://github.com/InternLM/lmdeploy>LMDeploy</a>: A toolkit for compressing, deploying, and serving LLMs, featuring the TurboMind inference engine, emphasizing high request throughput and efficient quantization.</li><li><a href=https://github.com/sgl-project/sglang>SGLang</a>: A framework for efficiently executing complex LLM programs (especially those involving structured generation) through co-design of a frontend language and a backend execution engine.</li><li><a href=https://github.com/huggingface/text-generation-inference>TGI (Text Generation Inference)</a>: Hugging Face&rsquo;s production-grade LLM serving solution, widely used and supporting multiple hardware backends. It leverages vLLM&rsquo;s PagedAttention kernel to provide high-concurrency, low-latency inference services.</li><li><a href=https://github.com/NVIDIA/TensorRT-LLM>TensorRT-LLM</a>: An open-source library from NVIDIA for optimizing and accelerating LLM inference on NVIDIA GPUs, utilizing TensorRT&rsquo;s ahead-of-time compilation and deep hardware optimization capabilities.</li></ul><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>vLLM, through its core technology PagedAttention, significantly alleviates the memory bottleneck in LLM serving caused by KV cache management, markedly improving memory utilization and throughput. PagedAttention, inspired by operating system paging mechanisms, enables non-contiguous storage, dynamic allocation, and efficient sharing of the KV cache (supporting parallel sampling, beam search, shared prefixes, etc.).</p><p>Building on V0, vLLM V1 comprehensively refactors and optimizes the core architecture. Through a multiprocessing architecture, flexible scheduler, zero-overhead prefix caching, symmetric tensor-parallel architecture, efficient input preparation, <code>torch.compile</code> integration, enhanced MLLM support, and FlashAttention 3 integration, V1 further reduces CPU overhead and enhances overall system performance, flexibility, and scalability, laying a solid foundation for rapid iteration of new features in the future.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Kwon, Woosuk, et al. <a href=https://arxiv.org/abs/2309.06180>&ldquo;Efficient memory management for large language model serving with pagedattention.&rdquo;</a> Proceedings of the 29th Symposium on Operating Systems Principles. 2023.</p><p>[2] vLLM Team. <a href=https://vllm.ai/blog/2023/06/20/vllm.html>&ldquo;vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention.&rdquo;</a> vLLM Blog, June 20, 2023.</p><p>[3] vLLM Team. <a href=https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>&ldquo;vLLM V1: A Major Upgrade to vLLM&rsquo;s Core Architecture.&rdquo;</a> vLLM Blog, Jan 27, 2025.</p><p>[4] NVIDIA. <a href=https://github.com/NVIDIA/FasterTransformer>&ldquo;FasterTransformer.&rdquo;</a> GitHub Repository, 2023.</p><p>[5] Yu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. <a href=https://www.usenix.org/conference/osdi22/presentation/yu>&ldquo;Orca: A Distributed Serving System for Transformer-Based Generative Models.&rdquo;</a> In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 2022.</p><p>[6] OpenAI. <a href="https://platform.openai.com/docs/guides/streaming-responses?api-mode=responses">&ldquo;API Reference - Streaming.&rdquo;</a> OpenAI Platform Documentation, 2025.</p><p>[7] Wolf, Thomas, et al. <a href=https://www.aclweb.org/anthology/2020.emnlp-demos.6>&ldquo;Transformers: State-of-the-Art Natural Language Processing.&rdquo;</a> In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.</p><p>[8] Hugging Face. <a href=https://github.com/huggingface/text-generation-inference>&ldquo;Text Generation Inference.&rdquo;</a> GitHub Repository, 2025.</p><p>[9] Shoeybi, Mohammad, et al. <a href=https://arxiv.org/abs/1909.08053>&ldquo;Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.&rdquo;</a> arXiv preprint arXiv:1909.08053 (2019).</p><p>[10] InternLM Team. <a href=https://github.com/InternLM/lmdeploy>&ldquo;LMDeploy.&rdquo;</a> GitHub Repository, 2025.</p><p>[12] Shah, Jay, et al. <a href=https://arxiv.org/abs/2407.08608>&ldquo;Flashattention-3: Fast and accurate attention with asynchrony and low-precision.&rdquo;</a> Advances in Neural Information Processing Systems 37 (2024): 68658-68685.</p><p>[13] ModelTC. <a href=https://github.com/ModelTC/lightllm>&ldquo;LightLLM.&rdquo;</a> GitHub Repository, 2025.</p><p>[14] Zheng, Lianmin, et al. <a href=https://arxiv.org/abs/2312.07104>&ldquo;Sglang: Efficient execution of structured language model programs.&rdquo;</a> Advances in Neural Information Processing Systems 37 (2024): 62557-62583.</p><p>[15] NVIDIA. <a href=https://github.com/NVIDIA/TensorRT-LLM>&ldquo;TensorRT-LLM.&rdquo;</a> GitHub Repository, 2025.</p><p>[16] vLLM Team. <a href="https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?slide=id.g31441846c39_0_0#slide=id.g31441846c39_0_0">&ldquo;NYC vLLM Meetup Presentation.&rdquo;</a> Google Slides, 2025.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reprinting or citing the content of this article, please indicate the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (May 2025). vLLM: High-Throughput, Memory-Efficient LLM Serving.
<a href=https://syhya.github.io/posts/2025-05-17-vllm>https://syhya.github.io/posts/2025-05-17-vllm</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025vllm-en</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;vLLM: High-Throughput, Memory-Efficient LLM Serving&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;May&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-05-17-vllm&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/vllm/>VLLM</a></li><li><a href=https://syhya.github.io/tags/pagedattention/>PagedAttention</a></li><li><a href=https://syhya.github.io/tags/llm-serving/>LLM Serving</a></li><li><a href=https://syhya.github.io/tags/inference/>Inference</a></li><li><a href=https://syhya.github.io/tags/kv-cache/>KV Cache</a></li><li><a href=https://syhya.github.io/tags/memory-optimization/>Memory Optimization</a></li><li><a href=https://syhya.github.io/tags/llms/>LLMs</a></li><li><a href=https://syhya.github.io/tags/ai-infrastructure/>AI Infrastructure</a></li><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=next href=https://syhya.github.io/posts/2025-05-04-multimodal-llm/><span class=title>Next »</span><br><span>Multimodal Large Language Models</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM: High-Throughput, Memory-Efficient LLM Serving on x" href="https://x.com/intent/tweet/?text=vLLM%3a%20High-Throughput%2c%20Memory-Efficient%20LLM%20Serving&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-05-17-vllm%2f&amp;hashtags=vLLM%2cPagedAttention%2cLLMServing%2cInference%2cKVCache%2cMemoryOptimization%2cLLMs%2cAIInfrastructure%2cDeepLearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM: High-Throughput, Memory-Efficient LLM Serving on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-05-17-vllm%2f&amp;title=vLLM%3a%20High-Throughput%2c%20Memory-Efficient%20LLM%20Serving&amp;summary=vLLM%3a%20High-Throughput%2c%20Memory-Efficient%20LLM%20Serving&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-05-17-vllm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM: High-Throughput, Memory-Efficient LLM Serving on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-05-17-vllm%2f&title=vLLM%3a%20High-Throughput%2c%20Memory-Efficient%20LLM%20Serving"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM: High-Throughput, Memory-Efficient LLM Serving on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-05-17-vllm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM: High-Throughput, Memory-Efficient LLM Serving on whatsapp" href="https://api.whatsapp.com/send?text=vLLM%3a%20High-Throughput%2c%20Memory-Efficient%20LLM%20Serving%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-05-17-vllm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM: High-Throughput, Memory-Efficient LLM Serving on telegram" href="https://telegram.me/share/url?text=vLLM%3a%20High-Throughput%2c%20Memory-Efficient%20LLM%20Serving&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-05-17-vllm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share vLLM: High-Throughput, Memory-Efficient LLM Serving on ycombinator" href="https://news.ycombinator.com/submitlink?t=vLLM%3a%20High-Throughput%2c%20Memory-Efficient%20LLM%20Serving&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-05-17-vllm%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>