<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA | Yue Shui Blog</title><meta name=keywords content="Deep Learning,AI,Transformer,Attention Mechanism,MHA,MQA,GQA,KV Cache,NLP,LLM"><meta name=description content="Background
The Transformer (Vaswani et al., 2017) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (Devlin et al., 2018) which uses only the encoder, GPT (Radford et al., 2018) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (Touvron et al., 2023) and GPT-4 (OpenAI et al., 2024), most of which adopt a decoder-only architecture."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-01-16-group-query-attention/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-01-16-group-query-attention/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-01-16-group-query-attention/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-01-16-group-query-attention/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA"><meta property="og:description" content="Background The Transformer (Vaswani et al., 2017) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (Devlin et al., 2018) which uses only the encoder, GPT (Radford et al., 2018) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (Touvron et al., 2023) and GPT-4 (OpenAI et al., 2024), most of which adopt a decoder-only architecture."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-16T12:00:00+08:00"><meta property="article:modified_time" content="2025-01-16T12:00:00+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="AI"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="Attention Mechanism"><meta property="article:tag" content="MHA"><meta property="article:tag" content="MQA"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA"><meta name=twitter:description content="Background
The Transformer (Vaswani et al., 2017) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (Devlin et al., 2018) which uses only the encoder, GPT (Radford et al., 2018) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (Touvron et al., 2023) and GPT-4 (OpenAI et al., 2024), most of which adopt a decoder-only architecture."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA","item":"https://syhya.github.io/posts/2025-01-16-group-query-attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA","name":"Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA","description":"Background The Transformer (Vaswani et al., 2017) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (Devlin et al., 2018) which uses only the encoder, GPT (Radford et al., 2018) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (Touvron et al., 2023) and GPT-4 (OpenAI et al., 2024), most of which adopt a decoder-only architecture.\n","keywords":["Deep Learning","AI","Transformer","Attention Mechanism","MHA","MQA","GQA","KV Cache","NLP","LLM"],"articleBody":"Background The Transformer (Vaswani et al., 2017) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (Devlin et al., 2018) which uses only the encoder, GPT (Radford et al., 2018) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (Touvron et al., 2023) and GPT-4 (OpenAI et al., 2024), most of which adopt a decoder-only architecture.\nNotations Symbol Meaning \\(B\\) Batch Size \\(S\\) Sequence Length \\(d\\) Hidden Dimension / Model Size \\(H\\) Number of Heads in Multi-Head Attention \\(G\\) Number of Groups, used for Grouped-Query Attention (GQA) \\(d_{\\text{head}} = \\frac{d}{H}\\) Dimension of each attention head \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) Input sequence, with batch size \\(B\\), sequence length \\(S\\), and hidden dimension \\(d\\) \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{B \\times S \\times d}\\) Query, Key, and Value matrices after linear transformation \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) Trainable linear projection matrices for generating \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) respectively \\(W_O \\in \\mathbb{R}^{d \\times d}\\) Trainable linear projection matrix for mapping multi-head/grouped attention outputs back to dimension \\(d\\) \\(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h \\in \\mathbb{R}^{B \\times S \\times d_{\\text{head}}}\\) Query, Key, and Value sub-matrices for the \\(h\\)-th attention head \\(\\mathbf{K}^*, \\mathbf{V}^*\\) Shared \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) obtained by averaging or merging all heads’ \\(\\mathbf{K}_h, \\mathbf{V}_h\\) in Multi-Query Attention (MQA) \\(\\mathbf{q}, \\mathbf{k}\\in \\mathbb{R}^{d_{\\text{head}}}\\) Single query and key vectors used in mathematical derivations (Central Limit Theorem) in Scaled Dot-Product Attention Attention Mechanism in Transformers The core of the Transformer model is the Self-Attention Mechanism, which allows the model to dynamically focus on different parts of the sequence when processing sequential data. Specifically, given an input sequence \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) (batch size \\(B\\), sequence length \\(S\\), hidden dimension \\(d\\)), the Transformer projects it into queries (\\(\\mathbf{Q}\\)), keys (\\(\\mathbf{K}\\)), and values (\\(\\mathbf{V}\\)) through three linear layers:\n\\[ \\mathbf{Q} = \\mathbf{X} W_Q, \\quad \\mathbf{K} = \\mathbf{X} W_K, \\quad \\mathbf{V} = \\mathbf{X} W_V \\]where \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) are trainable weight matrices. MHA enhances the model’s representational capacity by splitting these projections into multiple heads, each responsible for different subspace representations.\nThere are various forms of attention mechanisms, and the Transformer relies on Scaled Dot-Product Attention: given query matrix \\(\\mathbf{Q}\\), key matrix \\(\\mathbf{K}\\), and value matrix \\(\\mathbf{V}\\), the output is a weighted sum of the value vectors, where each weight is determined by the dot product of the query with the corresponding key:\n\\[ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V} \\] Fig. 1. Scaled Dot-Product Attention. (Image source: Vaswani et al., 2017)\nMulti-Head Attention (MHA) Multi-Head Attention (MHA) splits \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) into multiple heads, each with independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\), thereby increasing the model’s capacity and flexibility:\n\\[ \\text{MHA}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]where each head is computed as:\n\\[ \\text{head}_h = \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q}_h \\mathbf{K}_h^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V}_h \\] Fig. 2. Multi-Head Attention. (Image source: Vaswani et al., 2017)\nBenefits of Using Multi-Head Attention Capturing Diverse Features: A single-head attention mechanism can only focus on one type of feature or pattern in the input sequence, whereas MHA can simultaneously focus on different features or patterns across multiple attention heads, enabling the model to understand the input data more comprehensively. Enhanced Expressive Power: Each attention head can learn different representations, enhancing the model’s expressive power. Different heads can focus on different parts or relationships within the input sequence, helping the model capture complex dependencies more effectively. Improved Stability and Performance: MHA reduces noise and instability from individual attention heads by averaging or combining multiple heads, thereby improving the model’s stability and performance. Parallel Computation: MHA allows for parallel computation since each attention head’s calculations are independent. This boosts computational efficiency, especially when using hardware accelerators like GPUs or TPUs. Softmax in Scaled Dot-Product Attention The softmax function transforms a vector \\(\\mathbf{z} = [z_1, z_2, \\dots, z_n]\\) into a probability distribution \\(\\mathbf{y} = [y_1, y_2, \\dots, y_n]\\) defined as:\n\\[ y_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)} \\quad \\text{for} \\quad i = 1, 2, \\dots, n \\]In the attention mechanism, the softmax function is used to convert the scaled dot product \\(\\tfrac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\) into attention weights:\n\\[ \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr) = \\Bigl[ \\frac{\\exp\\Bigl(\\frac{Q_1 \\cdot K_1}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_1 \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)}, \\dots, \\frac{\\exp\\Bigl(\\frac{Q_S \\cdot K_S}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_S \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)} \\Bigr] \\]In the Transformer’s attention mechanism, the scaling factor \\(\\sqrt{d_{\\text{head}}}\\) in the scaled dot-product attention formula ensures that the dot product results do not become excessively large as the vector dimension increases before applying softmax. This is primarily for the following reasons:\nPreventing Gradient Vanishing: Scaling the attention scores avoids overly large inputs to the softmax function, preventing gradients from vanishing during backpropagation.\nNumerical Stability: Scaling ensures that the input range to the softmax function remains reasonable, avoiding extreme values that could lead to numerical instability and overflow issues, especially when the vector dimensions are large. Without scaling, the dot product results could cause the softmax’s exponential function to produce excessively large values.\nMathematical Explanation: Suppose vectors \\(\\mathbf{q}\\) and \\(\\mathbf{k}\\) have independent and identically distributed components with mean 0 and variance 1. Their dot product \\(\\mathbf{q} \\cdot \\mathbf{k}\\) has a mean of 0 and a variance of \\(d_{\\text{head}}\\). To prevent the dot product’s variance from increasing with the dimension \\(d_{\\text{head}}\\), it is scaled by \\(\\frac{1}{\\sqrt{d_{\\text{head}}}}\\). This scaling ensures that the variance of the scaled dot product remains 1, independent of \\(d_{\\text{head}}\\).\nAccording to statistical principles, dividing a random variable by a constant scales its variance by the inverse square of that constant. Therefore, the scaling factor \\(\\tfrac{1}{\\sqrt{d_{\\text{head}}}}\\) effectively controls the magnitude of the attention scores, enhancing numerical stability. The detailed derivation is as follows:\nAssume \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^{d_{\\text{head}}}\\) with independent and identically distributed components, mean 0, and variance 1. Their dot product is:\n\\[ \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d_{\\text{head}}} q_i k_i \\]By the Central Limit Theorem, for large \\(d_{\\text{head}}\\), the dot product \\(\\mathbf{q} \\cdot \\mathbf{k}\\) approximately follows a normal distribution with mean 0 and variance \\(d_{\\text{head}}\\):\n\\[ \\mathbf{q} \\cdot \\mathbf{k} \\sim \\mathcal{N}(0, d_{\\text{head}}) \\]To achieve unit variance in the scaled dot product, we divide by \\(\\sqrt{d_{\\text{head}}}\\):\n\\[ \\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}} \\;\\sim\\; \\mathcal{N}\\!\\Bigl(0, \\frac{d_{\\text{head}}}{d_{\\text{head}}}\\Bigr) = \\mathcal{N}(0, 1) \\]Thus, the scaled dot product \\(\\tfrac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}}\\) maintains a variance of 1, independent of \\(d_{\\text{head}}\\). This scaling operation keeps the dot product within a stable range, preventing the softmax function from encountering numerical instability due to excessively large or small input values.\nMulti-Query Attention (MQA) Multi-Query Attention (MQA) (Shazeer, 2019) significantly reduces memory bandwidth requirements by allowing all query heads to share the same set of keys (\\(\\mathbf{K}\\)) and values (\\(\\mathbf{V}\\)). Specifically, if we average all \\(\\mathbf{K}_h\\) and \\(\\mathbf{V}_h\\) from traditional MHA as follows:\n\\[ \\mathbf{K}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{K}_h, \\quad \\mathbf{V}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{V}_h, \\]where \\(H\\) is the number of query heads, and \\(\\mathbf{K}_h\\) and \\(\\mathbf{V}_h\\) are the keys and values for the \\(h\\)-th head, respectively. During inference, each head only needs to use the same \\(\\mathbf{K}^*\\) and \\(\\mathbf{V}^*\\), significantly reducing memory bandwidth usage. Finally, all head outputs are concatenated and projected back to the output space:\n\\[ \\text{MQA}(\\mathbf{Q}, \\mathbf{K}^*, \\mathbf{V}^*) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]Since keys and values are consolidated into a single set, MQA inference is faster but may limit the model’s expressive capacity and performance in certain scenarios.\nGrouped-Query Attention (GQA) Grouped-Query Attention (GQA) (Ainslie, 2023) serves as a compromise between MHA and MQA. It divides query heads into multiple groups, allowing each group to share a set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads, thereby balancing inference speed and model performance. Each group contains \\(\\frac{H}{G}\\) query heads and shares one set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads. The specific process is as follows:\nProjection: Project the input \\(\\mathbf{X}\\) into \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) via linear transformations. Grouped Query: After splitting \\(\\mathbf{Q}\\) into \\(H\\) heads, further divide these heads into \\(G\\) groups. Grouped Key/Value: Split \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) into \\(G\\) groups, with each group sharing a set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Within-Group Attention: Perform attention calculations for each group’s \\(\\mathbf{Q}\\) with the shared \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Concatenate Outputs: Concatenate the attention results from all groups along the channel dimension and project them through a linear layer to obtain the final output. Relationship Between the Three Attention Methods Fig. 3. Overview of grouped-query method. (Image source: Ainslie et al., 2023)\nFigure 3 intuitively illustrates the relationship between the three attention mechanisms: MHA maintains independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) for each query head; MQA allows all query heads to share the same set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\); GQA strikes a balance by sharing \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) within groups.\nWhen \\(G=1\\): All query heads share the same set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). In this case, GQA degenerates into MQA.\nNumber of \\(\\mathbf{K}/\\mathbf{V}\\) Heads: 1 Model Behavior: All heads use the same \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) for attention calculations, significantly reducing memory bandwidth requirements. When \\(G=H\\): Each query head has its own independent set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). In this case, GQA degenerates into MHA.\nNumber of \\(\\mathbf{K}/\\mathbf{V}\\) Heads: \\(H\\) Model Behavior: Each head uses completely independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\), maintaining the high model capacity and performance of MHA. By adjusting the number of groups \\(G\\), GQA allows flexible switching between MHA and MQA, achieving a balance between maintaining high model performance and improving inference speed.\nImplementation Code Snippet Below is a simple PyTorch implementation of MHA 、MQA和 GQA. For GQA, two approaches are demonstrated: broadcasting and repeating.\nAdditionally, note that in the actual implementation of LLaMA3, GQA incorporates KV Cache for optimization. To keep the example concise, this part is omitted in the code below. For more comprehensive details, you can refer to the official source code in model.py.\nMHA Code Snippet multi_head_attention.py\nimport math import torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # (nums_head * head_dim = hidden_dim) assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(dropout_rate) # Define linear projection layers self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): # x has shape: (batch_size, seq_len, hidden_dim) batch_size, seq_len, _ = x.size() # Q, K, V each has shape: (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) K = self.k_proj(x) V = self.v_proj(x) # Reshaping from (batch_size, seq_len, hidden_dim) to (batch_size, seq_len, nums_head, head_dim) # Then transpose to (batch_size, nums_head, seq_len, head_dim) # q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3) # [Another approach to do it] q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) k = K.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Matrix multiplication: (batch_size, nums_head, seq_len, head_dim) * (batch_size, nums_head, head_dim, seq_len) # Resulting shape: (batch_size, nums_head, seq_len, seq_len) # Note that the scaling factor uses head_dim, not hidden_dim. attention_val = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\"attention_val shape is {attention_val.size()}\") print(f\"attention_mask shape is {attention_mask.size()}\") if attention_mask is not None: # If attention_mask is provided, it should have shape (batch_size, nums_head, seq_len, seq_len). assert attention_val.size() == attention_mask.size() attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\"-inf\")) # Apply softmax along the last dimension to get attention weights. attention_weight = torch.softmax(attention_val, dim=-1) # Dropout on attention weights attention_weight = self.dropout(attention_weight) # Multiply attention weights with V: # (batch_size, nums_head, seq_len, seq_len) * (batch_size, nums_head, seq_len, head_dim) # -\u003e (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v # Transpose back: (batch_size, nums_head, seq_len, head_dim) # -\u003e (batch_size, seq_len, nums_head, head_dim) # -\u003e (batch_size, seq_len, hidden_dim) # # Note: The transpose operation changes the dimension ordering but does not change the memory layout, # resulting in a non-contiguous tensor. The contiguous() method makes the tensor contiguous in memory, # allowing subsequent view or reshape operations without error. output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) # output = output_mid.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.hidden_dim) # # [Another approach to do it] output = self.output_proj(output_tmp) return output if __name__ == \"__main__\": x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 # attention_mask has shape: (batch_size, nums_head, seq_len, seq_len). # Here we use a lower-triangular mask to simulate causal masking. attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_head_attention = MultiHeadAttention(hidden_dim=hidden_dim, nums_head=nums_head) x_forward = multi_head_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) MQA Code Snippet multi_query_attention.py\nimport torch import torch.nn as nn import math class MultiQueryAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(p=dropout) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # For kv, project: hidden_dim -\u003e head_dim self.k_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.v_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() Q, K, V = self.q_proj(x), self.k_proj(x), self.v_proj(x) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Broadcast k and v to match q's dimensions for attention computation # k -\u003e (batch_size, 1, seq_len, head_dim) # v -\u003e (batch_size, 1, seq_len, head_dim) k = K.unsqueeze(1) v = V.unsqueeze(1) # (batch_size, head_num, seq_len, head_dim) * (batch_size, 1, head_dim, seq_len) # -\u003e (batch_size, head_num, seq_len, seq_len) attention_val = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\"attention_val shape is {attention_val.size()}\") if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\"-inf\")) attention_weight = torch.softmax(attention_val, dim=-1) print(f\"attention_weight is {attention_weight}\") attention_weight = self.dropout(attention_weight) # (batch_size, head_num, seq_len, seq_len) * (batch_size, 1, seq_len, head_dim) # -\u003e (batch_size, head_num, seq_len, head_dim) output_tmp = attention_weight @ v # -\u003e (batch_size, seq_len, head_num, head_dim) # -\u003e (batch_size, seq_len, hidden_dim) output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_tmp) return output if __name__ == \"__main__\": x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_query_attention = MultiQueryAttention(hidden_dim=hidden_dim, nums_head=nums_head, dropout=0.2) x_forward = multi_query_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) GQA Code Snippet group_query_attention.py\nimport math import torch import torch.nn as nn class GQABroadcast(nn.Module): \"\"\" Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u003c nums_kv_head \u003c nums_head: Generic Grouped Query Attention (GQA) \"\"\" def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # Total number of Q heads (H) self.nums_kv_head = nums_kv_head # Number of K, V heads (G, groups) assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head # Number of Q heads per group self.q_heads_per_group = nums_head // nums_kv_head self.dropout = nn.Dropout(dropout_rate) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # Projection output dimensions for K, V = nums_kv_head * head_dim self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask= None): batch_size, seq_len, _ = x.size() Q = self.q_proj(x) # (batch_size, seq_len, hidden_dim) K = self.k_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) V = self.v_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) # Q: (batch_size, seq_len, hidden_dim) # -\u003e (batch_size, seq_len, nums_head, head_dim) # -\u003e (batch_size, nums_head, seq_len, head_dim) # -\u003e (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2).contiguous() q = q.view(batch_size, self.nums_kv_head, self.q_heads_per_group, seq_len, self.head_dim) # K, V: (batch_size, seq_len, nums_kv_head * head_dim) # -\u003e (batch_size, seq_len, nums_kv_head, head_dim) # -\u003e (batch_size, nums_kv_head, seq_len, head_dim # -\u003e (batch_size, nums_kv_head, 1, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) # q: (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) * (batch_size, nums_kv_head, 1, head_dim, seq_len) # -\u003e (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_val = q @ k.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\"-inf\")) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) * (batch_size, nums_kv_head, 1, seq_len, head_dim) # -\u003e (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) output_tmp = attention_weight @ v # (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) # -\u003e (batch_size, nums_head, seq_len, head_dim) output_tmp = output_tmp.view(batch_size, self.nums_head, seq_len, self.head_dim) # (batch_size, nums_head, seq_len, head_dim) # -\u003e (batch_size, seq_len, nums_head, head_dim) -\u003e (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output class GQARepeat(nn.Module): \"\"\" Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u003c nums_kv_head \u003c nums_head: Generic Grouped Query Attention (GQA) \"\"\" def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head self.nums_kv_head = nums_kv_head assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head self.q_head_per_group = nums_head // nums_kv_head self.q_proj = nn.Linear(hidden_dim, nums_head * self.head_dim) self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) self.dropout = nn.Dropout(dropout_rate) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() # (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) # (batch_size, seq_len, nums_kv_head * self.head_dim) K = self.k_proj(x) V = self.v_proj(x) # -\u003e (batch_size, seq_len, nums_head, head_dim) # -\u003e (batch_size, nums_head, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # -\u003e (batch_size, seq_len, nums_kv_head, head_dim) # -\u003e (batch_size, nums_kv_head, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) # (batch_size, nums_head, seq_len, head_dim) k_repeat = k.repeat_interleave(self.q_head_per_group, dim=1) v_repeat = v.repeat_interleave(self.q_head_per_group, dim=1) # (batch_size, nums_head, seq_len, seq_len) attention_val = q @ k_repeat.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float('-inf')) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v_repeat # (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output if __name__ == \"__main__\": x = torch.randn(2, 3, 16) batch_size, seq_len, hidden_dim = x.size() nums_head = 8 head_dim = hidden_dim // nums_head nums_kv_head = 4 q_heads_per_group = nums_head // nums_kv_head # v1: Broadcast # attention_mask_v1 has shape: (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_mask_v1 = torch.tril(torch.ones(batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len)) gqa_broadcast = GQABroadcast(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v1 = gqa_broadcast.forward(x, attention_mask=attention_mask_v1) # print(x_forward_v1) print(x_forward_v1.size()) # v2: Repeat # attention_mask_v2 has shape: (batch_size, nums_head, seq_len, seq_len) attention_mask_v2 = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) gqa_repeat = GQARepeat(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v2 = gqa_repeat.forward(x, attention_mask=attention_mask_v2) # print(x_forward_v2) print(x_forward_v2.size()) Time and Space Complexity Analysis Note: The following discussion focuses on the computational complexity of a single forward pass. In training, one must also account for backward pass and parameter updates, which rely on the intermediate activations stored during the forward pass. The additional computation to calculate gradients and maintain partial derivatives usually makes the total training cost (both computation and memory usage) significantly higher—often multiple times the forward-pass cost.\nWhen analyzing different attention mechanisms (MHA, MQA, GQA), our main focus is on their time complexity and space complexity during the forward pass of either self-attention or cross-attention. Even though their implementation details (e.g., whether \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) are shared) can differ, the overall computational cost and memory usage are roughly on the same order of magnitude.\nAssume that each position in the sequence produces its own representations of query \\(\\mathbf{Q}\\), key \\(\\mathbf{K}\\), and value \\(\\mathbf{V}\\). After splitting by batch size and number of heads, their shapes can be written as:\n\\[ \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\;\\in\\; \\mathbb{R}^{B \\times H \\times S \\times d_{\\text{head}}} \\]Time Complexity Analysis General Time Complexity of Matrix Multiplication For two matrices \\(\\mathbf{A}\\) of shape \\(m \\times n\\) and \\(\\mathbf{B}\\) of shape \\(n \\times p\\), the complexity of computing the product \\(\\mathbf{A}\\mathbf{B}\\) is typically expressed as:\n\\[ \\mathcal{O}(m \\times n \\times p) \\]In attention-related computations, this formula is frequently used to analyze \\(\\mathbf{Q}\\mathbf{K}^\\top\\) and the multiplication of attention scores by \\(\\mathbf{V}\\).\nMain Steps and Complexity in Self-Attention Dot Product (\\(\\mathbf{Q}\\mathbf{K}^\\top\\))\nShape of \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nShape of \\(\\mathbf{K}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nConsequently, the result of \\(\\mathbf{Q}\\mathbf{K}^\\top\\) has shape \\(B \\times H \\times S \\times S\\)\nThe calculation can be viewed as \\(S \\times S\\) dot products for each head in each batch. Each dot product involves \\(d_{\\text{head}}\\) multiply-add operations.\nHence, its time complexity is:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S \\times S \\times d_{\\text{head}}\\bigr) \\;=\\; \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] Softmax Operation\nApplied element-wise to the attention score matrix of shape \\(B \\times H \\times S \\times S\\)\nSoftmax entails computing exponentials and performing normalization on each element. The complexity is approximately:\n\\[ \\mathcal{O}(\\text{number of elements}) = \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) \\] Compared with the matrix multiplication above, this step’s dependency on \\(d_{\\text{head}}\\) is negligible and is thus often considered a smaller overhead.\nWeighted Averaging (Multiplying Attention Scores with \\(\\mathbf{V}\\))\nShape of \\(\\mathbf{V}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nShape of the attention score matrix: \\(B \\times H \\times S \\times S\\)\nMultiplying each position’s attention scores by the corresponding \\(\\mathbf{V}\\) vector yields an output of shape \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nIts time complexity is analogous to that of \\(\\mathbf{Q}\\mathbf{K}^\\top\\):\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] Combining these three steps, the dominant costs come from the two matrix multiplications, each of complexity \\(\\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}})\\). Therefore, for a single full forward pass, the total complexity can be denoted as:\n\\[ \\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}}) = \\mathcal{O}(B \\times S^2 \\times d) \\]Here, we use \\(d_{\\text{head}} = \\frac{d}{H}\\).\nTime Complexity in Incremental Decoding/Inference with KV Cache Fig. 4. KV cache example. (Image source: Efficient NLP YouTube Channel)\nAs depicted in Figure 4, incremental decoding (especially in autoregressive generation) often employs a KV Cache to store previously computed \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Thus, one does not have to recalculate keys and values at each new time step. With every new token generated (i.e., a new time step), the following operations are performed:\nCompute \\(\\mathbf{Q}\\) for the New Token (and corresponding \\(\\mathbf{K}, \\mathbf{V}\\))\nIf only the projection weights are retained, then generating the new \\(\\mathbf{Q}\\) vector and the local \\(\\mathbf{K}, \\mathbf{V}\\) involves \\(\\mathcal{O}(d^2)\\) parameters, but this overhead is small as it is only for a single token. Perform Attention with the Existing KV Cache\nThe KV Cache stores all previous \\(\\mathbf{K}, \\mathbf{V}\\) vectors, with shape:\n\\[ B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} \\]Here, \\(S_{\\text{past}}\\) is the length of the already-generated sequence.\nThe new \\(\\mathbf{Q}\\) has shape \\(B \\times H \\times 1 \\times d_{\\text{head}}\\). Hence, computing the attention scores for the new token:\n\\[ \\mathbf{Q}\\mathbf{K}^\\top : \\; \\mathcal{O}\\bigl(B \\times H \\times 1 \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] Similarly, multiplying these scores by \\(\\mathbf{V}\\) has the same order:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] Update the KV Cache\nAppend the newly generated \\(\\mathbf{K}, \\mathbf{V}\\) to the cache, so they can be used at the subsequent time step. This merely requires a concatenation or append operation, which primarily grows the memory usage rather than incurring high compute. Thus, for incremental decoding, each new token involves:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] in computation, instead of the \\(S \\times S\\) scale for each forward pass. If one aims to generate \\(S\\) tokens in total, the cumulative complexity (under ideal conditions) becomes:\n\\[ \\sum_{k=1}^{S} \\mathcal{O}\\bigl(B \\times H \\times k \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] which is the same order as the one-shot computation. The difference is that incremental decoding computes one token at a time, thus requiring lower temporary memory usage per step and avoiding a full \\(S \\times S\\) attention score matrix at once.\nSummary of Time Complexity MHA (Multi-Head Attention): Multiple heads, each computing its own \\(\\mathbf{K}, \\mathbf{V}\\). MQA (Multi-Query Attention): Multiple heads share \\(\\mathbf{K}, \\mathbf{V}\\). GQA (Grouped Query Attention): The \\(H\\) heads are divided into \\(G\\) groups, each group sharing a single \\(\\mathbf{K}, \\mathbf{V}\\). Regardless of whether we use MHA, MQA, or GQA, in a full forward pass (or the forward portion during training), the main matrix multiplications have roughly the same complexity:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) \\]On the other hand, in incremental inference with KV Cache, the per-token complexity decreases to\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] but one must maintain and update the KV Cache over multiple decoding steps.\nSpace Complexity Analysis Space complexity encompasses both model parameters (weights) and intermediate activations needed during the forward pass—particularly the attention score matrices, weighted outputs, and potential KV Cache.\nModel Parameter Scale Parameters for the Linear Projection Layers\nProjecting the input vector of dimension \\(d\\) into \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\):\n\\[ \\underbrace{d \\times d}_{\\text{Q projection}} + \\underbrace{d \\times d}_{\\text{K projection}} + \\underbrace{d \\times d}_{\\text{V projection}} = 3d^2 \\] These parameters may be split among heads, but the total remains \\(\\mathcal{O}(d^2)\\), independent of the number of heads \\(H\\).\nOutput Merging Layer\nAfter concatenating multiple heads, there is typically another \\(d \\times d\\) linear layer to project the concatenated outputs back into dimension \\(d\\). This is also \\(\\mathcal{O}(d^2)\\).\nTherefore, combining these yields:\n\\[ 3d^2 + d^2 = 4d^2 \\] which remains \\(\\mathcal{O}(d^2)\\).\nIntermediate Activations for the Forward Pass During training or a full forward pass, the following key tensors often need to be stored:\nAttention Score Matrix\nShape: \\(B \\times H \\times S \\times S\\). Regardless of MHA, MQA, or GQA, each head (or group) computes \\(\\mathbf{Q}\\mathbf{K}^\\top\\) for the attention scores, yielding:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) \\] Weighted Output\nShape: \\(B \\times H \\times S \\times d_{\\text{head}}\\), corresponding to the contextual vectors after weighting \\(\\mathbf{V}\\). Its size is:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S \\times d\\bigr) \\] Storage of \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) for Backprop\nIn backward propagation, we need the forward outputs (or intermediate gradients). If explicitly stored, their shapes and scales are usually:\nMHA (Multi-Head Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) MQA (Multi-Query Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\) (shared): \\(B \\times S \\times d\\) GQA (Grouped Query Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\) (shared by group): \\(B \\times G \\times S \\times d_{\\text{head}}\\), where \\(G \\times d_{\\text{head}} = d\\). Space Usage in Incremental Decoding (KV Cache) In inference with incremental decoding, a KV Cache is typically used to store all previously computed keys and values, thus avoiding repeated computation for past tokens. The structure is generally as follows:\nKV Cache Dimensions (MHA example):\n\\[ \\mathbf{K}, \\mathbf{V} : B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} \\] As the generated sequence length \\(S_{\\text{past}}\\) grows, the cache usage increases linearly.\nPer-Step Attention Score Matrix:\nEach new token only requires a score matrix of shape:\n\\[ B \\times H \\times 1 \\times S_{\\text{past}} \\]which is much smaller than the \\(B \\times H \\times S \\times S\\) matrix used during training.\nTherefore, in incremental decoding, large temporary activations—such as the \\(S \\times S\\) score matrix—are not needed; however, the KV Cache itself (size \\(\\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}})\\)) must be maintained and grows along with the sequence length.\nCombined Space Complexity Training / Full Forward\nThe main activations (attention scores + weighted outputs + explicit storage of Q,K,V) add up to:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) \\]For large \\(S\\), the \\(\\mathcal{O}(B \\times H \\times S^2)\\) term tends to dominate.\nInference / Incremental Decoding (KV Cache)\nThere is no need for the full \\(S^2\\) attention matrix, but a KV Cache of size\n\\[ \\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}) \\] must be stored. This grows linearly with the decoding steps \\(S_{\\text{past}}\\).\nMeanwhile, the per-step attention matrix is only \\(B \\times H \\times 1 \\times S_{\\text{past}}\\), significantly smaller than the \\(\\mathcal{O}(S^2)\\) scenario in training.\nConclusions and Comparisons Time Complexity\nFor self-attention—whether using MHA, MQA, or GQA—in a full forward pass (which also applies to the forward portion during training), the principal matrix multiplications remain:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) \\] In incremental inference (KV Cache), each new token only requires\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\]but the KV Cache must be updated and maintained throughout the decoding sequence.\nSpace Complexity\nModel Parameters: All three attention mechanisms (MHA, MQA, GQA) reside in \\(\\mathcal{O}(d^2)\\) parameter space.\nIntermediate Activations (Training / Full Forward): Dominated by the attention score matrix and weighted outputs:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) \\] Incremental Decoding (KV Cache): Saves on the \\(\\mathcal{O}(S^2)\\) score matrix cost but requires\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] of storage for the KV Cache, which increases linearly with \\(S_{\\text{past}}\\).\nBenefits of MQA / GQA\nAlthough from a high-level perspective, MHA, MQA, and GQA share similar asymptotic complexities when \\(S\\) is large, MQA and GQA can achieve improved efficiency in practice due to key/value sharing (or partial sharing) which can reduce memory bandwidth demands and improve cache locality. Consequently, in real-world systems, they often deliver better speed and memory performance. The table below summarizes the main differences among MHA, MQA, and GQA attention mechanisms:\nFeature MHA MQA GQA Number of K/V Heads Same as number of heads (\\(H\\)) Single K/V head Number of groups (\\(G\\)), one K/V head per group Inference Time Slower Fastest Faster, but slightly slower than MQA Memory Bandwidth Requirement Highest, \\(H\\) times K/V loading Lowest, only one K/V head Between MHA and MQA, \\(G\\) times K/V loading Model Capacity Highest Lowest Moderate, depending on the number of groups \\(G\\) Performance Best Slightly lower than MHA Close to MHA, significantly better than MQA Uptraining Requirement None High, requires more stability and tuning Lower, GQA models stabilize after minimal uptraining Applicable Scenarios Applications with high performance requirements but insensitive to inference speed Scenarios requiring extremely fast inference with lower model performance demands Applications needing a balance between inference speed and model performance In summary, from a theoretical standpoint, all three attention mechanisms (MHA, MQA, GQA) share \\(\\mathcal{O}(B \\times S^2 \\times d)\\) complexity in a full pass and \\(\\mathcal{O}(B \\times S_{\\text{past}} \\times d)\\) per-step complexity in incremental decoding.\nExperimental Results Performance Testing This experiment was conducted on an environment equipped with dual NVIDIA RTX 4090 GPUs using data parallelism (DP), evenly splitting the batch size across both GPUs. The experiment only tested the performance of the forward pass, including average latency time (Time_mean, unit: ms) and peak memory usage (Peak_Mem_mean, unit: MB), to evaluate the resource requirements and efficiency of different attention mechanisms (MHA, MQA, and GQA) during the inference phase. You can get the source code in benchmark_attention.py.\nThe tests were based on Llama3 8B hyperparameters. Fig. 5. Overview of the key hyperparameters of Llama 3. (Image source: Grattafiori et al., 2024)\nThe main configuration parameters are as follows:\nTotal Layers: 32 layers. Hidden Layer Dimension: 4096. Total Number of Multi-Head Attention Heads: 32. Different Group Configurations (nums_kv_head): 32 (MHA), 1 (MQA), 8 (GQA-8). Experimental Results This section primarily introduces the experimental performance of MHA, MQA, and GQA-8 under different sequence lengths (512, 1024, and 1536), including data on latency and memory usage. For ease of comparison, the table below presents the specific test results for the three attention mechanisms.\nModel Size Method nums_kv_head Seq Length Time_mean (ms) Peak_Mem_mean (MB) Llama3 8B GQA-8 8 512 40.8777 2322.328 Llama3 8B MHA 32 512 53.0167 2706.375 Llama3 8B MQA 1 512 37.3592 2210.314 Llama3 8B GQA-8 8 1024 85.5850 6738.328 Llama3 8B MQA 1 1024 80.8002 6570.314 Llama3 8B MHA 32 1024 102.0514 7314.375 Llama3 8B GQA-8 8 1536 147.5949 13586.328 Llama3 8B MHA 32 1536 168.8142 14354.375 Llama3 8B MQA 1 1536 141.5059 13362.314 Fig. 6. Average Time Benchmark.\nFig. 7. Average Peak Memory Benchmark.\nIn scenarios sensitive to memory and time overheads, MQA and GQA-8 are more efficient choices, with MQA performing the best but potentially lacking in model performance capabilities; GQA-8 achieves a good balance between efficiency and performance.\nGQA Paper Experimental Results Inference Performance Fig. 8. Inference time and performance comparison. (Image source: Ainslie et al., 2023)\nFig. 9. Additional Experimental Results. (Image source: Ainslie et al., 2023)\nThe experimental results show that:\nInference Speed:\nMHA-XXL’s inference time is significantly higher than MHA-Large, primarily due to its larger number of heads and model size. Compared to MHA-XXL, MQA-XXL and GQA-8-XXL reduce inference time to approximately 1/6 and 1/5, respectively. Performance:\nMHA-XXL performs best across all tasks but has longer inference times. MQA-XXL has an advantage in inference speed, with average scores only slightly lower than MHA-XXL. GQA-8-XXL has inference speed close to MQA-XXL but nearly matches MHA-XXL in performance, demonstrating the efficiency and superiority of GQA. Checkpoint Conversion Fig. 10. Ablation Study on Checkpoint Conversion Methods. (Image source: Ainslie et al., 2023)\nFigure 10 compares the performance of different methods for checkpoint conversion. The mean pooling method performs best in retaining model information, followed by selecting the first head, while random initialization performs the worst. Mean pooling effectively integrates information from multiple \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads, maintaining model performance.\nUptraining Ratio Fig. 11. Ablation Study on Uptraining Ratios. (Image source: Ainslie et al., 2023)\nFigure 11 shows how performance varies with uptraining proportion for T5 XXL with MQA and GQA.\nGQA: Even with only conversion (no uptraining), GQA already has certain performance. As the uptraining ratio increases, performance continues to improve. MQA: Requires at least a 5% uptraining ratio to achieve practical performance, and as the ratio increases, performance gains tend to plateau. Effect of Number of GQA Groups on Inference Speed Fig. 12. Effect of the Number of GQA Groups on Inference Speed. (Image source: Ainslie et al., 2023)\nFigure 12 demonstrates that as the number of groups increases, inference time slightly rises, but it still maintains a significant speed advantage over MHA. Choosing an appropriate number of groups, such as 8, can achieve a good balance between speed and performance. Figure 3 also shows that models ranging from 7B to 405B parameters in Llama3 adopt 8 as the number of groups (key/value heads = 8).\nOther Optimization Methods In addition to optimizing the attention mechanism, researchers have proposed various methods to enhance the inference and training efficiency of Transformer models:\nLoRA (Hu et al., 2021): Achieves efficient parameter fine-tuning by adding low-rank matrices to the pretrained model’s weight matrices. Flash Attention (Dao et al., 2022): Reduces memory and computational overhead by optimizing attention calculations. Quantization Techniques: LLM.int8 (Dettmers et al., 2022) and GPTQ (Frantar et al., 2022) reduce memory usage and computational costs by lowering the precision of model weights and activations. Model Distillation (Hinton et al., 2015): Reduces model size by training smaller models to mimic the behavior of larger models. Speculative Sampling (Chen et al., 2023): Enhances generation efficiency through parallel generation and filtering. Key Takeaways Uptraining methods can effectively utilize existing MHA model checkpoints. By performing a small amount of additional training, they can transform these into more efficient MQA or GQA models, significantly reducing training costs. Grouped-Query Attention (GQA) strikes a good balance between inference efficiency and model performance, making it especially suitable for applications requiring both high-efficiency inference and high performance. Experimental results demonstrate that GQA can significantly improve inference speed while maintaining performance comparable to MHA models, making it suitable for large-scale model deployment and real-time applications. References [1] Vaswani A. Attention is all you need [J]. Advances in Neural Information Processing Systems, 2017.\n[2] Devlin J. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] Radford A. Improving Language Understanding by Generative Pre-Training [J]. 2018.\n[4] Touvron H, Lavril T, Izacard G, et al. LLaMA: Open and Efficient Foundation Language Models [J]. arXiv preprint arXiv:2302.13971, 2023.\n[5] Achiam J, Adler S, Agarwal S, et al. GPT-4 Technical Report [J]. arXiv preprint arXiv:2303.08774, 2023.\n[6] Shazeer N. Fast Transformer Decoding: One Write-Head is All You Need [J]. arXiv preprint arXiv:1911.02150, 2019.\n[7] Ainslie J, Lee-Thorp J, de Jong M, et al. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints [J]. arXiv preprint arXiv:2305.13245, 2023.\n[8] Hu E J, Shen Y, Wallis P, et al. LoRA: Low-Rank Adaptation of Large Language Models [J]. arXiv preprint arXiv:2106.09685, 2021.\n[9] Dettmers T, Lewis M, Belkada Y, et al. GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale [J]. Advances in Neural Information Processing Systems, 2022, 35: 30318-30332.\n[10] Frantar E, Ashkboos S, Hoefler T, et al. GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers [J]. arXiv preprint arXiv:2210.17323, 2022.\n[11] Hinton G. Distilling the Knowledge in a Neural Network [J]. arXiv preprint arXiv:1503.02531, 2015.\n[12] Chen C, Borgeaud S, Irving G, et al. Accelerating Large Language Model Decoding with Speculative Sampling [J]. arXiv preprint arXiv:2302.01318, 2023.\nCitation Citation: To reproduce or cite the content of this article, please acknowledge the original author and source.\nCited as:\nYue Shui. (Jan 2025). Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA.\nhttps://syhya.github.io/posts/2025-01-16-group-query-attention/\nOr\n@article{syhya2025gqa, title = \"Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Jan\", url = \"https://syhya.github.io/posts/2025-01-16-group-query-attention/\" } ","wordCount":"6139","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-01-16T12:00:00+08:00","dateModified":"2025-01-16T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-01-16-group-query-attention/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA</h1><div class=post-meta><span title='2025-01-16 12:00:00 +0800 +0800'>2025-01-16</span>&nbsp;·&nbsp;29 min&nbsp;·&nbsp;6139 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-01-16-group-query-attention/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#background>Background</a></li><li><a href=#notations>Notations</a></li><li><a href=#attention-mechanism-in-transformers>Attention Mechanism in Transformers</a><ul><li><a href=#multi-head-attention-mha>Multi-Head Attention (MHA)</a><ul><li><a href=#benefits-of-using-multi-head-attention>Benefits of Using Multi-Head Attention</a></li><li><a href=#softmax-in-scaled-dot-product-attention>Softmax in Scaled Dot-Product Attention</a></li></ul></li><li><a href=#multi-query-attention-mqa>Multi-Query Attention (MQA)</a></li><li><a href=#grouped-query-attention-gqa>Grouped-Query Attention (GQA)</a></li><li><a href=#relationship-between-the-three-attention-methods>Relationship Between the Three Attention Methods</a></li><li><a href=#implementation-code-snippet>Implementation Code Snippet</a></li></ul></li><li><a href=#time-and-space-complexity-analysis>Time and Space Complexity Analysis</a><ul><li><a href=#time-complexity-analysis>Time Complexity Analysis</a><ul><li><a href=#general-time-complexity-of-matrix-multiplication>General Time Complexity of Matrix Multiplication</a></li><li><a href=#main-steps-and-complexity-in-self-attention>Main Steps and Complexity in Self-Attention</a></li><li><a href=#time-complexity-in-incremental-decodinginference-with-kv-cache>Time Complexity in Incremental Decoding/Inference with KV Cache</a></li><li><a href=#summary-of-time-complexity>Summary of Time Complexity</a></li></ul></li><li><a href=#space-complexity-analysis>Space Complexity Analysis</a><ul><li><a href=#model-parameter-scale>Model Parameter Scale</a></li><li><a href=#intermediate-activations-for-the-forward-pass>Intermediate Activations for the Forward Pass</a></li><li><a href=#space-usage-in-incremental-decoding-kv-cache>Space Usage in Incremental Decoding (KV Cache)</a></li><li><a href=#combined-space-complexity>Combined Space Complexity</a></li></ul></li><li><a href=#conclusions-and-comparisons>Conclusions and Comparisons</a></li></ul></li><li><a href=#experimental-results>Experimental Results</a><ul><li><a href=#performance-testing>Performance Testing</a></li><li><a href=#experimental-results-1>Experimental Results</a></li><li><a href=#gqa-paper-experimental-results>GQA Paper Experimental Results</a><ul><li><a href=#inference-performance>Inference Performance</a></li><li><a href=#checkpoint-conversion>Checkpoint Conversion</a></li><li><a href=#uptraining-ratio>Uptraining Ratio</a></li><li><a href=#effect-of-number-of-gqa-groups-on-inference-speed>Effect of Number of GQA Groups on Inference Speed</a></li></ul></li></ul></li><li><a href=#other-optimization-methods>Other Optimization Methods</a></li><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>The Transformer (<a href=https://arxiv.org/abs/1706.03762>Vaswani et al., 2017</a>) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (<a href=https://arxiv.org/abs/1810.04805>Devlin et al., 2018</a>) which uses only the encoder, GPT (<a href=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf>Radford et al., 2018</a>) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (<a href=https://arxiv.org/abs/2302.13971>Touvron et al., 2023</a>) and GPT-4 (<a href=https://arxiv.org/abs/2303.08774>OpenAI et al., 2024</a>), most of which adopt a decoder-only architecture.</p><h2 id=notations>Notations<a hidden class=anchor aria-hidden=true href=#notations>#</a></h2><table><thead><tr><th>Symbol</th><th>Meaning</th></tr></thead><tbody><tr><td>\(B\)</td><td>Batch Size</td></tr><tr><td>\(S\)</td><td>Sequence Length</td></tr><tr><td>\(d\)</td><td>Hidden Dimension / Model Size</td></tr><tr><td>\(H\)</td><td>Number of Heads in Multi-Head Attention</td></tr><tr><td>\(G\)</td><td>Number of Groups, used for Grouped-Query Attention (GQA)</td></tr><tr><td>\(d_{\text{head}} = \frac{d}{H}\)</td><td>Dimension of each attention head</td></tr><tr><td>\(\mathbf{X} \in \mathbb{R}^{B \times S \times d}\)</td><td>Input sequence, with batch size \(B\), sequence length \(S\), and hidden dimension \(d\)</td></tr><tr><td>\(\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{B \times S \times d}\)</td><td>Query, Key, and Value matrices after linear transformation</td></tr><tr><td>\(W_Q, W_K, W_V \in \mathbb{R}^{d \times d}\)</td><td>Trainable linear projection matrices for generating \(\mathbf{Q}, \mathbf{K}, \mathbf{V}\) respectively</td></tr><tr><td>\(W_O \in \mathbb{R}^{d \times d}\)</td><td>Trainable linear projection matrix for mapping multi-head/grouped attention outputs back to dimension \(d\)</td></tr><tr><td>\(\mathbf{Q}_h, \mathbf{K}_h, \mathbf{V}_h \in \mathbb{R}^{B \times S \times d_{\text{head}}}\)</td><td>Query, Key, and Value sub-matrices for the \(h\)-th attention head</td></tr><tr><td>\(\mathbf{K}^*, \mathbf{V}^*\)</td><td>Shared \(\mathbf{K}\) and \(\mathbf{V}\) obtained by averaging or merging all heads&rsquo; \(\mathbf{K}_h, \mathbf{V}_h\) in Multi-Query Attention (MQA)</td></tr><tr><td>\(\mathbf{q}, \mathbf{k}\in \mathbb{R}^{d_{\text{head}}}\)</td><td>Single query and key vectors used in mathematical derivations (Central Limit Theorem) in Scaled Dot-Product Attention</td></tr></tbody></table><h2 id=attention-mechanism-in-transformers>Attention Mechanism in Transformers<a hidden class=anchor aria-hidden=true href=#attention-mechanism-in-transformers>#</a></h2><p>The core of the Transformer model is the <strong>Self-Attention Mechanism</strong>, which allows the model to dynamically focus on different parts of the sequence when processing sequential data. Specifically, given an input sequence \(\mathbf{X} \in \mathbb{R}^{B \times S \times d}\) (batch size \(B\), sequence length \(S\), hidden dimension \(d\)), the Transformer projects it into queries (\(\mathbf{Q}\)), keys (\(\mathbf{K}\)), and values (\(\mathbf{V}\)) through three linear layers:</p>\[
\mathbf{Q} = \mathbf{X} W_Q, \quad
\mathbf{K} = \mathbf{X} W_K, \quad
\mathbf{V} = \mathbf{X} W_V
\]<p>where \(W_Q, W_K, W_V \in \mathbb{R}^{d \times d}\) are trainable weight matrices. MHA enhances the model&rsquo;s representational capacity by splitting these projections into multiple heads, each responsible for different subspace representations.</p><p>There are various forms of attention mechanisms, and the Transformer relies on <strong>Scaled Dot-Product Attention</strong>: given query matrix \(\mathbf{Q}\), key matrix \(\mathbf{K}\), and value matrix \(\mathbf{V}\), the output is a weighted sum of the value vectors, where each weight is determined by the dot product of the query with the corresponding key:</p>\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})
= \text{softmax}\!\Bigl(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d_{\text{head}}}}\Bigr)\,\mathbf{V}
\]<figure class=align-center><img loading=lazy src=scaled_dot_product_attention.png#center alt="Fig. 1. Scaled Dot-Product Attention. (Image source: Vaswani et al., 2017)" width=50%><figcaption><p>Fig. 1. Scaled Dot-Product Attention. (Image source: <a href=https://arxiv.org/abs/1706.03762>Vaswani et al., 2017</a>)</p></figcaption></figure><h3 id=multi-head-attention-mha>Multi-Head Attention (MHA)<a hidden class=anchor aria-hidden=true href=#multi-head-attention-mha>#</a></h3><p>Multi-Head Attention (MHA) splits \(\mathbf{Q}\), \(\mathbf{K}\), and \(\mathbf{V}\) into multiple heads, each with independent \(\mathbf{K}\) and \(\mathbf{V}\), thereby increasing the model&rsquo;s capacity and flexibility:</p>\[
\text{MHA}(\mathbf{Q}, \mathbf{K}, \mathbf{V})
= \text{Concat}(\text{head}_1, \dots, \text{head}_H)\, W_O
\]<p>where each head is computed as:</p>\[
\text{head}_h
= \text{Attention}(\mathbf{Q}_h, \mathbf{K}_h, \mathbf{V}_h)
= \text{softmax}\!\Bigl(\frac{\mathbf{Q}_h \mathbf{K}_h^\top}{\sqrt{d_{\text{head}}}}\Bigr)\,\mathbf{V}_h
\]<figure class=align-center><img loading=lazy src=multi_head_attention.png#center alt="Fig. 2. Multi-Head Attention. (Image source: Vaswani et al., 2017)" width=50%><figcaption><p>Fig. 2. Multi-Head Attention. (Image source: <a href=https://arxiv.org/abs/1706.03762>Vaswani et al., 2017</a>)</p></figcaption></figure><h4 id=benefits-of-using-multi-head-attention>Benefits of Using Multi-Head Attention<a hidden class=anchor aria-hidden=true href=#benefits-of-using-multi-head-attention>#</a></h4><ul><li><strong>Capturing Diverse Features</strong>: A single-head attention mechanism can only focus on one type of feature or pattern in the input sequence, whereas MHA can simultaneously focus on different features or patterns across multiple attention heads, enabling the model to understand the input data more comprehensively.</li><li><strong>Enhanced Expressive Power</strong>: Each attention head can learn different representations, enhancing the model&rsquo;s expressive power. Different heads can focus on different parts or relationships within the input sequence, helping the model capture complex dependencies more effectively.</li><li><strong>Improved Stability and Performance</strong>: MHA reduces noise and instability from individual attention heads by averaging or combining multiple heads, thereby improving the model&rsquo;s stability and performance.</li><li><strong>Parallel Computation</strong>: MHA allows for parallel computation since each attention head&rsquo;s calculations are independent. This boosts computational efficiency, especially when using hardware accelerators like GPUs or TPUs.</li></ul><h4 id=softmax-in-scaled-dot-product-attention>Softmax in Scaled Dot-Product Attention<a hidden class=anchor aria-hidden=true href=#softmax-in-scaled-dot-product-attention>#</a></h4><p>The softmax function transforms a vector \(\mathbf{z} = [z_1, z_2, \dots, z_n]\) into a probability distribution \(\mathbf{y} = [y_1, y_2, \dots, y_n]\) defined as:</p>\[
y_i = \frac{\exp(z_i)}{\sum_{j=1}^{n} \exp(z_j)}
\quad \text{for} \quad i = 1, 2, \dots, n
\]<p>In the attention mechanism, the softmax function is used to convert the scaled dot product \(\tfrac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d_{\text{head}}}}\) into attention weights:</p>\[
\text{softmax}\!\Bigl(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d_{\text{head}}}}\Bigr)
= \Bigl[
\frac{\exp\Bigl(\frac{Q_1 \cdot K_1}{\sqrt{d_{\text{head}}}}\Bigr)}{\sum_{j=1}^{S} \exp\Bigl(\frac{Q_1 \cdot K_j}{\sqrt{d_{\text{head}}}}\Bigr)},
\dots,
\frac{\exp\Bigl(\frac{Q_S \cdot K_S}{\sqrt{d_{\text{head}}}}\Bigr)}{\sum_{j=1}^{S} \exp\Bigl(\frac{Q_S \cdot K_j}{\sqrt{d_{\text{head}}}}\Bigr)}
\Bigr]
\]<p>In the Transformer&rsquo;s attention mechanism, the scaling factor \(\sqrt{d_{\text{head}}}\) in the scaled dot-product attention formula ensures that the dot product results do not become excessively large as the vector dimension increases before applying softmax. This is primarily for the following reasons:</p><ul><li><p><strong>Preventing Gradient Vanishing</strong>: Scaling the attention scores avoids overly large inputs to the softmax function, preventing gradients from vanishing during backpropagation.</p></li><li><p><strong>Numerical Stability</strong>: Scaling ensures that the input range to the softmax function remains reasonable, avoiding extreme values that could lead to numerical instability and overflow issues, especially when the vector dimensions are large. Without scaling, the dot product results could cause the softmax&rsquo;s exponential function to produce excessively large values.</p></li><li><p><strong>Mathematical Explanation</strong>: Suppose vectors \(\mathbf{q}\) and \(\mathbf{k}\) have independent and identically distributed components with mean 0 and variance 1. Their dot product \(\mathbf{q} \cdot \mathbf{k}\) has a mean of 0 and a variance of \(d_{\text{head}}\). To prevent the dot product&rsquo;s variance from increasing with the dimension \(d_{\text{head}}\), it is scaled by \(\frac{1}{\sqrt{d_{\text{head}}}}\). This scaling ensures that the variance of the scaled dot product remains 1, independent of \(d_{\text{head}}\).</p></li></ul><p>According to statistical principles, dividing a random variable by a constant scales its variance by the inverse square of that constant. Therefore, the scaling factor \(\tfrac{1}{\sqrt{d_{\text{head}}}}\) effectively controls the magnitude of the attention scores, enhancing numerical stability. The detailed derivation is as follows:</p><p>Assume \(\mathbf{q}, \mathbf{k} \in \mathbb{R}^{d_{\text{head}}}\) with independent and identically distributed components, mean 0, and variance 1. Their dot product is:</p>\[
\mathbf{q} \cdot \mathbf{k} = \sum_{i=1}^{d_{\text{head}}} q_i k_i
\]<p>By the Central Limit Theorem, for large \(d_{\text{head}}\), the dot product \(\mathbf{q} \cdot \mathbf{k}\) approximately follows a normal distribution with mean 0 and variance \(d_{\text{head}}\):</p>\[
\mathbf{q} \cdot \mathbf{k} \sim \mathcal{N}(0, d_{\text{head}})
\]<p>To achieve unit variance in the scaled dot product, we divide by \(\sqrt{d_{\text{head}}}\):</p>\[
\frac{\mathbf{q} \cdot \mathbf{k}}{\sqrt{d_{\text{head}}}} \;\sim\; \mathcal{N}\!\Bigl(0, \frac{d_{\text{head}}}{d_{\text{head}}}\Bigr) = \mathcal{N}(0, 1)
\]<p>Thus, the scaled dot product \(\tfrac{\mathbf{q} \cdot \mathbf{k}}{\sqrt{d_{\text{head}}}}\) maintains a variance of 1, independent of \(d_{\text{head}}\). This scaling operation keeps the dot product within a stable range, preventing the softmax function from encountering numerical instability due to excessively large or small input values.</p><h3 id=multi-query-attention-mqa>Multi-Query Attention (MQA)<a hidden class=anchor aria-hidden=true href=#multi-query-attention-mqa>#</a></h3><p>Multi-Query Attention (MQA) (<a href=https://arxiv.org/abs/1911.02150>Shazeer, 2019</a>) significantly reduces memory bandwidth requirements by allowing all query heads to share the same set of keys (\(\mathbf{K}\)) and values (\(\mathbf{V}\)). Specifically, if we average all \(\mathbf{K}_h\) and \(\mathbf{V}_h\) from traditional MHA as follows:</p>\[
\mathbf{K}^* = \frac{1}{H} \sum_{h=1}^{H} \mathbf{K}_h,
\quad
\mathbf{V}^* = \frac{1}{H} \sum_{h=1}^{H} \mathbf{V}_h,
\]<p>where \(H\) is the number of query heads, and \(\mathbf{K}_h\) and \(\mathbf{V}_h\) are the keys and values for the \(h\)-th head, respectively. During inference, each head only needs to use the same \(\mathbf{K}^*\) and \(\mathbf{V}^*\), significantly reducing memory bandwidth usage. Finally, all head outputs are concatenated and projected back to the output space:</p>\[
\text{MQA}(\mathbf{Q}, \mathbf{K}^*, \mathbf{V}^*)
= \text{Concat}(\text{head}_1, \dots, \text{head}_H)\, W_O
\]<p>Since keys and values are consolidated into a single set, MQA inference is faster but may limit the model&rsquo;s expressive capacity and performance in certain scenarios.</p><h3 id=grouped-query-attention-gqa>Grouped-Query Attention (GQA)<a hidden class=anchor aria-hidden=true href=#grouped-query-attention-gqa>#</a></h3><p>Grouped-Query Attention (GQA) (<a href=https://arxiv.org/pdf/2305.13245>Ainslie, 2023</a>) serves as a compromise between MHA and MQA. It divides query heads into multiple groups, allowing each group to share a set of \(\mathbf{K}\) and \(\mathbf{V}\) heads, thereby balancing inference speed and model performance. Each group contains \(\frac{H}{G}\) query heads and shares one set of \(\mathbf{K}\) and \(\mathbf{V}\) heads. The specific process is as follows:</p><ul><li><strong>Projection</strong>: Project the input \(\mathbf{X}\) into \(\mathbf{Q}\), \(\mathbf{K}\), and \(\mathbf{V}\) via linear transformations.</li><li><strong>Grouped Query</strong>: After splitting \(\mathbf{Q}\) into \(H\) heads, further divide these heads into \(G\) groups.</li><li><strong>Grouped Key/Value</strong>: Split \(\mathbf{K}\) and \(\mathbf{V}\) into \(G\) groups, with each group sharing a set of \(\mathbf{K}\) and \(\mathbf{V}\).</li><li><strong>Within-Group Attention</strong>: Perform attention calculations for each group&rsquo;s \(\mathbf{Q}\) with the shared \(\mathbf{K}\) and \(\mathbf{V}\).</li><li><strong>Concatenate Outputs</strong>: Concatenate the attention results from all groups along the channel dimension and project them through a linear layer to obtain the final output.</li></ul><h3 id=relationship-between-the-three-attention-methods>Relationship Between the Three Attention Methods<a hidden class=anchor aria-hidden=true href=#relationship-between-the-three-attention-methods>#</a></h3><figure class=align-center><img loading=lazy src=attention_comparison.png#center alt="Fig. 3. Overview of grouped-query method. (Image source: Ainslie et al., 2023)" width=100%><figcaption><p>Fig. 3. Overview of grouped-query method. (Image source: <a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>)</p></figcaption></figure><p>Figure 3 intuitively illustrates the relationship between the three attention mechanisms: MHA maintains independent \(\mathbf{K}\) and \(\mathbf{V}\) for each query head; MQA allows all query heads to share the same set of \(\mathbf{K}\) and \(\mathbf{V}\); GQA strikes a balance by sharing \(\mathbf{K}\) and \(\mathbf{V}\) within groups.</p><ul><li><p><strong>When \(G=1\)</strong>: All query heads share the same set of \(\mathbf{K}\) and \(\mathbf{V}\). In this case, GQA degenerates into MQA.</p><ul><li><strong>Number of \(\mathbf{K}/\mathbf{V}\) Heads</strong>: 1</li><li><strong>Model Behavior</strong>: All heads use the same \(\mathbf{K}\) and \(\mathbf{V}\) for attention calculations, significantly reducing memory bandwidth requirements.</li></ul></li><li><p><strong>When \(G=H\)</strong>: Each query head has its own independent set of \(\mathbf{K}\) and \(\mathbf{V}\). In this case, GQA degenerates into MHA.</p><ul><li><strong>Number of \(\mathbf{K}/\mathbf{V}\) Heads</strong>: \(H\)</li><li><strong>Model Behavior</strong>: Each head uses completely independent \(\mathbf{K}\) and \(\mathbf{V}\), maintaining the high model capacity and performance of MHA.</li></ul></li></ul><p>By adjusting the number of groups \(G\), GQA allows flexible switching between MHA and MQA, achieving a balance between maintaining high model performance and improving inference speed.</p><h3 id=implementation-code-snippet>Implementation Code Snippet<a hidden class=anchor aria-hidden=true href=#implementation-code-snippet>#</a></h3><p>Below is a simple PyTorch implementation of <strong>MHA</strong> 、<strong>MQA</strong>和 <strong>GQA</strong>. For GQA, two approaches are demonstrated: broadcasting and repeating.</p><p>Additionally, note that in the actual implementation of LLaMA3, GQA incorporates KV Cache for optimization. To keep the example concise, this part is omitted in the code below. For more comprehensive details, you can refer to the official source code in <a href=https://github.com/meta-llama/llama3/blob/main/llama/model.py>model.py</a>.</p><p><details><summary markdown=span>MHA Code Snippet</summary><p><a href=https://github.com/syhya/syhya.github.io/blob/main/content/en/posts/2025-01-16-group-query-attention/multi_head_attention.py>multi_head_attention.py</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_head</span><span class=p>,</span> <span class=n>dropout_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span> <span class=o>=</span> <span class=n>nums_head</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (nums_head * head_dim = hidden_dim)</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>hidden_dim</span> <span class=o>%</span> <span class=n>nums_head</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span> <span class=o>//</span> <span class=n>nums_head</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Define linear projection layers</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># x has shape: (batch_size, seq_len, hidden_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Q, K, V each has shape: (batch_size, seq_len, hidden_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Reshaping from (batch_size, seq_len, hidden_dim) to (batch_size, seq_len, nums_head, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Then transpose to (batch_size, nums_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3)  # [Another approach to do it]</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>K</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=n>V</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Matrix multiplication: (batch_size, nums_head, seq_len, head_dim) * (batch_size, nums_head, head_dim, seq_len)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Resulting shape: (batch_size, nums_head, seq_len, seq_len)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Note that the scaling factor uses head_dim, not hidden_dim.</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_val</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;attention_val shape is </span><span class=si>{</span><span class=n>attention_val</span><span class=o>.</span><span class=n>size</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;attention_mask shape is </span><span class=si>{</span><span class=n>attention_mask</span><span class=o>.</span><span class=n>size</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>attention_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># If attention_mask is provided, it should have shape (batch_size, nums_head, seq_len, seq_len).</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=n>attention_val</span><span class=o>.</span><span class=n>size</span><span class=p>()</span> <span class=o>==</span> <span class=n>attention_mask</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_val</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>attention_val</span><span class=p>,</span> <span class=n>attention_mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&#34;-inf&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Apply softmax along the last dimension to get attention weights.</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weight</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attention_val</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Dropout on attention weights</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weight</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attention_weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Multiply attention weights with V:</span>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, nums_head, seq_len, seq_len) * (batch_size, nums_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, nums_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_tmp</span> <span class=o>=</span> <span class=n>attention_weight</span> <span class=o>@</span> <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Transpose back: (batch_size, nums_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, seq_len, nums_head, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, seq_len, hidden_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1>#</span>
</span></span><span class=line><span class=cl>        <span class=c1># Note: The transpose operation changes the dimension ordering but does not change the memory layout,</span>
</span></span><span class=line><span class=cl>        <span class=c1># resulting in a non-contiguous tensor. The contiguous() method makes the tensor contiguous in memory,</span>
</span></span><span class=line><span class=cl>        <span class=c1># allowing subsequent view or reshape operations without error.</span>
</span></span><span class=line><span class=cl>        <span class=n>output_tmp</span> <span class=o>=</span> <span class=n>output_tmp</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># output = output_mid.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.hidden_dim)  # # [Another approach to do it]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_proj</span><span class=p>(</span><span class=n>output_tmp</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>hidden_dim</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>nums_head</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># attention_mask has shape: (batch_size, nums_head, seq_len, seq_len).</span>
</span></span><span class=line><span class=cl>    <span class=c1># Here we use a lower-triangular mask to simulate causal masking.</span>
</span></span><span class=line><span class=cl>    <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>nums_head</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>attention_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>multi_head_attention</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>hidden_dim</span><span class=o>=</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_head</span><span class=o>=</span><span class=n>nums_head</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>x_forward</span> <span class=o>=</span> <span class=n>multi_head_attention</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>x_forward</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>x_forward</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span></code></pre></div></details></p><p><details><summary markdown=span>MQA Code Snippet</summary><p><a href=https://github.com/syhya/syhya.github.io/blob/main/content/en/posts/2025-01-16-group-query-attention/multi_query_attention.py>multi_query_attention.py</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiQueryAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_head</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span> <span class=o>=</span> <span class=n>nums_head</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>hidden_dim</span> <span class=o>%</span> <span class=n>nums_head</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span> <span class=o>//</span> <span class=n>nums_head</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>        
</span></span><span class=line><span class=cl>        <span class=c1># For kv, project: hidden_dim -&gt; head_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>*</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>*</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Broadcast k and v to match q&#39;s dimensions for attention computation</span>
</span></span><span class=line><span class=cl>        <span class=c1># k -&gt; (batch_size, 1, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># v -&gt; (batch_size, 1, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>K</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=n>V</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, head_num, seq_len, head_dim) * (batch_size, 1, head_dim, seq_len)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, head_num, seq_len, seq_len)</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_val</span> <span class=o>=</span> <span class=p>(</span><span class=n>q</span> <span class=o>@</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;attention_val  shape is </span><span class=si>{</span><span class=n>attention_val</span><span class=o>.</span><span class=n>size</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span>  <span class=n>attention_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_val</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>attention_val</span><span class=p>,</span> <span class=n>attention_mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&#34;-inf&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>          
</span></span><span class=line><span class=cl>        <span class=n>attention_weight</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attention_val</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;attention_weight is </span><span class=si>{</span><span class=n>attention_weight</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weight</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attention_weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, head_num, seq_len, seq_len) * (batch_size, 1, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, head_num, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_tmp</span> <span class=o>=</span> <span class=n>attention_weight</span> <span class=o>@</span> <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, seq_len, head_num, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, seq_len, hidden_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_tmp</span> <span class=o>=</span> <span class=n>output_tmp</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_proj</span><span class=p>(</span><span class=n>output_tmp</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>hidden_dim</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>nums_head</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>    <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>nums_head</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>attention_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>multi_query_attention</span> <span class=o>=</span> <span class=n>MultiQueryAttention</span><span class=p>(</span><span class=n>hidden_dim</span><span class=o>=</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_head</span><span class=o>=</span><span class=n>nums_head</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>x_forward</span> <span class=o>=</span> <span class=n>multi_query_attention</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>x_forward</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>x_forward</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span></code></pre></div></details></p><p><details><summary markdown=span>GQA Code Snippet</summary><p><a href=https://github.com/syhya/syhya.github.io/blob/main/content/en/posts/2025-01-16-group-query-attention/group_query_attention.py>group_query_attention.py</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GQABroadcast</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Group Query Attention (GQA) implementation:
</span></span></span><span class=line><span class=cl><span class=s2>    By configuring `nums_kv_head` (G, the number of groups), this module supports:
</span></span></span><span class=line><span class=cl><span class=s2>      - When nums_kv_head == nums_head: Multi-Head Attention (MHA)
</span></span></span><span class=line><span class=cl><span class=s2>      - When nums_kv_head == 1: Multi-Query Attention (MQA)
</span></span></span><span class=line><span class=cl><span class=s2>      - When 1 &lt; nums_kv_head &lt; nums_head: Generic Grouped Query Attention (GQA)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_head</span><span class=p>,</span> <span class=n>nums_kv_head</span><span class=p>,</span> <span class=n>dropout_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span> <span class=o>=</span> <span class=n>nums_head</span>  <span class=c1># Total number of Q heads (H)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nums_kv_head</span> <span class=o>=</span> <span class=n>nums_kv_head</span> <span class=c1># Number of K, V heads (G, groups)</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>hidden_dim</span> <span class=o>%</span> <span class=n>nums_head</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>nums_head</span> <span class=o>%</span> <span class=n>nums_kv_head</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span> <span class=o>//</span> <span class=n>nums_head</span>
</span></span><span class=line><span class=cl>        <span class=c1># Number of Q heads per group</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q_heads_per_group</span> <span class=o>=</span> <span class=n>nums_head</span> <span class=o>//</span> <span class=n>nums_kv_head</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Projection output dimensions for K, V = nums_kv_head * head_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_kv_head</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_kv_head</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span> <span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># (batch_size, seq_len, hidden_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># (batch_size, seq_len, nums_kv_head * head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># (batch_size, seq_len, nums_kv_head * head_dim)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Q: (batch_size, seq_len, hidden_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, seq_len, nums_head, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, nums_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_kv_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_heads_per_group</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># K, V: (batch_size, seq_len, nums_kv_head * head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1>#  -&gt; (batch_size, seq_len, nums_kv_head, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, nums_kv_head, seq_len, head_dim</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, nums_kv_head, 1, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>K</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_kv_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=n>V</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_kv_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># q: (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) * (batch_size, nums_kv_head, 1, head_dim, seq_len)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len)</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_val</span> <span class=o>=</span> <span class=n>q</span> <span class=o>@</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># mask</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>attention_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_val</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>attention_val</span><span class=p>,</span> <span class=n>attention_mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&#34;-inf&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># softmax</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weight</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attention_val</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># dropout</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weight</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attention_weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) * (batch_size, nums_kv_head, 1, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_tmp</span> <span class=o>=</span> <span class=n>attention_weight</span> <span class=o>@</span> <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, nums_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_tmp</span> <span class=o>=</span> <span class=n>output_tmp</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, nums_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, seq_len, nums_head, head_dim) -&gt; (batch_size, seq_len, hidden_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_concat</span> <span class=o>=</span> <span class=n>output_tmp</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_proj</span><span class=p>(</span><span class=n>output_concat</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GQARepeat</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Group Query Attention (GQA) implementation:
</span></span></span><span class=line><span class=cl><span class=s2>    By configuring `nums_kv_head` (G, the number of groups), this module supports:
</span></span></span><span class=line><span class=cl><span class=s2>      - When nums_kv_head == nums_head: Multi-Head Attention (MHA)
</span></span></span><span class=line><span class=cl><span class=s2>      - When nums_kv_head == 1: Multi-Query Attention (MQA)
</span></span></span><span class=line><span class=cl><span class=s2>      - When 1 &lt; nums_kv_head &lt; nums_head: Generic Grouped Query Attention (GQA)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_head</span><span class=p>,</span> <span class=n>nums_kv_head</span><span class=p>,</span> <span class=n>dropout_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span> <span class=o>=</span> <span class=n>nums_head</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nums_kv_head</span> <span class=o>=</span> <span class=n>nums_kv_head</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>hidden_dim</span> <span class=o>%</span> <span class=n>nums_head</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>nums_head</span> <span class=o>%</span> <span class=n>nums_kv_head</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span> <span class=o>//</span> <span class=n>nums_head</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q_head_per_group</span> <span class=o>=</span> <span class=n>nums_head</span> <span class=o>//</span> <span class=n>nums_kv_head</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_head</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_kv_head</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_kv_head</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, seq_len, hidden_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, seq_len, nums_kv_head * self.head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, seq_len, nums_head, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, nums_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, seq_len, nums_kv_head, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=c1># -&gt; (batch_size, nums_kv_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>K</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_kv_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=n>V</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>nums_kv_head</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, nums_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>k_repeat</span> <span class=o>=</span> <span class=n>k</span><span class=o>.</span><span class=n>repeat_interleave</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>q_head_per_group</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v_repeat</span> <span class=o>=</span> <span class=n>v</span><span class=o>.</span><span class=n>repeat_interleave</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>q_head_per_group</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, nums_head, seq_len, seq_len)</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_val</span> <span class=o>=</span> <span class=n>q</span> <span class=o>@</span> <span class=n>k_repeat</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># mask</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>attention_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_val</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>attention_val</span><span class=p>,</span> <span class=n>attention_mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># softmax</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weight</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attention_val</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># dropout</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_weight</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attention_weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, nums_head, seq_len, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_tmp</span> <span class=o>=</span> <span class=n>attention_weight</span> <span class=o>@</span> <span class=n>v_repeat</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (batch_size, seq_len, hidden_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>output_concat</span> <span class=o>=</span> <span class=n>output_tmp</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_proj</span><span class=p>(</span><span class=n>output_concat</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>hidden_dim</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>nums_head</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>    <span class=n>head_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span> <span class=o>//</span> <span class=n>nums_head</span>
</span></span><span class=line><span class=cl>    <span class=n>nums_kv_head</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl>    <span class=n>q_heads_per_group</span> <span class=o>=</span> <span class=n>nums_head</span> <span class=o>//</span> <span class=n>nums_kv_head</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># v1: Broadcast</span>
</span></span><span class=line><span class=cl>    <span class=c1># attention_mask_v1 has shape: (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len)</span>
</span></span><span class=line><span class=cl>    <span class=n>attention_mask_v1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>nums_kv_head</span><span class=p>,</span> <span class=n>q_heads_per_group</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>gqa_broadcast</span> <span class=o>=</span> <span class=n>GQABroadcast</span><span class=p>(</span><span class=n>hidden_dim</span><span class=o>=</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_head</span><span class=o>=</span><span class=n>nums_head</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                                <span class=n>nums_kv_head</span><span class=o>=</span><span class=n>nums_kv_head</span><span class=p>,</span> <span class=n>dropout_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_forward_v1</span> <span class=o>=</span> <span class=n>gqa_broadcast</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask_v1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># print(x_forward_v1)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>x_forward_v1</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># v2: Repeat</span>
</span></span><span class=line><span class=cl>    <span class=c1># attention_mask_v2 has shape: (batch_size, nums_head, seq_len, seq_len)</span>
</span></span><span class=line><span class=cl>    <span class=n>attention_mask_v2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>nums_head</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>,</span> <span class=n>seq_len</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>gqa_repeat</span> <span class=o>=</span> <span class=n>GQARepeat</span><span class=p>(</span><span class=n>hidden_dim</span><span class=o>=</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>nums_head</span><span class=o>=</span><span class=n>nums_head</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                                <span class=n>nums_kv_head</span><span class=o>=</span><span class=n>nums_kv_head</span><span class=p>,</span> <span class=n>dropout_rate</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x_forward_v2</span> <span class=o>=</span> <span class=n>gqa_repeat</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask_v2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># print(x_forward_v2)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>x_forward_v2</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span></code></pre></div></details></p><h2 id=time-and-space-complexity-analysis>Time and Space Complexity Analysis<a hidden class=anchor aria-hidden=true href=#time-and-space-complexity-analysis>#</a></h2><blockquote><p><strong>Note</strong>: The following discussion focuses on the computational complexity of a single <strong>forward pass</strong>. In <strong>training</strong>, one must also account for <strong>backward pass</strong> and parameter updates, which rely on the intermediate activations stored during the forward pass. The additional computation to calculate gradients and maintain partial derivatives usually makes the total training cost (both computation and memory usage) significantly higher—often multiple times the forward-pass cost.</p></blockquote><p>When analyzing different attention mechanisms (MHA, MQA, GQA), our main focus is on their time complexity and space complexity <strong>during the forward pass</strong> of either <strong>self-attention</strong> or <strong>cross-attention</strong>. Even though their implementation details (e.g., whether \(\mathbf{K}\) and \(\mathbf{V}\) are shared) can differ, the overall computational cost and memory usage are roughly on the same order of magnitude.</p><p>Assume that each position in the sequence produces its own representations of query \(\mathbf{Q}\), key \(\mathbf{K}\), and value \(\mathbf{V}\). After splitting by batch size and number of heads, their shapes can be written as:</p>\[
\mathbf{Q}, \mathbf{K}, \mathbf{V} \;\in\; \mathbb{R}^{B \times H \times S \times d_{\text{head}}}
\]<h3 id=time-complexity-analysis>Time Complexity Analysis<a hidden class=anchor aria-hidden=true href=#time-complexity-analysis>#</a></h3><h4 id=general-time-complexity-of-matrix-multiplication>General Time Complexity of Matrix Multiplication<a hidden class=anchor aria-hidden=true href=#general-time-complexity-of-matrix-multiplication>#</a></h4><p>For two matrices \(\mathbf{A}\) of shape \(m \times n\) and \(\mathbf{B}\) of shape \(n \times p\), the complexity of computing the product \(\mathbf{A}\mathbf{B}\) is typically expressed as:</p>\[
\mathcal{O}(m \times n \times p)
\]<p>In attention-related computations, this formula is frequently used to analyze \(\mathbf{Q}\mathbf{K}^\top\) and the multiplication of attention scores by \(\mathbf{V}\).</p><h4 id=main-steps-and-complexity-in-self-attention>Main Steps and Complexity in Self-Attention<a hidden class=anchor aria-hidden=true href=#main-steps-and-complexity-in-self-attention>#</a></h4><ol><li><p><strong>Dot Product (\(\mathbf{Q}\mathbf{K}^\top\))</strong></p><ul><li><p>Shape of \(\mathbf{Q}\): \(B \times H \times S \times d_{\text{head}}\)</p></li><li><p>Shape of \(\mathbf{K}\): \(B \times H \times S \times d_{\text{head}}\)</p></li><li><p>Consequently, the result of \(\mathbf{Q}\mathbf{K}^\top\) has shape \(B \times H \times S \times S\)</p></li><li><p>The calculation can be viewed as \(S \times S\) dot products for each head in each batch. Each dot product involves \(d_{\text{head}}\) multiply-add operations.</p></li><li><p>Hence, its time complexity is:</p>\[
\mathcal{O}\bigl(B \times H \times S \times S \times d_{\text{head}}\bigr)
\;=\;
\mathcal{O}\bigl(B \times H \times S^2 \times d_{\text{head}}\bigr)
\]</li></ul></li><li><p><strong>Softmax Operation</strong></p><ul><li><p>Applied element-wise to the attention score matrix of shape \(B \times H \times S \times S\)</p></li><li><p>Softmax entails computing exponentials and performing normalization on each element. The complexity is approximately:</p>\[
\mathcal{O}(\text{number of elements})
= \mathcal{O}\bigl(B \times H \times S^2\bigr)
\]</li><li><p>Compared with the matrix multiplication above, this step’s dependency on \(d_{\text{head}}\) is negligible and is thus often considered a smaller overhead.</p></li></ul></li><li><p><strong>Weighted Averaging (Multiplying Attention Scores with \(\mathbf{V}\))</strong></p><ul><li><p>Shape of \(\mathbf{V}\): \(B \times H \times S \times d_{\text{head}}\)</p></li><li><p>Shape of the attention score matrix: \(B \times H \times S \times S\)</p></li><li><p>Multiplying each position’s attention scores by the corresponding \(\mathbf{V}\) vector yields an output of shape \(B \times H \times S \times d_{\text{head}}\)</p></li><li><p>Its time complexity is analogous to that of \(\mathbf{Q}\mathbf{K}^\top\):</p>\[
\mathcal{O}\bigl(B \times H \times S^2 \times d_{\text{head}}\bigr)
\]</li></ul></li></ol><p>Combining these three steps, the dominant costs come from the two matrix multiplications, each of complexity \(\mathcal{O}(B \times H \times S^2 \times d_{\text{head}})\). Therefore, for a <strong>single full forward</strong> pass, the total complexity can be denoted as:</p>\[
\mathcal{O}(B \times H \times S^2 \times d_{\text{head}})
= \mathcal{O}(B \times S^2 \times d)
\]<p>Here, we use \(d_{\text{head}} = \frac{d}{H}\).</p><h4 id=time-complexity-in-incremental-decodinginference-with-kv-cache>Time Complexity in Incremental Decoding/Inference with KV Cache<a hidden class=anchor aria-hidden=true href=#time-complexity-in-incremental-decodinginference-with-kv-cache>#</a></h4><figure class=align-center><img loading=lazy src=kv_cache.png#center alt="Fig. 4. KV cache example. (Image source: Efficient NLP YouTube Channel)" width=90%><figcaption><p>Fig. 4. KV cache example. (Image source: <a href="https://www.youtube.com/watch?v=80bIUggRJf4">Efficient NLP YouTube Channel</a>)</p></figcaption></figure><p>As depicted in Figure 4, <strong>incremental decoding</strong> (especially in autoregressive generation) often employs a <strong>KV Cache</strong> to store previously computed \(\mathbf{K}\) and \(\mathbf{V}\). Thus, one does not have to recalculate keys and values at each new time step. With every new token generated (i.e., a new time step), the following operations are performed:</p><ol><li><p><strong>Compute \(\mathbf{Q}\) for the New Token (and corresponding \(\mathbf{K}, \mathbf{V}\))</strong></p><ul><li>If only the projection weights are retained, then generating the new \(\mathbf{Q}\) vector and the local \(\mathbf{K}, \mathbf{V}\) involves \(\mathcal{O}(d^2)\) parameters, but this overhead is small as it is only for a single token.</li></ul></li><li><p><strong>Perform Attention with the Existing KV Cache</strong></p><ul><li><p>The KV Cache stores all previous \(\mathbf{K}, \mathbf{V}\) vectors, with shape:</p>\[
B \times H \times S_{\text{past}} \times d_{\text{head}}
\]<p>Here, \(S_{\text{past}}\) is the length of the already-generated sequence.</p></li><li><p>The new \(\mathbf{Q}\) has shape \(B \times H \times 1 \times d_{\text{head}}\). Hence, computing the attention scores for the new token:</p>\[
\mathbf{Q}\mathbf{K}^\top : \; \mathcal{O}\bigl(B \times H \times 1 \times S_{\text{past}} \times d_{\text{head}}\bigr)
= \mathcal{O}\bigl(B \times H \times S_{\text{past}} \times d_{\text{head}}\bigr)
\]</li><li><p>Similarly, multiplying these scores by \(\mathbf{V}\) has the same order:</p>\[
\mathcal{O}\bigl(B \times H \times S_{\text{past}} \times d_{\text{head}}\bigr)
\]</li></ul></li><li><p><strong>Update the KV Cache</strong></p><ul><li>Append the newly generated \(\mathbf{K}, \mathbf{V}\) to the cache, so they can be used at the subsequent time step. This merely requires a concatenation or append operation, which primarily grows the memory usage rather than incurring high compute.</li></ul></li></ol><p>Thus, for incremental decoding, each new token involves:</p>\[
\mathcal{O}\bigl(B \times H \times S_{\text{past}} \times d_{\text{head}}\bigr)
\]<p>in computation, instead of the \(S \times S\) scale for each forward pass. If one aims to generate \(S\) tokens in total, the cumulative complexity (under ideal conditions) becomes:</p>\[
\sum_{k=1}^{S} \mathcal{O}\bigl(B \times H \times k \times d_{\text{head}}\bigr)
= \mathcal{O}\bigl(B \times H \times S^2 \times d_{\text{head}}\bigr)
\]<p>which is the same order as the one-shot computation. The difference is that incremental decoding computes one token at a time, thus requiring lower temporary memory usage per step and avoiding a full \(S \times S\) attention score matrix at once.</p><h4 id=summary-of-time-complexity>Summary of Time Complexity<a hidden class=anchor aria-hidden=true href=#summary-of-time-complexity>#</a></h4><ul><li><strong>MHA (Multi-Head Attention)</strong>: Multiple heads, each computing its own \(\mathbf{K}, \mathbf{V}\).</li><li><strong>MQA (Multi-Query Attention)</strong>: Multiple heads share \(\mathbf{K}, \mathbf{V}\).</li><li><strong>GQA (Grouped Query Attention)</strong>: The \(H\) heads are divided into \(G\) groups, each group sharing a single \(\mathbf{K}, \mathbf{V}\).</li></ul><p>Regardless of whether we use MHA, MQA, or GQA, in a <strong>full forward pass</strong> (or the forward portion during training), the main matrix multiplications have roughly the same complexity:</p>\[
\mathcal{O}\bigl(B \times H \times S^2 \times d_{\text{head}}\bigr)
= \mathcal{O}\bigl(B \times S^2 \times d\bigr)
\]<p>On the other hand, in <strong>incremental inference</strong> with KV Cache, the per-token complexity decreases to</p>\[
\mathcal{O}\bigl(B \times H \times S_{\text{past}} \times d_{\text{head}}\bigr)
\]<p>but one must maintain and update the KV Cache over multiple decoding steps.</p><h3 id=space-complexity-analysis>Space Complexity Analysis<a hidden class=anchor aria-hidden=true href=#space-complexity-analysis>#</a></h3><p>Space complexity encompasses both <strong>model parameters (weights)</strong> and <strong>intermediate activations</strong> needed during the forward pass—particularly the attention score matrices, weighted outputs, and potential KV Cache.</p><h4 id=model-parameter-scale>Model Parameter Scale<a hidden class=anchor aria-hidden=true href=#model-parameter-scale>#</a></h4><ol><li><p><strong>Parameters for the Linear Projection Layers</strong><br>Projecting the input vector of dimension \(d\) into \(\mathbf{Q}, \mathbf{K}, \mathbf{V}\):</p>\[
\underbrace{d \times d}_{\text{Q projection}}
+
\underbrace{d \times d}_{\text{K projection}}
+
\underbrace{d \times d}_{\text{V projection}}
= 3d^2
\]<p>These parameters may be split among heads, but the total remains \(\mathcal{O}(d^2)\), independent of the number of heads \(H\).</p></li><li><p><strong>Output Merging Layer</strong><br>After concatenating multiple heads, there is typically another \(d \times d\) linear layer to project the concatenated outputs back into dimension \(d\). This is also \(\mathcal{O}(d^2)\).<br>Therefore, combining these yields:</p>\[
3d^2 + d^2 = 4d^2
\]<p>which remains \(\mathcal{O}(d^2)\).</p></li></ol><h4 id=intermediate-activations-for-the-forward-pass>Intermediate Activations for the Forward Pass<a hidden class=anchor aria-hidden=true href=#intermediate-activations-for-the-forward-pass>#</a></h4><p>During <strong>training</strong> or a <strong>full forward</strong> pass, the following key tensors often need to be stored:</p><ol><li><p><strong>Attention Score Matrix</strong><br>Shape: \(B \times H \times S \times S\). Regardless of MHA, MQA, or GQA, each head (or group) computes \(\mathbf{Q}\mathbf{K}^\top\) for the attention scores, yielding:</p>\[
\mathcal{O}\bigl(B \times H \times S^2\bigr)
\]</li><li><p><strong>Weighted Output</strong><br>Shape: \(B \times H \times S \times d_{\text{head}}\), corresponding to the contextual vectors after weighting \(\mathbf{V}\). Its size is:</p>\[
\mathcal{O}\bigl(B \times H \times S \times d_{\text{head}}\bigr)
= \mathcal{O}\bigl(B \times S \times d\bigr)
\]</li><li><p><strong>Storage of \(\mathbf{Q}, \mathbf{K}, \mathbf{V}\) for Backprop</strong><br>In <strong>backward propagation</strong>, we need the forward outputs (or intermediate gradients). If explicitly stored, their shapes and scales are usually:</p><ul><li><strong>MHA (Multi-Head Attention)</strong><ul><li>\(\mathbf{Q}\): \(B \times H \times S \times d_{\text{head}}\)</li><li>\(\mathbf{K}, \mathbf{V}\): \(B \times H \times S \times d_{\text{head}}\)</li></ul></li><li><strong>MQA (Multi-Query Attention)</strong><ul><li>\(\mathbf{Q}\): \(B \times H \times S \times d_{\text{head}}\)</li><li>\(\mathbf{K}, \mathbf{V}\) (shared): \(B \times S \times d\)</li></ul></li><li><strong>GQA (Grouped Query Attention)</strong><ul><li>\(\mathbf{Q}\): \(B \times H \times S \times d_{\text{head}}\)</li><li>\(\mathbf{K}, \mathbf{V}\) (shared by group): \(B \times G \times S \times d_{\text{head}}\), where \(G \times d_{\text{head}} = d\).</li></ul></li></ul></li></ol><h4 id=space-usage-in-incremental-decoding-kv-cache>Space Usage in Incremental Decoding (KV Cache)<a hidden class=anchor aria-hidden=true href=#space-usage-in-incremental-decoding-kv-cache>#</a></h4><p>In <strong>inference</strong> with incremental decoding, a <strong>KV Cache</strong> is typically used to store all previously computed keys and values, thus avoiding repeated computation for past tokens. The structure is generally as follows:</p><ul><li><p><strong>KV Cache Dimensions</strong> (MHA example):</p>\[
\mathbf{K}, \mathbf{V} : B \times H \times S_{\text{past}} \times d_{\text{head}}
\]<p>As the generated sequence length \(S_{\text{past}}\) grows, the cache usage increases linearly.</p></li><li><p><strong>Per-Step Attention Score Matrix</strong>:</p><p>Each new token only requires a score matrix of shape:</p>\[
B \times H \times 1 \times S_{\text{past}}
\]<p>which is much smaller than the \(B \times H \times S \times S\) matrix used during training.</p></li></ul><p>Therefore, in <strong>incremental decoding</strong>, large temporary activations—such as the \(S \times S\) score matrix—are not needed; however, the KV Cache itself (size \(\mathcal{O}(B \times H \times S_{\text{past}} \times d_{\text{head}})\)) must be maintained and grows along with the sequence length.</p><h4 id=combined-space-complexity>Combined Space Complexity<a hidden class=anchor aria-hidden=true href=#combined-space-complexity>#</a></h4><ul><li><p><strong>Training / Full Forward</strong><br>The main activations (attention scores + weighted outputs + explicit storage of Q,K,V) add up to:</p>\[
\mathcal{O}\bigl(B \times H \times S^2 + B \times S \times d\bigr)
\]<p>For large \(S\), the \(\mathcal{O}(B \times H \times S^2)\) term tends to dominate.</p></li><li><p><strong>Inference / Incremental Decoding (KV Cache)</strong><br>There is no need for the full \(S^2\) attention matrix, but a KV Cache of size</p>\[
\mathcal{O}(B \times H \times S_{\text{past}} \times d_{\text{head}})
\]<p>must be stored. This grows linearly with the decoding steps \(S_{\text{past}}\).<br>Meanwhile, the per-step attention matrix is only \(B \times H \times 1 \times S_{\text{past}}\), significantly smaller than the \(\mathcal{O}(S^2)\) scenario in training.</p></li></ul><h3 id=conclusions-and-comparisons>Conclusions and Comparisons<a hidden class=anchor aria-hidden=true href=#conclusions-and-comparisons>#</a></h3><ol><li><p><strong>Time Complexity</strong></p><ul><li><p>For <strong>self-attention</strong>—whether using <strong>MHA</strong>, <strong>MQA</strong>, or <strong>GQA</strong>—in a <strong>full forward pass</strong> (which also applies to the forward portion during training), the principal matrix multiplications remain:</p>\[
\mathcal{O}\bigl(B \times H \times S^2 \times d_{\text{head}}\bigr)
= \mathcal{O}\bigl(B \times S^2 \times d\bigr)
\]</li><li><p>In <strong>incremental inference</strong> (KV Cache), each new token only requires</p>\[
\mathcal{O}\bigl(B \times H \times S_{\text{past}} \times d_{\text{head}}\bigr)
\]<p>but the KV Cache must be updated and maintained throughout the decoding sequence.</p></li></ul></li><li><p><strong>Space Complexity</strong></p><ul><li><p><strong>Model Parameters</strong>: All three attention mechanisms (MHA, MQA, GQA) reside in \(\mathcal{O}(d^2)\) parameter space.</p></li><li><p><strong>Intermediate Activations</strong> (Training / Full Forward): Dominated by the attention score matrix and weighted outputs:</p>\[
\mathcal{O}\bigl(B \times H \times S^2 + B \times S \times d\bigr)
\]</li><li><p><strong>Incremental Decoding (KV Cache)</strong>: Saves on the \(\mathcal{O}(S^2)\) score matrix cost but requires</p>\[
\mathcal{O}\bigl(B \times H \times S_{\text{past}} \times d_{\text{head}}\bigr)
\]<p>of storage for the KV Cache, which increases linearly with \(S_{\text{past}}\).</p></li></ul></li><li><p><strong>Benefits of MQA / GQA</strong></p><ul><li>Although from a high-level perspective, MHA, MQA, and GQA share similar asymptotic complexities when \(S\) is large, <strong>MQA</strong> and <strong>GQA</strong> can achieve improved efficiency in practice due to <strong>key/value sharing</strong> (or partial sharing) which can reduce memory bandwidth demands and improve cache locality. Consequently, in real-world systems, they often deliver better <strong>speed and memory</strong> performance.</li></ul></li></ol><p>The table below summarizes the main differences among MHA, MQA, and GQA attention mechanisms:</p><table><thead><tr><th style=text-align:center>Feature</th><th style=text-align:center>MHA</th><th style=text-align:center>MQA</th><th style=text-align:center>GQA</th></tr></thead><tbody><tr><td style=text-align:center><strong>Number of K/V Heads</strong></td><td style=text-align:center>Same as number of heads (\(H\))</td><td style=text-align:center>Single K/V head</td><td style=text-align:center>Number of groups (\(G\)), one K/V head per group</td></tr><tr><td style=text-align:center><strong>Inference Time</strong></td><td style=text-align:center>Slower</td><td style=text-align:center>Fastest</td><td style=text-align:center>Faster, but slightly slower than MQA</td></tr><tr><td style=text-align:center><strong>Memory Bandwidth Requirement</strong></td><td style=text-align:center>Highest, \(H\) times K/V loading</td><td style=text-align:center>Lowest, only one K/V head</td><td style=text-align:center>Between MHA and MQA, \(G\) times K/V loading</td></tr><tr><td style=text-align:center><strong>Model Capacity</strong></td><td style=text-align:center>Highest</td><td style=text-align:center>Lowest</td><td style=text-align:center>Moderate, depending on the number of groups \(G\)</td></tr><tr><td style=text-align:center><strong>Performance</strong></td><td style=text-align:center>Best</td><td style=text-align:center>Slightly lower than MHA</td><td style=text-align:center>Close to MHA, significantly better than MQA</td></tr><tr><td style=text-align:center><strong>Uptraining Requirement</strong></td><td style=text-align:center>None</td><td style=text-align:center>High, requires more stability and tuning</td><td style=text-align:center>Lower, GQA models stabilize after minimal uptraining</td></tr><tr><td style=text-align:center><strong>Applicable Scenarios</strong></td><td style=text-align:center>Applications with high performance requirements but insensitive to inference speed</td><td style=text-align:center>Scenarios requiring extremely fast inference with lower model performance demands</td><td style=text-align:center>Applications needing a balance between inference speed and model performance</td></tr></tbody></table><p>In summary, from a <strong>theoretical</strong> standpoint, all three attention mechanisms (MHA, MQA, GQA) share <strong>\(\mathcal{O}(B \times S^2 \times d)\)</strong> complexity in a full pass and <strong>\(\mathcal{O}(B \times S_{\text{past}} \times d)\)</strong> per-step complexity in incremental decoding.</p><h2 id=experimental-results>Experimental Results<a hidden class=anchor aria-hidden=true href=#experimental-results>#</a></h2><h3 id=performance-testing>Performance Testing<a hidden class=anchor aria-hidden=true href=#performance-testing>#</a></h3><p>This experiment was conducted on an environment equipped with dual NVIDIA RTX 4090 GPUs using data parallelism (DP), evenly splitting the batch size across both GPUs. The experiment only tested the performance of the forward pass, including average latency time (Time_mean, unit: ms) and peak memory usage (Peak_Mem_mean, unit: MB), to evaluate the resource requirements and efficiency of different attention mechanisms (MHA, MQA, and GQA) during the inference phase.
You can get the source code in <a href=https://github.com/syhya/syhya.github.io/blob/main/content/en/posts/2025-01-16-group-query-attention/benchmark_attention.py>benchmark_attention.py</a>.</p><ul><li>The tests were based on Llama3 8B hyperparameters.</li></ul><figure class=align-center><img loading=lazy src=llama3_key_hyperparameters.png#center alt="Fig. 5. Overview of the key hyperparameters of Llama 3. (Image source: Grattafiori et al., 2024)" width=100%><figcaption><p>Fig. 5. Overview of the key hyperparameters of Llama 3. (Image source: <a href=https://arxiv.org/abs/2407.21783>Grattafiori et al., 2024</a>)</p></figcaption></figure><p>The main configuration parameters are as follows:</p><ul><li><strong>Total Layers</strong>: 32 layers.</li><li><strong>Hidden Layer Dimension</strong>: 4096.</li><li><strong>Total Number of Multi-Head Attention Heads</strong>: 32.</li><li><strong>Different Group Configurations (nums_kv_head)</strong>: 32 (MHA), 1 (MQA), 8 (GQA-8).</li></ul><h3 id=experimental-results-1>Experimental Results<a hidden class=anchor aria-hidden=true href=#experimental-results-1>#</a></h3><p>This section primarily introduces the experimental performance of MHA, MQA, and GQA-8 under different sequence lengths (512, 1024, and 1536), including data on latency and memory usage. For ease of comparison, the table below presents the specific test results for the three attention mechanisms.</p><table><thead><tr><th>Model Size</th><th>Method</th><th>nums_kv_head</th><th>Seq Length</th><th>Time_mean (ms)</th><th>Peak_Mem_mean (MB)</th></tr></thead><tbody><tr><td>Llama3 8B</td><td>GQA-8</td><td>8</td><td>512</td><td>40.8777</td><td>2322.328</td></tr><tr><td>Llama3 8B</td><td>MHA</td><td>32</td><td>512</td><td>53.0167</td><td>2706.375</td></tr><tr><td>Llama3 8B</td><td>MQA</td><td>1</td><td>512</td><td>37.3592</td><td>2210.314</td></tr><tr><td>Llama3 8B</td><td>GQA-8</td><td>8</td><td>1024</td><td>85.5850</td><td>6738.328</td></tr><tr><td>Llama3 8B</td><td>MQA</td><td>1</td><td>1024</td><td>80.8002</td><td>6570.314</td></tr><tr><td>Llama3 8B</td><td>MHA</td><td>32</td><td>1024</td><td>102.0514</td><td>7314.375</td></tr><tr><td>Llama3 8B</td><td>GQA-8</td><td>8</td><td>1536</td><td>147.5949</td><td>13586.328</td></tr><tr><td>Llama3 8B</td><td>MHA</td><td>32</td><td>1536</td><td>168.8142</td><td>14354.375</td></tr><tr><td>Llama3 8B</td><td>MQA</td><td>1</td><td>1536</td><td>141.5059</td><td>13362.314</td></tr></tbody></table><figure class=align-center><img loading=lazy src=benchmark_time_Llama3_8B.svg#center alt="Fig. 6. Average Time Benchmark." width=90%><figcaption><p>Fig. 6. Average Time Benchmark.</p></figcaption></figure><figure class=align-center><img loading=lazy src=benchmark_mem_Llama3_8B.svg#center alt="Fig. 7. Average Peak Memory Benchmark." width=90%><figcaption><p>Fig. 7. Average Peak Memory Benchmark.</p></figcaption></figure><p>In scenarios sensitive to memory and time overheads, MQA and GQA-8 are more efficient choices, with MQA performing the best but potentially lacking in model performance capabilities; GQA-8 achieves a good balance between efficiency and performance.</p><h3 id=gqa-paper-experimental-results>GQA Paper Experimental Results<a hidden class=anchor aria-hidden=true href=#gqa-paper-experimental-results>#</a></h3><h4 id=inference-performance>Inference Performance<a hidden class=anchor aria-hidden=true href=#inference-performance>#</a></h4><figure class=align-center><img loading=lazy src=inference_benchmark_table.png#center alt="Fig. 8. Inference time and performance comparison. (Image source: Ainslie et al., 2023)" width=100%><figcaption><p>Fig. 8. Inference time and performance comparison. (Image source: <a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>)</p></figcaption></figure><figure class=align-center><img loading=lazy src=inference_benchmark_image.png#center alt="Fig. 9. Additional Experimental Results. (Image source: Ainslie et al., 2023)" width=80%><figcaption><p>Fig. 9. Additional Experimental Results. (Image source: <a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>)</p></figcaption></figure><p>The experimental results show that:</p><ul><li><p><strong>Inference Speed</strong>:</p><ul><li>MHA-XXL&rsquo;s inference time is significantly higher than MHA-Large, primarily due to its larger number of heads and model size.</li><li>Compared to MHA-XXL, MQA-XXL and GQA-8-XXL reduce inference time to approximately 1/6 and 1/5, respectively.</li></ul></li><li><p><strong>Performance</strong>:</p><ul><li>MHA-XXL performs best across all tasks but has longer inference times.</li><li>MQA-XXL has an advantage in inference speed, with average scores only slightly lower than MHA-XXL.</li><li>GQA-8-XXL has inference speed close to MQA-XXL but nearly matches MHA-XXL in performance, demonstrating the efficiency and superiority of GQA.</li></ul></li></ul><h4 id=checkpoint-conversion>Checkpoint Conversion<a hidden class=anchor aria-hidden=true href=#checkpoint-conversion>#</a></h4><figure class=align-center><img loading=lazy src=checkpoint_conversion.png#center alt="Fig. 10. Ablation Study on Checkpoint Conversion Methods. (Image source: Ainslie et al., 2023)" width=80%><figcaption><p>Fig. 10. Ablation Study on Checkpoint Conversion Methods. (Image source: <a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>)</p></figcaption></figure><p>Figure 10 compares the performance of different methods for checkpoint conversion. The mean pooling method performs best in retaining model information, followed by selecting the first head, while random initialization performs the worst. Mean pooling effectively integrates information from multiple \(\mathbf{K}\) and \(\mathbf{V}\) heads, maintaining model performance.</p><h4 id=uptraining-ratio>Uptraining Ratio<a hidden class=anchor aria-hidden=true href=#uptraining-ratio>#</a></h4><figure class=align-center><img loading=lazy src=uptraining_ratios.png#center alt="Fig. 11. Ablation Study on Uptraining Ratios. (Image source: Ainslie et al., 2023)" width=80%><figcaption><p>Fig. 11. Ablation Study on Uptraining Ratios. (Image source: <a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>)</p></figcaption></figure><p>Figure 11 shows how performance varies with uptraining proportion for T5 XXL with MQA and GQA.</p><ul><li><strong>GQA</strong>: Even with only conversion (no uptraining), GQA already has certain performance. As the uptraining ratio increases, performance continues to improve.</li><li><strong>MQA</strong>: Requires at least a 5% uptraining ratio to achieve practical performance, and as the ratio increases, performance gains tend to plateau.</li></ul><h4 id=effect-of-number-of-gqa-groups-on-inference-speed>Effect of Number of GQA Groups on Inference Speed<a hidden class=anchor aria-hidden=true href=#effect-of-number-of-gqa-groups-on-inference-speed>#</a></h4><figure class=align-center><img loading=lazy src=group_number.png#center alt="Fig. 12. Effect of the Number of GQA Groups on Inference Speed. (Image source: Ainslie et al., 2023)" width=80%><figcaption><p>Fig. 12. Effect of the Number of GQA Groups on Inference Speed. (Image source: <a href=https://arxiv.org/abs/2305.13245>Ainslie et al., 2023</a>)</p></figcaption></figure><p>Figure 12 demonstrates that as the number of groups increases, inference time slightly rises, but it still maintains a significant speed advantage over MHA. Choosing an appropriate number of groups, such as 8, can achieve a good balance between speed and performance. Figure 3 also shows that models ranging from 7B to 405B parameters in Llama3 adopt 8 as the number of groups (key/value heads = 8).</p><h2 id=other-optimization-methods>Other Optimization Methods<a hidden class=anchor aria-hidden=true href=#other-optimization-methods>#</a></h2><p>In addition to optimizing the attention mechanism, researchers have proposed various methods to enhance the inference and training efficiency of Transformer models:</p><ul><li><strong>LoRA</strong> (<a href=https://arxiv.org/abs/2106.09685>Hu et al., 2021</a>): Achieves efficient parameter fine-tuning by adding low-rank matrices to the pretrained model&rsquo;s weight matrices.</li><li><strong>Flash Attention</strong> (<a href=https://arxiv.org/abs/2205.14135>Dao et al., 2022</a>): Reduces memory and computational overhead by optimizing attention calculations.</li><li><strong>Quantization Techniques</strong>: LLM.int8 (<a href=https://arxiv.org/pdf/2208.07339>Dettmers et al., 2022</a>) and GPTQ (<a href=https://arxiv.org/abs/2210.17323>Frantar et al., 2022</a>) reduce memory usage and computational costs by lowering the precision of model weights and activations.</li><li><strong>Model Distillation</strong> (<a href=https://arxiv.org/abs/1503.02531>Hinton et al., 2015</a>): Reduces model size by training smaller models to mimic the behavior of larger models.</li><li><strong>Speculative Sampling</strong> (<a href=https://arxiv.org/pdf/2302.01318>Chen et al., 2023</a>): Enhances generation efficiency through parallel generation and filtering.</li></ul><h2 id=key-takeaways>Key Takeaways<a hidden class=anchor aria-hidden=true href=#key-takeaways>#</a></h2><ol><li><strong>Uptraining</strong> methods can effectively utilize existing MHA model checkpoints. By performing a small amount of additional training, they can transform these into more efficient MQA or GQA models, significantly reducing training costs.</li><li><strong>Grouped-Query Attention (GQA)</strong> strikes a good balance between inference efficiency and model performance, making it especially suitable for applications requiring both high-efficiency inference and high performance.</li><li>Experimental results demonstrate that GQA can significantly improve inference speed while maintaining performance comparable to MHA models, making it suitable for large-scale model deployment and real-time applications.</li></ol><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Vaswani A. <a href=https://arxiv.org/abs/1706.03762>Attention is all you need</a> [J]. <em>Advances in Neural Information Processing Systems</em>, 2017.<br>[2] Devlin J. <a href=https://arxiv.org/abs/1810.04805>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> [J]. <em>arXiv preprint arXiv:1810.04805</em>, 2018.<br>[3] Radford A. <a href=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf>Improving Language Understanding by Generative Pre-Training</a> [J]. 2018.<br>[4] Touvron H, Lavril T, Izacard G, et al. <a href=https://arxiv.org/abs/2302.13971>LLaMA: Open and Efficient Foundation Language Models</a> [J]. <em>arXiv preprint arXiv:2302.13971</em>, 2023.<br>[5] Achiam J, Adler S, Agarwal S, et al. <a href=https://arxiv.org/abs/2303.08774>GPT-4 Technical Report</a> [J]. <em>arXiv preprint arXiv:2303.08774</em>, 2023.<br>[6] Shazeer N. <a href=https://arxiv.org/pdf/1911.02150>Fast Transformer Decoding: One Write-Head is All You Need</a> [J]. <em>arXiv preprint arXiv:1911.02150</em>, 2019.<br>[7] Ainslie J, Lee-Thorp J, de Jong M, et al. <a href=https://arxiv.org/pdf/2305.13245>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a> [J]. <em>arXiv preprint arXiv:2305.13245</em>, 2023.<br>[8] Hu E J, Shen Y, Wallis P, et al. <a href=https://arxiv.org/pdf/2106.09685>LoRA: Low-Rank Adaptation of Large Language Models</a> [J]. <em>arXiv preprint arXiv:2106.09685</em>, 2021.<br>[9] Dettmers T, Lewis M, Belkada Y, et al. <a href=https://arxiv.org/pdf/2208.07339>GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a> [J]. <em>Advances in Neural Information Processing Systems</em>, 2022, 35: 30318-30332.<br>[10] Frantar E, Ashkboos S, Hoefler T, et al. <a href=https://arxiv.org/abs/2210.17323>GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers</a> [J]. <em>arXiv preprint arXiv:2210.17323</em>, 2022.<br>[11] Hinton G. <a href=https://arxiv.org/abs/1503.02531>Distilling the Knowledge in a Neural Network</a> [J]. <em>arXiv preprint arXiv:1503.02531</em>, 2015.<br>[12] Chen C, Borgeaud S, Irving G, et al. <a href=https://arxiv.org/pdf/2302.01318>Accelerating Large Language Model Decoding with Speculative Sampling</a> [J]. <em>arXiv preprint arXiv:2302.01318</em>, 2023.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: To reproduce or cite the content of this article, please acknowledge the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Jan 2025). Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA.<br><a href=https://syhya.github.io/posts/2025-01-16-group-query-attention/>https://syhya.github.io/posts/2025-01-16-group-query-attention/</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025gqa</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Jan&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-01-16-group-query-attention/&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/transformer/>Transformer</a></li><li><a href=https://syhya.github.io/tags/attention-mechanism/>Attention Mechanism</a></li><li><a href=https://syhya.github.io/tags/mha/>MHA</a></li><li><a href=https://syhya.github.io/tags/mqa/>MQA</a></li><li><a href=https://syhya.github.io/tags/gqa/>GQA</a></li><li><a href=https://syhya.github.io/tags/kv-cache/>KV Cache</a></li><li><a href=https://syhya.github.io/tags/nlp/>NLP</a></li><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-01-27-deepseek-r1/><span class=title>« Prev</span><br><span>OpenAI o1 Replication Progress: DeepSeek-R1</span>
</a><a class=next href=https://syhya.github.io/posts/2025-01-05-domain-llm-training/><span class=title>Next »</span><br><span>Building Domain-Specific LLMs</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA on x" href="https://x.com/intent/tweet/?text=Attention%20Mechanisms%20in%20Transformers%3a%20Comparing%20MHA%2c%20MQA%2c%20and%20GQA&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-16-group-query-attention%2f&amp;hashtags=DeepLearning%2cAI%2cTransformer%2cAttentionMechanism%2cMHA%2cMQA%2cGQA%2cKVCache%2cNLP%2cLLM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-16-group-query-attention%2f&amp;title=Attention%20Mechanisms%20in%20Transformers%3a%20Comparing%20MHA%2c%20MQA%2c%20and%20GQA&amp;summary=Attention%20Mechanisms%20in%20Transformers%3a%20Comparing%20MHA%2c%20MQA%2c%20and%20GQA&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-16-group-query-attention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-16-group-query-attention%2f&title=Attention%20Mechanisms%20in%20Transformers%3a%20Comparing%20MHA%2c%20MQA%2c%20and%20GQA"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-16-group-query-attention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA on whatsapp" href="https://api.whatsapp.com/send?text=Attention%20Mechanisms%20in%20Transformers%3a%20Comparing%20MHA%2c%20MQA%2c%20and%20GQA%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-16-group-query-attention%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA on telegram" href="https://telegram.me/share/url?text=Attention%20Mechanisms%20in%20Transformers%3a%20Comparing%20MHA%2c%20MQA%2c%20and%20GQA&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-16-group-query-attention%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA on ycombinator" href="https://news.ycombinator.com/submitlink?t=Attention%20Mechanisms%20in%20Transformers%3a%20Comparing%20MHA%2c%20MQA%2c%20and%20GQA&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-01-16-group-query-attention%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>