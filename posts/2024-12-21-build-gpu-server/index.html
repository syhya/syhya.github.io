<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building a Home Deep Learning Rig with Dual RTX 4090 GPUs | Yue Shui Blog</title><meta name=keywords content="Deep Learning,AI,LLM,RTX 4090,AI Hardware,GPU"><meta name=description content="Rent a GPU or Buy Your Own?
Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2024-12-21-build-gpu-server/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3dc782d653c34c6a0c8f1a261092c93b4c57a4549c8e3b36275bd5d52648e773.css integrity="sha256-PceC1lPDTGoMjxomEJLJO0xXpFScjjs2J1vV1SZI53M=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2024-12-21-build-gpu-server/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2024-12-21-build-gpu-server/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Building a Home Deep Learning Rig with Dual RTX 4090 GPUs"><meta property="og:description" content="Rent a GPU or Buy Your Own? Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-21T12:00:00+08:00"><meta property="article:modified_time" content="2025-06-16T18:31:58+08:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLM"><meta property="article:tag" content="RTX 4090"><meta property="article:tag" content="AI Hardware"><meta property="article:tag" content="GPU"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Building a Home Deep Learning Rig with Dual RTX 4090 GPUs"><meta name=twitter:description content="Rent a GPU or Buy Your Own?
Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs","item":"https://syhya.github.io/posts/2024-12-21-build-gpu-server/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs","name":"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs","description":"Rent a GPU or Buy Your Own? Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\n","keywords":["Deep Learning","AI","LLM","RTX 4090","AI Hardware","GPU"],"articleBody":"Rent a GPU or Buy Your Own? Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\nAdvantages of Renting GPUs:\nNo high upfront hardware costs Elastic scalability according to project needs Cloud vendors typically provide data compliance and security assurances, reducing hardware maintenance concerns Advantages of Buying GPUs:\nLower total cost if used extensively over the long term Higher privacy and control for in-house data and models Hardware can be upgraded or adjusted at any time, offering more flexible deployment Personal Suggestions\nIf you have a limited budget or are just starting out, use Colab, Kaggle, or cloud-based GPU solutions first. Once computing needs and privacy requirements increase, consider building your own multi-GPU server or renting a multi-node, multi-GPU cluster. Background In September 2023, to continue my exploration and research on Large Language Models (LLMs) outside of work, I assembled a dual RTX 4090 personal AI server. It has been running for nearly a year, and here are some observations:\nNoise: Placed under my desk, the fans can get quite loud under full load. However, noise levels are acceptable during inference or moderate loads. Inference Performance: With a total of 48GB of VRAM, and by using 4-bit quantization, it can handle 70B-level models (e.g., Llama 70B, Qwen 72B). Training Performance: By using DeepSpeed with distributed and offload techniques (ZeRO-3 + CPU offload), I can finetune around 34B parameter models (e.g., CodeLlama 34B). Cost-Effectiveness: For personal or small-team experiments and small-to-medium scale model training, this configuration is quite practical. However, full-scale training of very large models (hundreds of billions of parameters) still requires more professional GPUs (e.g., multi-GPU A100 or H100 clusters). Below is an illustration of VRAM requirements for various model sizes and training approaches :\nFig. 1. Hardware Requirement. (Image source: LLaMA-Factory)\nAssembly Strategy \u0026 Configuration Details The total budget is around 40,000 RMB (~6,000 USD). The final build list is as follows (for reference only):\nComponent Model Price (RMB) GPU RTX 4090 * 2 25098 Motherboard + CPU AMD R9 7900X + MSI MPG X670E CARBON 5157.55 Memory Corsair 48GB * 2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + Samsung 990PRO 4TB 4587 PSU Corsair AX1600i 2699 Fans Phanteks T30 120mm P * 6 1066.76 CPU Cooler Thermalright FC140 BLACK 419 Chassis Phanteks 620PC Full Tower 897.99 GPU Riser Cable Phanteks FL60 PCI-E4.0 *16 399 Total: ~ 42,723.3 RMB\nGPU Selection For large-scale model research, floating-point performance (TFLOPS) and VRAM capacity are the most critical metrics. Professional GPUs (A100, H100, etc.) boast higher VRAM and NVLink support but can easily cost hundreds of thousands of RMB, which is prohibitive for most individual users. According to Tim Dettmers, the RTX 4090 offers excellent performance-per-dollar and supports features like BF16 and Flash Attention, making it a high-value option.\nCooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling Cooling Method Advantages Disadvantages Best For Blower Fan Compact form factor; good for multi-GPU setups Loud noise, generally weaker cooling Server racks, dense multi-GPU deployments Air-Cooling Good balance of performance and noise; easy upkeep Cards are often large, require space Home or personal research (with enough space) Liquid-Cooling Excellent cooling, quieter under full load Risk of leaks, higher cost Extreme quiet needs or heavy overclocking Home Setup Recommendation: Air-cooled GPUs are typically the best balance of cooling efficiency, noise, and maintenance. They are generally more user-friendly compared to blower or liquid-cooled alternatives.\nCPU \u0026 Motherboard In deep learning scenarios, the CPU handles data preprocessing, pipeline scheduling, and multi-process/thread management, ensuring high-throughput, low-latency data delivery to the GPUs. Thus, the CPU’s main requirements include sufficient PCIe lanes and robust multi-threaded performance.\nIntel: 13th/14th Generation i9 (e.g., 13900K) offers 20 primary PCIe lanes, enough for dual GPUs at x8 + x8. AMD: The Ryzen 7000/9000 series (e.g., 7950X) provides 28 (24 usable) PCIe lanes and also supports dual GPUs at x8 + x8, with enough bandwidth for multiple M.2 SSDs. MSI MPG X670E CARBON Motherboard Expandability: Supports PCIe 5.0 and DDR5 memory, offering ample future-proofing. Stability: High-quality power delivery, ensuring stable operation for both CPU and multiple GPUs. Rich I/O: Supports multiple M.2 SSDs and USB4 for various usage scenarios. AMD Ryzen 9 7900X Highlights Cores \u0026 Threads: 12 cores, 24 threads, excellent for data preprocessing and multitasking in deep learning workloads. PCIe Bandwidth: Provides 28 (24 usable) PCIe 5.0 lanes to support dual GPUs at x8 + x8 while also powering high-speed M.2 SSDs. Power Efficiency: Built on the Zen 4 architecture, delivering outstanding performance-to-power ratio for high-performance computing tasks. Key Motherboard Considerations Physical Layout RTX 4090 cards are huge, often occupying multiple slots. Confirm the board can hold two such cards simultaneously. If space or thermal conflicts arise, use a riser cable for vertical placement of the second GPU. PCIe Lane Splitting Ensure the motherboard can run two PCIe 4.0 x8 + x8 slots. Avoid a setup ending up as x16 + x2, which severely limits the second GPU’s bandwidth and can lead to a significant performance bottleneck, especially critical in large model training. Expandability With two GPUs installed, you still need enough M.2 slots and external ports for additional storage or peripherals. After evaluating expandability, performance, and cost-effectiveness, I chose the AMD Ryzen 9 7900X paired with the MSI MPG X670E CARBON. A GPU riser cable resolved the slot conflicts caused by the thickness of dual RTX 4090s.\nBIOS Setup Recommendations Memory Optimization Enable XMP/EXPO (Intel/AMD) to boost memory clock speeds and bandwidth. Overclocking If additional performance is needed, enable PBO (Precision Boost Overdrive) or Intel Performance Tuning and monitor system stability. Thermals \u0026 Stability Avoid extreme overclocking and keep temperatures under control to maintain system stability. Memory During deep learning training, large amounts of system memory are used for data loading and optimizer states (especially in multi-GPU scenarios with Zero-stage parallelism). Aim for at least 2× the total GPU VRAM capacity. This build uses 48GB * 2 (96GB in total), sufficient for everyday multitasking and distributed training without frequent swapping.\nStorage Prefer M.2 NVMe SSDs: They offer superior read/write performance, critical for loading large model weights, caching intermediate files, and storing training logs. Capacity ≥ 2TB: Model files continue to grow in size. 2TB can fill up quickly; consider 4TB or more depending on your needs. SSD Brands: Samsung, SK Hynix, and Western Digital have reliable high-end product lines. Power Supply Dual RTX 4090s can draw 900W–1000W under full load, and you also need overhead for the CPU, motherboard, and storage. Generally, 1,500W+ Platinum or Titanium PSUs are recommended to ensure stable power delivery and mitigate voltage fluctuations.\nI opted for a Corsair AX1600i (digital PSU), which provides real-time power monitoring and sufficient headroom.\nCooling \u0026 Fans I chose an air-cooling setup:\nCPU Cooler: Thermalright FC140, a dual-tower air cooler offering solid thermal performance with relatively low noise. Case Fans: Phanteks T30 120mm * 6, maintaining slightly positive internal pressure to ensure proper airflow across the GPUs and power delivery modules. For prolonged high-load training (such as distributed training of large models), chassis airflow management and adequate fans are crucial. Monitor CPU, GPU, and VRM temperatures regularly and adjust fan curves as needed.\nAdvanced Cooling\nFor even quieter operation, consider a Hybrid or partial liquid-cooling solution, along with finely tuned fan curves. Regularly cleaning dust filters, using dust guards, and refreshing thermal compound can also help improve thermals and stability. Chassis Because the RTX 4090 is massive, and two of them stacked requires ample internal space and airflow, a full-tower chassis is recommended for better cable routing and thermal design. I went with the Phanteks 620PC, which offers ample space and built-in cable management.\nBelow is a picture of the built computer:\nFig. 2. Computer\nSystem \u0026 Software Environment Operating System: Linux (e.g., Ubuntu 22.04 LTS) is highly recommended due to better compatibility and support for CUDA, NVIDIA drivers, and popular deep learning frameworks. The general setup flow:\nOS Installation: Ubuntu or another Linux distribution. NVIDIA Driver Installation: Make sure nvidia-smi detects both 4090 GPUs correctly:\nFig. 3. nvidia-smi Output\nCUDA Toolkit: Verify via nvcc -V:\nFig. 4. nvcc -V Output\ncuDNN: Ensure your deep learning frameworks can leverage GPU-accelerated kernels for convolution, RNN, etc. Framework Testing: Use PyTorch, TensorFlow, or JAX to confirm basic inference and training functionality. Docker Containerization: With nvidia-container-toolkit, containers can directly access GPU resources, eliminating host-environment conflicts. For multi-node, multi-GPU setups, consider Kubernetes, Ray, or Slurm for cluster scheduling and resource management. Recommended Tools \u0026 Frameworks Training Frameworks\nLLaMA-Factory: Offers user-friendly packaging for large language model training and inference. Great for beginners. DeepSpeed: Provides distributed training for large models, with multiple parallelization strategies and optimizations. Megatron-LM: NVIDIA’s official large-scale language model training framework, ideal for multi-node, multi-GPU scenarios. Monitoring \u0026 Visualization\nWeights \u0026 Biases or TensorBoard: Real-time monitoring of loss, learning rate, etc., with remote-friendly UI. Inference Tools\nollama: Based on llama.cpp, easy local inference setup. vLLM: Optimized for high-concurrency, multi-user scenarios, delivering superior throughput. Framework ollama vLLM Function Simple local LLM deployment High-concurrency / high-throughput LLM Concurrent Performance drops as concurrency increases Handles higher concurrency with better TPS 16 Threads ~17s/req ~9s/req Throughput Slower token generation speeds ~2× faster token generation Max Concur. Performance deteriorates over 32 threads Remains stable under large concurrency Use Cases Personal or low-traffic apps Enterprise or multi-user high concurrency WebUI\nOpen-WebUI: A user-friendly, web-based solution that integrates multiple AI backends (ollama, OpenAI API, etc.), handy for rapid prototyping and visualization. Advanced Suggestions Development \u0026 Debugging Efficiency\nUse SSH for remote development, and create custom Docker images to reduce setup overhead. Quantization \u0026 Pruning\nTechniques like 4-bit or 8-bit quantization and pruning can reduce model size and VRAM usage, thereby improving inference performance. Mixed-Precision Training\nSwitch to BF16 or FP16 training to accelerate training speed, combined with gradient scaling (GradScaler) to maintain numerical stability. CPU Coordination\nEnhance data loading using multi-threading, multi-processing, or RAM disks for streaming large pretraining datasets. Multi-Node Cluster Deployment\nConnect nodes via InfiniBand or high-speed Ethernet and use Kubernetes for efficient resource scheduling. Summary With the above configuration and methodology, I successfully built a dual RTX 4090 deep learning workstation. It excels at inference and small to medium scale fine-tuning scenarios—ideal for individuals or small teams focusing on LLM research or application development. This setup strikes a balance between cost-effectiveness and flexibility. However, if you plan to train massive models (hundreds of billions of parameters) in a full-parameter regime, you will still need more GPUs (e.g., multi-GPU A100 or H100 clusters).\nFrom personal experience, dual 4090 GPUs provide sufficient performance within a reasonable budget, meeting the majority of small-to-medium-scale R\u0026D needs—a solid option for qualified individuals or teams to consider.\nReferences Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe lane specs AMD R5 7600X PCIe lane specs MSI MPG X670E CARBON Specifications nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI Copyright \u0026 Citation Disclaimer: The configurations, prices, and recommendations in this article are for technical discussion and research purposes only. Actual purchases and deployments should be carefully evaluated based on individual budgets and requirements. The author assumes no liability for any direct or indirect consequences resulting from following or adapting the information provided herein.\nCitation: When reposting or referencing this content, please credit the original author and source.\nCited as:\nYue Shui. (Dec 2024). Building a Home Deep Learning Rig with Dual RTX 4090 GPUs. https://syhya.github.io/posts/2024-12-21-build-gpu-server\nOr\n@article{syhya2024build, title = \"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2024\", month = \"Dec\", url = \"https://syhya.github.io/posts/2024-12-21-build-gpu-server/\" ","wordCount":"1988","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-12-21T12:00:00+08:00","dateModified":"2025-06-16T18:31:58+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2024-12-21-build-gpu-server/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Building a Home Deep Learning Rig with Dual RTX 4090 GPUs</h1><div class=post-meta><span title='2024-12-21 12:00:00 +0800 +0800'>2024-12-21</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;1988 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2024-12-21-build-gpu-server/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#rent-a-gpu-or-buy-your-own>Rent a GPU or Buy Your Own?</a></li><li><a href=#background>Background</a></li><li><a href=#assembly-strategy--configuration-details>Assembly Strategy & Configuration Details</a><ul><li><a href=#gpu-selection>GPU Selection</a><ul><li><a href=#cooling-options-blower-vs-air-cooling-vs-liquid-cooling>Cooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling</a></li></ul></li><li><a href=#cpu--motherboard>CPU & Motherboard</a><ul><li><a href=#msi-mpg-x670e-carbon-motherboard>MSI MPG X670E CARBON Motherboard</a></li><li><a href=#amd-ryzen-9-7900x-highlights>AMD Ryzen 9 7900X Highlights</a></li><li><a href=#key-motherboard-considerations>Key Motherboard Considerations</a></li><li><a href=#bios-setup-recommendations>BIOS Setup Recommendations</a></li></ul></li><li><a href=#memory>Memory</a></li><li><a href=#storage>Storage</a></li><li><a href=#power-supply>Power Supply</a></li><li><a href=#cooling--fans>Cooling & Fans</a></li><li><a href=#chassis>Chassis</a></li></ul></li><li><a href=#system--software-environment>System & Software Environment</a></li><li><a href=#recommended-tools--frameworks>Recommended Tools & Frameworks</a></li><li><a href=#advanced-suggestions>Advanced Suggestions</a></li><li><a href=#summary>Summary</a></li><li><a href=#references>References</a></li><li><a href=#copyright--citation>Copyright & Citation</a></li></ul></nav></div></details></div><div class=post-content><h2 id=rent-a-gpu-or-buy-your-own>Rent a GPU or Buy Your Own?<a hidden class=anchor aria-hidden=true href=#rent-a-gpu-or-buy-your-own>#</a></h2><p>Before setting up a deep learning environment, consider <strong>usage duration</strong>, <strong>budget</strong>, <strong>data privacy</strong>, and <strong>maintenance overhead</strong>. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.</p><ul><li><p><strong>Advantages of Renting GPUs</strong>:</p><ul><li>No high upfront hardware costs</li><li>Elastic scalability according to project needs</li><li>Cloud vendors typically provide data compliance and security assurances, reducing hardware maintenance concerns</li></ul></li><li><p><strong>Advantages of Buying GPUs</strong>:</p><ul><li>Lower total cost if used extensively over the long term</li><li>Higher privacy and control for in-house data and models</li><li>Hardware can be upgraded or adjusted at any time, offering more flexible deployment</li></ul></li></ul><blockquote><p><strong>Personal Suggestions</strong></p><ol><li>If you have a limited budget or are just starting out, use Colab, Kaggle, or cloud-based GPU solutions first.</li><li>Once computing needs and privacy requirements increase, consider building your own multi-GPU server or renting a multi-node, multi-GPU cluster.</li></ol></blockquote><hr><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>In September 2023, to continue my exploration and research on Large Language Models (LLMs) outside of work, I assembled a <strong>dual RTX 4090</strong> personal AI server. It has been running for nearly a year, and here are some observations:</p><ul><li><strong>Noise</strong>: Placed under my desk, the fans can get quite loud under full load. However, noise levels are acceptable during inference or moderate loads.</li><li><strong>Inference Performance</strong>: With a total of 48GB of VRAM, and by using 4-bit quantization, it can handle 70B-level models (e.g., Llama 70B, Qwen 72B).</li><li><strong>Training Performance</strong>: By using <a href=https://github.com/microsoft/DeepSpeed>DeepSpeed</a> with distributed and offload techniques (ZeRO-3 + CPU offload), I can finetune around 34B parameter models (e.g., CodeLlama 34B).</li><li><strong>Cost-Effectiveness</strong>: For personal or small-team experiments and small-to-medium scale model training, this configuration is quite practical. However, full-scale training of very large models (hundreds of billions of parameters) still requires more professional GPUs (e.g., multi-GPU A100 or H100 clusters).</li></ul><p>Below is an illustration of VRAM requirements for various model sizes and training approaches :</p><figure class=align-center><img loading=lazy src=hardware_requirement.png#center alt="Fig. 1. Hardware Requirement. (Image source: LLaMA-Factory)"><figcaption><p>Fig. 1. Hardware Requirement. (Image source: <a href=https://github.com/hiyouga/LLaMA-Factory#hardware-requirement>LLaMA-Factory</a>)</p></figcaption></figure><hr><h2 id=assembly-strategy--configuration-details>Assembly Strategy & Configuration Details<a hidden class=anchor aria-hidden=true href=#assembly-strategy--configuration-details>#</a></h2><p>The total budget is around <strong>40,000 RMB (~6,000 USD)</strong>. The final build list is as follows (for reference only):</p><table><thead><tr><th style=text-align:center>Component</th><th style=text-align:left>Model</th><th style=text-align:center>Price (RMB)</th></tr></thead><tbody><tr><td style=text-align:center><strong>GPU</strong></td><td style=text-align:left>RTX 4090 * 2</td><td style=text-align:center>25098</td></tr><tr><td style=text-align:center><strong>Motherboard + CPU</strong></td><td style=text-align:left>AMD R9 7900X + MSI MPG X670E CARBON</td><td style=text-align:center>5157.55</td></tr><tr><td style=text-align:center><strong>Memory</strong></td><td style=text-align:left>Corsair 48GB * 2 (DDR5 5600)</td><td style=text-align:center>2399</td></tr><tr><td style=text-align:center><strong>SSD</strong></td><td style=text-align:left>SOLIDIGM 944 PRO 2TB *2 + Samsung 990PRO 4TB</td><td style=text-align:center>4587</td></tr><tr><td style=text-align:center><strong>PSU</strong></td><td style=text-align:left>Corsair AX1600i</td><td style=text-align:center>2699</td></tr><tr><td style=text-align:center><strong>Fans</strong></td><td style=text-align:left>Phanteks T30 120mm P * 6</td><td style=text-align:center>1066.76</td></tr><tr><td style=text-align:center><strong>CPU Cooler</strong></td><td style=text-align:left>Thermalright FC140 BLACK</td><td style=text-align:center>419</td></tr><tr><td style=text-align:center><strong>Chassis</strong></td><td style=text-align:left>Phanteks 620PC Full Tower</td><td style=text-align:center>897.99</td></tr><tr><td style=text-align:center><strong>GPU Riser Cable</strong></td><td style=text-align:left>Phanteks FL60 PCI-E4.0 *16</td><td style=text-align:center>399</td></tr></tbody></table><p><strong>Total</strong>: ~ 42,723.3 RMB</p><h3 id=gpu-selection>GPU Selection<a hidden class=anchor aria-hidden=true href=#gpu-selection>#</a></h3><p>For large-scale model research, <strong>floating-point performance (TFLOPS)</strong> and <strong>VRAM capacity</strong> are the most critical metrics. Professional GPUs (A100, H100, etc.) boast higher VRAM and NVLink support but can easily cost hundreds of thousands of RMB, which is prohibitive for most individual users. According to <a href=https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/>Tim Dettmers</a>, the RTX 4090 offers excellent performance-per-dollar and supports features like BF16 and Flash Attention, making it a high-value option.</p><h4 id=cooling-options-blower-vs-air-cooling-vs-liquid-cooling>Cooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling<a hidden class=anchor aria-hidden=true href=#cooling-options-blower-vs-air-cooling-vs-liquid-cooling>#</a></h4><table><thead><tr><th style=text-align:center><strong>Cooling Method</strong></th><th style=text-align:left><strong>Advantages</strong></th><th style=text-align:center><strong>Disadvantages</strong></th><th style=text-align:center><strong>Best For</strong></th></tr></thead><tbody><tr><td style=text-align:center><strong>Blower Fan</strong></td><td style=text-align:left>Compact form factor; good for multi-GPU setups</td><td style=text-align:center>Loud noise, generally weaker cooling</td><td style=text-align:center>Server racks, dense multi-GPU deployments</td></tr><tr><td style=text-align:center><strong>Air-Cooling</strong></td><td style=text-align:left>Good balance of performance and noise; easy upkeep</td><td style=text-align:center>Cards are often large, require space</td><td style=text-align:center>Home or personal research (with enough space)</td></tr><tr><td style=text-align:center><strong>Liquid-Cooling</strong></td><td style=text-align:left>Excellent cooling, quieter under full load</td><td style=text-align:center>Risk of leaks, higher cost</td><td style=text-align:center>Extreme quiet needs or heavy overclocking</td></tr></tbody></table><blockquote><p><strong>Home Setup Recommendation</strong>: <strong>Air-cooled GPUs</strong> are typically the best balance of cooling efficiency, noise, and maintenance. They are generally more user-friendly compared to blower or liquid-cooled alternatives.</p></blockquote><h3 id=cpu--motherboard>CPU & Motherboard<a hidden class=anchor aria-hidden=true href=#cpu--motherboard>#</a></h3><p>In deep learning scenarios, the CPU handles data preprocessing, pipeline scheduling, and multi-process/thread management, ensuring high-throughput, low-latency data delivery to the GPUs. Thus, the CPU’s main requirements include <strong>sufficient PCIe lanes</strong> and <strong>robust multi-threaded performance</strong>.</p><ul><li><strong>Intel</strong>: 13th/14th Generation i9 (e.g., 13900K) offers 20 primary PCIe lanes, enough for dual GPUs at x8 + x8.</li><li><strong>AMD</strong>: The Ryzen 7000/9000 series (e.g., 7950X) provides 28 (24 usable) PCIe lanes and also supports dual GPUs at x8 + x8, with enough bandwidth for multiple M.2 SSDs.</li></ul><h4 id=msi-mpg-x670e-carbon-motherboard>MSI MPG X670E CARBON Motherboard<a hidden class=anchor aria-hidden=true href=#msi-mpg-x670e-carbon-motherboard>#</a></h4><ul><li><strong>Expandability</strong>: Supports PCIe 5.0 and DDR5 memory, offering ample future-proofing.</li><li><strong>Stability</strong>: High-quality power delivery, ensuring stable operation for both CPU and multiple GPUs.</li><li><strong>Rich I/O</strong>: Supports multiple M.2 SSDs and USB4 for various usage scenarios.</li></ul><h4 id=amd-ryzen-9-7900x-highlights>AMD Ryzen 9 7900X Highlights<a hidden class=anchor aria-hidden=true href=#amd-ryzen-9-7900x-highlights>#</a></h4><ul><li><strong>Cores & Threads</strong>: 12 cores, 24 threads, excellent for data preprocessing and multitasking in deep learning workloads.</li><li><strong>PCIe Bandwidth</strong>: Provides 28 (24 usable) PCIe 5.0 lanes to support dual GPUs at x8 + x8 while also powering high-speed M.2 SSDs.</li><li><strong>Power Efficiency</strong>: Built on the Zen 4 architecture, delivering outstanding performance-to-power ratio for high-performance computing tasks.</li></ul><h4 id=key-motherboard-considerations>Key Motherboard Considerations<a hidden class=anchor aria-hidden=true href=#key-motherboard-considerations>#</a></h4><ol><li><strong>Physical Layout</strong><ul><li>RTX 4090 cards are huge, often occupying multiple slots. Confirm the board can hold two such cards simultaneously. If space or thermal conflicts arise, use a riser cable for vertical placement of the second GPU.</li></ul></li><li><strong>PCIe Lane Splitting</strong><ul><li>Ensure the motherboard can run two PCIe 4.0 x8 + x8 slots. Avoid a setup ending up as x16 + x2, which severely limits the second GPU’s bandwidth and can lead to a significant performance bottleneck, especially critical in large model training.</li></ul></li><li><strong>Expandability</strong><ul><li>With two GPUs installed, you still need enough M.2 slots and external ports for additional storage or peripherals.</li></ul></li></ol><p>After evaluating expandability, performance, and cost-effectiveness, I chose the <strong>AMD Ryzen 9 7900X paired with the MSI MPG X670E CARBON</strong>. A GPU riser cable resolved the slot conflicts caused by the thickness of dual RTX 4090s.</p><h4 id=bios-setup-recommendations>BIOS Setup Recommendations<a hidden class=anchor aria-hidden=true href=#bios-setup-recommendations>#</a></h4><ol><li><strong>Memory Optimization</strong><ul><li>Enable <strong>XMP/EXPO</strong> (Intel/AMD) to boost memory clock speeds and bandwidth.</li></ul></li><li><strong>Overclocking</strong><ul><li>If additional performance is needed, enable <strong>PBO (Precision Boost Overdrive)</strong> or Intel Performance Tuning and monitor system stability.</li></ul></li><li><strong>Thermals & Stability</strong><ul><li>Avoid extreme overclocking and keep temperatures under control to maintain system stability.</li></ul></li></ol><h3 id=memory>Memory<a hidden class=anchor aria-hidden=true href=#memory>#</a></h3><p>During deep learning training, large amounts of system memory are used for data loading and optimizer states (especially in multi-GPU scenarios with Zero-stage parallelism). <strong>Aim for at least 2× the total GPU VRAM capacity</strong>. This build uses 48GB * 2 (96GB in total), sufficient for everyday multitasking and distributed training without frequent swapping.</p><h3 id=storage>Storage<a hidden class=anchor aria-hidden=true href=#storage>#</a></h3><ul><li><strong>Prefer M.2 NVMe SSDs</strong>: They offer superior read/write performance, critical for loading large model weights, caching intermediate files, and storing training logs.</li><li><strong>Capacity ≥ 2TB</strong>: Model files continue to grow in size. 2TB can fill up quickly; consider 4TB or more depending on your needs.</li><li><strong>SSD Brands</strong>: Samsung, SK Hynix, and Western Digital have reliable high-end product lines.</li></ul><h3 id=power-supply>Power Supply<a hidden class=anchor aria-hidden=true href=#power-supply>#</a></h3><p>Dual RTX 4090s can draw <strong>900W–1000W</strong> under full load, and you also need overhead for the CPU, motherboard, and storage. Generally, <strong>1,500W+</strong> Platinum or Titanium PSUs are recommended to ensure stable power delivery and mitigate voltage fluctuations.<br>I opted for a Corsair AX1600i (digital PSU), which provides real-time power monitoring and sufficient headroom.</p><h3 id=cooling--fans>Cooling & Fans<a hidden class=anchor aria-hidden=true href=#cooling--fans>#</a></h3><p>I chose an <strong>air-cooling</strong> setup:</p><ul><li><strong>CPU Cooler</strong>: Thermalright FC140, a dual-tower air cooler offering solid thermal performance with relatively low noise.</li><li><strong>Case Fans</strong>: Phanteks T30 120mm * 6, maintaining slightly positive internal pressure to ensure proper airflow across the GPUs and power delivery modules.</li></ul><p>For prolonged high-load training (such as distributed training of large models), chassis airflow management and adequate fans are crucial. Monitor CPU, GPU, and VRM temperatures regularly and adjust fan curves as needed.</p><blockquote><p><strong>Advanced Cooling</strong></p><ul><li>For even quieter operation, consider a <em>Hybrid</em> or partial liquid-cooling solution, along with finely tuned fan curves.</li><li>Regularly cleaning dust filters, using dust guards, and refreshing thermal compound can also help improve thermals and stability.</li></ul></blockquote><h3 id=chassis>Chassis<a hidden class=anchor aria-hidden=true href=#chassis>#</a></h3><p>Because the RTX 4090 is massive, and two of them stacked requires ample internal space and airflow, a full-tower chassis is recommended for better cable routing and thermal design. I went with the Phanteks 620PC, which offers ample space and built-in cable management.</p><p>Below is a picture of the built computer:<br><figure class=align-center><img loading=lazy src=computer.jpeg#center alt="Fig. 2. Computer"><figcaption><p>Fig. 2. Computer</p></figcaption></figure></p><hr><h2 id=system--software-environment>System & Software Environment<a hidden class=anchor aria-hidden=true href=#system--software-environment>#</a></h2><p><strong>Operating System</strong>: Linux (e.g., <strong>Ubuntu 22.04 LTS</strong>) is highly recommended due to better compatibility and support for CUDA, NVIDIA drivers, and popular deep learning frameworks. The general setup flow:</p><ol><li><strong>OS Installation</strong>: Ubuntu or another Linux distribution.</li><li><strong>NVIDIA Driver Installation</strong>: Make sure <code>nvidia-smi</code> detects both 4090 GPUs correctly:<br><figure class=align-center><img loading=lazy src=nvidia_smi.png#center alt="Fig. 3. nvidia-smi Output"><figcaption><p>Fig. 3. nvidia-smi Output</p></figcaption></figure></li><li><strong>CUDA Toolkit</strong>: Verify via <code>nvcc -V</code>:<br><figure class=align-center><img loading=lazy src=nvcc.png#center alt="Fig. 4. nvcc -V Output"><figcaption><p>Fig. 4. nvcc -V Output</p></figcaption></figure></li><li><strong>cuDNN</strong>: Ensure your deep learning frameworks can leverage GPU-accelerated kernels for convolution, RNN, etc.</li><li><strong>Framework Testing</strong>: Use <a href=https://pytorch.org/>PyTorch</a>, <a href=https://www.tensorflow.org/>TensorFlow</a>, or <a href=https://github.com/google/jax>JAX</a> to confirm basic inference and training functionality.</li><li><strong>Docker Containerization</strong>:<ul><li>With <a href=https://github.com/NVIDIA/nvidia-container-toolkit>nvidia-container-toolkit</a>, containers can directly access GPU resources, eliminating host-environment conflicts.</li><li>For multi-node, multi-GPU setups, consider <strong>Kubernetes</strong>, <strong>Ray</strong>, or <strong>Slurm</strong> for cluster scheduling and resource management.</li></ul></li></ol><hr><h2 id=recommended-tools--frameworks>Recommended Tools & Frameworks<a hidden class=anchor aria-hidden=true href=#recommended-tools--frameworks>#</a></h2><ol><li><p><strong>Training Frameworks</strong></p><ul><li><a href=https://github.com/hiyouga/LLaMA-Factory><strong>LLaMA-Factory</strong></a>: Offers user-friendly packaging for large language model training and inference. Great for beginners.</li><li><a href=https://github.com/microsoft/DeepSpeed><strong>DeepSpeed</strong></a>: Provides distributed training for large models, with multiple parallelization strategies and optimizations.</li><li><a href=https://github.com/NVIDIA/Megatron-LM><strong>Megatron-LM</strong></a>: NVIDIA’s official large-scale language model training framework, ideal for multi-node, multi-GPU scenarios.</li></ul></li><li><p><strong>Monitoring & Visualization</strong></p><ul><li><a href=https://wandb.ai/><strong>Weights & Biases</strong></a> or <a href=https://www.tensorflow.org/tensorboard><strong>TensorBoard</strong></a>: Real-time monitoring of loss, learning rate, etc., with remote-friendly UI.</li></ul></li><li><p><strong>Inference Tools</strong></p><ul><li><a href=https://github.com/jmorganca/ollama><strong>ollama</strong></a>: Based on <a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a>, easy local inference setup.</li><li><a href=https://github.com/vllm-project/vllm><strong>vLLM</strong></a>: Optimized for high-concurrency, multi-user scenarios, delivering superior throughput.</li></ul><table><thead><tr><th style=text-align:center><strong>Framework</strong></th><th style=text-align:center><strong>ollama</strong></th><th style=text-align:center><strong>vLLM</strong></th></tr></thead><tbody><tr><td style=text-align:center><strong>Function</strong></td><td style=text-align:center>Simple local LLM deployment</td><td style=text-align:center>High-concurrency / high-throughput LLM</td></tr><tr><td style=text-align:center><strong>Concurrent</strong></td><td style=text-align:center>Performance drops as concurrency increases</td><td style=text-align:center>Handles higher concurrency with better TPS</td></tr><tr><td style=text-align:center><strong>16 Threads</strong></td><td style=text-align:center>~17s/req</td><td style=text-align:center>~9s/req</td></tr><tr><td style=text-align:center><strong>Throughput</strong></td><td style=text-align:center>Slower token generation speeds</td><td style=text-align:center>~2× faster token generation</td></tr><tr><td style=text-align:center><strong>Max Concur.</strong></td><td style=text-align:center>Performance deteriorates over 32 threads</td><td style=text-align:center>Remains stable under large concurrency</td></tr><tr><td style=text-align:center><strong>Use Cases</strong></td><td style=text-align:center>Personal or low-traffic apps</td><td style=text-align:center>Enterprise or multi-user high concurrency</td></tr></tbody></table></li><li><p><strong>WebUI</strong></p><ul><li><a href=https://github.com/open-webui/open-webui><strong>Open-WebUI</strong></a>: A user-friendly, web-based solution that integrates multiple AI backends (ollama, OpenAI API, etc.), handy for rapid prototyping and visualization.</li></ul></li></ol><hr><h2 id=advanced-suggestions>Advanced Suggestions<a hidden class=anchor aria-hidden=true href=#advanced-suggestions>#</a></h2><ol><li><p><strong>Development & Debugging Efficiency</strong></p><ul><li>Use SSH for remote development, and create custom Docker images to reduce setup overhead.</li></ul></li><li><p><strong>Quantization & Pruning</strong></p><ul><li>Techniques like 4-bit or 8-bit quantization and pruning can reduce model size and VRAM usage, thereby improving inference performance.</li></ul></li><li><p><strong>Mixed-Precision Training</strong></p><ul><li>Switch to BF16 or FP16 training to accelerate training speed, combined with gradient scaling (GradScaler) to maintain numerical stability.</li></ul></li><li><p><strong>CPU Coordination</strong></p><ul><li>Enhance data loading using multi-threading, multi-processing, or RAM disks for streaming large pretraining datasets.</li></ul></li><li><p><strong>Multi-Node Cluster Deployment</strong></p><ul><li>Connect nodes via InfiniBand or high-speed Ethernet and use Kubernetes for efficient resource scheduling.</li></ul></li></ol><hr><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>With the above configuration and methodology, I successfully built a <strong>dual RTX 4090</strong> deep learning workstation. It excels at <strong>inference</strong> and <strong>small to medium scale fine-tuning</strong> scenarios—ideal for individuals or small teams focusing on LLM research or application development. This setup strikes a balance between <strong>cost-effectiveness</strong> and <strong>flexibility</strong>. However, if you plan to train massive models (hundreds of billions of parameters) in a full-parameter regime, you will still need more GPUs (e.g., multi-GPU A100 or H100 clusters).</p><p>From personal experience, dual 4090 GPUs provide sufficient performance within a reasonable budget, meeting the majority of small-to-medium-scale R&amp;D needs—a solid option for qualified individuals or teams to consider.</p><hr><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/>Tim Dettmers: Which GPU for Deep Learning? (2023)</a></li><li><a href=https://www.intel.com/content/www/us/en/products/sku/236773/intel-core-i9-processor-14900k-36m-cache-up-to-6-00-ghz/specifications.html>Intel 14900K PCIe lane specs</a></li><li><a href=https://www.amd.com/en/products/processors/desktops/ryzen/7000-series/amd-ryzen-5-7600.html>AMD R5 7600X PCIe lane specs</a></li><li><a href=https://www.msi.com/Motherboard/MPG-X670E-CARBON-WIFI/Specification>MSI MPG X670E CARBON Specifications</a></li><li><a href=https://github.com/NVIDIA/nvidia-container-toolkit>nvidia-container-toolkit</a></li><li><a href=https://github.com/hiyouga/LLaMA-Factory>LLaMA-Factory</a></li><li><a href=https://github.com/microsoft/DeepSpeed>DeepSpeed</a></li><li><a href=https://github.com/NVIDIA/Megatron-LM>Megatron-LM</a></li><li><a href=https://github.com/jmorganca/ollama>ollama</a></li><li><a href=https://github.com/vllm-project/vllm>vLLM</a></li><li><a href=https://medium.com/@naman1011/ollama-vs-vllm-which-tool-handles-ai-models-better-a93345b911e6>Ollama vs VLLM: Which Tool Handles AI Models Better?</a></li><li><a href=https://github.com/open-webui/open-webui>Open-WebUI</a></li></ol><hr><h2 id=copyright--citation>Copyright & Citation<a hidden class=anchor aria-hidden=true href=#copyright--citation>#</a></h2><blockquote><p><strong>Disclaimer</strong>: The configurations, prices, and recommendations in this article are for technical discussion and research purposes only. Actual purchases and deployments should be carefully evaluated based on individual budgets and requirements. The author assumes no liability for any direct or indirect consequences resulting from following or adapting the information provided herein.</p></blockquote><blockquote><p><strong>Citation</strong>: When reposting or referencing this content, please credit the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (Dec 2024). Building a Home Deep Learning Rig with Dual RTX 4090 GPUs.
<a href=https://syhya.github.io/posts/2024-12-21-build-gpu-server>https://syhya.github.io/posts/2024-12-21-build-gpu-server</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2024build</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;Building a Home Deep Learning Rig with Dual RTX 4090 GPUs&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2024&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Dec&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2024-12-21-build-gpu-server/&#34;</span>
</span></span><span class=line><span class=cl>  
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/tags/rtx-4090/>RTX 4090</a></li><li><a href=https://syhya.github.io/tags/ai-hardware/>AI Hardware</a></li><li><a href=https://syhya.github.io/tags/gpu/>GPU</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-01-05-domain-llm-training/><span class=title>« Prev</span><br><span>Building Domain-Specific LLMs</span>
</a><a class=next href=https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/><span class=title>Next »</span><br><span>Stock Price Prediction and Quantitative Strategy Based on Deep Learning</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Home Deep Learning Rig with Dual RTX 4090 GPUs on x" href="https://x.com/intent/tweet/?text=Building%20a%20Home%20Deep%20Learning%20Rig%20with%20Dual%20RTX%204090%20GPUs&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2024-12-21-build-gpu-server%2f&amp;hashtags=DeepLearning%2cAI%2cLLM%2cRTX4090%2cAIHardware%2cGPU"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Home Deep Learning Rig with Dual RTX 4090 GPUs on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2024-12-21-build-gpu-server%2f&amp;title=Building%20a%20Home%20Deep%20Learning%20Rig%20with%20Dual%20RTX%204090%20GPUs&amp;summary=Building%20a%20Home%20Deep%20Learning%20Rig%20with%20Dual%20RTX%204090%20GPUs&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2024-12-21-build-gpu-server%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Home Deep Learning Rig with Dual RTX 4090 GPUs on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2024-12-21-build-gpu-server%2f&title=Building%20a%20Home%20Deep%20Learning%20Rig%20with%20Dual%20RTX%204090%20GPUs"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Home Deep Learning Rig with Dual RTX 4090 GPUs on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2024-12-21-build-gpu-server%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Home Deep Learning Rig with Dual RTX 4090 GPUs on whatsapp" href="https://api.whatsapp.com/send?text=Building%20a%20Home%20Deep%20Learning%20Rig%20with%20Dual%20RTX%204090%20GPUs%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2024-12-21-build-gpu-server%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Home Deep Learning Rig with Dual RTX 4090 GPUs on telegram" href="https://telegram.me/share/url?text=Building%20a%20Home%20Deep%20Learning%20Rig%20with%20Dual%20RTX%204090%20GPUs&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2024-12-21-build-gpu-server%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Building a Home Deep Learning Rig with Dual RTX 4090 GPUs on ycombinator" href="https://news.ycombinator.com/submitlink?t=Building%20a%20Home%20Deep%20Learning%20Rig%20with%20Dual%20RTX%204090%20GPUs&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2024-12-21-build-gpu-server%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>