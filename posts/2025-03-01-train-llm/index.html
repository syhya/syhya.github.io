<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Parallelism and Memory Optimization Techniques for Training Large Models | Yue Shui Blog</title>
<meta name=keywords content="LLMs,Pre-training,Distributed Training,Memory Optimization,Data Parallelism,Model Parallelism,Pipeline Parallelism,Tensor Parallelism,Sequence Parallelism,Hybrid Parallelism,Heterogeneous Systems,MoE,ZeRO,LoRA,AI,Deep Learning,AI Infrastructure"><meta name=description content="Background
Recently, the number of parameters in large models has been continuously increasing, from the initial billions to today&rsquo;s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-03-01-train-llm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-03-01-train-llm/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-03-01-train-llm/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-03-01-train-llm/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Parallelism and Memory Optimization Techniques for Training Large Models"><meta property="og:description" content="Background Recently, the number of parameters in large models has been continuously increasing, from the initial billions to today’s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-01T12:00:00+08:00"><meta property="article:modified_time" content="2025-03-01T12:00:00+08:00"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="Pre-Training"><meta property="article:tag" content="Distributed Training"><meta property="article:tag" content="Memory Optimization"><meta property="article:tag" content="Data Parallelism"><meta property="article:tag" content="Model Parallelism"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Parallelism and Memory Optimization Techniques for Training Large Models"><meta name=twitter:description content="Background
Recently, the number of parameters in large models has been continuously increasing, from the initial billions to today&rsquo;s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Parallelism and Memory Optimization Techniques for Training Large Models","item":"https://syhya.github.io/posts/2025-03-01-train-llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Parallelism and Memory Optimization Techniques for Training Large Models","name":"Parallelism and Memory Optimization Techniques for Training Large Models","description":"Background Recently, the number of parameters in large models has been continuously increasing, from the initial billions to today\u0026rsquo;s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models.\n","keywords":["LLMs","Pre-training","Distributed Training","Memory Optimization","Data Parallelism","Model Parallelism","Pipeline Parallelism","Tensor Parallelism","Sequence Parallelism","Hybrid Parallelism","Heterogeneous Systems","MoE","ZeRO","LoRA","AI","Deep Learning","AI Infrastructure"],"articleBody":"Background Recently, the number of parameters in large models has been continuously increasing, from the initial billions to today’s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models.\nTraining Challenges of Large Models Explosive Growth in Parameter Scale With the continuous pursuit of model capacity and performance, the number of parameters in neural networks is growing exponentially. Today, models ranging from millions to billions, hundreds of billions, and even trillions of parameters are emerging. For example, Llama 3.1 405B has approximately 405 billion parameters, while it is rumored that GPT-4 may have as many as 1.7 trillion parameters. This massive parameter scale has led to a sharp increase in computing and memory demands, bringing unprecedented pressure to the training process.\nSoaring Computational Complexity The rapid increase in the number of parameters directly leads to a significant increase in overall computational complexity. Training a large model once may take weeks or even months. Even with large-scale high-performance GPU clusters, the training cycle is still unsatisfactory, severely restricting model iteration speed and research efficiency.\nIncreasingly Prominent Memory Bottleneck In addition to storing massive model parameters, large models must also save intermediate activations, gradient information, and optimizer states during training. This data poses a huge challenge to GPU memory. Even with high-end GPUs equipped with A100, H100 (80GB memory), H200 (141GB memory), or GB200 (384GB memory), single-card memory is often insufficient to meet the needs of models with hundreds of billions or even trillions of parameters, leading to frequent “Out of Memory (OOM)” errors.\nCommunication Overhead Becomes a Bottleneck In multi-GPU distributed training environments, inter-node communication is frequently required for data synchronization (such as gradient aggregation). As the model size and the number of GPUs increase, this communication volume rises sharply. Even in high-bandwidth networks, All-Reduce operations to transmit massive amounts of data consume a significant amount of time, becoming one of the main bottlenecks of overall parallel efficiency.\nTraining Stability Challenges Ultra-large-scale models are more prone to gradient vanishing or gradient explosion problems during training, leading to unstable training processes and difficulty in convergence. Although mixed-precision training can accelerate training and reduce memory footprint to some extent, it may also introduce new numerical stability issues, requiring researchers to invest more effort in detailed tuning.\nNecessity of Distributed Training Faced with the above challenges, distributed training technology has become a key solution to support the training of large models. By splitting training tasks and distributing them to multiple GPUs or computing nodes, distributed training can fully utilize parallel computing and cluster memory resources, thereby breaking through the limitations of a single GPU. The main advantages are reflected in the following aspects:\nBreaking Through the Computing Power Limit of a Single GPU The computing power of a single GPU is ultimately limited and cannot cope with the massive computing demands of trillion-parameter models. With data parallelism and model parallelism techniques, training tasks can be evenly distributed to multiple GPUs, thereby significantly shortening the overall training time.\nOvercoming the Memory Bottleneck of a Single GPU By distributing model parameters, intermediate activations, and optimizer states across the memory of multiple GPUs, distributed training effectively expands the available memory capacity. Typical technologies such as ZeRO, through sharding data processing, make the training of ultra-large-scale models possible.\nAccelerating Model Iteration and R\u0026D Cycle The high parallelism of distributed training makes it possible to complete training tasks that originally required weeks or even months in just a few days, thereby greatly improving the model iteration speed and enabling new architectures and strategies to be verified and applied more quickly.\nSupporting Exploration of Larger-Scale Models Distributed training provides a solid foundation for exploring larger-scale and more complex neural network architectures. It is with this technical support that trillion-parameter models (such as Switch Transformer) can be successfully trained and put into practical applications.\nImproving the Robustness and Scalability of Training Systems Distributed systems have excellent fault tolerance. When a GPU node fails, other nodes can quickly take over the task, ensuring that the training process is not interrupted. At the same time, the cluster size can be flexibly expanded or reduced according to specific needs, meeting the training requirements of different scale models.\nParallelism Training The following figure intuitively shows the differences between various parallel training strategies. Different colors represent different model layers (e.g., three layers), and dashed lines distinguish different GPUs. From left to right are data parallelism, model parallelism (including pipeline parallelism and tensor parallelism), and expert parallelism (MoE).\nFig. 1. An illustration of various parallelism strategies on a three-layer model. Each color refers to one layer and dashed lines separate different GPUs. (Image source: OpenAI Blog, 2022)\nData Parallelism The complete model is copied to each GPU, and the dataset is divided into different batches and distributed to each GPU for parallel computation. Finally, the gradients of all GPUs are aggregated during parameter updates.\nModel Parallelism The model is divided and distributed across different GPUs, with each GPU responsible for computing only a part of the model. It can be further divided into the following two categories:\nPipeline Parallelism: The model is split layer-wise (vertically), with different GPUs responsible for different layers. Micro-batches are passed through the pipeline to execute forward and backward computations in parallel. Tensor Parallelism: Large-scale tensor operations (such as large matrix multiplications) within a layer are split horizontally. Each GPU performs part of the computation in parallel and aggregates the results when necessary. Expert Parallelism Through a gating strategy, each input sample only passes through a subset of experts (sub-networks), thus distributing the entire model across different GPUs by “expert modules”. Commonly used in Mixture-of-Experts (MOE) structures, it can achieve ultra-large parameter scales but only activate a portion of experts during inference/training.\nBelow, I will elaborate on various parallel methods.\nData Parallelism Fig. 2. Data Parallelism. (Image source: Clolossal-AI Documentation)\nIn deep learning training, Data Parallelism (DP) is the most commonly used parallel strategy. Its core idea is:\nReplicate Model Parameters: Place a complete copy of the model parameters on each computing device (usually a GPU). Partition Training Data: Divide the large-scale dataset into multiple subsets along the sample dimension. Different subsets are assigned to different GPUs for processing. Local Forward and Backward Propagation: Each GPU independently computes the loss and corresponding local gradients. Gradient/Parameter Synchronization: Aggregate the gradients from each GPU and update the model parameters, ensuring that the model replicas on all GPUs remain consistent after each iteration. The following shows the data parallelism workflow:\nDataset Partitioning Divide the training dataset $D$ into $N$ non-overlapping subsets ${D_1, D_2, \\dots, D_N}$, where $N$ is the number of GPUs. Usually, it is ensured that the size of each subset is similar to achieve load balancing.\nModel Replication Replicate a complete copy of the model parameters $\\theta$ on each GPU. At the beginning of training, these parameters are the same on each GPU.\nData Distribution Distribute subset $D_i$ to the $i$-th GPU, allowing it to be stored locally and used for subsequent calculations.\nLocal Forward Propagation Each GPU performs forward propagation based on its local data subset $D_i$ to obtain the local loss $L_i(\\theta, D_i)$.\nLocal Backward Propagation Each GPU performs backward propagation based on the local loss $L_i$ to calculate the local gradient\n$$ g_i = \\nabla_{\\theta} L_i(\\theta, D_i). $$ Gradient Synchronization Gradient synchronization (usually All-Reduce) is performed between GPUs to aggregate all local gradients ${g_1, g_2, \\ldots, g_N}$ to obtain the global average gradient\n$$ \\bar{g} = \\frac{1}{N} \\sum_{i=1}^{N} g_i. $$ Parameter Update Each GPU uses the global average gradient $\\bar{g}$ to update its local model parameters:\n$$ \\theta \\leftarrow \\theta - \\eta \\cdot \\bar{g}, $$where $\\eta$ is the learning rate.\nIterative Loop Repeat steps 4-7 until the model converges or reaches the preset number of training epochs.\nBulk Synchronous Parallelism vs. Asynchronous Parallelism In step 6 “Gradient Synchronization” above, how and when to perform “synchronization” is one of the important factors affecting the performance and convergence behavior of data parallelism. It is generally divided into the following two categories:\nBulk Synchronous Parallelism (BSP) is the most common and easiest to understand synchronization mode in data parallelism. Its characteristics can be summarized as “globally synchronizing gradients and updating parameters once after each mini-batch iteration”. The specific process is:\nLocal Computation: Each GPU performs forward and backward propagation based on its data subset $D_i$ to obtain the local gradient $g_i$. Global Communication: All GPUs synchronize (e.g., through All-Reduce) to calculate $\\bar{g}$. Parameter Update: Each node uses $\\bar{g}$ to update its local parameter replica $\\theta$. Wait and Next Iteration: All nodes complete the above operations before entering the next iteration. Asynchronous Parallelism (ASP) aims to get rid of the global synchronization point of BSP and allow each node to perform calculations and parameter updates independently. Its typical implementation is the asynchronous push-pull process under the “Parameter Server (PS)” architecture:\nEach node calculates the gradient $g_i$ locally, and then pushes it to the parameter server; Once the parameter server receives the gradient, it immediately updates the global model parameters; Other nodes will pull down the latest parameters when they need them to continue the next step of calculation. BSP vs. ASP The following table summarizes the main differences between synchronous and asynchronous parallelism in a data parallel environment:\nComparison Dimension Synchronous Parallelism (BSP) Asynchronous Parallelism (ASP) Parameter Update Timing Global synchronization once per mini-batch or after a certain number of iterations Each node updates parameters independently, without needing to keep the same timestep as others Convergence Stability High. The gradients used are the latest, the convergence path is controllable and easy to analyze Lower. Stale gradients exist, convergence rate and stability may be affected Communication Requirements Highly dependent on All-Reduce, all nodes need to wait and exchange data during synchronization Each node asynchronously pushes/pulls to the parameter server, communication is more flexible, but the parameter server may become a bottleneck Hardware Resource Utilization If there are slow nodes or network delays, other nodes need to wait, and resource utilization may be reduced No need to wait for slow nodes, computing resources can be used efficiently Implementation Complexity Relatively low, mainstream frameworks (PyTorch DDP, Horovod, etc.) have built-in support Relatively higher, parameter server and other components are required, more synchronization logic and data consistency need to be handled Applicable Scenarios Homogeneous hardware, good network bandwidth, pursuit of higher convergence quality Heterogeneous hardware, unstable or low bandwidth network, need for extremely high throughput and tolerance for certain convergence risks Typical Implementations PyTorch DDP, TensorFlow MirroredStrategy Parameter Server architecture (MXNet, TensorFlow ParameterServer mode, etc.) Recommendation: In actual projects, start with simple synchronous parallelism (BSP), and use PyTorch DDP or similar tools for multi-GPU training. If the network environment is heterogeneous, there are many nodes, or the task requires extremely high throughput, you can try asynchronous parallelism (ASP) or parameter server solutions, and cooperate with Gradient Accumulation to balance bandwidth and update frequency.\nGradient Accumulation When the batch size is large or communication becomes the main bottleneck, Gradient Accumulation can be used to reduce the synchronization frequency. Its core idea is:\nContinuously calculate the local gradients of multiple mini-batches and accumulate them in the local accumulation buffer; When the number of accumulated mini-batches reaches $K$, trigger a global gradient synchronization and parameter update. Let $g_j$ be the gradient of the $j$-th mini-batch, then in an “accumulation cycle”, we get\n$$ G = \\sum_{j=1}^{K} g_j. $$Then update with learning rate $\\eta$:\n$$ \\theta \\leftarrow \\theta - \\eta \\cdot G. $$Since gradient synchronization is no longer performed for each mini-batch, but once every $K$ accumulated mini-batches, the communication overhead can be significantly reduced. However, the reduced parameter update frequency may also slow down the training convergence speed, and a trade-off between throughput and convergence performance is needed.\nDistributed Data Parallelism Distributed Data Parallelism (DDP) is a highly optimized implementation of BSP in PyTorch v1.5 (Li et al. 2020), which facilitates data parallelism for single-machine multi-GPU and even multi-machine multi-GPU. Its main optimizations include:\nGradient Bucketing: Divide model parameters into multiple “buckets”; when backpropagation is performed, once all gradients in a bucket are calculated, an All-Reduce for that bucket is immediately initiated, instead of waiting for all gradients to be calculated before synchronizing at once. Communication and Computation Overlap: DDP uses asynchronous communication and non-blocking operations to overlap gradient synchronization (communication) with forward propagation and backward propagation (computation) as much as possible, thereby reducing communication overhead. This overlap strategy improves overall parallel efficiency. Gradient Accumulation: DDP can also be easily combined with gradient accumulation. Combined use, by increasing the gradient update interval for each synchronization, reduces the synchronization frequency. In large-scale distributed training, this helps to further reduce communication overhead and improve training efficiency. Fig. 3. Pseudo code for Pytorch DDP. (Image source: Li et al. 2020)\nRing All-Reduce In a multi-GPU (especially single-machine multi-GPU) environment, if there is high-speed interconnect (such as NVLink, PCIe switch, etc.), Ring All-Reduce can be used to significantly reduce communication overhead. The idea is:\nOrganize $k$ nodes into a ring and divide the gradient vector into $k$ parts equally. In the “summation phase”, each node sends a part of its local gradient to the next node and adds it to the received gradient; after several rounds of this process, each node will hold the complete “aggregated” gradient. In the “broadcast phase”, the final result is distributed to all nodes along the ring. Ideally, the communication cost of Ring All-Reduce is approximately independent of the number of nodes (can be regarded as $\\mathcal{O}(1)$), which is very suitable for gradient synchronization in a multi-GPU environment. It is a core communication mode widely used in libraries such as Horovod and NCCL.\nParameter Server When the cluster scale expands to multi-machine multi-GPU, simple single-point aggregation (such as a central server) is often difficult to support parallel training of massive data. Parameter Server (PS) (Li, et al. 2014) is a typical architecture designed for scalable distributed training:\nParameter Sharding: Split model parameters in the form of key-value pairs. Different PS nodes only manage parameters of specific shards. push-pull Semantics: After the computing node obtains the gradient locally, it pushes it to the corresponding PS; after the PS updates the parameters of the shard, the computing node can pull down the latest version when needed for the next step of calculation. Flexible Fault Tolerance and Expansion: By adding or removing PS nodes, capacity can be flexibly expanded in terms of bandwidth or computing needs; backup and fault tolerance strategies can also be implemented on PS. This PS + Worker mode can combine data parallelism and model parallelism simultaneously, splitting ultra-large models and storing them on multiple PSs, and performing distributed training on ultra-large data. PS itself can also be split and merged according to the load situation to form a more complex hierarchical topology.\nModel Parallelism Model Parallelism (MP) is a parallel method that splits the model itself across multiple computing devices (GPUs) for training. When the model parameter size exceeds the memory capacity of a single GPU, model parallelism becomes a necessary choice. Model parallelism is mainly divided into two types: Pipeline Parallelism and Tensor Parallelism.\nNaive Model Parallelism and Bubble Problem\nFig. 4. A naive model parallelism setup where the model is vertically split into 4 partitions. Data is processed by one worker at a time due to sequential dependency, leading to large “bubbles” of idle time. (Image source: Huang et al. 2018)\nNaive model parallelism implementation, which simply divides the model layer by layer and executes it sequentially on different GPUs, will encounter a serious “bubble” problem. Due to the dependencies between layers, when one GPU is processing a certain stage of a data sample, other GPUs may be idle, waiting for the output of the previous GPU or the input of the next GPU. This GPU idle time is called “bubble”, which seriously reduces the efficiency of pipeline parallelism.\nWhere $F_i$ represents the forward propagation of Stage $i$, and $B_i$ represents the backward propagation of Stage $i$. It can be seen that in naive pipeline parallelism, only one GPU is working most of the time, and other GPUs are idle, resulting in low efficiency.\nReasons for the bubble problem:\nInter-layer dependency: There is a sequential dependency between the layers of the neural network. The calculation of the next layer must depend on the output of the previous layer. Sequential execution: Naive model parallelism executes layer by layer in order, which prevents GPUs from working in full parallelism. Pipeline Parallelism Fig. 5. Pipeline Parallelism. (Image source: Clolossal-AI Documentation)\nPipeline Parallelism (PP) divides the model layer by layer into multiple stages, and each stage is assigned to a GPU. Data is passed between different GPUs like a pipeline. The output of the previous GPU serves as the input of the next GPU. Pipeline parallelism aims to improve the efficiency of model parallel training and reduce GPU idle time.\nGPipe GPipe (Huang et al. 2018) is an efficient pipeline parallel training system proposed by Google, which aims to solve the bubble problem of naive pipeline parallelism. The core idea of GPipe is to divide a mini-batch into multiple micro-batches and use synchronous gradient aggregation to alleviate the bubble problem and improve pipeline efficiency.\nFig. 6. Illustration of pipeline parallelism in GPipe with 4 microbatches and 4 partitions. GPipe aggregates and updates gradients across devices synchronously at the end of every batch. (Image source: Huang et al. 2018)\nThe following is the GPipe scheduling strategy:\nMicro-batch Partitioning: Divide a mini-batch into $m$ micro-batches. The size of each micro-batch after partitioning is $1/m$ of the original mini-batch. Pipeline Stage Partitioning: Divide the model layer by layer into $d$ stages, and assign each stage to a GPU. Pipeline Execution: Process each micro-batch in sequence, performing forward and backward propagation in the pipeline. The specific process is as follows: Forward Propagation: For each micro-batch, perform forward propagation sequentially on Stage 1, Stage 2, …, Stage $d$. The output of Stage $i$ serves as the input of Stage $i+1$. Backward Propagation: When the forward propagation of all micro-batches is completed, backward propagation begins. For each micro-batch, perform backward propagation sequentially on Stage $d$, Stage $d-1$, …, Stage $1$. The gradient of Stage $i$ serves as the input of Stage $i-1$. Synchronous Gradient Aggregation: After the backward propagation of all micro-batches is completed, aggregate the gradients of all micro-batches (e.g., averaging) to obtain the global average gradient. Parameter Update: Each GPU uses the global average gradient to update its local model parameters. GPipe Bubble Ratio Formula Assuming that the forward and backward propagation time of each micro-batch is 1 unit, the pipeline depth is $d$, and the number of micro-batches is $m$, the bubble ratio of GPipe is:\n$$ \\text{Bubble Ratio} = 1 - \\frac{2md}{(2m + 2(d-1))d} = \\frac{d-1}{m+d-1} $$When the number of micro-batches $m$ is much larger than the pipeline depth $d$ ($m \\gg d$), the bubble ratio approaches 0, and the pipeline efficiency is close to linear acceleration. The GPipe paper points out that when $m \u003e 4d$, the bubble overhead can be almost ignored (in the case of activation recomputation). Therefore, there are the following benefits:\nReduce Bubbles: GPipe significantly reduces the bubble problem of naive pipeline parallelism through micro-batch partitioning and pipeline scheduling, improving GPU utilization and training efficiency. Synchronous Gradient Aggregation: GPipe adopts synchronous gradient aggregation, which ensures the synchronicity of the training process and good model convergence. Linear Acceleration Potential: When the number of micro-batches is large enough, GPipe can achieve near-linear acceleration. PipeDream Fig. 7. Illustration of 1F1B microbatch scheduling in PipeDream. (Image source: Harlap et al. 2018)\nPipeDream (Harlap et al. 2018) is another efficient pipeline parallel training system. It adopts the 1F1B (1-Forward-1-Backward) scheduling strategy and introduces Weight Stashing technology to further reduce bubbles, improve pipeline efficiency, and solve the weight version inconsistency problem that may be caused by 1F1B scheduling.\nThe core idea of PipeDream’s 1F1B scheduling strategy is that each GPU (Stage) alternately performs forward propagation and backward propagation, working in parallel as much as possible to reduce GPU idle time. The specific process is as follows:\nMicro-batch Partitioning: Divide a mini-batch into $m$ micro-batches. Pipeline Stage Partitioning: Divide the model layer by layer into $d$ stages, and assign each stage to a GPU. 1F1B Scheduling Execution: Each GPU takes turns to perform forward propagation and backward propagation. Weight Stashing Since forward propagation and backward propagation may use different versions of model weights in 1F1B scheduling, it will cause weight version inconsistency problems, affecting the correctness and convergence of training. PipeDream introduces Weight Stashing technology to solve this problem. The core idea of weight stashing is that each GPU maintains multiple versions of model weights and ensures that forward propagation and backward propagation use the same version of weights.\nWeight Stashing Implementation:\nVersion Management: Each GPU maintains a weight version queue to store multiple versions of model weights. Version Selection: When performing forward propagation, select the latest weight version. When performing backward propagation, select the same weight version as the corresponding forward propagation. Version Update: After completing backward propagation of all micro-batches in a mini-batch, update the model weights and generate a new weight version. To further optimize the memory usage of PipeDream, especially in terms of weight stashing, PipeDream has derived two memory optimization variants: PipeDream-flush and PipeDream-2BW.\nPipeDream-flush Fig. 8. Illustration of pipeline scheduling in PipeDream-flush. (Image source: Narayanan et al. 2020)\nPipeDream-flush periodically performs global synchronous pipeline flushing on the basis of PipeDream, similar to GPipe’s synchronous gradient aggregation. By periodically flushing, PipeDream-flush can greatly reduce the memory space required for weight stashing, only needing to maintain a single version of model weights, but it will sacrifice a small amount of throughput.\nPipeDream-2BW PipeDream-2BW (Double-Buffered Weights) maintains two versions of model weights, namely “double-buffered weights”. It updates the model version every $k$ micro-batches, where $k$ is greater than the pipeline depth $d$ ($k \u003e d$). The newly updated model version does not immediately completely replace the old version, because there may still be some remaining backward propagation operations that depend on the old version. With double-buffered weights, PipeDream-2BW can reduce the memory overhead of weight stashing to only maintaining two versions of model weights, significantly reducing memory footprint.\nFig. 9. Illustration of pipeline scheduling in PipeDream-2BW. (Image source: Narayanan et al. 2020)\nThe PipeDream-2BW strategy has the following advantages:\nLower Bubble Overhead: The 1F1B scheduling strategy can further reduce bubbles compared to GPipe, improving GPU utilization and training efficiency. Weight Stashing Solves Version Consistency: Weight stashing technology ensures that forward propagation and backward propagation use the same version of weights, solving the weight version inconsistency problem that may be caused by 1F1B scheduling. Memory Optimization Variants: PipeDream-flush and PipeDream-2BW further optimize memory usage, reduce the memory overhead of weight stashing, and make pipeline parallelism more suitable for memory-constrained scenarios. Tensor Parallelism Tensor Parallelism (TP) is a parallel method that splits tensors in the model (usually weight matrices) along specific dimensions and distributes the split shards to different GPUs for computation. Tensor parallelism has the following advantages:\nBreaking Through Single GPU Memory Limits: Tensor parallelism can distribute model parameters across multiple GPUs, breaking through the memory capacity limit of a single GPU and supporting the training of larger-scale models. Intra-layer Parallelism: Tensor parallelism can achieve parallelization within model layers, such as parallel computation of matrix multiplication operations, improving computational efficiency. Combination with Data Parallelism and Pipeline Parallelism: Tensor parallelism can be combined with other parallel technologies such as data parallelism and pipeline parallelism to form multi-dimensional parallel strategies, further improving training efficiency and scalability. Megatron-LM Megatron-LM (Shoeybi et al. 2019) is a system proposed by NVIDIA for training ultra-large language models. It adopts tensor parallelism technology to parallelize matrix multiplication operations within Transformer model layers, including matrix multiplications in self-attention and MLP.\nFig. 10. Illustration of tensor parallelism for key transformer components proposed in Megatron-LM. (Image source: Shoeybi et al. 2019)\nThe MLP layer of Transformer usually contains two linear layers. The calculation of the first linear layer can be expressed as $Y = \\text{GeLU}(XA)$, where $X$ is the input matrix, $A$ is the weight matrix, and GeLU is the activation function. Megatron-LM splits the weight matrix $A$ along the column dimension into $P$ shards $[A_1, A_2, …, A_P]$, where $P$ is the number of GPUs. Each GPU $i$ is responsible for storing and computing the weight shard $A_i$.\nTensor Parallelism Computation Process of MLP Layer:\n$$ \\begin{aligned} \\text { Split } A \u0026 =\\left[A_1, A_2\\right] \\\\ Y \u0026 =\\operatorname{GeLU}(X A) \\\\ {\\left[Y_1, Y_2\\right] } \u0026 =\\left[\\operatorname{GeLU}\\left(X A_1\\right), \\operatorname{GeLU}\\left(X A_2\\right)\\right] \\end{aligned} $$ Weight Sharding: Split the weight matrix $A$ along the column dimension into $P$ shards $[A_1, A_2, …, A_P]$ and assign shard $A_i$ to GPU $i$. Local Matrix Multiplication: Each GPU $i$ uses the input matrix $X$ and weight shard $A_i$ to perform matrix multiplication to obtain the local output $Y_i = \\text{GeLU}(XA_i)$. Global Concatenation (All-Gather): All GPUs use All-Gather operation to concatenate the local outputs ${Y_1, Y_2, …, Y_P}$ into a complete output matrix $Y = [Y_1, Y_2, …, Y_P]$. Tensor Parallelism of Self-Attention Layer\nMegatron-LM also performs tensor parallel sharding on the Query ($Q$), Key ($K$), Value ($V$) weight matrices in the Transformer’s self-attention layer, and performs corresponding local matrix multiplication and global concatenation operations to achieve tensor parallelism of the self-attention layer. The calculation formula of the self-attention layer is:\n$$ \\text{Attention}(X, Q, K, V) = \\text{softmax}\\left(\\frac{(XQ)(XK)^T}{\\sqrt{d_k}}\\right)XV $$PTD-P PTD-P (Pipeline, Tensor, and Data Parallelism) (Narayanan et al. 2021) is a multi-dimensional parallel strategy that combines pipeline parallelism, tensor parallelism, and data parallelism. PTD-P aims to fully utilize the advantages of various parallel technologies to improve the efficiency and scalability of training ultra-large models.\nFeatures of PTD-P:\nMulti-dimensional Parallelism Combination: PTD-P uses pipeline parallelism, tensor parallelism, and data parallelism simultaneously, which can parallelize the training process from multiple dimensions. Interleaved 1F1B Scheduling: PTD-P adopts the interleaved 1F1B scheduling strategy. Unlike traditional pipeline parallelism, it divides the model into multiple discontinuous layer blocks (model chunks) and assigns multiple layer blocks to each GPU. This scheduling strategy can further reduce bubbles and improve pipeline efficiency. Flexible Parallelism Configuration: PTD-P allows users to flexibly configure the combination of various parallel technologies according to the model structure and hardware resources. For example, tensor parallelism and data parallelism can be used alone, or pipeline parallelism, tensor parallelism, and data parallelism can be used simultaneously. Traditional pipeline parallelism usually divides the model into continuous layer blocks, and each GPU is responsible for a continuous layer block. PTD-P’s interleaved 1F1B scheduling divides the model into multiple discontinuous layer blocks. For example, GPU 1 is responsible for layers 1, 2, 9, 10, GPU 2 is responsible for layers 3, 4, 11, 12, and so on. Each GPU is responsible for multiple discontinuous layer blocks, which can more effectively utilize GPU resources and reduce bubble overhead.\nFig. 11.(Top) Default 1F1B pipeline schedule as in PipeDream-flush.(Bottom) Interleaved 1F1B pipeline schedule. First model chunks are in dark colors and second chunks are in light colors. (Image source: Narayanan et al. 2021)\nMixture-of-Experts Model Mixture-of-Experts (MoE) (Shazeer et al. 2017) is a sparsely activated model that significantly increases the model’s parameter size and performance without significantly increasing the computational cost by combining multiple independent “expert” networks and a gating network. The core idea of MoE is Sparse Activation, that is, for each input sample, only a part of the expert networks are activated, rather than the entire model. This method not only improves computational efficiency but also enhances the model’s expressive ability, making it perform well in LLMs.\nMoE’s design inspiration comes from Ensemble learning, a technology that decomposes complex tasks into multiple subtasks and completes them collaboratively by different models. In MoE, these “subtasks” are processed by multiple independent expert networks, and the gating network is responsible for dynamically selecting the most suitable experts based on the characteristics of the input sample. This division of labor and cooperation mechanism is similar to an expert team in human society: experts in different fields provide professional opinions for specific problems, and finally, a comprehensive result is obtained.\nFig. 12. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)\nCore Components of MoE A typical MoE contains the following components:\nExpert Networks: A set of independent neural networks ${E_1, E_2, …, E_n}$. Each expert network $E_i$ can be any type of neural network, such as FFN, CNN, RNN, etc. The number of expert networks $n$ can be very large, such as dozens, hundreds, or even thousands. Gating Network: A trainable neural network $G$ used to learn a probability distribution based on the input sample $x$ to determine which experts to activate. The input of the gating network is the input sample $x$, and the output is an $n$-dimensional probability vector $p = G(x) = [p_1, p_2, …, p_n]$, where $p_i$ represents the probability of activating expert $E_i$. Expert Output Aggregation: According to the output probability distribution of the gating network, the outputs of the activated expert networks are weighted and summed to obtain the final output $y$ of the MoE layer. Noisy Top-k Gating To achieve sparse activation and ensure balanced expert usage, MoE usually adopts Noisy Top-k Gating as the gating mechanism. This method guarantees computational efficiency and avoids uneven expert load through the introduction of noise and top-k selection. The detailed workflow is as follows:\nGating Score Calculation: For an input sample $x$, the gating network first calculates the gating score $H^{(i)}(x)$ for each expert. This score consists of two parts: linear transformation and noise term, as shown in the formula:\n$$ H^{(i)}(x) =(x W_g)^{(i)} + \\epsilon \\cdot \\text{softplus}\\left((x W_{\\text{noise}})^{(i)} \\right), \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$ Parameter Description: $W_g \\in \\mathbb{R}^{d \\times n}$: Trainable weight matrix of the gating network, where $d$ is the input feature dimension and $n$ is the number of experts. $W_{\\text{noise}} \\in \\mathbb{R}^{d \\times n}$: Weight matrix used to generate noise. $\\epsilon \\sim \\mathcal{N}(0, 1)$: Standard Gaussian noise, increasing gating randomness. $\\text{softplus}(x) = \\log(1 + e^x)$: Smooth activation function to ensure that the noise is non-negative. The introduction of noise avoids the gating network always selecting fixed experts and enhances the robustness and diversity of the model.\nTop-k Selection: After calculating the gating score vector $H(x) = [H^{(1)}(x), H^{(2)}(x), \\dots, H^{(n)}(x)]$, the gating network selects the top $k$ experts with the largest values (usually $k \\ll n$). This step is implemented by the $\\text{topk}(v, k)$ function:\n$$ \\text{topk}^{(i)}(v, k) = \\begin{cases} v^{(i)} \u0026 \\text{if } v^{(i)} \\text{ is in the top } k \\text{ elements of } v \\\\ -\\infty \u0026 \\text{otherwise} \\end{cases} $$Setting the scores of non-Top-k experts to $-\\infty$ ensures that the probabilities of these experts in the subsequent softmax operation are 0, achieving sparsity.\nSoftmax Normalization: Perform softmax normalization on the gating scores of the Top-k experts to obtain a sparse probability distribution $G(x)$:\n$$ G(x) = \\text{softmax}\\left( \\text{topk}(H(x), k) \\right) $$Only the probabilities of the Top-k experts are non-zero, and the rest are 0. For example, if $n=100, k=2$, then the probabilities of 98 experts are 0.\nWeighted Summation: Weight and sum the outputs of the Top-k experts according to the probabilities to obtain the output of the MoE layer:\n$$ y = \\sum_{i=1}^{n} G^{(i)}(x) E_i(x) $$Since only $k$ experts are activated, the amount of calculation is much lower than activating all $n$ experts.\nAuxiliary Loss To prevent the gating network from being overly biased towards a few experts, MoE introduces Auxiliary Loss (Shazeer et al. 2017) to encourage all experts to be used evenly. A common method is based on the square of the Coefficient of Variation (CV) of expert usage rate:\n$$ \\mathcal{L}_{\\text{aux}} = w_{\\text{aux}} \\cdot \\text{CV}\\left( \\sum_{x \\in X} G(x) \\right)^2 $$ Parameter Description:\n$X$: Input samples of a mini-batch. $\\sum_{x \\in X} G(x)$: Statistics on the number of times each expert is activated in a mini-batch. $\\text{CV}$: The ratio of standard deviation to mean, measuring the uniformity of expert usage distribution. $w_{\\text{aux}}$: Weight of auxiliary loss, which needs to be adjusted manually. Function: By minimizing $\\mathcal{L}_{\\text{aux}}$, the model optimizes the balance of expert selection and avoids some experts being overused while others are idle.\nGShard GShard (Lepikhin et al. 2020) mainly shards the MoE layer, distributing the expert networks ${E_1, E_2, …, E_n}$ in the MoE layer to multiple TPU devices. For example, if there are $P$ TPU devices, the expert networks can be divided into $P$ groups, and each group of expert networks is assigned to a TPU device. Other layers of the Transformer model (such as self-attention layer, LayerNorm layer) are replicated on all TPU devices.\nImproved Gating Mechanism of GShard:\nGShard has made some improvements on the basis of Noisy Top-k Gating to improve the performance and stability of the gating mechanism:\nExpert Capacity: To avoid expert overload, GShard introduces expert capacity limits. Each expert network has a capacity limit, indicating the maximum number of tokens it can process. If a token is routed to an expert network that has reached its capacity limit, the token will be marked as “overflowed”, and the gating output will be set to a zero vector, indicating that the token will not be routed to any expert network.\nLocal Group Dispatching: To improve gating efficiency, GShard groups tokens and enforces expert capacity limits at the group level. For example, divide the tokens in a mini-batch into multiple local groups, each local group containing a certain number of tokens. The gating network selects the top-k expert networks for each local group and ensures that the number of tokens processed by each expert network in a local group does not exceed its capacity limit.\nAuxiliary Loss: GShard also uses an auxiliary loss function to balance expert load. Different from the auxiliary loss of the original MoE model, GShard’s auxiliary loss aims to minimize the mean square error of the proportion of data routed to each expert network, which more directly measures the degree of expert load balance.\nRandom Routing: To increase the randomness of routing, GShard introduces a random routing mechanism when selecting the top-k expert networks. In addition to selecting the best top-k expert networks, GShard also randomly selects suboptimal expert networks with a certain probability to increase the diversity of expert networks and improve the generalization ability of the model.\nBelow is the core algorithm flow of GShard:\nFig. 13. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)\nSwitch Transformer Switch Transformer (Fedus et al. 2021) is a MoE model proposed by Google with a parameter size of trillions. Its core innovation is to replace the dense feed-forward network (FFN) layer in the Transformer model with a sparse Switch FFN layer. Unlike GShard’s Top-2 Gating, Switch Transformer only routes each input token to one expert network, which has higher sparsity and further reduces computational costs, making it possible to train trillion-parameter models. It encourages token routing to be more balanced among $N$ experts. The auxiliary loss of Switch Transformer is based on the cumulative product of the actual routing ratio and the predicted routing probability. The specific formula is as follows:\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ Parameter Description: $N$: Total number of experts.\n$f_i$: The proportion of tokens routed to the $i$-th expert, defined as:\n$$ f_i = \\frac{1}{T} \\sum_{x \\in B} 1\\{\\text{argmax } p(x) = i\\} $$ $P_i$: The routing probability of the $i$-th expert predicted by the gating network, defined as:\n$$ P_i = \\frac{1}{T} \\sum_{x \\in B} p_i(x) $$ $T$: Total number of tokens in batch $B$.\n$\\alpha$: Weight hyperparameter of auxiliary loss, usually set to $10^{-2}$.\nBy minimizing $\\text{loss}$, the model makes the actual routing ratio $f_i$ consistent with the predicted probability $P_i$, thereby indirectly promoting load balancing between experts and avoiding some experts being idle.\nFig. 14. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)\nSwitch Router Mechanism:\nRouting Prediction: For an input token $x$, Switch Router predicts the routing probability $p_i = G^{(i)}(x)$ of each expert network, where $i = 1, 2, …, n$, and n is the number of expert networks.\nExpert Selection: Select the expert network with the highest routing probability as the best expert network. Switch Transformer adopts the Top-1 routing strategy, that is, each token is only routed to the expert network with the highest routing probability.\nToken Routing: Route the input token $x$ to the selected best expert network for processing.\nTraining Stability Optimization of Switch Transformer:\nTo improve the training stability of Switch Transformer, the paper proposes the following optimization strategies:\nSelective Precision Using FP32 precision inside the routing function can improve training stability and avoid additional overhead caused by FP32 tensor communication. Specifically, the calculation process of Switch Router uses FP32 throughout, and the final result is converted to FP16 to balance efficiency and precision.\nSmaller Initialization It is recommended to adjust the weight initialization scale parameter $s$ of Transformer from 1 to 0.1. A smaller initialization scale helps to alleviate the risk of gradient explosion in the early stage of training, thereby improving overall training stability. The specific implementation is to sample from a truncated normal distribution with a mean of 0 and a standard deviation of $\\sqrt{s/n}$ (where $n$ is the number of input units).\nHigher Expert Dropout Using a higher dropout rate (e.g., 0.4) in the expert FFN layer, while maintaining a lower dropout rate (e.g., 0.1) in non-expert layers, this setting can effectively prevent overfitting and thus enhance the generalization ability of the model. The experimental results in the figure below show that the model performs best when the dropout rate of the expert layer is set to 0.4 on tasks such as GLUE, CNNDM, SQuAD, and SuperGLUE.\nFig. 15. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: Fedus et al. 2021)\nThe Switch Transformers paper uses the following figure to intuitively show how different parallel technologies split model weights and data:\nFig. 16. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)\nExpert Choice Expert Choice (EC) (Zhou et al. 2022) is a routing strategy opposite to token choice routing (such as GShard’s top-2 or Switch Transformer’s top-1). In token choice routing, each token selects top-k experts from all experts for routing; while in expert choice routing, each expert selects top-k tokens from all tokens for processing. This method aims to solve the problems of load imbalance and token waste in token choice routing, and significantly improve training efficiency. The following is the specific calculation process:\nCalculate Token-to-Expert Affinity Score\nFor an input matrix $X \\in \\mathbb{R}^{n \\times d}$, the process of calculating the token-to-expert affinity score matrix $S \\in \\mathbb{R}^{n \\times e}$ is:\n$$ S = \\text{softmax}(X \\cdot W_g), \\quad \\text{where } W_g \\in \\mathbb{R}^{d \\times e}. $$ Here, $W_g$ is the gating weight matrix, and $e$ is the number of experts.\nExpert Selects Tokens\nEach expert selects top-k tokens from all tokens for processing. Top-k selection is performed on $S^T$:\n$$ G, I = \\text{top-}k(S^T, k), $$to get:\nGating matrix $G \\in \\mathbb{R}^{e \\times k}$: Records the routing weights corresponding to the tokens selected by the experts, where $G[i, j]$ represents the weight of the $j$-th token selected by expert $i$; Token index matrix $I \\in \\mathbb{R}^{e \\times k}$: Represents the index of the token selected by each expert in the input. One-hot Encoding\nConvert the token index matrix $I$ into a one-hot encoding matrix $P \\in \\mathbb{R}^{e \\times k \\times n}$ for subsequent calculations:\n$$ P = \\operatorname{one}-\\operatorname{hot}(I) $$ Construct Gated FFN Layer Input\nFor each expert $i$, the input of its gated FFN layer is:\n$$ (P \\cdot X) \\in \\mathbb{R}^{e \\times k \\times d}. $$ EC controls the sparsity of the model by regularizing and limiting the number of experts to which each token is routed. A common regularization target is as follows:\n$$ \\begin{aligned} \u0026 \\max_{A} \\langle S^{\\top}, A \\rangle + \\lambda H(A) \\\\ \u0026 \\text{s.t. } \\forall i: \\sum_{j'} A[i, j'] = k, \\quad \\forall j: \\sum_{i'} A[i', j] \\leq b, \\quad \\forall i,j: 0 \\leq A[i, j] \\leq 1, \\end{aligned} $$In the optimization problem considered, a matrix $A$ is defined, and the element in the $i$-th row and $j$-th column indicates whether the $i$-th expert has selected the $j$-th token (value 0 or 1). Since this optimization problem is complex to solve, the paper uses the Dijkstra’s algorithm (to obtain an approximate solution through multiple iterations) to solve it.\nThe parameter $b$ is usually determined by the total number of tokens $n$ in the batch and the capacity factor, where the capacity factor represents the average number of experts used by each token. Most experiments use a higher capacity factor. The experimental results show that even when the capacity is reduced, EC (Expert Choice) still performs better than traditional top-1 token choice routing, although capped expert choice slightly reduces fine-tuning performance.\nThe advantages of EC are mainly reflected in the following two aspects:\nPerfect Load Balancing: Each expert processes a fixed number of $k$ tokens, thus avoiding the problem of some experts being overloaded while others are idle, achieving ideal load balancing. Higher Training Efficiency: Experiments show that EC can improve the training convergence speed by about 2 times, which is more efficient than traditional token choice routing. However, EC also has the following limitations:\nBatch Size Requirements: Since EC has high requirements for batch size, it is not suitable for scenarios with smaller batch sizes. Autoregressive Generation Limitations: In autoregressive text generation tasks, EC’s top-k selection cannot be implemented because future tokens cannot be predicted, so it is not suitable for such tasks. Sequence Parallelism Sequence Parallelism (SP) is a parallelization strategy proposed for long sequence models (such as Transformer). By partitioning the input in the sequence dimension, it greatly reduces activation memory footprint and improves training efficiency. It is often used in combination with data parallelism, tensor parallelism, or pipeline parallelism, and is especially suitable for processing ultra-long text or other sequence data.\nColossal-AI Sequence Parallelism Fig. 17. The overall architecture of the proposed sequence parallelism and existing parallel approaches. For sequence parallelism, Device 1 and Device 2 share the same trainable parameters. (Image source: Li, et al. 2021)\nThe computational complexity and memory overhead of self-attention are proportional to the square of the sequence length $s$, $O(s^2)$. Long sequence data will increase the intermediate activation memory usage, thus limiting the training capacity of the device. Colossal-AI sequence parallelism (Li, et al. 2021) proposes splitting ultra-long sequences to multiple cards from a system perspective. The specific solution steps are as follows.\nSequence Chunking Divide the input sequence into several chunks, each chunk is saved and computed by different GPUs; therefore, each card only needs to store the activation of its corresponding sequence chunk, avoiding single-card memory explosion. Ring Communication + Self-Attention Propose Ring Self-Attention (RSA) mechanism: each GPU first calculates local attention, and then sequentially transmits (ring structure) Key/Value chunks to adjacent GPUs. After multiple iterations, it is guaranteed that each GPU can obtain global sequence information. Combination with Other Parallelism Methods Not restricted by hyperparameters such as the number of attention heads and layers, it can be combined with data parallelism, tensor parallelism, pipeline parallelism and other technologies to jointly break through the sequence length limit of large-scale models. Fig. 18. Ring Self-Attention. (Image source: Li, et al. 2021)\nMegatron-LM Sequence Parallelism Megatron-LM (Shoeybi et al. 2019) originally used tensor parallelism to share part of the activation values, but the activation values of operations such as LayerNorm and Dropout in Transformer still need to be completely saved on a single card, and the memory consumption is still huge. Therefore, NVIDIA proposed Megatron-LM sequence parallelism (Korthikanti, et al. 2022) to split these activation values in the sequence dimension, greatly reducing the footprint.\nFig. 19. Transformer layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)\nFig. 20. MLP layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)\nSequence Dimension Splitting For activations that are difficult to split in the tensor dimension, such as LayerNorm and Dropout, divide them along the sequence dimension, so that each GPU only processes a part of the sequence’s nonlinear operations. Tensor Parallelism is Still Retained Linear operations such as Attention and MLP continue to use tensor parallelism; the activations of sequence parallelism need to perform corresponding All-Gather or Reduce-Scatter before and after to exchange data. Selective Activation Recomputation For some operations with small computational load but large activation volume, choose to temporarily recompute during backpropagation to further save memory. DeepSpeed-Ulysses Sequence Parallelism DeepSpeed-Ulysses (Jacobs et al. 2023) proposes an efficient sequence parallelism scheme for ultra-long sequence training. By partitioning the input in the sequence dimension and combining two-stage all-to-all communication, it effectively reduces communication volume and activation memory, thereby supporting the training of million-token long sequence Transformer models.\nFig. 21. DeepSpeed sequence parallelism(DeepSpeed-Ulysses) design. (Image source: Jacobs et al. 2023)\nSequence Partitioning + All-to-All Communication Divide the input sequence along the sequence dimension to $P$ GPUs, and each GPU only processes a local $N/P$ sequence; before attention calculation, exchange Query ($Q$), Key ($K$), and Value ($V$) through All-to-All operation, so that each GPU obtains complete sequence information, but only calculates the assigned attention heads.\nTwo-Stage Communication Optimization\nFirst All-to-All: Perform all-to-all exchange on $Q$/$K$/$V$ before attention calculation to disperse activation calculation and reduce memory pressure per card; Second All-to-All: After attention calculation, collect the output context and remap it to local sequence partitions, which not only restores the original sequence structure but also significantly reduces the amount of communication data. Efficient Communication and Generality Using all-to-all communication, the communication volume is reduced to $O(N/P)$, which saves nearly $P$ times the bandwidth compared to the traditional All-Gather method (communication volume $O(N)$); at the same time, this scheme is suitable for dense and sparse attention and can be seamlessly integrated with ZeRO-3 memory optimization, thereby supporting efficient training of larger models and longer sequences.\nFig. 22. DeepSpeed-Ulysses vs Megatron LM. (Image source: DeepSpeed Blogs)\nIn a 64-card A100 environment, the throughput is increased by up to 2.5 times compared to Megatron-LM sequence parallelism, and longer sequences (million-level tokens) can be processed; The convergence performance is the same as the original model, and it can be easily integrated into the Megatron-DeepSpeed framework. Optimizer-Related Parallelism: ZeRO ZeRO (Zero Redundancy Optimizer) (Rajbhandari et al. 2019) is an optimizer parallelism technology designed to eliminate memory redundancy when training large models. The main memory consumption for training large models is in two parts:\nModel States: Including optimizer states (such as momentum and second-order moments of Adam), gradients, and model parameters. Mixed-precision training not only requires storing FP16 data but also needs to retain FP32 versions of parameters and states, resulting in higher memory footprint. Activations, Temporary Buffers, and Memory Fragmentation (Residual States): These data are only used once in forward and backward propagation, but they also occupy a lot of memory. To solve the memory redundancy problem, ZeRO adopts two major strategies:\nZeRO-DP (Data Parallelism): For model states, by sharding and distributing optimizer states, gradients, and parameters to multiple data parallel processes, redundancy is eliminated, and communication volume is reduced by using dynamic communication scheduling.\nZeRO-R (Residuals Optimization): For activations and temporary buffers, memory usage is optimized by using sharded activation recomputation, fixed buffer size, and real-time memory fragmentation management.\nZeRO Sharding Strategy ZeRO is divided into three stages, each stage further reduces memory redundancy on the basis of the previous stage, thus making it possible to train ultra-large models:\nZeRO-1 (Optimizer State Sharding) Principle: Shard optimizer states (such as Adam’s momentum and second-order moments) along the parameter dimension into $P$ shards ($P$ is the number of GPUs), and each GPU only stores the states corresponding to the model parameters it is responsible for. Local Update: Each GPU only updates its locally stored state and parameter shards during the parameter update phase, without additional cross-GPU communication. ZeRO-2 (Gradient Sharding) Principle: On the basis of optimizer state sharding, gradients are also sharded along the parameter dimension, and each GPU only stores the corresponding gradient shard. Each GPU calculates local gradients and uses efficient Reduce-Scatter operations to aggregate gradients and then update local parameter shards. ZeRO-3 (Parameter Sharding) Principle: On the basis of ZeRO-1 and ZeRO-2, model parameters (usually 16-bit data) are also sharded, and each GPU only stores the parameter shards corresponding to it. Parameter Collection on Demand: During forward or backward propagation, if a GPU needs complete model parameters, it collects the missing shards from other GPUs. This process is only performed when necessary to reduce communication overhead. The following figure shows the comparison of model state memory consumption per device in different stages:\nFig. 23. Comparing the per-device memory consumption of model states, with three stages of ZeRO-DP optimizations. (Image source: Rajbhandari et al. 2019)\nComparison of DeepSpeed ZeRO Sharding and Offload Strategies To better understand DeepSpeed’s ZeRO strategy, the following compares each stage and Offload scheme:\nZeRO Stage Description Memory Footprint Training Speed ZeRO-0 Pure data parallelism, no sharding, all states are fully replicated on each GPU. Highest Fastest ZeRO-1 Optimizer states are sharded only, gradients and parameters are still replicated. Higher Slightly slower than ZeRO-0 ZeRO-2 Optimizer states and gradients are sharded. Medium Slower than ZeRO-1 ZeRO-3 Optimizer states, gradients, and model parameters are sharded. Lowest Significantly slower than ZeRO-2, affected by model size and network bandwidth Offload Type Description Memory Footprint Training Speed ZeRO-1 + CPU Offload On the basis of ZeRO-1, optimizer states are offloaded to CPU memory, reducing GPU memory footprint, but relying on PCIe bandwidth and occupying CPU memory. Medium-Low Slower than ZeRO-1 ZeRO-2 + CPU Offload On the basis of ZeRO-2, optimizer states are offloaded to CPU memory, further reducing GPU memory footprint for large models, but increasing CPU-GPU data transfer. Low Slower than ZeRO-2 ZeRO-3 + CPU Offload On the basis of ZeRO-3, optimizer states and model parameters are offloaded to CPU, GPU memory footprint is the lowest, but CPU-GPU communication overhead is extremely large. Extremely Low Very Slow ZeRO-Infinity (NVMe Offload) Based on ZeRO-3, states are offloaded to NVMe devices, breaking through CPU memory limits, suitable for ultra-large models; performance is highly dependent on NVMe parallel read and write speed. Extremely LowNVMe support required Slower than ZeRO-3, but usually better than CPU Offload scheme Communication Volume and Performance Impact ZeRO-0/1/2: Mainly rely on All-Reduce for gradient synchronization, and the communication volume is relatively low.\nZeRO-3: All-Gather/All-Reduce operations are required for model parameters, and the communication volume increases significantly. Network bandwidth becomes a key bottleneck.\nOffload Strategy (CPU/NVMe): Data transmission is mainly between CPU ↔ GPU or NVMe ↔ GPU. The transmission bandwidth is much lower than the communication between GPUs, which may significantly affect the training speed, especially in ZeRO-3 scenarios.\nMulti-dimensional Parallelism Multi-dimensional Parallelism refers to the organic combination of multiple parallel technologies such as data parallelism, model parallelism, and pipeline parallelism in distributed training to fully utilize the computing resources of modern GPU clusters. Through this “3D parallelism” or “4D parallelism” strategy, not only memory efficiency can be improved, but also computational efficiency can be improved, thereby achieving efficient training of ultra-large-scale (even trillion-parameter level) models.\n3D Parallelism With the rapid improvement of the computing power of GPU clusters, training trillion-parameter models is no longer out of reach. DeepSpeed integrates data parallelism, model parallelism, and pipeline parallelism to build a “3D parallelism” strategy. This strategy mainly solves the two major challenges faced by training ultra-large models:\nMemory Efficiency: Model layers are divided into different pipeline stages, and each stage is further divided by model parallelism, reducing the memory occupied by models, optimizers, and activations. However, it should be noted that model splitting cannot be unlimited, otherwise, the communication overhead will increase significantly, which will affect computational efficiency.\nComputational Efficiency: To make the number of computing workers exceed the limitations of simple model and pipeline parallelism, and to ensure computational efficiency, DeepSpeed expands with ZeRO-DP (data parallelism based on optimizer state sharding). ZeRO-DP not only further optimizes memory usage but also allocates data parallel groups to devices with local high-bandwidth communication through topology-aware mapping, greatly reducing communication overhead.\nThe following diagram shows the overall strategy of 3D parallelism:\nFig. 24. Example 3D parallelism with 32 workers. Layers of the neural network are divided among four pipeline stages. Layers within each pipeline stage are further partitioned among four model parallel workers. Lastly, each pipeline is replicated across two data parallel instances, and ZeRO partitions the optimizer states across the data parallel replicas. (Image source: Majumder et al. 2020)\nEach parallel dimension (data, model, pipeline) is carefully mapped to fully utilize the communication bandwidth within and between nodes. Specific strategies include:\nOptimize Intra-node Communication: Since model parallelism has the largest communication overhead, model parallel groups are preferentially arranged within the same node to utilize higher intra-node bandwidth (e.g., using NVIDIA Megatron-LM’s tensor sharding method); Data Parallelism and Pipeline Parallelism: When model parallelism does not cover the entire node, data parallel groups are arranged within the same node as much as possible; pipeline parallelism can be flexibly arranged for cross-node scheduling due to its smaller communication volume. By reducing the amount of communication data in each data parallel group and increasing the parallelism of local parallel communication, the overall communication bandwidth is effectively amplified.\nFig. 25. Mapping of workers in Figure 24 to GPUs on a system with eight nodes, each with four GPUs. Coloring denotes GPUs on the same node. (Image source: Majumder et al. 2020)\n4D Parallelism To further expand the model scale, Llama3 (Grattafiori et al. 2024) adopted a 4D parallel strategy during training. It combines four parallel methods to shard the model in a more fine-grained manner, so that the model parameters, optimizer states, gradients, and activations on each GPU can be adapted to the capacity limit of high-bandwidth memory (HBM). These four parallel methods are:\nTensor Parallelism (TP): Divide a single weight tensor into multiple blocks and distribute them across different devices; Pipeline Parallelism (PP): Vertically divide the model into multiple stages, and each stage processes different micro-batches in parallel on different devices; Context Parallelism (CP): Divide the input context into multiple segments to alleviate the memory bottleneck when inputting long sequences; Data Parallelism (DP), usually using Fully Sharded Data Parallelism (FSDP): Shard models, optimizer states, and gradients, and synchronize after each training step. The following diagram shows an example of 4D parallelism implemented on 16 GPUs. The position of each GPU is represented by a vector [D1, D2, D3, D4], where each dimension corresponds to a parallel strategy. GPUs are grouped according to four dimensions [TP, CP, PP, DP], and the group size of each dimension is 2. For example, GPU0 and GPU1 belong to the same tensor parallel group; GPU0 and GPU2 belong to the same context parallel group; GPU0 and GPU4 belong to the same pipeline parallel group; GPU0 and GPU8 belong to the same data parallel group:\nFig. 26. Illustration of 4D parallelism. (Image source: Grattafiori et al. 2024)\nThrough the 4D parallel strategy, Llama3 can fully utilize the computing resources of multiple GPUs during training, while effectively reducing memory footprint and supporting the training of ultra-large-scale models.\nMemory Optimization Techniques In addition to parallel training techniques, there are many memory optimization techniques designed to help train LLMs. These designs mainly start from reducing the memory footprint of each stage in the training process.\nCPU Offloading CPU Offloading (Rhu et al. 2016) refers to a common and intuitive practice of offloading data or tensors that are temporarily not needed to the CPU when GPU memory is insufficient and loading them back to the GPU when needed. Its main purpose is to use the larger capacity of CPU memory to expand available space, so that larger-scale models can be trained even in memory-constrained environments. However, this method will bring additional data transmission overhead and usually reduce training speed, so its application has been relatively reduced in recent years.\nIdentify Offloadable Tensors: Identify tensors that are temporarily not used during training, such as model parameters, optimizer states, intermediate activations, etc. The basis for judging whether a tensor can be offloaded can be the frequency of use, life cycle, etc. of the tensor. Tensor Offloading: Move offloadable tensors from GPU memory to CPU memory. Data transmission is usually performed through the PCIe bus. Tensor Prefetching: Before needing to use tensors offloaded to CPU memory, load the tensors from CPU memory back to GPU memory in advance. Prefetching operations can be performed in parallel with GPU computing operations to reduce data loading latency. Tensor Usage: The GPU uses tensors loaded back into GPU memory for computation. Tensor Re-offloading: After the tensor is used up, if the tensor is no longer needed for a period of time, it can be offloaded to CPU memory again to release GPU memory space. ZeRO-Offload and ZeRO-Infinity are memory optimization technologies based on CPU offloading implemented in the DeepSpeed library. ZeRO-Offload offloads optimizer states to CPU memory, and ZeRO-Infinity goes further, offloading model parameters to CPU memory or even NVMe disks, breaking through the GPU memory wall and supporting the training of larger-scale models.\nThe following figure intuitively shows the memory optimization technology of Heterogeneous system:\nFig. 27. Heterogenous system illustration. (Image source: Clolossal-AI Documentation)\nActivation Recomputation/Gradient Checkpointing Activation Recomputation/Gradient Checkpointing (Chen et al. 2016) is a technology that trades computation for memory. During training, only part of the activation values are saved (e.g., the input activation values of each Transformer layer). During backpropagation, the unsaved activation values are recomputed. Activation recomputation can significantly reduce the activation memory footprint during training, especially when training deep neural networks.\nSelect Checkpoints: Select some layers in the model as checkpoints. Usually, key layers in the model are selected as checkpoints, such as the input layer of the Transformer layer. Forward Pass: During forward propagation, only the activation values of checkpoint layers are saved. For non-checkpoint layers, the activation values are immediately released after calculation and not saved. Backward Pass: During backpropagation, when it is necessary to calculate the gradient of a non-checkpoint layer, forward propagation is performed again first to calculate the activation value of the layer (recomputation), and then backward propagation is performed to calculate the gradient. For checkpoint layers, since the activation values of checkpoint layers have been saved, the saved activation values can be directly used for backpropagation without recomputation. The following is a memory cost analysis of activation recomputation. For ease of analysis, assume that the model has a total of $n$ network layers and divides them evenly into $k$ segments. In this way, each segment contains approximately $n/k$ network layers. When doing activation recomputation, we only save the activation values at the boundaries of each segment (i.e., checkpoints), and recompute the rest when needed. The following function represents the memory requirement during training:\n$$ \\text{cost-total} \\;=\\; \\max_{i=1,\\ldots,k}\\bigl[\\text{cost-of-segment}(i)\\bigr] \\;+\\; O(k) \\;=\\; O\\Bigl(\\tfrac{n}{k}\\Bigr) \\;+\\; O(k). $$Next, consider how to choose the optimal $k$ to minimize $f(k)$ given $n$:\n$$ f(k) \\;=\\; \\frac{n}{k} \\;+\\; k. $$Take the derivative of $f(k)$ with respect to $k$ and set it to 0 (only consider $k\u003e0$):\n$$ f'(k) \\;=\\; -\\frac{n}{k^2} \\;+\\; 1 \\;=\\; 0 \\quad\\Longrightarrow\\quad k^2 = n \\quad\\Longrightarrow\\quad k = \\sqrt{n}. $$Substituting $k = \\sqrt{n}$, we can get the minimum memory overhead of approximately\n$$ f(\\sqrt{n}) \\;=\\; \\frac{n}{\\sqrt{n}} \\;+\\; \\sqrt{n} \\;=\\; 2\\sqrt{n}. $$Therefore, the overall peak memory requirement of this approach can be reduced to the order of $O(\\sqrt{n})$ (compared to the $O(n)$ memory of generally directly saving all activations), which is why activation recomputation technology can bring “sublinear” memory footprint. The following figure intuitively shows the effect of this trick.\nFig. 28. The memory cost of different memory saving algorithms. Sharing: Memory used by intermediate results is recycled when no longer needed. Inplace: Save the output directly into memory of an input value. (Image source: Chen et al. 2016)\nIt should be noted that activation recomputation requires additional forward recomputation in the backward propagation stage. Each segment needs to perform forward computation of $n/k$ layers. If the network is divided into $k$ segments, the total recomputation during backpropagation is approximately $k \\times \\bigl(n/k\\bigr) = n$ layers of forward operations, which is equivalent to doing approximately one more “forward propagation” in each training iteration. This is usually acceptable in LLM training because:\nCompared to quickly exhausting GPU memory and making it impossible to train large-scale models, this additional cost in computation is usually more bearable. When the model is very deep ($n$ is large), using activation recomputation technology can significantly reduce memory usage from $O(n)$ to $O(\\sqrt{n})$, making it possible to train more and deeper large models on given hardware. Mixed Precision Training Mixed Precision Training (Micikevicius al. 2017) is a technology that simultaneously uses low-precision floating-point numbers (such as FP16 or BF16) and high-precision floating-point numbers (such as FP32) during model training. Its core goal is to reduce memory footprint and accelerate training while maintaining model accuracy comparable to full-precision training as much as possible.\nModern GPUs have higher throughput and lower memory footprint in low-precision computing, thereby reducing memory access overhead and memory bandwidth requirements. Mixed-precision training can significantly improve training speed. The following figure shows the basic process of mixed-precision training in a network layer: forward and backward propagation mainly use half-precision (FP16) operations, while gradient accumulation and parameter updates use full-precision (FP32) to avoid numerical precision problems that may be caused by low-precision computing.\nFig. 29. Mixed precision training iteration for a layer. (Image source: Micikevicius al. 2017)\nMixed-precision training mainly relies on the following three key technologies:\nFull-Precision Master Copy of Weights To prevent gradients from being truncated to zero due to being too small in magnitude under FP16, a master copy of FP32 weights is maintained during training. The specific process is:\nInitialization: Use FP32 weights as the master copy of the model; Forward/Backward Propagation: Before each iteration starts, convert FP32 weights to FP16 for forward propagation and backward propagation to calculate FP16 gradients; Parameter Update: Before updating parameters, convert FP16 gradients to FP32 and use them to update the FP32 master copy. This design not only utilizes the efficiency of low-precision computing but also ensures the accuracy of parameter updates.\nLoss Scaling To avoid gradient underflow due to the limited representation range of low precision, the loss value is usually amplified before backpropagation. The specific process is:\nUse FP32 to calculate the loss $L$; Multiply the loss by a scaling factor $S$ to get $L’ = L \\times S$, and then perform backpropagation to calculate FP16 gradients; Before parameter update, divide the gradient by $S$ to restore it to the true gradient. The choice of scaling factor is crucial: too small may not avoid gradient underflow, and too large may cause gradient overflow. Dynamic loss scaling technology can automatically adjust the scaling factor according to the actual situation of gradients during training.\nAs shown in the figure below, amplifying the loss makes the gradient distribution more concentrated in the higher numerical part, thereby retaining the subtle information that may be truncated under low-precision representation.\nFig. 30. The histogram of gradients in full precision. The left part up to $2^{-24}$ will be zero-ed off once the model switches to FP16. (Image source: Micikevicius al. 2017)\nArithmetic Precision Control For operations with high precision requirements (such as vector dot product and summation reduction), FP32 can be used for accumulation calculation, and then converted to FP16 for storage; for element-wise operations, FP16 or FP32 can be selected according to specific needs. Compression In the deep learning training process, intermediate results (such as activation values and gradient information), although only used once in forward propagation and once in backward propagation, often occupy a lot of memory. Considering that there is a significant time interval between two uses, data can be compressed after the first use, and then decompressed when needed later, thereby effectively reducing memory footprint.\nCompression technology is mainly applied to two scenarios:\nActivation Value Compression: Compress activation values after forward propagation and decompress before backward propagation. This is especially important for deep neural networks because activation values usually occupy a lot of memory. Gradient Compression: Compress gradients after calculating gradients in backpropagation and before gradient synchronization to reduce the amount of data for cross-GPU communication, thereby improving distributed training efficiency. Compression technology can be divided into two categories:\nLossless Compression: Methods such as Huffman coding or Lempel-Ziv algorithm are used to ensure that the decompressed data is completely consistent with the original data. However, due to the low compression rate, its memory saving effect is limited.\nLossy Compression: Algorithms such as JPEG or MPEG are used to obtain higher compression rates on the premise of allowing certain data loss. This method can significantly reduce memory footprint, but may have a certain impact on model accuracy and convergence.\nGist (Jain et al. 2018) is a memory optimization technology for activation value compression. Its core lies in using data encoding strategies to compress intermediate results, mainly including two encoding schemes:\nLayer-Specific Lossless Encoding: Design special lossless encoding schemes for specific layer structures (such as ReLU-Pool and ReLU-Conv):\nFor ReLU-Pool layers, binary encoding can be used; For ReLU-Conv layers, sparse storage and dense computation encoding are used. Aggressive Lossy Encoding: Delayed Precision Reduction (DPR) technology is used. The core idea of DPR is: activation values need to maintain high precision during forward propagation, while lower precision can be tolerated during backward propagation. Therefore, activation values are compressed to lower precision after forward propagation, and then decompressed to high precision before backward propagation.\nMemory-Efficient Optimizers Traditional optimizers (such as Adam, SGD with Momentum) need to maintain a large amount of state data (such as momentum and variance) for each model parameter during training. Their memory footprint is often comparable to or even higher than the model parameter size. For example, taking the Adam optimizer (Kingma et al. 2014) as an example, each parameter needs to store the first-order moment and the second-order moment. Adding up with the parameter itself and its gradient, the entire training process requires approximately 4 times the memory of the model weights, which poses a severe challenge to large model training.\nTo reduce memory consumption, memory-efficient optimizers are mainly designed through the following strategies:\nReduce the Number of State Variables: Only save necessary statistical information instead of complete matrices; Reduce the Precision of State Variables: Use FP16 or bfloat16 for storage; Share State Variables: Share part of the state information between multiple parameters. Adafactor Adafactor (Shazeer et al. 2018) is a memory-efficient adaptive learning rate optimizer. Unlike Adam, Adafactor does not store the complete second-order moment estimation matrix, but only stores two vectors (row and column statistics) to replace the complete second-order moment matrix, which significantly reduces memory footprint, especially suitable for scenarios where the parameter matrix has a low-rank structure.\nSM3 SM3 (Sparse Momentum for Massive Models) (Anil et al. 2019) provides a memory-efficient adaptive optimization scheme through sparse updates and state sharing.\nSparse Momentum: Only update Momentum for parameters with non-zero gradients, thereby reducing computation and storage overhead; State Sharing: To a certain extent, allow different parameters to share state variables, further reducing memory consumption; Adaptive Learning Rate: Dynamically adjust the learning rate according to the gradients of each parameter, improving the stability and convergence speed of model training. Okay, here is the English translation of the provided text about LoRA:\nLoRA LoRA (Low-Rank Adaptation) (Hu et al. 2021) proposes a method that introduces low-rank adapters alongside pre-trained weights. This enables efficient fine-tuning by adding only a small number of parameters, while preserving the original inference capabilities of the pre-trained model.\nThe figure below intuitively illustrates the principle and initialization strategy of LoRA:\nFig. 31. An illustration of LoRA. (Image source: Hu et al. 2021)\nIn the standard forward pass, the model output is\n$$ h = W_0 x, $$With LoRA introduced, the output becomes\n$$ h = W_0 x + \\Delta W x = W_0 x + B A x. $$Where:\n$A \\in \\mathbb{R}^{r \\times k}$ (Down-projection matrix): Maps the input from $k$ dimensions to a lower $r$ dimension; $B \\in \\mathbb{R}^{d \\times r}$ (Up-projection matrix): Maps the reduced-dimension features from $r$ dimensions back to the original $d$ dimensions; Input $x$: Has dimension $\\mathbb{R}^{k}$; Original weight $W_0$: Has dimension $\\mathbb{R}^{d \\times k}$, thus $W_0 x \\in \\mathbb{R}^{d}$; Assuming the pre-trained weight matrix is $$ \\mathbf{W} \\in \\mathbb{R}^{d \\times k}, $$ LoRA adds a low-rank update term to it, resulting in the new weight representation:\n$$ \\mathbf{W}' = \\mathbf{W} + \\alpha\\, \\mathbf{B}\\mathbf{A}, $$Where:\n$\\mathbf{A} \\in \\mathbb{R}^{r \\times k}$ (Down-projection matrix): Maps the input from $k$ dimensions to a lower $r$ dimension; $\\mathbf{B} \\in \\mathbb{R}^{d \\times r}$ (Up-projection matrix): Maps the reduced-dimension features from $r$ dimensions back to the original $d$ dimensions; $r \\ll \\min(d, k)$ (Low rank dimension): Typically chosen from $4$ to $16$, balancing model expressiveness with minimizing added parameters; $\\alpha$ (Scaling factor): Used to scale the low-rank update $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$, compensating for the potentially small numerical magnitude resulting from the low-rank decomposition (often set to $\\alpha = 2 \\times r$, e.g., $\\alpha = 16$ when $r = 8$). During the fine-tuning process, the original weights $\\mathbf{W}$ are frozen, and only $\\mathbf{A}$ and $\\mathbf{B}$ are updated. This significantly reduces the number of trainable and storable parameters.\nTo ensure that the update term $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$ introduced at the beginning of fine-tuning has minimal impact on the original model, the following initialization strategies are commonly used:\nInitialization of the down-projection matrix $\\mathbf{A}$\nGaussian Initialization: Set $\\mathbf{A} \\sim \\mathcal{N}(0,\\sigma^2)$ (typically with a small $\\sigma$, e.g., 0.02) to ensure the initial update is small enough not to severely disrupt the model’s output. Kaiming (He) Initialization: Kaiming initialization is a weight initialization method designed specifically for deep networks, aiming to maintain stability of forward signals and backward gradients across layers. For LoRA, ensuring a small scale (or using an appropriate scaling factor $\\alpha$) can make the initial $\\Delta \\mathbf{W}$ close to zero. Initialization of the up-projection matrix $\\mathbf{B}$\nTypically, $\\mathbf{B}$ is initialized as a zero matrix, so that initially $\\mathbf{B}\\mathbf{A} = 0$, further minimizing the impact on the original model. Training with LoRA offers the following advantages:\nParameter Efficiency: Only introduces low-rank adapter parameters, reducing the total number of parameters that need to be trained and stored. Memory and Computation Efficiency: Freezes most pre-trained weights and updates only small-scale parameters during fine-tuning, significantly reducing memory footprint and computational overhead. No Additional Inference Latency: After training, the update term $\\Delta \\mathbf{W}$ can be merged back into the original weights ($\\mathbf{W}’ = \\mathbf{W} + \\alpha \\mathbf{B}\\mathbf{A}$), so no extra computation is added during the inference phase. Module Selection Flexibility: Using parameters like --lora_target or --lora-target, users can specify applying LoRA updates only to specific linear modules. Supported target modules include: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj. This design allows users to selectively fine-tune key modules based on specific task requirements, further enhancing fine-tuning efficiency and adaptability. QLoRA QLoRA (Dettmers et al. 2023) is a method for efficient fine-tuning of large-scale models based on LoRA combined with quantization ideas. Through the following three key improvements, it greatly reduces memory footprint while maintaining basically unchanged model accuracy:\n4-bit Normal Float (NF4) Quantization A block-based quantile quantization strategy is adopted to quantize the original model weights to 4 bits, thereby achieving significant storage compression with subtle loss of accuracy.\nDouble Quantization After performing quantization once on ordinary parameters, perform an additional quantization on the quantization constants to further reduce cache footprint.\nPaged Optimizer When memory usage is too high, automatically transfer part of the optimization process to CPU memory, thereby alleviating GPU memory pressure and improving scalability.\nDifferent from traditional LoRA, which only reduces the number of parameters to be fine-tuned, QLoRA also compresses all weights through 4-bit quantization, thereby maximizing the reduction of memory footprint and data transmission overhead while ensuring near-original accuracy.\nFig. 32. Different finetuning methods and their memory requirements. QLoRA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. (Image source: Dettmers et al. 2023)\nThis method can be regarded as a further extension of LoRA: LoRA improves efficiency by reducing the number of weights that need to be fine-tuned, while QLoRA, on this basis, quantizes all weights (including the un-fine-tuned part) to 4-bit precision, achieving dual compression of storage and computation in general, which is suitable for efficient fine-tuning of LLMs in resource-constrained environments.\nSummary Parallelism techniques and memory optimization strategies need to be weighed and selected according to the specific model structure, dataset size, hardware resources, and training goals. Usually, it is necessary to combine multiple technologies to effectively train large-scale models and achieve the best performance and efficiency.\nReferences [1] Weng, Lilian, and Greg Brockman. “Techniques for training large neural networks.” OpenAI Blog, 2022.\n[2] Shenggui Li, Siqi Mai. “Paradigms of Parallelism.” Colossal-AI Documentation, 2024.\n[3] Li, Shen, et al. “Pytorch distributed: Experiences on accelerating data parallel training.” arXiv preprint, 2020.\n[4] Li, Mu, et al. “Communication efficient distributed machine learning with the parameter server.” Advances in Neural Information Processing Systems 27, 2014.\n[5] Huang, Yanping, et al. “Gpipe: Efficient training of giant neural networks using pipeline parallelism.” Advances in Neural Information Processing Systems 32, 2019.\n[6] Harlap, Aaron, et al. “Pipedream: Fast and efficient pipeline parallel dnn training.” arXiv preprint, 2018.\n[7] Narayanan, Deepak, et al. “Memory-efficient pipeline-parallel dnn training.” International Conference on Machine Learning, PMLR, 2021.\n[8] Shoeybi, Mohammad, et al. “Megatron-lm: Training multi-billion parameter language models using model parallelism.” arXiv preprint, 2019.\n[9] Narayanan, Deepak, et al. “Efficient large-scale language model training on gpu clusters using megatron-lm.” Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021.\n[10] Shazeer, Noam, et al. “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.” arXiv preprint, 2017.\n[11] Lepikhin, Dmitry, et al. “Gshard: Scaling giant models with conditional computation and automatic sharding.” arXiv preprint, 2020.\n[12] Fedus, William, Barret Zoph, and Noam Shazeer. “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.” Journal of Machine Learning Research 23.120, 2022: 1–39.\n[13] Zhou, Yanqi, et al. “Mixture-of-experts with expert choice routing.” Advances in Neural Information Processing Systems 35, 2022: 7103–7114.\n[14] Li, Shenggui, et al. “Sequence parallelism: Long sequence training from system perspective.” arXiv preprint, 2021.\n[15] Korthikanti, Vijay Anand, et al. “Reducing activation recomputation in large transformer models.” Proceedings of Machine Learning and Systems 5, 2023: 341–353.\n[16] Jacobs, Sam Ade, et al. “Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.” arXiv preprint, 2023.\n[17] DeepSpeed. “DeepSpeed Ulysses README.” GitHub repository.\n[18] Rajbhandari, Samyam, et al. “Zero: Memory optimizations toward training trillion parameter models.” SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, 2020.\n[19] Microsoft Research. “DeepSpeed: Extreme-scale model training for everyone.” 2020.\n[20] Dubey, Abhimanyu, et al. “The llama 3 herd of models.” arXiv preprint, 2024.\n[21] Rhu, Minsoo, et al. “vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design.” 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture(MICRO), IEEE, 2016.\n[22] Chen, Tianqi, et al. “Training deep nets with sublinear memory cost.” arXiv preprint, 2016.\n[23] Micikevicius, Paulius, et al. “Mixed precision training.” arXiv preprint, 2017.\n[24] Jain, Animesh, et al. “Gist: Efficient data encoding for deep neural network training.” 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture(ISCA), IEEE, 2018.\n[25] Kingma, Diederik P., and Jimmy Ba. “Adam: A method for stochastic optimization.” arXiv preprint, 2014.\n[26] Shazeer, Noam, and Mitchell Stern. “Adafactor: Adaptive learning rates with sublinear memory cost.” International Conference on Machine Learning, PMLR, 2018.\n[27] Ginsburg, Boris, et al. “Stochastic gradient methods with layer-wise adaptive moments for training of deep networks.” arXiv preprint, 2019.\n[28] Hu, Edward J., et al. “LoRA: Low-rank adaptation of large language models.” ICLR, 2022: 3.\n[29] Dettmers, Tim, et al. “Qlora: Efficient finetuning of quantized llms.” Advances in Neural Information Processing Systems 36, 2023: 10088–10115.\n[30] Weng, Lilian. “How to Train Really Large Models on Many GPUs?” Lil’blog, 2021.\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui.(Mar 2025). Parallelism and Memory Optimization Techniques for Training Large Models. https://syhya.github.io/posts/2025-03-01-train-llm\nOr\n@article{syhya2025train-llm, title = \"Parallelism and Memory Optimization Techniques for Training Large Models\", author = \"Yue Shui\", journal = \"syhya.github.io\", year = \"2025\", month = \"Mar\", url = \"https://syhya.github.io/posts/2025-03-01-train-llm\" } ","wordCount":"12817","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-03-01T12:00:00+08:00","dateModified":"2025-03-01T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-03-01-train-llm/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Parallelism and Memory Optimization Techniques for Training Large Models</h1><div class=post-meta><span title='2025-03-01 12:00:00 +0800 +0800'>2025-03-01</span>&nbsp;·&nbsp;61 min&nbsp;·&nbsp;12817 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-03-01-train-llm/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#background>Background</a><ul><li><a href=#training-challenges-of-large-models>Training Challenges of Large Models</a></li><li><a href=#necessity-of-distributed-training>Necessity of Distributed Training</a></li></ul></li><li><a href=#parallelism-training>Parallelism Training</a></li><li><a href=#data-parallelism>Data Parallelism</a><ul><li><a href=#bulk-synchronous-parallelism-vs-asynchronous-parallelism>Bulk Synchronous Parallelism vs. Asynchronous Parallelism</a></li><li><a href=#bsp-vs-asp>BSP vs. ASP</a></li><li><a href=#gradient-accumulation>Gradient Accumulation</a></li><li><a href=#distributed-data-parallelism>Distributed Data Parallelism</a></li><li><a href=#ring-all-reduce>Ring All-Reduce</a></li><li><a href=#parameter-server>Parameter Server</a></li></ul></li><li><a href=#model-parallelism>Model Parallelism</a></li><li><a href=#pipeline-parallelism>Pipeline Parallelism</a><ul><li><a href=#gpipe>GPipe</a></li><li><a href=#gpipe-bubble-ratio-formula>GPipe Bubble Ratio Formula</a></li><li><a href=#pipedream>PipeDream</a></li><li><a href=#weight-stashing>Weight Stashing</a></li><li><a href=#pipedream-flush>PipeDream-flush</a></li><li><a href=#pipedream-2bw>PipeDream-2BW</a></li></ul></li><li><a href=#tensor-parallelism>Tensor Parallelism</a><ul><li><a href=#megatron-lm>Megatron-LM</a></li><li><a href=#ptd-p>PTD-P</a></li></ul></li><li><a href=#mixture-of-experts-model>Mixture-of-Experts Model</a><ul><li><a href=#core-components-of-moe>Core Components of MoE</a></li><li><a href=#noisy-top-k-gating>Noisy Top-k Gating</a></li><li><a href=#auxiliary-loss>Auxiliary Loss</a></li><li><a href=#gshard>GShard</a></li><li><a href=#switch-transformer>Switch Transformer</a></li><li><a href=#expert-choice>Expert Choice</a></li></ul></li><li><a href=#sequence-parallelism>Sequence Parallelism</a><ul><li><a href=#colossal-ai-sequence-parallelism>Colossal-AI Sequence Parallelism</a></li><li><a href=#megatron-lm-sequence-parallelism>Megatron-LM Sequence Parallelism</a></li><li><a href=#deepspeed-ulysses-sequence-parallelism>DeepSpeed-Ulysses Sequence Parallelism</a></li></ul></li><li><a href=#optimizer-related-parallelism-zero>Optimizer-Related Parallelism: ZeRO</a><ul><li><a href=#zero-sharding-strategy>ZeRO Sharding Strategy</a><ul><li><a href=#zero-1-optimizer-state-sharding>ZeRO-1 (Optimizer State Sharding)</a></li><li><a href=#zero-2-gradient-sharding>ZeRO-2 (Gradient Sharding)</a></li><li><a href=#zero-3-parameter-sharding>ZeRO-3 (Parameter Sharding)</a></li></ul></li><li><a href=#comparison-of-deepspeed-zero-sharding-and-offload-strategies>Comparison of DeepSpeed ZeRO Sharding and Offload Strategies</a></li><li><a href=#communication-volume-and-performance-impact>Communication Volume and Performance Impact</a></li></ul></li><li><a href=#multi-dimensional-parallelism>Multi-dimensional Parallelism</a><ul><li><a href=#3d-parallelism>3D Parallelism</a></li><li><a href=#4d-parallelism>4D Parallelism</a></li></ul></li><li><a href=#memory-optimization-techniques>Memory Optimization Techniques</a><ul><li><a href=#cpu-offloading>CPU Offloading</a></li><li><a href=#activation-recomputationgradient-checkpointing>Activation Recomputation/Gradient Checkpointing</a></li><li><a href=#mixed-precision-training>Mixed Precision Training</a></li><li><a href=#compression>Compression</a></li><li><a href=#memory-efficient-optimizers>Memory-Efficient Optimizers</a><ul><li><a href=#adafactor>Adafactor</a></li><li><a href=#sm3>SM3</a></li></ul></li><li><a href=#lora>LoRA</a></li><li><a href=#qlora>QLoRA</a></li></ul></li><li><a href=#summary>Summary</a></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>Recently, the number of parameters in large models has been continuously increasing, from the initial billions to today&rsquo;s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models.</p><h3 id=training-challenges-of-large-models>Training Challenges of Large Models<a hidden class=anchor aria-hidden=true href=#training-challenges-of-large-models>#</a></h3><ul><li><p><strong>Explosive Growth in Parameter Scale</strong>
With the continuous pursuit of model capacity and performance, the number of parameters in neural networks is growing exponentially. Today, models ranging from millions to billions, hundreds of billions, and even trillions of parameters are emerging. For example, Llama 3.1 405B has approximately 405 billion parameters, while it is rumored that GPT-4 may have as many as 1.7 trillion parameters. This massive parameter scale has led to a sharp increase in computing and memory demands, bringing unprecedented pressure to the training process.</p></li><li><p><strong>Soaring Computational Complexity</strong>
The rapid increase in the number of parameters directly leads to a significant increase in overall computational complexity. Training a large model once may take weeks or even months. Even with large-scale high-performance GPU clusters, the training cycle is still unsatisfactory, severely restricting model iteration speed and research efficiency.</p></li><li><p><strong>Increasingly Prominent Memory Bottleneck</strong>
In addition to storing massive model parameters, large models must also save intermediate activations, gradient information, and optimizer states during training. This data poses a huge challenge to GPU memory. Even with high-end GPUs equipped with A100, H100 (80GB memory), H200 (141GB memory), or GB200 (384GB memory), single-card memory is often insufficient to meet the needs of models with hundreds of billions or even trillions of parameters, leading to frequent &ldquo;Out of Memory (OOM)&rdquo; errors.</p></li><li><p><strong>Communication Overhead Becomes a Bottleneck</strong>
In multi-GPU distributed training environments, inter-node communication is frequently required for data synchronization (such as gradient aggregation). As the model size and the number of GPUs increase, this communication volume rises sharply. Even in high-bandwidth networks, All-Reduce operations to transmit massive amounts of data consume a significant amount of time, becoming one of the main bottlenecks of overall parallel efficiency.</p></li><li><p><strong>Training Stability Challenges</strong>
Ultra-large-scale models are more prone to gradient vanishing or gradient explosion problems during training, leading to unstable training processes and difficulty in convergence. Although mixed-precision training can accelerate training and reduce memory footprint to some extent, it may also introduce new numerical stability issues, requiring researchers to invest more effort in detailed tuning.</p></li></ul><h3 id=necessity-of-distributed-training>Necessity of Distributed Training<a hidden class=anchor aria-hidden=true href=#necessity-of-distributed-training>#</a></h3><p>Faced with the above challenges, distributed training technology has become a key solution to support the training of large models. By splitting training tasks and distributing them to multiple GPUs or computing nodes, distributed training can fully utilize parallel computing and cluster memory resources, thereby breaking through the limitations of a single GPU. The main advantages are reflected in the following aspects:</p><ul><li><p><strong>Breaking Through the Computing Power Limit of a Single GPU</strong>
The computing power of a single GPU is ultimately limited and cannot cope with the massive computing demands of trillion-parameter models. With data parallelism and model parallelism techniques, training tasks can be evenly distributed to multiple GPUs, thereby significantly shortening the overall training time.</p></li><li><p><strong>Overcoming the Memory Bottleneck of a Single GPU</strong>
By distributing model parameters, intermediate activations, and optimizer states across the memory of multiple GPUs, distributed training effectively expands the available memory capacity. Typical technologies such as ZeRO, through sharding data processing, make the training of ultra-large-scale models possible.</p></li><li><p><strong>Accelerating Model Iteration and R&amp;D Cycle</strong>
The high parallelism of distributed training makes it possible to complete training tasks that originally required weeks or even months in just a few days, thereby greatly improving the model iteration speed and enabling new architectures and strategies to be verified and applied more quickly.</p></li><li><p><strong>Supporting Exploration of Larger-Scale Models</strong>
Distributed training provides a solid foundation for exploring larger-scale and more complex neural network architectures. It is with this technical support that trillion-parameter models (such as Switch Transformer) can be successfully trained and put into practical applications.</p></li><li><p><strong>Improving the Robustness and Scalability of Training Systems</strong>
Distributed systems have excellent fault tolerance. When a GPU node fails, other nodes can quickly take over the task, ensuring that the training process is not interrupted. At the same time, the cluster size can be flexibly expanded or reduced according to specific needs, meeting the training requirements of different scale models.</p></li></ul><h2 id=parallelism-training>Parallelism Training<a hidden class=anchor aria-hidden=true href=#parallelism-training>#</a></h2><p>The following figure intuitively shows the differences between various parallel training strategies. Different colors represent different model layers (e.g., three layers), and dashed lines distinguish different GPUs. From left to right are data parallelism, model parallelism (including pipeline parallelism and tensor parallelism), and expert parallelism (MoE).</p><figure class=align-center><img loading=lazy src=parallelism_compare.png#center alt="Fig. 1. An illustration of various parallelism strategies on a three-layer model. Each color refers to one layer and dashed lines separate different GPUs. (Image source: OpenAI Blog, 2022)" width=90%><figcaption><p>Fig. 1. An illustration of various parallelism strategies on a three-layer model. Each color refers to one layer and dashed lines separate different GPUs. (Image source: <a href=https://openai.com/index/techniques-for-training-large-neural-networks/>OpenAI Blog, 2022</a>)</p></figcaption></figure><ul><li><p><strong>Data Parallelism</strong>
The complete model is copied to each GPU, and the dataset is divided into different batches and distributed to each GPU for parallel computation. Finally, the gradients of all GPUs are aggregated during parameter updates.</p></li><li><p><strong>Model Parallelism</strong>
The model is divided and distributed across different GPUs, with each GPU responsible for computing only a part of the model. It can be further divided into the following two categories:</p><ul><li><strong>Pipeline Parallelism</strong>: The model is split layer-wise (vertically), with different GPUs responsible for different layers. Micro-batches are passed through the pipeline to execute forward and backward computations in parallel.</li><li><strong>Tensor Parallelism</strong>: Large-scale tensor operations (such as large matrix multiplications) within a layer are split horizontally. Each GPU performs part of the computation in parallel and aggregates the results when necessary.</li></ul></li><li><p><strong>Expert Parallelism</strong>
Through a gating strategy, each input sample only passes through a subset of experts (sub-networks), thus distributing the entire model across different GPUs by &ldquo;expert modules&rdquo;. Commonly used in Mixture-of-Experts (MOE) structures, it can achieve ultra-large parameter scales but only activate a portion of experts during inference/training.</p></li></ul><p>Below, I will elaborate on various parallel methods.</p><h2 id=data-parallelism>Data Parallelism<a hidden class=anchor aria-hidden=true href=#data-parallelism>#</a></h2><figure class=align-center><img loading=lazy src=data_parallelism.png#center alt="Fig. 2. Data Parallelism. (Image source: Clolossal-AI Documentation)" width=60%><figcaption><p>Fig. 2. Data Parallelism. (Image source: <a href=https://colossalai.org/docs/concepts/paradigms_of_parallelism>Clolossal-AI Documentation</a>)</p></figcaption></figure><p>In deep learning training, <strong>Data Parallelism (DP)</strong> is the most commonly used parallel strategy. Its core idea is:</p><ol><li><strong>Replicate Model Parameters</strong>: Place a complete copy of the model parameters on each computing device (usually a GPU).</li><li><strong>Partition Training Data</strong>: Divide the large-scale dataset into multiple subsets along the sample dimension. Different subsets are assigned to different GPUs for processing.</li><li><strong>Local Forward and Backward Propagation</strong>: Each GPU independently computes the loss and corresponding local gradients.</li><li><strong>Gradient/Parameter Synchronization</strong>: Aggregate the gradients from each GPU and update the model parameters, ensuring that the model replicas on all GPUs remain consistent after each iteration.</li></ol><p>The following shows the <strong>data parallelism</strong> workflow:</p><ol><li><p><strong>Dataset Partitioning</strong>
Divide the training dataset $D$ into $N$ non-overlapping subsets ${D_1, D_2, \dots, D_N}$, where $N$ is the number of GPUs. Usually, it is ensured that the size of each subset is similar to achieve load balancing.</p></li><li><p><strong>Model Replication</strong>
Replicate a complete copy of the model parameters $\theta$ on each GPU. At the beginning of training, these parameters are the same on each GPU.</p></li><li><p><strong>Data Distribution</strong>
Distribute subset $D_i$ to the $i$-th GPU, allowing it to be stored locally and used for subsequent calculations.</p></li><li><p><strong>Local Forward Propagation</strong>
Each GPU performs forward propagation based on its local data subset $D_i$ to obtain the local loss $L_i(\theta, D_i)$.</p></li><li><p><strong>Local Backward Propagation</strong>
Each GPU performs backward propagation based on the local loss $L_i$ to calculate the local gradient</p>$$
g_i = \nabla_{\theta} L_i(\theta, D_i).
$$</li><li><p><strong>Gradient Synchronization</strong>
Gradient synchronization (usually All-Reduce) is performed between GPUs to aggregate all local gradients ${g_1, g_2, \ldots, g_N}$ to obtain the global average gradient</p>$$
\bar{g} = \frac{1}{N} \sum_{i=1}^{N} g_i.
$$</li><li><p><strong>Parameter Update</strong>
Each GPU uses the global average gradient $\bar{g}$ to update its local model parameters:</p>$$
\theta \leftarrow \theta - \eta \cdot \bar{g},
$$<p>where $\eta$ is the learning rate.</p></li><li><p><strong>Iterative Loop</strong>
Repeat steps 4-7 until the model converges or reaches the preset number of training epochs.</p></li></ol><h3 id=bulk-synchronous-parallelism-vs-asynchronous-parallelism>Bulk Synchronous Parallelism vs. Asynchronous Parallelism<a hidden class=anchor aria-hidden=true href=#bulk-synchronous-parallelism-vs-asynchronous-parallelism>#</a></h3><p>In step 6 &ldquo;Gradient Synchronization&rdquo; above, how and when to perform &ldquo;synchronization&rdquo; is one of the important factors affecting the performance and convergence behavior of data parallelism. It is generally divided into the following two categories:</p><p><strong>Bulk Synchronous Parallelism (BSP)</strong> is the most common and easiest to understand synchronization mode in data parallelism. Its characteristics can be summarized as &ldquo;globally synchronizing gradients and updating parameters once after each mini-batch iteration&rdquo;. The specific process is:</p><ol><li><strong>Local Computation</strong>: Each GPU performs forward and backward propagation based on its data subset $D_i$ to obtain the local gradient $g_i$.</li><li><strong>Global Communication</strong>: All GPUs synchronize (e.g., through All-Reduce) to calculate $\bar{g}$.</li><li><strong>Parameter Update</strong>: Each node uses $\bar{g}$ to update its local parameter replica $\theta$.</li><li><strong>Wait and Next Iteration</strong>: All nodes complete the above operations before entering the next iteration.</li></ol><p><strong>Asynchronous Parallelism (ASP)</strong> aims to get rid of the global synchronization point of BSP and allow each node to perform calculations and parameter updates independently. Its typical implementation is the <strong>asynchronous push-pull</strong> process under the &ldquo;Parameter Server (PS)&rdquo; architecture:</p><ol><li>Each node calculates the gradient $g_i$ locally, and then <strong>pushes</strong> it to the parameter server;</li><li>Once the parameter server receives the gradient, it immediately updates the global model parameters;</li><li>Other nodes will <strong>pull</strong> down the latest parameters when they need them to continue the next step of calculation.</li></ol><h3 id=bsp-vs-asp>BSP vs. ASP<a hidden class=anchor aria-hidden=true href=#bsp-vs-asp>#</a></h3><p>The following table summarizes the main differences between synchronous and asynchronous parallelism in a data parallel environment:</p><table><thead><tr><th style=text-align:left><strong>Comparison Dimension</strong></th><th style=text-align:left><strong>Synchronous Parallelism (BSP)</strong></th><th style=text-align:left><strong>Asynchronous Parallelism (ASP)</strong></th></tr></thead><tbody><tr><td style=text-align:left><strong>Parameter Update Timing</strong></td><td style=text-align:left>Global synchronization once per mini-batch or after a certain number of iterations</td><td style=text-align:left>Each node updates parameters independently, without needing to keep the same timestep as others</td></tr><tr><td style=text-align:left><strong>Convergence Stability</strong></td><td style=text-align:left><strong>High</strong>. The gradients used are the latest, the convergence path is controllable and easy to analyze</td><td style=text-align:left><strong>Lower</strong>. Stale gradients exist, convergence rate and stability may be affected</td></tr><tr><td style=text-align:left><strong>Communication Requirements</strong></td><td style=text-align:left>Highly dependent on All-Reduce, all nodes need to wait and exchange data during synchronization</td><td style=text-align:left>Each node asynchronously pushes/pulls to the parameter server, communication is more flexible, but the parameter server may become a bottleneck</td></tr><tr><td style=text-align:left><strong>Hardware Resource Utilization</strong></td><td style=text-align:left>If there are slow nodes or network delays, other nodes need to wait, and resource utilization may be reduced</td><td style=text-align:left>No need to wait for slow nodes, computing resources can be used efficiently</td></tr><tr><td style=text-align:left><strong>Implementation Complexity</strong></td><td style=text-align:left>Relatively low, mainstream frameworks (PyTorch DDP, Horovod, etc.) have built-in support</td><td style=text-align:left>Relatively higher, parameter server and other components are required, more synchronization logic and data consistency need to be handled</td></tr><tr><td style=text-align:left><strong>Applicable Scenarios</strong></td><td style=text-align:left>Homogeneous hardware, good network bandwidth, pursuit of higher convergence quality</td><td style=text-align:left>Heterogeneous hardware, unstable or low bandwidth network, need for extremely high throughput and tolerance for certain convergence risks</td></tr><tr><td style=text-align:left><strong>Typical Implementations</strong></td><td style=text-align:left>PyTorch DDP, TensorFlow MirroredStrategy</td><td style=text-align:left>Parameter Server architecture (MXNet, TensorFlow ParameterServer mode, etc.)</td></tr></tbody></table><blockquote><p><strong>Recommendation</strong>: In actual projects, start with simple synchronous parallelism (BSP), and use PyTorch DDP or similar tools for multi-GPU training. If the network environment is heterogeneous, there are many nodes, or the task requires extremely high throughput, you can try asynchronous parallelism (ASP) or parameter server solutions, and cooperate with Gradient Accumulation to balance bandwidth and update frequency.</p></blockquote><h3 id=gradient-accumulation>Gradient Accumulation<a hidden class=anchor aria-hidden=true href=#gradient-accumulation>#</a></h3><p>When the batch size is large or communication becomes the main bottleneck, <strong>Gradient Accumulation</strong> can be used to reduce the synchronization frequency. Its core idea is:</p><ul><li>Continuously calculate the local gradients of multiple mini-batches and accumulate them in the local accumulation buffer;</li><li>When the number of accumulated mini-batches reaches $K$, trigger a global gradient synchronization and parameter update.</li></ul><p>Let $g_j$ be the gradient of the $j$-th mini-batch, then in an &ldquo;accumulation cycle&rdquo;, we get</p>$$
G = \sum_{j=1}^{K} g_j.
$$<p>Then update with learning rate $\eta$:</p>$$
\theta \leftarrow \theta - \eta \cdot G.
$$<p>Since gradient synchronization is no longer performed for each mini-batch, but once every $K$ accumulated mini-batches, the communication overhead can be significantly reduced. However, the reduced parameter update frequency may also slow down the training convergence speed, and a trade-off between throughput and convergence performance is needed.</p><h3 id=distributed-data-parallelism>Distributed Data Parallelism<a hidden class=anchor aria-hidden=true href=#distributed-data-parallelism>#</a></h3><p><strong>Distributed Data Parallelism (DDP)</strong> is a highly optimized implementation of BSP in PyTorch v1.5 (<a href=https://arxiv.org/pdf/2006.15704>Li et al. 2020</a>), which facilitates data parallelism for single-machine multi-GPU and even multi-machine multi-GPU. Its main optimizations include:</p><ol><li><strong>Gradient Bucketing</strong>: Divide model parameters into multiple &ldquo;buckets&rdquo;; when backpropagation is performed, once all gradients in a bucket are calculated, an <strong>All-Reduce for that bucket</strong> is immediately initiated, instead of waiting for all gradients to be calculated before synchronizing at once.</li><li><strong>Communication and Computation Overlap</strong>: DDP uses asynchronous communication and non-blocking operations to overlap gradient synchronization (communication) with forward propagation and backward propagation (computation) as much as possible, thereby reducing communication overhead. This overlap strategy improves overall parallel efficiency.</li><li><strong>Gradient Accumulation</strong>: DDP can also be easily combined with <strong>gradient accumulation</strong>. Combined use, by increasing the gradient update interval for each synchronization, reduces the synchronization frequency. In large-scale distributed training, this helps to further reduce communication overhead and improve training efficiency.</li></ol><figure class=align-center><img loading=lazy src=pytorch_ddp.png#center alt="Fig. 3. Pseudo code for Pytorch DDP. (Image source: Li et al. 2020)" width=80%><figcaption><p>Fig. 3. Pseudo code for Pytorch DDP. (Image source: <a href=https://arxiv.org/pdf/2006.15704>Li et al. 2020</a>)</p></figcaption></figure><h3 id=ring-all-reduce>Ring All-Reduce<a hidden class=anchor aria-hidden=true href=#ring-all-reduce>#</a></h3><p>In a multi-GPU (especially single-machine multi-GPU) environment, if there is high-speed interconnect (such as NVLink, PCIe switch, etc.), <strong>Ring All-Reduce</strong> can be used to significantly reduce communication overhead. The idea is:</p><ol><li>Organize $k$ nodes into a ring and divide the gradient vector into $k$ parts equally.</li><li>In the &ldquo;summation phase&rdquo;, each node sends a part of its local gradient to the next node and adds it to the received gradient; after several rounds of this process, each node will hold the complete &ldquo;aggregated&rdquo; gradient.</li><li>In the &ldquo;broadcast phase&rdquo;, the final result is distributed to all nodes along the ring.</li></ol><p>Ideally, the communication cost of Ring All-Reduce is approximately independent of the number of nodes (can be regarded as $\mathcal{O}(1)$), which is very suitable for gradient synchronization in a multi-GPU environment. It is a core communication mode widely used in libraries such as Horovod and NCCL.</p><h3 id=parameter-server>Parameter Server<a hidden class=anchor aria-hidden=true href=#parameter-server>#</a></h3><p>When the cluster scale expands to multi-machine multi-GPU, simple single-point aggregation (such as a central server) is often difficult to support parallel training of massive data. <strong>Parameter Server (PS)</strong> (<a href=https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf>Li, et al. 2014</a>) is a typical architecture designed for scalable distributed training:</p><ol><li><strong>Parameter Sharding</strong>: Split model parameters in the form of key-value pairs. Different PS nodes only manage parameters of specific shards.</li><li><strong>push-pull Semantics</strong>: After the computing node obtains the gradient locally, it <strong>pushes</strong> it to the corresponding PS; after the PS updates the parameters of the shard, the computing node can <strong>pull</strong> down the latest version when needed for the next step of calculation.</li><li><strong>Flexible Fault Tolerance and Expansion</strong>: By adding or removing PS nodes, capacity can be flexibly expanded in terms of bandwidth or computing needs; backup and fault tolerance strategies can also be implemented on PS.</li></ol><p>This <strong>PS + Worker</strong> mode can combine data parallelism and model parallelism <strong>simultaneously</strong>, splitting ultra-large models and storing them on multiple PSs, and performing distributed training on ultra-large data. PS itself can also be split and merged according to the load situation to form a more complex hierarchical topology.</p><h2 id=model-parallelism>Model Parallelism<a hidden class=anchor aria-hidden=true href=#model-parallelism>#</a></h2><p><strong>Model Parallelism (MP)</strong> is a parallel method that splits the model itself across multiple computing devices (GPUs) for training. When the model parameter size exceeds the memory capacity of a single GPU, model parallelism becomes a necessary choice. Model parallelism is mainly divided into two types: Pipeline Parallelism and Tensor Parallelism.</p><p><strong>Naive Model Parallelism and Bubble Problem</strong></p><figure class=align-center><img loading=lazy src=naive_mp.png#center alt="Fig. 4. A naive model parallelism setup where the model is vertically split into 4 partitions. Data is processed by one worker at a time due to sequential dependency, leading to large “bubbles” of idle time. (Image source: Huang et al. 2018)" width=100%><figcaption><p>Fig. 4. A naive model parallelism setup where the model is vertically split into 4 partitions. Data is processed by one worker at a time due to sequential dependency, leading to large “bubbles” of idle time. (Image source: <a href=https://arxiv.org/abs/1811.06965>Huang et al. 2018</a>)</p></figcaption></figure><p>Naive model parallelism implementation, which simply divides the model layer by layer and executes it sequentially on different GPUs, will encounter a serious &ldquo;bubble&rdquo; problem. Due to the dependencies between layers, when one GPU is processing a certain stage of a data sample, other GPUs may be idle, waiting for the output of the previous GPU or the input of the next GPU. This GPU idle time is called &ldquo;bubble&rdquo;, which seriously reduces the efficiency of pipeline parallelism.</p><p>Where $F_i$ represents the forward propagation of Stage $i$, and $B_i$ represents the backward propagation of Stage $i$. It can be seen that in naive pipeline parallelism, only one GPU is working most of the time, and other GPUs are idle, resulting in low efficiency.</p><p><strong>Reasons for the bubble problem:</strong></p><ul><li><strong>Inter-layer dependency:</strong> There is a sequential dependency between the layers of the neural network. The calculation of the next layer must depend on the output of the previous layer.</li><li><strong>Sequential execution:</strong> Naive model parallelism executes layer by layer in order, which prevents GPUs from working in full parallelism.</li></ul><h2 id=pipeline-parallelism>Pipeline Parallelism<a hidden class=anchor aria-hidden=true href=#pipeline-parallelism>#</a></h2><figure class=align-center><img loading=lazy src=pipeline_parallelism.png#center alt="Fig. 5. Pipeline Parallelism. (Image source: Clolossal-AI Documentation)" width=60%><figcaption><p>Fig. 5. Pipeline Parallelism. (Image source: <a href=https://colossalai.org/docs/concepts/paradigms_of_parallelism>Clolossal-AI Documentation</a>)</p></figcaption></figure><p><strong>Pipeline Parallelism (PP)</strong> divides the model layer by layer into multiple stages, and each stage is assigned to a GPU. Data is passed between different GPUs like a pipeline. The output of the previous GPU serves as the input of the next GPU. Pipeline parallelism aims to improve the efficiency of model parallel training and reduce GPU idle time.</p><h3 id=gpipe>GPipe<a hidden class=anchor aria-hidden=true href=#gpipe>#</a></h3><p>GPipe (<a href=https://arxiv.org/abs/1811.06965>Huang et al. 2018</a>) is an efficient pipeline parallel training system proposed by Google, which aims to solve the bubble problem of naive pipeline parallelism. The core idea of GPipe is to divide a <strong>mini-batch</strong> into multiple <strong>micro-batches</strong> and use <strong>synchronous gradient aggregation</strong> to alleviate the bubble problem and improve pipeline efficiency.</p><figure class=align-center><img loading=lazy src=gpipe.png#center alt="Fig. 6. Illustration of pipeline parallelism in GPipe with 4 microbatches and 4 partitions. GPipe aggregates and updates gradients across devices synchronously at the end of every batch. (Image source: Huang et al. 2018)" width=100%><figcaption><p>Fig. 6. Illustration of pipeline parallelism in GPipe with 4 microbatches and 4 partitions. GPipe aggregates and updates gradients across devices synchronously at the end of every batch. (Image source: <a href=https://arxiv.org/abs/1811.06965>Huang et al. 2018</a>)</p></figcaption></figure><p>The following is the GPipe scheduling strategy:</p><ol><li><strong>Micro-batch Partitioning:</strong> Divide a mini-batch into $m$ micro-batches. The size of each micro-batch after partitioning is $1/m$ of the original mini-batch.</li><li><strong>Pipeline Stage Partitioning:</strong> Divide the model layer by layer into $d$ stages, and assign each stage to a GPU.</li><li><strong>Pipeline Execution:</strong> Process each micro-batch in sequence, performing forward and backward propagation in the pipeline. The specific process is as follows:<ul><li><strong>Forward Propagation:</strong> For each micro-batch, perform forward propagation sequentially on Stage 1, Stage 2, &mldr;, Stage $d$. The output of Stage $i$ serves as the input of Stage $i+1$.</li><li><strong>Backward Propagation:</strong> When the forward propagation of all micro-batches is completed, backward propagation begins. For each micro-batch, perform backward propagation sequentially on Stage $d$, Stage $d-1$, &mldr;, Stage $1$. The gradient of Stage $i$ serves as the input of Stage $i-1$.</li></ul></li><li><strong>Synchronous Gradient Aggregation:</strong> After the backward propagation of all micro-batches is completed, aggregate the gradients of all micro-batches (e.g., averaging) to obtain the global average gradient.</li><li><strong>Parameter Update:</strong> Each GPU uses the global average gradient to update its local model parameters.</li></ol><h3 id=gpipe-bubble-ratio-formula>GPipe Bubble Ratio Formula<a hidden class=anchor aria-hidden=true href=#gpipe-bubble-ratio-formula>#</a></h3><p>Assuming that the forward and backward propagation time of each micro-batch is 1 unit, the pipeline depth is $d$, and the number of micro-batches is $m$, the bubble ratio of GPipe is:</p>$$
\text{Bubble Ratio} = 1 - \frac{2md}{(2m + 2(d-1))d} = \frac{d-1}{m+d-1}
$$<p>When the number of micro-batches $m$ is much larger than the pipeline depth $d$ ($m \gg d$), the bubble ratio approaches 0, and the pipeline efficiency is close to linear acceleration. The GPipe paper points out that when $m > 4d$, the bubble overhead can be almost ignored (in the case of activation recomputation). Therefore, there are the following benefits:</p><ul><li><strong>Reduce Bubbles:</strong> GPipe significantly reduces the bubble problem of naive pipeline parallelism through micro-batch partitioning and pipeline scheduling, improving GPU utilization and training efficiency.</li><li><strong>Synchronous Gradient Aggregation:</strong> GPipe adopts synchronous gradient aggregation, which ensures the synchronicity of the training process and good model convergence.</li><li><strong>Linear Acceleration Potential:</strong> When the number of micro-batches is large enough, GPipe can achieve near-linear acceleration.</li></ul><h3 id=pipedream>PipeDream<a hidden class=anchor aria-hidden=true href=#pipedream>#</a></h3><figure class=align-center><img loading=lazy src=pipe_dream.png#center alt="Fig. 7. Illustration of 1F1B microbatch scheduling in PipeDream. (Image source: Harlap et al. 2018)" width=100%><figcaption><p>Fig. 7. Illustration of 1F1B microbatch scheduling in PipeDream. (Image source: <a href=https://arxiv.org/abs/1806.03377>Harlap et al. 2018</a>)</p></figcaption></figure><p>PipeDream (<a href=https://arxiv.org/abs/1806.03377>Harlap et al. 2018</a>) is another efficient pipeline parallel training system. It adopts the 1F1B (1-Forward-1-Backward) scheduling strategy and introduces Weight Stashing technology to further reduce bubbles, improve pipeline efficiency, and solve the weight version inconsistency problem that may be caused by 1F1B scheduling.</p><p>The core idea of PipeDream&rsquo;s 1F1B scheduling strategy is that each GPU (Stage) alternately performs forward propagation and backward propagation, working in parallel as much as possible to reduce GPU idle time. The specific process is as follows:</p><ol><li><strong>Micro-batch Partitioning:</strong> Divide a mini-batch into $m$ micro-batches.</li><li><strong>Pipeline Stage Partitioning:</strong> Divide the model layer by layer into $d$ stages, and assign each stage to a GPU.</li><li><strong>1F1B Scheduling Execution:</strong> Each GPU takes turns to perform forward propagation and backward propagation.</li></ol><h3 id=weight-stashing>Weight Stashing<a hidden class=anchor aria-hidden=true href=#weight-stashing>#</a></h3><p>Since forward propagation and backward propagation may use different versions of model weights in 1F1B scheduling, it will cause weight version inconsistency problems, affecting the correctness and convergence of training. PipeDream introduces Weight Stashing technology to solve this problem. The core idea of weight stashing is that each GPU maintains multiple versions of model weights and ensures that forward propagation and backward propagation use the same version of weights.</p><p><strong>Weight Stashing Implementation:</strong></p><ul><li><strong>Version Management:</strong> Each GPU maintains a weight version queue to store multiple versions of model weights.</li><li><strong>Version Selection:</strong> When performing forward propagation, select the latest weight version. When performing backward propagation, select the same weight version as the corresponding forward propagation.</li><li><strong>Version Update:</strong> After completing backward propagation of all micro-batches in a mini-batch, update the model weights and generate a new weight version.</li></ul><blockquote><p>To further optimize the memory usage of PipeDream, especially in terms of weight stashing, PipeDream has derived two memory optimization variants: PipeDream-flush and PipeDream-2BW.</p></blockquote><h3 id=pipedream-flush>PipeDream-flush<a hidden class=anchor aria-hidden=true href=#pipedream-flush>#</a></h3><figure class=align-center><img loading=lazy src=pipe_dream_flush.png#center alt="Fig. 8. Illustration of pipeline scheduling in PipeDream-flush. (Image source: Narayanan et al. 2020)" width=100%><figcaption><p>Fig. 8. Illustration of pipeline scheduling in PipeDream-flush. (Image source: <a href=https://arxiv.org/abs/2006.09503>Narayanan et al. 2020</a>)</p></figcaption></figure><p>PipeDream-flush periodically performs global synchronous pipeline flushing on the basis of PipeDream, similar to GPipe&rsquo;s synchronous gradient aggregation. By periodically flushing, PipeDream-flush can greatly reduce the memory space required for weight stashing, only needing to maintain a single version of model weights, but it will sacrifice a small amount of throughput.</p><h3 id=pipedream-2bw>PipeDream-2BW<a hidden class=anchor aria-hidden=true href=#pipedream-2bw>#</a></h3><p>PipeDream-2BW (Double-Buffered Weights) maintains two versions of model weights, namely &ldquo;double-buffered weights&rdquo;. It updates the model version every $k$ micro-batches, where $k$ is greater than the pipeline depth $d$ ($k > d$). The newly updated model version does not immediately completely replace the old version, because there may still be some remaining backward propagation operations that depend on the old version. With double-buffered weights, PipeDream-2BW can reduce the memory overhead of weight stashing to only maintaining two versions of model weights, significantly reducing memory footprint.</p><figure class=align-center><img loading=lazy src=pipe_dream_2bw.png#center alt="Fig. 9. Illustration of pipeline scheduling in PipeDream-2BW. (Image source: Narayanan et al. 2020)" width=100%><figcaption><p>Fig. 9. Illustration of pipeline scheduling in PipeDream-2BW. (Image source: <a href=https://arxiv.org/abs/2006.09503>Narayanan et al. 2020</a>)</p></figcaption></figure><p>The PipeDream-2BW strategy has the following advantages:</p><ul><li><strong>Lower Bubble Overhead:</strong> The 1F1B scheduling strategy can further reduce bubbles compared to GPipe, improving GPU utilization and training efficiency.</li><li><strong>Weight Stashing Solves Version Consistency:</strong> Weight stashing technology ensures that forward propagation and backward propagation use the same version of weights, solving the weight version inconsistency problem that may be caused by 1F1B scheduling.</li><li><strong>Memory Optimization Variants:</strong> PipeDream-flush and PipeDream-2BW further optimize memory usage, reduce the memory overhead of weight stashing, and make pipeline parallelism more suitable for memory-constrained scenarios.</li></ul><h2 id=tensor-parallelism>Tensor Parallelism<a hidden class=anchor aria-hidden=true href=#tensor-parallelism>#</a></h2><p><strong>Tensor Parallelism (TP)</strong> is a parallel method that splits tensors in the model (usually weight matrices) along specific dimensions and distributes the split shards to different GPUs for computation. Tensor parallelism has the following advantages:</p><ul><li><strong>Breaking Through Single GPU Memory Limits:</strong> Tensor parallelism can distribute model parameters across multiple GPUs, breaking through the memory capacity limit of a single GPU and supporting the training of larger-scale models.</li><li><strong>Intra-layer Parallelism:</strong> Tensor parallelism can achieve parallelization within model layers, such as parallel computation of matrix multiplication operations, improving computational efficiency.</li><li><strong>Combination with Data Parallelism and Pipeline Parallelism:</strong> Tensor parallelism can be combined with other parallel technologies such as data parallelism and pipeline parallelism to form multi-dimensional parallel strategies, further improving training efficiency and scalability.</li></ul><h3 id=megatron-lm>Megatron-LM<a hidden class=anchor aria-hidden=true href=#megatron-lm>#</a></h3><p>Megatron-LM (<a href=https://arxiv.org/abs/1909.08053>Shoeybi et al. 2019</a>) is a system proposed by NVIDIA for training ultra-large language models. It adopts tensor parallelism technology to parallelize matrix multiplication operations within Transformer model layers, including matrix multiplications in <strong>self-attention</strong> and <strong>MLP</strong>.</p><figure class=align-center><img loading=lazy src=Megatron-LM.png#center alt="Fig. 10. Illustration of tensor parallelism for key transformer components proposed in Megatron-LM. (Image source: Shoeybi et al. 2019)" width=100%><figcaption><p>Fig. 10. Illustration of tensor parallelism for key transformer components proposed in Megatron-LM. (Image source: <a href=https://arxiv.org/abs/1909.08053>Shoeybi et al. 2019</a>)</p></figcaption></figure><p>The MLP layer of Transformer usually contains two linear layers. The calculation of the first linear layer can be expressed as $Y = \text{GeLU}(XA)$, where $X$ is the input matrix, $A$ is the weight matrix, and GeLU is the activation function. Megatron-LM splits the weight matrix $A$ along the column dimension into $P$ shards $[A_1, A_2, &mldr;, A_P]$, where $P$ is the number of GPUs. Each GPU $i$ is responsible for storing and computing the weight shard $A_i$.</p><p><strong>Tensor Parallelism Computation Process of MLP Layer:</strong></p>$$
\begin{aligned}
\text { Split } A & =\left[A_1, A_2\right] \\
Y & =\operatorname{GeLU}(X A) \\
{\left[Y_1, Y_2\right] } & =\left[\operatorname{GeLU}\left(X A_1\right), \operatorname{GeLU}\left(X A_2\right)\right]
\end{aligned}
$$<ol><li><strong>Weight Sharding:</strong> Split the weight matrix $A$ along the column dimension into $P$ shards $[A_1, A_2, &mldr;, A_P]$ and assign shard $A_i$ to GPU $i$.</li><li><strong>Local Matrix Multiplication:</strong> Each GPU $i$ uses the input matrix $X$ and weight shard $A_i$ to perform matrix multiplication to obtain the local output $Y_i = \text{GeLU}(XA_i)$.</li><li><strong>Global Concatenation (All-Gather):</strong> All GPUs use All-Gather operation to concatenate the local outputs ${Y_1, Y_2, &mldr;, Y_P}$ into a complete output matrix $Y = [Y_1, Y_2, &mldr;, Y_P]$.</li></ol><p><strong>Tensor Parallelism of Self-Attention Layer</strong></p><p>Megatron-LM also performs tensor parallel sharding on the Query ($Q$), Key ($K$), Value ($V$) weight matrices in the Transformer&rsquo;s self-attention layer, and performs corresponding local matrix multiplication and global concatenation operations to achieve tensor parallelism of the self-attention layer. The calculation formula of the self-attention layer is:</p>$$
\text{Attention}(X, Q, K, V) = \text{softmax}\left(\frac{(XQ)(XK)^T}{\sqrt{d_k}}\right)XV
$$<h3 id=ptd-p>PTD-P<a hidden class=anchor aria-hidden=true href=#ptd-p>#</a></h3><p>PTD-P (Pipeline, Tensor, and Data Parallelism) (<a href=https://arxiv.org/abs/2104.04473>Narayanan et al. 2021</a>) is a multi-dimensional parallel strategy that combines pipeline parallelism, tensor parallelism, and data parallelism. PTD-P aims to fully utilize the advantages of various parallel technologies to improve the efficiency and scalability of training ultra-large models.</p><p><strong>Features of PTD-P:</strong></p><ul><li><strong>Multi-dimensional Parallelism Combination:</strong> PTD-P uses pipeline parallelism, tensor parallelism, and data parallelism simultaneously, which can parallelize the training process from multiple dimensions.</li><li><strong>Interleaved 1F1B Scheduling:</strong> PTD-P adopts the interleaved 1F1B scheduling strategy. Unlike traditional pipeline parallelism, it divides the model into multiple discontinuous layer blocks (model chunks) and assigns multiple layer blocks to each GPU. This scheduling strategy can further reduce bubbles and improve pipeline efficiency.</li><li><strong>Flexible Parallelism Configuration:</strong> PTD-P allows users to flexibly configure the combination of various parallel technologies according to the model structure and hardware resources. For example, tensor parallelism and data parallelism can be used alone, or pipeline parallelism, tensor parallelism, and data parallelism can be used simultaneously.</li></ul><p>Traditional pipeline parallelism usually divides the model into continuous layer blocks, and each GPU is responsible for a continuous layer block. PTD-P&rsquo;s interleaved 1F1B scheduling divides the model into multiple discontinuous layer blocks. For example, GPU 1 is responsible for layers 1, 2, 9, 10, GPU 2 is responsible for layers 3, 4, 11, 12, and so on. Each GPU is responsible for multiple discontinuous layer blocks, which can more effectively utilize GPU resources and reduce bubble overhead.</p><figure class=align-center><img loading=lazy src=PTD-P.png#center alt="Fig. 11.(Top) Default 1F1B pipeline schedule as in PipeDream-flush.(Bottom) Interleaved 1F1B pipeline schedule. First model chunks are in dark colors and second chunks are in light colors. (Image source: Narayanan et al. 2021)" width=100%><figcaption><p>Fig. 11.(Top) Default 1F1B pipeline schedule as in PipeDream-flush.(Bottom) Interleaved 1F1B pipeline schedule. First model chunks are in dark colors and second chunks are in light colors. (Image source: <a href=https://arxiv.org/abs/2104.04473>Narayanan et al. 2021</a>)</p></figcaption></figure><h2 id=mixture-of-experts-model>Mixture-of-Experts Model<a hidden class=anchor aria-hidden=true href=#mixture-of-experts-model>#</a></h2><p><strong>Mixture-of-Experts (MoE)</strong> (<a href=https://arxiv.org/abs/1701.06538>Shazeer et al. 2017</a>) is a sparsely activated model that significantly increases the model&rsquo;s parameter size and performance without significantly increasing the computational cost by combining multiple independent &ldquo;expert&rdquo; networks and a gating network. The core idea of MoE is <strong>Sparse Activation</strong>, that is, for each input sample, only a part of the expert networks are activated, rather than the entire model. This method not only improves computational efficiency but also enhances the model&rsquo;s expressive ability, making it perform well in LLMs.</p><p>MoE&rsquo;s design inspiration comes from <strong>Ensemble learning</strong>, a technology that decomposes complex tasks into multiple subtasks and completes them collaboratively by different models. In MoE, these &ldquo;subtasks&rdquo; are processed by multiple independent expert networks, and the gating network is responsible for dynamically selecting the most suitable experts based on the characteristics of the input sample. This division of labor and cooperation mechanism is similar to an expert team in human society: experts in different fields provide professional opinions for specific problems, and finally, a comprehensive result is obtained.</p><figure class=align-center><img loading=lazy src=moe.png#center alt="Fig. 12. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)" width=100%><figcaption><p>Fig. 12. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: <a href=https://arxiv.org/abs/1701.06538>Shazeer et al. 2017</a>)</p></figcaption></figure><h3 id=core-components-of-moe>Core Components of MoE<a hidden class=anchor aria-hidden=true href=#core-components-of-moe>#</a></h3><p>A typical MoE contains the following components:</p><ul><li><strong>Expert Networks:</strong> A set of independent neural networks ${E_1, E_2, &mldr;, E_n}$. Each expert network $E_i$ can be any type of neural network, such as FFN, CNN, RNN, etc. The number of expert networks $n$ can be very large, such as dozens, hundreds, or even thousands.</li><li><strong>Gating Network:</strong> A trainable neural network $G$ used to learn a probability distribution based on the input sample $x$ to determine which experts to activate. The input of the gating network is the input sample $x$, and the output is an $n$-dimensional probability vector $p = G(x) = [p_1, p_2, &mldr;, p_n]$, where $p_i$ represents the probability of activating expert $E_i$.</li><li><strong>Expert Output Aggregation:</strong> According to the output probability distribution of the gating network, the outputs of the activated expert networks are weighted and summed to obtain the final output $y$ of the MoE layer.</li></ul><h3 id=noisy-top-k-gating>Noisy Top-k Gating<a hidden class=anchor aria-hidden=true href=#noisy-top-k-gating>#</a></h3><p>To achieve sparse activation and ensure balanced expert usage, MoE usually adopts <strong>Noisy Top-k Gating</strong> as the gating mechanism. This method guarantees computational efficiency and avoids uneven expert load through the introduction of noise and top-k selection. The detailed workflow is as follows:</p><ol><li><strong>Gating Score Calculation:</strong></li></ol><p>For an input sample $x$, the gating network first calculates the gating score $H^{(i)}(x)$ for each expert. This score consists of two parts: linear transformation and noise term, as shown in the formula:</p>$$
H^{(i)}(x) =(x W_g)^{(i)} + \epsilon \cdot \text{softplus}\left((x W_{\text{noise}})^{(i)} \right), \quad \epsilon \sim \mathcal{N}(0, 1)
$$<ul><li><strong>Parameter Description</strong>:<ul><li>$W_g \in \mathbb{R}^{d \times n}$: Trainable weight matrix of the gating network, where $d$ is the input feature dimension and $n$ is the number of experts.</li><li>$W_{\text{noise}} \in \mathbb{R}^{d \times n}$: Weight matrix used to generate noise.</li><li>$\epsilon \sim \mathcal{N}(0, 1)$: Standard Gaussian noise, increasing gating randomness.</li><li>$\text{softplus}(x) = \log(1 + e^x)$: Smooth activation function to ensure that the noise is non-negative.</li></ul></li></ul><p>The introduction of noise avoids the gating network always selecting fixed experts and enhances the robustness and diversity of the model.</p><ol start=2><li><strong>Top-k Selection:</strong></li></ol><p>After calculating the gating score vector $H(x) = [H^{(1)}(x), H^{(2)}(x), \dots, H^{(n)}(x)]$, the gating network selects the top $k$ experts with the largest values (usually $k \ll n$). This step is implemented by the $\text{topk}(v, k)$ function:</p>$$
\text{topk}^{(i)}(v, k) =
\begin{cases}
v^{(i)} & \text{if } v^{(i)} \text{ is in the top } k \text{ elements of } v \\
-\infty & \text{otherwise}
\end{cases}
$$<p>Setting the scores of non-Top-k experts to $-\infty$ ensures that the probabilities of these experts in the subsequent softmax operation are 0, achieving sparsity.</p><ol start=3><li><strong>Softmax Normalization:</strong></li></ol><p>Perform softmax normalization on the gating scores of the Top-k experts to obtain a sparse probability distribution $G(x)$:</p>$$
G(x) = \text{softmax}\left( \text{topk}(H(x), k) \right)
$$<p>Only the probabilities of the Top-k experts are non-zero, and the rest are 0. For example, if $n=100, k=2$, then the probabilities of 98 experts are 0.</p><ol start=4><li><strong>Weighted Summation:</strong></li></ol><p>Weight and sum the outputs of the Top-k experts according to the probabilities to obtain the output of the MoE layer:</p>$$
y = \sum_{i=1}^{n} G^{(i)}(x) E_i(x)
$$<p>Since only $k$ experts are activated, the amount of calculation is much lower than activating all $n$ experts.</p><h3 id=auxiliary-loss>Auxiliary Loss<a hidden class=anchor aria-hidden=true href=#auxiliary-loss>#</a></h3><p>To prevent the gating network from being overly biased towards a few experts, MoE introduces Auxiliary Loss (<a href=https://arxiv.org/abs/1701.06538>Shazeer et al. 2017</a>) to encourage all experts to be used evenly. A common method is based on the square of the <a href=https://en.wikipedia.org/wiki/Coefficient_of_variation>Coefficient of Variation (CV)</a> of expert usage rate:</p>$$
\mathcal{L}_{\text{aux}} = w_{\text{aux}} \cdot \text{CV}\left( \sum_{x \in X} G(x) \right)^2
$$<ul><li><p><strong>Parameter Description</strong>:</p><ul><li>$X$: Input samples of a mini-batch.</li><li>$\sum_{x \in X} G(x)$: Statistics on the number of times each expert is activated in a mini-batch.</li><li>$\text{CV}$: The ratio of standard deviation to mean, measuring the uniformity of expert usage distribution.</li><li>$w_{\text{aux}}$: Weight of auxiliary loss, which needs to be adjusted manually.</li></ul></li><li><p><strong>Function</strong>: By minimizing $\mathcal{L}_{\text{aux}}$, the model optimizes the balance of expert selection and avoids some experts being overused while others are idle.</p></li></ul><h3 id=gshard>GShard<a hidden class=anchor aria-hidden=true href=#gshard>#</a></h3><p>GShard (<a href=https://arxiv.org/abs/2006.16668>Lepikhin et al. 2020</a>) mainly shards the MoE layer, distributing the expert networks ${E_1, E_2, &mldr;, E_n}$ in the MoE layer to multiple TPU devices. For example, if there are $P$ TPU devices, the expert networks can be divided into $P$ groups, and each group of expert networks is assigned to a TPU device. Other layers of the Transformer model (such as self-attention layer, LayerNorm layer) are replicated on all TPU devices.</p><p><strong>Improved Gating Mechanism of GShard:</strong></p><p>GShard has made some improvements on the basis of Noisy Top-k Gating to improve the performance and stability of the gating mechanism:</p><ul><li><p><strong>Expert Capacity:</strong>
To avoid expert overload, GShard introduces expert capacity limits. Each expert network has a capacity limit, indicating the maximum number of tokens it can process. If a token is routed to an expert network that has reached its capacity limit, the token will be marked as &ldquo;overflowed&rdquo;, and the gating output will be set to a zero vector, indicating that the token will not be routed to any expert network.</p></li><li><p><strong>Local Group Dispatching:</strong>
To improve gating efficiency, GShard groups tokens and enforces expert capacity limits at the group level. For example, divide the tokens in a mini-batch into multiple local groups, each local group containing a certain number of tokens. The gating network selects the top-k expert networks for each local group and ensures that the number of tokens processed by each expert network in a local group does not exceed its capacity limit.</p></li><li><p><strong>Auxiliary Loss:</strong>
GShard also uses an auxiliary loss function to balance expert load. Different from the auxiliary loss of the original MoE model, GShard&rsquo;s auxiliary loss aims to minimize the mean square error of the proportion of data routed to each expert network, which more directly measures the degree of expert load balance.</p></li><li><p><strong>Random Routing:</strong>
To increase the randomness of routing, GShard introduces a random routing mechanism when selecting the top-k expert networks. In addition to selecting the best top-k expert networks, GShard also randomly selects suboptimal expert networks with a certain probability to increase the diversity of expert networks and improve the generalization ability of the model.</p></li></ul><p>Below is the core algorithm flow of GShard:</p><figure class=align-center><img loading=lazy src=gshard.png#center alt="Fig. 13. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)" width=100%><figcaption><p>Fig. 13. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: <a href=https://arxiv.org/abs/2006.16668>Lepikhin et al. 2020</a>)</p></figcaption></figure><h3 id=switch-transformer>Switch Transformer<a hidden class=anchor aria-hidden=true href=#switch-transformer>#</a></h3><p>Switch Transformer (<a href=https://arxiv.org/pdf/2101.03961>Fedus et al. 2021</a>) is a MoE model proposed by Google with a parameter size of <strong>trillions</strong>. Its core innovation is to replace the dense feed-forward network (FFN) layer in the Transformer model with a sparse Switch FFN layer. Unlike GShard&rsquo;s Top-2 Gating, Switch Transformer only routes each input token to one expert network, which has higher sparsity and further reduces computational costs, making it possible to train trillion-parameter models. It encourages token routing to be more balanced among $N$ experts. The auxiliary loss of Switch Transformer is based on the cumulative product of the actual routing ratio and the predicted routing probability. The specific formula is as follows:</p>$$
\text{loss} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i
$$<ul><li><strong>Parameter Description</strong>:<ul><li><p>$N$: Total number of experts.</p></li><li><p>$f_i$: The proportion of tokens routed to the $i$-th expert, defined as:</p>$$
f_i = \frac{1}{T} \sum_{x \in B} 1\{\text{argmax } p(x) = i\}
$$</li><li><p>$P_i$: The routing probability of the $i$-th expert predicted by the gating network, defined as:</p>$$
P_i = \frac{1}{T} \sum_{x \in B} p_i(x)
$$</li><li><p>$T$: Total number of tokens in batch $B$.</p></li><li><p>$\alpha$: Weight hyperparameter of auxiliary loss, usually set to $10^{-2}$.</p></li></ul></li></ul><p>By minimizing $\text{loss}$, the model makes the actual routing ratio $f_i$ consistent with the predicted probability $P_i$, thereby indirectly promoting load balancing between experts and avoiding some experts being idle.</p><figure class=align-center><img loading=lazy src=switch_transformer.png#center alt="Fig. 14. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)" width=100%><figcaption><p>Fig. 14. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: <a href=https://arxiv.org/abs/2101.03961>Fedus et al. 2021</a>)</p></figcaption></figure><p><strong>Switch Router Mechanism:</strong></p><ol><li><p><strong>Routing Prediction:</strong>
For an input token $x$, Switch Router predicts the routing probability $p_i = G^{(i)}(x)$ of each expert network, where $i = 1, 2, &mldr;, n$, and n is the number of expert networks.</p></li><li><p><strong>Expert Selection:</strong>
Select the expert network with the highest routing probability as the best expert network. Switch Transformer adopts the Top-1 routing strategy, that is, each token is only routed to the expert network with the highest routing probability.</p></li><li><p><strong>Token Routing:</strong>
Route the input token $x$ to the selected best expert network for processing.</p></li></ol><p><strong>Training Stability Optimization of Switch Transformer:</strong></p><p>To improve the training stability of Switch Transformer, the paper proposes the following optimization strategies:</p><ul><li><p><strong>Selective Precision</strong>
Using FP32 precision inside the routing function can improve training stability and avoid additional overhead caused by FP32 tensor communication. Specifically, the calculation process of Switch Router uses FP32 throughout, and the final result is converted to FP16 to balance efficiency and precision.</p></li><li><p><strong>Smaller Initialization</strong>
It is recommended to adjust the weight initialization scale parameter $s$ of Transformer from 1 to 0.1. A smaller initialization scale helps to alleviate the risk of gradient explosion in the early stage of training, thereby improving overall training stability. The specific implementation is to sample from a truncated normal distribution with a mean of 0 and a standard deviation of $\sqrt{s/n}$ (where $n$ is the number of input units).</p></li><li><p><strong>Higher Expert Dropout</strong>
Using a higher dropout rate (e.g., 0.4) in the expert FFN layer, while maintaining a lower dropout rate (e.g., 0.1) in non-expert layers, this setting can effectively prevent overfitting and thus enhance the generalization ability of the model. The experimental results in the figure below show that the model performs best when the dropout rate of the expert layer is set to 0.4 on tasks such as GLUE, CNNDM, SQuAD, and SuperGLUE.</p></li></ul><figure class=align-center><img loading=lazy src=switch_transformer_fine_tuning_result.png#center alt="Fig. 15. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: Fedus et al. 2021)" width=100%><figcaption><p>Fig. 15. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: <a href=https://arxiv.org/abs/2101.03961>Fedus et al. 2021</a>)</p></figcaption></figure><p>The Switch Transformers paper uses the following figure to intuitively show how different parallel technologies split model weights and data:</p><figure class=align-center><img loading=lazy src=switch_transformer_parallelism.png#center alt="Fig. 16. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)" width=100%><figcaption><p>Fig. 16. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: <a href=https://arxiv.org/abs/2101.03961>Fedus et al. 2021</a>)</p></figcaption></figure><h3 id=expert-choice>Expert Choice<a hidden class=anchor aria-hidden=true href=#expert-choice>#</a></h3><p>Expert Choice (EC) (<a href=https://arxiv.org/abs/2202.09368>Zhou et al. 2022</a>) is a routing strategy opposite to token choice routing (such as GShard&rsquo;s top-2 or Switch Transformer&rsquo;s top-1). In token choice routing, each token selects top-k experts from all experts for routing; while in expert choice routing, each expert selects top-k tokens from all tokens for processing. This method aims to solve the problems of load imbalance and token waste in token choice routing, and significantly improve training efficiency. The following is the specific calculation process:</p><ol><li><p><strong>Calculate Token-to-Expert Affinity Score</strong></p><p>For an input matrix $X \in \mathbb{R}^{n \times d}$, the process of calculating the token-to-expert affinity score matrix $S \in \mathbb{R}^{n \times e}$ is:</p>$$
S = \text{softmax}(X \cdot W_g), \quad \text{where } W_g \in \mathbb{R}^{d \times e}.
$$<p>Here, $W_g$ is the gating weight matrix, and $e$ is the number of experts.</p></li><li><p><strong>Expert Selects Tokens</strong></p><p>Each expert selects top-k tokens from all tokens for processing. Top-k selection is performed on $S^T$:</p>$$
G, I = \text{top-}k(S^T, k),
$$<p>to get:</p><ul><li><strong>Gating matrix $G \in \mathbb{R}^{e \times k}$:</strong> Records the routing weights corresponding to the tokens selected by the experts, where $G[i, j]$ represents the weight of the $j$-th token selected by expert $i$;</li><li><strong>Token index matrix $I \in \mathbb{R}^{e \times k}$:</strong> Represents the index of the token selected by each expert in the input.</li></ul></li><li><p><strong>One-hot Encoding</strong></p><p>Convert the token index matrix $I$ into a one-hot encoding matrix $P \in \mathbb{R}^{e \times k \times n}$ for subsequent calculations:</p>$$
P = \operatorname{one}-\operatorname{hot}(I)
$$</li><li><p><strong>Construct Gated FFN Layer Input</strong></p><p>For each expert $i$, the input of its gated FFN layer is:</p>$$
(P \cdot X) \in \mathbb{R}^{e \times k \times d}.
$$</li></ol><p>EC controls the sparsity of the model by regularizing and limiting the number of experts to which each token is routed. A common regularization target is as follows:</p>$$
\begin{aligned}
& \max_{A} \langle S^{\top}, A \rangle + \lambda H(A) \\
& \text{s.t. } \forall i: \sum_{j'} A[i, j'] = k, \quad \forall j: \sum_{i'} A[i', j] \leq b, \quad \forall i,j: 0 \leq A[i, j] \leq 1,
\end{aligned}
$$<p>In the optimization problem considered, a matrix $A$ is defined, and the element in the $i$-th row and $j$-th column indicates whether the $i$-th expert has selected the $j$-th token (value 0 or 1). Since this optimization problem is complex to solve, the paper uses the <a href=https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm>Dijkstra&rsquo;s algorithm</a> (to obtain an approximate solution through multiple iterations) to solve it.</p><p>The parameter $b$ is usually determined by the total number of tokens $n$ in the batch and the capacity factor, where the capacity factor represents the average number of experts used by each token. Most experiments use a higher capacity factor. The experimental results show that even when the capacity is reduced, EC (Expert Choice) still performs better than traditional top-1 token choice routing, although capped expert choice slightly reduces fine-tuning performance.</p><p>The advantages of EC are mainly reflected in the following two aspects:</p><ul><li><strong>Perfect Load Balancing:</strong> Each expert processes a fixed number of $k$ tokens, thus avoiding the problem of some experts being overloaded while others are idle, achieving ideal load balancing.</li><li><strong>Higher Training Efficiency:</strong> Experiments show that EC can improve the training convergence speed by about 2 times, which is more efficient than traditional token choice routing.</li></ul><p>However, EC also has the following limitations:</p><ul><li><strong>Batch Size Requirements:</strong> Since EC has high requirements for batch size, it is not suitable for scenarios with smaller batch sizes.</li><li><strong>Autoregressive Generation Limitations:</strong> In autoregressive text generation tasks, EC&rsquo;s top-k selection cannot be implemented because future tokens cannot be predicted, so it is not suitable for such tasks.</li></ul><h2 id=sequence-parallelism>Sequence Parallelism<a hidden class=anchor aria-hidden=true href=#sequence-parallelism>#</a></h2><p><strong>Sequence Parallelism (SP)</strong> is a parallelization strategy proposed for long sequence models (such as Transformer). By partitioning the input in the sequence dimension, it greatly reduces activation memory footprint and improves training efficiency. It is often used in combination with data parallelism, tensor parallelism, or pipeline parallelism, and is especially suitable for processing ultra-long text or other sequence data.</p><h3 id=colossal-ai-sequence-parallelism>Colossal-AI Sequence Parallelism<a hidden class=anchor aria-hidden=true href=#colossal-ai-sequence-parallelism>#</a></h3><figure class=align-center><img loading=lazy src=colossal_sp.png#center alt="Fig. 17. The overall architecture of the proposed sequence parallelism and existing parallel approaches. For sequence parallelism, Device 1 and Device 2 share the same trainable parameters. (Image source: Li, et al. 2021)" width=100%><figcaption><p>Fig. 17. The overall architecture of the proposed sequence parallelism and existing parallel approaches. For sequence parallelism, Device 1 and Device 2 share the same trainable parameters. (Image source: <a href=https://arxiv.org/abs/2105.13120>Li, et al. 2021</a>)</p></figcaption></figure><p>The computational complexity and memory overhead of self-attention are proportional to the square of the sequence length $s$, $O(s^2)$. Long sequence data will increase the intermediate activation memory usage, thus limiting the training capacity of the device. Colossal-AI sequence parallelism (<a href=https://arxiv.org/abs/2105.13120>Li, et al. 2021</a>) proposes <strong>splitting ultra-long sequences to multiple cards</strong> from a system perspective. The specific solution steps are as follows.</p><ol><li><strong>Sequence Chunking</strong>
Divide the input sequence into several chunks, each chunk is saved and computed by different GPUs; therefore, each card only needs to store the activation of its corresponding sequence chunk, avoiding single-card memory explosion.</li><li><strong>Ring Communication + Self-Attention</strong>
Propose Ring Self-Attention (RSA) mechanism: each GPU first calculates local attention, and then sequentially transmits (ring structure) Key/Value chunks to adjacent GPUs. After multiple iterations, it is guaranteed that each GPU can obtain global sequence information.</li><li><strong>Combination with Other Parallelism Methods</strong>
Not restricted by hyperparameters such as the number of attention heads and layers, it can be combined with data parallelism, tensor parallelism, pipeline parallelism and other technologies to jointly break through the sequence length limit of large-scale models.</li></ol><figure class=align-center><img loading=lazy src=ring_self_attention.png#center alt="Fig. 18. Ring Self-Attention. (Image source: Li, et al. 2021)" width=100%><figcaption><p>Fig. 18. Ring Self-Attention. (Image source: <a href=https://arxiv.org/abs/2105.13120>Li, et al. 2021</a>)</p></figcaption></figure><h3 id=megatron-lm-sequence-parallelism>Megatron-LM Sequence Parallelism<a hidden class=anchor aria-hidden=true href=#megatron-lm-sequence-parallelism>#</a></h3><p>Megatron-LM (<a href=https://arxiv.org/pdf/1909.08053>Shoeybi et al. 2019</a>) originally used tensor parallelism to share part of the activation values, but the activation values of operations such as LayerNorm and Dropout in Transformer still need to be completely saved on a single card, and the memory consumption is still huge. Therefore, NVIDIA proposed Megatron-LM sequence parallelism (<a href=https://arxiv.org/abs/2205.05198>Korthikanti, et al. 2022</a>) to <strong>split these activation values</strong> in the sequence dimension, greatly reducing the footprint.</p><figure class=align-center><img loading=lazy src=Megatron-LM-transformer-sp.png#center alt="Fig. 19. Transformer layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)" width=100%><figcaption><p>Fig. 19. Transformer layer with tensor and sequence parallelism. (Image source: <a href=https://arxiv.org/abs/2205.05198>Korthikanti, et al. 2022</a>)</p></figcaption></figure><figure class=align-center><img loading=lazy src=Megatron-LM-mlp-sp.png#center alt="Fig. 20. MLP layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)" width=100%><figcaption><p>Fig. 20. MLP layer with tensor and sequence parallelism. (Image source: <a href=https://arxiv.org/abs/2205.05198>Korthikanti, et al. 2022</a>)</p></figcaption></figure><ol><li><strong>Sequence Dimension Splitting</strong>
For activations that are difficult to split in the tensor dimension, such as LayerNorm and Dropout, divide them along the sequence dimension, so that each GPU only processes a part of the sequence&rsquo;s nonlinear operations.</li><li><strong>Tensor Parallelism is Still Retained</strong>
Linear operations such as Attention and MLP continue to use tensor parallelism; the activations of sequence parallelism need to perform corresponding All-Gather or Reduce-Scatter before and after to exchange data.</li><li><strong>Selective Activation Recomputation</strong>
For some operations with small computational load but large activation volume, choose to temporarily recompute during backpropagation to further save memory.</li></ol><h3 id=deepspeed-ulysses-sequence-parallelism>DeepSpeed-Ulysses Sequence Parallelism<a hidden class=anchor aria-hidden=true href=#deepspeed-ulysses-sequence-parallelism>#</a></h3><p>DeepSpeed-Ulysses (<a href=https://arxiv.org/abs/2309.14509>Jacobs et al. 2023</a>) proposes an efficient sequence parallelism scheme for ultra-long sequence training. By partitioning the input in the sequence dimension and combining two-stage all-to-all communication, it effectively reduces communication volume and activation memory, thereby supporting the training of million-token long sequence Transformer models.</p><figure class=align-center><img loading=lazy src=deepspeed_sp.png#center alt="Fig. 21. DeepSpeed sequence parallelism(DeepSpeed-Ulysses) design. (Image source: Jacobs et al. 2023)" width=100%><figcaption><p>Fig. 21. DeepSpeed sequence parallelism(DeepSpeed-Ulysses) design. (Image source: <a href=https://arxiv.org/abs/2309.14509>Jacobs et al. 2023</a>)</p></figcaption></figure><ol><li><p><strong>Sequence Partitioning + All-to-All Communication</strong>
Divide the input sequence along the sequence dimension to $P$ GPUs, and each GPU only processes a local $N/P$ sequence; before attention calculation, exchange Query ($Q$), Key ($K$), and Value ($V$) through All-to-All operation, so that each GPU obtains complete sequence information, but only calculates the assigned attention heads.</p></li><li><p><strong>Two-Stage Communication Optimization</strong></p><ul><li><strong>First All-to-All:</strong> Perform all-to-all exchange on $Q$/$K$/$V$ before attention calculation to disperse activation calculation and reduce memory pressure per card;</li><li><strong>Second All-to-All:</strong> After attention calculation, collect the output context and remap it to local sequence partitions, which not only restores the original sequence structure but also significantly reduces the amount of communication data.</li></ul></li><li><p><strong>Efficient Communication and Generality</strong>
Using all-to-all communication, the communication volume is reduced to $O(N/P)$, which saves nearly $P$ times the bandwidth compared to the traditional All-Gather method (communication volume $O(N)$); at the same time, this scheme is suitable for dense and sparse attention and can be seamlessly integrated with ZeRO-3 memory optimization, thereby supporting efficient training of larger models and longer sequences.</p></li></ol><figure class=align-center><img loading=lazy src=deepspeed_ulysses_compare.png#center alt="Fig. 22. DeepSpeed-Ulysses vs Megatron LM. (Image source: DeepSpeed Blogs)" width=100%><figcaption><p>Fig. 22. DeepSpeed-Ulysses vs Megatron LM. (Image source: <a href=https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md>DeepSpeed Blogs</a>)</p></figcaption></figure><ul><li>In a 64-card A100 environment, the throughput is increased by up to 2.5 times compared to Megatron-LM sequence parallelism, and longer sequences (million-level tokens) can be processed;</li><li>The convergence performance is the same as the original model, and it can be easily integrated into the Megatron-DeepSpeed framework.</li></ul><h2 id=optimizer-related-parallelism-zero>Optimizer-Related Parallelism: ZeRO<a hidden class=anchor aria-hidden=true href=#optimizer-related-parallelism-zero>#</a></h2><p><strong>ZeRO (Zero Redundancy Optimizer)</strong> (<a href=https://arxiv.org/abs/1910.02054>Rajbhandari et al. 2019</a>) is an optimizer parallelism technology designed to eliminate memory redundancy when training large models. The main memory consumption for training large models is in two parts:</p><ul><li><strong>Model States:</strong> Including <strong>optimizer states</strong> (such as momentum and second-order moments of Adam), <strong>gradients</strong>, and <strong>model parameters</strong>. Mixed-precision training not only requires storing FP16 data but also needs to retain FP32 versions of parameters and states, resulting in higher memory footprint.</li><li><strong>Activations, Temporary Buffers, and Memory Fragmentation (Residual States):</strong> These data are only used once in forward and backward propagation, but they also occupy a lot of memory.</li></ul><p>To solve the memory redundancy problem, ZeRO adopts two major strategies:</p><ol><li><p><strong>ZeRO-DP (Data Parallelism):</strong>
For model states, by sharding and distributing optimizer states, gradients, and parameters to multiple data parallel processes, redundancy is eliminated, and communication volume is reduced by using dynamic communication scheduling.</p></li><li><p><strong>ZeRO-R (Residuals Optimization):</strong>
For activations and temporary buffers, memory usage is optimized by using sharded activation recomputation, fixed buffer size, and real-time memory fragmentation management.</p></li></ol><h3 id=zero-sharding-strategy>ZeRO Sharding Strategy<a hidden class=anchor aria-hidden=true href=#zero-sharding-strategy>#</a></h3><p>ZeRO is divided into three stages, each stage further reduces memory redundancy on the basis of the previous stage, thus making it possible to train ultra-large models:</p><h4 id=zero-1-optimizer-state-sharding>ZeRO-1 (Optimizer State Sharding)<a hidden class=anchor aria-hidden=true href=#zero-1-optimizer-state-sharding>#</a></h4><ul><li><strong>Principle:</strong><ul><li>Shard optimizer states (such as Adam&rsquo;s momentum and second-order moments) along the parameter dimension into $P$ shards ($P$ is the number of GPUs), and each GPU only stores the states corresponding to the model parameters it is responsible for.</li><li>Local Update: Each GPU only updates its locally stored state and parameter shards during the parameter update phase, without additional cross-GPU communication.</li></ul></li></ul><h4 id=zero-2-gradient-sharding>ZeRO-2 (Gradient Sharding)<a hidden class=anchor aria-hidden=true href=#zero-2-gradient-sharding>#</a></h4><ul><li><strong>Principle:</strong><ul><li>On the basis of optimizer state sharding, gradients are also sharded along the parameter dimension, and each GPU only stores the corresponding gradient shard.</li><li>Each GPU calculates local gradients and uses efficient Reduce-Scatter operations to aggregate gradients and then update local parameter shards.</li></ul></li></ul><h4 id=zero-3-parameter-sharding>ZeRO-3 (Parameter Sharding)<a hidden class=anchor aria-hidden=true href=#zero-3-parameter-sharding>#</a></h4><ul><li><strong>Principle:</strong><ul><li>On the basis of ZeRO-1 and ZeRO-2, model parameters (usually 16-bit data) are also sharded, and each GPU only stores the parameter shards corresponding to it.</li><li>Parameter Collection on Demand: During forward or backward propagation, if a GPU needs complete model parameters, it collects the missing shards from other GPUs. This process is only performed when necessary to reduce communication overhead.</li></ul></li></ul><p>The following figure shows the comparison of model state memory consumption per device in different stages:</p><figure class=align-center><img loading=lazy src=deepspeed_zero.png#center alt="Fig. 23. Comparing the per-device memory consumption of model states, with three stages of ZeRO-DP optimizations. (Image source: Rajbhandari et al. 2019)" width=100%><figcaption><p>Fig. 23. Comparing the per-device memory consumption of model states, with three stages of ZeRO-DP optimizations. (Image source: <a href=https://arxiv.org/abs/1910.02054>Rajbhandari et al. 2019</a>)</p></figcaption></figure><h3 id=comparison-of-deepspeed-zero-sharding-and-offload-strategies>Comparison of DeepSpeed ZeRO Sharding and Offload Strategies<a hidden class=anchor aria-hidden=true href=#comparison-of-deepspeed-zero-sharding-and-offload-strategies>#</a></h3><p>To better understand DeepSpeed&rsquo;s ZeRO strategy, the following compares each stage and Offload scheme:</p><table><thead><tr><th><strong>ZeRO Stage</strong></th><th><strong>Description</strong></th><th><strong>Memory Footprint</strong></th><th><strong>Training Speed</strong></th></tr></thead><tbody><tr><td><strong>ZeRO-0</strong></td><td>Pure data parallelism, no sharding, all states are fully replicated on each GPU.</td><td>Highest</td><td><strong>Fastest</strong></td></tr><tr><td><strong>ZeRO-1</strong></td><td>Optimizer states are sharded only, gradients and parameters are still replicated.</td><td>Higher</td><td>Slightly slower than ZeRO-0</td></tr><tr><td><strong>ZeRO-2</strong></td><td>Optimizer states and gradients are sharded.</td><td>Medium</td><td>Slower than ZeRO-1</td></tr><tr><td><strong>ZeRO-3</strong></td><td>Optimizer states, gradients, and model parameters are sharded.</td><td>Lowest</td><td>Significantly slower than ZeRO-2, affected by model size and network bandwidth</td></tr></tbody></table><table><thead><tr><th><strong>Offload Type</strong></th><th><strong>Description</strong></th><th><strong>Memory Footprint</strong></th><th><strong>Training Speed</strong></th></tr></thead><tbody><tr><td><strong>ZeRO-1 + CPU Offload</strong></td><td>On the basis of ZeRO-1, optimizer states are offloaded to CPU memory, reducing GPU memory footprint, but relying on PCIe bandwidth and occupying CPU memory.</td><td>Medium-Low</td><td>Slower than ZeRO-1</td></tr><tr><td><strong>ZeRO-2 + CPU Offload</strong></td><td>On the basis of ZeRO-2, optimizer states are offloaded to CPU memory, further reducing GPU memory footprint for large models, but increasing CPU-GPU data transfer.</td><td>Low</td><td>Slower than ZeRO-2</td></tr><tr><td><strong>ZeRO-3 + CPU Offload</strong></td><td>On the basis of ZeRO-3, optimizer states and model parameters are offloaded to CPU, GPU memory footprint is the lowest, but CPU-GPU communication overhead is extremely large.</td><td><strong>Extremely Low</strong></td><td><strong>Very Slow</strong></td></tr><tr><td><strong>ZeRO-Infinity (NVMe Offload)</strong></td><td>Based on ZeRO-3, states are offloaded to NVMe devices, breaking through CPU memory limits, suitable for ultra-large models; performance is highly dependent on NVMe parallel read and write speed.</td><td><strong>Extremely Low</strong>NVMe support required</td><td>Slower than ZeRO-3, but usually better than CPU Offload scheme</td></tr></tbody></table><h3 id=communication-volume-and-performance-impact>Communication Volume and Performance Impact<a hidden class=anchor aria-hidden=true href=#communication-volume-and-performance-impact>#</a></h3><ul><li><p><strong>ZeRO-0/1/2:</strong>
Mainly rely on All-Reduce for gradient synchronization, and the communication volume is relatively low.</p></li><li><p><strong>ZeRO-3:</strong>
All-Gather/All-Reduce operations are required for model parameters, and the communication volume increases significantly. Network bandwidth becomes a key bottleneck.</p></li><li><p><strong>Offload Strategy (CPU/NVMe):</strong>
Data transmission is mainly between CPU ↔ GPU or NVMe ↔ GPU. The transmission bandwidth is much lower than the communication between GPUs, which may significantly affect the training speed, especially in ZeRO-3 scenarios.</p></li></ul><h2 id=multi-dimensional-parallelism>Multi-dimensional Parallelism<a hidden class=anchor aria-hidden=true href=#multi-dimensional-parallelism>#</a></h2><p><strong>Multi-dimensional Parallelism</strong> refers to the organic combination of multiple parallel technologies such as data parallelism, model parallelism, and pipeline parallelism in distributed training to fully utilize the computing resources of modern GPU clusters. Through this &ldquo;3D parallelism&rdquo; or &ldquo;4D parallelism&rdquo; strategy, not only memory efficiency can be improved, but also computational efficiency can be improved, thereby achieving efficient training of ultra-large-scale (even trillion-parameter level) models.</p><h3 id=3d-parallelism>3D Parallelism<a hidden class=anchor aria-hidden=true href=#3d-parallelism>#</a></h3><p>With the rapid improvement of the computing power of GPU clusters, training trillion-parameter models is no longer out of reach. DeepSpeed integrates data parallelism, model parallelism, and pipeline parallelism to build a &ldquo;3D parallelism&rdquo; strategy. This strategy mainly solves the two major challenges faced by training ultra-large models:</p><ul><li><p><strong>Memory Efficiency:</strong>
Model layers are divided into different pipeline stages, and each stage is further divided by model parallelism, reducing the memory occupied by models, optimizers, and activations. However, it should be noted that model splitting cannot be unlimited, otherwise, the communication overhead will increase significantly, which will affect computational efficiency.</p></li><li><p><strong>Computational Efficiency:</strong>
To make the number of computing workers exceed the limitations of simple model and pipeline parallelism, and to ensure computational efficiency, DeepSpeed expands with ZeRO-DP (data parallelism based on optimizer state sharding). ZeRO-DP not only further optimizes memory usage but also allocates data parallel groups to devices with local high-bandwidth communication through topology-aware mapping, greatly reducing communication overhead.</p></li></ul><p>The following diagram shows the overall strategy of 3D parallelism:</p><figure class=align-center><img loading=lazy src=zero_3d.png#center alt="Fig. 24. Example 3D parallelism with 32 workers. Layers of the neural network are divided among four pipeline stages. Layers within each pipeline stage are further partitioned among four model parallel workers. Lastly, each pipeline is replicated across two data parallel instances, and ZeRO partitions the optimizer states across the data parallel replicas. (Image source: Majumder et al. 2020)" width=100%><figcaption><p>Fig. 24. Example 3D parallelism with 32 workers. Layers of the neural network are divided among four pipeline stages. Layers within each pipeline stage are further partitioned among four model parallel workers. Lastly, each pipeline is replicated across two data parallel instances, and ZeRO partitions the optimizer states across the data parallel replicas. (Image source: <a href=https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/>Majumder et al. 2020</a>)</p></figcaption></figure><p>Each parallel dimension (data, model, pipeline) is carefully mapped to fully utilize the communication bandwidth within and between nodes. Specific strategies include:</p><ul><li><strong>Optimize Intra-node Communication:</strong> Since model parallelism has the largest communication overhead, model parallel groups are preferentially arranged within the same node to utilize higher intra-node bandwidth (e.g., using NVIDIA Megatron-LM&rsquo;s tensor sharding method);</li><li><strong>Data Parallelism and Pipeline Parallelism:</strong> When model parallelism does not cover the entire node, data parallel groups are arranged within the same node as much as possible; pipeline parallelism can be flexibly arranged for cross-node scheduling due to its smaller communication volume.</li></ul><p>By reducing the amount of communication data in each data parallel group and increasing the parallelism of local parallel communication, the overall communication bandwidth is effectively amplified.</p><figure class=align-center><img loading=lazy src=3d_parallelism.png#center alt="Fig. 25. Mapping of workers in Figure 24 to GPUs on a system with eight nodes, each with four GPUs. Coloring denotes GPUs on the same node. (Image source: Majumder et al. 2020)" width=100%><figcaption><p>Fig. 25. Mapping of workers in Figure 24 to GPUs on a system with eight nodes, each with four GPUs. Coloring denotes GPUs on the same node. (Image source: <a href=https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/>Majumder et al. 2020</a>)</p></figcaption></figure><h3 id=4d-parallelism>4D Parallelism<a hidden class=anchor aria-hidden=true href=#4d-parallelism>#</a></h3><p>To further expand the model scale, Llama3 (<a href=https://arxiv.org/abs/2407.21783>Grattafiori et al. 2024</a>) adopted a 4D parallel strategy during training. It combines four parallel methods to shard the model in a more fine-grained manner, so that the model parameters, optimizer states, gradients, and activations on each GPU can be adapted to the capacity limit of high-bandwidth memory (HBM). These four parallel methods are:</p><ul><li><strong>Tensor Parallelism (TP):</strong> Divide a single weight tensor into multiple blocks and distribute them across different devices;</li><li><strong>Pipeline Parallelism (PP):</strong> Vertically divide the model into multiple stages, and each stage processes different micro-batches in parallel on different devices;</li><li><strong>Context Parallelism (CP):</strong> Divide the input context into multiple segments to alleviate the memory bottleneck when inputting long sequences;</li><li><strong>Data Parallelism (DP), usually using Fully Sharded Data Parallelism (FSDP):</strong> Shard models, optimizer states, and gradients, and synchronize after each training step.</li></ul><p>The following diagram shows an example of 4D parallelism implemented on 16 GPUs. The position of each GPU is represented by a vector [D1, D2, D3, D4], where each dimension corresponds to a parallel strategy. GPUs are grouped according to four dimensions [TP, CP, PP, DP], and the group size of each dimension is 2. For example, GPU0 and GPU1 belong to the same tensor parallel group; GPU0 and GPU2 belong to the same context parallel group; GPU0 and GPU4 belong to the same pipeline parallel group; GPU0 and GPU8 belong to the same data parallel group:</p><figure class=align-center><img loading=lazy src=4d_parallelism.png#center alt="Fig. 26. Illustration of 4D parallelism. (Image source: Grattafiori et al. 2024)" width=100%><figcaption><p>Fig. 26. Illustration of 4D parallelism. (Image source: <a href=https://arxiv.org/abs/2407.21783>Grattafiori et al. 2024</a>)</p></figcaption></figure><p>Through the 4D parallel strategy, Llama3 can fully utilize the computing resources of multiple GPUs during training, while effectively reducing memory footprint and supporting the training of ultra-large-scale models.</p><h2 id=memory-optimization-techniques>Memory Optimization Techniques<a hidden class=anchor aria-hidden=true href=#memory-optimization-techniques>#</a></h2><p>In addition to parallel training techniques, there are many memory optimization techniques designed to help train LLMs. These designs mainly start from reducing the memory footprint of each stage in the training process.</p><h3 id=cpu-offloading>CPU Offloading<a hidden class=anchor aria-hidden=true href=#cpu-offloading>#</a></h3><p>CPU Offloading (<a href=https://arxiv.org/abs/1602.08124>Rhu et al. 2016</a>) refers to a common and intuitive practice of offloading data or tensors that are temporarily not needed to the CPU when GPU memory is insufficient and loading them back to the GPU when needed. Its main purpose is to use the larger capacity of CPU memory to expand available space, so that larger-scale models can be trained even in memory-constrained environments. However, this method will bring additional data transmission overhead and usually reduce training speed, so its application has been relatively reduced in recent years.</p><ol><li><strong>Identify Offloadable Tensors:</strong> Identify tensors that are temporarily not used during training, such as model parameters, optimizer states, intermediate activations, etc. The basis for judging whether a tensor can be offloaded can be the frequency of use, life cycle, etc. of the tensor.</li><li><strong>Tensor Offloading:</strong> Move offloadable tensors from GPU memory to CPU memory. Data transmission is usually performed through the PCIe bus.</li><li><strong>Tensor Prefetching:</strong> Before needing to use tensors offloaded to CPU memory, load the tensors from CPU memory back to GPU memory in advance. Prefetching operations can be performed in parallel with GPU computing operations to reduce data loading latency.</li><li><strong>Tensor Usage:</strong> The GPU uses tensors loaded back into GPU memory for computation.</li><li><strong>Tensor Re-offloading:</strong> After the tensor is used up, if the tensor is no longer needed for a period of time, it can be offloaded to CPU memory again to release GPU memory space.</li></ol><p>ZeRO-Offload and ZeRO-Infinity are memory optimization technologies based on CPU offloading implemented in the DeepSpeed library. ZeRO-Offload offloads optimizer states to CPU memory, and ZeRO-Infinity goes further, offloading model parameters to CPU memory or even NVMe disks, breaking through the GPU memory wall and supporting the training of larger-scale models.</p><p>The following figure intuitively shows the memory optimization technology of <strong>Heterogeneous system</strong>:</p><figure class=align-center><img loading=lazy src=heterogenous_system.png#center alt="Fig. 27. Heterogenous system illustration. (Image source: Clolossal-AI Documentation)" width=100%><figcaption><p>Fig. 27. Heterogenous system illustration. (Image source: <a href=https://colossalai.org/docs/concepts/paradigms_of_parallelism>Clolossal-AI Documentation</a>)</p></figcaption></figure><h3 id=activation-recomputationgradient-checkpointing>Activation Recomputation/Gradient Checkpointing<a hidden class=anchor aria-hidden=true href=#activation-recomputationgradient-checkpointing>#</a></h3><p>Activation Recomputation/Gradient Checkpointing (<a href=https://arxiv.org/abs/1604.06174>Chen et al. 2016</a>) is a <strong>technology that trades computation for memory</strong>. During training, only part of the activation values are saved (e.g., the input activation values of each Transformer layer). During backpropagation, the unsaved activation values are recomputed. Activation recomputation can significantly reduce the activation memory footprint during training, especially when training deep neural networks.</p><ol><li><strong>Select Checkpoints:</strong> Select some layers in the model as checkpoints. Usually, key layers in the model are selected as checkpoints, such as the input layer of the Transformer layer.</li><li><strong>Forward Pass:</strong> During forward propagation, only the activation values of checkpoint layers are saved. For non-checkpoint layers, the activation values are immediately released after calculation and not saved.</li><li><strong>Backward Pass:</strong> During backpropagation, when it is necessary to calculate the gradient of a non-checkpoint layer, forward propagation is performed again first to calculate the activation value of the layer (recomputation), and then backward propagation is performed to calculate the gradient. For checkpoint layers, since the activation values of checkpoint layers have been saved, the saved activation values can be directly used for backpropagation without recomputation.</li></ol><p>The following is a memory cost analysis of activation recomputation. For ease of analysis, assume that the model has a total of $n$ network layers and divides them <strong>evenly</strong> into $k$ segments. In this way, each segment contains approximately $n/k$ network layers. When doing activation recomputation, we only save the activation values at the boundaries of each segment (i.e., checkpoints), and recompute the rest when needed. The following function represents the memory requirement during training:</p>$$
\text{cost-total} \;=\; \max_{i=1,\ldots,k}\bigl[\text{cost-of-segment}(i)\bigr] \;+\; O(k)
\;=\; O\Bigl(\tfrac{n}{k}\Bigr) \;+\; O(k).
$$<p>Next, consider how to choose the optimal $k$ to minimize $f(k)$ given $n$:</p>$$
f(k) \;=\; \frac{n}{k} \;+\; k.
$$<p>Take the derivative of $f(k)$ with respect to $k$ and set it to 0 (only consider $k>0$):</p>$$
f'(k) \;=\; -\frac{n}{k^2} \;+\; 1 \;=\; 0
\quad\Longrightarrow\quad
k^2 = n
\quad\Longrightarrow\quad
k = \sqrt{n}.
$$<p>Substituting $k = \sqrt{n}$, we can get the minimum memory overhead of approximately</p>$$
f(\sqrt{n}) \;=\; \frac{n}{\sqrt{n}} \;+\; \sqrt{n}
\;=\; 2\sqrt{n}.
$$<p>Therefore, the overall peak memory requirement of this approach can be reduced to the order of $O(\sqrt{n})$ (compared to the $O(n)$ memory of generally directly saving all activations), which is why activation recomputation technology can bring &ldquo;sublinear&rdquo; memory footprint. The following figure intuitively shows the effect of this trick.</p><figure class=align-center><img loading=lazy src=activation_recomputation.png#center alt="Fig. 28. The memory cost of different memory saving algorithms. Sharing: Memory used by intermediate results is recycled when no longer needed. Inplace: Save the output directly into memory of an input value. (Image source: Chen et al. 2016)" width=100%><figcaption><p>Fig. 28. The memory cost of different memory saving algorithms. Sharing: Memory used by intermediate results is recycled when no longer needed. Inplace: Save the output directly into memory of an input value. (Image source: <a href=https://arxiv.org/abs/1604.06174>Chen et al. 2016</a>)</p></figcaption></figure><p>It should be noted that activation recomputation requires additional forward recomputation in the <strong>backward propagation</strong> stage. Each segment needs to perform forward computation of $n/k$ layers. If the network is divided into $k$ segments, the total recomputation during backpropagation is approximately $k \times \bigl(n/k\bigr) = n$ layers of forward operations, which is equivalent to doing approximately one more &ldquo;forward propagation&rdquo; in each training iteration. This is usually acceptable in LLM training because:</p><ul><li>Compared to quickly exhausting GPU memory and making it impossible to train large-scale models, this additional cost in computation is usually more bearable.</li><li>When the model is very deep ($n$ is large), using activation recomputation technology can significantly reduce memory usage from $O(n)$ to $O(\sqrt{n})$, making it possible to train more and deeper large models on given hardware.</li></ul><h3 id=mixed-precision-training>Mixed Precision Training<a hidden class=anchor aria-hidden=true href=#mixed-precision-training>#</a></h3><p>Mixed Precision Training (<a href=https://arxiv.org/abs/1710.03740>Micikevicius al. 2017</a>) is a technology that simultaneously uses low-precision floating-point numbers (such as FP16 or BF16) and high-precision floating-point numbers (such as FP32) during model training. Its core goal is to <strong>reduce memory footprint</strong> and <strong>accelerate training</strong> while maintaining model accuracy comparable to full-precision training as much as possible.</p><p>Modern GPUs have higher throughput and lower memory footprint in low-precision computing, thereby reducing memory access overhead and memory bandwidth requirements. Mixed-precision training can significantly improve training speed. The following figure shows the basic process of mixed-precision training in a network layer: forward and backward propagation mainly use half-precision (FP16) operations, while gradient accumulation and parameter updates use full-precision (FP32) to avoid numerical precision problems that may be caused by low-precision computing.</p><figure class=align-center><img loading=lazy src=mixed_precision.png#center alt="Fig. 29. Mixed precision training iteration for a layer. (Image source: Micikevicius al. 2017)" width=100%><figcaption><p>Fig. 29. Mixed precision training iteration for a layer. (Image source: <a href=https://arxiv.org/abs/1710.03740>Micikevicius al. 2017</a>)</p></figcaption></figure><p>Mixed-precision training mainly relies on the following three key technologies:</p><ol><li><p><strong>Full-Precision Master Copy of Weights</strong>
To prevent gradients from being truncated to zero due to being too small in magnitude under FP16, a master copy of FP32 weights is maintained during training. The specific process is:</p><ul><li><strong>Initialization:</strong> Use FP32 weights as the master copy of the model;</li><li><strong>Forward/Backward Propagation:</strong> Before each iteration starts, convert FP32 weights to FP16 for forward propagation and backward propagation to calculate FP16 gradients;</li><li><strong>Parameter Update:</strong> Before updating parameters, convert FP16 gradients to FP32 and use them to update the FP32 master copy.</li></ul><p>This design not only utilizes the efficiency of low-precision computing but also ensures the accuracy of parameter updates.</p></li><li><p><strong>Loss Scaling</strong>
To avoid gradient underflow due to the limited representation range of low precision, the loss value is usually amplified before backpropagation. The specific process is:</p></li></ol><ul><li>Use FP32 to calculate the loss $L$;</li><li>Multiply the loss by a scaling factor $S$ to get $L&rsquo; = L \times S$, and then perform backpropagation to calculate FP16 gradients;</li><li>Before parameter update, divide the gradient by $S$ to restore it to the true gradient.</li></ul><p>The choice of scaling factor is crucial: too small may not avoid gradient underflow, and too large may cause gradient overflow. Dynamic loss scaling technology can automatically adjust the scaling factor according to the actual situation of gradients during training.</p><p>As shown in the figure below, amplifying the loss makes the gradient distribution more concentrated in the higher numerical part, thereby retaining the subtle information that may be truncated under low-precision representation.</p><figure class=align-center><img loading=lazy src=mixed_precision_fp16.png#center alt="Fig. 30. The histogram of gradients in full precision. The left part up to $2^{-24}$ will be zero-ed off once the model switches to FP16. (Image source: Micikevicius al. 2017)" width=100%><figcaption><p>Fig. 30. The histogram of gradients in full precision. The left part up to $2^{-24}$ will be zero-ed off once the model switches to FP16. (Image source: <a href=https://arxiv.org/abs/1710.03740>Micikevicius al. 2017</a>)</p></figcaption></figure><ol start=3><li><strong>Arithmetic Precision Control</strong>
For operations with high precision requirements (such as vector dot product and summation reduction), FP32 can be used for accumulation calculation, and then converted to FP16 for storage; for element-wise operations, FP16 or FP32 can be selected according to specific needs.</li></ol><h3 id=compression>Compression<a hidden class=anchor aria-hidden=true href=#compression>#</a></h3><p>In the deep learning training process, intermediate results (such as activation values and gradient information), although only used once in forward propagation and once in backward propagation, often occupy a lot of memory. Considering that there is a significant time interval between two uses, data can be <strong>compressed</strong> after the first use, and then decompressed when needed later, thereby effectively reducing memory footprint.</p><p>Compression technology is mainly applied to two scenarios:</p><ul><li><strong>Activation Value Compression:</strong> Compress activation values after forward propagation and decompress before backward propagation. This is especially important for deep neural networks because activation values usually occupy a lot of memory.</li><li><strong>Gradient Compression:</strong> Compress gradients after calculating gradients in backpropagation and before gradient synchronization to reduce the amount of data for cross-GPU communication, thereby improving distributed training efficiency.</li></ul><p>Compression technology can be divided into two categories:</p><ol><li><p><strong>Lossless Compression:</strong>
Methods such as Huffman coding or Lempel-Ziv algorithm are used to ensure that the decompressed data is completely consistent with the original data. However, due to the low compression rate, its memory saving effect is limited.</p></li><li><p><strong>Lossy Compression:</strong>
Algorithms such as JPEG or MPEG are used to obtain higher compression rates on the premise of allowing certain data loss. This method can significantly reduce memory footprint, but may have a certain impact on model accuracy and convergence.</p></li></ol><p>Gist (<a href=https://www.microsoft.com/en-us/research/uploads/prod/2018/04/fiddle-gist-isca18.pdf>Jain et al. 2018</a>) is a memory optimization technology for activation value compression. Its core lies in using data encoding strategies to compress intermediate results, mainly including two encoding schemes:</p><ul><li><p><strong>Layer-Specific Lossless Encoding:</strong>
Design special lossless encoding schemes for specific layer structures (such as ReLU-Pool and ReLU-Conv):</p><ul><li>For ReLU-Pool layers, binary encoding can be used;</li><li>For ReLU-Conv layers, sparse storage and dense computation encoding are used.</li></ul></li><li><p><strong>Aggressive Lossy Encoding:</strong>
Delayed Precision Reduction (DPR) technology is used. The core idea of DPR is: activation values need to maintain high precision during forward propagation, while lower precision can be tolerated during backward propagation. Therefore, activation values are compressed to lower precision after forward propagation, and then decompressed to high precision before backward propagation.</p></li></ul><h3 id=memory-efficient-optimizers>Memory-Efficient Optimizers<a hidden class=anchor aria-hidden=true href=#memory-efficient-optimizers>#</a></h3><p>Traditional optimizers (such as Adam, SGD with Momentum) need to maintain a large amount of state data (such as momentum and variance) for each model parameter during training. Their memory footprint is often comparable to or even higher than the model parameter size. For example, taking the Adam optimizer (<a href=https://arxiv.org/pdf/1412.6980>Kingma et al. 2014</a>) as an example, each parameter needs to store the first-order moment and the second-order moment. Adding up with the parameter itself and its gradient, the entire training process requires approximately 4 times the memory of the model weights, which poses a severe challenge to large model training.</p><p>To reduce memory consumption, memory-efficient optimizers are mainly designed through the following strategies:</p><ul><li><strong>Reduce the Number of State Variables:</strong> Only save necessary statistical information instead of complete matrices;</li><li><strong>Reduce the Precision of State Variables:</strong> Use FP16 or bfloat16 for storage;</li><li><strong>Share State Variables:</strong> Share part of the state information between multiple parameters.</li></ul><h4 id=adafactor>Adafactor<a hidden class=anchor aria-hidden=true href=#adafactor>#</a></h4><p>Adafactor (<a href=https://arxiv.org/abs/1804.04235>Shazeer et al. 2018</a>) is a memory-efficient adaptive learning rate optimizer. Unlike Adam, Adafactor does not store the complete second-order moment estimation matrix, but only stores two vectors (row and column statistics) to replace the complete second-order moment matrix, which significantly reduces memory footprint, especially suitable for scenarios where the parameter matrix has a low-rank structure.</p><h4 id=sm3>SM3<a hidden class=anchor aria-hidden=true href=#sm3>#</a></h4><p>SM3 (Sparse Momentum for Massive Models) (<a href=https://arxiv.org/abs/1905.11286>Anil et al. 2019</a>) provides a memory-efficient adaptive optimization scheme through sparse updates and state sharing.</p><ul><li><strong>Sparse Momentum:</strong> Only update Momentum for parameters with non-zero gradients, thereby reducing computation and storage overhead;</li><li><strong>State Sharing:</strong> To a certain extent, allow different parameters to share state variables, further reducing memory consumption;</li><li><strong>Adaptive Learning Rate:</strong> Dynamically adjust the learning rate according to the gradients of each parameter, improving the stability and convergence speed of model training.</li></ul><p>Okay, here is the English translation of the provided text about LoRA:</p><h3 id=lora>LoRA<a hidden class=anchor aria-hidden=true href=#lora>#</a></h3><p>LoRA (Low-Rank Adaptation) (<a href=https://arxiv.org/abs/2106.09685>Hu et al. 2021</a>) proposes a method that introduces <strong>low-rank adapters</strong> alongside pre-trained weights. This enables efficient fine-tuning by adding only a small number of parameters, while preserving the original inference capabilities of the pre-trained model.</p><p>The figure below intuitively illustrates the principle and initialization strategy of LoRA:</p><figure class=align-center><img loading=lazy src=lora.png#center alt="Fig. 31. An illustration of LoRA. (Image source: Hu et al. 2021)" width=70%><figcaption><p>Fig. 31. An illustration of LoRA. (Image source: <a href=https://arxiv.org/abs/2106.09685>Hu et al. 2021</a>)</p></figcaption></figure><p>In the standard forward pass, the model output is</p>$$
h = W_0 x,
$$<p>With LoRA introduced, the output becomes</p>$$
h = W_0 x + \Delta W x = W_0 x + B A x.
$$<p>Where:</p><ul><li><strong>$A \in \mathbb{R}^{r \times k}$ (Down-projection matrix)</strong>: Maps the input from $k$ dimensions to a lower $r$ dimension;</li><li><strong>$B \in \mathbb{R}^{d \times r}$ (Up-projection matrix)</strong>: Maps the reduced-dimension features from $r$ dimensions back to the original $d$ dimensions;</li><li><strong>Input $x$:</strong> Has dimension $\mathbb{R}^{k}$;</li><li><strong>Original weight $W_0$:</strong> Has dimension $\mathbb{R}^{d \times k}$, thus $W_0 x \in \mathbb{R}^{d}$;</li></ul><p>Assuming the pre-trained weight matrix is</p>$$
\mathbf{W} \in \mathbb{R}^{d \times k},
$$<p>LoRA adds a low-rank update term to it, resulting in the new weight representation:</p>$$
\mathbf{W}' = \mathbf{W} + \alpha\, \mathbf{B}\mathbf{A},
$$<p>Where:</p><ul><li><strong>$\mathbf{A} \in \mathbb{R}^{r \times k}$ (Down-projection matrix)</strong>: Maps the input from $k$ dimensions to a lower $r$ dimension;</li><li><strong>$\mathbf{B} \in \mathbb{R}^{d \times r}$ (Up-projection matrix)</strong>: Maps the reduced-dimension features from $r$ dimensions back to the original $d$ dimensions;</li><li><strong>$r \ll \min(d, k)$ (Low rank dimension)</strong>: Typically chosen from $4$ to $16$, balancing model expressiveness with minimizing added parameters;</li><li><strong>$\alpha$ (Scaling factor)</strong>: Used to scale the low-rank update $\Delta \mathbf{W} = \mathbf{B}\mathbf{A}$, compensating for the potentially small numerical magnitude resulting from the low-rank decomposition (often set to $\alpha = 2 \times r$, e.g., $\alpha = 16$ when $r = 8$).</li></ul><p>During the fine-tuning process, the <strong>original weights $\mathbf{W}$ are frozen</strong>, and only $\mathbf{A}$ and $\mathbf{B}$ are updated. This significantly reduces the number of trainable and storable parameters.</p><p>To ensure that the update term $\Delta \mathbf{W} = \mathbf{B}\mathbf{A}$ introduced at the beginning of fine-tuning has minimal impact on the original model, the following initialization strategies are commonly used:</p><ol><li><p><strong>Initialization of the down-projection matrix $\mathbf{A}$</strong></p><ul><li><strong>Gaussian Initialization</strong>: Set $\mathbf{A} \sim \mathcal{N}(0,\sigma^2)$ (typically with a small $\sigma$, e.g., 0.02) to ensure the initial update is small enough not to severely disrupt the model&rsquo;s output.</li><li><strong>Kaiming (He) Initialization</strong>: Kaiming initialization is a weight initialization method designed specifically for deep networks, aiming to maintain stability of forward signals and backward gradients across layers. For LoRA, ensuring a small scale (or using an appropriate scaling factor $\alpha$) can make the initial $\Delta \mathbf{W}$ close to zero.</li></ul></li><li><p><strong>Initialization of the up-projection matrix $\mathbf{B}$</strong></p><ul><li>Typically, $\mathbf{B}$ is initialized as a zero matrix, so that initially $\mathbf{B}\mathbf{A} = 0$, further minimizing the impact on the original model.</li></ul></li></ol><p>Training with LoRA offers the following advantages:</p><ul><li><strong>Parameter Efficiency</strong>: Only introduces low-rank adapter parameters, reducing the total number of parameters that need to be trained and stored.</li><li><strong>Memory and Computation Efficiency</strong>: Freezes most pre-trained weights and updates only small-scale parameters during fine-tuning, significantly reducing memory footprint and computational overhead.</li><li><strong>No Additional Inference Latency</strong>: After training, the update term $\Delta \mathbf{W}$ can be merged back into the original weights ($\mathbf{W}&rsquo; = \mathbf{W} + \alpha \mathbf{B}\mathbf{A}$), so no extra computation is added during the inference phase.</li><li><strong>Module Selection Flexibility</strong>: Using parameters like <code>--lora_target</code> or <code>--lora-target</code>, users can specify applying LoRA updates only to specific linear modules. Supported target modules include: <code>q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj</code>. This design allows users to selectively fine-tune key modules based on specific task requirements, further enhancing fine-tuning efficiency and adaptability.</li></ul><h3 id=qlora>QLoRA<a hidden class=anchor aria-hidden=true href=#qlora>#</a></h3><p>QLoRA (<a href=https://arxiv.org/abs/2305.14314>Dettmers et al. 2023</a>) is a method for efficient fine-tuning of large-scale models based on LoRA combined with quantization ideas. Through the following three key improvements, it greatly reduces memory footprint while maintaining basically unchanged model accuracy:</p><ol><li><p><strong>4-bit Normal Float (NF4) Quantization</strong>
A block-based quantile quantization strategy is adopted to quantize the original model weights to 4 bits, thereby achieving significant storage compression with subtle loss of accuracy.</p></li><li><p><strong>Double Quantization</strong>
After performing quantization once on ordinary parameters, perform an additional quantization on the quantization constants to further reduce cache footprint.</p></li><li><p><strong>Paged Optimizer</strong>
When memory usage is too high, automatically transfer part of the optimization process to CPU memory, thereby alleviating GPU memory pressure and improving scalability.</p></li></ol><p>Different from traditional LoRA, which only reduces the number of parameters to be fine-tuned, QLoRA also <strong>compresses</strong> all weights through 4-bit quantization, thereby maximizing the reduction of memory footprint and data transmission overhead while ensuring near-original accuracy.</p><figure class=align-center><img loading=lazy src=qlora.png#center alt="Fig. 32. Different finetuning methods and their memory requirements. QLoRA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. (Image source: Dettmers et al. 2023)" width=100%><figcaption><p>Fig. 32. Different finetuning methods and their memory requirements. QLoRA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. (Image source: <a href=https://arxiv.org/abs/2305.14314>Dettmers et al. 2023</a>)</p></figcaption></figure><p>This method can be regarded as a further extension of LoRA: LoRA improves efficiency by reducing the number of weights that need to be fine-tuned, while QLoRA, on this basis, quantizes all weights (including the un-fine-tuned part) to 4-bit precision, achieving <strong>dual compression of storage and computation</strong> in general, which is suitable for efficient fine-tuning of LLMs in resource-constrained environments.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>Parallelism techniques and memory optimization strategies need to be weighed and selected according to the specific model structure, dataset size, hardware resources, and training goals. Usually, it is necessary to combine multiple technologies to effectively train large-scale models and achieve the best performance and efficiency.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Weng, Lilian, and Greg Brockman. <a href=https://openai.com/blog/techniques-for-training-large-neural-networks/>&ldquo;Techniques for training large neural networks.&rdquo;</a> OpenAI Blog, 2022.</p><p>[2] Shenggui Li, Siqi Mai. <a href=https://colossalai.org/docs/concepts/paradigms_of_parallelism/>&ldquo;Paradigms of Parallelism.&rdquo;</a> Colossal-AI Documentation, 2024.</p><p>[3] Li, Shen, et al. <a href=https://arxiv.org/abs/2006.15704>&ldquo;Pytorch distributed: Experiences on accelerating data parallel training.&rdquo;</a> arXiv preprint, 2020.</p><p>[4] Li, Mu, et al. <a href=https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf>&ldquo;Communication efficient distributed machine learning with the parameter server.&rdquo;</a> Advances in Neural Information Processing Systems 27, 2014.</p><p>[5] Huang, Yanping, et al. <a href=https://arxiv.org/abs/1811.06965>&ldquo;Gpipe: Efficient training of giant neural networks using pipeline parallelism.&rdquo;</a> Advances in Neural Information Processing Systems 32, 2019.</p><p>[6] Harlap, Aaron, et al. <a href=https://arxiv.org/abs/1806.03377>&ldquo;Pipedream: Fast and efficient pipeline parallel dnn training.&rdquo;</a> arXiv preprint, 2018.</p><p>[7] Narayanan, Deepak, et al. <a href=https://arxiv.org/abs/2006.09503>&ldquo;Memory-efficient pipeline-parallel dnn training.&rdquo;</a> International Conference on Machine Learning, PMLR, 2021.</p><p>[8] Shoeybi, Mohammad, et al. <a href=https://arxiv.org/abs/1909.08053>&ldquo;Megatron-lm: Training multi-billion parameter language models using model parallelism.&rdquo;</a> arXiv preprint, 2019.</p><p>[9] Narayanan, Deepak, et al. <a href=https://arxiv.org/abs/2104.04473>&ldquo;Efficient large-scale language model training on gpu clusters using megatron-lm.&rdquo;</a> Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021.</p><p>[10] Shazeer, Noam, et al. <a href=https://arxiv.org/abs/1701.06538>&ldquo;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.&rdquo;</a> arXiv preprint, 2017.</p><p>[11] Lepikhin, Dmitry, et al. <a href=https://arxiv.org/abs/2006.16668>&ldquo;Gshard: Scaling giant models with conditional computation and automatic sharding.&rdquo;</a> arXiv preprint, 2020.</p><p>[12] Fedus, William, Barret Zoph, and Noam Shazeer. <a href=https://arxiv.org/abs/2101.03961>&ldquo;Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.&rdquo;</a> Journal of Machine Learning Research 23.120, 2022: 1–39.</p><p>[13] Zhou, Yanqi, et al. <a href=https://arxiv.org/abs/2202.09368>&ldquo;Mixture-of-experts with expert choice routing.&rdquo;</a> Advances in Neural Information Processing Systems 35, 2022: 7103–7114.</p><p>[14] Li, Shenggui, et al. <a href=https://arxiv.org/abs/2105.13120>&ldquo;Sequence parallelism: Long sequence training from system perspective.&rdquo;</a> arXiv preprint, 2021.</p><p>[15] Korthikanti, Vijay Anand, et al. <a href=https://arxiv.org/abs/2205.05198>&ldquo;Reducing activation recomputation in large transformer models.&rdquo;</a> Proceedings of Machine Learning and Systems 5, 2023: 341–353.</p><p>[16] Jacobs, Sam Ade, et al. <a href=https://arxiv.org/abs/2309.14509>&ldquo;Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.&rdquo;</a> arXiv preprint, 2023.</p><p>[17] DeepSpeed. <a href=https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md>&ldquo;DeepSpeed Ulysses README.&rdquo;</a> GitHub repository.</p><p>[18] Rajbhandari, Samyam, et al. <a href=https://arxiv.org/abs/1910.02054>&ldquo;Zero: Memory optimizations toward training trillion parameter models.&rdquo;</a> SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, 2020.</p><p>[19] Microsoft Research. <a href=https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/>&ldquo;DeepSpeed: Extreme-scale model training for everyone.&rdquo;</a> 2020.</p><p>[20] Dubey, Abhimanyu, et al. <a href=https://arxiv.org/abs/2407.21783>&ldquo;The llama 3 herd of models.&rdquo;</a> arXiv preprint, 2024.</p><p>[21] Rhu, Minsoo, et al. <a href=https://arxiv.org/abs/1602.08124>&ldquo;vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design.&rdquo;</a> 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture(MICRO), IEEE, 2016.</p><p>[22] Chen, Tianqi, et al. <a href=https://arxiv.org/abs/1604.06174>&ldquo;Training deep nets with sublinear memory cost.&rdquo;</a> arXiv preprint, 2016.</p><p>[23] Micikevicius, Paulius, et al. <a href=https://arxiv.org/abs/1710.03740>&ldquo;Mixed precision training.&rdquo;</a> arXiv preprint, 2017.</p><p>[24] Jain, Animesh, et al. <a href=https://www.microsoft.com/en-us/research/uploads/prod/2018/04/fiddle-gist-isca18.pdf>&ldquo;Gist: Efficient data encoding for deep neural network training.&rdquo;</a> 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture(ISCA), IEEE, 2018.</p><p>[25] Kingma, Diederik P., and Jimmy Ba. <a href=https://arxiv.org/abs/1412.6980>&ldquo;Adam: A method for stochastic optimization.&rdquo;</a> arXiv preprint, 2014.</p><p>[26] Shazeer, Noam, and Mitchell Stern. <a href=https://arxiv.org/abs/1804.04235>&ldquo;Adafactor: Adaptive learning rates with sublinear memory cost.&rdquo;</a> International Conference on Machine Learning, PMLR, 2018.</p><p>[27] Ginsburg, Boris, et al. <a href=https://arxiv.org/abs/1905.11286>&ldquo;Stochastic gradient methods with layer-wise adaptive moments for training of deep networks.&rdquo;</a> arXiv preprint, 2019.</p><p>[28] Hu, Edward J., et al. <a href=https://arxiv.org/abs/2106.09685>&ldquo;LoRA: Low-rank adaptation of large language models.&rdquo;</a> ICLR, 2022: 3.</p><p>[29] Dettmers, Tim, et al. <a href=https://arxiv.org/abs/2305.14314>&ldquo;Qlora: Efficient finetuning of quantized llms.&rdquo;</a> Advances in Neural Information Processing Systems 36, 2023: 10088–10115.</p><p>[30] Weng, Lilian. <a href=https://lilianweng.github.io/posts/2021-09-25-train-large/>&ldquo;How to Train Really Large Models on Many GPUs?&rdquo;</a> Lil&rsquo;blog, 2021.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reprinting or citing the content of this article, please indicate the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui.(Mar 2025). Parallelism and Memory Optimization Techniques for Training Large Models.
<a href=https://syhya.github.io/posts/2025-03-01-train-llm>https://syhya.github.io/posts/2025-03-01-train-llm</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>syhya2025train-llm</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;Parallelism and Memory Optimization Techniques for Training Large Models&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;Mar&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/posts/2025-03-01-train-llm&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/llms/>LLMs</a></li><li><a href=https://syhya.github.io/tags/pre-training/>Pre-Training</a></li><li><a href=https://syhya.github.io/tags/distributed-training/>Distributed Training</a></li><li><a href=https://syhya.github.io/tags/memory-optimization/>Memory Optimization</a></li><li><a href=https://syhya.github.io/tags/data-parallelism/>Data Parallelism</a></li><li><a href=https://syhya.github.io/tags/model-parallelism/>Model Parallelism</a></li><li><a href=https://syhya.github.io/tags/pipeline-parallelism/>Pipeline Parallelism</a></li><li><a href=https://syhya.github.io/tags/tensor-parallelism/>Tensor Parallelism</a></li><li><a href=https://syhya.github.io/tags/sequence-parallelism/>Sequence Parallelism</a></li><li><a href=https://syhya.github.io/tags/hybrid-parallelism/>Hybrid Parallelism</a></li><li><a href=https://syhya.github.io/tags/heterogeneous-systems/>Heterogeneous Systems</a></li><li><a href=https://syhya.github.io/tags/moe/>MoE</a></li><li><a href=https://syhya.github.io/tags/zero/>ZeRO</a></li><li><a href=https://syhya.github.io/tags/lora/>LoRA</a></li><li><a href=https://syhya.github.io/tags/ai/>AI</a></li><li><a href=https://syhya.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://syhya.github.io/tags/ai-infrastructure/>AI Infrastructure</a></li></ul><nav class=paginav><a class=prev href=https://syhya.github.io/posts/2025-03-27-llm-agent/><span class=title>« Prev</span><br><span>Large Language Model Agents</span>
</a><a class=next href=https://syhya.github.io/posts/2025-02-08-dpo/><span class=title>Next »</span><br><span>LLMs Alignment: DPO</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism and Memory Optimization Techniques for Training Large Models on x" href="https://x.com/intent/tweet/?text=Parallelism%20and%20Memory%20Optimization%20Techniques%20for%20Training%20Large%20Models&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-01-train-llm%2f&amp;hashtags=LLMs%2cPre-training%2cDistributedTraining%2cMemoryOptimization%2cDataParallelism%2cModelParallelism%2cPipelineParallelism%2cTensorParallelism%2cSequenceParallelism%2cHybridParallelism%2cHeterogeneousSystems%2cMoE%2cZeRO%2cLoRA%2cAI%2cDeepLearning%2cAIInfrastructure"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism and Memory Optimization Techniques for Training Large Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-01-train-llm%2f&amp;title=Parallelism%20and%20Memory%20Optimization%20Techniques%20for%20Training%20Large%20Models&amp;summary=Parallelism%20and%20Memory%20Optimization%20Techniques%20for%20Training%20Large%20Models&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-01-train-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism and Memory Optimization Techniques for Training Large Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-01-train-llm%2f&title=Parallelism%20and%20Memory%20Optimization%20Techniques%20for%20Training%20Large%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism and Memory Optimization Techniques for Training Large Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-01-train-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism and Memory Optimization Techniques for Training Large Models on whatsapp" href="https://api.whatsapp.com/send?text=Parallelism%20and%20Memory%20Optimization%20Techniques%20for%20Training%20Large%20Models%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-01-train-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism and Memory Optimization Techniques for Training Large Models on telegram" href="https://telegram.me/share/url?text=Parallelism%20and%20Memory%20Optimization%20Techniques%20for%20Training%20Large%20Models&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-01-train-llm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism and Memory Optimization Techniques for Training Large Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=Parallelism%20and%20Memory%20Optimization%20Techniques%20for%20Training%20Large%20Models&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-03-01-train-llm%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>