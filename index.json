[{"content":"In August 2025, the AI field witnessed a period of intensive releases from OpenAI. Following GPT-2 (OpenAI, 2019) in 2019, OpenAI has once again contributed to the open-source community with its first open-weight large language model series, gpt-oss (OpenAI, 2025), available in 120B and 20B sizes. Shortly after, the highly anticipated next-generation flagship model, GPT-5 (OpenAI, 2025), was also officially launched. This series of releases not only marks a new high for open-source models in reasoning and agent capabilities but also reveals OpenAI\u0026rsquo;s latest advancements in model architecture, training methodologies, and safety alignment.\ngpt-oss gpt-oss (OpenAI, 2025) is OpenAI\u0026rsquo;s first open-weight language model series released since GPT-2, designed to provide the open-source community with powerful reasoning and tool-use capabilities. The series includes two versions, gpt-oss-120b and gpt-oss-20b, both released under the Apache 2.0 license.\nArchitecture Overview Fig. 1. A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B. (Image source: Raschka, 2025)\ngpt-oss is built upon the GPT series architecture and incorporates several mainstream technologies from recent years, including RMSNorm, SwiGLU, GQA, RoPE, YaRN, and MoE.\nThe table below provides a direct comparison of the differences between the GPT-OSS 20B and GPT-2 XL 1.5B models.\nFeature GPT-OSS 20B (2025) GPT-2 XL 1.5B (2019) Release Date 2025 2019 Model Size 20B parameters 1.5B parameters Active Parameters 3.5B (per inference) 1.5B (all activated) Vocabulary Size 200k tokens 50k tokens Embedding Dimension 2,880 1,600 Number of Transformer Layers 24 layers 48 layers Number of Attention Heads 64 25 Context Length 131k tokens 1,024 tokens Positional Encoding RoPE (Rotary Positional Embeddings) Absolute Positional Embeddings Attention Mechanism Grouped-Query Attention Multi-head Attention Feed-Forward Network SwiGLU activation + MoE GELU activation MoE Architecture 32 experts, 4 activated None Normalization Method RMSNorm (2 locations) LayerNorm (2 locations) Dropout None Yes Sliding Window Attention Used in every other layer(window of 128 tokens) None Training Features Includes Supervised Fine-Tuning + Reinforcement Learning Pre-training only Quantization Support MXFP4 (Can run on a single GPU) No special quantization License Apache 2.0 MIT Efficient Attention Mechanisms To maintain high efficiency while supporting a 128k long context, gpt-oss employs several advanced attention mechanisms.\nGrouped-Query Attention (GQA): gpt-oss has 64 query heads and 8 key/value heads, meaning every 8 query heads share a single K/V pair. This significantly reduces the size of the KV cache and memory bandwidth requirements, thereby substantially increasing inference throughput with almost no loss in model performance.\nSliding Window Attention: To further reduce computational complexity, gpt-oss draws inspiration from Longformer (Jiang et al., 2020) and Mistral (Jiang et al., 2023) by adopting a sliding window attention. Its Transformer layers alternate between Dense Attention and Locally Banded Sparse Attention. The latter is Sliding Window Attention, which limits the attention scope of each token to a fixed-size local window.\nFig. 2. Comparison between regular attention (left) and sliding window attention (right). (Image source: Jiang et al., 2023)\nIn gpt-oss, this window size is set to 128 tokens. This means that in a local attention layer, a token can only attend to the 128 tokens preceding it, not the entire context. This design reduces the computational complexity of attention from \\( O(L^2) \\) to \\( O(L \\cdot W) \\), where \\( L \\) is the sequence length and \\( W \\) is the window size. By alternating with full attention layers, the model can efficiently process local information while integrating global information through the full attention layers.\nAttention Sinks: The model introduces Attention Sinks (Xiao et al., 2023), which learns a bias \\( \\mathbf{s}_h \\) added to the attention scores. This allows initial tokens to be consistently attended to, helping to stabilize the attention distribution and prevent information loss in long-context scenarios. \\[ \\text{Attention}(Q, K, V)_h = \\text{softmax}\\left(\\frac{Q_h K_h^T}{\\sqrt{d_k}} + \\mathbf{s}_h\\right)V_h \\] Fig. 3. Illustration of StreamingLLM vs existing methods. (Image source: Xiao et al., 2023)\nThe figure above compares the performance and efficiency of StreamingLLM with three common long-text processing methods. Assume a language model is pre-trained on texts of length $L$ and needs to predict the $T$-th token during inference (where $T \\gg L$):\nDense Attention: Retains the key-values (KV) of all historical tokens and computes full attention. The time complexity is $O(T^2)$, and the cache size grows continuously. Performance drops significantly when the input length exceeds the pre-training length $L$. Window Attention: Caches only the KV of the most recent $L$ tokens. It is efficient for inference, but performance plummets once the information from early tokens is replaced. Sliding Window with Re-computation: Reconstructs the KV state from the most recent $L$ tokens each time a new token is generated. Although it performs well on long texts, the re-computation involves quadratic attention, leading to a high time complexity of $O(TL^2)$ and slow inference speed. This method combines attention sinks with recent tokens during computation, not only maintaining inference efficiency but also sustaining a stable attention distribution and low perplexity in long-text scenarios.\nMXFP4 Quantization Fig. 4. Faster MXFP4 Backpropagation via Stochastic Rounding and Hadamard Transform. (Image source: Tseng et al., 2025)\nTo enable the large model to run on consumer-grade hardware, gpt-oss uses the MXFP4 (Tseng et al., 2025) format to quantize the MoE weights. MXFP4 is a micro-scaling floating-point format that can effectively quantize weights to about 4.25 bits. Since MoE weights account for over 90% of the model\u0026rsquo;s total parameters, this method drastically compresses the model size, allowing the 120B model to fit into a single 80GB GPU and the 20B model to run on devices with 16GB of VRAM.\nTraining Pre-training: The model is pre-trained on a dataset of several trillion tokens of text, with a focus on STEM, coding, and general knowledge. To enhance safety, the pre-training data reuses the CBRN content filters from GPT-4o.\nPost-training (Reasoning \u0026amp; Tool Use): After pre-training, the model undergoes post-training using CoT RL techniques similar to those for OpenAI\u0026rsquo;s o3. The goal of this stage is to teach the model to:\nReason: Generate detailed Chain-of-Thought (CoT) to solve complex problems. Use Tools: Learn to call external tools (like web search, code execution) to enhance its capabilities. To achieve these advanced agent functionalities, OpenAI designed the Harmony Chat Format. This format introduces the concept of \u0026ldquo;channels\u0026rdquo; (e.g., analysis for CoT, commentary for tool calls, final for the final answer) and establishes a strict instruction hierarchy (System \u0026gt; Developer \u0026gt; User \u0026gt; Assistant \u0026gt; Tool) to ensure the model\u0026rsquo;s behavior is controllable.\nAdditionally, the model supports Variable Effort Reasoning. Users can set Reasoning: low/medium/high in the system prompt to trade off between latency and performance. A higher effort level generates a longer CoT, which typically leads to higher accuracy but also increased latency.\nFig. 5. Accuracy vs. average CoT length for different reasoning levels on AIME and GPQA benchmarks. (Image source: OpenAI, 2025)\nEvaluation Fig. 6. Main capabilities evaluations for gpt-oss series. (Image source: OpenAI, 2025)\nBenchmark results show that gpt-oss-120b\u0026rsquo;s accuracy surpasses that of OpenAI\u0026rsquo;s o3-mini and approaches o4-mini. Meanwhile, gpt-oss-20b, at only one-sixth the size, also demonstrates competitive performance.\nGPT-5 GPT-5 (OpenAI, 2025) is not a single model but a unified intelligent system. It is not a monolithic, massive model but a complex system of multiple specialized models and an intelligent routing mechanism working in concert to balance performance, speed, and cost.\nSystem Architecture Fig. 7. GPT-5 Unified System Architecture. (Image source: Latent Space, 2025)\nThe GPT-5 system consists of three core components:\ngpt-5-main: As the system\u0026rsquo;s default model, it is fast and efficient, handling the vast majority of user queries. It can be considered the successor to GPT-4o. gpt-5-thinking: Used for more complex problems that require deep thought. This model is activated when the user explicitly requests it (e.g., \u0026ldquo;think hard about this\u0026rdquo;) or when the system determines the task requires it. It can be seen as the successor to OpenAI\u0026rsquo;s o3. Real-time Router: This is a continuously trained decision-making model that quickly determines which model to assign a user request to based on various signals. Its decisions are based on: Conversation Type: Whether it\u0026rsquo;s a casual chat, Q\u0026amp;A, or a task-oriented conversation. Complexity: The difficulty of the question and the depth of reasoning required. Tool Needs: Whether external tools like web search or a code interpreter need to be called. Explicit Intent: Users can guide the router to select the deep reasoning model with explicit instructions (e.g., \u0026ldquo;think hard about this\u0026rdquo;). The router continuously learns from real user signals (such as user model-switching behavior, response preference rates, and measured answer correctness) to constantly optimize its decision-making capabilities.\nSafe Completions The traditional safety training paradigm is Hard Refusals, where the model decides whether to answer fully or refuse directly based on a binary classification of user intent (safe or unsafe). This approach is effective for clearly malicious prompts but is very brittle when dealing with ambiguous intent or topics involving dual-use (e.g., biology, cybersecurity), often leading to over-refusals.\nSafe Completions (Baker et al., 2025) moves away from binary classification of user intent and instead focuses on maximizing the model\u0026rsquo;s helpfulness while adhering to safety policies.\nFor clearly harmful requests: The model will still refuse. For dual-use requests (e.g., in biology, chemistry, cybersecurity): The model provides safe, high-level answers without directly executable details, rather than refusing entirely. For requests with ambiguous intent: The model attempts to complete the task in a safe manner or offers safe alternatives. This approach significantly improves the model\u0026rsquo;s safety and utility in dual-use domains and reduces unnecessary over-refusals.\nFig. 8. Left: Overall structure of the safe-completion training stack. Right: Details of the safecompletion reward design. (Image source: OpenAI, 2025)\nChain-of-Thought Monitoring OpenAI employs Chain-of-Thought Monitoring (CoT Monitoring) (Baker et al., 2025) to ensure the reliability and safety of its reasoning models and to prevent issues like reward hacking. Unlike some approaches that try to optimize CoT through SFT, GPT-5\u0026rsquo;s CoT training does not impose direct alignment. This allows the CoT to more genuinely reflect the model\u0026rsquo;s \u0026ldquo;thinking\u0026rdquo; process, serving as an effective window for detecting erroneous behavior, deceptive intent, or potential risks.\nFig. 9. Monitoring Frontier Reasoning Models for Reward Hacking. (Image source: Baker et al., 2025)\nThrough CoT monitoring, OpenAI found that the incidence of deceptive behavior in gpt-5-thinking was reduced to 2.1% from 4.8% in o3. This technology is crucial for understanding and mitigating the risks of advanced AI systems.\nEvaluation GPT-5 excels across multiple benchmarks, setting new standards, especially in reasoning, coding, and multimodal capabilities. Compared to its predecessors, it not only achieves a leap in accuracy but also makes significant strides in efficiency, often achieving or surpassing the performance of o3 with 50-80% less output.\nFig. 10. GPT-5 performance in SWE-bench Verified Software Engineering. (Image source: OpenAI, 2025)\n🧠 Intelligence Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano AIME ’25 94.6% 91.1% 85.2% 88.9% 92.7% 46.4% 40.2% - FrontierMath (with python tool only) 26.3% 22.1% 9.6% 15.8% 15.4% - - - GPQA diamond 85.7% 82.3% 71.2% 83.3% 81.4% 66.3% 65.0% 50.3% HLE 24.8% 16.7% 8.7% 20.2% 14.7% 5.4% 3.7% - HMMT 2025 93.3% 87.8% 75.6% 81.7% 85.0% 28.9% 35.0% - 🖼️ Multimodal Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano MMMU 84.2% 81.6% 75.6% 82.9% 81.6% 74.8% 72.7% 55.4% MMMU-Pro (avg across standard and vision sets) 78.4% 74.1% 62.6% 76.4% 73.4% 60.3% 58.9% 33.0% CharXiv reasoning (python enabled) 81.1% 75.5% 62.7% 78.6% 72.0% 56.7% 56.8% 40.5% VideoMMMU (max frame 256) 84.6% 82.5% 66.8% 83.3% 79.4% 60.9% 55.1% 30.2% ERQA 65.7% 62.9% 50.1% 64.0% 56.5% 44.3% 42.3% 26.5% 💻 Coding Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano SWE-Lancer: IC SWE Diamond Freelance Coding Tasks $112K $75K $49K $86K $66K $34K $31K $9K SWE-bench Verified 74.9% 71.0% 54.7% 69.1% 68.1% 54.6% 23.6% - Aider polyglot (diff) 88.0% 71.6% 48.4% 79.6% 58.2% 52.9% 31.6% 6.2% 📋 Instruction Following Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano Scale multichallenge (o3-mini grader) 69.6% 62.3% 54.9% 60.4% 57.5% 46.2% 42.2% 31.1% Internal API instruction following eval (hard) 64.0% 65.8% 56.1% 47.4% 44.7% 49.1% 45.1% 31.6% COLLIE 99.0% 98.5% 96.9% 98.4% 96.1% 65.8% 54.6% 42.5% 🔧 Function Calling Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano Tau²-bench airline 62.6% 60.0% 41.0% 64.8% 60.2% 56.0% 51.0% 14.0% Tau²-bench retail 81.1% 78.3% 62.3% 80.2% 70.5% 74.0% 66.0% 21.5% Tau²-bench telecom 96.7% 74.1% 35.5% 58.2% 40.5% 34.0% 44.0% 12.1% 📚 Long Context Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini GPT-4.1 nano OpenAI-MRCR: 2 needle 128k 95.2% 84.3% 43.2% 55.0% 56.4% 57.2% 47.2% 36.6% OpenAI-MRCR: 2 needle 256k 86.8% 58.8% 34.9% - - 56.2% 45.5% 22.6% Graphwalks bfs \u0026lt;128k 78.3% 73.4% 64.0% 77.3% 62.3% 61.7% 61.7% 25.0% Graphwalks parents \u0026lt;128k 73.3% 64.3% 43.8% 72.9% 51.1% 58.0% 60.5% 9.4% BrowseComp Long Context 128k 90.0% 89.4% 80.4% 88.3% 80.0% 85.9% 89.0% 89.4% BrowseComp Long Context 256k 88.8% 86.0% 68.4% - - 75.5% 81.6% 19.1% VideoMME (long, with subtitle category) 86.7% 78.5% 65.7% 84.9% 79.5% 78.7% 68.4% 55.2% 🚨 Hallucinations Benchmark GPT-5 (high) GPT-5 mini (high) GPT-5 nano (high) OpenAI o3 (high) OpenAI o4-mini (high) GPT-4.1 GPT-4.1 mini LongFact-Concepts hallucination rate (no tools) 1.0% 0.7% 1.0% 5.2% 3.0% 0.7% 1.1% LongFact-Objects hallucination rate (no tools) 1.2% 1.3% 2.8% 6.8% 8.9% 1.1% 1.8% FActScore hallucination rate (no tools) 2.8% 3.5% 7.3% 23.5% 38.7% 6.7% 10.9% These results indicate that GPT-5 has made significant improvements in complex tasks requiring deep reasoning (like GPQA, AIME) and agentic tasks that require interaction with external environments (like SWE-bench, τ²-bench). At the same time, the substantial improvement in factual accuracy (hallucination rate reduced by nearly 8x) makes it more reliable for practical applications.\nReferences [1] Raschka, S. (2025). \u0026ldquo;From GPT-2 to gpt-oss: Analyzing the Architectural Advances.\u0026rdquo; Ahead of AI.\n[2] Radford, Alec, et al. \u0026ldquo;Language models are unsupervised multitask learners.\u0026rdquo; OpenAI blog 1.8 (2019): 9.\n[3] OpenAI. (2025). \u0026ldquo;Introducing gpt-oss.\u0026rdquo; OpenAI Blog.\n[4] OpenAI. (2025). \u0026ldquo;Introducing GPT-5.\u0026rdquo; OpenAI Blog.\n[5] OpenAI. (2025). \u0026ldquo;gpt-oss-120b \u0026amp; gpt-oss-20b Model Card.\u0026rdquo;\n[6] Beltagy, Iz, Matthew E. Peters, and Arman Cohan. \u0026ldquo;Longformer: The long-document transformer.\u0026rdquo; arXiv preprint arXiv:2004.05150 (2020).\n[7] Jiang, Dongsheng, et al. \u0026ldquo;Mistral 7B.\u0026rdquo; arXiv preprint arXiv:2310.08825 (2023).\n[8] Xiao, G., et al. (2023). \u0026ldquo;Efficient Streaming Language Models with Attention Sinks.\u0026rdquo; arXiv preprint arXiv:2309.17453.\n[9] Tseng, Albert, Tao Yu, and Youngsuk Park. \u0026ldquo;Training llms with mxfp4.\u0026rdquo; arXiv preprint arXiv:2502.20586 (2025).\n[10] Yuan, Yuan, et al. \u0026ldquo;From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training.\u0026rdquo; arXiv preprint arXiv:2508.09224 (2025).\n[11] B. Baker, J. Huizinga, L. Gao, Z. Dou, M. Y. Guan, A. Madry, W. Zaremba, J. Pachocki, and D. Farhi, \u0026ldquo;Monitoring reasoning models for misbehavior and the risks of promoting obfuscation.\u0026rdquo; arXiv preprint arXiv:2503.11926, 2025. Submitted on 14 March 2025.\n[12] OpenAI. (2025). \u0026ldquo;GPT-5 System Card.\u0026rdquo;\n[13] OpenAI. (2025). \u0026ldquo;Introducing GPT-5 for developers.\u0026rdquo; OpenAI Blog.\nCitation Citation: When reproducing or citing the content of this article, please credit the original author and source.\nCited as:\nYue Shui. (Aug 2025). gpt-oss \u0026amp; GPT-5. https://syhya.github.io/posts/2025-08-24-gpt5\nOr\n@article{yue_shui_gpt5_2025 title = \u0026#34;gpt-oss \u0026amp; GPT-5\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Aug\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-08-24-gpt5\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-08-24-gpt5/","summary":"\u003cp\u003eIn August 2025, the AI field witnessed a period of intensive releases from OpenAI. Following \u003cstrong\u003eGPT-2\u003c/strong\u003e (\u003ca href=\"https://openai.com/index/better-language-models/\"\u003eOpenAI, 2019\u003c/a\u003e) in 2019, OpenAI has once again contributed to the open-source community with its first open-weight large language model series, \u003cstrong\u003egpt-oss\u003c/strong\u003e (\u003ca href=\"https://openai.com/index/introducing-gpt-oss/\"\u003eOpenAI, 2025\u003c/a\u003e), available in 120B and 20B sizes. Shortly after, the highly anticipated next-generation flagship model, \u003cstrong\u003eGPT-5\u003c/strong\u003e (\u003ca href=\"https://openai.com/index/introducing-gpt-5/\"\u003eOpenAI, 2025\u003c/a\u003e), was also officially launched. This series of releases not only marks a new high for open-source models in reasoning and agent capabilities but also reveals OpenAI\u0026rsquo;s latest advancements in model architecture, training methodologies, and safety alignment.\u003c/p\u003e","title":"gpt-oss \u0026 GPT-5"},{"content":"In recent years, Large Language Models (LLMs) have achieved revolutionary breakthroughs in fields such as natural language processing, code generation, and even multimodal interaction. However, the powerful capabilities of these models come at the cost of enormous computational and memory overhead, especially during the inference stage. Efficiently deploying and running these models, which have billions or even trillions of parameters, has become a core challenge in scaling LLM technology for real-world applications.\nThe challenges of LLM inference primarily stem from two aspects:\nHuge Memory Footprint: In addition to the model parameters themselves, the inference process requires storing a large amount of intermediate state, especially the KV Cache. For example, for a request with a batch size of 512 and a sequence length of 2048, its KV cache can be as large as 3TB, several times the size of the model itself. Furthermore, the computational complexity of self-attention grows quadratically with the sequence length. Low parallelizability: The text generation of LLMs is inherently an autoregressive process, meaning tokens are generated one by one, with the generation of the next token depending on all previously generated tokens. This serial nature makes the decoding process difficult to parallelize efficiently. Principles of Token Generation To better understand the optimization techniques that follow, we first need to understand how large models generate text and identify the key bottlenecks in their inference process.\nAutoregressive Generation Mainstream large language models like GPT use a Decoder-Only Transformer architecture and generate text in an autoregressive manner. The basic idea is that the probability of a text sequence can be decomposed into a product of a series of conditional probabilities. Given an initial context word sequence $W_0$ (usually the user\u0026rsquo;s input prompt), the model predicts the next word (token) one at a time, adding the newly generated word to the context as input for the next prediction step. This process can be represented by the following formula:\n$$ P(w_{1:T} | W_0) = \\prod_{t=1}^{T} P(w_t | w_{1:t-1}, W_0), \\text{ with } w_{1:0} = \\emptyset $$where $w_t$ is the word generated at time step $t$, and $w_{1:t-1}$ is the sequence of all words generated before time step $t$. The generation process continues until the model produces a special end-of-sequence (EOS) token or reaches a predefined maximum length $T$.\nPrefilling and Decoding The autoregressive nature of generation dictates that LLM inference can be clearly divided into two stages: the Prefilling stage and the Decoding stage.\nFig. 1. The Prefilling and Decoding Stages of LLM Inference. (Image source: Zhou et al., 2024)\nPrefilling Stage: In this stage, the model processes the entire input prompt in parallel (e.g., \u0026ldquo;I, like, natural, language\u0026rdquo; in Figure 1) and computes the probability distribution for the first output token (\u0026ldquo;Processing\u0026rdquo;). This stage is characterized by high parallelism, as all input tokens can be fed into the Transformer model at once. This allows compute-intensive operations (like matrix multiplication) to fully utilize the parallel processing power of GPUs, making it compute-bound.\nDecoding Stage: In this stage, the model generates subsequent tokens one by one. Each time a token is generated, it is appended to the end of the existing sequence and used as input for the next prediction. This process is serial because the generation of the next token depends on the previous one. Consequently, this stage is memory-bound, with the main bottleneck being the loading of the massive model weights from GPU memory, rather than the computation itself.\nFig. 2. Illustration of the memory variation through time (latency) during one generation process. Note that the author ignores the activation size in this figure for simplification. (Image source: Zhou et al., 2024)\nTo accelerate the decoding process, modern LLM inference frameworks widely adopt the KV Cache technique. In the Transformer\u0026rsquo;s self-attention mechanism, each token needs to interact with all preceding tokens. To avoid recomputing the Key (K) and Value (V) vectors for all previous tokens when generating each new token, the system caches these computed K and V values. This cache is the KV Cache.\nAs shown in Figure 2, the size of the KV Cache grows linearly as the generated sequence lengthens. For a model with billions of parameters and long sequences, the KV Cache can occupy several gigabytes or even tens of gigabytes of VRAM. This makes VRAM the scarcest resource in LLM inference, severely limiting the number of requests the system can handle simultaneously (i.e., the batch size), which directly impacts inference throughput. Therefore, how to efficiently manage and optimize the KV Cache is one of the core problems in LLM inference optimization.\nDecoding Strategies At each decoding step, the model outputs a probability distribution over the entire vocabulary. The method used to select the next token from this distribution is determined by the decoding strategy (or token generation strategy). Different strategies significantly affect the quality, creativity, and coherence of the generated text.\nGreedy Search Greedy search is the simplest decoding strategy. At each time step $t$, it always selects the token with the highest probability as the output:\n$$ w_t = \\underset{w}{\\operatorname{argmax}} P(w | w_{1:t-1}) $$This method greatly reduces computational complexity and produces results quickly, but it has clear limitations. Because it only makes locally optimal choices at each step, greedy search can easily get stuck in local optima, overlooking globally better possibilities. This often results in generated text that is dull, repetitive, and lacks diversity and creativity.\nFig. 3. At each time step, greedy search selects the token with the highest conditional probability. (Image source: d2l-en, 2019)\nCode Implementation:\nimport torch import torch.nn.functional as F def greedy_search(model, input_ids, max_len=20, eos_token_id=2): \u0026#34;\u0026#34;\u0026#34; A simple implementation of Greedy Search. `model` should be a function that takes input_ids and returns logits. \u0026#34;\u0026#34;\u0026#34; generated_sequence = input_ids for _ in range(max_len): # Get logits for the last token logits = model(generated_sequence) next_token_logits = logits[:, -1, :] # Select the token with the highest probability next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1) # Append the new token to the sequence generated_sequence = torch.cat([generated_sequence, next_token], dim=1) # Stop if EOS token is generated if next_token.item() == eos_token_id: break return generated_sequence Beam Search To overcome the local optima problem of greedy search, beam search maintains $k$ (num_beams or beam width) most likely candidate sequences (called \u0026ldquo;beams\u0026rdquo;) at each decoding step. In the next step, it expands based on these $k$ sequences and again selects the $k$ new sequences with the highest overall probability. Finally, the algorithm selects the candidate sequence with the highest overall probability from all completed sequences as the final output.\nFig. 4. The process of beam search (beam size $=2$; maximum length of an output sequence $=3$ ). The candidate output sequences are $A, C, A B, C E, A B D$, and $C E D$. (Image source: d2l-en, 2019)\nThis approach expands the search space, effectively reducing the impact of local optima and typically generating higher-quality, more coherent text. However, the essence of beam search is still to choose the path with the highest overall probability, which makes it prone to producing high-frequency, common expressions in open-ended generation tasks, potentially lacking creativity and diverse output.\nTemperature Sampling Fig. 5. Illustration of Temperature Sampling. (Image source: Big Hummingbird Blogs, 2024)\nUnlike deterministic search methods, sampling methods introduce randomness, making the generated text more diverse and creative. The most basic sampling method is to sample directly from the model\u0026rsquo;s probability distribution. Temperature sampling adjusts the shape of the original probability distribution using a temperature coefficient $T$, applied to the Softmax function. The temperature coefficient controls the flatness of the token probability distribution output by the LLM. A higher temperature makes the distribution flatter and the output more random, while a lower temperature makes the distribution more extreme and the output more stable.\n$$ P_T(w_i) = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)} $$where $z_i$ is the logit output by the model for word $w_i$.\nWhen $T \\to 1$, the probability distribution remains unchanged. When $T \u0026lt; 1$ (cooling), the distribution becomes \u0026ldquo;sharper,\u0026rdquo; making high-probability words more likely to be selected, and the generated result is closer to greedy search. When $T \u0026gt; 1$ (heating), the distribution becomes \u0026ldquo;flatter,\u0026rdquo; giving low-probability words a chance to be selected, resulting in more diverse and random output. Top-K Sampling Top-K sampling (Fan et al., 2018) retains only the $K$ most probable candidate words before sampling, then renormalizes and samples from this set of $K$ words. This effectively prevents the model from sampling from extremely low-probability words, avoiding the generation of incoherent text. However, its drawback is that the value of $K$ is fixed and cannot adapt dynamically to different probability distributions.\nFig. 6. Illustration of Top-K Sampling. (Image source: Big Hummingbird Blogs, 2024)\nTop-p (Nucleus) Sampling Top-p sampling (Holtzman et al., 2019) uses a dynamic method to select the set of candidate words. It starts with the most probable word and accumulates their probabilities until the sum exceeds a preset threshold $p$ (e.g., 0.9). The model then samples only from this dynamically generated, minimal set of candidate words $V_{\\text{top-p}}$. This method balances text coherence and creativity and is currently one of the most commonly used and effective strategies for open-ended text generation.\nFig. 7. Illustration of Top-p Sampling. (Image source: Big Hummingbird Blogs, 2024)\nCombined Sampling Code Implementation (Top-K, Top-p, Temperature):\nimport torch import torch.nn.functional as F @torch.no_grad() def generate_with_sampling(model, idx, max_new_tokens, temperature=1.0, top_k=None, top_p=None, eos_token_id=2): for _ in range(max_new_tokens): # Crop context if it\u0026#39;s too long idx_cond = idx if idx.size(1) \u0026lt;= model.config.max_position_embeddings else idx[:, -model.config.max_position_embeddings:] # Forward pass to get logits logits = model(idx_cond).logits[:, -1, :] # Apply temperature if temperature \u0026gt; 0: logits = logits / temperature # Apply Top-K filtering if top_k is not None: v, _ = torch.topk(logits, min(top_k, logits.size(-1))) logits[logits \u0026lt; v[:, [-1]]] = -float(\u0026#39;Inf\u0026#39;) # Apply Top-p (Nucleus) filtering if top_p is not None: sorted_logits, sorted_indices = torch.sort(logits, descending=True) cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) # Remove tokens with cumulative probability above the threshold sorted_indices_to_remove = cumulative_probs \u0026gt; top_p # Shift the indices to the right to keep the first token above the threshold sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 indices_to_remove = sorted_indices[sorted_indices_to_remove] logits.scatter_(1, indices_to_remove, -float(\u0026#39;Inf\u0026#39;)) # Convert logits to probabilities and sample probs = F.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) # Append sampled index and check for EOS idx = torch.cat((idx, idx_next), dim=1) if idx_next.item() == eos_token_id: break return idx Speculative Decoding Speculative Decoding (Leviathan et al., 2023) is an innovative acceleration technique that aims to achieve the generation quality of a large model at the speed of a small model, thereby reducing latency without sacrificing quality.\nIt uses a small, fast Draft Model to generate multiple (e.g., $k$) candidate tokens at once. Then, the large Target Model performs a single forward pass to validate these $k$ tokens in parallel. If the tokens predicted by the draft model match those of the target model, they are accepted, effectively generating multiple tokens with a single forward pass. If they don\u0026rsquo;t match, the subsequent predictions from the draft model are discarded, and the target model\u0026rsquo;s prediction is used for correction.\nFig. 8. Overview of online speculative decoding (OSD) framework: For each prompt, the draft model suggests multiple tokens and the target model performs the verification. (Image source: Liu et al., 2024)\nAs long as there is some consistency between the predictions of the draft and target models, speculative decoding can significantly reduce generation latency. Variations include Self-speculative decoding, which uses the early layers of the model itself as the draft model.\nHeuristic Strategies Best-of-N / Majority Vote: These methods improve the quality and robustness of the final result by generating multiple candidate answers. Best-of-N: The LLM generates N answers, which are then scored by an independent evaluation model (Verifier) or a Reward Model. The answer with the highest score (Best) is selected as the final output. Majority Vote / Self-Consistency: The LLM generates multiple different reasoning paths (Chain-of-Thought) and answers for the same question. The most consistent answer is then selected through a majority vote. This method is particularly effective for tasks requiring complex reasoning. Overview of Optimization Methods Now that we understand the basic principles of inference, let\u0026rsquo;s delve into how to optimize this process. The main goals of inference optimization are to reduce latency, increase throughput, and decrease memory footprint. Existing techniques can be broadly categorized into three areas: model compression, memory and computation optimization, and efficient model architectures.\nTypically, the goals of model inference optimization include:\nReducing the model\u0026rsquo;s memory footprint by using fewer GPU devices and less VRAM. Reducing computational complexity by decreasing the number of floating-point operations (FLOPs) required. Reducing inference latency to make the model run faster. To reduce the cost of inference in terms of memory and time, several methods can be employed:\nApplying various parallelization techniques to scale the model across a large number of GPUs. By intelligently parallelizing model components and data, it\u0026rsquo;s possible to run models with trillions of parameters. Memory Offloading, which moves temporarily unused data to the CPU and reads it back when needed. This helps reduce memory usage but increases latency. Smart Batching Strategies; for example, EffectiveTransformer packs consecutive sequences together to eliminate padding in batches. Network compression techniques, such as pruning, quantization, and distillation. Models with fewer parameters or lower bit-width naturally require less memory and run faster. Improvements specific to model architectures. Many architectural changes, especially to the attention layer, help speed up Transformer decoding. You can refer to a previous post on Large Model Training for different types of training parallelization and memory-saving designs, including CPU memory offloading. This article will focus on network compression techniques and architectural improvements for Transformer models.\nKnowledge Distillation Knowledge Distillation (KD) (Hinton et al., 2015) is a direct method for building a smaller model to accelerate inference by transferring the knowledge from a pre-trained, expensive model (the \u0026ldquo;teacher model\u0026rdquo;) to a smaller, cheaper model (the \u0026ldquo;student model\u0026rdquo;). There are few restrictions on the student model\u0026rsquo;s architecture, other than requiring its output space to match the teacher\u0026rsquo;s to construct a suitable learning objective.\nFig. 9. The generic framework of teacher-student knowledge distillation training. (Image source: Gou et al., 2020)\nGiven a dataset, the student model learns to mimic the teacher\u0026rsquo;s output through a distillation loss function. Neural networks typically have a softmax layer; for example, an LLM outputs a probability distribution over tokens. Let $\\mathbf{z}_t$ and $\\mathbf{z}_s$ be the pre-softmax logits of the teacher and student models, respectively. The distillation loss minimizes the difference between the two softmax outputs, both with a high temperature $T$. When ground truth labels $\\mathbf{y}$ are available, we can combine this with a supervised learning objective (e.g., cross-entropy) that operates on the ground truth labels and the student\u0026rsquo;s soft logits.\n$$ \\mathcal{L}_{\\mathrm{KD}}=\\mathcal{L}_{\\text {distill }}\\left(\\operatorname{softmax}\\left(\\mathbf{z}_t, T\\right), \\operatorname{softmax}\\left(\\mathbf{z}_s, T\\right)\\right)+\\lambda \\mathcal{L}_{\\mathrm{CE}}\\left(\\mathbf{y}, \\mathbf{z}_s\\right) $$where $\\lambda$ is a hyperparameter that balances learning from soft and hard targets. A common choice for $\\mathcal{L}_{\\text {distill}}$ is KL-divergence or cross-entropy.\nAn early success story is DistilBERT (Sanh et al. 2019), which reduced BERT\u0026rsquo;s parameters by 40% while retaining 97% of its performance on downstream fine-tuning tasks and running 71% faster. DistilBERT\u0026rsquo;s pre-training loss is a combination of a soft distillation loss, a supervised training loss (masked language modeling loss $\\mathcal{L}_{\\text{MLM}}$ in BERT), and a special cosine embedding loss to align the hidden state vectors of the teacher and student models.\nFig. 10. The performance of DistilBERT (Image source: Sanh et al., 2019)\nDistillation can be easily combined with quantization, pruning, or sparsification techniques, where the teacher model is the original full-precision, dense model, and the student model is quantized, pruned, or sparsified to achieve higher sparsity.\nQuantization To further improve model performance during inference, we can go beyond low-precision floating-point numbers and use quantization. Quantization converts the model\u0026rsquo;s floating-point weights into low-bit integer representations, such as 8-bit integers (INT8) or even 4-bit integers (INT4).\nThere are generally two ways to apply quantization to deep neural networks:\nPost-Training Quantization (PTQ): The model is first trained to convergence, and then its weights are converted to a lower precision without further training. This method is typically low-cost to implement compared to training. Quantization-Aware Training (QAT): Quantization is applied during pre-training or further fine-tuning. QAT can achieve better performance but requires additional computational resources and access to representative training data. Precision Comparison In the field of deep learning, numerical precision determines the delicate balance between computational speed and model performance. Understanding the pros and cons of different floating-point and integer formats is key to optimizing the performance of large-scale models. Floating-point numbers are represented in a computer with three parts:\nSign: Indicates whether the number is positive or negative. Exponent: Determines the dynamic range of the number. Mantissa (or Significand): Determines the precision of the number. For convenience, we often refer to the mantissa as the fraction. Fig. 11. fp32 vs fp16 vs bf16 (Image source: Raschka, 2023)\nType Total Bits Sign Bits Exponent Bits Mantissa Bits Characteristics FP64 (Double-precision) 64 1 11 52 Extremely high precision, widely used in scientific computing, but computationally expensive and memory-intensive, rarely used in deep learning. FP32 (Single-precision) 32 1 8 23 Standard format for deep learning training, moderate speed, larger memory footprint. FP16 (Half-precision) 16 1 5 10 Faster computation, half the memory footprint of FP32, but limited dynamic range, prone to numerical overflow. BF16 (Brain Floating Point) 16 1 8 7 Same dynamic range as FP32, avoids overflow, better suited for LLMs, slightly lower precision than FP16. While pure FP16 precision is fast and memory-efficient, its limited dynamic range makes it highly susceptible to numerical overflow and underflow, which can make training unstable or even prevent convergence. Therefore, using Mixed-Precision Training is crucial.\nQuantization maps floating-point numbers to integers, further reducing computational complexity and memory footprint. Specifically:\nINT8: Occupies only 1/4 of the memory of FP32, significantly accelerating inference speed, but may slightly reduce model accuracy. INT4: A more extreme compression scheme, better suited for devices with extremely limited resources or inference scenarios requiring very high throughput. Challenges in Transformer Quantization Many studies on Transformer model quantization share a common finding: simple low-precision (e.g., 8-bit) post-training quantization leads to a significant performance drop. This is mainly due to the high dynamic range of activation values, and a simple activation quantization strategy cannot maintain the model\u0026rsquo;s performance.\nFig. 12. Only quantizing model weights to 8-bit while keeping activation at full precision (W8A32) achieves much better results when activations are quantized to 8-bit irrespective of whether weights are in lower precision (W8A8 and W32A8). (Image source: Bondarenko et al. 2021)\nBondarenko et al. (2021) found in experiments with small BERT models that the input and output of the FFN (feed-forward network) have very different dynamic ranges due to strong outliers in the output tensor. Therefore, per-tensor quantization of the FFN\u0026rsquo;s residual sum can lead to significant errors.\nAs model sizes grow to billions of parameters, large-magnitude outlier features begin to appear in all Transformer layers, causing simple low-bit quantization to fail. Researchers observed this phenomenon in the OPT (Zhang et al. 2022) model, which is larger than 6.7B parameters. Larger models have more layers with extreme outliers, and these outlier features have a significant impact on model performance. In a few dimensions, the magnitude of activation outliers can be about 100 times larger than most other values.\nFig. 13. The mean zero-shot accuracy over a set of language tasks (WinoGrande, HellaSwag, PIQA, LAMBADA) of OPT models of increasing sizes. (Image source: Dettmers et al. 2022)\nPost-Training Quantization (PTQ) Mixed-precision quantization The most direct way to solve the aforementioned quantization challenges is to implement quantization with different precisions for weights and activations.\nGOBO (Zadeh et al. 2020) was one of the first models to apply post-training quantization to BERT. It assumes that the model weights of each layer follow a Gaussian distribution and thus detects outliers by tracking the mean and standard deviation of each layer. Outlier features are kept in their original form, while other values are divided into multiple bins, storing only the corresponding bin index and centroid value.\nFig. 14. The pseudocode for the GOBO algorithm. (Image source: Zadeh et al. 2020)\nBased on the observation that only certain activation layers in BERT (e.g., the residual connection after the FFN) cause large performance drops, Bondarenko et al. (2021) adopted mixed-precision quantization, using 16-bit quantization for problematic activations and 8-bit for others.\nLLM.int8() (Dettmers et al. 2022) achieves mixed-precision quantization through two mixed-precision decompositions:\nSince matrix multiplication consists of a series of independent inner products between row and column vectors, we can apply independent quantization to each inner product: each row and column is scaled by its absolute maximum value and then quantized to INT8. Outlier activation features (e.g., 20 times larger than other dimensions) are kept in FP16 format, but they only account for a small fraction of the total weights. How to identify outliers is empirical. Fig. 15. Two mixed-precision decompositions of LLM.int8(). (Image source: Dettmers et al. 2022)\nQuantization at fine-grained granularity Fig. 16. Comparison of quantization at different granularities. $d$ is the model size / hidden state dimension and $h$ is the number of heads in one MHSA (multi-head self-attention) component. (Image source: Lilian, 2023)\nSimply quantizing the entire weight matrix of a layer (\u0026ldquo;per-tensor\u0026rdquo; or \u0026ldquo;per-layer\u0026rdquo; quantization) is the easiest to implement but cannot achieve good quantization granularity.\nQ-BERT (Shen, et al. 2020) applies group-wise quantization to a fine-tuned BERT model, treating the individual matrix $W$ corresponding to each head in the MHSA (multi-head self-attention) as a group, and then applies Hessian-based mixed-precision quantization.\nPer-embedding group (PEG) (Bondarenko et al. 2021) activation quantization is motivated by the observation that outliers only appear in a few dimensions of the $d$ (hidden state/model size) dimension. Per-embedding quantization is quite computationally expensive. In contrast, PEG quantization divides the activation tensor into several uniformly sized groups along the embedding dimension, where elements in the same group share quantization parameters. To ensure that all outliers are assigned to the same group, they apply a deterministic range-based permutation of the embedding dimensions, where dimensions are sorted by their value range.\nZeroQuant (Yao et al. 2022) uses group-wise quantization for weights (same as Q-BERT) and token-wise quantization for activations. To avoid expensive quantization and dequantization computations, ZeroQuant builds custom kernels that fuse the quantization operation with its preceding operation.\nSecond-order information for quantization Q-BERT (Shen, et al. 2020) developed Hessian AWare Quantization (HAWQ) (Dong, et al. 2019) for its mixed-precision quantization. The motivation is that parameters with a higher Hessian spectrum (i.e., larger top eigenvalues) are more sensitive to quantization and thus require higher precision. This is essentially a method for identifying outliers.\nFrom another perspective, the quantization problem is an optimization problem. Given a weight matrix $\\mathbf{W}$ and an input matrix $\\mathbf{X}$, we want to find a quantized weight matrix $\\hat{\\mathbf{W}}$ that minimizes the mean squared error (MSE):\n$$ \\hat{\\mathbf{W}}^* = \\arg \\min_{\\hat{\\mathbf{W}}} |\\mathbf{W}\\mathbf{X} - \\hat{\\mathbf{W}}\\mathbf{X}| $$GPTQ (Frantar et al. 2022) builds on the OBC (Optimal Brain Compression) (Frantar et al. 2022) method, treating the weight matrix $\\mathbf{W}$ as a set of row vectors $\\mathbf{w}$ and quantizing each row independently. GPTQ iteratively quantizes more weights, which are chosen greedily to minimize the quantization error. The update for the selected weights has a closed-form solution that utilizes the Hessian matrix.\nFig. 17. The pseudocode for the GPTQ algorithm. (Image source: Frantar et al. 2022)\nGPTQ can reduce the bit-width of weights in OPT-175B to 3-bit or 4-bit without much performance loss, but it only applies to model weights, not activations.\nOutlier smoothing Fig. 18. Magnitude of the input activations and weights of a linear layer in OPT-13B before and after SmoothQuant (Image source: Xiao et al. 2022)\nFrom the figure above, we can see that in Transformer models, activations are harder to quantize than weights. There are three main characteristics:\nActivations are harder to quantize than weights: Quantizing weights to INT8/INT4 barely affects accuracy, but activations are more sensitive. Outliers amplify the difficulty of activation quantization: Extreme values in activations are about 100 times larger than normal values. Direct INT8 quantization would crush most small values to zero. Outliers are fixed in a few channels: These extreme values are consistently concentrated in specific channels, leading to a highly uneven distribution across channels. SmoothQuant (Xiao et al. 2022) proposes a clever solution by smoothing outlier features from activations to weights through a mathematically equivalent transformation, and then quantizing both weights and activations (W8A8). Therefore, SmoothQuant has better hardware efficiency than mixed-precision quantization.\nFig. 19. SmoothQuant migrates the scale variance from activations to weights offline to reduce the difficulty of activation quantization. Both the resulting new weight and activation matrices are easy to quantize. (Image source: Xiao et al. 2022)\nConsidering a per-channel smoothing factor $\\mathbf{s}$, SmoothQuant scales the weights according to the following formula:\n$$ \\mathbf{Y} = (\\mathbf{X}\\text{diag}(\\mathbf{s})^{-1}) \\cdot (\\text{diag}(\\mathbf{s})\\mathbf{W}) = \\hat{\\mathbf{X}}\\hat{\\mathbf{W}} $$The smoothing factor can be easily fused into the parameters of the preceding layer offline. A hyperparameter $\\alpha$ controls the degree to which the quantization difficulty is migrated from activations to weights: $\\mathbf{s} = \\max(|\\mathbf{X}_j|)^\\alpha / \\max(|\\mathbf{W}_j|)^{1-\\alpha}$. The paper finds that for many LLMs, $\\alpha=0.5$ is an optimal choice in experiments. For models with more significant outliers in activations, $\\alpha$ can be adjusted to be larger.\nQuantization-Aware Training (QAT) Quantization-Aware Training (QAT) integrates the quantization operation into the pre-training or fine-tuning process, directly learning a low-bit representation of the model weights. It achieves higher performance at the cost of additional training time and computational resources.\nCommon QAT methods include:\nDirect Fine-tuning: The model is first quantized once, and then further fine-tuned on the original pre-training dataset or a representative training dataset. This makes the model sensitive to quantization errors and allows it to actively compensate, thereby improving the performance of the quantized model. The training objective can be the same as the original pre-training objective (e.g., negative log-likelihood NLL or masked language modeling MLM in language models) or a task-specific objective (e.g., cross-entropy for classification tasks). A typical implementation is QLoRA, which achieves efficient fine-tuning by combining a low-bit (e.g., 4-bit) base model with full-precision LoRA adapters.\nKnowledge Distillation: A full-precision model acts as the teacher, and a low-precision model acts as the student. A distillation loss guides the student model to approach the teacher\u0026rsquo;s performance. The Layer-by-layer Knowledge Distillation (LKD) technique used by ZeroQuant (Yao et al. 2022) is an example of this method. It quantizes the model weights layer by layer, with each quantized layer using the corresponding full-precision layer as a teacher, minimizing the mean squared error (MSE) between their weight computation results to improve performance.\n$$ \\mathcal{L}_{L K D, k}=M S E\\left(L_k \\cdot L_{k-1} \\cdot L_{k-2} \\cdot \\ldots \\cdot L_1(\\boldsymbol{X})-\\widehat{L}_k \\cdot L_{k-1} \\cdot L_{k-2} \\cdot \\ldots \\cdot L_1(\\boldsymbol{X})\\right) $$Pruning Network pruning reduces model size by removing unimportant model weights or connections, thereby achieving model compression while maintaining performance as much as possible. Depending on the implementation, pruning can be divided into unstructured pruning and structured pruning.\nUnstructured pruning: Not limited to a specific pattern, it can discard weights or connections at any position in the network, thus disrupting the original structural regularity of the network. Because the resulting sparse patterns are difficult to adapt to modern hardware architectures, this method usually cannot effectively improve actual inference efficiency.\nStructured pruning: Maintains the network\u0026rsquo;s structure by trimming entire structures (such as convolutional kernels, channels, or layers). The pruned network is still suitable for dense matrix computations optimized by existing hardware, thus significantly improving actual inference performance. In this article, we focus on structured pruning to achieve efficient sparse structures in Transformer models.\nA typical workflow for network pruning includes the following three steps:\nTrain a full, dense network until convergence. Prune the network by removing redundant structures or weights. (Optional) Further fine-tune the network to recover the performance of the pruned model. The Lottery Ticket Hypothesis One of the theoretical foundations of pruning is the Lottery Ticket Hypothesis (LTH) (Frankle \u0026amp; Carbin, 2019). This hypothesis posits that a randomly initialized dense neural network contains certain sparse subnetworks (the \u0026ldquo;winning tickets\u0026rdquo;) that, when trained in isolation, can achieve performance comparable to or even better than the full network.\nThe core idea of LTH is that not all parameters are equally important. Only a small fraction of the parameters in a model play a crucial role. This suggests that a large number of parameters are not primarily for solving overfitting but mainly provide a sufficient initialization search space for high-performance subnetworks to be discovered.\nTo test this hypothesis, Frankle and Carbin proposed the following experimental steps:\nRandomly initialize a dense neural network with initial weights $\\theta_0$. Train the full network to achieve good performance, with final parameters $\\theta$. Prune the trained parameters $\\theta$ to generate a sparse mask $m$. Select the \u0026ldquo;winning ticket\u0026rdquo; subnetwork, with initial parameters defined as $m \\odot \\theta_0$. The experiment found that by using only the small number of \u0026ldquo;winning ticket\u0026rdquo; parameters selected in step 1 and training them from their original random initial values, the model could still achieve almost the same accuracy as the original network.\nThis result indicates that the vast initial parameter space is not necessary for the final deployed model but provides a large number of initial possibilities during the training phase, allowing the network to discover high-performing sparse structures. This also explains why, although a pruned model is significantly smaller, training the same sparse structure from scratch is often difficult to succeed.\nPruning Strategies Magnitude pruning is the simplest yet quite effective pruning method—weights with the smallest absolute values are pruned. In fact, some studies (Gale et al. 2019) have found that simple magnitude pruning methods can achieve comparable or better results than complex pruning methods like variational dropout (Molchanov et al. 2017) and $l_0$ regularization (Louizos et al. 2017). Magnitude pruning is easy to apply to large models and achieves fairly consistent performance across a wide range of hyperparameters.\nGradual Magnitude Pruning (GMP) (Zhu \u0026amp; Gupta, 2017) is based on the idea that large sparse models can achieve better performance than small but dense models. It proposes gradually increasing the sparsity of the network during training. At each training step, weights with the smallest absolute values are masked to zero to achieve a desired sparsity level $s$, and the masked weights do not receive gradient updates during backpropagation. The desired sparsity level $s$ increases with the number of training steps. The GMP process is sensitive to the learning rate schedule, which should be higher than that used in dense network training but not so high that it fails to converge.\nIterative pruning (Renda et al. 2020) iterates through step 2 (pruning) and step 3 (retraining) multiple times: in each iteration, only a small fraction of weights are pruned, and then the model is retrained. This process is repeated until the desired sparsity level is reached.\nRetraining The retraining step can be simple fine-tuning, using the same pre-training data or other task-specific datasets.\nThe Lottery Ticket Hypothesis proposes a retraining technique called weight rewinding: after pruning, the unpruned weights are re-initialized to their original values from early in training, and then retrained using the same learning rate schedule.\nLearning rate rewinding (Renda et al. 2020) only resets the learning rate to its early value, while the unpruned weights remain unchanged from the end of the previous training phase. They observed that (1) on various networks and datasets, retraining with weight rewinding is superior to retraining with fine-tuning; and (2) in all tested scenarios, learning rate rewinding is comparable or superior to weight rewinding.\nSparsity Sparsity is an effective way to scale model capacity while maintaining computational efficiency for model inference. Here we consider two types of sparsity for Transformers:\nSparsified dense layers, including self-attention and FFN layers. Sparse model architectures; i.e., by introducing Mixture-of-Experts (MoE) components. N:M Sparsity via Pruning N:M sparsity is a structured sparse pattern that works well with modern GPU hardware optimizations, where N out of every M consecutive elements are zero. For example, the Nvidia A100\u0026rsquo;s sparse tensor cores support 2:4 sparsity for faster inference.\nFig. 20. The illustration of achieving N:M structure sparsity. (Image source: Zhou et al. 2021)\nTo sparsify a dense neural network to follow an N:M structured sparse pattern, Nvidia recommends a three-step conventional workflow for training the pruned network: train -\u0026gt; prune to meet 2:4 sparsity -\u0026gt; retrain.\nPermutations can provide more options during pruning to preserve large-magnitude parameters or satisfy special constraints like N:M sparsity. The result of a matrix multiplication does not change as long as the paired axes of two matrices are permuted in the same order. For example:\n(1) Within a self-attention module, if the same permutation order is applied to axis 1 of the query embedding matrix $\\mathbf{Q}$ and axis 0 of the key embedding matrix $\\mathbf{K}^\\top$, the final matrix multiplication result of $\\mathbf{Q}\\mathbf{K}^\\top$ will remain unchanged.\nFig. 21. Illustration of the same permutation on $\\mathbf{Q}$ (axis 1) and $\\mathbf{K}^\\top$ (axis 0) to keep the results of a self-attention module unchanged. (Image source: Lilian, 2023)\n(2) Within an FFN layer containing two MLP layers and a ReLU non-linearity, we can permute axis 1 of the first linear weight matrix $\\mathbf{W}_1$ and axis 0 of the second linear weight matrix $\\mathbf{W}_2$ in the same order.\nFig. 22. Illustration of the same permutation on $\\mathbf{W}_1$ (axis 1) and $\\mathbf{W}_2$ (axis 0) to keep the FFN layer\u0026rsquo;s output unchanged. For simplicity, the bias terms are skipped but the same permutation should be applied to them too. (Image source: Lilian, 2023)\nTo enforce N:M structured sparsity, we divide the columns of a matrix into segments of M columns (called \u0026ldquo;stripes\u0026rdquo;). We can easily observe that the order of columns within each stripe and the order of the stripes themselves have no effect on the N:M sparsity constraint.\nChannel Permutations Channel Permutations (Pool \u0026amp; Yu, 2021) employs an iterative greedy algorithm to find the optimal permutation that maximizes the weight magnitude for N:M sparsity. All pairs of channels are speculatively swapped, and only the swap that results in the largest increase in magnitude is adopted, thus generating a new permutation and ending a single iteration. The greedy algorithm may only find a local optimum, so they introduce two techniques to escape local optima:\nBounded regressions: In practice, randomly swap two channels for a fixed number of times. The solution search is limited to a depth of one channel swap to keep the search space broad and shallow. Narrow, deep search: Select multiple stripes and optimize them simultaneously. Fig. 23. Algorithm for finding the best permutation for N:M sparsity greedily and iteratively. (Image source: Pool \u0026amp; Yu, 2021)\nIf the network is permuted before pruning, it can achieve better performance compared to pruning it in its default channel order.\nSTE \u0026amp; SR-STE The Straight-Through Estimator (STE) (Bengio et al. 2013) computes the gradient of the dense parameters with respect to the pruned network $\\widetilde{W}$, $\\partial\\mathcal{L}/\\partial\\widetilde{W}$, and applies it to the dense network $W$ as an approximation: $$ W_{t+1} \\leftarrow W_t - \\gamma \\frac{\\partial\\mathcal{L}}{\\partial\\widetilde{W}} $$Sparse-refined STE (SR-STE) (Zhou et al. 2021) extends the STE method to train a model with N:M sparsity from scratch. It is commonly used for backpropagation updates in model quantization and is adapted for magnitude pruning and sparse parameter updates. The dense weights $W$ are updated as follows: $$ W_{t+1} \\leftarrow W_t - \\gamma \\frac{\\partial\\mathcal{L}}{\\partial\\widetilde{W}} + \\lambda_W(\\overline{\\mathcal{E}} \\odot W_t) $$ where $\\overline{\\mathcal{E}}$ is the mask matrix of $\\widetilde{W}$, and $\\odot$ is element-wise multiplication. SR-STE aims to prevent large changes in the binary mask by (1) limiting the values of weights pruned in $\\widetilde{W}_t$, and (2) boosting the weights that are not pruned in $\\widetilde{W}_t$.\nFig. 24. Comparison of STE and SR-STE. $\\odot$ is element-wise product; $\\otimes$ is matrix multiplication. (Image source: Zhou et al. 2021)\nTop-KAST The Top-K Always Sparse Training (Top-KAST) (Jayakumar et al. 2021) method, unlike STE or SR-STE, can maintain constant sparsity in both the forward and backward passes without needing dense parameters or dense gradients for the forward pass.\nAt a training step $t$, Top-KAST proceeds as follows:\nSparse Forward Pass: Select a subset of parameters $A^t \\subset \\Theta$, containing the top $K$ parameters of each layer sorted by magnitude, limited to the top $D$ proportion of weights. In the parameterization $\\alpha^t$ at time $t$, if a parameter is not in $A^t$ (the active weights), its value is zero. $$ \\alpha_i^t = \\begin{cases} \\theta_i^t \u0026 \\text{if } i \\in A^t = \\{i \\mid \\theta_i^t \\in \\text{TopK}(\\theta^t, D)\\} \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$ where $\\text{TopK}(\\theta, x)$ selects the top $x$ proportion of weights from $\\theta$ based on magnitude.\nSparse Backward Pass: The gradient is then applied to a larger subset of parameters $B \\subset \\Theta$, where $B$ contains a $(D+M)$ proportion of weights and $A \\subset B$. Updating a larger proportion of weights allows for more effective exploration of different pruning masks, making it more likely to cause permutations in the top $D$ proportion of active weights. $$ \\Delta\\theta_i^t = \\begin{cases} -\\eta \\nabla_{\\alpha_t} \\mathcal{L}(y, x, \\alpha^t)_i \u0026 \\text{if } i \\in B^t = \\{i \\mid \\theta_i^t \\in \\text{TopK}(\\theta^t, D+M)\\} \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$ Training is divided into two phases, and the additional coordinates in the set $B \\setminus A$ control how much exploration is introduced. The amount of exploration is expected to decrease gradually during the training process, and the mask will eventually stabilize.\nFig. 25. The pruning mask of Top-KAST stabilizes in time. (Image source: Jayakumar et al. 2021)\nTo prevent the \u0026ldquo;rich get richer\u0026rdquo; phenomenon, Top-KAST penalizes the magnitude of active weights through an L2 regularization loss to encourage the exploration of new items. Parameters in $B \\setminus A$ are penalized more than those in $A$ to set a higher selection threshold for a stable mask during updates. $$ L_{\\text{penalty}}(\\alpha_i^t) = \\begin{cases} |\\theta_i^t| \u0026 \\text{if } i \\in A^t \\\\ |\\theta_i^t|/D \u0026 \\text{if } i \\in B^t \\setminus A^t \\\\ 0 \u0026 \\text{otherwise} \\end{cases} $$Sparsified Transformer Scaling Transformer (Jaszczur et al. 2021) sparsifies the self-attention and FFN layers in the Transformer architecture, achieving a 37x speedup in single-sample inference.\nFig. 26. Decoding speed of a single token for Terraformer with 17B parameters is 37x faster than a dense baseline model. (Image source: Jaszczur et al. 2021)\nSparse FFN Layer: Each FFN layer contains 2 MLPs and a ReLU. Because ReLU introduces a large number of zero values, they enforce a fixed structure on the activations, forcing only one non-zero value in a block of $N$ elements. The sparse pattern is dynamic and different for each token. $$ \\begin{aligned} Y_{\\text{sparse}} \u0026= \\max(0, xW_1 + b_1) \\odot \\text{Controller}(x) \\\\ \\text{SparseFFN}(x) \u0026= Y_{\\text{sparse}} W_2 + b_2 \\\\ \\text{Controller}(x) \u0026= \\arg\\max(\\text{Reshape}(xC_1C_2, (-1, N))) \\end{aligned} $$ where each activation in $Y_{\\text{sparse}}$ corresponds to a column in $W_1$ and a row in $W_2$. The controller is a low-rank bottleneck dense layer, $C_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{lowrank}}}, C_2 \\in \\mathbb{R}^{d_{\\text{lowrank}} \\times d_{\\text{ff}}}$ and $d_{\\text{lowrank}} = d_{\\text{model}}/N$. It uses arg max at inference time to select which columns should be non-zero, and the Gumbel-softmax trick during training. Because we can compute $\\text{Controller}(x)$ before loading the FFN weight matrices, we know which columns will be zeroed out and thus choose not to load them into memory to speed up inference.\nFig. 27. (a) Sparse FFN layer; columns in red are not loaded in memory for faster inference. (b) Sparse FFN controller for 1:4 sparsity. (Image source: Jaszczur et al. 2021)\nSparse QKV (Attention) Layer: In the attention layer, the dimension $d_{\\text{model}}$ is divided into $S$ modules, each of size $M = d_{\\text{model}}/S$. To ensure that each sub-part can access any part of the embedding, Scaling Transformer introduces a multiplicative layer (i.e., a layer that multiplies inputs from multiple neural network layers element-wise), which can represent any permutation but contains fewer parameters than a dense layer.\nGiven an input vector $x \\in \\mathbb{R}^{d_{\\text{model}}}$, the multiplicative layer outputs $y \\in \\mathbb{R}^{S \\times M}$: $$ y_{s,m} = \\sum_i x_i D_{i,s} E_{i,m} \\quad \\text{where } D \\in \\mathbb{R}^{d_{\\text{model}} \\times S}, E \\in \\mathbb{R}^{d_{\\text{model}} \\times M} $$ The output of the multiplicative layer is a tensor of size $\\mathbb{R}^{\\text{batch size} \\times \\text{length} \\times S \\times M}$. It is then processed by a 2D convolutional layer, where length and $S$ are treated as the height and width of an image. Such a convolutional layer further reduces the number of parameters and computation time of the attention layer.\nFig. 28. (a) A multiplicative layer is introduced to enable partitions to access any part of an embedding. (b) Combination of multiplicative dense layer and 2-D convolutional layer reduces the number of parameters and computation time of the attention layer. (Image source: Jaszczur et al. 2021)\nTo better handle long sequences, Scaling Transformer is further equipped with LSH (Locality-Sensitive Hashing) attention and FFN block recurrence from Reformer (Kitaev, et al. 2020).\nMixture of Experts Mixture-of-Experts (MoE) models consist of multiple \u0026ldquo;expert\u0026rdquo; networks, where each input sample activates only a subset of these experts for computation.\nFig. 29. Dense Transformer vs Sparse Expert Transformer. (Image source: Fedus et al. 2022)\nDense Model: All input tokens are processed using the same feed-forward network (FFN) parameters. While the structure is simple and easy to train, its computational cost increases rapidly as the model size grows.\nSparse Expert Model: Each input token is independently routed to a few experts among many for processing. This sparse routing mechanism allows the model to have more unique parameters without a significant increase in overall computational cost, thus improving parameter efficiency and scalability, and effectively reducing the computational cost during inference.\nRouting Strategy Improvements The MoE layer has a routing network that assigns a subset of experts to each input token. In traditional MoE models, the routing strategy routes each token to its preferred expert in the order they appear in the natural sequence. If a token is routed to an expert that has already reached its capacity, the token is marked as \u0026ldquo;overflowed\u0026rdquo; and skipped.\nVision MoE (V-MoE) (Riquelme et al. 2021) adds MoE layers to the ViT (Vision Transformer). It achieves previous SOTA performance with only half the inference computation. V-MoE can be scaled up to 15B parameters. Their experiments used $k=2$, 32 experts, and placed an expert layer every 2 layers (meaning MoE was placed in every other layer).\nDue to the limited capacity of each expert, some important and informative tokens might be dropped if they appear too late (e.g., the order of words in a sentence, or the order of image patches). To avoid this drawback of the traditional routing scheme, V-MoE employs Batch Priority Routing (BPR), which first assigns experts to tokens with high priority. BPR computes a priority score for each token before expert assignment (the maximum or sum of the top-k router scores) and changes the order of tokens accordingly. This ensures that the expert capacity buffer will be filled with critical tokens first.\nFig. 30. How image patches are discarded according to priority scores when $C \u0026lt; 1$. (Image source: Riquelme et al. 2021)\nWhen $C \\le 0.5$, BPR performs much better than traditional routing, as the model starts to drop a large number of tokens. It allows the model to compete with dense networks even at fairly low capacities.\nWhen studying how to interpret the association between image classes and experts, they observed that early MoE layers are more general, while later MoE layers may specialize in a few image classes.\nTask MoE (Task-level Mixture-of-Experts) (Kudugunta et al. 2021) considers task information and routes tokens at the task level rather than the word or token level in machine translation. They use MNMT (Multilingual Neural Machine Translation) as an example and group translation tasks based on the target language or language pair.\nToken-level routing is dynamic, with routing decisions made independently for each token. Therefore, at inference time, the server needs to pre-load all experts. In contrast, task-level routing is static for a given fixed task, so an inference server for a task only needs to pre-load $k$ experts (assuming top-k routing). According to their experiments, Task MoE can achieve similar performance gains as Token MoE compared to a dense model baseline, with 2.6x higher peak throughput and only 1.6% of the decoder size.\nTask-level MoE essentially classifies the task distribution based on predefined heuristic rules and incorporates this human knowledge into the router. When such heuristic rules do not exist (e.g., for a general sentence completion task), how to utilize Task MoE is less straightforward.\nPR-MoE (Pyramid residual MoE) (Rajbhandari et al. 2022) has each token pass through a fixed MLP and a selected expert. Observing that MoE is more beneficial in later layers, PR-MoE employs more experts in the later layers. The DeepSpeed library implements a flexible multi-expert, multi-data parallel system to support training PR-MoE with varying numbers of experts.\nFig. 31. Illustration of PR-MoE architecture in comparison with a standard MoE. (Image source: Rajbhandari et al. 2022)\nKernel Improvement Expert networks can be hosted on different devices. However, as the number of GPUs increases, the number of experts per GPU decreases, and the communication between experts (\u0026ldquo;All-to-all\u0026rdquo;) becomes more expensive. All-to-all communication between experts across multiple GPUs relies on NCCL\u0026rsquo;s P2P API, which cannot saturate the bandwidth of high-speed links (like NVLink, HDR InfiniBand) at a large scale because individual data blocks become smaller as more nodes are used. Existing all-to-all algorithms perform poorly in large-scale scenarios with small workloads. There are several kernel improvements that enable more efficient MoE computation, such as making all-to-all communication cheaper/faster.\nDeepSpeed (Rajbhandari et al. 2022) and TUTEL (Hwang et al. 2022) both implement a tree-based hierarchical all-to-all algorithm that first runs an intra-node all-to-all, followed by an inter-node all-to-all. It reduces the number of communication hops from $O(G)$ to $O(G_{\\text{node}} + G/G_{\\text{node}})$, where $G$ is the total number of GPU nodes and $G_{\\text{node}}$ is the number of GPU cores per node. Although the communication volume is doubled in this implementation, it achieves better scalability in small-batch, large-scale scenarios because the bottleneck is latency rather than communication bandwidth.\nDynaMoE (Kossmann et al. 2022) uses dynamic recompilation to adapt computational resources to the dynamic workload among experts. The RECOMPILE mechanism compiles the computation graph from scratch and reallocates resources only when needed. It measures the number of samples assigned to each expert and dynamically adjusts their capacity factor $C$ to reduce memory and computational requirements at runtime. Based on the observation that sample-expert assignments converge early in training, a sample assignment cache is introduced after convergence, and then RECOMPILE is used to eliminate dependencies between the gating network and the experts.\nArchitectural Optimization Efficient Transformers The survey paper Efficient Transformers (Tay et al. 2020) reviews a series of Transformer architectures with improvements in computational and memory efficiency. Readers interested in this topic can read the original paper.\nFig. 32. Categorization of efficient transformer models. (Image source: Tay et al. 2020)\nKV Cache Optimization Multi-Query Attention (MQA) \u0026amp; Grouped-Query Attention (GQA): In standard Multi-Head Attention (MHA), each head has its own set of Key and Value projection matrices. MQA (Shazeer, 2019) proposed having all Query heads share the same set of Key and Value heads, which greatly reduces the size of the KV Cache. GQA (Ainslie et al., 2023) is a compromise between MHA and MQA, grouping query heads so that heads within a group share a set of K/V, achieving a good balance between performance and effectiveness.\nvLLM(Kwon et al., 2023) introduced PagedAttention, inspired by virtual memory and paging in operating systems. It divides the KV Cache into fixed-size blocks, which can be stored non-contiguously in physical VRAM. A \u0026ldquo;block table\u0026rdquo; manages the mapping from logical blocks to physical blocks. For a detailed introduction to vLLM, you can refer to my previous blog post vLLM: High-Throughput and Memory-Efficient LLM Serving Engine. This method almost completely eliminates memory fragmentation (both internal and external), bringing VRAM utilization close to 100%. More importantly, through a Copy-on-Write mechanism, it can efficiently share the KV Cache across requests, greatly increasing throughput for complex decoding scenarios like parallel sampling and Beam Search.\nFlashAttention FlashAttention(Dao et al., 2022) is an IO-aware exact attention algorithm. It recognizes that the main bottleneck in standard Attention implementations is the data transfer between GPU HBM (High-Bandwidth Memory) and SRAM (on-chip high-speed cache). FlashAttention uses Tiling and Recomputation techniques to fuse the entire Attention computation into a single CUDA kernel, avoiding the need to write and read the huge $N \\times N$ attention matrix to and from HBM. This dramatically reduces memory access, thereby speeding up Attention computation by several times without sacrificing accuracy. FlashAttention-2(Dao, 2023) further optimizes parallelism and hardware utilization. Fig. 33. FlashAttention uses tiling to avoid materializing the large N × N attention matrix on slow GPU HBM, achieving up to 7.6× speedup over PyTorch. (Image source: Dao et al., 2022)\nReferences [1] Zhou, Zixuan, et al. “A survey on efficient inference for large language models.” arXiv preprint arXiv:2404.14294 (2024).\n[2] Zhang, Aston, et al. “Dive into Deep Learning.”. Cambridge University Press, 2023.\n[3] Big Hummingbird Blogs. (2024). “A Visual Explanation of LLM Hyperparameters.” Blog post.\n[4] Fan, Angela, Mike Lewis, and Yann Dauphin. “Hierarchical neural story generation.” arXiv preprint arXiv:1805.04833 (2018). int arXiv:1805.04832.\n[5] Holtzman, Ari, et al. “The curious case of neural text degeneration.” arXiv preprint arXiv:1904.09751 (2019).\n[6] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. “Fast inference from transformers via speculative decoding.” International Conference on Machine Learning. PMLR, 2023.\n[7] Liu, Xiaoxuan, et al. “Online speculative decoding.” arXiv preprint arXiv:2310.07177 (2023).\n[8] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “Distilling the knowledge in a neural network.” arXiv preprint arXiv:1503.02531 (2015).\n[9] Gou, Jianping, et al. “Knowledge distillation: A survey.” International Journal of Computer Vision 129.6 (2021): 1789-1819.\n[10] Sanh, Victor, et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.” arXiv preprint arXiv:1910.01108 (2019).\n[11] Raschka, S. (2023). “Accelerating Large Language Models with Mixed-Precision Techniques.” Blog post.\n[12] Bondarenko, Yelysei, Markus Nagel, and Tijmen Blankevoort. \u0026ldquo;Understanding and overcoming the challenges of efficient transformer quantization.\u0026rdquo; arXiv preprint arXiv:2109.12948 (2021).\n[13] Zhang, S., Roller, S., Goyal, N., et al. (2022). “OPT: Open pre-trained transformer language models.” arXiv preprint arXiv:2205.01068.\n[14] Dettmers, T., et al. (2022). “LLM.int8(): 8-bit matrix multiplication for transformers at scale.” arXiv preprint arXiv:2208.07339.\n[15] Zadeh, V. K., et al. (2020). “GOBO: Quantizing attention-based NLP models for low latency and energy efficient inference.” arXiv preprint arXiv:2005.03842.\n[16] Weng, L. (2023). “Large Transformer Model Inference Optimization.” Lil’Log blog post.\n[17] Shen, Z., Dong, Z., Ye, J., et al. (2020). “Q-BERT: Hessian-based ultra-low-precision quantization of BERT.” arXiv preprint arXiv:1909.05840.\n[18] Dong, Z., Yao, Z., Gholami, A., et al. (2019). “HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision” arXiv preprint arXiv:1905.03696.\n[19] Yao, Z., et al. (2022). “ZeroQuant: Efficient and affordable post-training quantization for large-scale transformers.” arXiv preprint arXiv:2206.01861.\n[20] Frantar, E., et al. (2022). “GPTQ: Accurate post-training quantization for generative pre-trained transformers.” arXiv preprint arXiv:2210.17323.\n[21] Xiao, G., \u0026amp; Lin, J. (2022). “SmoothQuant: Accurate and efficient post-training quantization for large language models.” arXiv preprint arXiv:2211.10438.\n[22] Frankle, J., \u0026amp; Carbin, M. (2019). “The lottery ticket hypothesis: Finding sparse, trainable neural networks.” arXiv preprint arXiv:1803.03635.\n[23] Gale, T., Elsen, E., \u0026amp; Hooker, S. (2019). “The state of sparsity in deep neural networks.” arXiv preprint arXiv:1902.09574.\n[24] Molchanov, D., Ashukha, A., \u0026amp; Vetrov, D. (2017). “Variational dropout sparsifies deep neural networks.” arXiv preprint arXiv:1701.05369.\n[25] Louizos, Christos, Max Welling, and Diederik P. Kingma. \u0026ldquo;Learning sparse neural networks through $ L_0 $ regularization.\u0026rdquo; arXiv preprint arXiv:1712.01312 (2017).\n[26] Zhu, M., \u0026amp; Gupta, S. (2017). “To prune, or not to prune: Exploring the efficacy of pruning for model compression.” arXiv preprint arXiv:1710.01878.\n[27] Renda, A., Frankle, J., \u0026amp; Carbin, M. (2020). “Comparing rewinding and fine-tuning in neural network pruning.” arXiv preprint arXiv:2003.02389.\n[28] Nvidia. (2020). “NVIDIA A100 Tensor Core GPU.” Nvidia Blog.\n[29] Zhou, A., \u0026amp; Ma, X. (2021). “Learning N:M fine-grained structured sparse neural networks from scratch.” arXiv preprint arXiv:2102.04010.\n[30] Pool, J., \u0026amp; Yu, F. (2021). “Channel permutations for N:M structured sparsity.” Advances in Neural Information Processing Systems 34.\n[31] Bengio, Y., Léonard, N., \u0026amp; Courville, A. (2013). “Estimating or propagating gradients through stochastic neurons for conditional computation.” arXiv preprint arXiv:1308.3432.\n[32] Jayakumar, S. M., Pascanu, R., Rae, J., et al. (2021). “Top-KAST: Top-K always sparse training.” arXiv preprint arXiv:2106.03517.\n[33] Jaszczur, S., et al. (2021). “Sparse is enough in scaling transformers.” Advances in Neural Information Processing Systems 34.\n[34] Kitaev, N., Kaiser, Ł., \u0026amp; Levskaya, A. (2020). “Reformer: The efficient transformer.” arXiv preprint arXiv:2001.04451.\n[35] Fedus, W., et al. (2022). “A review of sparse expert models in deep learning.” arXiv preprint arXiv:2209.01667.\n[36] Riquelme, C., et al. (2021). “Scaling vision with sparse mixture of experts.” Advances in Neural Information Processing Systems 34: 8583-8595.\n[37] Kudugunta, S., Lepikhin, D., Heafield, K., et al. (2021). “Beyond domain adaptation: Multi-task mixture-of-experts for zero-shot generalization.” arXiv preprint arXiv:2110.03742.\n[38] Rajbhandari, S., et al. (2022). “DeepSpeed-MoE: Advancing mixture-of-experts inference and training to power next-generation AI scale.” arXiv preprint arXiv:2201.05596.\n[39] Hwang, I., et al. (2022). “Tutel: Adaptive mixture-of-experts at scale.” arXiv preprint arXiv:2206.03382.\n[40] Kossmann, F., et al. (2022). “Optimizing Mixture of Experts using Dynamic Recompilations” arXiv preprint arXiv:2205.01848.\n[41] Tay, Y., et al. (2020). “Efficient transformers: A survey.” arXiv preprint arXiv:2009.06732.\n[42] Shazeer, N. (2019). “Fast transformer decoding: One write-head is all you need.” arXiv preprint arXiv:1911.02150.\n[43] Ainslie, J., et al. (2023). “GQA: Training generalized multi-query transformer models from multi-head checkpoints.” arXiv preprint arXiv:2305.13245.\n[44] Kwon, W., et al. (2023). “Efficient memory management for large language model serving with PagedAttention.” Proceedings of the 29th Symposium on Operating Systems Principles.\n[45] Dao, T., et al. (2022). “FlashAttention: Fast and memory-efficient exact attention with IO-awareness.” Advances in Neural Information Processing Systems 35: 16344-16359.\n[46] Dao, T. (2023). “FlashAttention-2: Faster attention with better parallelism and work partitioning.” arXiv preprint arXiv:2307.08691.\n[47] Pope, R., et al. (2022). “Efficiently scaling transformer inference.” arXiv preprint arXiv:2211.05102.\n[48] von Platen, P. (2020). “How to generate text: Using different decoding methods for language generation with Transformers.” Hugging Face Blog.\nCitation Citation: When reproducing or citing the content of this article, please credit the original author and source.\nCited as:\nYue Shui. (Jun 2025). Large Language Model Inference. https://syhya.github.io/posts/2025-06-29-llm-inference\nOr\n@article{syhya2025llminferencesurvey, title = \u0026#34;Large Language Model Inference\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jun\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-06-29-llm-inference\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-06-29-llm-inference/","summary":"\u003cp\u003eIn recent years, Large Language Models (LLMs) have achieved revolutionary breakthroughs in fields such as natural language processing, code generation, and even multimodal interaction. However, the powerful capabilities of these models come at the cost of enormous computational and memory overhead, especially during the inference stage. Efficiently deploying and running these models, which have billions or even trillions of parameters, has become a core challenge in scaling LLM technology for real-world applications.\u003c/p\u003e","title":"Large Language Model Inference"},{"content":"As the parameters of Large Language Models (LLMs) continue to grow, deploying and serving these models presents significant challenges. vLLM is an open-source library designed for fast, convenient, and cost-effective LLM inference and online serving. Its core lies in the PagedAttention algorithm, which efficiently manages the KV Cache in the attention mechanism.\nEvaluation Metrics To evaluate the performance of LLM inference and serving engines, we primarily focus on the following metrics:\nTime To First Token (TTFT) Time To First Token (TTFT) refers to the time elapsed from when the model receives user input (Prompt) to when it generates the first output token. A shorter TTFT means less waiting time for the user, which is particularly important for real-time interactive scenarios; in offline scenarios, TTFT is relatively less critical.\nTime Per Output Token (TPOT) Time Per Output Token (TPOT) indicates the average time required for the model to generate one new token. It directly determines the user-perceived \u0026ldquo;speed\u0026rdquo; of the response. To enhance user experience, Streaming is commonly used in practical applications. For example, if TPOT is 0.1 seconds/token, it means the model can generate about 10 tokens per second, equivalent to approximately 450 words per minute, which exceeds the reading speed of most people.\nLatency Latency is the total time required for the model to generate a complete response for the user. It can be calculated from TTFT and TPOT using the following formula:\n$$ \\text{Latency} = \\text{TTFT} + \\text{TPOT} \\times (\\text{Number of Output Tokens}) $$Throughput Throughput measures the total number of tokens (including input and output tokens) that the model inference server can generate per unit of time for all user requests. It reflects the server\u0026rsquo;s processing efficiency and concurrency capability. The specific calculation formula is as follows:\n$$ \\text{Throughput} = \\frac{\\text{Batch Size} \\times (\\text{Number of Input Tokens} + \\text{Number of Output Tokens})}{\\text{End-to-End Latency}} $$Inter-Token Latency (ITL) Inter-Token Latency (ITL) represents the average time interval between the generation of two consecutive tokens after the first token has been generated. It reflects the speed at which each subsequent token is generated, calculated as:\n$$ \\text{ITL} = \\frac{\\text{End-to-End Latency} - \\text{TTFT}}{\\text{Batch Size} \\times (\\text{Number of Output Tokens} - 1)} $$These metrics reflect the inference engine\u0026rsquo;s response speed, processing efficiency, and concurrency capabilities, serving as important benchmarks for evaluating and optimizing LLM inference performance.\nvLLM V0 Since its initial release in June 2023, vLLM, equipped with PagedAttention, has significantly raised the performance benchmark for LLM serving. It demonstrates a notable throughput advantage over HuggingFace Transformers (HF) and Text Generation Inference (TGI), without requiring any modifications to the model architecture.\nFig. 1. Throughput comparison (single output completion) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)\nSingle output inference: The figure shows vLLM achieves 14x-24x higher throughput than HF and 2.2x-2.5x higher throughput than TGI. Fig. 2. Throughput comparison (three parallel output completions) on LLaMA models. vLLM vs. HF and TGI. (Image source: vLLM Blog, 2023)\nThree-way parallel inference: The figure shows vLLM achieves 8.5x-15x higher throughput than HF and 3.3x-3.5x higher throughput than TGI. Batching Traditional Dynamic Batching waits for an entire batch of requests to complete before processing the next batch. If some requests finish early, this leads to GPU idle time and reduced resource utilization.\nIn contrast, Continuous Batching, employed by vLLM, allows new request sequences to be dynamically inserted during batch execution. Once a sequence completes, it can be immediately replaced by a new sequence, significantly improving GPU utilization and throughput.\nFig. 3. Dynamic Batching vs Continuous Batching. (Image source: NYC vLLM Meetup, 2025)\nDynamic Batching: As shown on the left, sequences S₁-S₄ are processed in parallel from T1-T4. At T5, S₁ and S₃ finish early. However, because S₂ and S₄ are still running, new sequences cannot join immediately, leading to partial GPU idleness. New sequences can only start after S₂ finishes at T6 and S₄ finishes at T7.\nContinuous Batching: As shown on the right, T1-T4 are similar to dynamic batching. However, at T5, when S₁ and S₃ complete, new sequences S₅ and S₆ can join and start processing immediately, while S₂ and S₄ continue running. When S₂ finishes at T6, S₇ can join instantly. This approach keeps the GPU almost fully utilized, greatly enhancing efficiency.\nKV Cache The primary bottleneck in LLM serving performance is memory management. During the autoregressive decoding process, LLMs generate attention Key and Value tensors for each token in the input sequence. These tensors (KV cache) must be retained in GPU memory to generate subsequent tokens. The KV cache has the following characteristics:\nLarge: For the LLaMA-13B model, the KV cache for a single sequence can be up to 1.7 GB. Dynamic: The size of the KV cache depends on the sequence length, which is highly variable and unpredictable. Inefficient Management: Existing inference frameworks like FasterTransformer and Orca (Yu et al. 2022) typically store the KV cache in contiguous memory blocks. To handle its dynamic nature, they need to pre-allocate memory blocks large enough to accommodate the maximum possible sequence length. This leads to severe memory waste: Internal Fragmentation: Reserved space is much larger than actually needed. External Fragmentation: Pre-allocated blocks of different sizes make it difficult to utilize memory space efficiently. Over-reservation: Space reserved for future tokens cannot be used by other requests currently. The figure below illustrates the types of memory waste caused by KV cache management in existing inference systems:\nFig. 4. KV cache memory management in existing systems, showing reserved waste, internal fragmentation, and external fragmentation. (Image source: Kwon et al. 2023)\nThe left image below shows the memory distribution when running a 13B parameter LLM on an NVIDIA A100 GPU: approximately 65% of the memory is used for static model weights (gray), about 30% is dynamically allocated on demand for the KV cache (red) to store the attention context of preceding tokens, and a small amount of memory (yellow) is used for temporary activation computations. The right image indicates that vLLM effectively alleviates the memory bottleneck by smoothing the rapid growth of KV cache memory usage, thereby significantly enhancing batch processing capabilities and overall service throughput.\nFig. 5. Left: Memory layout for a 13B LLM on an NVIDIA A100—gray is persistent parameters, red is per-request KV cache, and yellow is temporary activation memory. Right: vLLM limits rapid KV cache growth, improving throughput. (Image source: Kwon et al. 2023)\nPagedAttention PagedAttention (Kwon et al. 2023) is inspired by Virtual Memory and Paging concepts from operating systems. It allows logically contiguous KV Cache to be stored in physically non-contiguous GPU memory.\nSpecifically, PagedAttention divides the KV Cache of each sequence into fixed-size Blocks. Each block contains the Key and Value vectors for a fixed number of tokens. The system maintains a Block Table that records the mapping from logical blocks to physical blocks for each sequence.\nFig. 6. Illustration of the PagedAttention algorithm, where KV vectors are stored in non-contiguous blocks. (Image source: Kwon et al. 2023)\nThe core idea of PagedAttention borrows from the virtual memory and paging mechanisms of operating systems to manage the KV cache.\nSpecifically, the design philosophy of PagedAttention can be summarized as follows:\nAnalogy:\nBlocks of the KV cache are analogous to Pages in OS memory management. Tokens are analogous to Bytes. Sequences are analogous to Processes. Mapping Mechanism: PagedAttention uses a Block Table to maintain the mapping from a sequence\u0026rsquo;s contiguous Logical Blocks to Physical Blocks. These physical blocks can be non-contiguous in memory, much like an OS page table maps virtual addresses to physical page frames.\nAllocate-on-Demand: Crucially, Physical Blocks are not pre-allocated for the maximum sequence length. Instead, they are allocated on demand when new Key-Values need to be stored (i.e., when new tokens are generated).\nThis on-demand, non-contiguous memory management allows PagedAttention to utilize memory more effectively, avoiding the waste and internal fragmentation caused by pre-allocating large contiguous spaces, thereby improving GPU memory utilization.\nMathematically, PagedAttention transforms attention computation into block-wise computation. Let the block size be $B$. The $j$-th Key block is $K_{j}=\\left(k_{(j-1) B+1}, \\ldots, k_{j B}\\right)$, and the Value block is $V_{j}=\\left(v_{(j-1) B+1}, \\ldots, v_{j B}\\right)$. For a query vector $q_i$, the attention computation becomes:\n\\[ A_{i j}=\\frac{\\exp \\left(q_{i}^{\\top} K_{j} / \\sqrt{d}\\right)}{\\sum_{t=1}^{\\lceil i / B\\rceil} \\exp \\left(q_{i}^{\\top} K_{t} \\mathbf{1} / \\sqrt{d}\\right)}, \\quad o_{i}=\\sum_{j=1}^{\\lceil i / B\\rceil} V_{j} A_{i j}^{\\top} \\]where $A_{i j}=\\left(a_{i,(j-1) B+1}, \\ldots, a_{i, j B}\\right)$ is the row vector of attention scores for the $i$-th query on the $j$-th KV block. During computation, the PagedAttention kernel efficiently identifies and fetches the required physical blocks.\nKV Cache Manager vLLM\u0026rsquo;s memory manager draws inspiration from the virtual memory mechanisms of operating systems:\nLogical vs. Physical Blocks: Each request\u0026rsquo;s KV cache is represented as a series of logical blocks. The Block Engine on GPU worker nodes allocates physical memory and divides it into physical blocks. Block Table: Maintains the mapping from logical blocks to physical blocks for each request. Each entry records the physical block address and the number of tokens filled within the block. Dynamic Allocation: Physical blocks are allocated on demand, eliminating the need to pre-reserve space for the maximum length, thereby significantly reducing memory waste. Fig. 7. Block table translation in vLLM. Logical blocks are mapped to non-contiguous physical blocks. (Image source: Kwon et al. 2023)\nConsider the example in Fig. 7:\nPrefill Stage: The input prompt has 7 tokens. Assume a block size of 4. vLLM allocates 2 physical blocks (e.g., physical blocks 7 and 1) and updates the block table, mapping logical block 0 to physical block 7, and logical block 1 to physical block 1. The KV cache for the prompt is computed and filled into these two physical blocks. Logical block 0 is filled with 4 tokens, and logical block 1 is filled with 3 tokens, leaving 1 slot reserved. Decode Stage: Step 1: The next token is computed using PagedAttention. Since logical block 1 still has an empty slot, the new KV cache is stored directly in physical block 1, and the fill count for logical block 1 in the block table is updated. Step 2: Logical block 1 is now full. vLLM allocates a new physical block (e.g., physical block 3), updates the block table to map the new logical block 2 to physical block 3, and stores the newly generated KV cache in physical block 3. This on-demand allocation method limits memory waste to the last block of each sequence, achieving near-optimal memory utilization (waste below 4%). This allows for batching more requests, thereby increasing throughput.\nFig. 8 shows how vLLM manages memory for two sequences. The logical blocks of the two sequences are mapped to different physical blocks reserved by the block engine on the GPU worker. This means that even logically adjacent blocks do not need to be contiguous in physical GPU memory, allowing both sequences to effectively share and utilize the physical memory space.\nFig. 8. Storing the KV cache of two requests concurrently in vLLM using paged memory. (Image source: Kwon et al. 2023)\nMemory Sharing Another key advantage of PagedAttention is efficient memory sharing, especially for complex decoding strategies.\nParallel Sampling When a request needs to generate multiple output sequences from the same prompt (e.g., code completion suggestions), the KV cache for the prompt part can be shared.\nFig. 9. Parallel sampling example. Logical blocks for the shared prompt map to the same physical blocks. (Image source: Kwon et al. 2023)\nvLLM achieves sharing via the block table:\nShared Mapping: Logical blocks of different sequences can map to the same physical block. Reference Counting: Each physical block maintains a reference count. Copy-on-Write (CoW): When a shared block (reference count \u0026gt; 1) needs to be written to, vLLM allocates a new physical block, copies the content of the original block, updates the block table mapping for the writing sequence, and decrements the reference count of the original physical block. Subsequent writes to this physical block (when its reference count is 1) are performed directly. This mechanism significantly reduces memory overhead for Parallel Sampling, with experiments showing memory savings of up to 55%.\nBeam Search During Beam Search, different candidate sequences (beams) not only share the prompt part but may also share the KV cache of subsequently generated tokens. The sharing pattern changes dynamically.\nFig. 10. Beam search example ($k=4$). Blocks are dynamically shared and freed based on candidate survival. (Image source: Kwon et al. 2023)\nvLLM efficiently manages this dynamic sharing using reference counting and the CoW mechanism, avoiding the frequent and costly memory copy operations found in traditional implementations. Most blocks can be shared; CoW is only needed when newly generated tokens fall into an old shared block (requiring only a single block copy).\nShared Prefix For applications where many prompts share a common prefix (e.g., system instructions, few-shot examples), vLLM can pre-compute and cache the KV cache of these Shared Prefixes into a set of physical blocks.\nFig. 11. Shared prompt example for machine translation using few-shot examples. (Image source: Kwon et al. 2023)\nWhen processing a request containing such a prefix, its logical blocks are simply mapped to the cached physical blocks (with the last block marked as CoW), thus avoiding redundant computation for the prefix part.\nScheduling and Preemption vLLM employs an FCFS scheduling policy. When GPU memory is insufficient to accommodate newly generated KV cache, preemption is necessary:\nPreemption Unit: Preemption occurs at the Sequence Group level (e.g., all candidate sequences of a beam search request). This ensures that the earliest arrived requests are served first, and the latest requests are preempted first. Recovery Mechanisms: Swapping: The KV blocks of preempted sequences are copied to CPU memory. They are swapped back to the GPU when resources become available. This is suitable for scenarios with high PCIe bandwidth and larger block sizes. Recomputation: The KV cache of preempted sequences is discarded. When resources are available, the original prompt and already generated tokens are concatenated, and the KV cache is recomputed efficiently in a single prompt phase. This is suitable for scenarios with lower PCIe bandwidth or smaller block sizes. Distributed Execution vLLM supports Megatron-LM style tensor model parallelism.\nFig. 12. vLLM system overview showing centralized scheduler and distributed workers. (Image source: Kwon et al. 2023)\nCentralized Scheduler: Contains the KV cache manager, maintaining the global mapping from logical to physical blocks. Shared Mapping: All GPU workers share the block table. Local Storage: Each worker only stores the portion of the KV cache corresponding to the attention heads it is responsible for. Execution Flow: The scheduler broadcasts input token IDs and the block table to all workers -\u0026gt; workers execute model computation (including PagedAttention) -\u0026gt; workers synchronize intermediate results via All-Reduce -\u0026gt; workers return sampled results to the scheduler. Memory management information is broadcast once at the beginning of each step, requiring no synchronization between workers. Kernel Optimization To efficiently implement PagedAttention, vLLM develops custom CUDA kernels:\nFused Reshape and Block Write: Combines splitting new KV cache into blocks, reshaping the layout, and writing to the block table into a single kernel. Fused Block Read and Attention Computation: Modifies FasterTransformer\u0026rsquo;s attention kernel to read non-contiguous blocks according to the block table and compute attention on-the-fly, optimizing memory access patterns. Fused Block Copy: Batches multiple small block copy operations triggered by CoW into a single kernel launch. vLLM V1 In January 2025, the vLLM team released the alpha version of vLLM V1, a major upgrade to its core architecture. Based on development experience over the past year and a half, the V1 release revisits key design decisions, integrates various features, and simplifies the codebase.\nBuilding on the success and lessons learned from vLLM V0, vLLM V1 introduces significant upgrades to the core architecture, aiming to provide a cleaner, more modular, easily extensible, and higher-performance codebase.\nMotivation and Goals for V1 Challenges of V0: As features and hardware support expanded, V0\u0026rsquo;s code complexity increased, making it difficult to combine features effectively and accumulating technical debt. Goals of V1: A simple, modular, and easy-to-modify codebase. High performance with near-zero CPU overhead. Unify key optimizations into the architecture. Enable optimizations by default for zero-configuration. Optimized Execution Loop \u0026amp; API Server Fig. 13. vLLM V1\u0026rsquo;s multiprocessing architecture with an isolated EngineCore. (Image source: vLLM Blog, 2025)\nAs GPU computation speeds increase (e.g., Llama-8B inference time on H100 is only ~5ms), CPU overhead (API serving, scheduling, input preparation, decoding, streaming responses) becomes a bottleneck. V1 adopts a multiprocessing architecture:\nIsolated EngineCore: Isolates the scheduler and model executor in a core engine loop. CPU Task Offloading: Moves CPU-intensive tasks like Tokenization, multimodal input processing, Detokenization, and streaming to separate processes, executing them in parallel with the EngineCore to maximize model throughput. Simple \u0026amp; Flexible Scheduler Fig. 14. vLLM V1\u0026rsquo;s scheduler treats prompt and generated tokens uniformly, enabling features like chunked prefill. (Image source: vLLM Blog, 2025)\nUniform Processing: No longer distinguishes between \u0026ldquo;prefill\u0026rdquo; and \u0026ldquo;decode\u0026rdquo; phases, treating user input tokens and model-generated tokens uniformly. Simple Representation: Scheduling decisions are represented by a dictionary, e.g., {request_id: num_tokens}, specifying how many tokens to process for each request per step. Generality: This representation is sufficient to support features like Chunked Prefills, Prefix Caching, and Speculative Decoding. For example, chunked prefill is implemented by dynamically allocating the processing quantity for each request under a fixed token budget. Zero-Overhead Prefix Caching Fig. 15. Performance comparison of prefix caching in vLLM V0 and V1. V1 achieves near-zero overhead even at 0% hit rate. (Image source: vLLM Blog, 2025)\nV1 optimizes the implementation of prefix caching (based on hash matching and LRU eviction):\nOptimized Data Structures: Implements constant-time cache eviction. Reduced Python Object Overhead: Minimizes object creation. Result: Performance degradation is less than 1% even with a 0% cache hit rate. At high hit rates, performance improves severalfold. Therefore, V1 enables prefix caching by default. Clean TP Architecture (Tensor-Parallel) Fig. 16. vLLM V1\u0026rsquo;s symmetric tensor-parallel architecture using diff-based updates. (Image source: vLLM Blog, 2025)\nV1 addresses the asymmetric architecture issue in V0 caused by the coupling of the scheduler and Worker 0:\nWorker-Side State Caching: Request states are cached on the worker side. Incremental Updates: Only incremental changes (diffs) to the state are transmitted each step, greatly reducing inter-process communication. Symmetric Architecture: The scheduler and Worker 0 can run in different processes, resulting in a cleaner, symmetric architecture. Abstracted Distributed Logic: Workers behave consistently in single-GPU and multi-GPU setups. Efficient Input Preparation Fig. 17. vLLM V1 uses Persistent Batch to cache input tensors and apply diffs. (Image source: vLLM Blog, 2025)\nV0 recreates model input tensors and metadata at each step, leading to high CPU overhead. V1 adopts the Persistent Batch technique:\nCache Input Tensors: Caches input tensors. Apply Diffs: Only applies incremental changes each step. Numpy Optimization: Extensively uses Numpy operations instead of native Python operations to reduce CPU overhead in updating tensors. Comprehensive Optimizations torch.compile and Piecewise CUDA Graphs\ntorch.compile Integration: V1 fully leverages vLLM\u0026rsquo;s torch.compile integration to automatically optimize models, supporting efficient operation for various models and significantly reducing the need for manually writing CUDA kernels. Piecewise CUDA Graphs: By introducing piecewise CUDA graphs, V1 successfully overcomes the limitations of native CUDA graphs, enhancing model flexibility and performance. Enhanced Support for Multimodal LLMs\nV1 introduces several key improvements for Multimodal Large Language Models (MLLMs): Optimized Preprocessing: CPU-intensive preprocessing tasks like image decoding, cropping, and transformation are moved to non-blocking separate processes to prevent GPU work from being blocked. A preprocessing cache is also introduced to reuse processed inputs for subsequent requests, especially beneficial for identical multimodal inputs. Multimodal Prefix Caching: In addition to token ID hashes, V1 uses image hashes to identify KV cache entries containing image inputs. This improvement is particularly advantageous in multi-turn dialogue scenarios involving image inputs. Encoder Cache: For applications requiring visual encoder outputs, V1 temporarily caches visual embeddings, allowing the scheduler to process text inputs in chunks without recomputing visual embeddings at each step, thus supporting chunked-fill scheduling for MLLMs. FlashAttention 3 Integration\nDue to V1\u0026rsquo;s high dynamism (e.g., combining prefill and decode within the same batch), a flexible and high-performance attention kernel was needed. FlashAttention 3 perfectly meets this requirement, providing robust feature support while maintaining excellent performance across various use cases. Performance Comparison Thanks to architectural improvements and significantly reduced CPU overhead, V1 achieves up to 1.7x higher throughput compared to V0 (without multi-step scheduling). Performance improvements are even more pronounced for multimodal models.\nFig. 18. Performance comparison between vLLM V0 and V1 on Llama 3.1 8B \u0026amp; Llama 3.3 70B (1xH100). (Image source: vLLM Blog, 2025)\nFig. 19. Performance comparison between vLLM V0 and V1 on Qwen2-VL 7B (1xH100). (Image source: vLLM Blog, 2025)\nComparison Table:\nFeature vLLM V0 vLLM V1 Improvement Point Core Technology PagedAttention PagedAttention + Comprehensive Architectural Refactor Retains PagedAttention benefits, optimizes overall architecture Memory Efficiency Extremely High (Waste \u0026lt; 4%) Extremely High (Waste \u0026lt; 4%) Maintains high memory efficiency Memory Sharing Supported (CoW) Supported (CoW) Maintains efficient sharing CPU Overhead Relatively high, especially in complex scenarios or low-hit-rate prefix caching Significantly reduced, near-zero overhead Multiprocessing, Persistent Batch, optimized data structures, etc. Execution Loop Single process, API server tightly coupled with engine Multiprocess, API server decoupled from EngineCore, highly parallel Improves CPU/GPU parallelism, reduces blocking Scheduler Differentiates Prefill/Decode Uniform token processing, dictionary-based scheduling Simpler, more flexible, easily supports advanced features Prefix Caching Disabled by default (overhead at low hit rates) Enabled by default (zero-overhead design) Optimized for low hit rates, default enabled for ease of use Tensor Parallelism Asymmetric architecture (Scheduler+Worker0 in same process) Symmetric architecture (Scheduler \u0026amp; Worker separated) Cleaner architecture, IPC overhead controlled by state caching \u0026amp; Diffs Multimodal Support Basic support Enhanced support (non-blocking preprocessing, image prefix cache, encoder cache, etc.) Improves VLM performance and usability Compiler Integration Limited Integrated torch.compile Automated model optimization, reduces manual Kernel writing Attention Kernel Custom Kernel (based on FasterTransformer) Integrated FlashAttention 3 Adopts industry standard for better performance and feature support Performance (vs V0) Baseline Up to 1.7x throughput increase (text), MLLMs more significant Gains from comprehensive CPU overhead optimization Code Complexity Increased with features Simpler, more modular Lowers maintenance cost, facilitates community contribution \u0026amp; dev Other Inference Frameworks LightLLM: A Python-based lightweight inference and serving framework, known for its lightweight design, scalability, and high-speed performance, drawing on the strengths of other open-source projects like vLLM. LMDeploy: A toolkit for compressing, deploying, and serving LLMs, featuring the TurboMind inference engine, emphasizing high request throughput and efficient quantization. SGLang: A framework for efficiently executing complex LLM programs (especially those involving structured generation) through co-design of a frontend language and a backend execution engine. TGI (Text Generation Inference): Hugging Face\u0026rsquo;s production-grade LLM serving solution, widely used and supporting multiple hardware backends. It leverages vLLM\u0026rsquo;s PagedAttention kernel to provide high-concurrency, low-latency inference services. TensorRT-LLM: An open-source library from NVIDIA for optimizing and accelerating LLM inference on NVIDIA GPUs, utilizing TensorRT\u0026rsquo;s ahead-of-time compilation and deep hardware optimization capabilities. Summary vLLM, through its core technology PagedAttention, significantly alleviates the memory bottleneck in LLM serving caused by KV cache management, markedly improving memory utilization and throughput. PagedAttention, inspired by operating system paging mechanisms, enables non-contiguous storage, dynamic allocation, and efficient sharing of the KV cache (supporting parallel sampling, beam search, shared prefixes, etc.).\nBuilding on V0, vLLM V1 comprehensively refactors and optimizes the core architecture. Through a multiprocessing architecture, flexible scheduler, zero-overhead prefix caching, symmetric tensor-parallel architecture, efficient input preparation, torch.compile integration, enhanced MLLM support, and FlashAttention 3 integration, V1 further reduces CPU overhead and enhances overall system performance, flexibility, and scalability, laying a solid foundation for rapid iteration of new features in the future.\nReferences [1] Kwon, Woosuk, et al. \u0026ldquo;Efficient memory management for large language model serving with pagedattention.\u0026rdquo; Proceedings of the 29th Symposium on Operating Systems Principles. 2023.\n[2] vLLM Team. \u0026ldquo;vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention.\u0026rdquo; vLLM Blog, June 20, 2023.\n[3] vLLM Team. \u0026ldquo;vLLM V1: A Major Upgrade to vLLM\u0026rsquo;s Core Architecture.\u0026rdquo; vLLM Blog, Jan 27, 2025.\n[4] NVIDIA. \u0026ldquo;FasterTransformer.\u0026rdquo; GitHub Repository, 2023.\n[5] Yu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. \u0026ldquo;Orca: A Distributed Serving System for Transformer-Based Generative Models.\u0026rdquo; In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 2022.\n[6] OpenAI. \u0026ldquo;API Reference - Streaming.\u0026rdquo; OpenAI Platform Documentation, 2025.\n[7] Wolf, Thomas, et al. \u0026ldquo;Transformers: State-of-the-Art Natural Language Processing.\u0026rdquo; In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.\n[8] Hugging Face. \u0026ldquo;Text Generation Inference.\u0026rdquo; GitHub Repository, 2025.\n[9] Shoeybi, Mohammad, et al. \u0026ldquo;Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.\u0026rdquo; arXiv preprint arXiv:1909.08053 (2019).\n[10] InternLM Team. \u0026ldquo;LMDeploy.\u0026rdquo; GitHub Repository, 2025.\n[12] Shah, Jay, et al. \u0026ldquo;Flashattention-3: Fast and accurate attention with asynchrony and low-precision.\u0026rdquo; Advances in Neural Information Processing Systems 37 (2024): 68658-68685.\n[13] ModelTC. \u0026ldquo;LightLLM.\u0026rdquo; GitHub Repository, 2025.\n[14] Zheng, Lianmin, et al. \u0026ldquo;Sglang: Efficient execution of structured language model programs.\u0026rdquo; Advances in Neural Information Processing Systems 37 (2024): 62557-62583.\n[15] NVIDIA. \u0026ldquo;TensorRT-LLM.\u0026rdquo; GitHub Repository, 2025.\n[16] vLLM Team. \u0026ldquo;NYC vLLM Meetup Presentation.\u0026rdquo; Google Slides, 2025.\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui. (May 2025). vLLM: High-Throughput, Memory-Efficient LLM Serving. https://syhya.github.io/posts/2025-05-17-vllm\nOr\n@article{syhya2025vllm-en, title = \u0026#34;vLLM: High-Throughput, Memory-Efficient LLM Serving\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;May\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-05-17-vllm\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-05-17-vllm/","summary":"\u003cp\u003eAs the parameters of Large Language Models (LLMs) continue to grow, deploying and serving these models presents significant challenges. \u003ca href=\"https://github.com/vllm-project/vllm\"\u003evLLM\u003c/a\u003e is an open-source library designed for fast, convenient, and cost-effective LLM inference and online serving. Its core lies in the \u003cstrong\u003ePagedAttention\u003c/strong\u003e algorithm, which efficiently manages the KV Cache in the attention mechanism.\u003c/p\u003e\n\u003ch2 id=\"evaluation-metrics\"\u003eEvaluation Metrics\u003c/h2\u003e\n\u003cp\u003eTo evaluate the performance of LLM inference and serving engines, we primarily focus on the following metrics:\u003c/p\u003e","title":"vLLM: High-Throughput, Memory-Efficient LLM Serving"},{"content":"Humans interact with the world through multiple senses (vision, hearing, touch, etc.), with each sensory channel offering unique advantages in representing and communicating specific concepts. This multimodal interaction fosters our deep understanding of the world. One of the core goals in the field of artificial intelligence is to develop general-purpose assistants that can effectively follow multimodal instructions (such as visual and linguistic ones), enabling them to perform various real-world tasks like humans. In recent years, with the release of models like GPT-4o (OpenAI, 2024), Gemini 2.5 Pro (DeepMind, 2025), and o3/o4-mini (OpenAI, 2025), Multimodal Large Language Models (MLLMs) have made significant progress. They can not only understand information from multiple modalities like images, videos, and audio but also perform complex reasoning and generation.\nNotations The following table lists the key mathematical symbols used in this article and their meanings to help you read more easily.\nSymbol Description \\( I, \\mathbf{X}_v \\) Image input, \\( I \\) usually refers to the raw image matrix \\( \\in \\mathbb{R}^{H \\times W \\times C} \\) \\( T, \\mathbf{X}_c, \\mathbf{X}_q, \\mathbf{X}_a, \\mathbf{X}_{\\text{instruct}} \\) Text input, specifically could refer to image caption (\\( \\mathbf{X}_c \\)), user question (\\( \\mathbf{X}_q \\)), model answer (\\( \\mathbf{X}_a \\)), or instruction (\\( \\mathbf{X}_{\\text{instruct}} \\)) \\( V, \\mathbf{Z}_v \\) Raw image features or embedding sequence output by the image encoder \\( L, \\mathbf{H}_q, \\mathbf{H}_a \\) Text features or embedding sequence output by the text encoder \\( \\mathbf{H}_v \\) Visual token sequence input to the LLM after processing by a projection layer \\( Z \\) Query embeddings output by Q-Former, serving as a compressed representation of visual information \\( P_Z \\) Soft Visual Prompt derived from the Q-Former output \\( I_e, T_e \\) Image and text embeddings in CLIP\u0026rsquo;s shared multimodal embedding space \\( z_p \\) Embedding vector of a single image patch after linear projection in ViT \\( x_{class} \\) Embedding of the learnable [class] token used for classification tasks in ViT \\( x_i \\) The \\( i \\)-th element or token in a sequence (e.g., word \\( w_i \\) in a text sequence) \\( E_{img}, g(\\cdot) \\) Image encoder model (e.g., ViT) \\( E_{text}, f_{\\phi}(\\cdot) \\) Text encoder or LLM \\( E, \\mathbf{W}, \\mathbf{W}_i, \\mathbf{W}_t \\) Linear projection matrix, used for feature transformation or modality alignment \\( E_{pos} \\) Positional encoding vector, used to provide sequence position information to the Transformer \\( Q, K, V \\) Query, Key, Value matrices in the attention mechanism \\( W_Q, W_K, W_V \\) Learnable projection matrices used to compute Q, K, V from input \\( \\theta, \\phi \\) Set of trainable parameters for the entire model or a specific part (e.g., LLM \\( \\phi \\)) \\( P \\) Side length of an image patch defined in the ViT model \\( N \\) Batch Size, usually refers to the number of samples in a batch \\( N_{patches} \\) Number of image patches the ViT model divides an image into \\( D \\) Main dimension of embedding vectors in the model \\( d, d_k \\) Dimension of the key vector in the attention mechanism, used for scaling the dot product \\( T_{turns} \\) Total number of conversation turns in multi-turn dialogue data \\( \\mathcal{L} \\) Loss function, the objective optimized by the model (e.g., \\( \\mathcal{L}_{ITC}, \\mathcal{L}_{ITM}, \\mathcal{L}_{LM}, \\mathcal{L}_{CLIP}, \\mathcal{L}_{siglip} \\)) \\( \\tau \\) Learnable parameter, such as temperature in contrastive loss or KL regularization weight in reinforcement learning \\( \\lambda \\) Hyperparameter, such as the weight of different loss terms or length penalty factor in reinforcement learning \\( y \\) Target label or category (e.g., ITM loss); or the final answer generated by the model (e.g., Kimi-VL RL) \\( x \\) Input data, context, or question \\( z \\) Intermediate reasoning steps or chain-of-thought generated by the model \\( y^* \\) Reference answer or ground truth answer $\\operatorname{sim}(u, v) = s(u, v)$ Similarity calculation between vectors \\( u \\) and \\( v \\), usually cosine similarity \\( \\mathbb{E} \\) Mathematical expectation KL KL Divergence (Kullback–Leibler Divergence), used to measure the difference between two probability distributions \\( \\pi_{\\theta} \\) Policy model, outputs actions or text sequences based on parameters \\( \\theta \\) \\( r \\) Reward function, evaluates the quality of the generated output Multimodal Fundamentals Before diving into specific technologies, let\u0026rsquo;s understand some basic concepts of multimodality.\nWhat is Multimodality? Multimodality refers to the use of multiple different types of data or information channels (modalities) to represent and process information. Humans are inherently multimodal beings; we perceive and understand the world through vision, hearing, touch, smell and taste. In the field of artificial intelligence, multimodal learning aims to build models capable of processing and correlating information from different modalities (such as text, images, videos, audio, etc.).\nFig. 1. Multimodality Data. (Image source: GPT-4o Image Generation)\nCommon Modalities:\nText: Natural language text, the primary means of information transmission and knowledge expression. Image: Static visual information, containing rich details of scenes, objects, and textures. Video: Dynamic visual information, composed of sequential image frames, often accompanied by audio. Video contains not only spatial information but also temporal information. Audio: Sound information, including speech, music, and environmental sounds. Others: Tabular data, 3D point clouds, sensor data (e.g., radar, LiDAR), biological signals (e.g., EEG, ECG), etc. Why Do We Need Multimodal AI? More Comprehensive World Understanding: The real world is multimodal. A single modality often provides only partial information. For example, text descriptions alone may struggle to fully convey a complex scene, whereas combining images or videos offers more intuitive and richer information. Multimodal models can integrate information from different sources to form a more comprehensive and accurate understanding. Enhanced Task Performance: In many tasks, combining information from multiple modalities can significantly improve performance. For instance, in Visual Question Answering (VQA), the model needs to understand both the image content and the text question to provide the correct answer. In video captioning, combining visual frames and audio information can generate more vivid and accurate descriptions. More Natural Interaction: Multimodal AI makes human-computer interaction more natural and flexible. Users can interact with AI systems through various means like voice, text, and images, and the AI system can respond in multiple modalities (e.g., generating text replies with images, or generating voice answers). Unlocking New Application Scenarios: Multimodal capabilities have given rise to many new applications, such as autonomous driving (fusing data from cameras, radar, LiDAR), medical diagnosis (combining medical images and patient records), content creation (text-to-image, text-to-video), virtual assistants, robot interaction, etc. Promoting Accessibility: Multimodal technology can assist individuals with sensory impairments. For example, image captioning can help visually impaired people understand image content, while speech recognition and synthesis can aid those with hearing or speech impairments in communication. Common Multimodal Tasks The following table lists some common multimodal tasks, which typically require processing and generating information by combining multiple modalities.\nTask Name Description Visual Question Answering (VQA) Generate text answers based on an image and a related question. Image/Video Captioning Generate natural language descriptions for images or videos. Text-to-X Generation Generate corresponding image, video, or audio content from text descriptions. Cross-Modal Retrieval Use one modality (e.g., text) to query relevant data in another modality (e.g., image). Multimodal Sentiment Analysis Determine sentiment by combining information from text, audio, video, etc. Visual Reasoning Perform logical judgment and relationship inference based on image or video content. Visual Language Navigation (VLN) Guide an agent to navigate in a visual environment based on natural language instructions. Multimodal Machine Translation (MMT) Utilize relevant image information to assist text translation and resolve ambiguity. Audio-Visual Speech Recognition (AVSR) Perform speech recognition by combining audio signals and visual information of the speaker\u0026rsquo;s lip movements. Visual Grounding Associate words or phrases in text with corresponding regions or objects in images or videos. Key Technologies The development of multimodal large models is driven by a series of technologies. The figure below visually illustrates the related technologies for multimodal understanding and generation. The author will introduce some key models and methods among them.\nFig. 2. The general model architecture of MM-LLMs and the implementation choices for each component. (Image source: Zhang et al., 2024)\nVision Transformer (ViT) Vision Transformer (ViT) (Dosovitskiy et al., 2020) successfully applied the Transformer architecture to the field of computer vision, becoming the preferred visual encoder for many advanced MLLMs today.\nFig. 3. ViT model overview. (Image source: Dosovitskiy et al., 2020)\nCore Idea: ViT treats an image as a sequence of Image Patches and then uses the Transformer\u0026rsquo;s self-attention mechanism to process these patches, thereby capturing global dependencies.\nWorkflow:\nPatch Embedding: Divide the input image \\( I \\in \\mathbb{R}^{H \\times W \\times C} \\) into \\( N_{patches} \\) fixed-size non-overlapping image patches \\( x_p \\in \\mathbb{R}^{P^2 \\times C} \\), where \\( (H, W) \\) is the image resolution, \\( C \\) is the number of channels, \\( P \\) is the size of each patch, and \\( N_{patches} = HW/P^2 \\) is the number of patches. Linear Projection: Flatten each patch \\( x_p \\) into a 1D vector and map it to a \\( D \\)-dimensional embedding space using a learnable linear projection matrix \\( E \\), resulting in patch embeddings \\( z_p = x_p E \\). Position Embedding: To preserve the spatial position information of the patches, ViT adds learnable Position Embeddings \\( E_{pos} \\) to the patch embeddings. \\[ z_0 = [x_{class}; z_p^1; z_p^2; \\dots; z_p^{N_{patches}}] + E_{pos}, \\quad E \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}, E_{pos} \\in \\mathbb{R}^{(N_{patches}+1) \\times D} \\] Often, a learnable [class] token embedding \\( x_{class} \\) is also added. Its corresponding vector at the Transformer\u0026rsquo;s output is used for image classification tasks. Transformer Encoder: Feed the sequence of patch embeddings with added position encodings into a standard Transformer encoder. The encoder consists of multiple layers of Multi-Head Self-Attention (MSA) and Feed Forward Network (FFN). MSA: Captures global dependencies between image patches. For an input sequence \\( Z_{l-1} \\), the self-attention is computed as: \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] where \\( Q = Z_{l-1}W_Q, K = Z_{l-1}W_K, V = Z_{l-1}W_V \\) are the query, key, and value matrices, and \\( d_k \\) is the dimension of the key vectors. Multi-head attention splits \\( Q, K, V \\) into multiple heads, computes attention in parallel, and then concatenates the results. FFN: Typically consists of two linear layers and a non-linear activation function (e.g., GELU). The computation in each layer can be represented as: \\[ Z'_l = \\text{MSA}(\\text{LN}(Z_{l-1})) + Z_{l-1} \\] \\[ Z_l = \\text{FFN}(\\text{LN}(Z'_l)) + Z'_l \\] where LN denotes Layer Normalization. Output: The output \\( Z_L \\) of the Transformer encoder serves as the image\u0026rsquo;s feature representation \\( V \\). Fig. 4. Performance versus pre-training compute for different architectures: Vision Transformers, ResNets, and hybrids. Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models. (Image source: Dosovitskiy et al., 2020)\nCompared to traditional Convolutional Neural Networks (CNNs), ViT offers the following advantages:\nGlobal Dependency Modeling: Self-attention directly connects any two patches, explicitly capturing long-range spatial relationships, making it better at integrating semantic information across the entire image than traditional CNNs. Strong Transferability from Large-Scale Pre-training: After pre-training on massive datasets like JFT-300M or ImageNet-22K, ViT can be easily transferred to over 20 downstream tasks such as classification, detection, and segmentation, with performance scaling almost linearly with model/data size. Simple Architecture, Easy to Scale and Parallelize: The pure Transformer stack is easy to scale in three dimensions: depth, width, and input resolution. Computations consist mainly of matrix multiplications and Softmax, naturally suited for large-batch parallel processing and mixed-precision training on GPUs/TPUs. As research progresses, ViT itself is continuously being optimized to meet the demands of MLLMs:\nNative Dynamic Resolution: Traditional ViTs typically require a fixed input resolution. Models like Qwen2-VL and Kimi-VL have introduced the capability to handle dynamic resolutions. They often remove absolute position embeddings in ViT and instead use 2D rotary position embeddings to encode two-dimensional spatial information. This allows the model to process images of arbitrary resolutions and aspect ratios, converting them into variable-length visual token sequences, better preserving detailed information. Kimi-VL\u0026rsquo;s MoonViT also borrows the image packing technique from NaViT, packing sequences of image patches with different resolutions into the Transformer, improving training efficiency. Window Attention: To reduce the quadratic computational complexity of self-attention when processing high-resolution images, Qwen2.5-VL employs window attention in most layers of its ViT. Attention computation is restricted within local windows, making the complexity linear with respect to the number of patches, significantly improving efficiency while maintaining global information interaction through a few full attention layers. Architecture Alignment with LLM: Models like Qwen2.5-VL and Kimi-VL have also fine-tuned their ViT architectures to be closer to LLM designs, such as using RMSNorm for normalization and SwiGLU as the activation function, to enhance computational efficiency and cross-modal compatibility. CLIP CLIP (Contrastive Language-Image Pre-training) (Radford et al., 2021) is a landmark work in the multimodal field. It proposed a simple yet effective method for learning the association between images and text, laying the foundation for many subsequent MLLMs.\nCore Idea: CLIP aims to learn a Multimodal Embedding Space where matched image-text pairs have high similarity, and mismatched pairs have low similarity. It achieves this through Contrastive Learning, leveraging natural language supervision.\nArchitecture: CLIP consists of two main parts:\nImage Encoder: Can be a ResNet or ViT, responsible for encoding the input image \\( I \\) into image features \\( V \\). Text Encoder: Typically a Transformer, responsible for encoding the input text \\( T \\) into text features \\( L \\). Linear Projection Layer: Projects the image features \\( V \\) and text features \\( L \\) into the shared multimodal embedding space, obtaining \\( I_e = V W_i \\) and \\( T_e = L W_t \\), where \\( W_i \\) and \\( W_t \\) are learnable projection matrices. Fig. 5. CLIP Architecture Overview. CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset\u0026rsquo;s classes. (Image source: Radford et al., 2021)\nTraining Data: CLIP\u0026rsquo;s success is largely attributed to its massive pre-training dataset, WIT (WebImageText). The research team collected 400 million (image, text) pairs from the internet. They built the dataset by searching for approximately 500,000 query terms (derived from Wikipedia vocabulary, high-frequency bigrams, Wikipedia article titles, and WordNet synsets), limiting the number of samples per query to a maximum of 20,000 to balance the data distribution. This approach of using native web image-text pairs is called natural language supervision, which avoids expensive manual annotation and allows for easy scaling of data size.\nContrastive Loss: The core of CLIP is the contrastive learning objective. Given a batch of \\( N \\) (image, text) pairs \\( \\{(I_1, T_1), \\dots, (I_N, T_N)\\} \\), the model\u0026rsquo;s goal is to predict which of the \\( N \\times N \\) possible pairings are the true pairings.\nCompute all image embeddings \\( \\{I_{e,1}, \\dots, I_{e,N}\\} \\) and text embeddings \\( \\{T_{e,1}, \\dots, T_{e,N}\\} \\). L2 normalization is typically applied, dividing each image or text embedding by its own L2 norm (Euclidean norm). Calculate the Cosine Similarity between all \\( N^2 \\) pairs \\( (I_{e,i}, T_{e,j}) \\). \\[ \\text{logits}_{i,j} = \\text{sim}(I_{e,i}, T_{e,j}) \\cdot \\exp(\\tau) = \\frac{I_{e,i} \\cdot T_{e,j}}{\\|I_{e,i}\\| \\|T_{e,j}\\|} \\cdot \\exp(\\tau) \\] where \\( \\tau \\) is a learnable temperature parameter used to scale the range of the logits. Compute the Symmetric Cross-Entropy Loss. The problem is treated as two classification tasks: For each image \\( I_i \\), find the matching text \\( T_i \\) among the \\( N \\) texts. The loss is \\( \\mathcal{L}_{\\text{image}} \\). For each text \\( T_j \\), find the matching image \\( I_j \\) among the \\( N \\) images. The loss is \\( \\mathcal{L}_{\\text{text}} \\). The total loss is: \\[ \\mathcal{L}_{CLIP} = \\frac{1}{2} (\\mathcal{L}_{\\text{image}} + \\mathcal{L}_{\\text{text}}) \\] where, \\[ \\mathcal{L}_{\\text{image}} = -\\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(\\text{sim}(I_{e,i}, T_{e,i}) / \\tau)}{\\sum_{j=1}^N \\exp(\\text{sim}(I_{e,i}, T_{e,j}) / \\tau)} \\] \\[ \\mathcal{L}_{\\text{text}} = -\\frac{1}{N} \\sum_{j=1}^N \\log \\frac{\\exp(\\text{sim}(I_{e,j}, T_{e,j}) / \\tau)}{\\sum_{i=1}^N \\exp(\\text{sim}(I_{e,i}, T_{e,j}) / \\tau)} \\] This loss function encourages the similarity of positive pairs (matching image-text) to be higher than that of negative pairs (mismatched image-text). CLIP Core Pseudocode # image_encoder - ResNet or Vision Transformer # text_encoder - CBOW or Text Transformer # I[n, h, w, c] - minibatch of aligned images # T[n, l] - minibatch of aligned texts # W_i[d_i, d_e] - learned proj of image to embed # W_t[d_t, d_e] - learned proj of text to embed # t - learned temperature parameter (tau in text) # extract feature representations of each modality I_f = image_encoder(I) #[n, d_i] T_f = text_encoder(T) #[n, d_t] # joint multimodal embedding [n, d_e] # l2_normalize projects the embeddings onto the unit hypersphere I_e = l2_normalize(np.dot(I_f, W_i), axis=1) T_e = l2_normalize(np.dot(T_f, W_t), axis=1) # scaled pairwise cosine similarities [n, n] # The temperature parameter t scales the logits # Note: using dot product on normalized vectors is equivalent to cosine similarity logits = np.dot(I_e, T_e.T) * np.exp(t) # symmetric loss function # labels are the indices [0, 1, ..., n-1] indicating the correct pairings labels = np.arange(n) # Calculate cross-entropy loss for image-to-text classification # (Predict correct text for each image) loss_i = cross_entropy_loss(logits, labels, axis=1) # axis=1 for softmax over columns # Calculate cross-entropy loss for text-to-image classification # (Predict correct image for each text) loss_t = cross_entropy_loss(logits, labels, axis=0) # axis=0 for softmax over rows # Final loss is the average of the two losses loss = (loss_i + loss_t)/2 Zero-Shot Transfer: CLIP\u0026rsquo;s powerful capability lies in its zero-shot transfer performance. For a new image classification task, without any fine-tuning, CLIP can make predictions as follows:\nObtain all class names for the task (e.g., \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;dog\u0026rdquo;). Use Prompt Engineering to structure the class names into sentences, like \u0026ldquo;A photo of a {label}.\u0026rdquo; This helps bridge the distribution gap between the pre-training data (often sentences) and downstream tasks (often word labels). The CLIP paper found that using prompt templates and ensembling multiple prompts significantly improves performance (nearly 5% gain on ImageNet). Use CLIP\u0026rsquo;s text encoder to compute the text embeddings for each constructed sentence. These embeddings form the weight vectors of the zero-shot classifier. For a new image to be classified, use CLIP\u0026rsquo;s image encoder to compute its image embedding. Calculate the cosine similarity between this image embedding and all class text embeddings. The class with the highest similarity is predicted as the result. Fig. 6. Prompt engineering and ensembling improve zero-shot performance. Compared to the baseline of using contextless class names, prompt engineering and ensembling boost zero-shot classification performance by almost 5 points on average across 36 datasets. This improvement is similar to the gain from using 4 times more compute with the baseline zero-shot method but is \u0026lsquo;free\u0026rsquo; when amortized over many predictions. (Image source: Radford et al., 2021)\nImpact of CLIP: CLIP demonstrated that powerful, transferable visual representations can be learned through large-scale natural language supervision and contrastive learning. Its learned multimodal embedding space and strong image encoder have been widely adopted in subsequent MLLMs (like Flamingo, BLIP-2, LLaVA) and text-to-image models (like DALL-E 2, Stable Diffusion).\nCLIP primarily focuses on learning aligned representations but has limited capabilities in generative tasks. Subsequent work began exploring unified model architectures capable of both understanding and generation.\nBLIP BLIP (Bootstrapping Language-Image Pre-training) (Li et al., 2022) aimed to address the limitations of existing Vision-Language Pre-training (VLP) methods in terms of both models and data: models often excel at either understanding or generation, but not both; data relies on massive and noisy web image-text pairs.\nBLIP proposed the Multimodal Encoder-Decoder (MED) architecture, designed to unify understanding and generation tasks. It combines the advantages of CLIP\u0026rsquo;s contrastive learning and autoregressive generation, capable of handling multimodal data.\nFig. 7. BLIP Pre-training Model Architecture and Objectives (same parameters have the same color). We propose multimodal mixture of encoder-decoder (MED), a unified vision-language model which can operate in one of the three functionalities. (Image source: Li et al., 2022)\nImage Encoder: Uses ViT. Text Encoder/Decoder: Based on the BERT architecture but modified to accommodate multimodal tasks and different functional modes. Unimodal Encoder: Standard ViT and BERT, processing images and text separately. Image-grounded Text Encoder: Inserts Cross-Attention (CA) layers between the Self-Attention (SA) layer and the Feed-Forward Network (FFN) layer in each Transformer block of the text encoder to inject visual information. A [Encode] token is prepended to the text input, and its output embedding serves as the multimodal representation of the image-text pair. Image-grounded Text Decoder: Replaces the bidirectional SA layers in the encoder with Causal Self-Attention layers for autoregressive generation. Shares the CA and FFN layers with the encoder. Uses a [Decode] token as the sequence start symbol. Pre-training Objectives: BLIP jointly optimizes three objectives:\nImage-Text Contrastive (ITC) Loss: Similar to CLIP, uses the unimodal encoders to align the feature spaces of images and text. BLIP adopts the momentum encoder and soft label strategy proposed by ALBEF (Li et al., 2021) to improve contrastive learning. $$L_{ITC} = \\frac{1}{2N} \\sum_{i=1}^{N} \\left( -\\log \\frac{\\exp(s(v_i, t_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(s(v_i, t_j)/\\tau)} -\\log \\frac{\\exp(s(v_i, t_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(s(v_j, t_i)/\\tau)} \\right)$$ where $v_i, t_j$ are image and text features, $s$ is the similarity function, and $\\tau$ is the temperature parameter.\nImage-Text Matching (ITM) Loss: Uses the image-grounded text encoder to learn fine-grained image-text alignment. This is a binary classification task predicting whether an image-text pair is matched or mismatched. A hard negative mining strategy is employed. $$L_{ITM} = -\\mathbb{E}_{(I,T)\\sim D} [y \\log p_{match} + (1-y) \\log(1 - p_{match})]$$ where $y$ is the label and $p_{match}$ is the matching probability.\nLanguage Modeling (LM) Loss: Uses the image-grounded text decoder to generate text descriptions based on the image. A standard cross-entropy loss (with label smoothing) is used. $$L_{L M}=-\\mathbb{E}_{(I, T) \\sim D} \\sum_{k=1}^L \\log P\\left(w_k \\mid I, w_{\\lt k} ; \\theta\\right)$$ where $w_k$ is a word in the text sequence and $\\theta$ are the model parameters.\nTotal Loss Function: These three losses are typically optimized jointly (e.g., summed with equal weights): $$L_{BLIP} = L_{ITC} + L_{ITM} + L_{LM}$$Parameter Sharing: For efficiency and the benefits of multi-task learning, the text encoder and decoder share all parameters except for the SA layers (i.e., embedding layer, CA layers, FFN layers).\nCapFilt (Captioning and Filtering) is an innovative dataset bootstrapping method used to generate high-quality synthetic captions for unlabeled web images and filter out noisy data (including original web text and synthetic text).\nFig. 8. BLIP Learning Framework. We introduce a captioner to produce synthetic captions for web images, and a filter to remove noisy image-text pairs. (Image source: Li et al., 2022)\nInitialization: Initialize two modules using a pre-trained MED model: a Captioner (image-grounded text decoder) and a Filter (image-grounded text encoder). Fine-tuning: Fine-tune the Captioner (using LM loss) and Filter (using ITC and ITM losses) separately on a high-quality human-annotated dataset (e.g., COCO). This is a lightweight process. Generation and Filtering: The Captioner generates synthetic captions \\( T_s \\) for web images \\( I_w \\). The Filter determines whether the original web text \\( T_w \\) and the synthetic text \\( T_s \\) match the image \\( I_w \\). Texts predicted as mismatched are considered noise and removed. Bootstrapped Dataset: Combine the filtered high-quality image-text pairs (from original web data and synthetic data) with human-annotated data to form a new bootstrapped dataset. Re-Pre-training: Pre-train a new BLIP model from scratch using the bootstrapped dataset. Effect: CapFilt significantly improved the model\u0026rsquo;s performance on various downstream tasks (like retrieval, captioning, VQA), demonstrating the effectiveness of improving noisy data quality through bootstrapping. BLIP also showcased the flexibility of a unified model for both understanding and generation tasks.\nBLIP-2 BLIP-2 (Li et al., 2023) addresses the high cost of VLP training by proposing an efficient pre-training strategy: freeze pre-trained image encoders and large language models, training only a lightweight bridging module, the Q-Former.\nCore Contributions:\nLeveraging Frozen Models: Avoids end-to-end training of the entire large model, significantly reducing computational costs and leveraging the capabilities of powerful pre-trained unimodal models. Q-Former (Querying Transformer): Proposed a lightweight Transformer structure as a trainable bridge connecting the frozen image encoder and the frozen LLM. Two-Stage Pre-training: Designed a two-stage strategy to effectively bridge the modality gap: Stage 1: Vision-Language Representation Learning: Bootstrapped from a frozen image encoder. Stage 2: Vision-to-Language Generative Learning: Bootstrapped from a frozen LLM. Architecture (Q-Former):\nQ-Former is a lightweight Transformer with 188M parameters. It uses a set of Learnable Query Embeddings (e.g., 32 vectors of 768 dimensions) as input. These query vectors interact with each other through Self-Attention layers. They interact with the image features output by the frozen image encoder through Cross-Attention layers to extract visual information. The output of the query vectors \\( Z \\) (e.g., \\( 32 \\times 768 \\) dimensions) has a much lower dimension than the original image features, acting as an Information Bottleneck, forcing the Q-Former to extract the visual information most relevant to the language model. Q-Former internally contains two sub-modules, an image Transformer and a text Transformer, which share self-attention layers. Fig. 9. (Left) Model architecture of Q-Former and BLIP-2\u0026rsquo;s first-stage vision-language representation learning objectives. (Right) The self-attention masking strategy for each objective to control query-text interaction. (Image source: Li et al., 2023)\nTwo-Stage Pre-training:\nStage 1 (Representation Learning):\nConnect the Q-Former to a frozen image encoder (e.g., CLIP ViT-L/14, EVA-CLIP ViT-g/14). Pre-train using image-text pairs, aiming for the Q-Former\u0026rsquo;s query vectors to learn to extract visual representations most relevant to the text. Jointly optimize three objectives similar to BLIP (sharing input format and model parameters, but freezing the image encoder and training only the Q-Former): Image-Text Contrastive (ITC) Loss: Align the Q-Former\u0026rsquo;s output query representations \\( z \\) and text representations \\( t \\). Uses In-batch Negatives. $$L_{ITC} = \\frac{1}{2N} \\sum_{i=1}^{N} \\left( -\\log \\frac{\\exp(s(z_i, t_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(s(z_i, t_j)/\\tau)} -\\log \\frac{\\exp(s(z_i, t_i)/\\tau)}{\\sum_{j=1}^{N} \\exp(s(z_j, t_i)/\\tau)} \\right)$$ Image-Text Matching (ITM) Loss: Predict whether an image-text pair matches. Uses the Q-Former\u0026rsquo;s output multimodal query representation for classification. $$L_{ITM} = -\\mathbb{E}_{(I,T)\\sim D} [y \\log p_{match} + (1-y) \\log(1 - p_{match})]$$ Image-grounded Text Generation (ITG) Loss: Train the Q-Former to generate text. The query vectors need to capture all information required for text generation and pass it to the text tokens via self-attention layers. $$L_{ITG} = -\\mathbb{E}_{(I,T)\\sim D} \\sum_{k=1}^{L} \\log P(w_k | Z_q, w_{\\lt k}; \\theta_{Q-Former})$$ where $Z_q$ is the query output of the Q-Former. Different self-attention masks are used to control query-text interaction for different objectives. Stage 1 Total Loss Function: $$L_{Stage1} = L_{ITC} + L_{ITM} + L_{ITG}$$ Stage 2 (Generative Learning):\nConnect the Q-Former pre-trained in Stage 1 (and its connected frozen image encoder) to a frozen LLM (e.g., OPT series, FlanT5 series). Use a Fully Connected (FC) Layer to linearly project the Q-Former\u0026rsquo;s output query embeddings \\( Z \\) to the same dimension as the LLM\u0026rsquo;s text embeddings, obtaining soft visual prompts $P_Z$. Prepend the projected query embeddings as Soft Visual Prompts to the LLM\u0026rsquo;s input text embeddings. Training Objective: Train the Q-Former (and the FC layer) so that its output visual representations can be understood by the frozen LLM and used for text generation. For Decoder-only LLMs (e.g., OPT): Use standard language modeling loss, i.e., generate subsequent text based on the visual prompt. For Encoder-Decoder LLMs (e.g., FlanT5): Use prefix language modeling loss, splitting the text into prefix and suffix. The visual prompt and prefix are input to the Encoder, and the Decoder generates the suffix. $$L_{Stage2} = L_{LM} = -\\mathbb{E}_{(I, T_{prompt}, T_{gen})\\sim D} \\sum_{k=1}^{M} \\log P_{LLM}(w_k | P_Z, T_{prompt}, w_{\\lt k}; \\theta_{LLM\\_frozen})$$ where $\\theta_{L L M_{-} \\text {frozen }}$ are the parameters of the frozen LLM, used only for forward propagation and not involved in gradient updates. Fig. 10. BLIP-2\u0026rsquo;s second-stage vision-to-language generative pre-training, which bootstraps from frozen large language models (LLMs). (Top) Bootstrapping a decoder-based LLM (e.g. OPT). (Bottom) Bootstrapping an encoder-decoder-based LLM (e.g. FlanT5). (Image source: Li et al., 2023)\nEffects and Advantages:\nEfficiency: Since only the lightweight Q-Former is trained, the pre-training cost is much lower than end-to-end training of large models. High Performance: Achieved SOTA levels on tasks like VQA, Captioning, and Retrieval, even surpassing models with significantly more parameters (like Flamingo). Versatility: Can easily connect to different frozen image encoders and LLMs, leveraging the latest advancements in respective fields. Zero-Shot Capability: Leveraging powerful frozen LLMs (especially instruction-tuned FlanT5), BLIP-2 demonstrated impressive zero-shot instruction-based image-to-text generation capabilities, performing various visual-language tasks based on natural language instructions (e.g., visual dialogue, visual knowledge reasoning). LLaVA LLaVA (Large Language and Vision Assistant) (Liu et al., 2023) is a significant work in the open-source community for Visual Instruction Tuning, being the first to attempt extending the instruction tuning concept from NLP to the multimodal domain.\nCore Contributions:\nProposed Visual Instruction Tuning: Explored applying instruction tuning to language-image multimodal models, aiming to build general-purpose visual assistants. GPT-Assisted Data Generation: Facing the lack of visual instruction data, innovatively used a language-only model GPT-4 to generate multimodal language-image instruction-following data containing visual content. Built LLaVA Model: Proposed an end-to-end trained architecture connecting a pre-trained visual encoder (CLIP ViT-L/14) and a large language model (LLM, Vicuna). Created Evaluation Benchmark: Constructed LLaVA-Bench, comprising diverse and challenging tasks to evaluate the instruction-following capabilities of multimodal models. Open Source Contribution: Released the GPT-4 generated visual instruction data, model code, and pre-trained weights, greatly promoting community research in this direction. GPT-Assisted Visual Instruction Data Generation:\nThe key challenge LLaVA addressed was the lack of large-scale, high-quality visual instruction-following data. The researchers proposed a method using existing multimodal large models like GPT-4 to generate such data based on existing image-text pairs, essentially a form of knowledge distillation from the closed-source GPT-4 model.\nChallenge Faced: Simply extending image-caption pairs into the format (Instruction: Describe the image, Image -\u0026gt; Answer: Caption) is cheap but lacks diversity in instructions and responses, as well as deep reasoning. Solution: Use GPT-4 as a \u0026ldquo;teacher model\u0026rdquo;. Since these models only accept text input, the researchers conveyed image content through Symbolic Representations: Captions: Provide overall or multi-faceted descriptions of the image scene. Bounding Boxes: Provide class concepts and spatial location information of objects in the image (e.g., person: [0.681, 0.242, 0.774, 0.694]). Prompting and In-context Learning: Input the symbolic representations (descriptions and bounding boxes) of the image to GPT-4. To guide GPT-4 to generate output in specific formats and content, the researchers manually designed a small number of high-quality Seed Examples, leveraging GPT-4\u0026rsquo;s In-context Learning ability for few-shot inference. Generating Three Types of Data (based on COCO images): Through carefully designed prompts, GPT-4 was guided to generate three types of instruction data: Conversation: Generate multi-turn dialogues simulating interaction between a human and an assistant about image content, including questions about object recognition, counting, localization, actions, relationships, etc. Detailed Description: Generate comprehensive, detailed descriptions of the image based on specific instructions (e.g., \u0026ldquo;Describe the image below in detail\u0026rdquo;). Complex Reasoning: Generate questions and answers requiring logical reasoning based on image content or combined with background knowledge (e.g., \u0026ldquo;What challenges might the person in the picture be facing?\u0026rdquo;). Fig. 11. One example to illustrate the instruction-following data. (Image source: Liu et al., 2023)\nDataset: A total of 158K unique language-image instruction samples were collected, specifically including: 58K conversation samples, 23K detailed description samples, and 77K complex reasoning samples. Experiments found that data generated by GPT-4 was generally of higher quality than that from ChatGPT. Fig. 12. LLaVA network architecture. (Image source: Liu et al., 2023)\nLLaVA\u0026rsquo;s architecture is designed to effectively combine the capabilities of pre-trained visual models and LLMs, as shown in the figure above.\nVisual Encoder \\( g(\\cdot) \\): Uses a frozen CLIP ViT-L/14 model. For an input image \\( \\mathbf{X}_{\\mathrm{v}} \\), it extracts visual features \\( \\mathbf{Z}_{\\mathrm{v}} = g(\\mathbf{X}_{\\mathrm{v}}) \\). The paper mentions experimenting with grid features from before and after the last Transformer layer.\nProjection Layer: Uses a trainable linear projection matrix \\( \\mathbf{W} \\) to map the visual features \\( \\mathbf{Z}_{\\mathrm{v}} \\) into the word embedding space of the language model. $$ \\mathbf{H}_{\\mathrm{v}} = \\mathbf{W} \\cdot \\mathbf{Z}_{\\mathrm{v}} $$ Here, \\( \\mathbf{H}_{\\mathrm{v}} \\) is a sequence of visual tokens whose dimension matches the LLM\u0026rsquo;s word embedding dimension. This simple linear projection is lightweight and efficient, facilitating rapid data-centric experiments. More complex connection methods (like gated cross-attention in Flamingo or Q-Former in BLIP-2) could be explored in future work.\nLarge Language Model (LLM) \\( f_{\\phi}(\\cdot) \\): Uses Vicuna, with its parameters denoted as \\( \\phi \\). The LLM receives the visual tokens \\( \\mathbf{H}_{\\mathrm{v}} \\) and the text instruction \\( \\mathbf{X}_{\\text{instruct}} \\), and autoregressively generates the answer \\( \\mathbf{X}_{\\mathrm{a}} \\).\nTwo-Stage Training:\nLLaVA employs a two-stage instruction fine-tuning process.\nStage 1: Feature Alignment Pre-training:\nGoal: Align the visual features \\( \\mathbf{H}_{\\mathrm{v}} \\) with the LLM\u0026rsquo;s word embedding space, which can be viewed as training a compatible \u0026ldquo;visual tokenizer\u0026rdquo; for the frozen LLM. Data: Used a filtered subset of the CC3M dataset (approx. 595K image-text pairs). These pairs were simply converted into instruction data: for an image \\( \\mathbf{X}_{\\mathrm{v}} \\), randomly select a simple descriptive instruction \\( \\mathbf{X}_{\\mathrm{q}} \\) (e.g., \u0026ldquo;Briefly describe this picture\u0026rdquo;), and use the original caption \\( \\mathbf{X}_{\\mathrm{c}} \\) as the answer \\( \\mathbf{X}_{\\mathrm{a}} \\). This can be considered a single-turn conversation. Training: Freeze the weights of the visual encoder \\( g(\\cdot) \\) and the LLM \\( f_{\\phi}(\\cdot) \\), and only train the projection layer \\( \\mathbf{W} \\). The training objective is to maximize the likelihood of the answer (i.e., the image caption). Stage 2: Fine-tuning End-to-End:\nGoal: Enhance the model\u0026rsquo;s instruction-following and conversational abilities on multimodal tasks. Data: Use the previously generated 158K visual instruction data (including conversation, detailed description, and complex reasoning types, sampled uniformly during training). Training: Freeze the visual encoder \\( g(\\cdot) \\), and train both the projection layer \\( \\mathbf{W} \\) and the LLM \\( f_{\\phi}(\\cdot) \\) weights. Training Objective:\nFor each image \\( \\mathbf{X}_{\\mathrm{v}} \\), multi-turn dialogue data \\( \\left(\\mathbf{X}_{\\mathrm{q}}^{1}, \\mathbf{X}_{\\mathrm{a}}^{1}, \\cdots, \\mathbf{X}_{\\mathrm{q}}^{T_{turns}}, \\mathbf{X}_{\\mathrm{a}}^{T_{turns}}\\right) \\) containing \\( T_{turns} \\) turns is generated, where \\( T_{turns} \\) is the total number of conversation turns. This data is organized into a sequence, and all answers \\( \\mathbf{X}_{\\mathrm{a}} \\) are treated as the model\u0026rsquo;s responses. The input sequence format follows the Vicuna style. In the \\( t \\)-th turn of the conversation, the instruction \\( \\mathbf{X}_{\\text{instruct}}^{t} \\) is defined as:\n$$ \\mathbf{X}_{\\text{instruct}}^{t} = \\left\\{ \\begin{array}{ll} \\text{Randomly choose } [\\mathbf{X}_{\\mathrm{q}}^{1}, \\mathbf{X}_{\\mathrm{v}}] \\text{or } [\\mathbf{X}_{\\mathrm{v}}, \\mathbf{X}_{\\mathrm{q}}^{1}], \u0026 \\text{ if } t=1 \\text{ (the first turn)} \\\\ \\mathbf{X}_{\\mathrm{q}}^{t}, \u0026 \\text{ if } t\u003e1 \\text{ (the remaining turns)} \\end{array} \\right. $$The objective is to predict the answer sequence \\( \\mathbf{X}_{\\mathrm{a}} = (\\mathbf{X}_{\\mathrm{a}}^{1}, \\dots, \\mathbf{X}_{\\mathrm{a}}^{T_{turns}}) \\). The model needs to maximize the probability of generating the correct answer sequence given the image \\( \\mathbf{X}_{\\mathrm{v}} \\) and all instructions \\( \\mathbf{X}_{\\text{instruct}} = (\\mathbf{X}_{\\text{instruct}}^{1}, \\dots, \\mathbf{X}_{\\text{instruct}}^{T_{turns}}) \\). For the complete answer sequence of length \\( L_{seq} \\) (concatenation of all \\( \\mathbf{X}_{\\mathrm{a}} \\) turns), the probability is calculated as follows:\n$$ p\\left(\\mathbf{X}_{\\mathrm{a}} \\mid \\mathbf{X}_{\\mathrm{v}}, \\mathbf{X}_{\\text {instruct }}\\right)=\\prod_{i=1}^{L_{seq}} p_{\\boldsymbol{\\theta}}\\left(x_i \\mid \\mathbf{X}_{\\mathrm{v}}, \\mathbf{X}_{\\text {instruct },\\lt i}, \\mathbf{X}_{\\mathrm{a},\\lt i}\\right) $$where:\n\\( \\boldsymbol{\\theta} \\) are the trainable parameters of the model. In Stage 1, \\( \\boldsymbol{\\theta} = \\{ \\mathbf{W} \\} \\). In Stage 2, \\( \\boldsymbol{\\theta} = \\{ \\mathbf{W}, \\phi \\} \\). \\( x_i \\) is the \\( i \\)-th token in the answer sequence \\( \\mathbf{X}_{\\mathrm{a}} \\). \\( \\mathbf{X}_{\\text{instruct},\\lt i} \\) and \\( \\mathbf{X}_{\\mathrm{a},\\lt i} \\) represent all instruction tokens and generated answer tokens received by the model before predicting \\( x_i \\). The training loss function is the Negative Log-Likelihood of the above probability, and the loss is calculated only on the answer tokens (i.e., tokens in \\( \\mathbf{X}_{\\mathrm{a}} \\)). Effects and Impact:\nLLaVA demonstrated impressive capabilities in multimodal dialogue, sometimes exhibiting behavior similar to multimodal GPT-4 on unseen images and instructions. After fine-tuning on the ScienceQA benchmark, the combination of LLaVA and GPT-4 achieved a state-of-the-art accuracy of 92.53% at the time.\nFig. 13. Accuracy (%) on Science QA dataset. (Image source: Liu et al., 2023)\nLLaVA\u0026rsquo;s success proved the effectiveness of visual instruction tuning. Its open-sourced data, code, and model greatly facilitated subsequent research on multimodal large models, paving new ways for building general-purpose AI assistants capable of understanding and following visual and language instructions.\nQwen-VL The Qwen-VL (Bai et al., 2023) model is the first open-source large vision-language model developed by the Qwen team. Its architecture consists of three main modules:\nLarge Language Model: Uses the pre-trained Qwen-7B text model as the language decoder. This part is responsible for understanding and generating text, consistent with standard LLM architectures.\nVisual Encoder: Employs a Vision Transformer to extract image features. Specifically, Qwen-VL initializes the visual encoding part using the ViT-bigG model from OpenCLIP. During training and inference, input images are resized to a specific resolution. The visual encoder extracts a set of image features by dividing the image into patches with a stride of 14.\nPosition-aware Vision-Language Adapter: To efficiently fuse long sequences of image features, an adapter is introduced to compress the visual feature sequence to a fixed length. Specifically, this adapter contains a set of randomly initialized learnable query vectors. It computes with the image features output by ViT through a single-layer cross-attention module, compressing the image features into a sequence of fixed length 256.\nThe attention calculation formula is as follows:\n$$ \\text{CrossAttn}(Q, K, V) = \\mathrm{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V $$where \\(Q\\) is the matrix of trainable query vectors defined within the adapter, and both \\(K\\) and \\(V\\) directly use the image feature sequence output by the visual encoder (ViT) as keys and values.\nThrough this mechanism, the adapter can select and aggregate the most relevant information from numerous image features based on the learned query vectors. Furthermore, to mitigate the potential loss of spatial position information during image feature compression, 2D absolute position encodings are additionally incorporated into the query-key pairs in the attention calculation, enhancing the perception of the image\u0026rsquo;s spatial structure.\nFig. 14. The training pipeline of the Qwen-VL series. (Image source: Bai et al., 2023)\nQwen-VL adopts a \u0026ldquo;three-stage\u0026rdquo; progressive training strategy to inject visual perception capabilities into the general large model. The first stage freezes the LLM and trains only the visual modules; the second stage unfreezes and performs joint multi-task training; the third stage involves instruction fine-tuning to obtain the chat model. In the figure above, the snowflake ❄ indicates frozen components, and the flame 🔥 indicates components participating in training.\nTraining Strategy: The Qwen-VL series employs a three-stage progressive training flow:\nPure Image-Text Pre-training Stage:\nFix the language model (7B) parameters, training only the visual encoder and VL adapter. Use approximately 1.4 billion pairs of weakly labeled image-text data (77.3% English, 22.7% Chinese). Images are uniformly scaled to a lower resolution (e.g., 224×224) for efficiency. Use autoregressive language modeling to train the model to generate image descriptions. After about 50,000 steps (1.5 billion samples), preliminary image-text alignment capability is achieved (Qwen-VL). Multi-task Joint Training Stage:\nUnfreeze the language model, training it end-to-end together with the visual part. Increase the input image resolution (e.g., 448×448 or higher). Incorporate various fine-grained visual tasks (e.g., image captioning, visual question answering, content localization, OCR recognition, etc.), covering 7 major task categories. Training data mixes datasets from multiple sources, adding about 24.8 million OCR data points and 7.8 million pure text data points. All task data are randomly mixed for training, with each sample prefixed by a task identifier and padded to a sequence length of 2048. The model significantly improves capabilities in image understanding, cross-modal retrieval, localization, reading, etc. Supervised Fine-tuning (SFT) Stage:\nFine-tune on multimodal instruction data (approx. 350K samples) to obtain the dialogue-enhanced version, Qwen-VL-Chat. Specifically design complex data for multi-image reasoning, fine-grained localization, and multi-turn interaction tasks. During fine-tuning, freeze the visual encoder again, fine-tuning only the language model and the adapter. The final model exhibits excellent multimodal dialogue, instruction following, and complex reasoning abilities. Qwen2-VL Qwen2-VL (Wang et al., 2024) is an upgraded version of Qwen-VL, making advancements in handling variable-resolution visual inputs and fusing multimodal positional information.\nFig. 15. Qwen2-VL is capable of accurately identifying and comprehending the content within images, regardless of their clarity, resolution, or extreme aspect ratios.: (Wang et al., 2024)\nAs seen in the figure above, Qwen2-VL can accurately identify and understand content within images of varying resolutions and aspect ratios. It primarily employs the following techniques:\nNative Dynamic Resolution: Inspired by NaViT (Dehghani et al., 2023), the model can process images of arbitrary resolutions and dynamically convert them into variable-length visual token sequences.\nRemoves absolute position embeddings from ViT and introduces 2D Rotary Position Embedding (2D-RoPE) (Su et al., 2024; Su, 2021) to encode 2D spatial information. During inference, variable-resolution images are processed in packed batches, limiting the total token length to manage memory usage. After ViT output, an MLP compresses adjacent \\( 2 \\times 2 \\) tokens into one, reducing the sequence length input to the LLM. Visual tokens are wrapped with \u0026lt;|vision_start|\u0026gt; and \u0026lt;|vision_end|\u0026gt;. Multimodal Rotary Position Embedding (M-RoPE): Proposed a novel position embedding method that can uniformly handle positional information for text, images, and videos.\nDecomposes RoPE into three components: Temporal, Height, and Width. Text: All three components use the same position ID, equivalent to 1D-RoPE. Image: Temporal ID is constant; Height and Width IDs are assigned based on the token\u0026rsquo;s 2D position in the image. Video: Temporal ID increases with frame number; Height and Width IDs are assigned as in images. Multimodal Input: Position IDs for different modalities increase sequentially. Advantage: Uniformly encodes multimodal positional information, reduces the magnitude of image/video position IDs, facilitating extrapolation to longer sequences during inference. Fig. 16. Illustration of M-RoPE. By decomposing rotary embedding into temporal, height, and width components, M-RoPE can explicitly model the positional information of text, images, and video in LLM. (Image source: Wang et al., 2024)\nUnified Image and Video Understanding: Adopts a mixed training paradigm and specific architectural designs (like 3D convolution for video processing) to handle both images and videos simultaneously. Trains on a mixture of image and video data. Videos are sampled at 2 FPS. Integrates 3D convolution in ViT to process video input (handling \\( 2 \\times 14 \\times 14 \\) 3D patches), reducing the number of tokens. Images are treated as two identical video frames. Dynamically adjusts video frame resolution, limiting the total number of tokens per video segment (e.g., to 16384). Training: Follows Qwen-VL\u0026rsquo;s three-stage training: ViT pre-training -\u0026gt; Full model pre-training -\u0026gt; LLM instruction fine-tuning. Pre-training data includes image-text pairs, OCR, interleaved image-text documents, VQA, video dialogue, image knowledge, etc. Instruction fine-tuning uses the ChatML format. Released models in 2B, 8B, and 72B sizes, exploring the scaling laws of MLLMs.\nEffect: Qwen2-VL demonstrates outstanding performance in understanding images of various resolutions and aspect ratios, long video understanding (over 20 minutes), and visual agent capabilities.\nQwen2.5-VL Qwen2.5-VL (Bai et al., 2025) further optimizes efficiency and temporal modeling capabilities based on Qwen2-VL.\nFig. 17. The Qwen2.5-VL framework demonstrates the integration of a vision encoder and a language model decoder to process multimodal inputs. The vision encoder is designed to handle inputs at their native resolution and supports dynamic FPS sampling. TMRoPE aligns time IDs with absolute time along the temporal dimension. (Image source: Bai et al., 2025)\nModel Optimization:\nQwen2.5-VL incorporates several optimizations over Qwen2-VL, primarily including:\nEfficient ViT Architecture: Introduces Window Attention mechanism in the Vision Transformer, restricting attention computation in most layers to local windows (e.g., $8 \\times 8$ patches). This makes the computational complexity grow linearly with the number of image patches, significantly improving efficiency for high-resolution image processing. Meanwhile, global attention is performed only in a few layers (e.g., every 8 layers) to retain overall context information.\nDynamic FPS Sampling \u0026amp; Video Processing: Introduces Dynamic FPS (Frames Per Second) sampling mechanism, extending the dynamic resolution concept to the temporal dimension, enhancing the model\u0026rsquo;s adaptability to videos with varying frame rates. For video processing, it maintains the 3D patch structure ($2 \\times 14 \\times 14$) and combines dynamic FPS with time-aware encoding to optimize overall temporal modeling.\nStronger Data \u0026amp; Task Capability Support: The model is pre-trained and fine-tuned on large-scale (4.1T tokens), high-quality datasets, with a focus on enhancing document parsing (tables, charts, formulas, sheet music, etc.), object localization (supporting point and box annotations), long video understanding (hour-level), and Agent multi-task capabilities, broadening the application boundaries of multimodal understanding.\nData Augmentation:\nFull Document Parsing Data: Constructed HTML-formatted data containing tables, charts, formulas, images, sheet music, chemical formulas, including layout bounding boxes and coordinates. Localization Data: Expanded bounding box and point localization data covering over 10,000 categories, and synthesized hard examples containing non-existent objects and multiple instances of objects. Tools like Grounding DINO and SAM were used for data synthesis. OCR Data: Increased multilingual OCR data (covering major European languages, Japanese, Korean, Arabic, Vietnamese, etc.), including various scenarios like handwriting, dense text, web pages, formulas, charts, and tables. Video Data: Added dense captioning data for long videos (over half an hour) and trained using dynamic FPS sampling. Timestamp annotations include both seconds and HMSF formats. Agent Data: Collected screenshots and action trajectories from mobile, web, and desktop environments, unified into a function call format, and synthesized CoT reasoning processes. Effect: Qwen2.5-VL achieved SOTA performance on document understanding, fine-grained localization, long video understanding, and Agent tasks. The 72B version rivals or even surpasses GPT-4o and Claude 3.5 Sonnet on several benchmarks.\nQwen2.5-Omni Fig. 18. Qwen2.5-Omni is an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. (Image source: Qwen Team, 2025)\nQwen2.5-Omni (Qwen Team, 2025) is an end-to-end multimodal model similar to GPT-4o (OpenAI, 2024), supporting input processing across all modalities including text, image, audio, and video, and capable of simultaneously streaming text and natural speech output.\nAs shown in the figure below, Qwen2.5-Omni adopts the Thinker-Talker architecture, with key features including:\nFig. 19. Qwen2.5-Omni Overview. Adopts Thinker-Talker architecture. Thinker is tasked with text generation while Talker focuses on generating streaming speech tokens by receiving high-level representations directly from Thinker. (Image source: Qwen Team, 2025)\nUnified Multimodal Processing and Temporal Modeling:\nOmni-modal Perception: A single model can simultaneously process text, image, audio, and video inputs, achieving unified multimodal understanding. Fig. 20. An illustration of Time-aligned Multimodal RoPE (TMRoPE). (Image source: Qwen Team, 2025)\nTime-aligned Multimodal RoPE (TMRoPE): Further optimizes TMRoPE based on Qwen2.5-VL. Through a Time-interleaving structure, video and audio frames are chunked every 2 seconds and arranged chronologically, with video preceding audio within each chunk. All modalities are aligned using absolute timestamps (40ms granularity) and position encoding (TMRoPE), enabling precise audio-video synchronization.\nInput Processing Details: Text uses the Qwen tokenizer; audio is sampled at 16kHz, represented as 128-channel Mel spectrograms (25ms window, 10ms step), with each frame approx. 40ms, processed by the Qwen2-Audio encoder; images/videos are processed via Qwen2.5-VL\u0026rsquo;s ViT architecture, with video supporting dynamic FPS sampling.\nThinker-Talker Architecture Design and Functional Decoupling:\nProposes the innovative Thinker-Talker architecture, decoupling text generation and speech generation to avoid mutual interference while allowing end-to-end joint training. Thinker: Based on Qwen2.5\u0026rsquo;s Transformer decoder, processes multimodal input, generates high-level hidden representations (containing semantic and prosodic information) and text token outputs. Talker: A dual-track autoregressive Transformer decoder, receives hidden representations and text tokens from the Thinker, combined with the ability to disambiguate speech, autoregressively generates discrete speech tokens. Thinker and Talker share historical context, supporting end-to-end training, enhancing speech generation consistency and context retention. Efficient Streaming Capability:\nInput Streaming: Audio and visual encoders use Block-wise Processing, supporting streaming input and Prefilling. Output Streaming: Discrete speech tokens generated by the Talker are fed in real-time to a Streaming Audio Codec. The codec employs Sliding Window Block Attention (looking back 2 blocks, looking ahead 1 block) based on a Diffusion Transformer (DiT) to control the receptive field, enabling streaming generation. Uses Flow Matching (Lipman et al., 2022) to convert discrete tokens into Mel spectrograms, which are then streamed into an improved BigVGAN (Lee et al., 2022) to convert spectrograms into audio waveforms, effectively reducing first-packet latency and improving generation real-time performance. Training: Consists of three stages: Encoder-LLM alignment -\u0026gt; Full model multimodal pre-training -\u0026gt; Long context pre-training (32k). The Talker undergoes separate three-stage training: Context learning -\u0026gt; DPO (optimizing stability) -\u0026gt; Multi-speaker instruction fine-tuning (improving naturalness).\nEffect: Qwen2.5-Omni performs comparably or better than Qwen2.5-VL (vision) and Qwen2-Audio (audio) of similar scale on respective unimodal benchmarks. Achieves SOTA on multimodal fusion benchmarks like OmniBench. Speech instruction following capability is close to text instructions. Speech generation robustness and naturalness surpass most existing models.\nKimi-VL Kimi-VL (Kimi Team, 2025) is an open-source efficient Mixture-of-Experts (MoE) vision-language model.\nFig. 21. Model architecture of Kimi-VL and Kimi-VL-Thinking, consisting of a MoonViT that allows native-resolution images, an MLP projector, and a Mixture-of-Experts (MoE) language decoder. (Image source: Kimi Team, 2025)\nArchitecture Details:\nEfficient MoE Architecture: The language model part uses an MoE architecture (based on Moonlight, similar to DeepSeek-V3 architecture), with a total of 16B parameters, activating only 2.8B parameters per inference (e.g., activating 2/8 experts per layer). This significantly reduces computational cost while maintaining model performance. Supports a maximum context window of 128K tokens, suitable for long documents, long videos, etc.\nNative Resolution Vision Encoder: Proposes a 400M parameter vision encoder, MoonViT, supporting native resolution processing for images without scaling or padding, maximally preserving image details. The architecture is based on ViT and incorporates the following techniques:\nNaViT Patch n\u0026rsquo; Pack strategy: Enables efficient batch processing of variable-length image sequences. Interpolated Absolute Position Embeddings: Initialized from SigLIP (Zhai et al. 2023), enhancing positional awareness. 2D Rotary Position Embeddings (2D-RoPE): Enhances spatial structure understanding. Dynamic Resolution Training: Samples images of different sizes during training to improve generalization. Multimodal Fusion Module: Image features output by MoonViT pass through a two-layer MLP Projector containing a Pixel Shuffle operation for spatial compression and format conversion. They are then concatenated with text token-level features and input into the MoE LLM for image-text fusion processing.\nLong Chain-of-Thought Reasoning : Based on the main model, a long-chain thinking training process, including Long CoT SFT and Reinforcement Learning optimization, enhances the model\u0026rsquo;s performance in multi-turn, multi-step reasoning tasks, supporting complex logical Q\u0026amp;A and scene understanding.\nTraining:\nFig. 22. The pre-training stages of Kimi-VL and Kimi-VL-Thinking, including ViT pre-training, joint pre-training, joint cooling, and joint long-context activation. (Image source: Kimi Team, 2025)\nPre-training (4 stages, 4.4T tokens total): ViT Training (2.1T): Train MoonViT separately (initialized from SigLIP), using contrastive loss SigLIP and cross-entropy caption generation. $$ \\mathcal{L}=\\mathcal{L}_{\\text {siglip }}+\\lambda \\mathcal{L}_{\\text {caption }}, \\text { where } \\lambda=2 $$ Joint Pre-training (1.4T): Jointly train ViT, Projector, LLM (initialized from Moonlight 5.2T checkpoint), mixing text and multimodal data. Joint Cooling (0.6T): Continue joint training with high-quality text and multimodal data. Joint Long Context Activation (0.3T): Expand context from 8K to 128K using long text, long video, and long document data. Fig. 23. The post-training stages of Kimi-VL and Kimi-VL-Thinking, including two stages of joint SFT in 32K and 128K context, and further long-CoT SFT and RL stages to activate and enhance long thinking abilities. (Image source: Kimi Team, 2025)\nPost-Training: Joint SFT: Use ChatML format, fine-tune on mixed text and multimodal instruction data (first 32K then 128K context). Long CoT SFT: Perform SFT using a small amount of high-quality long CoT data to activate long-chain reasoning capabilities. Reinforcement Learning: Employ the same Online Policy Mirror Descent algorithm used for the KIMI K1.5 model (Kimi Team, 2025) for training. This stage aims to further enhance the model\u0026rsquo;s complex reasoning and planning abilities (e.g., error identification, backtracking, solution optimization) through reinforcement learning, enabling it to utilize long Chain-of-Thought (long CoT) context for implicit search, thereby approximating the effectiveness of explicit planning algorithms while maintaining the simplicity of autoregressive generation. Core Objective: Optimize the policy model $\\pi_{\\theta}$ such that for a question $x \\in \\mathcal{D}$, the generated chain-of-thought $z$ and final answer $y$ maximize the expected reward based on the ground truth answer $y^$: $$ \\max _{\\theta} \\mathbb{E}_{\\left(x, y^{*}\\right) \\sim \\mathcal{D},(y, z) \\sim \\pi_{\\theta}}\\left[r\\left(x, y, y^{*}\\right)\\right] $$ where $r(x, y, y^)$ is typically a correctness reward of 0 or 1.\nReward Mechanism:\nCorrectness Reward ($r$): Primarily based on the correctness of the final answer $y$, judged according to the task type: For programming problems: Judged by running automatically generated test cases. For math problems: Evaluated using a high-precision Chain-of-Thought reward model (Chain-of-Thought RM, with 98.5% accuracy). For visual problems: Utilizes diverse data sources like real-world images, synthetic visual reasoning data, and text-rendered images, defining rewards based on task objectives. Length Penalty: To address \u0026ldquo;overthinking\u0026rdquo; and improve token efficiency, an additional length reward $\\text{len_reward}(i)$ is introduced. For a question $x$, sample $k$ responses $(y_i, z_i)$ ($i=1, \\dots, k$) from the current policy. Let $\\text{len}(i)$ be the token length of response $i$, $\\text{min_len} = \\min_i \\text{len}(i)$, and $\\text{max_len} = \\max_i \\text{len}(i)$. If $\\text{max_len} \u0026gt; \\text{min_len}$, the length reward is: $$ \\text{len_reward}(i) = \\begin{cases} \\lambda \u0026 \\text{if } r(x, y_i, y^*) = 1 \\\\ \\min(0, \\lambda) \u0026 \\text{if } r(x, y_i, y^*) = 0 \\end{cases} $$ where the length penalty factor $\\lambda = 0.5 - \\frac{\\text{len}(i) - \\text{min_len}}{\\text{max_len} - \\text{min_len}}$. The final total reward used for optimization is a weighted sum of the correctness reward and the length reward. This penalty is introduced gradually (warm-up). Training Characteristics:\nAlgorithm: Based on Online Policy Mirror Descent, the training process is iterative. In iteration $i$, the current model $\\pi_{\\theta_i}$ is used as the reference policy to optimize the following objective with relative entropy (KL divergence) regularization: $$ \\max _{\\theta} \\mathbb{E}_{\\left(x, y^{*}\\right) \\sim \\mathcal{D}}\\left[\\mathbb{E}_{(y, z) \\sim \\pi_{\\theta}}\\left[r\\left(x, y, y^{*}\\right)\\right]-\\tau \\operatorname{KL}\\left(\\pi_{\\theta}(x) \\| \\pi_{\\theta_{i}}(x)\\right)\\right] $$ where $\\tau \u0026gt; 0$ controls the regularization strength. Optimization: Actual updates use off-policy data (i.e., sampled from the reference policy $\\pi_{\\theta_i}$) and approximate gradients. For each question $x$, sample $k$ responses $(y_j, z_j)$ from $\\pi_{\\theta_i}$, calculate the empirical average reward $\\bar{r} = \\frac{1}{k}\\sum_{j=1}^{k} r(x, y_j, y^*)$ as a baseline. The gradient of the model parameters $\\theta$ is approximated as: $$ \\frac{1}{k} \\sum_{j=1}^{k}\\left(\\nabla_{\\theta} \\log \\pi_{\\theta}\\left(y_{j}, z_{j} \\mid x\\right)\\left(r\\left(x, y_{j}, y^{*}\\right)-\\bar{r}\\right)-\\frac{\\tau}{2} \\nabla_{\\theta}\\left(\\log \\frac{\\pi_{\\theta}\\left(y_{j}, z_{j} \\mid x\\right)}{\\pi_{\\theta_{i}}\\left(y_{j}, z_{j} \\mid x\\right)}\\right)^{2}\\right) $$ This gradient form resembles policy gradient with baseline but includes an $l_2$ regularization term (gradient of the last term) and uses off-policy samples. The value network is discarded during training to encourage exploration. Sampling Strategy: To improve training efficiency, a combination is used: Curriculum Sampling: Gradually increase the difficulty of training problems from easy to hard. Prioritized Sampling: Prioritize sampling problems with lower historical success rates $s_i$ with probability proportional to $1-s_i$. o3 \u0026amp; o4-mini OpenAI\u0026rsquo;s o3 and o4-mini (OpenAI, 2025) are the latest iterations of its o-series reasoning models, characterized by Longer Thinking Time and Full Tool Access.\nCore Contributions:\nEnhanced Reasoning: Models are trained to think longer and deeper (akin to CoT or more complex reasoning processes) before responding, significantly improving performance on complex tasks like coding, math, science, and visual perception. o3 achieves SOTA on benchmarks like Codeforces, SWE-bench, and MMMU.\nFull Tool Access: Models can seamlessly call various tools, such as Web Search, Code Interpreter, GPT‑4o Image Generation, and Function Calling via API. The models are trained to autonomously decide when and how to use these tools to solve problems.\nMultimodal Reasoning: Models can directly integrate images into their chain of thought, enabling deep fusion of visual and textual reasoning, rather than just using images as input. This makes them excel at analyzing charts, diagrams, etc.\nEfficiency vs. Performance Trade-off: o3 is the current most powerful model, suitable for complex queries; o4-mini is optimized for speed and cost, with fewer parameters, but still performs well on math, coding, and visual tasks, especially adept at using tools (e.g., using a Python interpreter in the AIME competition).\nLarge-Scale Reinforcement Learning: The performance improvements of the o-series models are largely attributed to the application of large-scale reinforcement learning (RL), validating the potential of RL in enhancing reasoning capabilities, with performance scaling with increased compute.\nFig. 24. o3 model demonstrates its multimodal CoT capability by analyzing a user-uploaded image, identifying the ship, and using tools (web search) to find information, ultimately answering the ship\u0026rsquo;s name and its next port of call. (Image source: OpenAI, 2025)\nWorking Mechanism:\nLonger Thinking Time: Borrows the idea of \u0026ldquo;trading compute for performance\u0026rdquo; (Snell et al., 2024), improving performance on complex tasks by increasing computation at inference time (e.g., multiple sampling, using longer reasoning chains, search algorithms like MCTS), which might be more effective than simply increasing model parameters. The models are internally designed to perform multi-step reasoning or more complex computations. Users can adjust the model\u0026rsquo;s thinking time by selecting different reasoning effort settings (e.g., o4-mini-high).\nTool Use: Models learn tool usage strategies through RL or instruction fine-tuning. When faced with a problem, the model will:\nPlan: Analyze the problem, determine if tools are needed and which ones. Execute: Call the selected tools (e.g., perform a web search for latest information, run code for calculations). Integrate: Incorporate the results returned by the tools into its reasoning process to generate the final answer. This process can be multi-turn and iterative; the model can adjust its strategy based on the information returned by tools (e.g., performing a secondary search). Multimodal Chain-of-Thought (MCoT): Models can directly reference and analyze image content within their internal reasoning steps, such as identifying data points in a chart, understanding the steps in a flowchart, or interpreting details in a photograph. Interested readers can refer to the MCoT Survey (Wang et al., 2025) which introduces its extension to scenarios involving various modalities like images, videos, audio, 3D, tables/charts, etc.\nEffect:\nFig. 25. To highlight visual reasoning improvement versus our previous multimodal models, OpenAI tested o3 and o4-mini on a diverse set of human exams and ML benchmarks. These new visual reasoning models significantly outperform their predecessors on all multimodal tasks we tested. (Image source: OpenAI, 2025)\no3 and o4-mini demonstrate SOTA or near-SOTA performance on multiple benchmarks, especially on tasks requiring deep reasoning and tool assistance. Expert evaluations show they produce fewer serious errors compared to their predecessors o1/o3-mini, provide more useful and verifiable answers, and interact more naturally.\nSummary Multimodal Large Language Models are advancing towards being more comprehensive, intelligent, and efficient. They can not only understand and generate content across multiple modalities like text, images, videos, and audio, but also perform complex reasoning, planning, and tool invocation. In the future, we can expect further breakthroughs in MLLMs regarding efficiency optimization, deeper cross-modal fusion and reasoning, stronger temporal and spatial understanding capabilities, as well as safety and controllability.\nReferences [1] OpenAI. \u0026ldquo;Hello gpt-4o.\u0026rdquo; OpenAI Blog (2024).\n[2] DeepMind. \u0026ldquo;Gemini 2.5 Pro\u0026rdquo; DeepMind Blog (2025).\n[3] OpenAI. \u0026ldquo;Introducing OpenAI o3 and o4-mini.\u0026rdquo; OpenAI Blog (2025).\n[4] Zhang, Duzhen, et al. \u0026ldquo;Mm-llms: Recent advances in multimodal large language models.\u0026rdquo; arXiv preprint arXiv:2401.13601 (2024).\n[5] Dosovitskiy, Alexey, et al. \u0026ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.\u0026rdquo; arXiv preprint arXiv:2010.11929 (2020).\n[6] Radford, Alec, et al. \u0026ldquo;Learning transferable visual models from natural language supervision.\u0026rdquo; International conference on machine learning. PMLR, 2021.\n[7] Li, Junnan, et al. \u0026ldquo;Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.\u0026rdquo; International conference on machine learning. PMLR, 2022.\n[8] Li, Junnan, et al. \u0026ldquo;Align before fuse: Vision and language representation learning with momentum distillation.\u0026rdquo; Advances in neural information processing systems 34 (2021): 9694-9705.\n[9] Li, Junnan, et al. \u0026ldquo;Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\u0026rdquo; International conference on machine learning. PMLR, 2023.\n[10] Liu, Haotian, et al. \u0026ldquo;Visual instruction tuning.\u0026rdquo; arXiv preprint arXiv:2304.08485 (2023).\n[11] Bai, Jinze, et al. \u0026ldquo;Qwen-vl: A frontier large vision-language model with versatile abilities.\u0026rdquo; arXiv preprint arXiv:2308.12966 1.2 (2023): 3.\n[12] Wang, Peng, et al. \u0026ldquo;Qwen2-vl: Enhancing vision-language model\u0026rsquo;s perception of the world at any resolution.\u0026rdquo; arXiv preprint arXiv:2409.12191 (2024).\n[13] Dehghani, Mostafa, et al. \u0026ldquo;Patch n\u0026rsquo; pack: NaViT, a vision transformer for any aspect ratio and resolution.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 2252-2274.\n[14] Su, Jianlin, et al. \u0026ldquo;Roformer: Enhanced transformer with rotary position embedding.\u0026rdquo; Neurocomputing 568 (2024): 127063.\n[15] Su, Jianlin. \u0026ldquo;Transformer升级之路：4、二维位置的旋转位置编码.\u0026rdquo; (Path to Upgrading Transformers: 4. Rotary Position Embedding for 2D Positions) 科学空间 (Scientific Spaces) (blog) (2021).\n[16] Bai, Shuai, et al. \u0026ldquo;Qwen2.5-VL Technical Report.\u0026rdquo; arXiv preprint arXiv:2502.13923 (2025).\n[17] Xu, Jin, et al. \u0026ldquo;Qwen2.5-Omni Technical Report.\u0026rdquo; arXiv preprint arXiv:2503.20215 (2025).\n[18] Lipman, Yaron, et al. \u0026ldquo;Flow matching for generative modeling.\u0026rdquo; arXiv preprint arXiv:2210.02747 (2022).\n[19] Lee, Sang-gil, et al. \u0026ldquo;Bigvgan: A universal neural vocoder with large-scale training.\u0026rdquo; arXiv preprint arXiv:2206.04658 (2022).\n[20] Kimi Team. \u0026ldquo;Kimi-VL Technical Report.\u0026rdquo; arXiv preprint arXiv:2504.07491 (2025).\n[21] Zhai, Xiaohua, et al. \u0026ldquo;Sigmoid loss for language image pre-training.\u0026rdquo; Proceedings of the IEEE/CVF international conference on computer vision. 2023.\n[22] Kimi Team. \u0026ldquo;Kimi k1. 5: Scaling reinforcement learning with llms.\u0026rdquo; arXiv preprint arXiv:2501.12599 (2025).\n[23] Snell, Charlie, et al. \u0026ldquo;Scaling llm test-time compute optimally can be more effective than scaling model parameters.\u0026rdquo; arXiv preprint arXiv:2408.03314 (2024).\n[24] Wang, Yaoting, et al. \u0026ldquo;Multimodal chain-of-thought reasoning: A comprehensive survey.\u0026rdquo; arXiv preprint arXiv:2503.12605 (2025).\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui. (May 2025). Multimodal Large Language Models. https://syhya.github.io/posts/2025-05-04-multimodal-llm/\nOr\n@article{yue_shui_multimodal_llm_2025, title = \u0026#34;Multimodal Large Language Models\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;May\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-05-04-multimodal-llm/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-05-04-multimodal-llm/","summary":"\u003cp\u003eHumans interact with the world through multiple senses (vision, hearing, touch, etc.), with each sensory channel offering unique advantages in representing and communicating specific concepts. This multimodal interaction fosters our deep understanding of the world. One of the core goals in the field of artificial intelligence is to develop general-purpose assistants that can effectively follow multimodal instructions (such as visual and linguistic ones), enabling them to perform various real-world tasks like humans. In recent years, with the release of models like GPT-4o (\u003ca href=\"https://openai.com/index/hello-gpt-4o/\"\u003eOpenAI, 2024\u003c/a\u003e), Gemini 2.5 Pro (\u003ca href=\"https://deepmind.google/technologies/gemini/pro/\"\u003eDeepMind, 2025\u003c/a\u003e), and o3/o4-mini (\u003ca href=\"https://openai.com/index/introducing-o3-and-o4-mini/\"\u003eOpenAI, 2025\u003c/a\u003e), \u003cstrong\u003eMultimodal Large Language Models (MLLMs)\u003c/strong\u003e have made significant progress. They can not only understand information from multiple modalities like images, videos, and audio but also perform complex reasoning and generation.\u003c/p\u003e","title":"Multimodal Large Language Models"},{"content":"DeepSeek AI successively released DeepSeek-V2 (DeepSeek-AI, 2024) and DeepSeek-V3 (DeepSeek-AI, 2024), two powerful Mixture-of-Experts (MoE) language models that significantly optimize training costs and inference efficiency while maintaining state-of-the-art performance. DeepSeek-V2 has a total of 236B parameters, activating 21B per token, while DeepSeek-V3 further expands to 671B total parameters, activating 37B per token. Both support a 128K context length.\nThe core innovations of these two models lie in the adoption of Multi-head Latent Attention (MLA) and the DeepSeekMoE architecture (Dai et al., 2024). MLA drastically reduces GPU memory usage during inference by compressing the Key-Value (KV) cache into low-dimensional latent vectors, improving efficiency. DeepSeekMoE achieves stronger expert specialization capabilities and more economical training costs through fine-grained expert segmentation and shared expert isolation. Building upon V2, DeepSeek-V3 further introduces an Auxiliary-Loss-Free Load Balancing strategy (Wang et al., 2024) and the Multi-Token Prediction (MTP) (Gloeckle et al., 2024) training objective, further enhancing model performance and training efficiency.\nDeepSeek-V2 was pre-trained on 8.1T tokens, while DeepSeek-V3 was trained on a larger scale of 14.8T tokens. Both underwent Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) stages to fully unlock their potential. Evaluation results show that both DeepSeek-V2 and V3 achieved top-tier performance among open-source models across numerous benchmarks. DeepSeek-V3, in particular, has become one of the strongest open-source base models currently available, with performance comparable to top closed-source models.\nFig. 1. (a) MMLU accuracy vs. activated parameters, among different open-source models. (b) Training costs and inference efficiency of DeepSeek 67B (Dense) and DeepSeek-V2. (Image source: DeepSeek-AI, 2024)\nFig. 2. Benchmark performance of DeepSeek-V3 and its counterparts. (Image source: DeepSeek-AI, 2024)\nThis article will delve into the key technologies of DeepSeek-V2 and DeepSeek-V3, including their innovative model architectures, efficient training infrastructure, pre-training, and alignment processes.\nNotations The following table lists the mathematical notations used in this article to help you read more easily.\nSymbol Meaning \\( d \\) Embedding dimension \\( n_h \\) Number of attention heads \\( d_h \\) Dimension per attention head \\( \\mathbf{h}_t \\in \\mathbb{R}^d \\) Input to the attention layer for the \\( t \\)-th token \\( \\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t \\) Query, Key, Value vectors \\( W^Q, W^K, W^V, W^O \\) Projection matrices for Query, Key, Value, Output \\( \\mathbf{q}_{t,i}, \\mathbf{k}_{t,i}, \\mathbf{v}_{t,i} \\) Query, Key, Value vectors for the \\( i \\)-th attention head \\( \\mathbf{o}_{t,i} \\) Output of the \\( i \\)-th attention head \\( \\mathbf{u}_t \\) Final output of the attention layer \\( l \\) Number of model layers \\( \\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c} \\) Compressed latent vector for key-value \\( d_c \\) KV compression dimension \\( W^{DKV}, W^{UK}, W^{UV} \\) Down-projection matrix for KV, Up-projection matrix for Key, Up-projection matrix for Value \\( \\mathbf{k}_t^C, \\mathbf{v}_t^C \\) Key and Value recovered from the latent vector via up-projection \\( \\mathbf{c}_t^Q \\in \\mathbb{R}^{d_c'} \\) Compressed latent vector for query \\( d_c' \\) Query compression dimension \\( W^{DQ}, W^{UQ} \\) Down-projection matrix for Query, Up-projection matrix for Query \\( \\mathbf{q}_t^C \\) Query recovered from the latent vector via up-projection \\( \\mathbf{q}_{t,i}^R, \\mathbf{k}_t^R \\) Decoupled RoPE query and key \\( d_h^R \\) Head dimension for decoupled RoPE query/key \\( W^{QR}, W^{KR} \\) Generation matrices for decoupled RoPE query/key \\( \\operatorname{RoPE}(\\cdot) \\) Operation applying Rotary Position Embedding \\( [\\cdot ; \\cdot] \\) Concatenation operation \\( n_g \\) Number of groups in GQA \\( n \\) Total number of experts in MoE \\( E_i \\) The \\( i \\)-th expert network \\( G(\\cdot) \\) Gating network function \\( p_i \\) The \\( i \\)-th probability output by the gating network \\( H^{(i)}(x) \\) Gating score for expert \\( i \\) in Noisy Top-k Gating \\( W_g, W_{\\text{noise}} \\) Weight matrices for MoE gating network and noise network \\( \\epsilon \\) Standard Gaussian noise \\( \\text{softplus}(\\cdot) \\) Softplus activation function \\( k \\) Number of experts selected per token in MoE \\( \\text{topk}(\\cdot, k) \\) Function selecting the top k largest values \\( \\mathcal{L}_{\\text{aux}} \\) MoE auxiliary loss \\( w_{\\text{aux}} \\) Auxiliary loss weight \\( \\text{CV}(\\cdot) \\) Coefficient of Variation \\( N_s, N_r \\) Number of shared and routing experts in DeepSeekMoE \\( \\operatorname{FFN}_i^{(s)}(\\cdot), \\operatorname{FFN}_i^{(r)}(\\cdot) \\) The \\( i \\)-th shared expert and routing expert function \\( K_r \\) Number of activated routing experts in DeepSeekMoE \\( g_{i,t} \\) Gating value of the \\( i \\)-th expert for the \\( t \\)-th token \\( g_{i,t}' \\) Raw gating value after TopK selection (V3) \\( s_{i,t} \\) Affinity score of the \\( t \\)-th token for the \\( i \\)-th expert \\( \\mathbf{e}_i \\) Center vector for the \\( i \\)-th routing expert \\( M \\) Device/Node limit for routing \\( \\mathcal{L}_{\\text{ExpBal}}, \\mathcal{L}_{\\text{DevBal}}, \\mathcal{L}_{\\text{CommBal}} \\) Expert-level, Device-level, Communication-level load balancing losses \\( f_i, P_i \\) Load score and average affinity for expert \\( i \\) \\( \\alpha_1, \\alpha_2, \\alpha_3 \\) Hyperparameters for load balancing losses \\( T \\) Number of tokens in the sequence \\( D \\) Number of device/node groups \\( \\mathcal{E}_i \\) Set of experts on the \\( i \\)-th device/node \\( f_i', P_i' \\) Average load score and total affinity for device group \\( i \\) \\( f_i'', P_i'' \\) Proportion of tokens sent to device \\( i \\) and total affinity for device group \\( i \\) \\( b_i \\) Bias term for the \\( i \\)-th expert (aux-loss-free balancing) \\( \\gamma \\) Bias term update rate \\( \\mathcal{L}_{\\text{Bal}} \\) Sequence-level load balancing loss \\( \\alpha \\) Hyperparameter for sequence-level load balancing loss \\( D_{MTP} \\) MTP prediction depth \\( \\operatorname{Emb}(\\cdot), \\operatorname{OutHead}(\\cdot) \\) Shared embedding layer and output head (MTP) \\( \\operatorname{TRM}_k(\\cdot) \\) Transformer block for the \\( k \\)-th MTP module \\( M_k \\) Projection matrix for the \\( k \\)-th MTP module \\( \\mathbf{h}_i^k \\) Representation of the \\( i \\)-th token at the \\( k \\)-th MTP depth \\( \\mathbf{h}_i^{\\prime k} \\) Input to the Transformer block of the \\( k \\)-th MTP module \\( P_{i+k+1}^k \\) Predicted probability distribution for the \\( i+k+1 \\)-th token by the \\( k \\)-th MTP module \\( V \\) Vocabulary size \\( \\mathcal{L}_{\\text{MTP}}^k \\) Cross-entropy loss for the \\( k \\)-th MTP depth \\( \\mathcal{L}_{\\text{MTP}} \\) Total MTP loss \\( \\lambda \\) Weight factor for MTP loss \\( \\mathcal{J}_{GRPO}(\\theta) \\) GRPO objective function \\( A_i \\) Relative advantage value (GRPO) \\( \\varepsilon \\) Clipping hyperparameter in PPO/GRPO \\( \\beta \\) Coefficient for KL divergence penalty term \\( \\mathbb{D}_{KL}(\\pi_\\theta \\| \\pi_{ref}) \\) KL divergence \\( \\pi_\\theta, \\pi_{\\theta_{old}}, \\pi_{ref} \\) Current policy, old policy, reference policy models \\( r_i \\) Reward value for the \\( i \\)-th output \\( \\mathbb{1}(\\cdot) \\) Indicator function Core Architecture Both DeepSeek-V2 and V3 are based on the Transformer architecture, but employ innovative designs in the attention and feed-forward network (FFN) parts, such as MLA and DeepSeekMoE, to balance performance, training cost, and inference efficiency. The figure below illustrates the architecture of DeepSeek-V2 and V3.\nFig. 3. Illustration of the architecture of DeepSeek-V2 and DeepSeek-V3. MLA ensures efficient inference by significantly reducing the KV cache for generation, and DeepSeekMoE enables training strong models at an economical cost through the sparse architecture. (Image source: DeepSeek-AI, 2024)\nMulti-head Latent Attention (MLA) Traditional Transformer models typically use Multi-Head Attention (MHA) (Vaswani et al., 2017), but during generation, their large KV cache becomes a bottleneck limiting inference efficiency. To address this, researchers proposed Multi-Query Attention (MQA) (Shazeer, 2019) and Grouped-Query Attention (GQA) (Ainslie et al., 2023). While these methods reduce the KV cache, they often come at the cost of model performance.\nDeepSeek-V2 and V3 adopt the innovative Multi-head Latent Attention (MLA) mechanism. The core idea of MLA is Low-Rank Key-Value Joint Compression.\nFig. 4. Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. (Image source: DeepSeek-AI, 2024)\nMHA Recap Standard MHA first transforms the input \\(\\mathbf{h}_t \\in \\mathbb{R}^d\\) into query \\(\\mathbf{q}_t\\), key \\(\\mathbf{k}_t\\), and value \\(\\mathbf{v}_t \\in \\mathbb{R}^{d_h n_h}\\) using three projection matrices \\(W^Q, W^K, W^V \\in \\mathbb{R}^{d_h n_h \\times d}\\): \\[ \\begin{aligned} \\mathbf{q}_{t} \u0026= W^{Q} \\mathbf{h}_{t}, \\\\ \\mathbf{k}_{t} \u0026= W^{K} \\mathbf{h}_{t}, \\\\ \\mathbf{v}_{t} \u0026= W^{V} \\mathbf{h}_{t}. \\end{aligned} \\] Then, \\(\\mathbf{q}_t, \\mathbf{k}_t, \\mathbf{v}_t\\) are split into \\(n_h\\) heads for multi-head attention computation: \\[ \\begin{aligned} \u0026 [\\mathbf{q}_{t, 1} ; \\mathbf{q}_{t, 2} ; \\ldots ; \\mathbf{q}_{t, n_{h}}] = \\mathbf{q}_{t}, \\\\ \u0026 [\\mathbf{k}_{t, 1} ; \\mathbf{k}_{t, 2} ; \\ldots ; \\mathbf{k}_{t, n_{h}}] = \\mathbf{k}_{t}, \\\\ \u0026 [\\mathbf{v}_{t, 1} ; \\mathbf{v}_{t, 2} ; \\ldots ; \\mathbf{v}_{t, n_{h}}] = \\mathbf{v}_{t}, \\\\ \u0026 \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} \\operatorname{Softmax}_{j}\\left(\\frac{\\mathbf{q}_{t, i}^{T} \\mathbf{k}_{j, i}}{\\sqrt{d_{h}}}\\right) \\mathbf{v}_{j, i}, \\\\ \u0026 \\mathbf{u}_{t} = W^{O}\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right], \\end{aligned} \\] where \\(\\mathbf{q}_{t, i}, \\mathbf{k}_{t, i}, \\mathbf{v}_{t, i} \\in \\mathbb{R}^{d_h}\\) are the query, key, and value for the \\(i\\)-th head, respectively, and \\(W^O \\in \\mathbb{R}^{d \\times d_h n_h}\\) is the output projection matrix. During inference, keys and values for all \\(t\\) steps need to be cached, requiring \\(2 n_h d_h l\\) elements per token (\\(l\\) being the number of layers), which constitutes a huge KV cache overhead.\nLow-Rank Key-Value Joint Compression MLA introduces a low-dimensional latent vector \\(\\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c}\\) to jointly compress keys and values, where \\(d_c \\ll d_h n_h\\): \\[ \\begin{aligned} \\boxed{\\mathbf{c}_{t}^{K V}} \u0026= W^{D K V} \\mathbf{h}_{t}, \\\\ \\mathbf{k}_{t}^{C} \u0026= W^{U K} \\mathbf{c}_{t}^{K V}, \\\\ \\mathbf{v}_{t}^{C} \u0026= W^{U V} \\mathbf{c}_{t}^{K V}. \\end{aligned} \\] Here, \\(W^{DKV} \\in \\mathbb{R}^{d_c \\times d}\\) is the down-projection matrix, and \\(W^{UK}, W^{UV} \\in \\mathbb{R}^{d_h n_h \\times d_c}\\) are the up-projection matrices for keys and values, respectively. During inference, MLA only needs to cache the compressed latent vector \\(\\mathbf{c}_t^{KV}\\) (and the decoupled RoPE key \\(\\mathbf{k}_t^R\\) mentioned later), greatly reducing the KV cache size.\nTo reduce activation memory during training, MLA also applies similar low-rank compression to the query: \\[ \\begin{aligned} \\mathbf{c}_{t}^{Q} \u0026= W^{D Q} \\mathbf{h}_{t}, \\\\ \\mathbf{q}_{t}^{C} \u0026= W^{U Q} \\mathbf{c}_{t}^{Q}, \\end{aligned} \\] where \\(\\mathbf{c}_t^Q \\in \\mathbb{R}^{d_c'}\\) is the compressed latent vector for the query, \\(d_c' \\ll d_h n_h\\), and \\(W^{DQ} \\in \\mathbb{R}^{d_c' \\times d}\\) and \\(W^{UQ} \\in \\mathbb{R}^{d_h n_h \\times d_c'}\\) are the down-projection and up-projection matrices for the query, respectively.\nDecoupled Rotary Position Embedding Standard Rotary Position Embedding (RoPE) (Su et al., 2024) is applied directly to keys and queries, but this is incompatible with MLA\u0026rsquo;s low-rank KV compression. If RoPE were applied to the compressed key \\(\\mathbf{k}_t^C\\), the up-projection matrix \\(W^{UK}\\) would couple with the position-dependent RoPE matrix. This would prevent absorbing \\(W^{UK}\\) into \\(W^Q\\) during inference, requiring recomputation of keys for all prefix tokens, severely impacting efficiency.\nTo solve this, MLA proposes the Decoupled RoPE strategy. It introduces an additional multi-head query \\(\\mathbf{q}_{t, i}^R \\in \\mathbb{R}^{d_h^R}\\) and a shared key \\(\\mathbf{k}_t^R \\in \\mathbb{R}^{d_h^R}\\) specifically to carry the RoPE information: \\[ \\begin{aligned} \\left[\\mathbf{q}_{t,1}^R;\\,\\mathbf{q}_{t,2}^R;\\,\\dots;\\,\\mathbf{q}_{t,n_h}^R\\right] = \\mathbf{q}_t^R \u0026= \\operatorname{RoPE}\\bigl(W^{Q R}\\,\\mathbf{c}_t^Q\\bigr),\\\\ \\boxed{\\mathbf{k}_t^R} \u0026= \\operatorname{RoPE}\\bigl(W^{K R}\\,\\mathbf{h}_t\\bigr). \\end{aligned} \\] Here, \\(W^{QR} \\in \\mathbb{R}^{d_h^R n_h \\times d_c'}\\) and \\(W^{KR} \\in \\mathbb{R}^{d_h^R \\times d}\\) are matrices generating the decoupled query and key. The compressed key/query parts (\\(C\\)) are then concatenated with the decoupled RoPE parts (\\(R\\)) to form the final keys and queries: \\[ \\begin{aligned} \\mathbf{q}_{t, i} \u0026= [\\mathbf{q}_{t, i}^{C} ; \\mathbf{q}_{t, i}^{R}], \\\\ \\mathbf{k}_{t, i} \u0026= [\\mathbf{k}_{t, i}^{C} ; \\mathbf{k}_{t}^{R}]. \\end{aligned} \\] The final attention computation becomes: \\[ \\begin{aligned} \\mathbf{o}_{t, i} \u0026= \\sum_{j=1}^{t} \\operatorname{Softmax}_{j}\\left(\\frac{\\mathbf{q}_{t, i}^{T} \\mathbf{k}_{j, i}}{\\sqrt{d_{h}+d_{h}^{R}}}\\right) \\mathbf{v}_{j, i}^{C}, \\\\ \\mathbf{u}_{t} \u0026= W^{O}\\left[\\mathbf{o}_{t, 1} ; \\mathbf{o}_{t, 2} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right]. \\end{aligned} \\] During inference, besides caching \\(\\mathbf{c}_t^{KV}\\), the decoupled RoPE key \\(\\mathbf{k}_t^R\\) also needs to be cached. Therefore, DeepSeek-V2/V3 require caching a total of \\((d_c + d_h^R)l\\) elements per token.\nMatrix Absorption in MLA Inference A key advantage of MLA is the improvement in inference efficiency, partly due to the associative property of matrix multiplication allowing the up-projection matrices \\(W^{UK}\\) and \\(W^{UV}\\) to be \u0026ldquo;absorbed,\u0026rdquo; avoiding the explicit computation of the full keys \\(\\mathbf{k}_t^C\\) and values \\(\\mathbf{v}_t^C\\).\n1. Absorbing \\(W^{UK}\\) (Optimizing Attention Score Calculation):\nThe core of attention score calculation is the dot product of query and key \\(\\mathbf{q}_{t,i}^T \\mathbf{k}_{j,i}\\). Focusing on the \\(C\\) part generated from the compressed vector: \\[ (\\mathbf{q}_{t,i}^C)^T \\mathbf{k}_{j,i}^C \\] Substitute \\(\\mathbf{k}_{j,i}^C = W^{UK} \\mathbf{c}_j^{KV}\\): \\[ (\\mathbf{q}_{t,i}^C)^T (W^{UK} \\mathbf{c}_j^{KV}) \\] Using matrix multiplication associativity \\((AB)C = A(BC)\\) and transpose property \\((AB)^T = B^T A^T\\), the expression can be rewritten as: \\[ (\\mathbf{q}_{t,i}^C)^T (W^{UK} \\mathbf{c}_j^{KV}) = ((W^{UK})^T \\mathbf{q}_{t,i}^C)^T \\mathbf{c}_j^{KV} \\] The significance of this transformation is: we no longer need to apply \\(W^{UK}\\) to the cached \\(\\mathbf{c}_j^{KV}\\) to get \\(\\mathbf{k}_{j,i}^C\\). Instead, we can first compute an \u0026ldquo;effective query\u0026rdquo; \\(\\tilde{\\mathbf{q}}_{t,i}^C = (W^{UK})^T \\mathbf{q}_{t,i}^C\\), and then directly compute the dot product of this effective query with the cached latent vector \\(\\mathbf{c}_j^{KV}\\).\nThe original query \\(\\mathbf{q}_{t,i}^C\\) is computed from \\(\\mathbf{h}_t\\) via \\(W^{UQ}\\) and \\(W^{DQ}\\) (\\(\\mathbf{q}_{t,i}^C = (W^{UQ} W^{DQ} \\mathbf{h}_t)_i\\)). Thus, the entire computation from \\(\\mathbf{h}_t\\) to the effective query \\(\\tilde{\\mathbf{q}}_{t,i}^C\\) can be viewed as a new, effective query projection operation that incorporates \\(W^{UK}\\). In practice, this means after computing \\(\\mathbf{q}_{t,i}^C\\), one can left-multiply by \\((W^{UK})^T\\), or more efficiently, merge \\((W^{UK})^T\\) into the original query generation matrix \\(W^Q\\) (or \\(W^{UQ}W^{DQ}\\)) to form a new query projection matrix \\(\\tilde{W}^Q = (W^{UK})^T W^{UQ} W^{DQ}\\).\nCrucially, the computation involving \\(W^{UK}\\) is moved to the query side and performed once before calculating attention scores, eliminating the need to recover \\(\\mathbf{k}_{j,i}^C\\) from the cached \\(\\mathbf{c}_j^{KV}\\) using \\(W^{UK}\\) for every query.\n2. Absorbing \\(W^{UV}\\) (Optimizing Weighted Sum):\nThe output of an attention head \\(\\mathbf{o}_{t,i}\\) is the weighted sum of attention weights (denoted \\(w_{ij}\\)) and values \\(\\mathbf{v}_{j,i}^C\\): \\[ \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} w_{ij} \\cdot \\mathbf{v}_{j, i}^{C} \\] Substitute \\(\\mathbf{v}_{j,i}^C = (W^{UV} \\mathbf{c}_j^{KV})_i\\) (where \\((\\cdot)_i\\) denotes the part belonging to the \\(i\\)-th head): \\[ \\mathbf{o}_{t, i} = \\sum_{j=1}^{t} w_{ij} \\cdot (W^{UV} \\mathbf{c}_j^{KV})_i \\] The final attention layer output \\(\\mathbf{u}_t\\) is obtained by concatenating the outputs of all heads \\(\\mathbf{o}_{t,i}\\) and projecting through the output matrix \\(W^O\\): \\[ \\mathbf{u}_{t} = W^{O}\\left[\\mathbf{o}_{t, 1} ; \\ldots ; \\mathbf{o}_{t, n_{h}}\\right] = W^{O} \\begin{bmatrix} \\sum_{j} w_{1j} (W^{UV} \\mathbf{c}_j^{KV})_1 \\\\ \\vdots \\\\ \\sum_{j} w_{n_h j} (W^{UV} \\mathbf{c}_j^{KV})_{n_h} \\end{bmatrix} \\] Due to the linearity of matrix multiplication (\\(A(B+C) = AB + AC\\) and \\(A(cB) = c(AB)\\)), \\(W^{UV}\\) can be \u0026ldquo;factored out\u0026rdquo; of the summation (this is for intuitive understanding; the actual operation is at the matrix level): \\[ \\mathbf{u}_{t} \\approx W^{O} W^{UV} \\left( \\sum_{j=1}^{t} \\begin{bmatrix} w_{1j} (\\mathbf{c}_j^{KV})_1 \\\\ \\vdots \\\\ w_{n_h j} (\\mathbf{c}_j^{KV})_{n_h} \\end{bmatrix} \\right) \\] (Note: \\((\\mathbf{c}_j^{KV})_i\\) here is illustrative; in practice, operations are performed directly on the complete \\(\\mathbf{c}_j^{KV}\\), but the principle is the same: first perform the weighted sum on \\(\\mathbf{c}_j^{KV}\\), then apply \\(W^{UV}\\) and \\(W^O\\)).\nLet the effective output matrix be \\(\\tilde{W}^O = W^O W^{UV}\\). This means we can first compute the weighted sum of attention weights and the latent vectors \\(\\mathbf{c}_j^{KV}\\) (yielding an intermediate result \\(\\tilde{\\mathbf{o}}_t = \\sum_j w_{ij} \\mathbf{c}_j^{KV}\\) of dimension \\(d_c\\)), and then directly use this merged effective output matrix \\(\\tilde{W}^O\\) for the final projection to get \\(\\mathbf{u}_t\\). Similarly, the computation involving \\(W^{UV}\\) is merged into the final output projection step, eliminating the need to recover \\(\\mathbf{v}_{j,i}^C\\) from \\(\\mathbf{c}_j^{KV}\\) during the weighted sum calculation.\nSummary: Through matrix absorption, MLA avoids repeatedly computing the high-dimensional keys \\(\\mathbf{k}_{j,i}^C\\) and values \\(\\mathbf{v}_{j,i}^C\\) from the cached low-dimensional latent vectors \\(\\mathbf{c}_j^{KV}\\) during inference, significantly improving computational efficiency. Only \\(\\mathbf{c}_t^{KV}\\) and \\(\\mathbf{k}_t^R\\) are actually cached.\nKV Cache Comparison The table below compares the per-token KV cache size for different attention mechanisms. \\(n_h\\) is the number of attention heads, \\(d_h\\) is the dimension per head, \\(l\\) is the number of layers, \\(n_g\\) is the number of GQA groups, and \\(d_c\\) and \\(d_h^R\\) are MLA\u0026rsquo;s KV compression dimension and decoupled RoPE dimension. For DeepSeek-V2, \\(d_c = 4d_h\\), \\(d_h^R = d_h/2\\), making its KV cache equivalent to GQA with \\(n_g=2.25\\), but with performance superior to MHA. DeepSeek-V3 uses a similar configuration.\nAttention Mechanism Per-Token KV Cache Size (# elements) Capability Multi-Head Attention (MHA) \\(2 n_{h} d_{h} l\\) Strong Grouped-Query Attention (GQA) \\(2 n_{g} d_{h} l\\) Medium Multi-Query Attention (MQA) \\(2 d_{h} l\\) Weak Multi-head Latent Attention (MLA) \\(\\bigl(d_{c} + d_{h}^{R}\\bigr) l \\approx \\tfrac{9}{2} \\, d_{h} \\, l\\) Stronger The figure below shows that MLA not only significantly reduces the KV cache but also achieves performance superior to standard MHA.\nFig. 5. Comparison between MLA and MHA on hard benchmarks. DeepSeek-V2 shows better performance than MHA, but requires a significantly smaller amount of KV cache. (Image source: DeepSeek-AI, 2024)\nMixture-of-Experts Models Before diving into DeepSeekMoE, let\u0026rsquo;s review the basics of Mixture-of-Experts (MoE) models.\nMixture-of-Experts (MoE) (Shazeer et al. 2017) is a sparsely activated model that significantly increases model parameter count and performance without substantially increasing computational cost by combining multiple independent \u0026ldquo;expert\u0026rdquo; networks and a gating network. The core idea of MoE is Sparse Activation, meaning that for each input sample, only a subset of expert networks is activated, rather than the entire model. This approach enhances both computational efficiency and the model\u0026rsquo;s expressive power, leading to excellent performance in LLMs.\nMoE design is inspired by Ensemble learning, a technique that decomposes complex tasks into multiple subtasks handled collaboratively by different models. In MoE, these \u0026ldquo;subtasks\u0026rdquo; are processed by multiple independent expert networks, while a gating network dynamically selects the most suitable experts based on the input sample\u0026rsquo;s features. This division of labor resembles expert teams in human society: specialists from different fields provide expertise on specific problems, and their insights are combined to reach a final result.\nFig. 6. Illustration of a mixture-of-experts (MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)\nCore MoE Components A typical MoE layer includes the following components:\nExperts: A set of independent neural networks ${E_1, E_2, \u0026hellip;, E_n}$. Each expert network $E_i$ can be any type of neural network, such as an FFN, CNN, RNN, etc. The number of experts $n$ can be large, e.g., tens, hundreds, or even thousands. Gating Network: A trainable neural network $G$ that learns a probability distribution based on the input sample $x$ to decide which experts to activate. The gating network takes the input sample $x$ and outputs an $n$-dimensional probability vector $p = G(x) = [p_1, p_2, \u0026hellip;, p_n]$, where $p_i$ represents the probability of activating expert $E_i$. Expert Output Aggregation: Based on the probability distribution from the gating network, the outputs of the activated expert networks are weighted and summed to produce the final output $y$ of the MoE layer. Noisy Top-k Gating To achieve sparse activation and ensure balanced expert utilization, MoE typically employs Noisy Top-k Gating as the gating mechanism. This method introduces noise and top-k selection to ensure computational efficiency while preventing uneven expert load. Here\u0026rsquo;s the detailed workflow:\nGating Score Calculation:\nFor an input sample $x$, the gating network first computes a gating score $H^{(i)}(x)$ for each expert. This score consists of a linear transformation and a noise term, formulated as:\n$$ H^{(i)}(x) =(x W_g)^{(i)} + \\epsilon \\cdot \\text{softplus}\\left((x W_{\\text{noise}})^{(i)} \\right), \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$ Parameters: $W_g \\in \\mathbb{R}^{d \\times n}$: Trainable weight matrix of the gating network, where $d$ is the input feature dimension and $n$ is the number of experts. $W_{\\text{noise}} \\in \\mathbb{R}^{d \\times n}$: Weight matrix used to generate noise. $\\epsilon \\sim \\mathcal{N}(0, 1)$: Standard Gaussian noise, adding randomness to the gating. $\\text{softplus}(x) = \\log(1 + e^x)$: Smooth activation function ensuring non-negative noise. The introduction of noise prevents the gating network from always selecting the same experts, enhancing the model\u0026rsquo;s robustness and diversity.\nTop-k Selection:\nAfter computing the gating score vector $H(x) = [H^{(1)}(x), H^{(2)}(x), \\dots, H^{(n)}(x)]$, the gating network selects the top $k$ experts with the highest scores (usually $k \\ll n$). This step is implemented using the $\\text{topk}(v, k)$ function:\n$$ \\text{topk}^{(i)}(v, k) = \\begin{cases} v^{(i)} \u0026 \\text{if } v^{(i)} \\text{ is in the top } k \\text{ elements of } v \\\\ -\\infty \u0026 \\text{otherwise} \\end{cases} $$Setting the scores of non-top-k experts to $-\\infty$ ensures their probabilities become 0 after the subsequent softmax operation, achieving sparsity.\nSoftmax Normalization:\nThe gating scores of the top-k experts are normalized using softmax to obtain a sparse probability distribution $G(x)$:\n$$ G(x) = \\text{softmax}\\left( \\text{topk}(H(x), k) \\right) $$Only the top-k experts have non-zero probabilities; the rest are 0. For example, if $n=100, k=2$, then 98 experts will have a probability of 0.\nWeighted Sum:\nThe outputs of the top-k experts are weighted by their probabilities and summed to get the MoE layer\u0026rsquo;s output:\n$$ y = \\sum_{i=1}^{n} G^{(i)}(x) E_i(x) $$Since only $k$ experts are activated, the computational load is much lower than activating all $n$ experts.\nAuxiliary Loss To prevent the gating network from overly favoring a few experts, MoE introduces an Auxiliary Loss (Shazeer et al. 2017) to encourage uniform usage of all experts. A common method is based on the square of the Coefficient of Variation (CV) of expert usage:\n$$ \\mathcal{L}_{\\text{aux}} = w_{\\text{aux}} \\cdot \\text{CV}\\left( \\sum_{x \\in X} G(x) \\right)^2 $$ Parameters:\n$X$: A mini-batch of input samples. $\\sum_{x \\in X} G(x)$: Counts the number of times each expert is activated within the mini-batch. $\\text{CV}$: The ratio of the standard deviation to the mean, measuring the uniformity of expert usage distribution. $w_{\\text{aux}}$: Weight of the auxiliary loss, needs manual tuning. Purpose: By minimizing $\\mathcal{L}_{\\text{aux}}$, the model optimizes the balance of expert selection, preventing some experts from being overused while others remain idle.\nGShard GShard (Lepikhin et al. 2020) primarily focuses on sharding the MoE layer, distributing the expert networks ${E_1, E_2, \u0026hellip;, E_n}$ across multiple TPU devices. For instance, with $P$ TPU devices, the experts can be divided into $P$ groups, each assigned to one TPU device. Other layers of the Transformer model (e.g., self-attention, LayerNorm) are replicated across all TPU devices.\nGShard\u0026rsquo;s Improved Gating Mechanism:\nGShard builds upon Noisy Top-k Gating with several improvements to enhance performance and stability:\nExpert Capacity: To prevent expert overload, GShard introduces expert capacity limits. Each expert network has a maximum capacity, indicating the maximum number of tokens it can process. If a token is routed to an expert that has reached its capacity limit, the token is marked as \u0026ldquo;overflowed,\u0026rdquo; and its gating output is set to a zero vector, meaning it won\u0026rsquo;t be routed to any expert.\nLocal Group Dispatching: To improve gating efficiency, GShard groups tokens and enforces expert capacity limits at the group level. For example, tokens in a mini-batch are divided into multiple local groups, each containing a certain number of tokens. The gating network selects top-k experts for each local group, ensuring that the number of tokens processed by each expert within a group does not exceed its capacity limit.\nAuxiliary Loss: GShard also uses an auxiliary loss function to balance expert load. Unlike the original MoE model\u0026rsquo;s auxiliary loss, GShard\u0026rsquo;s loss aims to minimize the mean squared error of the proportion of data routed to each expert, more directly measuring expert load balance.\nRandom Routing: To increase routing randomness, GShard introduces a random routing mechanism when selecting the top-k experts. Besides selecting the best top-k experts, GShard also randomly selects sub-optimal experts with a certain probability, increasing expert diversity and improving the model\u0026rsquo;s generalization ability.\nBelow is the core algorithm flow of GShard:\nFig. 7. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)\nSwitch Transformer Switch Transformer (Fedus et al. 2021) is a trillion-parameter MoE model proposed by Google. Its core innovation is replacing the dense feed-forward network (FFN) layers in the Transformer model with sparse Switch FFN layers. Unlike GShard\u0026rsquo;s Top-2 Gating, Switch Transformer routes each input token to only one expert network, achieving higher sparsity and further reducing computational costs, making it possible to train trillion-parameter models. It encourages more balanced token routing among the $N$ experts. Switch Transformer\u0026rsquo;s auxiliary loss is based on the product sum of the actual routing fraction and the predicted routing probability, formulated as:\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ Parameters: $N$: Total number of experts. $f_i$: Fraction of tokens routed to the $i$-th expert, defined as: $$ f_i = \\frac{1}{T} \\sum_{x \\in B} \\mathbb{1}\\{\\text{argmax } p(x) = i\\} $$ $P_i$: Routing probability for the $i$-th expert predicted by the gating network, defined as: $$ P_i = \\frac{1}{T} \\sum_{x \\in B} p_i(x) $$ $T$: Total number of tokens in batch $B$. $\\alpha$: Weight hyperparameter for the auxiliary loss, typically set to $10^{-2}$. By minimizing this loss, the model encourages the actual routing fraction $f_i$ to align with the predicted probability $P_i$, indirectly promoting load balance among experts and preventing some from being idle.\nFig. 8. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)\nSwitch Router Mechanism:\nRouting Prediction: For an input token $x$, the Switch Router predicts the routing probability $p_i = G^{(i)}(x)$ for each expert network, where $i = 1, 2, \u0026hellip;, n$, and n is the number of expert networks.\nExpert Selection: Select the expert network with the highest routing probability as the best expert. Switch Transformer uses a Top-1 routing strategy, meaning each token is routed only to the expert with the highest probability.\nToken Routing: Route the input token $x$ to the selected best expert network for processing.\nSwitch Transformer Training Stability Optimizations:\nTo improve the training stability of Switch Transformer, the paper proposes the following optimization strategies:\nSelective Precision: Using FP32 precision inside the router function improves training stability without the overhead of FP32 tensor communication. Specifically, the Switch Router computations are performed entirely in FP32, and the final result is converted back to FP16 to balance efficiency and precision.\nSmaller Initialization: It is recommended to adjust the Transformer weight initialization scale parameter $s$ from 1.0 to 0.1. A smaller initialization scale helps mitigate the risk of gradient explosion early in training, thereby improving overall training stability. This is implemented by sampling from a truncated normal distribution with mean 0 and standard deviation $\\sqrt{s/n}$ (where $n$ is the number of input units).\nHigher Expert Dropout: Using a higher dropout rate (e.g., 0.4) in the expert FFN layers while maintaining a lower dropout rate (e.g., 0.1) in non-expert layers effectively prevents overfitting and enhances the model\u0026rsquo;s generalization ability. The experimental results in the figure below show that the model performs best on tasks like GLUE, CNNDM, SQuAD, and SuperGLUE when the expert layer dropout rate is set to 0.4.\nFig. 9. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set (higher numbers are better). (Image source: Fedus et al. 2021)\nThe Switch Transformers paper uses the following figure to intuitively illustrate how different parallelism techniques partition model weights and data:\nFig. 10. An illustration of various parallelism strategies on how (Top) model weights and (Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)\nExpert Choice Expert Choice (EC) (Zhou et al. 2022) is a routing strategy opposite to token choice routing (like GShard\u0026rsquo;s top-2 or Switch Transformer\u0026rsquo;s top-1). In token choice routing, each token selects top-k experts from all available experts. In expert choice routing, each expert selects top-k tokens from all available tokens to process. This approach aims to address the load imbalance and token dropping issues of token choice routing while significantly improving training efficiency. Here is the specific computation process:\nCompute token-to-expert affinity scores:\nFor an input matrix $X \\in \\mathbb{R}^{n \\times d}$, the token-to-expert affinity score matrix $S \\in \\mathbb{R}^{n \\times e}$ is computed as:\n$$ S = \\text{softmax}(X \\cdot W_g), \\quad \\text{where } W_g \\in \\mathbb{R}^{d \\times e}. $$ Here, $W_g$ is the gating weight matrix, and $e$ is the number of experts.\nExperts select tokens:\nEach expert selects the top-k tokens from all tokens to process. This is done by performing top-k selection on $S^T$:\n$$ G, I = \\text{top-}k(S^T, k), $$This yields:\nGating matrix $G \\in \\mathbb{R}^{e \\times k}$: Records the routing weights corresponding to the tokens selected by the experts, where $G[i, j]$ is the weight for the $j$-th token selected by expert $i$. Token index matrix $I \\in \\mathbb{R}^{e \\times k}$: Indicates the indices of the tokens selected by each expert in the input. One-hot encoding:\nConvert the token index matrix $I$ into a one-hot encoded matrix $P \\in \\mathbb{R}^{e \\times k \\times n}$ for subsequent calculations:\n$$ P = \\operatorname{one}-\\operatorname{hot}(I) $$ Construct input for Gated FFN layer:\nFor each expert $i$, the input to its gated FFN layer is:\n$$ (P \\cdot X) \\in \\mathbb{R}^{e \\times k \\times d}. $$ EC controls model sparsity by regularizing the number of experts each token is routed to. A common regularization objective is:\n$$ \\begin{aligned} \u0026 \\max_{A} \\langle S^{\\top}, A \\rangle + \\lambda H(A) \\\\ \u0026 \\text{s.t. } \\forall i: \\sum_{j'} A[i, j'] = k, \\quad \\forall j: \\sum_{i'} A[i', j] \\leq b, \\quad \\forall i,j: 0 \\leq A[i, j] \\leq 1, \\end{aligned} $$The optimization problem defines a matrix $A$ where the element at row $i$, column $j$ indicates whether expert $i$ selected token $j$ (value 0 or 1). Since solving this optimization problem is complex, the paper uses Dijkstra\u0026rsquo;s algorithm (obtaining an approximate solution through multiple iterations).\nThe parameter $b$ is typically determined by the total number of tokens $n$ in the batch and a capacity factor, which represents the average number of experts used per token. Most experiments use a high capacity factor. Experimental results show that even with reduced capacity, EC generally outperforms traditional top-1 token choice routing, although capped expert choice slightly degrades fine-tuning performance.\nThe advantages of EC are mainly twofold:\nPerfect Load Balancing: Each expert processes a fixed $k$ tokens, avoiding the issue of some experts being overloaded while others are idle, achieving ideal load balance. Higher Training Efficiency: Experiments show that EC can improve training convergence speed by about 2x, making it more efficient than traditional token choice routing. However, EC also has limitations:\nBatch Size Requirement: EC requires a relatively large batch size, making it unsuitable for scenarios with smaller batch sizes. Autoregressive Generation Limitation: In autoregressive text generation tasks, EC\u0026rsquo;s top-k selection cannot be implemented because future tokens are unknown, making it unsuitable for such tasks. DeepSeekMoE Mixture-of-Experts (MoE) models enhance efficiency and performance by routing computation to specific \u0026ldquo;expert\u0026rdquo; subnetworks. DeepSeek-V2 and V3 employ an architecture named DeepSeekMoE (Dai et al., 2024) in their FFN (Feed-Forward Network) layers. Compared to traditional MoE architectures like GShard, the core ideas of DeepSeekMoE are:\nFine-grained Expert Segmentation: Splitting expert networks into smaller units. This aims for higher expert specialization and more precise knowledge acquisition, as each expert can focus on a narrower domain. Shared Expert Isolation: The architecture includes a set of \u0026ldquo;shared experts\u0026rdquo; processed by all tokens, intended to handle general knowledge. This reduces knowledge redundancy among the \u0026ldquo;routing experts\u0026rdquo; that need to be selected, allowing them to focus more on specific knowledge. Basic Architecture For an input token representation \\(\\mathbf{u}_t\\) to the FFN layer, the output \\(\\mathbf{h}_t'\\) of DeepSeekMoE is computed by combining the outputs of shared experts and selected routing experts: \\[ \\mathbf{h}_{t}^{\\prime} = \\mathbf{u}_{t} + \\sum_{i=1}^{N_{s}} \\operatorname{FFN}_{i}^{(s)}(\\mathbf{u}_{t}) + \\sum_{i=1}^{N_{r}} g_{i, t} \\operatorname{FFN}_{i}^{(r)}(\\mathbf{u}_{t}), \\] where:\n\\(N_s\\) is the number of shared experts. \\(N_r\\) is the number of routing experts. \\(\\operatorname{FFN}_i^{(s)}\\) is the \\(i\\)-th shared expert network. \\(\\operatorname{FFN}_i^{(r)}\\) is the \\(i\\)-th routing expert network. \\(g_{i, t}\\) is the gating value (weight) assigned to the \\(i\\)-th routing expert for the \\(t\\)-th token. The calculation of the gating value \\(g_{i,t}\\), based on token-to-expert affinity scores \\(s_{i,t}\\) and selected via a Top-K routing mechanism, is one of the key differences between DeepSeek-V2 and V3.\nV2 vs V3 Gating Mechanism and Load Balancing Comparison A core challenge in MoE models is load balancing: ensuring all experts are effectively utilized, avoiding situations where some experts are overloaded while others are idle, which affects training stability and computational efficiency. DeepSeek-V2 and V3 adopt different approaches to gating mechanisms and load balancing strategies.\n1. Affinity Calculation (\\(s_{i,t}\\)) and Top-K Selection:\nDeepSeek-V2: Uses the Softmax function to compute the affinity score of each token for each routing expert. Top-K selection is directly based on these affinity scores \\(s_{i,t}\\). \\[ s_{i, t} = \\operatorname{Softmax}_{i}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i}) \\] where \\(\\mathbf{e}_i\\) is the learnable center vector for the \\(i\\)-th routing expert. The \\(K_r\\) experts with the highest \\(s_{i,t}\\) are selected.\nDeepSeek-V3: Uses the Sigmoid function to compute affinity scores. More importantly, it introduces a learnable bias term \\(b_i\\) for each routing expert. Top-K selection is based on the bias-adjusted affinity \\(s_{i,t} + b_i\\). \\[ s_{i, t} = \\operatorname{Sigmoid}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i}) \\] Selection is based on the \\(K_r\\) experts with the highest \\(s_{i,t} + b_i\\) values.\n2. Gating Value Calculation (\\(g_{i,t}\\)):\nDeepSeek-V2: For experts selected by Top-K, their gating value \\(g_{i,t}\\) is directly equal to their original affinity score \\(s_{i,t}\\). For unselected experts, \\(g_{i,t} = 0\\). \\[ g_{i, t}^{\\prime} = \\begin{cases} s_{i, t}, \u0026 s_{i, t} \\in \\operatorname{Topk}(\\{s_{j, t}\\}, K_{r}), \\\\ 0, \u0026 \\text{otherwise}, \\end{cases} \\] \\[ g_{i, t} = g_{i, t}^{\\prime} \\quad (\\text{No additional normalization in V2}) \\] DeepSeek-V3: For experts selected based on \\(s_{i,t} + b_i\\), their gating value \\(g_{i,t}\\) is obtained by normalizing the original affinity scores \\(s_{i,t}\\) of these selected experts. The bias \\(b_i\\) is only used for routing selection and does not affect the final weighted sum. \\[ g_{i, t}^{\\prime}= \\begin{cases} s_{i, t}, \u0026 s_{i, t}+b_{i} \\in \\operatorname{Topk}\\left(\\left\\{s_{j, t}+b_{j} \\mid 1 \\leqslant j \\leqslant N_{r}\\right\\}, K_{r}\\right) \\\\ 0, \u0026 \\text{otherwise.} \\end{cases} \\] \\[ g_{i, t} = \\frac{g_{i, t}^{\\prime}}{\\sum_{j=1}^{N_{r}} g_{j, t}^{\\prime}} \\quad (\\text{Normalize affinities of selected experts}) \\] 3. Load Balancing Strategy:\nDeepSeek-V2:\nPrimary Strategy: Auxiliary Losses V2 introduces multiple auxiliary loss terms to explicitly encourage load balancing: Expert-level Balancing Loss (\\(\\mathcal{L}_{\\text{ExpBal}}\\)): Encourages each expert to process roughly the same number of tokens. \\[ \\begin{aligned} \\mathcal{L}_{\\text{ExpBal}} \u0026= \\alpha_{1} \\sum_{i=1}^{N_{r}} f_{i} P_{i} \\\\ f_{i} \u0026= \\frac{N_{r}}{K_{r} T} \\sum_{t=1}^{T} \\mathbb{1}(\\text{Token } t \\text{ selects Expert } i) \\\\ P_{i} \u0026= \\frac{1}{T} \\sum_{t=1}^{T} s_{i, t} \\end{aligned} \\] where \\(T\\) is the total number of tokens in the batch, \\(f_i\\) is the fraction of tokens routed to expert \\(i\\) (relative to the ideal balanced state), \\(P_i\\) is the average affinity score for expert \\(i\\), and \\(\\alpha_1\\) is a hyperparameter. Device-level Balancing Loss (\\(\\mathcal{L}_{\\text{DevBal}}\\)): Encourages uniform distribution of computational load across different device groups (assuming experts are distributed across \\(D\\) device groups \\(\\{\\mathcal{E}_1, \\dots, \\mathcal{E}_D\\}\\)). \\[ \\begin{aligned} \\mathcal{L}_{\\text{DevBal}} \u0026= \\alpha_{2} \\sum_{i=1}^{D} f_{i}^{\\prime} P_{i}^{\\prime} \\\\ f_{i}^{\\prime} \u0026= \\frac{1}{|\\mathcal{E}_{i}|} \\sum_{j \\in \\mathcal{E}_{i}} f_{j} \\\\ P_{i}^{\\prime} \u0026= \\sum_{j \\in \\mathcal{E}_{i}} P_{j} \\end{aligned} \\] where \\(f_i'\\) is the average load score for device group \\(i\\), \\(P_i'\\) is the total affinity for device group \\(i\\), and \\(\\alpha_2\\) is a hyperparameter. Communication Balancing Loss (\\(\\mathcal{L}_{\\text{CommBal}}\\)): Encourages roughly equal numbers of tokens sent to each device to balance All-to-All communication load. \\[ \\begin{aligned} \\mathcal{L}_{\\text{CommBal}} \u0026= \\alpha_{3} \\sum_{i=1}^{D} f_{i}^{\\prime \\prime} P_{i}^{\\prime \\prime} \\\\ f_{i}^{\\prime \\prime} \u0026= \\frac{D}{M T} \\sum_{t=1}^{T} \\mathbb{1}(\\text{Token } t \\text{ is sent to Device } i) \\\\ P_{i}^{\\prime \\prime} \u0026= \\sum_{j \\in \\mathcal{E}_{i}} P_{j} \\end{aligned} \\] where \\(f_i''\\) is the fraction of tokens sent to device \\(i\\) (relative to the ideal balanced state), \\(P_i''\\) is the total affinity for device group \\(i\\), and \\(\\alpha_3\\) is a hyperparameter. Routing Restriction: Device-Limited Routing Limits each token to route to experts distributed on at most \\(M\\) different devices. In V2, \\(M=3\\). Token Dropping: During training, if a device receives more tokens than a preset capacity factor (usually slightly above the average), some tokens with the lowest routing weights (affinities) are dropped to avoid wasting computational resources. However, tokens from about 10% of sequences are preserved from dropping. DeepSeek-V3:\nPrimary Strategy: Auxiliary-Loss-Free Load Balancing V3 posits that auxiliary losses can harm model performance and thus adopts an innovative Auxiliary-Loss-Free Load Balancing (Wang et al., 2024). It achieves load balancing by dynamically adjusting the aforementioned learnable bias terms \\(b_i\\): Bias Update: After each training step, monitor the number of tokens processed by each expert \\(i\\) in the current batch. If expert \\(i\\) is overloaded (processed tokens \u0026gt; Total batch tokens / \\(N_r\\)), decrease its bias: \\(b_i \\leftarrow b_i - \\gamma\\). If expert \\(i\\) is underloaded (processed tokens \u0026lt; Total batch tokens / \\(N_r\\)), increase its bias: \\(b_i \\leftarrow b_i + \\gamma\\). \\(\\gamma\\) is a small positive step size (bias update rate hyperparameter). This way, highly loaded experts become less likely to be selected in subsequent routing, while lowly loaded experts become more likely, dynamically balancing the load at the batch level. Supplementary Strategy: Sequence-Level Auxiliary Loss (\\(\\mathcal{L}_{\\text{Bal}}\\)) V3 still retains an auxiliary loss with an extremely small weight (\\(\\alpha=0.0001\\)), but it acts on the expert selection balance within individual sequences, rather than the entire batch. This is mainly to prevent extreme imbalance within a single sequence. \\[ \\begin{gathered} \\mathcal{L}_{\\text{Bal}} = \\alpha \\sum_{i=1}^{N_{r}} f_{i} P_{i}, \\\\ f_{i} = \\frac{N_{r}}{K_{r} T_{seq}} \\sum_{t=1}^{T_{seq}} \\mathbb{1}\\left(s_{i, t} \\in \\operatorname{Topk}\\left(\\left\\{s_{j, t} \\mid 1 \\leqslant j \\leqslant N_{r}\\right\\}, K_{r}\\right)\\right), \\\\ s_{i, t}^{\\prime} = \\frac{s_{i, t}}{\\sum_{j=1}^{N_{r}} s_{j, t}}, \\quad P_{i} = \\frac{1}{T_{seq}} \\sum_{t=1}^{T_{seq}} s_{i, t}^{\\prime} \\end{gathered} \\] Note that \\(f_i, P_i\\) here are computed over a single sequence (length \\(T_{seq}\\)), and \\(s_{i,t}'\\) is the value of original \\(s_{i,t}\\) normalized within the sequence. Routing Restriction: Node-Limited Routing Similar to V2\u0026rsquo;s device limit, but applied at the node level. In V3, \\(M=4\\). No Token Dropping: Due to the effectiveness of bias-adjustment-based load balancing, V3 does not drop any tokens during training or inference. Advantages of V3\u0026rsquo;s Strategy: V3\u0026rsquo;s auxiliary-loss-free strategy aims to minimize the negative impact of the load balancing mechanism on the final model performance. By dynamically adjusting bias terms for batch-level load balancing, the constraints are looser compared to V2\u0026rsquo;s sequence-level balancing based on auxiliary losses. This allows experts to exhibit stronger specialization patterns across different domains, as routing decisions do not need to strictly follow a balanced distribution within each sequence. The figure below shows experimental results indicating this strategy outperforms auxiliary-loss-based methods on multiple benchmarks.\nFig. 11. Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. (Image source: DeepSeek-AI, 2024)\nThe key difference between auxiliary-loss-free load balancing and sequence-level auxiliary loss lies in their balancing scope: batch-level versus sequence-level. Compared to sequence-level auxiliary loss, batch-level balancing imposes more flexible constraints as it does not enforce domain balance within each sequence. This flexibility allows experts to specialize better across different domains. To validate this, the figure records and analyzes the expert load on different domains of the Pile test set for a 16B baseline model with auxiliary loss and a 16B model without auxiliary loss. It can be observed that the auxiliary-loss-free model exhibits more pronounced expert specialization patterns, as expected.\nFig. 12. Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. (Image source: DeepSeek-AI, 2024)\nDeepSeekMoE V2 vs V3 Comparison Summary Table Feature DeepSeek-V2 DeepSeek-V3 Affinity Calculation \\(s_{i,t}\\) \\(\\operatorname{Softmax}_{i}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i})\\) \\(\\operatorname{Sigmoid}(\\mathbf{u}_{t}^{T} \\mathbf{e}_{i})\\) TopK Selection Basis Original affinity \\(s_{i,t}\\) Bias-adjusted affinity \\(s_{i,t} + b_i\\) Gating Value Calc. \\(g_{i,t}\\) For selected experts, \\(g_{i,t} = s_{i,t}\\) (Usually no extra normalization) For selected experts, normalize based on original affinity \\(s_{i,t}\\): \\(g_{i, t} = \\frac{s_{i, t}}{\\sum_{j \\in \\text{Selected}} s_{j, t}}\\) Primary Load Balancing Auxiliary Losses: - \\(\\mathcal{L}_{\\text{ExpBal}}\\) (Expert-level) - \\(\\mathcal{L}_{\\text{DevBal}}\\) (Device-level) - \\(\\mathcal{L}_{\\text{CommBal}}\\) (Comm-level) Auxiliary-Loss-Free: - Dynamic adjustment of learnable bias \\(b_i\\) (step \\(\\gamma\\)) for batch-level balancing Supplementary Balancing No explicit supplementary strategy Sequence-Level Aux Loss \\(\\mathcal{L}_{\\text{Bal}}\\) (Weight \\(\\alpha\\) minimal, e.g., 0.0001), prevents extreme imbalance within single sequences Routing Restriction Device Limit: Each token routes to experts on at most \\(M=3\\) devices Node Limit: Each token routes to experts on at most \\(M=4\\) nodes Token Dropping Yes: During training, tokens exceeding device capacity with lowest affinity are dropped (preserving ~10% sequences) to mitigate bottlenecks No: No tokens dropped during training or inference Balancing Granularity Primarily enforced at sequence/batch level via auxiliary losses Primarily balanced dynamically at batch level via bias adjustment, looser constraints Impact on Performance Auxiliary losses might negatively impact model performance Designed to minimize negative impact of balancing strategy on performance, allowing better expert specialization Multi-Token Prediction (MTP) To further enhance model performance and data efficiency, DeepSeek-V3 introduces the Multi-Token Prediction (MTP) training objective (inspired by Gloeckle et al., 2024). Standard language models only predict the next token, whereas MTP makes the model predict multiple future tokens (in V3, \\(D_{MTP}=1\\), i.e., predicting the token after the next) at each position.\nMTP Implementation MTP is implemented through \\(D_{MTP}\\) sequential modules. The \\(k\\)-th MTP module (\\(k=1, \\dots, D_{MTP}\\)) contains:\nA shared embedding layer \\(\\operatorname{Emb}(\\cdot)\\) A shared output head \\(\\operatorname{OutHead}(\\cdot)\\) Independent Transformer blocks \\(\\operatorname{TRM}_k(\\cdot)\\) Independent projection matrices \\(M_k \\in \\mathbb{R}^{d \\times 2d}\\) For the \\(i\\)-th token \\(t_i\\) in the input sequence, at the \\(k\\)-th prediction depth:\nConcatenate the representation of the \\(i\\)-th token at depth \\(k-1\\), \\(\\mathbf{h}_i^{k-1}\\) (which is the main model output when \\(k=1\\)), with the embedding of the \\((i+k)\\)-th token, \\(\\operatorname{Emb}(t_{i+k})\\). Project this concatenation through matrix \\(M_k\\) to get the combined representation \\(\\mathbf{h}_i^{\\prime k}\\): \\[ \\mathbf{h}_{i}^{\\prime k} = M_{k}[\\operatorname{RMSNorm}(\\mathbf{h}_{i}^{k-1}) ; \\operatorname{RMSNorm}(\\operatorname{Emb}(t_{i+k}))] \\] Input the combined representation into the \\(k\\)-th Transformer block to get the output representation \\(\\mathbf{h}_i^k\\) for the current depth: \\[ \\mathbf{h}_{1: T-k}^{k} = \\operatorname{TRM}_{k}(\\mathbf{h}_{1: T-k}^{\\prime k}) \\] Use the shared output head to predict the probability distribution \\(P_{i+k+1}^k \\in \\mathbb{R}^V\\) for the \\((i+k+1)\\)-th token: \\[ P_{i+k+1}^{k} = \\operatorname{OutHead}(\\mathbf{h}_{i}^{k}) \\] Crucially, this implementation maintains the complete causal chain for each prediction depth, differing from methods that predict multiple tokens in parallel.\nFig. 13. Illustration of our Multi-Token Prediction (MTP) implementation. They keep the complete causal chain for the prediction of each token at each depth. (Image source: DeepSeek-AI, 2024)\nMTP Training Objective Compute the cross-entropy loss \\(\\mathcal{L}_{\\text{MTP}}^k\\) for each prediction depth \\(k\\): \\[ \\mathcal{L}_{\\text{MTP}}^{k} = \\operatorname{CrossEntropy}(P_{2+k: T+1}^{k}, t_{2+k: T+1}) = -\\frac{1}{T} \\sum_{i=2+k}^{T+1} \\log P_{i}^{k}[t_{i}] \\] The total MTP loss is the weighted average of losses across all depths: \\[ \\mathcal{L}_{\\text{MTP}} = \\frac{\\lambda}{D_{MTP}} \\sum_{k=1}^{D_{MTP}} \\mathcal{L}_{\\text{MTP}}^{k} \\] where \\(\\lambda\\) is a weighting factor (0.3 initially, 0.1 later in V3 training). This loss is added to the main model\u0026rsquo;s standard next-token prediction loss.\nMTP Inference MTP is primarily used to enhance the main model\u0026rsquo;s performance. During inference, the MTP modules can be simply discarded, and the main model works independently. Alternatively, the MTP modules can be utilized for speculative decoding (Leviathan et al., 2023; Xia et al., 2023) to accelerate the generation process. V3 experiments show an acceptance rate of 85%-90% for the second token, potentially speeding up decoding by about 1.8x.\nInfrastructure and Training Efficiency The efficient training and deployment of DeepSeek-V3 benefit from the synergistic design of algorithms, frameworks, and hardware.\nCompute Cluster DeepSeek-V3 was trained on a cluster equipped with 2048 NVIDIA H800 GPUs.\nIntra-node: Each node contains 8 H800 GPUs interconnected via high-speed NVLink and NVSwitch. Inter-node: Different nodes communicate using the InfiniBand (IB) network. Training Framework DeepSeek-V3 training is based on the self-developed, efficient, and lightweight framework HAI-LLM. Overall, it employs:\n16-way Pipeline Parallelism (PP) (Qi et al., 2023) 64-way Expert Parallelism (EP) (across 8 nodes) (Lepikhin et al., 2021) ZeRO-1 Data Parallelism (DP) (Rajbhandari et al., 2020) To achieve efficient training, Deepseek performed meticulous engineering optimizations:\nDesigned the DualPipe algorithm for efficient pipeline parallelism, reducing bubbles and overlapping computation with communication, addressing the heavy communication overhead introduced by cross-node expert parallelism. Developed efficient cross-node All-to-all communication Kernels that fully utilize IB and NVLink bandwidth while saving SM resources for communication. Carefully optimized memory usage during training, enabling DeepSeek-V3 to be trained without using Tensor Parallelism (TP). DualPipe and Computation-Communication Overlap Challenge: Cross-node expert parallelism leads to a computation-to-communication ratio close to 1:1, which is inefficient. Fig. 17. Overlapping strategy for a pair of forward and backward chunks with misaligned transformer block boundaries. Orange: forward, green: backward for input, blue: backward for weights, purple: PP communication, red: barriers. Both all-to-all and PP communications are fully hidden. (Image source: DeepSeek-AI, 2024)\nCore Idea: Overlap computation and communication within a pair of independent forward and backward chunks. Each chunk is decomposed into four components: Attention, All-to-all Dispatch, MLP, All-to-all Combine (backward Attention and MLP are further divided into backward for input and backward for weights, similar to ZeroBubble (Qi et al., 2023). By rearranging these components and manually adjusting the GPU SM ratio for communication versus computation, both All-to-all and PP communication can be fully hidden. Scheduling: Adopts a bidirectional pipeline schedule, feeding micro-batches from both ends of the pipeline simultaneously, allowing most communication to be completely overlapped. Fig. 18. Example DualPipe scheduling with 8 PP ranks and 20 micro-batches in both directions. The reverse-direction micro-batches mirror the forward ones, so their batch IDs are omitted for simplicity. Two cells within a shared black border represent mutually overlapped computation and communication. (Image source: DeepSeek-AI, 2024)\nAdvantages: Efficient even in general scenarios without heavy communication burden. Compared to ZB1P (Qi et al., 2023) and 1F1B (Harlap et al., 2018), significantly reduces pipeline bubbles, only increasing peak activation memory by a factor of \\(\\frac{1}{PP}\\). Although requiring two copies of model parameters, the memory increase is not significant due to the large EP size used in training. Compared to Chimera (Li and Hoefler, 2021), has looser requirements on the number of micro-batches (only needs to be divisible by 2), and bubble/activation memory does not grow with the number of micro-batches. Method (Method) Bubble (Bubble) Parameter (Parameter) Activation (Activation) 1F1B \\((PP-1)(F+B)\\) \\(1 \\times\\) \\(PP\\) ZB1P \\((PP-1)(F+B-2W)\\) \\(1 \\times\\) \\(PP\\) DualPipe (Deepseek V3) \\(\\left(\\frac{PP}{2}-1\\right)(F\\\u0026B+B-3W)\\) \\(2 \\times\\) \\(PP+1\\) The table above compares pipeline bubble and memory usage for different pipeline parallelism methods. \\(F\\): Forward chunk execution time; \\(B\\): Full backward chunk execution time; \\(W\\): \u0026ldquo;Backward for weights\u0026rdquo; chunk execution time; \\(F\\\u0026B\\): Execution time of two mutually overlapped forward and backward chunks.\nEfficient Cross-Node All-to-All Communication Implementation Goal: Provide sufficient computational performance for DualPipe by customizing efficient cross-node All-to-all communication Kernels (dispatching \u0026amp; combining), saving SMs dedicated to communication. Strategy: Combine MoE gating algorithm with cluster network topology (fully connected IB between nodes, NVLink within nodes). Bandwidth Utilization: NVLink bandwidth (\\(160 \\mathrm{~GB} / \\mathrm{s}\\)) is about 3.2 times IB bandwidth (\\(50 \\mathrm{~GB} / \\mathrm{s}\\)). Limit each token to be dispatched to at most 4 nodes to reduce IB traffic. Transmission Path: After token routing is determined, tokens are first transmitted via IB to the GPU with the same intra-node index on the target node. Upon arrival at the target node, they are immediately forwarded via NVLink to the specific GPU hosting the target expert, avoiding blockage by subsequently arriving tokens. Effect: IB and NVLink communication are fully overlapped. Each token can efficiently select an average of 3.2 experts/node without additional NVLink overhead. This means V3 actually selects 8 routing experts, but could theoretically scale up to 13 experts (4 nodes × 3.2 experts/node) with no increase in communication cost. Implementation: Use Warp Specialization (Bauer et al., 2014) technology to divide 20 SMs into 10 communication channels. Dispatch process: IB send, IB-to-NVLink forward, NVLink receive are handled by respective warps, with warp counts dynamically adjusted based on load. Combine process: NVLink send, NVLink-to-IB forward \u0026amp; accumulate, IB receive \u0026amp; accumulate are also handled by dynamically adjusted warps. Optimization: Dispatch and Combine Kernels overlap with computation streams. Use custom PTX instructions and automatically adjust communication chunk sizes to significantly reduce L2 cache usage and interference with other SM computation Kernels. Result: Only 20 SMs are required to fully utilize IB and NVLink bandwidth. Extreme Memory Optimization and Minimal Overhead To reduce training memory footprint, the following techniques were employed:\nRecomputation: Recompute all RMSNorm operations and MLA up-projections during backpropagation, avoiding storage of their output activations. Significantly reduces activation memory demand at minimal overhead. CPU Storage for EMA: Store the Exponential Moving Average (EMA) of model parameters in CPU memory and update asynchronously after each training step. Maintains EMA parameters without additional GPU memory or time overhead. Shared Embedding and Output Head: Leverage the DualPipe strategy to deploy the shallowest layers (including Embedding) and deepest layers (including Output Head) on the same PP rank. This allows MTP modules and the main model to physically share parameters and gradients for Embedding and Output Head, further enhancing memory efficiency. Effect: These optimizations enable DeepSeek-V3 to be trained without using expensive Tensor Parallelism (TP). FP8 Training To accelerate training and reduce GPU memory usage, DeepSeek-V3 employs an FP8 mixed-precision training framework (Dettmers et al., 2022; Noune et al., 2022; Peng et al., 2023), validating its effectiveness on ultra-large-scale models for the first time.\nMixed Precision Framework Core Computation (GEMM): Most GEMM operations (forward, activation gradient backward, weight gradient backward) use FP8 inputs, outputting BF16 or FP32, theoretically doubling computation speed. High Precision Retention: Parts sensitive to precision or with low computational overhead (e.g., Embedding, Output Head, MoE Gating, Normalization, Attention) retain BF16/FP32 precision. High Precision Storage: Main weights, weight gradients, and optimizer states (partially BF16) use higher precision, with ZeRO-1 sharding reducing GPU memory pressure. Fig. 14. The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. (Image source: DeepSeek-AI, 2024)\nPrecision Enhancement Strategies Fine-grained Quantization: To address FP8\u0026rsquo;s limited dynamic range and sensitivity to outliers (Fishman et al., 2024; He et al., 2024; Sun et al., 2024), adopt finer-grained quantization: Activations: Scaled in groups of \\(1 \\times 128\\) tiles. Weights: Scaled in groups of \\(128 \\times 128\\) blocks. This method allows scaling factors to better adapt to the range of local data, reducing quantization error. Improved Accumulation Precision: H800 Tensor Cores have limited accumulation precision (approx. 14 bits) for FP8 GEMM. To solve this, employ the Promotion to CUDA Cores strategy (Thakkar et al., 2023): Tensor Cores compute partial sums (e.g., every \\(N_C=128\\) elements), then transfer the results to CUDA Core FP32 registers for full-precision accumulation. Scaling factors from fine-grained quantization can also be efficiently applied on CUDA Cores. With concurrent execution of WGMMA operations, this method improves precision with minimal impact on computational efficiency. E4M3 Format: V3 uniformly uses the E4M3 format (4 exponent bits, 3 mantissa bits) for all tensors, rather than mixing with E5M2 (NVIDIA, 2024; Peng et al., 2023; Sun et al., 2019b). The fine-grained quantization strategy effectively mitigates the smaller dynamic range issue of E4M3. Online Quantization: Compute scaling factors based on the maximum absolute value of each tile/block in real-time, rather than relying on historical values (NVIDIA, 2024; Peng et al., 2023), ensuring quantization accuracy. Fig. 15. (a) Fine-grained quantization method to mitigate quantization errors. (b) Improved FP8 GEMM precision by promoting to CUDA Cores for high-precision accumulation. (Image source: DeepSeek-AI, 2024)\nLow-Precision Storage and Communication Optimizer States: First and second moments of AdamW (Loshchilov and Hutter, 2017) are stored in BF16. Main weights and gradient accumulation remain FP32. Activation Caching: Since Wgrad operations use FP8 inputs, activations can be cached as FP8. For specific sensitive operations (e.g., input to Linear after Attention), a custom E5M6 format with round scaling is used. Inputs to SwiGLU in MoE are also cached as FP8. Communication: Activations before MoE up-projection are quantized to FP8 for dispatch; activation gradients before MoE down-projection are also quantized to FP8. Combine operations retain BF16 precision. The figure below shows experiments demonstrating that the relative error of FP8 training loss compared to BF16 is less than 0.25%, which is within an acceptable range.\nFig. 16. Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9 DeepSeek-AI, 2024)\nInference and Deployment DeepSeek-V3 is deployed on an H800 cluster (NVLink intra-node, fully connected IB inter-node). To guarantee both SLO (Service Level Objective) for online services and high throughput, a deployment strategy separating Prefilling and Decoding stages is adopted.\nPrefilling Stage Minimum Deployment Unit: 4 nodes, 32 GPUs. Parallelism Strategy: Attention part: TP4 (Tensor Parallelism) + SP (Sequence Parallelism) combined with DP8 (Data Parallelism). The smaller TP size (4) limits TP communication overhead. MoE part: EP32 (Expert Parallelism), ensuring each expert processes a sufficiently large batch for computational efficiency. Shallow Dense MLP: Uses TP1 to save TP communication. MoE All-to-All Communication: Uses a method similar to training: first transmit tokens across nodes via IB, then forward within the node between GPUs via NVLink. Load Balancing: Employs a redundant expert deployment strategy. Based on statistics collected from online deployment, periodically (e.g., every 10 minutes) detect high-load experts and replicate them. After determining the redundant expert set, carefully reshuffle experts among GPUs within nodes based on observed load, balancing GPU load as much as possible without increasing cross-node All-to-all communication overhead. In DeepSeek-V3 deployment, the Prefilling stage sets up 32 redundant experts. Each GPU hosts its original 8 experts plus 1 additional redundant expert. Efficiency Optimization: To improve throughput and hide All-to-all and TP communication overhead, process two micro-batches with similar computational load concurrently, overlapping the Attention and MoE of one micro-batch with the Dispatch and Combine of the other. Exploration Direction: Dynamic redundancy strategy, where each GPU hosts more experts (e.g., 16), but only activates 9 per inference step. Dynamically compute the globally optimal routing scheme before each layer\u0026rsquo;s All-to-all operation begins. Since Prefilling is computationally intensive, the overhead of computing the routing scheme is almost negligible. Decoding Stage Expert Perspective: Treat the shared expert as one routing target. From this perspective, each token selects 9 experts during routing (the shared expert is considered a high-load expert that is always selected). Minimum Deployment Unit: 40 nodes, 320 GPUs. Parallelism Strategy: Attention part: TP4 + SP combined with DP80. MoE part: EP320. Each GPU hosts only one expert, with 64 GPUs responsible for hosting redundant and shared experts. All-to-All Communication: Dispatch and Combine parts use direct IB point-to-point transmission for low latency. Utilize IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency. Load Balancing: Similar to Prefilling, periodically determine the redundant expert set based on online service\u0026rsquo;s statistical expert load. However, since each GPU hosts only one expert, reshuffling is not needed. Exploration Directions: Dynamic redundancy strategy: Requires more careful optimization of the algorithm for computing the globally optimal routing scheme and fusion with the Dispatch Kernel to reduce overhead. Processing two micro-batches concurrently: Unlike Prefilling, the Attention phase takes a larger proportion of time in Decoding. Therefore, overlap the Attention of one micro-batch with the Dispatch+MoE+Combine of another. In the Decoding stage, the batch size per expert is relatively small (typically \u0026lt; 256 tokens), making memory access the bottleneck rather than computation. Since the MoE part only needs to load parameters for one expert, memory access overhead is small, and using fewer SMs does not significantly impact overall performance. Thus, only a small portion of SMs can be allocated to Dispatch+MoE+Combine without affecting the computation speed of the Attention part. Suggestions for Hardware Design Based on the implementation of All-to-all communication and the FP8 training scheme, the DeepSeek team proposes the following chip design suggestions to AI hardware vendors.\nCommunication Hardware Current State: Communication latency is hidden through computation/communication overlap, significantly reducing dependency on communication bandwidth. However, the current communication implementation relies on expensive SMs (e.g., 20 out of 132 SMs on H800 allocated for this purpose), limiting computational throughput. Furthermore, using SMs for communication leaves Tensor Cores completely idle, which is inefficient. Primary SM Tasks: Forwarding data between IB and NVLink domains, while aggregating IB traffic destined for multiple GPUs within the same node from a single GPU. Transferring data between RDMA buffers and input/output buffers. Performing Reduce operations for All-to-all Combine. Managing fine-grained memory layout when transmitting data in chunks to multiple experts across IB and NVLink domains. Expectation: Future vendors should develop hardware to offload these communication tasks from valuable computational units (SMs), potentially as GPU co-processors or network co-processors similar to NVIDIA SHARP (Graham et al., 2016). To reduce application programming complexity, this hardware should ideally unify IB (scale-out) and NVLink (scale-up) networks from the perspective of the computational units. Through this unified interface, computational units could easily perform operations like read, write, multicast, and reduce across the entire unified IB-NVLink domain by submitting communication requests based on simple primitives. Compute Hardware Higher Precision FP8 GEMM Accumulation in Tensor Cores: Problem: In the current NVIDIA Hopper architecture Tensor Core implementation, FP8 GEMM uses fixed-point accumulation, aligning mantissa products via right shifts before addition. Experiments show that after sign-extended right shifts, only the top 14 bits of each mantissa product are used, with bits beyond this range truncated. However, achieving an accurate FP32 result from accumulating, say, 32 FP8×FP8 products requires at least 34 bits of precision. Suggestion: Future chip designs should increase the accumulation precision within Tensor Cores to support full-precision accumulation, or select an appropriate accumulation bit-width based on the precision requirements of training and inference algorithms. This approach maintains computational efficiency while ensuring errors remain within acceptable limits. Support for Tile and Block Level Quantization: Problem: Current GPUs only support per-tensor quantization, lacking native support for finer-grained quantization like tile-wise and block-wise. In the current implementation, when the \\(N_C\\) interval is reached, partial results must be copied from Tensor Cores to CUDA Cores, multiplied by scaling factors, and then added to CUDA Core FP32 registers. Although combining this with the exact FP32 accumulation strategy significantly mitigates dequantization overhead, frequent data movement between Tensor Cores and CUDA Cores still limits computational efficiency. Suggestion: Future chips should support fine-grained quantization by enabling Tensor Cores to receive scaling factors and implement MMA with grouped scaling. This would allow the entire partial sum accumulation and dequantization to be performed directly within the Tensor Cores until the final result is produced, avoiding frequent data movement. Support for Online Quantization: Problem: Current implementations struggle to efficiently support online quantization, despite its proven effectiveness in research. The existing workflow requires reading 128 BF16 activation values (output from a previous computation) from HBM for quantization, writing the quantized FP8 values back to HBM, and then reading them again for MMA. Suggestion: Future chips should fuse FP8 type conversion and TMA (Tensor Memory Accelerator) access into a single operation. This would allow quantization to occur as activations are transferred from global memory to shared memory, avoiding frequent memory reads and writes. Recommend supporting warp-level cast instructions for acceleration, further promoting better fusion of Layer Normalization and FP8 cast. Alternatively, adopt near-memory computing approaches, placing computation logic near HBM. This way, BF16 elements could be directly converted to FP8 as they are read from HBM into the GPU, reducing off-chip memory access by about 50%. Support for Transposed GEMM Operations: Problem: Fusing matrix transpose with GEMM operations is cumbersome in the current architecture. In the workflow, activations from the forward pass are quantized into \\(1 \\times 128\\) FP8 tiles and stored. During backpropagation, the matrix needs to be read, dequantized, transposed, re-quantized into \\(128 \\times 1\\) tiles, and stored back into HBM. Suggestion: Future chips should support reading transposed matrices directly from shared memory before MMA operations (for the precisions required by training and inference). Combined with the fusion of FP8 format conversion and TMA access, this enhancement would significantly simplify the quantization workflow. Training Cost and Efficiency DeepSeek-V2: Compared to DeepSeek 67B (Dense), achieved 42.5% savings in training cost, 93.3% reduction in KV cache, and a 5.76x increase in maximum throughput. DeepSeek-V3: Extremely high training efficiency, requiring only 180K H800 GPU hours per 1T tokens trained. The total training cost (pre-training + context extension + post-training) was only 2.788M H800 GPU hours (approx. $5.58 million, assuming $2/hour). Pre-training took less than 2 months on the 2048 H800 GPU cluster. Training Stage H800 GPU Hours Estimated Cost (USD) Pre-training 2664 K $5.328 M Context Extension 119 K $0.238 M Post-training 5 K $0.01 M Total 2788 K $5.576 M Pre-training Data Construction Compared to DeepSeek-V2 (based on a 67B model, using a 100K vocabulary Byte-level BPE Tokenizer, 8.1T tokens), DeepSeek-V3 achieved larger scale and higher quality data construction during the pre-training phase through the following strategies:\nCorpus Expansion and Refinement\nDomain Focus: Significantly increased the proportion of text related to mathematics and programming, strengthening the model\u0026rsquo;s understanding and generation capabilities in technical domains. Multilingual Coverage: Added corpora in multiple languages beyond English and Chinese, improving cross-lingual generalization performance. Deduplication and Diversity: Employed efficient data deduplication and filtering processes to minimize redundancy while ensuring content diversity. Scale Increase: Ultimately constructed approximately 14.8T high-quality tokens, an increase of nearly 83% compared to V2. Training Strategy and Technical Innovation\nDocument Packing Combined the Document Packing (Ding et al., 2024) method, packing coherent texts into longer segments to improve GPU utilization and context integrity; did not use cross-sample attention masks to maintain implementation simplicity. Fill-in-Middle (FIM) Strategy Motivation: Inspired by DeepSeekCoder-V2 (DeepSeek-AI, 2024), aimed at enhancing the model\u0026rsquo;s ability to fill in missing information in the middle. Framework: Introduced the Prefix-Suffix-Middle (PSM) structure, with examples like: \u0026lt;|fim_begin|\u0026gt; f_pre \u0026lt;|fim_hole|\u0026gt; f_suf \u0026lt;|fim_end|\u0026gt; f_middle \u0026lt;|eos_token|\u0026gt; Application Ratio: Inserted FIM before document-level pre-packing, accounting for 10%, balancing generation and prediction tasks. Tokenizer Optimization\nBBPE Vocabulary Expansion: Adopted Byte-level BPE, expanding the vocabulary from 100K to 128K, improving coverage of rare words and proper nouns. Pre-tokenizer Improvement: Adjusted tokenization rules for multilingual scenarios, enhancing compression efficiency and encoding consistency. Boundary Bias Mitigation: Referenced Lundberg, 2023\u0026rsquo;s method to reduce bias from punctuation + newline combination tokens in few-shot scenarios by introducing a random splitting mechanism, exposing the model to more boundary variations. Hyperparameters Parameter DeepSeek-V2 DeepSeek-V3 Transformer Layers 60 61 Hidden Dimension \\(d\\) 5120 7168 Initialization Stddev 0.006 0.006 MLA Parameters Attention Heads \\(n_h\\) 128 128 Dim per Head \\(d_h\\) 128 128 KV Compression Dim \\(d_c\\) 512 (\\(4d_h\\)) 512 (\\(4d_h\\)) Query Compression Dim \\(d_c'\\) 1536 1536 Decoupled RoPE Dim \\(d_h^R\\) 64 (\\(d_h/2\\)) 64 (\\(d_h/2\\)) DeepSeekMoE Parameters MoE Layer Position All except layer 1 All except first 3 layers Shared Experts \\(N_s\\) 2 1 Routing Experts \\(N_r\\) 160 256 Expert Intermediate Dim 1536 2048 Activated Experts \\(K_r\\) 6 8 Device/Node Limit \\(M\\) 3 (Device) 4 (Node) Load Balancing Strategy Aux Losses (\\(\\alpha_1=0.003, \\alpha_2=0.05, \\alpha_3=0.02\\)) + Token Dropping Aux-Loss-Free (\\(\\gamma=0.001\\)) + Seq Loss (\\(\\alpha=0.0001\\)) MTP Parameters (V3 only) Prediction Depth \\(D_{MTP}\\) N/A 1 MTP Loss Weight \\(\\lambda\\) N/A 0.3 (first 10T) / 0.1 (last 4.8T) Training Parameters Optimizer AdamW (\\(\\beta_1=0.9, \\beta_2=0.95, wd=0.1\\)) AdamW (\\(\\beta_1=0.9, \\beta_2=0.95, wd=0.1\\)) Max Sequence Length 4K 4K Training Tokens 8.1T 14.8T Learning Rate Warmup + Step Decay (Max \\(2.4 \\times 10^{-4}\\)) Warmup + Cosine Decay + Constant (Max \\(2.2 \\times 10^{-4}\\)) Batch Size 2304 -\u0026gt; 9216 3072 -\u0026gt; 15360 Gradient Clipping 1.0 1.0 Precision BF16 FP8 Mixed Precision Long Context Extension Both models use the YaRN (Peng et al., 2023) technique to extend the context window.\nDeepSeek-V2: Extended from 4K to 128K. Used YaRN (scale \\(s=40, \\alpha=1, \\beta=32\\)), trained for 1000 steps on 32K sequence length. Adjusted length scaling factor \\(\\sqrt{t}=0.0707 \\ln s+1\\). DeepSeek-V3: Extended from 4K to 32K, then to 128K in two stages. Each stage trained for 1000 steps. YaRN parameters same as V2, length scaling factor \\(\\sqrt{t}=0.1 \\ln s+1\\). First stage sequence length 32K, second stage 128K. Both models demonstrated good long context capabilities in the NIAH test.\nFig. 19. Evaluation results on the \u0026lsquo;Needle In A Haystack\u0026rsquo; (NIAH) tests for DeepSeek-V2. (Image source: DeepSeek-AI, 2024)\nFig. 20. Evaluation results on the \u0026lsquo;Needle In A Haystack\u0026rsquo; (NIAH) tests for DeepSeek-V3. (Image source: DeepSeek-AI, 2024)\nEvaluation DeepSeek-V2 Evaluation Results:\nComparison of DeepSeek-V2 with representative open-source models (partial results). DeepSeek-V2 achieved state-of-the-art performance at the time with 21B activated parameters.\nBenchmark (Metric) # Shots DeepSeek 67B Qwen1.5 72B Mixtral 8x22B LLaMA 3 70B DeepSeek-V2 # Activated Params - 67B 72B 39B 70B 21B English MMLU (Hendrycks et al., 2020) (Acc.) 5-shot 71.3 77.2 77.6 78.9 78.5 Code HumanEval (Chen et al., 2021) (Pass@1) 0-shot 45.1 43.9 53.1 48.2 48.8 Math GSM8K (Cobbe et al., 2021) (EM) 8-shot 63.4 77.9 80.3 83.0 79.2 Chinese C-Eval (Huang et al., 2023) (Acc.) 5-shot 66.1 83.7 59.6 67.5 81.7 DeepSeek-V3 Evaluation Results:\nComparison of DeepSeek-V3-Base with representative open-source models (partial results). DeepSeek-V3-Base became the strongest open-source model on most benchmarks, especially in code and math.\nBenchmark (Metric) # Shots DeepSeek-V2 Base Qwen2.5 72B Base LLaMA-3.1 405B Base DeepSeek-V3 Base # Activated Params - 21B 72B 405B 37B English MMLU (Hendrycks et al., 2020) (EM) 5-shot 78.4 85.0 84.4 87.1 MMLU-Pro (Wang et al., 2024) (em) 5-shot 51.4 58.3 52.8 64.4 Code HumanEval (Chen et al., 2021) (Pass@1) 0-shot 43.3 53.0 54.9 65.2 LiveCodeBench-Base (Jain et al., 2024) (Pass@1) 3-shot 11.6 12.9 15.5 19.4 Math GSM8K (Cobbe et al., 2021) (Em) 8-shot 81.6 88.3 83.5 89.3 MATH (Hendrycks et al., 2021) (EM) 4-shot 43.4 54.4 49.0 61.6 Chinese C-Eval (Huang et al., 2023) (EM) 5-shot 81.4 89.2 72.5 90.1 Multilingual MMMLU-non-English (OpenAI, 2024) (em) 5-shot 64.0 74.8 73.8 79.4 Summary: DeepSeek-V3-Base, leveraging its architectural innovations, larger training dataset, and efficient training methods, comprehensively surpassed DeepSeek-V2-Base and other top open-source models (including LLaMA-3.1 405B, whose total parameter count far exceeds V3\u0026rsquo;s activated parameters).\nAlignment To enable the models to better understand instructions, follow human preferences, and enhance specific capabilities (like reasoning), both DeepSeek-V2 and V3 underwent Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL).\nSupervised Fine-Tuning DeepSeek-V2: Used approximately 1.5M instruction data points, including 1.2M helpfulness data and 0.3M safety data, focusing on improving data quality to reduce hallucinations and enhance writing ability. DeepSeek-V3: Reasoning Data: Utilized the internal DeepSeek-R1 model (Guo et al., 2025) to generate reasoning processes (math, code, logic, etc.). Since R1 outputs could be overly long or poorly formatted, V3 adopted a knowledge distillation approach: Train domain expert models (e.g., code expert): Combined original SFT data with R1-generated long CoT data (with system prompts guiding reflection/verification) for SFT+RL training. Use expert models to generate SFT data: The expert models learned during RL to blend R1\u0026rsquo;s reasoning patterns with the conciseness of regular SFT data. Rejection sampling: Filtered high-quality SFT data for the final V3 SFT. Non-Reasoning Data: Generated using DeepSeek-V2.5 and verified by human annotators. SFT Setup: Fine-tuned for 2 epochs, with learning rate cosine decayed from \\(5 \\times 10^{-6}\\) to \\(1 \\times 10^{-6}\\). Employed sample packing and mask isolation. Reinforcement Learning Both models used the Group Relative Policy Optimization (GRPO) algorithm (Shao et al., 2024) for RL. GRPO is an Actor-Only method that estimates advantage \\(A_i\\) by comparing the relative quality of a group (\\(G\\)) of candidate outputs. This avoids training a Critic model of the same size as the policy model, saving costs.\nGRPO objective function: \\[ \\begin{gathered} \\mathcal{J}_{G R P O}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_{i}\\right\\}_{i=1}^{G} \\sim \\pi_{\\theta_{o l d}}(O \\mid q)\\right] \\\\ \\frac{1}{G} \\sum_{i=1}^{G}\\left(\\min \\left(\\frac{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i} \\mid q\\right)} A_{i}, \\operatorname{clip}\\left(\\frac{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta_{o l d}}\\left(o_{i} \\mid q\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A_{i}\\right)-\\beta \\mathbb{D}_{K L}\\left(\\pi_{\\theta}| | \\pi_{r e f}\\right)\\right), \\end{gathered} \\] where the advantage \\(A_i\\) is obtained by standardizing the intra-group rewards \\(r_i\\): \\[ A_{i}=\\frac{r_{i}-\\operatorname{mean}\\left(\\left\\{r_{1}, r_{2}, \\cdots, r_{G}\\right\\}\\right)}{\\operatorname{std}\\left(\\left\\{r_{1}, r_{2}, \\cdots, r_{G}\\right\\}\\right)}. \\] The KL divergence penalty term uses the Schulman unbiased estimator: \\[ \\mathbb{D}_{K L}\\left(\\pi_{\\theta}| | \\pi_{r e f}\\right)=\\frac{\\pi_{r e f}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i} \\mid q\\right)}{\\pi_{\\theta}\\left(o_{i} \\mid q\\right)}-1. \\]Reward Model (RM):\nDeepSeek-V2: Employed a two-stage RL strategy. Reasoning Alignment: Used a specially trained \\(RM_{\\text{reasoning}}\\) to optimize for code and math reasoning tasks. Human Preference Alignment: Used a multi-reward framework combining \\(RM_{\\text{helpful}}\\), \\(RM_{\\text{safety}}\\), and rule-based \\(RM_{\\text{rule}}\\). DeepSeek-V3: Rule-based RM: For verifiable tasks (e.g., math answer format, LeetCode test cases), used rules to provide reliable rewards. Model-based RM: For free-form answers or tasks without standard answers (e.g., creative writing), used an RM initialized from the V3 SFT Checkpoint. This RM was trained on preference data with CoT to enhance reliability and reduce reward hacking risks. Self-Reward: V3 explored using the model\u0026rsquo;s own judgment capabilities (enhanced via voting) as a feedback source, especially in general scenarios, combined with ideas from Constitutional AI (Bai et al., 2022) for optimization. RL Training Optimization (V2/V3): Addressed the high resource demands of large-model RL through engineering optimizations like a hybrid engine (different parallelism strategies for training/inference), using vLLM (Kwon et al., 2023) for accelerated sampling, CPU offloading scheduling, etc.\nEvaluation DeepSeek-V2 Chat Evaluation:\nComparison of DeepSeek-V2 Chat (SFT/RL) with representative open-source Chat models on open-ended generation tasks. V2 Chat (RL) performed exceptionally well on AlpacaEval 2.0 and AlignBench.\nModel MT-Bench (Zheng et al., 2023) AlpacaEval 2.0 (Dubois et al., 2024) (LC Win Rate) AlignBench (Liu et al., 2023) (Chinese) DeepSeek 67B Chat 8.35 16.6 6.43 Mistral 8x22B Instruct 8.66 30.9 - Qwen1.5 72B Chat 8.61 36.6 7.19 LLaMA3 70B Instruct 8.95 34.4 - DeepSeek-V2 Chat (SFT) 8.62 30.0 7.74 DeepSeek-V2 Chat (RL) 8.97 38.9 7.91 DeepSeek-V3 Chat Evaluation:\nComparison of DeepSeek-V3 Chat with representative open-source and closed-source Chat models (partial results). V3 leads open-source models on most benchmarks and is comparable to top closed-source models in code, math, Chinese, and open-ended generation tasks.\nBenchmark (Metric) DeepSeek V2.5-0905 Qwen2.5 72B-Inst. LLaMA-3.1 405B-Inst. Claude-3.5- Sonnet-1022 GPT-4o 0513 DeepSeek V3 English MMLU (Hendrycks et al., 2020) (EM) 80.6 85.3 88.6 88.3 87.2 88.5 MMLU-Pro (Wang et al., 2024) (EM) 66.2 71.6 73.3 78.0 72.6 75.9 GPQA-Diamond (Rein et al., 2023) (Pass@1) 41.3 49.0 51.1 65.0 49.9 59.1 SimpleQA (OpenAI, 2024c) (Correct) 10.2 9.1 17.1 28.4 38.2 24.9 Code HumanEval-Mul (Pass@1) 77.4 77.3 77.2 81.7 80.5 82.6 LiveCodeBench (Jain et al., 2024) (Pass@1-COT) 29.2 31.1 28.4 36.3 33.4 40.5 SWE Verified (OpenAI, 2024d) (Resolved) 22.6 23.8 24.5 50.8 38.8 42.0 Math AIME 2024 (MAA, 2024 (Pass@1) 16.7 23.3 23.3 16.0 9.3 39.2 MATH-500 (Hendrycks et al., 2021) (ЕМ) 74.7 80.0 73.8 78.3 74.6 90.2 Chinese C-Eval (Huang et al., 2023) (EM) 79.5 86.1 61.5 76.7 76.0 86.5 C-SimpleQA (He et al., 2024) (Correct) 54.1 48.4 50.4 51.3 59.3 64.8 Open-Ended Arena-Hard (Li et al., 2024) 76.2 81.2 69.3 85.2 80.4 85.5 AlpacaEval 2.0 (Dubois et al., 2024) (LC Win Rate) 50.5 49.1 40.5 52.0 51.1 70.0 Summary:\nDeepSeek-V2 Chat (RL) was already a top-tier open-source chat model at its release, particularly excelling on AlpacaEval and the Chinese AlignBench. DeepSeek-V3 Chat further boosted performance, becoming the current strongest open-source chat model. It shows extremely strong performance in code, math, Chinese knowledge, and open-ended evaluations like Arena-Hard (Li et al., 2024) and AlpacaEval, reaching levels comparable to GPT-4o and Claude-3.5-Sonnet. V3\u0026rsquo;s R1 distillation significantly improved reasoning capabilities but might also increase response length, requiring a trade-off between accuracy and efficiency. V3\u0026rsquo;s self-reward capability (strong performance on RewardBench (Lambert et al., 2024)) provides an effective pathway for continuous alignment. Discussion Load Balancing Strategy Evolution: The shift from V2\u0026rsquo;s auxiliary losses to V3\u0026rsquo;s auxiliary-loss-free + bias adjustment reflects a trend towards minimizing interference with model performance while ensuring load balance. Batch-level balancing, compared to sequence-level, better facilitates expert specialization. Effectiveness of MTP: V3\u0026rsquo;s experiments demonstrate that multi-token prediction as an auxiliary training objective indeed improves model performance on standard evaluation tasks, while also offering potential for inference acceleration (speculative decoding). R1 Distillation: V3 successfully distilled the long-chain reasoning capabilities of DeepSeek-R1 into a standard LLM, significantly boosting math and code abilities. This is an important technical direction, though controlling generation length needs attention. Self-Reward: V3\u0026rsquo;s strong judgment capability (evidenced by RewardBench results (Lambert et al., 2024)) enables effective self-feedback and self-alignment. This is crucial for reducing reliance on human annotation and achieving continuous model self-improvement. SFT Data Quantity: While LIMA (Zhou et al., 2024) suggested that a small amount of high-quality SFT data can achieve good results, sufficient high-quality data is still necessary for specific skills (like instruction following, IFEval) to reach satisfactory performance. Alignment Tax: OpenAI noted in InstructGPT (Ouyang et al., 2022) that RL alignment, while improving open-ended generation capabilities, might sacrifice performance on some standard benchmarks. Both V2 and V3 made efforts in data processing and training strategies to mitigate this issue and achieve an acceptable balance. Conclusion, Limitations \u0026amp; Future Directions Conclusion DeepSeek-V2 and DeepSeek-V3 are two powerful, economical, and efficient MoE language models. Through innovations like the MLA and DeepSeekMoE architectures, along with V3\u0026rsquo;s introduction of auxiliary-loss-free load balancing, MTP, FP8 training, and R1 distillation, they have achieved breakthroughs in performance, training cost, and inference efficiency. DeepSeek-V3 has become one of the strongest open-source models currently available, with performance competitive with top closed-source models.\nLimitations General LLM Limitations: Such as knowledge cutoffs, hallucinations, factual errors, etc. Language Coverage: Primarily focused on Chinese and English, with limited capabilities in other languages (V2). V3 expanded multilingual support but remains predominantly focused on Chinese and English. Deployment Threshold (V3): Efficient inference requires relatively large deployment units (multi-node), which might be challenging for smaller teams. Inference Efficiency: Although V3\u0026rsquo;s inference efficiency improved over V2, there is still room for optimization. Future Directions Architectural Innovation: Continue optimizing MoE architectures, exploring new architectures supporting infinite context and overcoming Transformer limitations. Data Expansion: Improve the quantity, quality, and dimensionality (multimodality, etc.) of training data. Deeper Reasoning: Enhance the model\u0026rsquo;s reasoning length and depth, increasing intelligence levels. Evaluation Methods: Develop more comprehensive, multi-dimensional evaluation methods to avoid overfitting to specific benchmarks. Alignment and Safety: Continuously improve alignment techniques (e.g., self-reward) to ensure models are helpful, honest, harmless, and aligned with human values. References [1] Liu, Aixin, et al. \u0026ldquo;Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.\u0026rdquo; arXiv preprint arXiv:2405.04434 (2024).\n[2] Liu, Aixin, et al. \u0026ldquo;Deepseek-v3 technical report.\u0026rdquo; arXiv preprint arXiv:2412.19437 (2024).\n[3] Dai, Damai, et al. \u0026ldquo;Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models.\u0026rdquo; arXiv preprint arXiv:2401.06066 (2024).\n[4] Wang, Lean, et al. \u0026ldquo;Auxiliary-loss-free load balancing strategy for mixture-of-experts.\u0026rdquo; arXiv preprint arXiv:2408.15664 (2024).\n[5] Gloeckle, Fabian, et al. \u0026ldquo;Better \u0026amp; faster large language models via multi-token prediction.\u0026rdquo; Proceedings of the 41st International Conference on Machine Learning. PMLR 235:16821-16841 (2024).\n[6] Vaswani, Ashish, et al. \u0026ldquo;Attention is all you need.\u0026rdquo; Advances in neural information processing systems 30 (2017).\n[7] Shazeer, Noam. \u0026ldquo;Fast transformer decoding: One write-head is all you need.\u0026rdquo; arXiv preprint arXiv:1911.02150 (2019).\n[8] Ainslie, Joshua, et al. \u0026ldquo;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.\u0026rdquo; Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 4895-4901 (2023).\n[9] Su, Jianlin, et al. \u0026ldquo;Roformer: Enhanced transformer with rotary position embedding.\u0026rdquo; Neurocomputing 568 (2024): 127063.\n[10] Shazeer, Noam, et al. \u0026ldquo;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\u0026rdquo; arXiv preprint arXiv:1701.06538 (2017).\n[11] Lepikhin, Dmitry, et al. \u0026ldquo;Gshard: Scaling giant models with conditional computation and automatic sharding.\u0026rdquo; arXiv preprint arXiv:2006.16668 (2020).\n[12] Fedus, William, Barret Zoph, and Noam Shazeer. \u0026ldquo;Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\u0026rdquo; The Journal of Machine Learning Research 23.1: 5232-5270 (2022).\n[13] Zhou, Zexuan, et al. \u0026ldquo;Mixture-of-experts with expert choice routing.\u0026rdquo; Advances in Neural Information Processing Systems 35: 7103-7114 (2022).\n[14] Leviathan, Yaniv, Matan Kalman, and Yossi Matias. \u0026ldquo;Fast inference from transformers via speculative decoding.\u0026rdquo; Proceedings of the 40th International Conference on Machine Learning. PMLR 202:19274-19286 (2023).\n[15] Xia, Yichao, et al. \u0026ldquo;Accelerating large language model decoding with speculative sampling.\u0026rdquo; arXiv preprint arXiv:2302.01318 (2023).\n[16] Qi, Hai, et al. \u0026ldquo;ZeroBubble: A High-Performance Framework for Training Mixture-of-Experts Models.\u0026rdquo; arXiv preprint arXiv:2401.10241 (2024).\n[17] Rajbhandari, Samyam, et al. \u0026ldquo;Zero: Memory optimizations toward training trillion parameter models.\u0026rdquo; SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE (2020).\n[18] Harlap, Aaron, et al. \u0026ldquo;Pipedream: Fast and efficient pipeline parallel dnn training.\u0026rdquo; arXiv preprint arXiv:1806.03377 (2018).\n[19] Li, Shigang, and Torsten Hoefler. \u0026ldquo;Chimera: Efficiently training large-scale neural networks with bidirectional pipelines.\u0026rdquo; Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.\n[20] Bauer, Michael, Sean Treichler, and Alex Aiken. \u0026ldquo;Singe: Leveraging warp specialization for high performance on gpus.\u0026rdquo; Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming. 2014.\n[21] Dettmers, Tim, et al. \u0026ldquo;Llm. int8 (): 8-bit matrix multiplication for transformers at scale.\u0026rdquo; Advances in Neural Information Processing Systems 35: 34138-34151 (2022).\n[22] Noune, Badreddine, et al. \u0026ldquo;8-bit numerical formats for deep neural networks.\u0026rdquo; arXiv preprint arXiv:2206.02915 (2022).\n[23] Peng, Houwen, et al. \u0026ldquo;FP8-LM: Training FP8 Large Language Models.\u0026rdquo; arXiv preprint arXiv:2310.18313 (2023).\n[24] Fishman, Maxim, et al. \u0026ldquo;Scaling FP8 training to trillion-token LLMs.\u0026rdquo;) arXiv preprint arXiv:2409.12517 (2024).\n[25] He, Bobby, et al. \u0026ldquo;Understanding and minimising outlier features in neural network training.\u0026rdquo; arXiv preprint arXiv:2405.19279 (2024).\n[26] Sun, Xiao, et al. \u0026ldquo;Massive activations in large language models.\u0026rdquo; arXiv preprint arXiv:2402.17762 (2024).\n[27] NVIDIA. \u0026ldquo;Transformer Engine.\u0026rdquo; GitHub Repository (Accessed 2024).\n[28] Sun, Xiao, et al. \u0026ldquo;Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks.\u0026rdquo; Advances in neural information processing systems 32 (2019).\n[29] Loshchilov, Ilya, and Frank Hutter. \u0026ldquo;Decoupled weight decay regularization.\u0026rdquo; arXiv preprint arXiv:1711.05101 (2017).\n[30] NVIDIA. \u0026ldquo;GPUDirect Storage: A Direct Path Between Storage and GPU Memory.\u0026rdquo; NVIDIA Developer Blog (2022).\n[31] Graham, Richard L., et al. \u0026ldquo;Scalable hierarchical aggregation protocol (SHArP): A hardware architecture for efficient data reduction.\u0026rdquo; 2016 First International Workshop on Communication Optimizations in HPC (COMHPC). IEEE, 2016.\n[32] Ding, Yiran, et al. \u0026ldquo;Longrope: Extending llm context window beyond 2 million tokens.\u0026rdquo; arXiv preprint arXiv:2402.13753 (2024).\n[33] Zhu, Qihao, et al. \u0026ldquo;DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence.\u0026rdquo; arXiv preprint arXiv:2406.11931 (2024).\n[34] Lundberg, Scott M. \u0026ldquo;Guidance: Prompt Boundaries and Token Healing.\u0026rdquo; GitHub Notebook (2023).\n[35] Peng, Bowen, et al. \u0026ldquo;YaRN: Efficient Context Window Extension of Large Language Models.\u0026rdquo; arXiv preprint arXiv:2309.00071 (2023).\n[36] Hendrycks, Dan, et al. \u0026ldquo;Measuring massive multitask language understanding.\u0026rdquo; arXiv preprint arXiv:2009.03300 (2020).\n[37] Chen, Mark, et al. \u0026ldquo;Evaluating large language models trained on code.\u0026rdquo; arXiv preprint arXiv:2107.03374 (2021).\n[38] Cobbe, Karl, et al. \u0026ldquo;Training verifiers to solve math word problems.\u0026rdquo; arXiv preprint arXiv:2110.14168 (2021).\n[39] Huang, Yuzhen, et al. \u0026ldquo;C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 62991-63010.\n[40] Wang, Yubo, et al. \u0026ldquo;Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.\u0026rdquo; The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024.\n[41] Jain, Naman, et al. \u0026ldquo;LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code.\u0026rdquo; arXiv preprint arXiv:2403.07974 (2024).\n[42] Hendrycks, Dan, et al. \u0026ldquo;Measuring mathematical problem solving with the math dataset.\u0026rdquo; arXiv preprint arXiv:2103.03874 (2021).\n[43] OpenAI. \u0026ldquo;MMMLU Dataset.\u0026rdquo; Hugging Face Datasets (Accessed 2024).\n[44] Guo, Daya, et al. \u0026ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\u0026rdquo; arXiv preprint arXiv:2501.12948 (2025).\n[45] Shao, Zhihong, et al. \u0026ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\u0026rdquo; arXiv preprint arXiv:2402.03300 (2024).\n[46] Bai, Yuntao, et al. \u0026ldquo;Constitutional ai: Harmlessness from ai feedback.\u0026rdquo; arXiv preprint arXiv:2212.08073 (2022).\n[47] Kwon, Woosuk, et al. \u0026ldquo;Efficient memory management for large language model serving with pagedattention.\u0026rdquo; Proceedings of the 29th Symposium on Operating Systems Principles. 2023.\n[48] Zheng, Lianmin, et al. \u0026ldquo;Judging llm-as-a-judge with mt-bench and chatbot arena.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 46595-46623.\n[49] Dubois, Yann, et al. \u0026ldquo;Length-controlled alpacaeval: A simple way to debias automatic evaluators.\u0026rdquo; arXiv preprint arXiv:2404.04475 (2024).\n[50] Liu, Xiao, et al. \u0026ldquo;Alignbench: Benchmarking chinese alignment of large language models.\u0026rdquo; arXiv preprint arXiv:2311.18743 (2023).\n[51] Rein, David, et al. \u0026ldquo;GPQA: A Graduate-Level Google-Proof Q\u0026amp;A Benchmark.\u0026rdquo; First Conference on Language Modeling. 2024.\n[52] OpenAI. \u0026ldquo;Introducing SimpleQA\u0026rdquo; OpenAI Blog (2024).\n[53] OpenAI. \u0026ldquo;Introducing SWE-bench Verified\u0026rdquo; OpenAI Blog (2024).\n[54] Mathematical Association of America (MAA). \u0026ldquo;2024 AIME I Problems.\u0026rdquo; Art of Problem Solving Wiki (2024).\n[55] Li, Tianle, et al. \u0026ldquo;From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.\u0026rdquo; arXiv preprint arXiv:2406.11939 (2024).\n[56] Lambert, Nathan, et al. \u0026ldquo;RewardBench: Evaluating Reward Models for Language Modeling.\u0026rdquo; arXiv preprint arXiv:2403.13787 (2024).\n[57] Zhou, Chunting, et al. \u0026ldquo;Lima: Less is more for alignment.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 55006-55021.\n[58] Ouyang, Long, et al. \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in neural information processing systems 35 (2022): 27730-27744.\nCitation Citation: When reproducing or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Apr 2025). DeepSeek-V2 vs V3. https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3\nOr\n@article{syhya2025deepseekv2v3, title = \u0026#34;DeepSeek-V2 vs V3\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-04-18-deepseek-v2-v3/","summary":"\u003cp\u003eDeepSeek AI successively released \u003cstrong\u003eDeepSeek-V2\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/2405.04434\"\u003eDeepSeek-AI, 2024\u003c/a\u003e) and \u003cstrong\u003eDeepSeek-V3\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/2412.19437\"\u003eDeepSeek-AI, 2024\u003c/a\u003e), two powerful Mixture-of-Experts (MoE) language models that significantly optimize training costs and inference efficiency while maintaining state-of-the-art performance. DeepSeek-V2 has a total of 236B parameters, activating 21B per token, while DeepSeek-V3 further expands to 671B total parameters, activating 37B per token. Both support a 128K context length.\u003c/p\u003e\n\u003cp\u003eThe core innovations of these two models lie in the adoption of \u003cstrong\u003eMulti-head Latent Attention (MLA)\u003c/strong\u003e and the \u003cstrong\u003eDeepSeekMoE\u003c/strong\u003e architecture (\u003ca href=\"https://arxiv.org/abs/2401.06066\"\u003eDai et al., 2024\u003c/a\u003e). MLA drastically reduces GPU memory usage during inference by compressing the Key-Value (KV) cache into low-dimensional latent vectors, improving efficiency. DeepSeekMoE achieves stronger expert specialization capabilities and more economical training costs through fine-grained expert segmentation and shared expert isolation. Building upon V2, DeepSeek-V3 further introduces an \u003cstrong\u003eAuxiliary-Loss-Free Load Balancing\u003c/strong\u003e strategy (\u003ca href=\"https://arxiv.org/abs/2408.15664\"\u003eWang et al., 2024\u003c/a\u003e) and the \u003cstrong\u003eMulti-Token Prediction (MTP)\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/2404.19737\"\u003eGloeckle et al., 2024\u003c/a\u003e) training objective, further enhancing model performance and training efficiency.\u003c/p\u003e","title":"DeepSeek-V2 vs V3"},{"content":"LLaMA The LLaMA series of open-source models released by Meta AI has become one of the cornerstones of the large language model community, profoundly impacting the advancement of open research and applications. From the pioneering LLaMA released in early 2023, to the significantly improved LLaMA 2 later that year, to derivative models targeting specific domains (like code, safety), and the subsequent new generations LLaMA 3 and LLaMA 4 launched in 2024 and 2025 respectively, Meta has continuously committed to enhancing the performance of open-source models, gradually bringing them closer to state-of-the-art closed-source models. Below, we will introduce the key technical details of each major model in sequence.\nLLaMA 1 LLaMA 1 (Touvron et al., 2023a), released in February 2023, was Meta\u0026rsquo;s first series of open-source foundation language models. LLaMA was offered in four parameter sizes: 7B, 13B, 33B, and 65B. Its core characteristic was being trained entirely on publicly available datasets, without relying on any proprietary data. Despite having a significantly smaller parameter count than the contemporary GPT-3 (175B), the LLaMA 13B model outperformed GPT-3 on most benchmarks, while the 65B model achieved performance comparable to top models like Chinchilla-70B and PaLM-540B.\nFig. 1. Zero-shot performance of LLaMA models on Common Sense Reasoning tasks compared to other foundation models. (Source: Touvron et al., 2023a)\nTraining Data: LLaMA 1 was trained on large-scale public corpora. The 65B and 33B models used approximately 1.4 trillion tokens, while the 7B and 13B models used about 1 trillion tokens. The corpus sources were diverse, primarily including Common Crawl, C4, GitHub, Wikipedia, Books, ArXiv, and StackExchange, covering multiple domains and about 20 languages (predominantly English).\nArchitecture Design: LLaMA 1 employed a standard Transformer decoder architecture, incorporating the following key improvements to enhance performance and training efficiency:\nPre-normalization \u0026amp; RMSNorm: Adopted a Pre-normalization structure (applying normalization before each sub-layer input) and used RMSNorm (Root Mean Square Normalization) instead of standard LayerNorm. RMSNorm reduces computational complexity by omitting the mean centering step, scaling based only on the root mean square of the vector elements, while effectively maintaining training stability. SwiGLU Activation Function: Replaced the activation function in the feed-forward network (FFN) from ReLU to SwiGLU (Swish-Gated Linear Unit). SwiGLU combines the smooth non-linearity of the Swish activation function with a gating mechanism, enhancing the model\u0026rsquo;s expressive power. Concurrently, LLaMA adjusted the FFN\u0026rsquo;s hidden layer dimension (using $ \\frac{2}{3} \\times 4d $ instead of the standard $4d$, where $d$ is the model dimension) to roughly maintain the total parameter count and computational load of the FFN layer while introducing gating parameters. RoPE Rotary Position Embeddings: Utilized Rotary Position Embeddings (RoPE) for positional encoding. RoPE effectively incorporates relative positional information into self-attention calculations by applying position-dependent rotation operations to Query and Key vectors, enhancing the model\u0026rsquo;s ability to handle long sequences and capture long-range dependencies. LLaMA 1 had a maximum context length of 2048 tokens. Efficient Attention Implementation: Leveraged Meta\u0026rsquo;s open-source xformers library to implement a memory-efficient and computationally optimized causal multi-head attention mechanism. Fine-tuned Dialogue Models: At its release, LLaMA 1 primarily provided pre-trained model weights (with restricted commercial use) and did not include an official dialogue fine-tuned version. However, the open-source community quickly explored its potential. For instance, the Stanford Alpaca (Taori et al., 2023) project demonstrated that Supervised Fine-tuning (SFT) with only a small amount of instruction data could endow the LLaMA base model with strong conversational abilities, greatly promoting the research and application ecosystem of open-source LLMs.\nFig. 2. The pipeline for generating instruction-following demonstrations and training Alpaca 7B based on LLaMA 7B. (Source: Taori et al., 2023)\nTraining Stability \u0026amp; Loss Spikes\nFig. 3. Training loss curves over processed tokens for the LLaMA 7B, 13B, 33B, and 65B models. (Source: Touvron et al., 2023a)\nAs observed in Figure 3, the training loss of LLaMA models generally shows a downward trend, indicating relatively stable training. However, during the training of the 13B, 33B, and 65B models, Loss Spikes occurred, where the training loss suddenly and abnormally surged at certain points. The larger the model scale, the more pronounced the spike phenomenon seems to be, and it might occur multiple times during training.\nPhenomenon Description: A Loss Spike refers to a brief, sharp, and abnormal increase in the loss function value during model training. Potential Causes: Often related to multiple factors, including anomalous samples or distribution shifts in the training data, improper learning rate settings (too high or issues with decay strategy), interaction between the optimizer\u0026rsquo;s internal state (like Adam) and drastic gradient changes, and numerical instability in mixed-precision training (e.g., gradient overflow or underflow). Common Mitigation Strategies: Methods to resolve or mitigate Loss Spikes include: strengthening data cleaning and preprocessing; applying Gradient Clipping to limit the gradient norm; fine-tuning learning rate scheduling strategies (like Warmup, Decay); optimizing mixed-precision training configurations; and, after a spike occurs, resuming training from the nearest checkpoint, possibly skipping the specific data batch that caused the issue. LLaMA 2 LLaMA 2 (Touvron et al., 2023b), launched in July 2023, was a significant upgrade to LLaMA 1. Compared to the first generation, LLaMA 2 featured notable improvements in model scale, training data volume, context length, and model alignment. It also marked the first release of an official dialogue-optimized version, LLaMA 2-Chat, and came with a license permitting commercial use.\nArchitecture \u0026amp; Optimization: LLaMA 2\u0026rsquo;s base architecture largely inherited the successful design of LLaMA 1 (e.g., RMSNorm, SwiGLU, RoPE). Key technical updates included:\nGrouped Query Attention (GQA): For the larger 34B and 70B models, Grouped Query Attention (GQA) was adopted. GQA is a compromise between Multi-Head Attention (MHA) and Multi-Query Attention (MQA), allowing multiple Query heads to share the same set of Key and Value heads. This significantly reduces the memory footprint and computational overhead of the KV cache during inference, thereby improving the inference speed and deployment efficiency of large models with minimal impact on performance. Increased Context Length: The maximum context length of the model was extended from LLaMA 1\u0026rsquo;s 2048 tokens to 4096 tokens. This enabled the model to process longer text inputs, enhancing its capabilities in tasks like long-document question answering, summarization, and extended conversations. Training Data \u0026amp; Scale: LLaMA 2 was trained on a larger pre-training dataset, totaling approximately 2 trillion tokens, about a 40% increase compared to LLaMA 1. The data sources were more diverse, and underwent more rigorous filtering and cleaning.\nPost-Training (LLaMA 2-Chat): LLaMA 2-Chat is a meticulously aligned dialogue model. Its training process starts with the LLaMA 2 pre-trained base model and primarily involves the following stages:\nSupervised Fine-tuning (SFT): The pre-trained model is fine-tuned using high-quality instruction and dialogue samples, initially equipping it with the ability to follow instructions and engage in dialogue. Reinforcement Learning from Human Feedback (RLHF): This is a crucial step for enhancing the model\u0026rsquo;s Helpfulness and Safety. Reward Modeling: A large amount of human preference data is collected (i.e., ranking multiple responses generated by the model). One or more reward models are trained to learn to evaluate which response better aligns with human preferences (along dimensions of helpfulness and safety). RL Optimization: Using the trained reward model(s) as a reward signal, the SFT model is further optimized using PPO and Rejection Sampling. PPO aims to maximize the reward signal, while rejection sampling further improves model quality by selecting the highest-reward response from K samples generated by the model for gradient updates. This process is typically iterative, continually collecting new preference data to refine the reward model and the dialogue model itself. It drew inspiration from Anthropic\u0026rsquo;s Constitutional AI and HH-RLHF (Bai et al., 2022). The relevant HH-RLHF dataset is available on Hugging Face. Fig. 4. Illustration of the Llama 2-Chat fine-tuning process, including SFT and RLHF stages with rejection sampling and PPO. (Source: Touvron et al., 2023b)\nCode Llama Code Llama (Rozière et al., 2023), released by Meta in August 2023, is a family of large language models built upon LLaMA 2, specifically focused on coding capabilities. Through additional continued pre-training on massive amounts of programming code data and specific task fine-tuning, Code Llama demonstrates excellent capabilities in code generation, code completion, code understanding, and debugging.\nFig. 5. The Code Llama specialization pipeline, starting from Llama 2 and involving code-specific training stages. (Source: Rozière et al., 2023)\nTraining \u0026amp; Data: Code Llama started with LLaMA 2 weights and underwent continued pre-training on 500 billion tokens of code and code-related natural language corpora (for 7B/13B/34B versions) or 1 trillion tokens (for the 70B version). Training data primarily came from public code repositories and datasets. Key technical improvements include:\nLong Context Fine-tuning (LCFT): Code Llama paid special attention to long sequence processing during training, extending the sequence length to 16k tokens. To better handle long-range dependencies, the base period $\\theta$ of RoPE positional encoding was adjusted (increased from LLaMA 2\u0026rsquo;s 10,000 to 1,000,000), slowing down the decay of attention scores as the token distance increases. This allows the model to stably handle ultra-long contexts of up to 100k tokens during inference. Fig. 6. Effect of RoPE base period scaling on perplexity for long sequences, showing improved performance with a larger base. (Source: Rozière et al., 2023)\nFill-in-the-Middle (FIM): The training incorporated the Fill-in-the-Middle task. The model needs to insert appropriate code snippets given a code prefix and suffix. This capability is crucial for code completion features in Integrated Development Environments (IDEs). Model Variants: Code Llama offers several versions to meet the needs of different scenarios:\nCode Llama (Base): The foundational code model, adept at code completion and generating code from natural language. Code Llama - Python: Built upon the base model, specialized fine-tuning on an additional 100 billion tokens of Python code significantly enhances performance on Python-related tasks. Code Llama - Instruct: Fine-tuned on code-related instructions and human feedback data (approx. 5 billion tokens), enabling it to better understand natural language instructions to generate, explain, or modify code, making it more suitable as a code assistant. Each version is available in 7B, 13B, 34B, and 70B parameter sizes.\nLlama Guard Llama Guard (Inan et al., 2023), introduced by Meta in December 2023, is a model specifically designed for safeguarding the content of human-AI conversations. It aims to perform content review and risk classification for both user inputs (prompts) and model outputs (responses).\nFig. 7. Example task instructions for the Llama Guard prompt and response classification tasks, demonstrating its safety assessment capability. (Source: Inan et al., 2023)\nModel Overview: Llama Guard is based on the LLaMA 2-7B model and is specifically trained via instruction fine-tuning for the task of safety risk classification. It is not a generative model; instead, it takes a text input, determines if its content is safe, and can output specific risk category labels based on a predefined safety risk taxonomy.\nTraining \u0026amp; Taxonomy: Meta constructed a taxonomy containing various categories of unsafe content (e.g., violence, hate speech, sexual content, promotion of illegal acts) and collected high-quality labeled data for training. Llama Guard can perform multi-label classification, identifying potentially multiple risks present in the text simultaneously. Due to its instruction fine-tuning paradigm, users can flexibly adjust safety policies or customize classification standards by designing different prompts. Llama Guard can be deployed as a filter at the input end (detecting user input risks) and the output end (detecting risks in model-generated content) of a dialogue system.\nLlama Guard 3 Vision Llama Guard 3 Vision (Chi et al., 2024) is the multimodal upgraded version of Llama Guard, built upon the Llama-3.2-Vision model. It can simultaneously assess the safety risks of both image and text content, extending safety protection capabilities to multimodal scenarios. The model uses a special \u0026lt;|image|\u0026gt; token to integrate image information for unified multimodal safety review.\nFig. 8. Llama Guard 3 Vision classifying harmful content in a multimodal response classification task involving both image and text. (Source: Chi et al., 2024)\nLlama Guard 3 Vision adopts the safety risk classification standard defined by ML Commons (Vidgen et al., 2024) and expands upon it, adding detection for code interpreter abuse risks (Category S14).\nFig. 9. The 14 hazard categories used by Llama Guard 3 Vision, based on the MLCommons taxonomy with an added category for code interpreter abuse. (Source: Meta Llama, 2024)\nBenchmark results show that Llama Guard 3 Vision outperforms advanced models like GPT-4o and GPT-4o mini on multiple metrics within the MLCommons safety benchmark, both for detecting risks in user inputs and model outputs.\nFig. 10. Performance comparison of various models on the MLCommons hazard taxonomy internal test set, showing Llama Guard 3 Vision\u0026rsquo;s strong results. (Source: Chi et al., 2024)\nLLaMA 3 LLaMA 3 (Grattafiori et al., 2024) is the new generation of open-source large model series successively released by Meta starting from April 2024. It features optimizations in performance, scale, multilingual capabilities, multimodal support, and training efficiency.\nModel Scale \u0026amp; Version Evolution: The LLaMA 3 series covers a wide range of parameter scales, from small to ultra-large:\nLLaMA 3 (Initial Release, 2024/04): First released 8B and 70B scale pre-trained and instruction fine-tuned models. LLaMA 3.1 (2024/07): (Meta AI, 2024) Introduced the 405B parameter flagship model, with performance approaching GPT-4 levels on multiple benchmarks, along with updated 8B and 70B versions. LLaMA 3.2 (2024/10): Introduced lightweight models (e.g., 1B, 3B, 11B, 13B) optimized for edge devices (like phones, watches, smart homes), and released multimodal vision models (e.g., Llama-3.2-11B-Vision and Llama-3.2-90B-Vision). Fig. 11. Overview of the key hyperparameters for Llama 3 models of different scales. (Source: Grattafiori et al., 2024)\nFrom the figure above, it can be observed that training larger-scale LLMs typically requires using smaller peak learning rates. This is primarily due to:\nOptimization Landscape Complexity and Gradient Stability: Larger parameter counts lead to more complex and non-convex loss landscapes, making the model more sensitive to parameter updates. Smaller learning rates help limit the step size of each update, avoiding excessively large gradients in steep regions that could lead to training oscillations or divergence, thus ensuring a more stable convergence process. Avoiding Overfitting and Improving Generalization: Larger models have greater capacity and are more prone to overfitting the training data. Smaller learning rates allow the model to learn patterns in the data more slowly and robustly, reducing the risk of overfitting to noise or local features in the training data, which helps improve generalization performance on unseen data. Fine-grained Search and Parameter Adjustment: In high-dimensional parameter spaces, the optimal solution might reside in narrow regions. Small learning rates enable the optimization algorithm to perform a finer search, gradually approaching the optimum and avoiding \u0026ldquo;overshooting\u0026rdquo; the optimal region due to large step sizes, potentially leading to higher final model accuracy. Fig. 12. Comparison of the high-level architecture between Llama 2 and Llama 3. (Source: Umar Jamil\u0026rsquo;s PyTorch Llama Slides)\nArchitecture \u0026amp; Technical Innovations: LLaMA 3 incorporates several significant enhancements over LLaMA 2:\nMassive Pre-training Data: The pre-training data volume reached a staggering 15 trillion tokens, 7.5 times that of LLaMA 2. Data sources were broader, of higher quality, more diverse, and significantly increased the proportion of non-English languages (e.g., German, French, Spanish, Hindi, each \u0026gt;5% of total data) and code data. Optimized Tokenizer: Employed a new tokenizer implemented based on the tiktoken library, with the vocabulary size drastically expanded from LLaMA 2\u0026rsquo;s 32k to 128k. The larger vocabulary improves encoding efficiency for multiple languages (especially non-Latin scripts) and code, reducing input sequence length by about 15% on average, thereby indirectly boosting model processing efficiency and performance. Extended Context Length: The initial LLaMA 3 release (8B, 70B) supported an 8k token context window. LLaMA 3.1 (405B) further increased the maximum context window to 128k tokens, greatly enhancing the model\u0026rsquo;s ability to handle long documents, long conversation histories, and complex contextual reasoning. This is typically achieved through techniques like RoPE frequency adjustments and attention mechanism optimizations (e.g., FlashAttention). Universally Applied GQA: Unlike LLaMA 2, which only used GQA in larger models, all scales of LLaMA 3 models (including 8B) adopted Grouped Query Attention (GQA) to optimize memory usage and computation speed during inference. Advanced Alignment Techniques: During the instruction fine-tuning (Post-training) phase, LLaMA 3 combined multiple advanced techniques, including Supervised Fine-tuning (SFT), Rejection Sampling, and Direct Preference Optimization (DPO), aiming to comprehensively improve the model\u0026rsquo;s instruction-following ability, Helpfulness, and Safety. Multimodal Integration (LLaMA 3.2): Introduced a Vision Encoder and performed joint training to achieve fusion processing of images and text, leading to the Llama-3.2-Vision series of vision-language models. Lightweight Models (LLaMA 3.2): Targeted resource-constrained edge computing scenarios by introducing smaller models (1B, 3B, etc.) through model compression techniques (like pruning, distillation), achieving a good balance between performance and resource consumption. Fig. 13. Illustration of the overall post-training approach for Llama 3, involving multiple stages and iterative refinement. (Source: Grattafiori et al., 2024)\nAs shown in the figure above, LLaMA 3\u0026rsquo;s post-training (instruction fine-tuning) process is a carefully designed multi-stage iterative procedure:\nData Preparation: Collect large amounts of human preference data. This data typically includes a prompt and multiple model-generated responses, which annotators rank (e.g., selecting the best \u0026ldquo;chosen\u0026rdquo; response and a worse \u0026ldquo;rejected\u0026rdquo; response). High-quality SFT data (prompt-response pairs) are also collected.\nReward Modeling (RM): Utilize the collected human preference data triplets (prompt, chosen, rejected) to train one or more reward models. The goal of the reward model is to learn to predict the degree of human preference for model-generated responses, providing a quantitative signal for subsequent optimization. LLaMA 3 trained two separate reward models focusing on Helpfulness and Safety, respectively.\nRejection Sampling: Use the trained reward model(s) to score candidate responses generated by the model. Select the highest-scoring responses as high-quality samples for subsequent fine-tuning stages. This helps filter out samples of higher quality than the initial SFT data.\nSupervised Finetuning (SFT): Combine the initial human-annotated SFT data with the high-quality data filtered through rejection sampling to fine-tune the pre-trained base model. This stage aims to teach the model the format and style of following instructions and to initially grasp the required knowledge and abilities. LLaMA 3 used a mix of data from various sources in this stage.\nPreference Optimization: Starting from the SFT model, use the preference data (prompt, chosen, rejected) to further align the model via the Direct Preference Optimization (DPO) algorithm. DPO directly optimizes the model to increase the likelihood of the \u0026ldquo;chosen\u0026rdquo; response while decreasing the likelihood of the \u0026ldquo;rejected\u0026rdquo; response. Compared to RL-based PPO methods, it is simpler to implement and more stable to train. LLaMA 3 made improvements to DPO, such as masking special formatting tokens in the responses during DPO training and introducing a normalized negative log-likelihood (NLL) loss as a regularizer to enhance training stability and generation quality. Its loss function form can be roughly referenced from the loss in RPO (Pang et al., 2024), though LLaMA3\u0026rsquo;s specific implementation might differ slightly:\n$$ \\begin{aligned} \\mathcal{L}_{\\mathrm{DPO}+\\mathrm{NLL}} \u0026 =\\mathcal{L}_{\\mathrm{DPO}}\\left(y^w, y^l \\mid x\\right)+\\alpha \\mathcal{L}_{\\mathrm{NLL}}\\left(y^w \\mid x\\right) \\\\ \u0026 =-\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y^w \\mid x)}{\\pi_{\\mathrm{ref}}(y^w \\mid x)}-\\beta \\log \\frac{\\pi_\\theta(y^l \\mid x)}{\\pi_{\\mathrm{ref}}(y^l \\mid x)}\\right)-\\alpha \\frac{\\log \\pi_\\theta(y^w \\mid x)}{|y^w|} \\end{aligned} $$ Where:\n$x$ is the input prompt. $y^w$ is the preferred (winning/chosen) response, $y^l$ is the dispreferred (losing/rejected) response. $\\pi_\\theta$ is the current model policy being optimized (with parameters $\\theta$). $\\pi_{\\mathrm{ref}}$ is the reference model policy (often the SFT model or the model from the previous iteration). $\\beta$ is a hyperparameter controlling the strength of the preference margin. $\\sigma$ is the Sigmoid function. $\\alpha$ is the weight balancing the DPO loss and the NLL regularization loss. $|y^w|$ is the length of the winning response, used to normalize the NLL loss. This loss function encourages the model $\\pi_\\theta$ to prefer generating $y^w$ over $y^l$ relative to the reference model $\\pi_{\\mathrm{ref}}$, while the NLL regularization term helps maintain the fluency and linguistic quality of the generated text. Iterative Loop: The SFT and DPO (or RLHF variant) processes described above are repeated for multiple rounds (LLaMA 3 underwent five rounds). In each round, the model optimized in the previous round is used to generate new data, new human feedback is collected, new reward models are trained, and the next round of SFT and DPO optimization is performed. This iterative approach allows the model to continuously learn and improve.\nModel Weight Averaging: At certain stages, weight averaging might be performed across multiple model checkpoints trained with different data subsets or hyperparameters to obtain a final model that is more robust and has more balanced performance.\nLLaMA 4 The LLaMA 4 (Meta AI, 2025) series of models, released by Meta AI on April 5, 2025, marks the LLaMA ecosystem\u0026rsquo;s entry into a new phase of natively multimodal AI innovation. This generation introduces the Mixture-of-Experts (MoE) architecture for the first time and possesses unprecedented ultra-long context processing capabilities, aiming to provide more powerful and efficient open-source foundation models.\nModel Overview: Performance, Scale \u0026amp; Deployment\nLLaMA 4 initially released three models with different positionings, two of which have open weights:\nModel Name Active Params Num Experts Total Params Key Performance/Positioning Hardware Reference Context Window Llama 4 Scout 17B 16 109B Outperforms peer models like Gemma 3; 10M Token ultra-long context; Strong image understanding; High cost-performance Single H100 GPU (INT4 quantized) 10M Llama 4 Maverick 17B 128 400B Matches or surpasses GPT-4o/Gemini 2.0 Flash (reasoning/coding/multilingual); Fewer active params, high computational efficiency; Leading in image reasoning/understanding; LMArena ELO 1417 Single H100 host (multi-GPU) or distributed deployment 1M Llama 4 Behemoth 288B 16 ~2T Teacher Model (Unreleased); Surpasses GPT-4.5/Claude 3.7/Gemini 2.0 Pro on STEM benchmarks (MATH, GPQA); Improves Scout/Maverick via co-distillation Still in training, not publicly released (Not specified) Performance Highlights: Maverick (17B active parameters) demonstrates competitive strength against top-tier closed-source models like GPT-4o on several mainstream benchmarks, especially in reasoning, coding, and multilingual tasks, while having significantly fewer active parameters, reflecting excellent computational efficiency. Scout stands out among its peers with its astounding 10M token context window. Deployment Threshold: Scout\u0026rsquo;s INT4 quantized version can run on a single H100, lowering the deployment barrier for high-performance models. Although Maverick requires more compute power (e.g., a single H100 multi-GPU host), it still offers attractive cost-performance relative to its capabilities. (Note: Running these models on consumer-grade GPUs remains challenging) Core Architecture \u0026amp; Training Innovations\nLLaMA 4 features the following optimizations compared to the previous generation:\nMixture-of-Experts (MoE) Architecture:\nLLaMA 4 is the first Llama series to adopt MoE. MoE allows the model to activate only a small fraction of its total parameters (\u0026ldquo;active parameters\u0026rdquo;) during inference, achieving larger model capacity and stronger performance with lower computational cost. This is highly beneficial for compute-cost-sensitive (especially throughput-sensitive) inference scenarios. The Maverick model employs alternating dense and MoE layers. Its MoE layers contain 128 routing experts and one shared expert accessed by all tokens. Each token is routed to the shared expert plus one of the routing experts for processing. Native Multimodality \u0026amp; Early Fusion:\nMoving beyond \u0026lsquo;stitched\u0026rsquo; approaches: Unlike previous methods that \u0026ldquo;bolted on\u0026rdquo; visual modules to LLMs using late fusion, LLaMA 4 adopts an early fusion strategy from the ground up. Unified Backbone: Text tokens and visual tokens (from image and video frames) are seamlessly integrated and processed together in the early stages of the model\u0026rsquo;s backbone network. Deep Understanding: This enables joint pre-training on massive amounts of image-text and video data, allowing the model to learn deeper, more fine-grained cross-modal associations, achieve more natural interaction, and exhibit stronger visual grounding capabilities (accurately mapping text prompts to image regions), going beyond simple \u0026ldquo;image captioning.\u0026rdquo; Vision Encoder: Based on MetaCLIP (Xu et al., 2023), improved and co-trained with the Llama model to better suit the LLM\u0026rsquo;s needs. Ultra-Long Context:\n10M Token Limit: Llama 4 Scout achieves an industry-leading 10 million token context window. Technical Underpinnings: iRoPE Architecture: Combines ideas from RoPE (Rotary Position Embeddings) and NoPE (No Positional Encoding). Implemented via interleaved attention layers, where specific layers use NoPE (Kazemnejad et al., 2023), relying on the attention mechanism to implicitly learn positional relationships, while RoPE is still used in most other layers. (The \u0026ldquo;i\u0026rdquo; signifies both interleaved and the goal of infinite context). Scalable-Softmax: Combined with inference-time temperature scaling (Nakanishi et al., 2025), enhancing the model\u0026rsquo;s generalization ability to unseen lengths. Specialized Training: Underwent mid-training and post-training on specially constructed long-context datasets. Scout was trained on 256k context length and generalized to 10M via iRoPE and Scalable Softmax. Practicality Observation: While 10M tokens are appealing, processing such long contexts in practice may encounter issues like inference efficiency, attention diffusion, and bandwidth bottlenecks. Its effectiveness and efficiency in real-world scenarios remain to be validated by users. Fig. 14. Cumulative average NLL loss per sequence position for code generation, demonstrating Llama 4 Scout\u0026rsquo;s strong performance over long contexts. (Source: Meta AI, 2025)\nLarge-Scale High-Quality Pre-training: Data Scale: Training data exceeds 30 trillion tokens (more than double LLaMA 3), including text, images, and video. Multilingual Coverage: Covers 200 languages, with over 100 languages having more than 1 billion tokens each. Total multilingual token count is 10 times that of LLaMA 3. Training Efficiency: Trained using FP8 precision. Behemoth achieved high utilization of 390 TFLOPs/GPU on 32K GPUs. Utilized MetaP technology to reliably set hyperparameters. Revolutionary Post-training Process\nLLaMA 4 employs a new three-stage post-training process designed to balance instruction following, emergent intelligence, and dialogue quality:\nLightweight SFT (Supervised Fine-Tuning): Focuses on supervised fine-tuning using a small amount of harder datasets to teach the model basic instruction following and dialogue formats, avoiding overfitting simple patterns and preserving space for subsequent RL exploration. Significantly reduced simple SFT data compared to previous versions (Maverick \u0026gt;50%, Behemoth \u0026gt;95%). Online RL (Reinforcement Learning): The key stage for enhancing the model\u0026rsquo;s core intelligence and complex task capabilities. Employs a continuous online RL strategy where the model learns through interaction with the environment, explores using carefully selected harder prompts, and alternates between model training and data filtering (retaining medium-to-hard interaction data) to balance computation and effectiveness. Lightweight DPO (Direct Preference Optimization): Performed after RL to fine-tune the model\u0026rsquo;s response style, safety, and correct corner cases, serving as the final \u0026ldquo;refinement and polishing\u0026rdquo; step to ensure the unification of intelligence and smooth conversational experience. Teacher Model \u0026amp; Co-Distillation\nThe powerful Behemoth (2T), though unreleased, transferred its knowledge to Scout and Maverick during the pre-training phase via a novel co-distillation technique. This co-distillation occurred during pre-training, using a new distillation loss function with dynamically weighted soft targets (teacher model\u0026rsquo;s logits) and hard targets (true labels). This significantly improved the quality of the student models (especially in math, coding, etc.) while amortizing the training cost of the teacher model. Large-Scale RL Infrastructure\nTo train ultra-large MoE models like Behemoth, Meta completely overhauled its RL infrastructure, adopting a fully asynchronous online RL training framework. This optimized MoE parallelism, enabled flexible GPU resource allocation, and achieved nearly a 10x improvement in training efficiency.\nComparison Feature LLaMA 1 LLaMA 2 Code Llama Llama Guard LLaMA 3 LLaMA 4 Release Date 2023/02 2023/07 2023/08 2023/12+ 2024/04+ 2025/04+ Base Model - - LLaMA 2 LLaMA 2 / LLaMA 3 - - Model Scale 7B - 65B 7B, 13B, 70B 7B - 70B 7B / 8B (+Vision) 1B - 405B (+Vision) 109B, 400B, ~2T (MoE) Training Data Size 1T - 1.4T tokens 2T+ tokens + 0.5T/1T Code ~40k Safety Class. 15T+ tokens 30T+ tokens (Multimodal) Context Length 2k tokens 4k tokens 100k tokens 4k / 8k+ 8k / 128k tokens 1M / 10M tokens Tokenizer SentencePiece (32k) SentencePiece (32k) SentencePiece (32k) Based on LLaMA 2/3 tiktoken (128k) tiktoken (256k) Positional Encoding RoPE RoPE RoPE (Base adjusted) RoPE RoPE iRoPE Attention MHA MHA / GQA (34B, 70B) MHA / GQA (\u0026gt;13B) Based on LLaMA 2/3 GQA GQA Normalization RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) RMSNorm (PreNorm) Activation Func. SwiGLU SwiGLU SwiGLU SwiGLU SwiGLU SwiGLU Model Type Text Model Text Model Code Generation Safety Classifier Multimodal Model Multimodal Model Key Technology Analysis Below is an analysis of the key technologies widely adopted in the LLaMA series.\nRMS Normalization (RMSNorm) In deep learning model training, normalization layers are crucial for accelerating convergence, improving generalization, and stabilizing the training process. RMSNorm (Root Mean Square Normalization) (Zhang \u0026amp; Sennrich, 2019) is a simplified variant of Layer Normalization. It normalizes using only the Root Mean Square (RMS) of the inputs, omitting the mean centering step, thus reducing computation.\nIts mathematical expression is: $$ \\text{RMSNorm}(x) = \\frac{x}{\\text{RMS}(x)} \\cdot \\gamma = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$ Where:\n$ x \\in \\mathbb{R}^d $ is the input vector. $ d $ is the vector dimension. $ \\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon} $ calculates the root mean square of the input. $ \\epsilon $ is a small positive number (e.g., $10^{-6}$) to prevent division by zero and increase numerical stability. $ \\gamma \\in \\mathbb{R}^d $ is a learnable scaling parameter vector (gain). RMSNorm typically omits the learnable offset parameter (bias) $ \\beta $ found in LayerNorm. Main reasons for LLaMA series choosing RMSNorm:\nHigh Computational Efficiency: Compared to LayerNorm, RMSNorm requires less computation because it doesn\u0026rsquo;t need to calculate the mean. This is particularly important for computationally intensive large language model training and inference. Comparable Performance: Practice has shown that RMSNorm often achieves performance comparable to or even better than LayerNorm in architectures like Transformers, while maintaining training stability. Simple Implementation: Its computational logic is relatively simple, making it easy to implement efficiently on various hardware. For a comparison of various Norm techniques and code implementations, refer to the blog post: Normalization in Deep Learning.\nFFN_SwiGLU Swish-Gated Linear Unit (SwiGLU) (Shazeer, 2020) is a key technique used in LLaMA to enhance the non-linear expressive capability of the Feed-Forward Network (FFN). SwiGLU combines the Swish activation function with a gating mechanism, significantly improving the model\u0026rsquo;s expressiveness and performance. Furthermore, unlike the $4d$ hidden dimension used in PaLM (Chowdhery et al., 2022), LLaMA employs a $\\frac{2}{3} \\times 4d$ hidden dimension, achieving higher parameter efficiency while keeping the parameter count and computational load roughly constant.\nMathematical expression: $$ \\operatorname{FFN}_{\\mathrm{SwiGLU}}\\left(x, W_1, W_3, W_2\\right)=\\left(\\operatorname{Swish}\\left(x W_1\\right) \\otimes x W_3\\right) W_2 $$ Where:\n$ \\text{Swish}(x) = x \\cdot \\sigma(x) $ (Swish activation function). $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $ (Sigmoid function). $ \\otimes $ denotes element-wise multiplication. $ W_1, W_2, W_3 $ are linear transformation matrices. Advantages:\nEnhanced Non-linear Expression: By combining the Swish activation function with a gating mechanism, SwiGLU can more effectively capture complex patterns and relationships, boosting the expressive power of the FFN layer. Parameter Efficiency: Using a $\\frac{2}{3} \\times 4d$ hidden dimension allows the introduction of an additional linear transformation matrix while maintaining the total parameter count, leading to efficient parameter utilization. Performance Improvement: FFN_SwiGLU has shown significant performance improvements on various benchmarks, especially excelling in handling complex tasks and long texts. For example, in text generation and understanding tasks, SwiGLU helps the model better grasp context and long-range dependencies. Implementation Details:\nWeight Matrix Adjustment: To maintain the same parameter count and computational load as traditional FFN layers, SwiGLU reduces the hidden layer dimension (e.g., adjusting the hidden size from $4d$ to $\\frac{2}{3} \\times 4d$), ensuring the overall model efficiency is unaffected despite introducing an extra linear transformation matrix. Compatibility: As a member of the GLU family, SwiGLU can be seamlessly integrated into existing Transformer architectures, replacing traditional ReLU or GELU activation functions to enhance overall model performance. For implementation code, refer to this file: swiglu.py.\nGrouped Query Attention (GQA) Grouped Query Attention (GQA) (Ainslie et al., 2023) is a key optimization technique for the standard Multi-Head Attention (MHA) mechanism, particularly applied in large language models like LLaMA. Its primary goal is to reduce the memory bandwidth and capacity required for loading and storing the KV Cache during inference, thereby achieving a better balance between model performance and computational efficiency.\nGQA is a compromise between MHA and Multi-Query Attention (MQA):\nMHA: Has $H$ Query heads, each with its own independent set of $H$ Key (K) and Value (V) projections. Computation and KV Cache size are proportional to the number of heads $H$. MQA: Still has $H$ Query heads, but all heads share a single set of K and V projections. This drastically reduces the KV Cache size (to $1/H$ of MHA\u0026rsquo;s), but can potentially degrade model quality. GQA: Divides the $H$ Query heads into $G$ groups ($1 \u0026lt; G \u0026lt; H$, and $H$ is a multiple of $G$). The $H/G$ Query heads within each group share the same set of K and V projections. There are a total of $G$ sets of K and V projections. Fig. 15. Overview of Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped-Query Attention (GQA). GQA groups query heads to share key/value heads. (Source: Ainslie et al., 2023)\nThe computation steps are as follows:\nProjection: Input $X$ is still projected to get $Q, K, V$. $Q$ is split into $H$ heads $Q_1, \\dots, Q_H$. $K$ and $V$ are split into $G$ groups $K^1, \\dots, K^G$ and $V^1, \\dots, V^G$. Grouped Attention: For the $g$-th group ($g=1, \\dots, G$), the corresponding Query heads (e.g., $Q_i$ where $i$ belongs to group $g$) compute attention with the shared $K^g$ and $V^g$: $$ \\text{Attention}_i(Q_i, K^g, V^g) = \\text{softmax}\\left( \\frac{Q_i (K^g)^\\top}{\\sqrt{d_k}} \\right) V^g $$ where $d_k$ is the dimension of each K head (and also Q head). Concatenation \u0026amp; Output: The outputs of all heads $ \\text{Attention}_1, \\dots, \\text{Attention}_H $ are concatenated and then passed through an output projection matrix $W_O$ to get the final output. Advantages:\nBalances Performance and Efficiency: GQA significantly reduces the KV Cache size (to $G/H$ of MHA\u0026rsquo;s) while typically maintaining model quality closer to MHA than MQA does. Accelerates Inference: Reducing memory bandwidth requirements can significantly speed up inference for large models, especially in long sequence generation scenarios. For a more detailed comparison between MHA, MQA, and GQA attention mechanisms, along with code examples, refer to the blog post: Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA.\nRotary Positional Embeddings (RoPE) Rotary Positional Embeddings (RoPE) (Su et al., 2021) is an effective method for injecting positional information into the Transformer attention mechanism, particularly adept at encoding relative positional information. Unlike traditional absolute positional encodings (like sinusoidal or learnable embeddings), RoPE achieves this by applying position-dependent rotation operations to the Query and Key vectors.\nFig. 16. Implementation of Rotary Position Embedding(RoPE). (Source: Su et al., 2021)\nAssume $q_m$ and $k_n$ are the Query vector at position $m$ and the Key vector at position $n$, respectively. RoPE treats a $d$-dimensional vector $x$ ($q$ or $k$) as $d/2$ blocks of 2D vectors $[x^{(1)}, x^{(2)}, \\dots, x^{(d/2)}]$, where $x^{(i)} = [x_{2i-1}, x_{2i}]$. For position $m$, RoPE defines a rotation matrix $R_m$ composed of $d/2$ 2D rotation matrices: $$ R_m = \\text{diag}(R_{m,1}, R_{m,2}, \\dots, R_{m,d/2}) $$where each 2D rotation matrix is: $$ R_{m,i} = \\begin{pmatrix} \\cos(m\\theta_i) \u0026 -\\sin(m\\theta_i) \\\\ \\sin(m\\theta_i) \u0026 \\cos(m\\theta_i) \\end{pmatrix} $$The rotation frequencies are $ \\theta_i = b^{-2(i-1)/d} $, where $b$ is a predefined base (typically 10000 in LLaMA).\nAfter applying RoPE, the new Query and Key vectors are $q\u0026rsquo;_m = R_m q_m$ and $k\u0026rsquo;_n = R_n k_n$. The key insight is that their inner product (dot product, which determines attention scores) depends only on the relative position $m-n$:\n$$ (q'_m)^\\top k'_n = (R_m q_m)^\\top (R_n k_n) = q_m^\\top R_m^\\top R_n k_n = q_m^\\top R_{n-m} k_n $$This utilizes the property of rotation matrices $R_m^\\top R_n = R_{n-m}$.\nAdvantages:\nExplicit Relative Position Encoding: The inner product result directly depends on the relative distance $m-n$, which is very natural for capturing relationships between elements in a sequence. Long-Distance Decay Property: As the relative distance $|m-n|$ increases, the change in angle between vectors due to rotation typically causes the inner product value to decay, aligning with the intuition that more distant elements have weaker correlations. Good Extrapolation: Theoretically, RoPE can generalize better to sequence lengths longer than those seen during training because it doesn\u0026rsquo;t rely on a maximum absolute position. Adjusting the base $b$ (as in Code Llama and LLaMA 4\u0026rsquo;s iRoPE) can further optimize its performance on ultra-long contexts. No Extra Parameters: RoPE is a fixed, position-based transformation that introduces no additional learnable parameters. Compatibility with Linear Attention: It can be used in conjunction with various linear attention variants. Mixture-of-Experts (MoE) Mixture-of-Experts (MoE) is a neural network architecture paradigm designed to increase model capacity (total parameters) while controlling computational cost (active parameters). It replaces certain layers in the network (typically FFN layers) with multiple parallel expert subnetworks. A lightweight gating network dynamically selects a small number (usually Top-K, with K=1 or 2) of these experts for each input token to perform computation.\nFig. 17. The Illustration of a mixture-of-experts(MoE) in llama4. (Source: Meta AI, 2025)\nAssume an MoE layer has $N$ experts $E_1, E_2, \\dots, E_N$ (e.g., each expert is an independent FFN) and a gating network $G$. For an input token $x$, the computation process of the MoE layer is as follows:\nGating Calculation: The gating network $G$ (often a simple linear layer followed by Softmax) computes the probability or weight for selecting each expert: $p = G(x) = \\text{Softmax}(\\text{Linear}(x))$, where $p \\in \\mathbb{R}^N$. Expert Selection (Top-K Routing): Based on the gating output $p$, the K experts with the highest scores are selected. Let the set of selected expert indices be $\\mathcal{T} = \\text{TopKIndices}(p)$. Expert Computation: Only the selected K experts compute on the input $x$, yielding outputs $E_i(x)$ for $i \\in \\mathcal{T}$. Output Combination: The final output $y$ is the weighted sum of the outputs from the selected experts, using their gating weights (often re-normalized): $$ y = \\sum_{i \\in \\mathcal{T}} \\frac{p_i}{\\sum_{j \\in \\mathcal{T}} p_j} \\cdot E_i(x) $$ Alternatively, in some implementations, the weights $p_i$ might be used directly. Advantages:\nDecoupling Parameters and Computation: MoE allows models to have a massive total parameter count (by increasing the number of experts $N$), but the computational cost of each forward pass depends only on the computation of the activated K experts, which is much lower than that of a dense model with an equivalent total parameter count. This enables training larger capacity, potentially higher-performing models within a limited computational budget. Expert Specialization: Theoretically, different experts can learn to handle specific aspects of different types of data, patterns, or tasks, enabling modular storage and processing of knowledge, thereby enhancing the model\u0026rsquo;s overall capability and generalization. Challenges:\nLoad Balancing: Ensuring that all experts are utilized roughly equally is necessary to avoid some experts being overloaded while others remain idle. This often requires introducing auxiliary loss functions (like Load Balancing Loss) to encourage uniform routing. Communication Overhead: In distributed training and inference, efficient communication (e.g., All-to-All) is needed between different devices (GPUs) to route tokens to the devices storing the corresponding experts and to gather the results. This increases implementation complexity and communication costs. Training Stability: Training MoE models can be less stable than training dense models, requiring careful tuning of hyperparameters and training strategies. Memory Footprint: Although computation is sparse, the total number of parameters is huge, requiring substantial memory to store all expert weights. For a more detailed explanation of MoE, refer to the Mixture-of-Experts section in the blog post: Parallelism and Memory Optimization Techniques for Training Large Models.\nReferences [1] Touvron, Hugo, et al. \u0026ldquo;LLaMA: Open and Efficient Foundation Language Models.\u0026rdquo; arXiv preprint arXiv:2302.13971 (2023).\n[2] Facebook Research. \u0026ldquo;xformers.\u0026rdquo; GitHub repository (Accessed 2024).\n[3] Taori, Rohan, et al. \u0026ldquo;Alpaca: A Strong, Replicable Instruction-Following Model.\u0026rdquo; Stanford CRFM Blog (2023).\n[4] Touvron, Hugo, et al. \u0026ldquo;Llama 2: Open Foundation and Fine-Tuned Chat Models.\u0026rdquo; arXiv preprint arXiv:2307.09288 (2023).\n[5] Bai, Yuntao, et al. \u0026ldquo;Constitutional AI: Harmlessness from AI Feedback.\u0026rdquo; arXiv preprint arXiv:2212.08073 (2022).\n[6] Roziere, Baptiste, et al. \u0026ldquo;Code Llama: Open Foundation Models for Code.\u0026rdquo; arXiv preprint arXiv:2308.12950 (2023).\n[7] Inan, Hakan, et al. \u0026ldquo;Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations.\u0026rdquo; arXiv preprint arXiv:2312.06674 (2023).\n[8] Chi, Jianfeng, et al. \u0026ldquo;Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations.\u0026rdquo; arXiv preprint arXiv:2411.10414 (2024).\n[9] Vidgen, Bertie, et al. \u0026ldquo;Introducing v0.5 of the AI Safety Benchmark from MLCommons.\u0026rdquo; arXiv preprint arXiv:2404.12241 (2024).\n[10] Meta Llama. \u0026ldquo;Llama-Guard-3-8B.\u0026rdquo; Hugging Face Model (Accessed 2024).\n[11] Grattafiori, Aaron, et al. \u0026ldquo;The Llama 3 Herd of Models.\u0026rdquo; arXiv preprint arXiv:2407.21783 (2024).\n[12] Meta AI. \u0026ldquo;Introducing Llama 3.1: Our most capable models to date.\u0026rdquo; Meta AI Blog (2024).\n[13] Umar Jamil. \u0026ldquo;pytorch-llama Slides.\u0026rdquo; GitHub file (Accessed 2024).\n[14] Pang, Richard Yuanzhe, et al. \u0026ldquo;Iterative reasoning preference optimization.\u0026rdquo; Advances in Neural Information Processing Systems 37 (2024): 116617-116637.\n[15] Meta AI. \u0026ldquo;The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation\u0026rdquo; Meta AI Blog (2025).\n[16] Xu, Hu, et al. \u0026ldquo;Demystifying clip data.\u0026rdquo; arXiv preprint arXiv:2309.16671 (2023).\n[17] Kazemnejad, Amirhossein, et al. \u0026ldquo;The impact of positional encoding on length generalization in transformers.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 24892-24928.\n[18] Nakanishi, Ken M. \u0026ldquo;Scalable-Softmax Is Superior for Attention.\u0026rdquo; arXiv preprint arXiv:2501.19399 (2025).\n[19] Zhang, Biao, and Rico Sennrich. \u0026ldquo;Root mean square layer normalization.\u0026rdquo; Advances in Neural Information Processing Systems 32 (2019).\n[20] Shazeer, Noam. \u0026ldquo;Glu variants improve transformer.\u0026rdquo; arXiv preprint arXiv:2002.05202 (2020).\n[21] Ainslie, Joshua, et al. \u0026ldquo;Gqa: Training generalized multi-query transformer models from multi-head checkpoints.\u0026rdquo; arXiv preprint arXiv:2305.13245 (2023).\n[22] Su, Jianlin, et al. \u0026ldquo;Roformer: Enhanced transformer with rotary position embedding.\u0026rdquo; Neurocomputing 568 (2024): 127063.\nCitation Citation: When reproducing or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Apr 2025). The LLaMA Herd. https://syhya.github.io/posts/2025-04-06-llama\nOr\n@article{syhya2025llama, title = \u0026#34;The LLaMA Herd\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-04-06-llama\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-04-06-llama/","summary":"\u003ch2 id=\"llama\"\u003eLLaMA\u003c/h2\u003e\n\u003cp\u003eThe LLaMA series of open-source models released by Meta AI has become one of the cornerstones of the large language model community, profoundly impacting the advancement of open research and applications. From the pioneering LLaMA released in early 2023, to the significantly improved LLaMA 2 later that year, to derivative models targeting specific domains (like code, safety), and the subsequent new generations LLaMA 3 and LLaMA 4 launched in 2024 and 2025 respectively, Meta has continuously committed to enhancing the performance of open-source models, gradually bringing them closer to state-of-the-art closed-source models. Below, we will introduce the key technical details of each major model in sequence.\u003c/p\u003e","title":"The LLaMA Herd"},{"content":"Agents Since the release of ChatGPT by OpenAI in October 2022, and with the emergence of subsequent projects like AutoGPT and AgentGPT, LLM-related agents have become a research hotspot and a practical application direction in AI in recent years. This article will introduce the basic concepts, core technologies, and latest application progress of agents.\nLLM Agent A Large Language Model Agent (LLM agent) utilizes an LLM as its core \u0026ldquo;brain\u0026rdquo; and combines modules like planning, memory, and external tools to automate the execution of complex tasks.\nUser Request: The user interacts with the agent by inputting tasks through prompts. Agent: The system\u0026rsquo;s brain, composed of one or more LLMs, responsible for overall coordination and task execution. Planning: Decomposes complex tasks into smaller sub-tasks, formulates an execution plan, and continuously optimizes the results through reflection. Memory: Includes short-term memory (capturing task information in real-time using in-context learning) and long-term memory (using external vector stores to save and retrieve key information, ensuring information continuity for long-running tasks). Tools: Integrates external tools such as calculators, web search, and code interpreters to call external data, execute code, and obtain the latest information. Fig. 1. The illustration of LLM Agent Framework. (Image source: DAIR.AI, 2024)\nRL Agent The goal of Reinforcement Learning (RL) is to train an agent to take a series of actions ($a_t$) in a given environment. During interaction, the agent transitions from one state ($s_t$) to the next and receives a reward ($r_t$) from the environment after each action. This interaction generates a complete trajectory ($\\tau$), typically represented as:\n$$ \\tau = \\{(s_0, a_0, r_0), (s_1, a_1, r_1), \\dots, (s_T, a_T, r_T)\\}. $$The agent\u0026rsquo;s objective is to learn a policy ($\\pi$), which is a rule for selecting actions in each state, to maximize the expected cumulative reward, usually expressed as:\n$$ \\max_{\\pi} \\, \\mathbb{E}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right], $$ where $\\gamma \\in [0,1]$ is the discount factor, used to balance short-term and long-term rewards.\nFig. 2. The agent-environment interaction. (Image source: Sutton \u0026amp; Barto, 2018)\nIn the context of LLMs, the model can be viewed as an agent, and the \u0026ldquo;environment\u0026rdquo; can be understood as the user\u0026rsquo;s input and the desired response format:\nState ($s_t$): Can be the current dialogue context or the user\u0026rsquo;s question. Action ($a_t$): The text output by the model (answer, generated content, etc.). Reward ($r_t$): Feedback from the user or the system (e.g., user satisfaction, automatic scoring by a reward model). Trajectory ($\\tau$): The entire sequence of text interactions from the beginning to the end of a conversation, which can be used to evaluate the model\u0026rsquo;s overall performance. Policy ($\\pi$): The rule by which the LLM generates text in each state (dialogue context), typically determined by the model\u0026rsquo;s parameters. For LLMs, they are traditionally pre-trained on massive offline datasets. In the post-training reinforcement learning phase, the model is trained with feedback from humans or other models to produce high-quality text that better aligns with human preferences or task requirements.\nComparison The table below shows the differences between the two:\nComparison Dimension LLM Agent RL Agent Core Principle Automates complex tasks through planning, memory, and tools. Continuously optimizes its policy to maximize long-term rewards through a trial-and-error feedback loop with the environment. Optimization Method Does not directly update model parameters. Performance is improved mainly through context extension, external memory, and tools. Continuously and frequently updates policy model parameters, relying on reward signals from the environment for optimization. Interaction Method Interacts with users or external systems using natural language, flexibly calling various tools to obtain external information. Interacts with a real or simulated environment, which provides rewards or penalties, forming a closed feedback loop. Objective Decomposes complex tasks and completes them with the help of external resources, focusing on the quality and accuracy of the task outcome. Maximizes long-term rewards, seeking an optimal balance between short-term and long-term returns. As research progresses, the combination of LLM and RL agents presents more possibilities, such as:\nUsing reinforcement learning methods to train Reasoning LLMs (like o1/o3), making them more suitable as base models for LLM agents. Simultaneously, recording the data and feedback from LLM agents executing tasks to provide rich training data for Reasoning LLMs, thereby enhancing model performance. Prompt Engineering Prompt Engineering, also known as In-Context Prompting, is the technique of optimizing input prompts to guide an LLM to produce the desired output. Its core objective is to control the model\u0026rsquo;s behavior through effective communication without updating the model\u0026rsquo;s weights.\nZero-Shot Prompting Zero-Shot Prompting directly provides the model with task instructions without any examples. This method relies entirely on the knowledge and instruction-following capabilities the model learned during its pre-training phase. For example, for sentiment analysis:\nFig. 3. Zero-Shot Prompting.\nFor models that have undergone instruction fine-tuning, such as GPT-5 or Claude 4, they can understand and execute these direct instructions very well.\nFew-Shot Prompting Few-Shot Prompting provides a set of high-quality examples in the prompt, with each example containing an input and the desired output. Through these examples, the model can better understand the user\u0026rsquo;s intent and the specific requirements of the task, leading to better performance than zero-shot prompting. However, a drawback of this method is that it consumes more of the context window length. For example, providing a few sentiment analysis examples:\nFig. 4. Few-Shot Prompting.\nAutomatic Prompt Construction Automatic Prompt Engineer (APE) (Zhou et al. 2022) is a method that searches through a pool of candidate instructions generated by the model. It filters the candidate set and ultimately selects the highest-scoring instruction based on a chosen scoring function.\nFig. 5. Automatic Prompt Engineer (APE) workflow. (Image source: Zhou et al. 2022)\nAutomatic Chain-of-Thought (Auto-CoT) (Zhang et al. 2022) proposes a method for automatically constructing Chain-of-Thought examples, aiming to solve the problem of manual prompt design being time-consuming and potentially suboptimal. Its core idea is to sample questions through clustering techniques and then leverage the LLM\u0026rsquo;s own zero-shot reasoning capabilities to automatically generate reasoning chains, thereby constructing diverse, high-quality examples.\nFig. 6. Overview of the Auto-CoT method. (Image source: Zhang et al. 2022)\nAuto-CoT consists of two main stages:\nQuestion Clustering: Embeds the questions in the dataset and runs an algorithm like $k$-means for clustering. This step aims to group similar questions into the same cluster to ensure the diversity of subsequently sampled questions. Demonstration Selection \u0026amp; Rationale Generation: Selects one or more representative questions from each cluster (e.g., choosing the question closest to the cluster centroid). Then, it uses a Zero-Shot CoT prompt to have the LLM generate reasoning chains for these selected questions. These automatically generated \u0026ldquo;question-rationale\u0026rdquo; pairs form the final few-shot prompt used for task execution. Knowledge Enhancement When dealing with knowledge-intensive or commonsense reasoning tasks, relying solely on the LLM\u0026rsquo;s parametric knowledge is often insufficient and can lead to incorrect or outdated answers. To address this issue, researchers have proposed two main approaches:\nGenerated Knowledge Prompting (Liu et al. 2022) is a method where the model is first prompted to generate relevant knowledge before making a prediction. The core idea is that when a task requires commonsense or external information, the model may make mistakes due to a lack of context. If the model is first guided to generate knowledge related to the input and then answers based on that knowledge, the accuracy of its reasoning can be improved.\nFig. 7. Overview of the Generated Knowledge Prompting. (Image source: Liu et al. 2022)\nKnowledge Generation: Based on the input, the model first generates relevant factual knowledge. Knowledge Integration: The generated knowledge is combined with the original question to form a new prompt. Answer Prediction: The model provides an answer based on the enhanced input. Retrieval Augmented Generation (RAG) (Lewis et al. 2021) is a method that combines information retrieval with text generation to tackle knowledge-intensive tasks. Its core idea is that relying solely on an LLM\u0026rsquo;s parametric (static) knowledge can easily lead to factual errors. By introducing retrieval from external knowledge bases, the factual consistency and timeliness of the generated results can be improved.\nFig. 8. Overview of the Retrieval Augmented Generation. (Image source: Lewis et al. 2021)\nRetrieval: Retrieves relevant documents from an external knowledge source (e.g., Wikipedia or a private knowledge base). Augmentation: Concatenates the retrieved documents with the original input to serve as the prompt context. Generation: A generation model (the original paper used a pre-trained seq2seq model, but today LLMs are mainstream) outputs the answer based on the augmented prompt. Multimodal Chain-of-Thought Prompting Multimodal CoT Prompting (MCoT) (Zhang et al. 2023) integrates text and visual information into the reasoning process, breaking the limitation of traditional CoT, which relies solely on the language modality. Its framework is divided into two stages:\nRationale Generation: Generates an explanatory reasoning chain based on multimodal information (text + image). Answer Inference: Uses the generated rationale as an aid to infer the final answer. Fig. 9. Overview of our Multimodal-CoT framework. (Image source: Zhang et al. 2023)\nActive Prompt Active Prompt (Diao et al. 2023) addresses the limitation of traditional CoT methods that rely on a fixed set of manually annotated examples. The problem is that fixed examples are not necessarily optimal for all tasks, which can lead to poor generalization. Active Prompt introduces an active learning strategy to adaptively select and update the best task-relevant examples, thereby improving the model\u0026rsquo;s reasoning performance.\nFig. 10. Illustrations of active prompting framework. (Image source: Diao et al. 2023)\nUncertainty Estimation: With or without a small number of manual CoT examples, the LLM generates k answers for training questions (in the paper, k=5), and an uncertainty metric is calculated based on the variance of these answers. Selection: Based on the uncertainty level, the most uncertain questions are selected. Annotation: The selected questions are manually annotated to provide new, high-quality CoT examples. Inference: The newly annotated examples are used for inference, improving the model\u0026rsquo;s performance on the target task. Planning: Task Decomposition The core components of an LLM Agent include planning, memory, and tool use. These components work together to enable the agent to autonomously execute complex tasks.\nFig. 11. Overview of a LLM-powered autonomous agent system. (Image source: Weng, 2017)\nPlanning is crucial for the successful execution of complex tasks. It can be approached in different ways depending on the complexity and the need for iterative refinement. In simple scenarios, the planning module can use an LLM to outline a detailed plan in advance, including all necessary sub-tasks. This step ensures that the agent systematically performs task decomposition and follows a clear logical flow from the outset.\nChain of Thought Chain of Thought (CoT) (Wei et al. 2022) works by generating a series of short sentences step-by-step to describe the reasoning process; these sentences are called reasoning steps. Its purpose is to explicitly show the model\u0026rsquo;s reasoning path, helping it better handle complex reasoning tasks. The figure below shows the difference between few-shot prompting (left) and CoT prompting (right). The few-shot prompt leads to an incorrect answer, while the CoT method guides the model to state its reasoning process step-by-step, more clearly reflecting the model\u0026rsquo;s logical process and thus improving the answer\u0026rsquo;s accuracy and interpretability.\nFig. 12. The comparison example of few-shot prompting and CoT prompting. (Image source: Wei et al. 2022)\nZero-Shot CoT (Kojima et al. 2022) is a follow-up to CoT that proposes an extremely simple zero-shot prompting method. They found that by simply appending the phrase Let's think step by step to the end of the question, the LLM can produce a chain of thought, leading to more accurate answers.\nFig. 13. The comparison example of few-shot prompting and CoT prompting. (Image source: Kojima et al. 2022)\nSelf-Consistency Sampling Self-consistency sampling (Wang et al. 2022a) generates multiple diverse answers by sampling multiple times from the same prompt with a temperature \u0026gt; 0 and then selecting the best answer from the set. The core idea is to improve the final answer\u0026rsquo;s accuracy and robustness by sampling multiple reasoning paths and then taking a majority vote. The criteria for selecting the best answer can vary for different tasks, but majority voting is generally used as a universal solution. For tasks that are easy to verify, such as programming problems, the answers can be validated by running them through an interpreter and using unit tests. This is an optimization of CoT, and when used in combination, it can significantly improve the model\u0026rsquo;s performance on complex reasoning tasks.\nFig. 14. Overview of the Self-Consistency Method for Chain-of-Thought Reasoning. (Image source: Wang et al. 2022a)\nHere are some subsequent optimization works:\n(Wang et al. 2022b) later used another ensemble learning method for optimization, increasing randomness by changing the order of examples or replacing human-written reasoning with model-generated ones, followed by majority voting. Fig. 15. An overview of different ways of composing rationale-augmented ensembles, depending on how the randomness of rationales is introduced. (Image source: Wang et al. 2022b)\nIf training samples only provide the correct answer without the reasoning, the STaR (Self-Taught Reasoner) (Zelikman et al. 2022) method can be used: (1) Have the LLM generate reasoning chains, and keep only the reasoning for answers that are correct. (2) Fine-tune the model with the generated reasoning, iterating until convergence. Note that a high temperature can easily lead to results with the correct answer but incorrect reasoning. If there is no ground truth, majority voting can be considered the \u0026ldquo;correct answer.\u0026rdquo; Fig. 16. An overview of STaR and a STaR-generated rationale on CommonsenseQA. (Image source: Zelikman et al. 2022)\n(Fu et al. 2023) found that more complex examples (with more reasoning steps) can improve model performance. When separating reasoning steps, a newline character \\n works better than step i, ., or ;. Additionally, using a complexity-based consistency strategy, which only performs majority voting on the top-$k$ most complex generated reasoning chains, can further optimize the model\u0026rsquo;s output. It was also shown that replacing Q: with Question: in the prompt provides an additional performance boost. Fig. 17. Sensitivity analysis on step formatting. Complex prompts consistently lead to better performance with regard to different step formatting. (Image source: Fu et al. 2023)\nTree of Thoughts Tree of Thoughts (ToT) (Yao et al. 2023) expands on CoT by exploring multiple reasoning possibilities at each step. It first decomposes a problem into multiple thought steps and generates several different ideas at each step, forming a tree structure. The search process can use Breadth-First Search (BFS) or Depth-First Search (DFS), and each state is evaluated by a classifier (or by having the LLM score it) or through majority voting. It involves three main steps:\nExpand: Generate one or more candidate solutions. Score: Measure the quality of the candidate solutions. Prune: Keep the top-$k$ best candidate solutions. If no solution is found (or if the quality of the candidates is not high enough), the process backtracks to the expansion step.\nFig. 18. Schematic illustrating various approaches to problem solving with LLMs. (Image source: Yao et al. 2023)\nPlanning: Self-Reflexion Self-Reflexion is a key factor that enables an agent to achieve iterative improvement by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\nReAct The ReAct (Reason + Act) (Yao et al. 2023) framework achieves seamless integration of reasoning and acting in LLMs by combining task-specific discrete actions with the language space. This design not only allows the model to interact with the environment by calling external interfaces like the Wikipedia search API but also to generate detailed reasoning trajectories in natural language to solve complex problems.\nThe ReAct prompt template includes explicit thought steps, with the basic format as follows:\nThought: ... Action: ... Observation: ... ...(Repeated many times) Fig. 19. Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023)\nAs seen in the figure below, ReAct significantly outperforms the baseline Action-only method in both knowledge-intensive and decision-making tasks, demonstrating its advantages in enhancing reasoning effectiveness and interactive performance.\nFig. 20. PaLM-540B prompting results on HotpotQA and Fever. (Image source: Yao et al. 2023)\nReflexion Reflexion (Shinn et al. 2023) enables an LLM to iteratively improve and optimize its decisions through self-feedback and dynamic memory.\nThis method essentially borrows ideas from reinforcement learning. In the traditional Actor-Critic model, the Actor selects an action $a_t$ based on the current state $s_t$, while the Critic provides an evaluation (e.g., a value function $V(s_t)$ or an action-value function $Q(s_t, a_t)$) and gives feedback to the Actor for policy optimization. Correspondingly, in Reflexion\u0026rsquo;s three main components:\nActor: Played by the LLM, it outputs text and corresponding actions based on the environment\u0026rsquo;s state (including context and historical information). This can be denoted as:\n$$ a_t = \\pi_\\theta(s_t), $$where $\\pi_\\theta$ represents the policy based on parameters $\\theta$ (i.e., the LLM\u0026rsquo;s weights or prompt). The Actor interacts with the environment and produces a trajectory $\\tau = {(s_1,a_1,r_1), \\dots, (s_T,a_T,r_T)}$.\nEvaluator: Similar to the Critic, the Evaluator receives the trajectory generated by the Actor and outputs a reward signal $r_t$. In the Reflexion framework, the Evaluator can analyze the trajectory using pre-designed heuristic rules or an additional LLM to generate rewards. For example:\n$$ r_t = R(\\tau_t), $$where $R(\\cdot)$ is the reward function based on the current trajectory $\\tau_t$.\nSelf-Reflection: This module adds a self-regulating feedback mechanism on top of the Actor-Critic model. It integrates the current trajectory $\\tau$, reward signals ${r_t}$, and historical experience from long-term memory, using its language generation capabilities to produce self-improvement suggestions for the next decision. This feedback is then written to external memory, providing richer context for the Actor\u0026rsquo;s subsequent decisions, thus achieving iterative optimization similar to updating policy parameters $\\theta$ through dynamic adjustment of the prompt, without updating the LLM\u0026rsquo;s internal parameters.\nFig. 21. (a) Diagram of Reflexion. (b) Reflexion reinforcement algorithm. (Image source: Shinn et al. 2023)\nThe core loop and algorithm of Reflexion are described as follows:\nInitialization\nInstantiate the Actor, Evaluator, and Self-Reflection models simultaneously (all can be implemented by LLMs), denoted as $M_a, M_e, M_{sr}$ respectively. Initialize the policy $\\pi_\\theta$ (including the Actor\u0026rsquo;s model parameters or prompt, and initial memory). The Actor first generates an initial trajectory $\\tau_0$ according to the current policy $\\pi_\\theta$. After evaluation by $M_e$, $M_{sr}$ generates the first self-reflection text and stores it in long-term memory. Generate Trajectory\nIn each iteration, $M_a$ reads the current long-term memory and environmental observations, sequentially outputting actions ${a_1, a_2, \\ldots}$, interacting with the environment, and receiving corresponding feedback to form a new trajectory $\\tau_t$. $\\tau_t$ can be considered the short-term memory for this task, used only in the current iteration. Evaluation\n$M_e$ outputs rewards or scores ${r_1, r_2, \\ldots}$ based on the trajectory $\\tau_t$ (i.e., the sequence of the Actor\u0026rsquo;s behaviors and environmental feedback). This step corresponds to internal feedback from $M_e$ or results directly provided by the external environment. Self-Reflection\nThe $M_{sr}$ module synthesizes the trajectory $\\tau_t$ and reward signals ${r_t}$ to generate self-correction or improvement suggestions $\\mathrm{sr}_t$ at the language level. The reflection text can be seen as an analysis of errors or as providing new inspirational ideas, and it is stored in long-term memory. In practice, we can vectorize the feedback information and store it in a vector database. Update and Repeat\nAfter appending the latest self-reflection text $\\mathrm{sr}_t$ to long-term memory, the Actor can use RAG to retrieve relevant historical information in the next iteration to adjust its policy. Repeat the above steps until $M_e$ determines the task is completed or the maximum number of rounds is reached. In this loop, Reflexion relies on the continuous accumulation of self-reflection + long-term memory to improve decision-making, rather than directly modifying model parameters. Below are examples of Reflexion applied to decision-making, programming, and reasoning tasks:\nFig. 22. Reflexion works on decision-making, programming, and reasoning tasks. (Image source: Shinn et al. 2023)\nIn an experiment with 100 HotPotQA questions, a comparison between the CoT method and a method with episodic memory showed that the Reflexion method, with an added self-reflection step at the end, significantly improved its search, information retrieval, and reasoning capabilities.\nFig. 23. Comparative Analysis of Chain-of-Thought (CoT) and ReAct on the HotPotQA Benchmark. (Image source: Shinn et al. 2023)\nDeepSeek R1 DeepSeek-R1 (DeepSeek-AI, 2025) represents a major breakthrough in the open-source community\u0026rsquo;s efforts to replicate OpenAI\u0026rsquo;s o1 (OpenAI, 2024), successfully training an advanced reasoning model with deep reflection capabilities through reinforcement learning techniques.\nFor a detailed training process and technical implementation of DeepSeek R1, please refer to my previous blog post: Progress in Replicating OpenAI o1: DeepSeek-R1.\nA key transformation during the training of DeepSeek-R1-Zero is that as training progresses, the model gradually emerges with an outstanding self-evolution capability. This capability is manifested in three core aspects:\nSelf-Reflection: The model can look back and critically evaluate previous reasoning steps. Proactive Exploration: When it finds the current problem-solving path to be suboptimal, it can autonomously search for and try alternative solutions. Dynamic Thought Adjustment: It adaptively adjusts the number of generated tokens based on the complexity of the problem, achieving a deeper thought process. This dynamic and spontaneous reasoning behavior significantly enhances the model\u0026rsquo;s ability to solve complex problems, enabling it to tackle challenging tasks more efficiently and accurately.\nFig. 24. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)\nA typical \u0026ldquo;aha moment\u0026rdquo; also emerged during the training of DeepSeek-R1-Zero. At this critical stage, the model suddenly realized during the reasoning process that its previous line of thought was flawed, and it quickly adjusted its thinking direction, ultimately leading to the correct answer. This phenomenon strongly demonstrates that the model has developed powerful self-correction and reflection capabilities in its reasoning process, similar to the \u0026ldquo;aha\u0026rdquo; experience in human thinking.\nFig. 25. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: DeepSeek-AI, 2025)\nMemory Human Memory Memory refers to the process of acquiring, storing, retaining, and retrieving information. Human memory is primarily divided into the following three categories:\nFig. 26. Categorization of human memory. (Image source: Weng, 2017)\nSensory Memory: Used to briefly retain sensory information after the original stimulus (visual, auditory, tactile, etc.) has disappeared, typically lasting for milliseconds or seconds. Sensory memory is further divided into:\nIconic Memory: The transient image or visual impression retained by the visual channel, generally lasting 0.25–0.5 seconds, used to form visual continuity in video or animation scenes. Echoic Memory: The brief storage of auditory information, which can last for several seconds, allowing a person to replay recently heard sentences or sound clips. Haptic Memory: Used to retain brief tactile or force information, generally lasting from milliseconds to seconds, such as the brief finger sensations when typing on a keyboard or reading Braille. Short-Term Memory: Stores the information we are currently conscious of.\nLasts for about 20–30 seconds, with a capacity typically of 7±2 items. Responsible for the temporary processing and maintenance of information during complex cognitive tasks like learning and reasoning. Long-Term Memory: Can store information for days to decades, with a virtually unlimited capacity. Long-term memory is divided into:\nExplicit Memory: Can be consciously recalled, including episodic memory (personal experiences, event details) and semantic memory (facts and concepts). Implicit Memory: Unconscious memory, primarily related to skills and habits, such as riding a bike or touch typing. These three types of human memory are intertwined and together form our cognition and understanding of the world. When building LLM Agents, we can draw inspiration from this classification of human memory:\nSensory Memory corresponds to the LLM\u0026rsquo;s embedding representations of raw input data (such as text, images, and videos). Short-Term Memory corresponds to the LLM\u0026rsquo;s in-context learning, limited by the model\u0026rsquo;s context window max_tokens. When the conversation length exceeds the window, earlier information is truncated. Long-Term Memory corresponds to an external vector store or database, where the Agent can retrieve historical information on demand using RAG technology. LLM Agent Memory When an Agent engages in multi-turn interactions with a user or executes multi-step tasks, it can utilize different forms of memory and environmental information to complete its workflow.\nFig. 27. An overview of the sources, forms, and operations of the memory in LLM-based agents. (Image source: Zhang et al. 2024)\nTextual Memory\nFull Interaction: Records all dialogue and action trajectories, helping the Agent trace back the context. Recent Interaction: Retains only the dialogue content highly relevant to the current task, reducing unnecessary context usage. Retrieved Interaction: The Agent can retrieve historical dialogues or records related to the current task from an external knowledge base and integrate them into the current context. External Knowledge: When the Agent encounters a knowledge gap, it can retrieve and acquire additional information through APIs or external storage. Parametric Memory\nFine-tuning: Expands the model\u0026rsquo;s internal knowledge by injecting new information or knowledge into the LLM. Knowledge Editing: Modifies or updates existing knowledge at the model level, achieving dynamic adjustment of the model\u0026rsquo;s internal parametric memory. Environment\nRepresents the entities and context involved when the Agent interacts with users and external systems, such as the user Alice, or accessible tools or interfaces (e.g., a ticket booking system, a streaming platform). Agent\nThe LLM Agent is responsible for read and write operations, i.e., reading information from the external environment or knowledge base and writing new actions or content. It also includes a series of management functions, such as merging, reflecting, and forgetting, to dynamically maintain short-term and long-term memory. Another example is an Agent completing two different but related tasks, requiring the use of both short-term and long-term memory:\nTask A: Play a video: The Agent records the current plan, actions, and environmental state (e.g., searching, clicking, playing the video) in its short-term memory. This information is stored in memory and the LLM\u0026rsquo;s context window. Task B: Download a game: The Agent utilizes its long-term memory related to Arcane and League of Legends to quickly find a way to download the game. The figure shows the Agent searching on Google; we can consider Google\u0026rsquo;s knowledge base as an external knowledge source. All new search, click, and download actions are also updated in the short-term memory. Fig. 28: Illustration of short-term memory and long-term memory in an LLM-brained GUI agent. (Image source: Zhang et al. 2024)\nCommon memory elements and their corresponding storage methods can be summarized in the following table:\nMemory Element Memory Type Description Storage Medium / Method Action Short-Term Memory Historical action trajectory (e.g., clicking buttons, entering text) Memory, LLM Context Window Plan Short-Term Memory The plan for the next operation generated in the previous or current step Memory, LLM Context Window Execution Result Short-Term Memory The result returned after an action, error messages, and environmental feedback Memory, LLM Context Window Environment State Short-Term Memory Available buttons, page titles, system status, etc., in the current UI environment Memory, LLM Context Window Self-Experience Long-Term Memory Historical task trajectories and execution steps Database, Disk Self-Guidance Long-Term Memory Guiding rules and best practices summarized from historical successful trajectories Database, Disk External Knowledge Long-Term Memory External knowledge bases, documents, or other data sources to assist in task completion External Database, Vector Retrieval Task Success Metrics Long-Term Memory Records of task success rates, failure rates, etc., for improvement and analysis Database, Disk Furthermore, researchers have proposed new training and storage methods to enhance the memory capabilities of LLMs:\nLongMem (Language Models Augmented with Long-Term Memory) (Wang, et al. 2023) enables LLMs to remember long historical information. It adopts a decoupled network architecture, freezing the original LLM parameters as a memory encoder, while using an Adaptive Residual Side-Network (SideNet) as a memory retriever for memory checking and reading.\nFig. 29. Overview of the memory caching and retrieval flow of LongMem. (Image source: Wang, et al. 2023)\nIt mainly consists of three parts: Frozen LLM, Residual SideNet, and Cached Memory Bank. Its workflow is as follows:\nFirst, a long text sequence is split into fixed-length segments. Each segment is encoded layer by layer in the Frozen LLM, and at the $m$-th layer, the attention\u0026rsquo;s $K, V \\in \\mathbb{R}^{H \\times M \\times d}$ vector pairs are extracted and cached in the Cached Memory Bank. When a new input sequence is encountered, the model retrieves the top-$k$ most relevant key-value pairs from the long-term memory bank based on the current input\u0026rsquo;s query-key. These are then integrated into the subsequent language generation process. Meanwhile, the memory bank removes the oldest content to ensure the availability of the latest contextual information. The Residual SideNet fuses the hidden layer outputs of the frozen LLM with the retrieved historical key-values during inference, enabling effective modeling and utilization of context from ultra-long texts. Through this decoupled design, LongMem can flexibly schedule massive amounts of historical information without expanding its native context window, balancing both speed and long-term memory capabilities.\nTool Use Tool use is an important component of LLM Agents. By empowering LLMs with the ability to call external tools, their capabilities are significantly expanded: they can not only generate natural language but also obtain real-time information, perform complex calculations, and interact with various systems (such as databases, APIs, etc.), effectively breaking through the limitations of their pre-trained knowledge and avoiding the inefficiency of reinventing the wheel.\nTraditional LLMs primarily rely on pre-trained data for text generation, which makes them deficient in areas like mathematical operations, data retrieval, and real-time information updates. Through tool calling, models can:\nEnhance computational ability: For example, by calling a specialized calculator tool like Wolfram, the model can perform more precise mathematical calculations, compensating for its own arithmetic shortcomings. Obtain real-time information: Using search engines like Google, Bing, or database APIs, the model can access the latest information, ensuring the timeliness and accuracy of the generated content. Increase information credibility: With the support of external tools, the model can cite real data sources, reducing the risk of fabricating information and improving overall credibility. Improve system transparency: Tracking API call records can help users understand the model\u0026rsquo;s decision-making process, providing a degree of interpretability. Currently, various LLM applications based on tool calling have emerged, utilizing different strategies and architectures to cover everything from simple tasks to complex multi-step reasoning.\nToolformer Toolformer (Schick, et al. 2023) is an LLM that can use external tools through simple APIs. It is trained by fine-tuning the GPT-J model, requiring only a few examples for each API. The tools Toolformer learns to call include a question-answering system, Wikipedia search, a calculator, a calendar, and a translation system:\nFig. 30. Examples of inputs and outputs for all APIs used. (Image source: Schick, et al. 2023)\nHuggingGPT HuggingGPT (Shen, et al. 2023) is a framework that uses ChatGPT as a task planner. It selects available models from HuggingFace by reading their descriptions to complete user tasks and summarizes the results based on their execution.\nFig. 31. Illustration of how HuggingGPT works. (Image source: Shen, et al. 2023)\nThe system consists of the following four stages:\nTask Planning: Parses the user\u0026rsquo;s request into multiple sub-tasks. Each task has four attributes: task type, ID, dependencies, and parameters. The paper uses few-shot prompting to guide the model in task decomposition and planning. Model Selection: Assigns each sub-task to different expert models, using a multiple-choice format to determine the most suitable model. Due to the limited context length, models need to be initially filtered based on the task type. Task Execution: The expert models execute their assigned specific tasks and record the results, which are then passed to the LLM for further processing. Response Generation: Receives the execution results from each expert model and finally outputs a summary answer to the user. LLM Agent Applications Generative Agent The Generative Agent (Park, et al. 2023) experiment simulates realistic human behavior with 25 virtual characters driven by large language models in a sandbox environment. Its core design integrates memory, retrieval, reflection, and planning/reaction mechanisms, allowing agents to record and review their experiences and extract key information to guide future actions and interactions.\nFig. 32. The screenshot of generative agent sandbox. (Image source: Park, et al. 2023)\nThe entire system uses a long-term memory module to record all observed events, a retrieval model to extract information based on recency, importance, and relevance, and a reflection mechanism to generate high-level inferences, ultimately translating these outcomes into concrete actions. This simulation demonstrates emergent behaviors such as information diffusion, relationship memory, and social event coordination, providing a realistic simulation of human behavior for interactive applications.\nFig. 25. The generative agent architecture. (Park, et al. 2023)\nWebVoyager WebVoyager (He et al. 2024) is an autonomous web interaction agent based on large multimodal models, capable of controlling the mouse and keyboard for web browsing. WebVoyager uses the classic ReAct loop. In each interaction step, it views a browser screenshot annotated with a method similar to SoM (Set-of-Marks) (Yang, et al. 2023), which provides interaction cues by placing numerical labels on web elements, and then decides on the next action. This combination of visual annotation and the ReAct loop allows users to interact with web pages using natural language. For a concrete example, you can refer to the WebVoyager code using the LangGraph framework.\nFig. 33. The overall workflow of WebVoyager. (Image source: He et al. 2024)\nOpenAI Operator Operator (OpenAI, 2025) is an AI agent recently released by OpenAI, designed to autonomously execute web tasks. Operator can interact with web pages like a human user, completing specified tasks by typing, clicking, and scrolling. The core technology behind Operator is the Computer-Using Agent (CUA) (OpenAI, 2025). CUA combines the visual capabilities of GPT-4o with enhanced reasoning abilities gained through reinforcement learning, and it has been specially trained to interact with graphical user interfaces (GUIs), including buttons, menus, and text boxes that users see on the screen.\nFig. 34. Overview of OpenAI CUA. (Image source: OpenAI, 2025)\nCUA operates in an iterative loop consisting of three stages:\nPerception: CUA \u0026ldquo;observes\u0026rdquo; the web page content by capturing browser screenshots. This vision-based input allows it to understand the page\u0026rsquo;s layout and elements. Reasoning: Using a chain-of-thought reasoning process, CUA evaluates the next action based on the current and previous screenshots and the actions already taken. This reasoning ability enables it to track task progress, review intermediate steps, and make adjustments as needed. Action: CUA interacts with the browser by simulating mouse and keyboard operations (such as clicking, typing, and scrolling). This allows it to perform a wide range of web tasks without needing specific API integrations. The difference between CUA and the pre-existing WebVoyager is that CUA is an agent specifically trained with reinforcement learning, rather than a fixed-flow workflow built by directly calling GPT-4o. Although CUA is still in its early stages and has certain limitations, it has achieved state-of-the-art results on the following benchmarks.\nFig. 35. OpenAI CUA Benchmark Results. (Image source: OpenAI, 2025)\nDeep Research Deep Research is essentially a report generation system: given a user\u0026rsquo;s query, the system uses an LLM as its core agent to generate a structured and detailed report through multiple rounds of iterative information retrieval and analysis. Currently, the implementation logic of various Deep Research systems can be mainly divided into two approaches: Workflow Agent and RL Agent.\nWorkflow Agent vs RL Agent The Workflow Agent approach relies on developers to pre-design workflows and manually craft prompts to organize the entire report generation process. Its main features include:\nTask Decomposition and Flow Orchestration: The system breaks down the user\u0026rsquo;s query into several sub-tasks, such as generating an outline, information retrieval, and content summarization, and then executes them in a predetermined sequence. Fixed Process: The calls and interactions between different stages are pre-defined, similar to building a static flowchart or a directed acyclic graph (DAG), ensuring that each step has a clear responsibility. Dependence on Manual Design: This method heavily relies on the experience of engineers, who improve output quality through repeated prompt tuning. It is highly applicable but has limited flexibility. The LangGraph framework can be used to build and orchestrate workflows in the form of a graph.\nFig. 36. A workflow of the LangGraph. (Image source: LangGraph, 2025)\nThe following table compares 5 common workflow and agent patterns:\nPattern Core Mechanism Advantages Limitations Use Cases Prompt Chaining Sequentially calls LLMs, passing results step-by-step Suitable for phased reasoning, more accurate results Fixed process, high latency Document generation (outline → content), translation polishing Parallelization Splits sub-tasks for parallel processing, or multi-model voting Increases speed, more robust results Sub-tasks must be independent, high resource consumption Parallel content moderation, multi-model code detection Routing First classifies, then assigns to different models/processes Highly targeted, improves efficiency Effectiveness depends on classification accuracy Customer service query routing, dynamic model size selection Evaluator-Optimizer Generate → Evaluate → Optimize iteratively Improves result quality, suitable for tasks with standards High cost, multiple iterations increase latency Translation optimization, multi-round retrieval refinement Orchestrator-Worker Central orchestration, dynamically decomposes and schedules sub-tasks Flexible, can handle complex tasks Complex architecture, high scheduling cost Multi-file code modification, real-time research integration Agent LLM makes autonomous decisions, calls tools based on environmental feedback Highly flexible, adapts to dynamic environments Unpredictable, cost and security need control Autonomous research agents, interactive problem-solving Currently, there are several open-source projects on GitHub that have implemented workflow-based Deep Research Agents, such as GPT Researcher and open deep research.\nFig. 37. An overview of the open deep research. (Image source: LangChain, 2025)\nThe RL Agent is an alternative approach that uses RL to train a reasoning model to optimize the agent\u0026rsquo;s multi-round search, analysis, and report writing process. Its main features include:\nAutonomous Decision-Making Capability: The system is trained through reinforcement learning, allowing the agent to autonomously judge, make decisions, and adjust its strategy when facing complex search and content integration tasks, thereby generating reports more efficiently. Continuous Optimization: Using a reward mechanism to score and provide feedback on the generation process, the agent can continuously iterate and optimize its own policy, improving the overall quality from task decomposition to the final report. Reduced Manual Intervention: Compared to fixed processes that rely on manually crafted prompts, the reinforcement learning training approach reduces dependence on manual design, making it more suitable for handling variable and complex real-world application scenarios. The table below summarizes the main differences between these two approaches:\nFeature Workflow Agent RL Agent Process Design Pre-designed fixed workflow with clear task decomposition and flow orchestration End-to-end learning, with the agent making autonomous decisions and dynamically adjusting the process Autonomous Decision-Making Relies on manually designed prompts; the decision process is fixed and immutable Through reinforcement learning, the agent can autonomously judge, decide, and optimize its strategy Manual Intervention Requires extensive manual design and tuning of prompts; significant manual intervention Reduces manual intervention, achieving automatic feedback and continuous optimization through a reward mechanism Flexibility \u0026amp; Adaptability Weaker adaptability to complex or changing scenarios; limited extensibility Better suited for variable and complex real-world scenarios, with high flexibility Optimization Mechanism Optimization mainly relies on engineers\u0026rsquo; experience and adjustments; lacks an end-to-end feedback mechanism Utilizes reward feedback from reinforcement learning for continuous, automated performance improvement Implementation Difficulty Relatively straightforward to implement, but requires tedious process design and maintenance Requires training data and computational resources; higher initial development investment, but better long-term results Training Required No additional training needed; relies solely on manually constructed processes and prompts Requires training the agent through reinforcement learning to achieve autonomous decision-making OpenAI Deep Research OpenAI Deep Research (OpenAI, 2025) is an intelligent agent officially released by OpenAI in February 2025. Designed for complex scenarios, it can automatically search, filter, analyze, and integrate multi-source information to ultimately generate high-quality comprehensive reports. The system is built on o3 as its core base model and incorporates reinforcement learning methods, significantly improving the accuracy and robustness of its multi-round iterative search and reasoning processes.\nCompared to traditional ChatGPT plugin-based search or conventional RAG techniques, OpenAI Deep Research has the following outstanding advantages:\nReinforcement Learning-Driven Iterative Reasoning Leveraging the o3 reasoning model and reinforcement learning training strategies, the agent can continuously optimize its reasoning path during multi-round search and summarization, effectively reducing the risk of distortion caused by error accumulation.\nMulti-Source Information Integration and Cross-Validation Breaking the limitations of a single search engine, it can simultaneously call upon various authoritative data sources such as specific databases and professional knowledge bases, forming more reliable research conclusions through cross-validation.\nHigh-Quality Report Generation The training phase introduces an LLM-as-a-judge scoring mechanism and strict evaluation criteria, enabling the system to self-evaluate when outputting reports, thereby generating more clearly structured and rigorously argued professional texts.\nTraining Process The training process for OpenAI Deep Research utilized a browser interaction dataset specifically tailored for research scenarios. Through these datasets, the model mastered core browsing functions—including searching, clicking, scrolling, and file parsing—and also learned to use Python tools in a sandboxed environment for computation, data analysis, and visualization. Furthermore, with reinforcement learning training on these browsing tasks, the model can efficiently perform information retrieval, integration, and reasoning across a vast number of websites, quickly locating key information or generating comprehensive research reports.\nThese training datasets include both objective tasks with ground-truth answers that can be automatically scored, as well as open-ended tasks equipped with detailed scoring rubrics. During training, the model\u0026rsquo;s responses are rigorously compared against the ground truth or scoring criteria, and the model generates CoT thought processes to allow an evaluation model to provide feedback.\nAdditionally, the training process reused the safety datasets accumulated during the o1 model\u0026rsquo;s training phase and was specifically supplemented with safety training data for Deep Research scenarios, ensuring that the model strictly adheres to relevant compliance and safety requirements during automated search and browsing.\nPerformance The model achieved state-of-the-art results on the Humanity\u0026rsquo;s Last Exam benchmark (Phan, et al. 2025), which evaluates AI\u0026rsquo;s ability to answer expert-level questions across various professional domains.\nFig. 38. Humanity\u0026rsquo;s Last Exam Benchmark Results. (Image source: OpenAI, 2025)\nFuture Directions Intelligent agents show vast promise, but to achieve reliable and widespread application, the following key challenges still need to be addressed:\nContext Window Limitations: The limited context window of LLMs restricts the amount of information they can process, affecting long-term planning and memory capabilities and reducing task coherence. Current research is exploring external memory mechanisms and context compression techniques to enhance long-term memory and complex information processing abilities. Currently, OpenAI\u0026rsquo;s latest model, GPT-4.5 (OpenAI, 2025), has a maximum context window of 128k tokens.\nInterface Standardization and Interoperability: The current natural language-based interaction with tools suffers from inconsistent formatting. The Model Context Protocol (MCP) (Anthropic, 2024) aims to unify the interaction between LLMs and applications through an open standard, reducing development complexity and improving system stability and cross-platform compatibility.\nTask Planning and Decomposition Capabilities: Agents struggle to formulate coherent plans for complex tasks, effectively decompose sub-tasks, and lack the ability to dynamically adjust in unexpected situations. More powerful planning algorithms, self-reflection mechanisms, and dynamic policy adjustment methods are needed to flexibly respond to uncertain environments.\nComputational Resources and Economic Viability: Deploying large model agents is costly due to multiple API calls and intensive computation, limiting their use in some practical scenarios. Optimization directions include more efficient model architectures, quantization techniques, inference optimization, caching strategies, and intelligent scheduling mechanisms. With the development of specialized GPU hardware like the NVIDIA DGX B200 and distributed technologies, computational efficiency is expected to improve significantly.\nSecurity and Privacy Protection: Agents face security risks such as prompt injection and need robust authentication, permission control, input validation, and sandboxed environments. For multimodal inputs and external tools, data anonymization, the principle of least privilege, and audit logs must be strengthened to meet security and privacy compliance requirements.\nDecision Transparency and Explainability: The difficulty in explaining agent decisions limits their application in high-stakes domains. Enhancing explainability requires the development of visualization tools, chain-of-thought tracking, and decision rationale generation mechanisms to improve decision transparency, build user trust, and meet regulatory requirements.\nReferences [1] DAIR.AI. \u0026ldquo;LLM Agents.\u0026rdquo; Prompt Engineering Guide, 2024.\n[2] Sutton, Richard S., and Andrew G. Barto. \u0026ldquo;Reinforcement Learning: An Introduction.\u0026rdquo; MIT Press, 2018.\n[3] Weng, Lilian. \u0026ldquo;LLM-powered Autonomous Agents.\u0026rdquo; Lil’Log, 2023.\n[4] Zhou, Yongchao, et al. \u0026ldquo;Large language models are human-level prompt engineers.\u0026rdquo; The eleventh international conference on learning representations. 2022.\n[5] Zhang, Zhuosheng, et al. \u0026ldquo;Automatic chain of thought prompting in large language models.\u0026rdquo; arXiv preprint arXiv:2210.03493 (2022).\n[6] Liu, Jiacheng, et al. \u0026ldquo;Generated knowledge prompting for commonsense reasoning.\u0026rdquo; arXiv preprint arXiv:2110.08387 (2021).\n[7] Lewis, Patrick, et al. \u0026ldquo;Retrieval-augmented generation for knowledge-intensive nlp tasks.\u0026rdquo; Advances in neural information processing systems 33 (2020): 9459-9474.\n[8] Zhang, Zhuosheng, et al. \u0026ldquo;Multimodal chain-of-thought reasoning in language models.\u0026rdquo; arXiv preprint arXiv:2302.00923 (2023).\n[9] Diao, Shizhe, et al. \u0026ldquo;Active prompting with chain-of-thought for large language models.\u0026rdquo; arXiv preprint arXiv:2302.12246 (2023).\n[10] Wei, Jason, et al. \u0026ldquo;Chain-of-thought prompting elicits reasoning in large language models.\u0026rdquo; Advances in neural information processing systems 35 (2022): 24824-24837.\n[11] Kojima, Takeshi, et al. \u0026ldquo;Large language models are zero-shot reasoners.\u0026rdquo; Advances in neural information processing systems 35 (2022): 22199-22213.\n[12] Wang, Xuezhi, et al. \u0026ldquo;Self-consistency improves chain of thought reasoning in language models.\u0026rdquo; arXiv preprint arXiv:2203.11171 (2022).\n[13] Wang, Xuezhi, et al. \u0026ldquo;Rationale-augmented ensembles in language models.\u0026rdquo; arXiv preprint arXiv:2207.00747 (2022).\n[14] Zelikman, Eric, et al. \u0026ldquo;Star: Bootstrapping reasoning with reasoning.\u0026rdquo; Advances in Neural Information Processing Systems 35 (2022): 15476-15488.\n[15] Fu, Yao, et al. \u0026ldquo;Complexity-based prompting for multi-step reasoning.\u0026rdquo; arXiv preprint arXiv:2210.00720 (2022).\n[16] Yao, Shunyu, et al. \u0026ldquo;Tree of thoughts: Deliberate problem solving with large language models.\u0026rdquo; Advances in neural information processing systems 36 (2023): 11809-11822.\n[17] Yao, Shunyu, et al. \u0026ldquo;React: Synergizing reasoning and acting in language models.\u0026rdquo; International Conference on Learning Representations (ICLR). 2023.\n[18] Shinn, Noah, et al. \u0026ldquo;Reflexion: Language agents with verbal reinforcement learning.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 8634-8652.\n[19] Guo, Daya, et al. \u0026ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\u0026rdquo; arXiv preprint arXiv:2501.12948 (2025).\n[20] OpenAI. \u0026ldquo;Introducing OpenAI o1\u0026rdquo; OpenAI, 2024.\n[21] Zhang, Zeyu, et al. \u0026ldquo;A survey on the memory mechanism of large language model based agents.\u0026rdquo; arXiv preprint arXiv:2404.13501 (2024).\n[22] Zhang, Chaoyun, et al. \u0026ldquo;Large language model-brained gui agents: A survey.\u0026rdquo; arXiv preprint arXiv:2411.18279 (2024).\n[23] Wang, Weizhi, et al. \u0026ldquo;Augmenting language models with long-term memory.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 74530-74543.\n[24] Schick, Timo, et al. \u0026ldquo;Toolformer: Language models can teach themselves to use tools.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 68539-68551.\n[25] Shen, Yongliang, et al. \u0026ldquo;Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2023): 38154-38180.\n[26] Park, Joon Sung, et al. \u0026ldquo;Generative agents: Interactive simulacra of human behavior.\u0026rdquo; Proceedings of the 36th annual acm symposium on user interface software and technology. 2023.\n[27] He, Hongliang, et al. \u0026ldquo;WebVoyager: Building an end-to-end web agent with large multimodal models.\u0026rdquo; arXiv preprint arXiv:2401.13919 (2024).\n[28] Yang, Jianwei, et al. \u0026ldquo;Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v.\u0026rdquo; arXiv preprint arXiv:2310.11441 (2023).\n[29] OpenAI. \u0026ldquo;Introducing Operator.\u0026rdquo; OpenAI, 2025.\n[30] OpenAI. \u0026ldquo;Computer-Using Agent.\u0026rdquo; OpenAI, 2025.\n[31] OpenAI. \u0026ldquo;Introducing Deep Research.\u0026rdquo; OpenAI, 2025.\n[32] Phan, Long, et al. \u0026ldquo;Humanity\u0026rsquo;s Last Exam.\u0026rdquo; arXiv preprint arXiv:2501.14249 (2025).\n[33] OpenAI. \u0026ldquo;Introducing GPT-4.5.\u0026rdquo; OpenAI, 2025.\n[34] Anthropic. \u0026ldquo;Introducing the Model Context Protocol.\u0026rdquo; Anthropic, 2024.\n[35] LangGraph. \u0026ldquo;A workflow of the LangGraph.\u0026rdquo; LangGraph Tutorials, 2025.\n[36] Assaf Elovic. \u0026ldquo;GPT Researcher\u0026rdquo; GitHub Repository, 2025.\n[37] LangChain. \u0026ldquo;Open Deep Research\u0026rdquo; GitHub Repository, 2025.\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui.(Mar 2025). Large Language Model Agents. https://syhya.github.io/posts/2025-03-27-llm-agent\nOr\n@article{syhya2025llm-agent, title = \u0026#34;Large Language Model Agents\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Mar\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-03-27-llm-agent\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-03-27-llm-agent/","summary":"\u003ch2 id=\"agents\"\u003eAgents\u003c/h2\u003e\n\u003cp\u003eSince the release of ChatGPT by OpenAI in October 2022, and with the emergence of subsequent projects like \u003ca href=\"https://github.com/Significant-Gravitas/AutoGPT\"\u003eAutoGPT\u003c/a\u003e and \u003ca href=\"https://github.com/reworkd/AgentGPT\"\u003eAgentGPT\u003c/a\u003e, LLM-related agents have become a research hotspot and a practical application direction in AI in recent years. This article will introduce the basic concepts, core technologies, and latest application progress of agents.\u003c/p\u003e\n\u003ch3 id=\"llm-agent\"\u003eLLM Agent\u003c/h3\u003e\n\u003cp\u003eA \u003cstrong\u003eLarge Language Model Agent (LLM agent)\u003c/strong\u003e utilizes an LLM as its core \u0026ldquo;brain\u0026rdquo; and combines modules like planning, memory, and external tools to automate the execution of complex tasks.\u003c/p\u003e","title":"Large Language Model Agents"},{"content":"Background Recently, the number of parameters in large models has been continuously increasing, from the initial billions to today\u0026rsquo;s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models.\nTraining Challenges of Large Models Explosive Growth in Parameter Scale With the continuous pursuit of model capacity and performance, the number of parameters in neural networks is growing exponentially. Today, models ranging from millions to billions, hundreds of billions, and even trillions of parameters are emerging. For example, Llama 3.1 405B has approximately 405 billion parameters, while it is rumored that GPT-4 may have as many as 1.7 trillion parameters. This massive parameter scale has led to a sharp increase in computing and memory demands, bringing unprecedented pressure to the training process.\nSoaring Computational Complexity The rapid increase in the number of parameters directly leads to a significant increase in overall computational complexity. Training a large model once may take weeks or even months. Even with large-scale high-performance GPU clusters, the training cycle is still unsatisfactory, severely restricting model iteration speed and research efficiency.\nIncreasingly Prominent Memory Bottleneck In addition to storing massive model parameters, large models must also save intermediate activations, gradient information, and optimizer states during training. This data poses a huge challenge to GPU memory. Even with high-end GPUs equipped with A100, H100 (80GB memory), H200 (141GB memory), or GB200 (384GB memory), single-card memory is often insufficient to meet the needs of models with hundreds of billions or even trillions of parameters, leading to frequent \u0026ldquo;Out of Memory (OOM)\u0026rdquo; errors.\nCommunication Overhead Becomes a Bottleneck In multi-GPU distributed training environments, inter-node communication is frequently required for data synchronization (such as gradient aggregation). As the model size and the number of GPUs increase, this communication volume rises sharply. Even in high-bandwidth networks, All-Reduce operations to transmit massive amounts of data consume a significant amount of time, becoming one of the main bottlenecks of overall parallel efficiency.\nTraining Stability Challenges Ultra-large-scale models are more prone to gradient vanishing or gradient explosion problems during training, leading to unstable training processes and difficulty in convergence. Although mixed-precision training can accelerate training and reduce memory footprint to some extent, it may also introduce new numerical stability issues, requiring researchers to invest more effort in detailed tuning.\nNecessity of Distributed Training Faced with the above challenges, distributed training technology has become a key solution to support the training of large models. By splitting training tasks and distributing them to multiple GPUs or computing nodes, distributed training can fully utilize parallel computing and cluster memory resources, thereby breaking through the limitations of a single GPU. The main advantages are reflected in the following aspects:\nBreaking Through the Computing Power Limit of a Single GPU The computing power of a single GPU is ultimately limited and cannot cope with the massive computing demands of trillion-parameter models. With data parallelism and model parallelism techniques, training tasks can be evenly distributed to multiple GPUs, thereby significantly shortening the overall training time.\nOvercoming the Memory Bottleneck of a Single GPU By distributing model parameters, intermediate activations, and optimizer states across the memory of multiple GPUs, distributed training effectively expands the available memory capacity. Typical technologies such as ZeRO, through sharding data processing, make the training of ultra-large-scale models possible.\nAccelerating Model Iteration and R\u0026amp;D Cycle The high parallelism of distributed training makes it possible to complete training tasks that originally required weeks or even months in just a few days, thereby greatly improving the model iteration speed and enabling new architectures and strategies to be verified and applied more quickly.\nSupporting Exploration of Larger-Scale Models Distributed training provides a solid foundation for exploring larger-scale and more complex neural network architectures. It is with this technical support that trillion-parameter models (such as Switch Transformer) can be successfully trained and put into practical applications.\nImproving the Robustness and Scalability of Training Systems Distributed systems have excellent fault tolerance. When a GPU node fails, other nodes can quickly take over the task, ensuring that the training process is not interrupted. At the same time, the cluster size can be flexibly expanded or reduced according to specific needs, meeting the training requirements of different scale models.\nParallelism Training The following figure intuitively shows the differences between various parallel training strategies. Different colors represent different model layers (e.g., three layers), and dashed lines distinguish different GPUs. From left to right are data parallelism, model parallelism (including pipeline parallelism and tensor parallelism), and expert parallelism (MoE).\nFig. 1. An illustration of various parallelism strategies on a three-layer model. Each color refers to one layer and dashed lines separate different GPUs. (Image source: OpenAI Blog, 2022)\nData Parallelism The complete model is copied to each GPU, and the dataset is divided into different batches and distributed to each GPU for parallel computation. Finally, the gradients of all GPUs are aggregated during parameter updates.\nModel Parallelism The model is divided and distributed across different GPUs, with each GPU responsible for computing only a part of the model. It can be further divided into the following two categories:\nPipeline Parallelism: The model is split layer-wise (vertically), with different GPUs responsible for different layers. Micro-batches are passed through the pipeline to execute forward and backward computations in parallel. Tensor Parallelism: Large-scale tensor operations (such as large matrix multiplications) within a layer are split horizontally. Each GPU performs part of the computation in parallel and aggregates the results when necessary. Expert Parallelism Through a gating strategy, each input sample only passes through a subset of experts (sub-networks), thus distributing the entire model across different GPUs by \u0026ldquo;expert modules\u0026rdquo;. Commonly used in Mixture-of-Experts (MOE) structures, it can achieve ultra-large parameter scales but only activate a portion of experts during inference/training.\nBelow, I will elaborate on various parallel methods.\nData Parallelism Fig. 2. Data Parallelism. (Image source: Clolossal-AI Documentation)\nIn deep learning training, Data Parallelism (DP) is the most commonly used parallel strategy. Its core idea is:\nReplicate Model Parameters: Place a complete copy of the model parameters on each computing device (usually a GPU). Partition Training Data: Divide the large-scale dataset into multiple subsets along the sample dimension. Different subsets are assigned to different GPUs for processing. Local Forward and Backward Propagation: Each GPU independently computes the loss and corresponding local gradients. Gradient/Parameter Synchronization: Aggregate the gradients from each GPU and update the model parameters, ensuring that the model replicas on all GPUs remain consistent after each iteration. The following shows the data parallelism workflow:\nDataset Partitioning Divide the training dataset $D$ into $N$ non-overlapping subsets ${D_1, D_2, \\dots, D_N}$, where $N$ is the number of GPUs. Usually, it is ensured that the size of each subset is similar to achieve load balancing.\nModel Replication Replicate a complete copy of the model parameters $\\theta$ on each GPU. At the beginning of training, these parameters are the same on each GPU.\nData Distribution Distribute subset $D_i$ to the $i$-th GPU, allowing it to be stored locally and used for subsequent calculations.\nLocal Forward Propagation Each GPU performs forward propagation based on its local data subset $D_i$ to obtain the local loss $L_i(\\theta, D_i)$.\nLocal Backward Propagation Each GPU performs backward propagation based on the local loss $L_i$ to calculate the local gradient\n$$ g_i = \\nabla_{\\theta} L_i(\\theta, D_i). $$ Gradient Synchronization Gradient synchronization (usually All-Reduce) is performed between GPUs to aggregate all local gradients ${g_1, g_2, \\ldots, g_N}$ to obtain the global average gradient\n$$ \\bar{g} = \\frac{1}{N} \\sum_{i=1}^{N} g_i. $$ Parameter Update Each GPU uses the global average gradient $\\bar{g}$ to update its local model parameters:\n$$ \\theta \\leftarrow \\theta - \\eta \\cdot \\bar{g}, $$where $\\eta$ is the learning rate.\nIterative Loop Repeat steps 4-7 until the model converges or reaches the preset number of training epochs.\nBulk Synchronous Parallelism vs. Asynchronous Parallelism In step 6 \u0026ldquo;Gradient Synchronization\u0026rdquo; above, how and when to perform \u0026ldquo;synchronization\u0026rdquo; is one of the important factors affecting the performance and convergence behavior of data parallelism. It is generally divided into the following two categories:\nBulk Synchronous Parallelism (BSP) is the most common and easiest to understand synchronization mode in data parallelism. Its characteristics can be summarized as \u0026ldquo;globally synchronizing gradients and updating parameters once after each mini-batch iteration\u0026rdquo;. The specific process is:\nLocal Computation: Each GPU performs forward and backward propagation based on its data subset $D_i$ to obtain the local gradient $g_i$. Global Communication: All GPUs synchronize (e.g., through All-Reduce) to calculate $\\bar{g}$. Parameter Update: Each node uses $\\bar{g}$ to update its local parameter replica $\\theta$. Wait and Next Iteration: All nodes complete the above operations before entering the next iteration. Asynchronous Parallelism (ASP) aims to get rid of the global synchronization point of BSP and allow each node to perform calculations and parameter updates independently. Its typical implementation is the asynchronous push-pull process under the \u0026ldquo;Parameter Server (PS)\u0026rdquo; architecture:\nEach node calculates the gradient $g_i$ locally, and then pushes it to the parameter server; Once the parameter server receives the gradient, it immediately updates the global model parameters; Other nodes will pull down the latest parameters when they need them to continue the next step of calculation. BSP vs. ASP The following table summarizes the main differences between synchronous and asynchronous parallelism in a data parallel environment:\nComparison Dimension Synchronous Parallelism (BSP) Asynchronous Parallelism (ASP) Parameter Update Timing Global synchronization once per mini-batch or after a certain number of iterations Each node updates parameters independently, without needing to keep the same timestep as others Convergence Stability High. The gradients used are the latest, the convergence path is controllable and easy to analyze Lower. Stale gradients exist, convergence rate and stability may be affected Communication Requirements Highly dependent on All-Reduce, all nodes need to wait and exchange data during synchronization Each node asynchronously pushes/pulls to the parameter server, communication is more flexible, but the parameter server may become a bottleneck Hardware Resource Utilization If there are slow nodes or network delays, other nodes need to wait, and resource utilization may be reduced No need to wait for slow nodes, computing resources can be used efficiently Implementation Complexity Relatively low, mainstream frameworks (PyTorch DDP, Horovod, etc.) have built-in support Relatively higher, parameter server and other components are required, more synchronization logic and data consistency need to be handled Applicable Scenarios Homogeneous hardware, good network bandwidth, pursuit of higher convergence quality Heterogeneous hardware, unstable or low bandwidth network, need for extremely high throughput and tolerance for certain convergence risks Typical Implementations PyTorch DDP, TensorFlow MirroredStrategy Parameter Server architecture (MXNet, TensorFlow ParameterServer mode, etc.) Recommendation: In actual projects, start with simple synchronous parallelism (BSP), and use PyTorch DDP or similar tools for multi-GPU training. If the network environment is heterogeneous, there are many nodes, or the task requires extremely high throughput, you can try asynchronous parallelism (ASP) or parameter server solutions, and cooperate with Gradient Accumulation to balance bandwidth and update frequency.\nGradient Accumulation When the batch size is large or communication becomes the main bottleneck, Gradient Accumulation can be used to reduce the synchronization frequency. Its core idea is:\nContinuously calculate the local gradients of multiple mini-batches and accumulate them in the local accumulation buffer; When the number of accumulated mini-batches reaches $K$, trigger a global gradient synchronization and parameter update. Let $g_j$ be the gradient of the $j$-th mini-batch, then in an \u0026ldquo;accumulation cycle\u0026rdquo;, we get\n$$ G = \\sum_{j=1}^{K} g_j. $$Then update with learning rate $\\eta$:\n$$ \\theta \\leftarrow \\theta - \\eta \\cdot G. $$Since gradient synchronization is no longer performed for each mini-batch, but once every $K$ accumulated mini-batches, the communication overhead can be significantly reduced. However, the reduced parameter update frequency may also slow down the training convergence speed, and a trade-off between throughput and convergence performance is needed.\nDistributed Data Parallelism Distributed Data Parallelism (DDP) is a highly optimized implementation of BSP in PyTorch v1.5 (Li et al. 2020), which facilitates data parallelism for single-machine multi-GPU and even multi-machine multi-GPU. Its main optimizations include:\nGradient Bucketing: Divide model parameters into multiple \u0026ldquo;buckets\u0026rdquo;; when backpropagation is performed, once all gradients in a bucket are calculated, an All-Reduce for that bucket is immediately initiated, instead of waiting for all gradients to be calculated before synchronizing at once. Communication and Computation Overlap: DDP uses asynchronous communication and non-blocking operations to overlap gradient synchronization (communication) with forward propagation and backward propagation (computation) as much as possible, thereby reducing communication overhead. This overlap strategy improves overall parallel efficiency. Gradient Accumulation: DDP can also be easily combined with gradient accumulation. Combined use, by increasing the gradient update interval for each synchronization, reduces the synchronization frequency. In large-scale distributed training, this helps to further reduce communication overhead and improve training efficiency. Fig. 3. Pseudo code for Pytorch DDP. (Image source: Li et al. 2020)\nRing All-Reduce In a multi-GPU (especially single-machine multi-GPU) environment, if there is high-speed interconnect (such as NVLink, PCIe switch, etc.), Ring All-Reduce can be used to significantly reduce communication overhead. The idea is:\nOrganize $k$ nodes into a ring and divide the gradient vector into $k$ parts equally. In the \u0026ldquo;summation phase\u0026rdquo;, each node sends a part of its local gradient to the next node and adds it to the received gradient; after several rounds of this process, each node will hold the complete \u0026ldquo;aggregated\u0026rdquo; gradient. In the \u0026ldquo;broadcast phase\u0026rdquo;, the final result is distributed to all nodes along the ring. Ideally, the communication cost of Ring All-Reduce is approximately independent of the number of nodes (can be regarded as $\\mathcal{O}(1)$), which is very suitable for gradient synchronization in a multi-GPU environment. It is a core communication mode widely used in libraries such as Horovod and NCCL.\nParameter Server When the cluster scale expands to multi-machine multi-GPU, simple single-point aggregation (such as a central server) is often difficult to support parallel training of massive data. Parameter Server (PS) (Li, et al. 2014) is a typical architecture designed for scalable distributed training:\nParameter Sharding: Split model parameters in the form of key-value pairs. Different PS nodes only manage parameters of specific shards. push-pull Semantics: After the computing node obtains the gradient locally, it pushes it to the corresponding PS; after the PS updates the parameters of the shard, the computing node can pull down the latest version when needed for the next step of calculation. Flexible Fault Tolerance and Expansion: By adding or removing PS nodes, capacity can be flexibly expanded in terms of bandwidth or computing needs; backup and fault tolerance strategies can also be implemented on PS. This PS + Worker mode can combine data parallelism and model parallelism simultaneously, splitting ultra-large models and storing them on multiple PSs, and performing distributed training on ultra-large data. PS itself can also be split and merged according to the load situation to form a more complex hierarchical topology.\nModel Parallelism Model Parallelism (MP) is a parallel method that splits the model itself across multiple computing devices (GPUs) for training. When the model parameter size exceeds the memory capacity of a single GPU, model parallelism becomes a necessary choice. Model parallelism is mainly divided into two types: Pipeline Parallelism and Tensor Parallelism.\nNaive Model Parallelism and Bubble Problem\nFig. 4. A naive model parallelism setup where the model is vertically split into 4 partitions. Data is processed by one worker at a time due to sequential dependency, leading to large “bubbles” of idle time. (Image source: Huang et al. 2018)\nNaive model parallelism implementation, which simply divides the model layer by layer and executes it sequentially on different GPUs, will encounter a serious \u0026ldquo;bubble\u0026rdquo; problem. Due to the dependencies between layers, when one GPU is processing a certain stage of a data sample, other GPUs may be idle, waiting for the output of the previous GPU or the input of the next GPU. This GPU idle time is called \u0026ldquo;bubble\u0026rdquo;, which seriously reduces the efficiency of pipeline parallelism.\nWhere $F_i$ represents the forward propagation of Stage $i$, and $B_i$ represents the backward propagation of Stage $i$. It can be seen that in naive pipeline parallelism, only one GPU is working most of the time, and other GPUs are idle, resulting in low efficiency.\nReasons for the bubble problem:\nInter-layer dependency: There is a sequential dependency between the layers of the neural network. The calculation of the next layer must depend on the output of the previous layer. Sequential execution: Naive model parallelism executes layer by layer in order, which prevents GPUs from working in full parallelism. Pipeline Parallelism Fig. 5. Pipeline Parallelism. (Image source: Clolossal-AI Documentation)\nPipeline Parallelism (PP) divides the model layer by layer into multiple stages, and each stage is assigned to a GPU. Data is passed between different GPUs like a pipeline. The output of the previous GPU serves as the input of the next GPU. Pipeline parallelism aims to improve the efficiency of model parallel training and reduce GPU idle time.\nGPipe GPipe (Huang et al. 2018) is an efficient pipeline parallel training system proposed by Google, which aims to solve the bubble problem of naive pipeline parallelism. The core idea of GPipe is to divide a mini-batch into multiple micro-batches and use synchronous gradient aggregation to alleviate the bubble problem and improve pipeline efficiency.\nFig. 6. Illustration of pipeline parallelism in GPipe with 4 microbatches and 4 partitions. GPipe aggregates and updates gradients across devices synchronously at the end of every batch. (Image source: Huang et al. 2018)\nThe following is the GPipe scheduling strategy:\nMicro-batch Partitioning: Divide a mini-batch into $m$ micro-batches. The size of each micro-batch after partitioning is $1/m$ of the original mini-batch. Pipeline Stage Partitioning: Divide the model layer by layer into $d$ stages, and assign each stage to a GPU. Pipeline Execution: Process each micro-batch in sequence, performing forward and backward propagation in the pipeline. The specific process is as follows: Forward Propagation: For each micro-batch, perform forward propagation sequentially on Stage 1, Stage 2, \u0026hellip;, Stage $d$. The output of Stage $i$ serves as the input of Stage $i+1$. Backward Propagation: When the forward propagation of all micro-batches is completed, backward propagation begins. For each micro-batch, perform backward propagation sequentially on Stage $d$, Stage $d-1$, \u0026hellip;, Stage $1$. The gradient of Stage $i$ serves as the input of Stage $i-1$. Synchronous Gradient Aggregation: After the backward propagation of all micro-batches is completed, aggregate the gradients of all micro-batches (e.g., averaging) to obtain the global average gradient. Parameter Update: Each GPU uses the global average gradient to update its local model parameters. GPipe Bubble Ratio Formula Assuming that the forward and backward propagation time of each micro-batch is 1 unit, the pipeline depth is $d$, and the number of micro-batches is $m$, the bubble ratio of GPipe is:\n$$ \\text{Bubble Ratio} = 1 - \\frac{2md}{(2m + 2(d-1))d} = \\frac{d-1}{m+d-1} $$When the number of micro-batches $m$ is much larger than the pipeline depth $d$ ($m \\gg d$), the bubble ratio approaches 0, and the pipeline efficiency is close to linear acceleration. The GPipe paper points out that when $m \u0026gt; 4d$, the bubble overhead can be almost ignored (in the case of activation recomputation). Therefore, there are the following benefits:\nReduce Bubbles: GPipe significantly reduces the bubble problem of naive pipeline parallelism through micro-batch partitioning and pipeline scheduling, improving GPU utilization and training efficiency. Synchronous Gradient Aggregation: GPipe adopts synchronous gradient aggregation, which ensures the synchronicity of the training process and good model convergence. Linear Acceleration Potential: When the number of micro-batches is large enough, GPipe can achieve near-linear acceleration. PipeDream Fig. 7. Illustration of 1F1B microbatch scheduling in PipeDream. (Image source: Harlap et al. 2018)\nPipeDream (Harlap et al. 2018) is another efficient pipeline parallel training system. It adopts the 1F1B (1-Forward-1-Backward) scheduling strategy and introduces Weight Stashing technology to further reduce bubbles, improve pipeline efficiency, and solve the weight version inconsistency problem that may be caused by 1F1B scheduling.\nThe core idea of PipeDream\u0026rsquo;s 1F1B scheduling strategy is that each GPU (Stage) alternately performs forward propagation and backward propagation, working in parallel as much as possible to reduce GPU idle time. The specific process is as follows:\nMicro-batch Partitioning: Divide a mini-batch into $m$ micro-batches. Pipeline Stage Partitioning: Divide the model layer by layer into $d$ stages, and assign each stage to a GPU. 1F1B Scheduling Execution: Each GPU takes turns to perform forward propagation and backward propagation. Weight Stashing Since forward propagation and backward propagation may use different versions of model weights in 1F1B scheduling, it will cause weight version inconsistency problems, affecting the correctness and convergence of training. PipeDream introduces Weight Stashing technology to solve this problem. The core idea of weight stashing is that each GPU maintains multiple versions of model weights and ensures that forward propagation and backward propagation use the same version of weights.\nWeight Stashing Implementation:\nVersion Management: Each GPU maintains a weight version queue to store multiple versions of model weights. Version Selection: When performing forward propagation, select the latest weight version. When performing backward propagation, select the same weight version as the corresponding forward propagation. Version Update: After completing backward propagation of all micro-batches in a mini-batch, update the model weights and generate a new weight version. To further optimize the memory usage of PipeDream, especially in terms of weight stashing, PipeDream has derived two memory optimization variants: PipeDream-flush and PipeDream-2BW.\nPipeDream-flush Fig. 8. Illustration of pipeline scheduling in PipeDream-flush. (Image source: Narayanan et al. 2020)\nPipeDream-flush periodically performs global synchronous pipeline flushing on the basis of PipeDream, similar to GPipe\u0026rsquo;s synchronous gradient aggregation. By periodically flushing, PipeDream-flush can greatly reduce the memory space required for weight stashing, only needing to maintain a single version of model weights, but it will sacrifice a small amount of throughput.\nPipeDream-2BW PipeDream-2BW (Double-Buffered Weights) maintains two versions of model weights, namely \u0026ldquo;double-buffered weights\u0026rdquo;. It updates the model version every $k$ micro-batches, where $k$ is greater than the pipeline depth $d$ ($k \u0026gt; d$). The newly updated model version does not immediately completely replace the old version, because there may still be some remaining backward propagation operations that depend on the old version. With double-buffered weights, PipeDream-2BW can reduce the memory overhead of weight stashing to only maintaining two versions of model weights, significantly reducing memory footprint.\nFig. 9. Illustration of pipeline scheduling in PipeDream-2BW. (Image source: Narayanan et al. 2020)\nThe PipeDream-2BW strategy has the following advantages:\nLower Bubble Overhead: The 1F1B scheduling strategy can further reduce bubbles compared to GPipe, improving GPU utilization and training efficiency. Weight Stashing Solves Version Consistency: Weight stashing technology ensures that forward propagation and backward propagation use the same version of weights, solving the weight version inconsistency problem that may be caused by 1F1B scheduling. Memory Optimization Variants: PipeDream-flush and PipeDream-2BW further optimize memory usage, reduce the memory overhead of weight stashing, and make pipeline parallelism more suitable for memory-constrained scenarios. Tensor Parallelism Tensor Parallelism (TP) is a parallel method that splits tensors in the model (usually weight matrices) along specific dimensions and distributes the split shards to different GPUs for computation. Tensor parallelism has the following advantages:\nBreaking Through Single GPU Memory Limits: Tensor parallelism can distribute model parameters across multiple GPUs, breaking through the memory capacity limit of a single GPU and supporting the training of larger-scale models. Intra-layer Parallelism: Tensor parallelism can achieve parallelization within model layers, such as parallel computation of matrix multiplication operations, improving computational efficiency. Combination with Data Parallelism and Pipeline Parallelism: Tensor parallelism can be combined with other parallel technologies such as data parallelism and pipeline parallelism to form multi-dimensional parallel strategies, further improving training efficiency and scalability. Megatron-LM Megatron-LM (Shoeybi et al. 2019) is a system proposed by NVIDIA for training ultra-large language models. It adopts tensor parallelism technology to parallelize matrix multiplication operations within Transformer model layers, including matrix multiplications in self-attention and MLP.\nFig. 10. Illustration of tensor parallelism for key transformer components proposed in Megatron-LM. (Image source: Shoeybi et al. 2019)\nThe MLP layer of Transformer usually contains two linear layers. The calculation of the first linear layer can be expressed as $Y = \\text{GeLU}(XA)$, where $X$ is the input matrix, $A$ is the weight matrix, and GeLU is the activation function. Megatron-LM splits the weight matrix $A$ along the column dimension into $P$ shards $[A_1, A_2, \u0026hellip;, A_P]$, where $P$ is the number of GPUs. Each GPU $i$ is responsible for storing and computing the weight shard $A_i$.\nTensor Parallelism Computation Process of MLP Layer:\n$$ \\begin{aligned} \\text { Split } A \u0026 =\\left[A_1, A_2\\right] \\\\ Y \u0026 =\\operatorname{GeLU}(X A) \\\\ {\\left[Y_1, Y_2\\right] } \u0026 =\\left[\\operatorname{GeLU}\\left(X A_1\\right), \\operatorname{GeLU}\\left(X A_2\\right)\\right] \\end{aligned} $$ Weight Sharding: Split the weight matrix $A$ along the column dimension into $P$ shards $[A_1, A_2, \u0026hellip;, A_P]$ and assign shard $A_i$ to GPU $i$. Local Matrix Multiplication: Each GPU $i$ uses the input matrix $X$ and weight shard $A_i$ to perform matrix multiplication to obtain the local output $Y_i = \\text{GeLU}(XA_i)$. Global Concatenation (All-Gather): All GPUs use All-Gather operation to concatenate the local outputs ${Y_1, Y_2, \u0026hellip;, Y_P}$ into a complete output matrix $Y = [Y_1, Y_2, \u0026hellip;, Y_P]$. Tensor Parallelism of Self-Attention Layer\nMegatron-LM also performs tensor parallel sharding on the Query ($Q$), Key ($K$), Value ($V$) weight matrices in the Transformer\u0026rsquo;s self-attention layer, and performs corresponding local matrix multiplication and global concatenation operations to achieve tensor parallelism of the self-attention layer. The calculation formula of the self-attention layer is:\n$$ \\text{Attention}(X, Q, K, V) = \\text{softmax}\\left(\\frac{(XQ)(XK)^T}{\\sqrt{d_k}}\\right)XV $$PTD-P PTD-P (Pipeline, Tensor, and Data Parallelism) (Narayanan et al. 2021) is a multi-dimensional parallel strategy that combines pipeline parallelism, tensor parallelism, and data parallelism. PTD-P aims to fully utilize the advantages of various parallel technologies to improve the efficiency and scalability of training ultra-large models.\nFeatures of PTD-P:\nMulti-dimensional Parallelism Combination: PTD-P uses pipeline parallelism, tensor parallelism, and data parallelism simultaneously, which can parallelize the training process from multiple dimensions. Interleaved 1F1B Scheduling: PTD-P adopts the interleaved 1F1B scheduling strategy. Unlike traditional pipeline parallelism, it divides the model into multiple discontinuous layer blocks (model chunks) and assigns multiple layer blocks to each GPU. This scheduling strategy can further reduce bubbles and improve pipeline efficiency. Flexible Parallelism Configuration: PTD-P allows users to flexibly configure the combination of various parallel technologies according to the model structure and hardware resources. For example, tensor parallelism and data parallelism can be used alone, or pipeline parallelism, tensor parallelism, and data parallelism can be used simultaneously. Traditional pipeline parallelism usually divides the model into continuous layer blocks, and each GPU is responsible for a continuous layer block. PTD-P\u0026rsquo;s interleaved 1F1B scheduling divides the model into multiple discontinuous layer blocks. For example, GPU 1 is responsible for layers 1, 2, 9, 10, GPU 2 is responsible for layers 3, 4, 11, 12, and so on. Each GPU is responsible for multiple discontinuous layer blocks, which can more effectively utilize GPU resources and reduce bubble overhead.\nFig. 11.(Top) Default 1F1B pipeline schedule as in PipeDream-flush.(Bottom) Interleaved 1F1B pipeline schedule. First model chunks are in dark colors and second chunks are in light colors. (Image source: Narayanan et al. 2021)\nMixture-of-Experts Model Mixture-of-Experts (MoE) (Shazeer et al. 2017) is a sparsely activated model that significantly increases the model\u0026rsquo;s parameter size and performance without significantly increasing the computational cost by combining multiple independent \u0026ldquo;expert\u0026rdquo; networks and a gating network. The core idea of MoE is Sparse Activation, that is, for each input sample, only a part of the expert networks are activated, rather than the entire model. This method not only improves computational efficiency but also enhances the model\u0026rsquo;s expressive ability, making it perform well in LLMs.\nMoE\u0026rsquo;s design inspiration comes from Ensemble learning, a technology that decomposes complex tasks into multiple subtasks and completes them collaboratively by different models. In MoE, these \u0026ldquo;subtasks\u0026rdquo; are processed by multiple independent expert networks, and the gating network is responsible for dynamically selecting the most suitable experts based on the characteristics of the input sample. This division of labor and cooperation mechanism is similar to an expert team in human society: experts in different fields provide professional opinions for specific problems, and finally, a comprehensive result is obtained.\nFig. 12. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al. 2017)\nCore Components of MoE A typical MoE contains the following components:\nExpert Networks: A set of independent neural networks ${E_1, E_2, \u0026hellip;, E_n}$. Each expert network $E_i$ can be any type of neural network, such as FFN, CNN, RNN, etc. The number of expert networks $n$ can be very large, such as dozens, hundreds, or even thousands. Gating Network: A trainable neural network $G$ used to learn a probability distribution based on the input sample $x$ to determine which experts to activate. The input of the gating network is the input sample $x$, and the output is an $n$-dimensional probability vector $p = G(x) = [p_1, p_2, \u0026hellip;, p_n]$, where $p_i$ represents the probability of activating expert $E_i$. Expert Output Aggregation: According to the output probability distribution of the gating network, the outputs of the activated expert networks are weighted and summed to obtain the final output $y$ of the MoE layer. Noisy Top-k Gating To achieve sparse activation and ensure balanced expert usage, MoE usually adopts Noisy Top-k Gating as the gating mechanism. This method guarantees computational efficiency and avoids uneven expert load through the introduction of noise and top-k selection. The detailed workflow is as follows:\nGating Score Calculation: For an input sample $x$, the gating network first calculates the gating score $H^{(i)}(x)$ for each expert. This score consists of two parts: linear transformation and noise term, as shown in the formula:\n$$ H^{(i)}(x) =(x W_g)^{(i)} + \\epsilon \\cdot \\text{softplus}\\left((x W_{\\text{noise}})^{(i)} \\right), \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$ Parameter Description: $W_g \\in \\mathbb{R}^{d \\times n}$: Trainable weight matrix of the gating network, where $d$ is the input feature dimension and $n$ is the number of experts. $W_{\\text{noise}} \\in \\mathbb{R}^{d \\times n}$: Weight matrix used to generate noise. $\\epsilon \\sim \\mathcal{N}(0, 1)$: Standard Gaussian noise, increasing gating randomness. $\\text{softplus}(x) = \\log(1 + e^x)$: Smooth activation function to ensure that the noise is non-negative. The introduction of noise avoids the gating network always selecting fixed experts and enhances the robustness and diversity of the model.\nTop-k Selection: After calculating the gating score vector $H(x) = [H^{(1)}(x), H^{(2)}(x), \\dots, H^{(n)}(x)]$, the gating network selects the top $k$ experts with the largest values (usually $k \\ll n$). This step is implemented by the $\\text{topk}(v, k)$ function:\n$$ \\text{topk}^{(i)}(v, k) = \\begin{cases} v^{(i)} \u0026 \\text{if } v^{(i)} \\text{ is in the top } k \\text{ elements of } v \\\\ -\\infty \u0026 \\text{otherwise} \\end{cases} $$Setting the scores of non-Top-k experts to $-\\infty$ ensures that the probabilities of these experts in the subsequent softmax operation are 0, achieving sparsity.\nSoftmax Normalization: Perform softmax normalization on the gating scores of the Top-k experts to obtain a sparse probability distribution $G(x)$:\n$$ G(x) = \\text{softmax}\\left( \\text{topk}(H(x), k) \\right) $$Only the probabilities of the Top-k experts are non-zero, and the rest are 0. For example, if $n=100, k=2$, then the probabilities of 98 experts are 0.\nWeighted Summation: Weight and sum the outputs of the Top-k experts according to the probabilities to obtain the output of the MoE layer:\n$$ y = \\sum_{i=1}^{n} G^{(i)}(x) E_i(x) $$Since only $k$ experts are activated, the amount of calculation is much lower than activating all $n$ experts.\nAuxiliary Loss To prevent the gating network from being overly biased towards a few experts, MoE introduces Auxiliary Loss (Shazeer et al. 2017) to encourage all experts to be used evenly. A common method is based on the square of the Coefficient of Variation (CV) of expert usage rate:\n$$ \\mathcal{L}_{\\text{aux}} = w_{\\text{aux}} \\cdot \\text{CV}\\left( \\sum_{x \\in X} G(x) \\right)^2 $$ Parameter Description:\n$X$: Input samples of a mini-batch. $\\sum_{x \\in X} G(x)$: Statistics on the number of times each expert is activated in a mini-batch. $\\text{CV}$: The ratio of standard deviation to mean, measuring the uniformity of expert usage distribution. $w_{\\text{aux}}$: Weight of auxiliary loss, which needs to be adjusted manually. Function: By minimizing $\\mathcal{L}_{\\text{aux}}$, the model optimizes the balance of expert selection and avoids some experts being overused while others are idle.\nGShard GShard (Lepikhin et al. 2020) mainly shards the MoE layer, distributing the expert networks ${E_1, E_2, \u0026hellip;, E_n}$ in the MoE layer to multiple TPU devices. For example, if there are $P$ TPU devices, the expert networks can be divided into $P$ groups, and each group of expert networks is assigned to a TPU device. Other layers of the Transformer model (such as self-attention layer, LayerNorm layer) are replicated on all TPU devices.\nImproved Gating Mechanism of GShard:\nGShard has made some improvements on the basis of Noisy Top-k Gating to improve the performance and stability of the gating mechanism:\nExpert Capacity: To avoid expert overload, GShard introduces expert capacity limits. Each expert network has a capacity limit, indicating the maximum number of tokens it can process. If a token is routed to an expert network that has reached its capacity limit, the token will be marked as \u0026ldquo;overflowed\u0026rdquo;, and the gating output will be set to a zero vector, indicating that the token will not be routed to any expert network.\nLocal Group Dispatching: To improve gating efficiency, GShard groups tokens and enforces expert capacity limits at the group level. For example, divide the tokens in a mini-batch into multiple local groups, each local group containing a certain number of tokens. The gating network selects the top-k expert networks for each local group and ensures that the number of tokens processed by each expert network in a local group does not exceed its capacity limit.\nAuxiliary Loss: GShard also uses an auxiliary loss function to balance expert load. Different from the auxiliary loss of the original MoE model, GShard\u0026rsquo;s auxiliary loss aims to minimize the mean square error of the proportion of data routed to each expert network, which more directly measures the degree of expert load balance.\nRandom Routing: To increase the randomness of routing, GShard introduces a random routing mechanism when selecting the top-k expert networks. In addition to selecting the best top-k expert networks, GShard also randomly selects suboptimal expert networks with a certain probability to increase the diversity of expert networks and improve the generalization ability of the model.\nBelow is the core algorithm flow of GShard:\nFig. 13. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al. 2020)\nSwitch Transformer Switch Transformer (Fedus et al. 2021) is a MoE model proposed by Google with a parameter size of trillions. Its core innovation is to replace the dense feed-forward network (FFN) layer in the Transformer model with a sparse Switch FFN layer. Unlike GShard\u0026rsquo;s Top-2 Gating, Switch Transformer only routes each input token to one expert network, which has higher sparsity and further reduces computational costs, making it possible to train trillion-parameter models. It encourages token routing to be more balanced among $N$ experts. The auxiliary loss of Switch Transformer is based on the cumulative product of the actual routing ratio and the predicted routing probability. The specific formula is as follows:\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ Parameter Description: $N$: Total number of experts.\n$f_i$: The proportion of tokens routed to the $i$-th expert, defined as:\n$$ f_i = \\frac{1}{T} \\sum_{x \\in B} 1\\{\\text{argmax } p(x) = i\\} $$ $P_i$: The routing probability of the $i$-th expert predicted by the gating network, defined as:\n$$ P_i = \\frac{1}{T} \\sum_{x \\in B} p_i(x) $$ $T$: Total number of tokens in batch $B$.\n$\\alpha$: Weight hyperparameter of auxiliary loss, usually set to $10^{-2}$.\nBy minimizing $\\text{loss}$, the model makes the actual routing ratio $f_i$ consistent with the predicted probability $P_i$, thereby indirectly promoting load balancing between experts and avoiding some experts being idle.\nFig. 14. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)\nSwitch Router Mechanism:\nRouting Prediction: For an input token $x$, Switch Router predicts the routing probability $p_i = G^{(i)}(x)$ of each expert network, where $i = 1, 2, \u0026hellip;, n$, and n is the number of expert networks.\nExpert Selection: Select the expert network with the highest routing probability as the best expert network. Switch Transformer adopts the Top-1 routing strategy, that is, each token is only routed to the expert network with the highest routing probability.\nToken Routing: Route the input token $x$ to the selected best expert network for processing.\nTraining Stability Optimization of Switch Transformer:\nTo improve the training stability of Switch Transformer, the paper proposes the following optimization strategies:\nSelective Precision Using FP32 precision inside the routing function can improve training stability and avoid additional overhead caused by FP32 tensor communication. Specifically, the calculation process of Switch Router uses FP32 throughout, and the final result is converted to FP16 to balance efficiency and precision.\nSmaller Initialization It is recommended to adjust the weight initialization scale parameter $s$ of Transformer from 1 to 0.1. A smaller initialization scale helps to alleviate the risk of gradient explosion in the early stage of training, thereby improving overall training stability. The specific implementation is to sample from a truncated normal distribution with a mean of 0 and a standard deviation of $\\sqrt{s/n}$ (where $n$ is the number of input units).\nHigher Expert Dropout Using a higher dropout rate (e.g., 0.4) in the expert FFN layer, while maintaining a lower dropout rate (e.g., 0.1) in non-expert layers, this setting can effectively prevent overfitting and thus enhance the generalization ability of the model. The experimental results in the figure below show that the model performs best when the dropout rate of the expert layer is set to 0.4 on tasks such as GLUE, CNNDM, SQuAD, and SuperGLUE.\nFig. 15. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: Fedus et al. 2021)\nThe Switch Transformers paper uses the following figure to intuitively show how different parallel technologies split model weights and data:\nFig. 16. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)\nExpert Choice Expert Choice (EC) (Zhou et al. 2022) is a routing strategy opposite to token choice routing (such as GShard\u0026rsquo;s top-2 or Switch Transformer\u0026rsquo;s top-1). In token choice routing, each token selects top-k experts from all experts for routing; while in expert choice routing, each expert selects top-k tokens from all tokens for processing. This method aims to solve the problems of load imbalance and token waste in token choice routing, and significantly improve training efficiency. The following is the specific calculation process:\nCalculate Token-to-Expert Affinity Score\nFor an input matrix $X \\in \\mathbb{R}^{n \\times d}$, the process of calculating the token-to-expert affinity score matrix $S \\in \\mathbb{R}^{n \\times e}$ is:\n$$ S = \\text{softmax}(X \\cdot W_g), \\quad \\text{where } W_g \\in \\mathbb{R}^{d \\times e}. $$ Here, $W_g$ is the gating weight matrix, and $e$ is the number of experts.\nExpert Selects Tokens\nEach expert selects top-k tokens from all tokens for processing. Top-k selection is performed on $S^T$:\n$$ G, I = \\text{top-}k(S^T, k), $$to get:\nGating matrix $G \\in \\mathbb{R}^{e \\times k}$: Records the routing weights corresponding to the tokens selected by the experts, where $G[i, j]$ represents the weight of the $j$-th token selected by expert $i$; Token index matrix $I \\in \\mathbb{R}^{e \\times k}$: Represents the index of the token selected by each expert in the input. One-hot Encoding\nConvert the token index matrix $I$ into a one-hot encoding matrix $P \\in \\mathbb{R}^{e \\times k \\times n}$ for subsequent calculations:\n$$ P = \\operatorname{one}-\\operatorname{hot}(I) $$ Construct Gated FFN Layer Input\nFor each expert $i$, the input of its gated FFN layer is:\n$$ (P \\cdot X) \\in \\mathbb{R}^{e \\times k \\times d}. $$ EC controls the sparsity of the model by regularizing and limiting the number of experts to which each token is routed. A common regularization target is as follows:\n$$ \\begin{aligned} \u0026 \\max_{A} \\langle S^{\\top}, A \\rangle + \\lambda H(A) \\\\ \u0026 \\text{s.t. } \\forall i: \\sum_{j'} A[i, j'] = k, \\quad \\forall j: \\sum_{i'} A[i', j] \\leq b, \\quad \\forall i,j: 0 \\leq A[i, j] \\leq 1, \\end{aligned} $$In the optimization problem considered, a matrix $A$ is defined, and the element in the $i$-th row and $j$-th column indicates whether the $i$-th expert has selected the $j$-th token (value 0 or 1). Since this optimization problem is complex to solve, the paper uses the Dijkstra\u0026rsquo;s algorithm (to obtain an approximate solution through multiple iterations) to solve it.\nThe parameter $b$ is usually determined by the total number of tokens $n$ in the batch and the capacity factor, where the capacity factor represents the average number of experts used by each token. Most experiments use a higher capacity factor. The experimental results show that even when the capacity is reduced, EC (Expert Choice) still performs better than traditional top-1 token choice routing, although capped expert choice slightly reduces fine-tuning performance.\nThe advantages of EC are mainly reflected in the following two aspects:\nPerfect Load Balancing: Each expert processes a fixed number of $k$ tokens, thus avoiding the problem of some experts being overloaded while others are idle, achieving ideal load balancing. Higher Training Efficiency: Experiments show that EC can improve the training convergence speed by about 2 times, which is more efficient than traditional token choice routing. However, EC also has the following limitations:\nBatch Size Requirements: Since EC has high requirements for batch size, it is not suitable for scenarios with smaller batch sizes. Autoregressive Generation Limitations: In autoregressive text generation tasks, EC\u0026rsquo;s top-k selection cannot be implemented because future tokens cannot be predicted, so it is not suitable for such tasks. Sequence Parallelism Sequence Parallelism (SP) is a parallelization strategy proposed for long sequence models (such as Transformer). By partitioning the input in the sequence dimension, it greatly reduces activation memory footprint and improves training efficiency. It is often used in combination with data parallelism, tensor parallelism, or pipeline parallelism, and is especially suitable for processing ultra-long text or other sequence data.\nColossal-AI Sequence Parallelism Fig. 17. The overall architecture of the proposed sequence parallelism and existing parallel approaches. For sequence parallelism, Device 1 and Device 2 share the same trainable parameters. (Image source: Li, et al. 2021)\nThe computational complexity and memory overhead of self-attention are proportional to the square of the sequence length $s$, $O(s^2)$. Long sequence data will increase the intermediate activation memory usage, thus limiting the training capacity of the device. Colossal-AI sequence parallelism (Li, et al. 2021) proposes splitting ultra-long sequences to multiple cards from a system perspective. The specific solution steps are as follows.\nSequence Chunking Divide the input sequence into several chunks, each chunk is saved and computed by different GPUs; therefore, each card only needs to store the activation of its corresponding sequence chunk, avoiding single-card memory explosion. Ring Communication + Self-Attention Propose Ring Self-Attention (RSA) mechanism: each GPU first calculates local attention, and then sequentially transmits (ring structure) Key/Value chunks to adjacent GPUs. After multiple iterations, it is guaranteed that each GPU can obtain global sequence information. Combination with Other Parallelism Methods Not restricted by hyperparameters such as the number of attention heads and layers, it can be combined with data parallelism, tensor parallelism, pipeline parallelism and other technologies to jointly break through the sequence length limit of large-scale models. Fig. 18. Ring Self-Attention. (Image source: Li, et al. 2021)\nMegatron-LM Sequence Parallelism Megatron-LM (Shoeybi et al. 2019) originally used tensor parallelism to share part of the activation values, but the activation values of operations such as LayerNorm and Dropout in Transformer still need to be completely saved on a single card, and the memory consumption is still huge. Therefore, NVIDIA proposed Megatron-LM sequence parallelism (Korthikanti, et al. 2022) to split these activation values in the sequence dimension, greatly reducing the footprint.\nFig. 19. Transformer layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)\nFig. 20. MLP layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)\nSequence Dimension Splitting For activations that are difficult to split in the tensor dimension, such as LayerNorm and Dropout, divide them along the sequence dimension, so that each GPU only processes a part of the sequence\u0026rsquo;s nonlinear operations. Tensor Parallelism is Still Retained Linear operations such as Attention and MLP continue to use tensor parallelism; the activations of sequence parallelism need to perform corresponding All-Gather or Reduce-Scatter before and after to exchange data. Selective Activation Recomputation For some operations with small computational load but large activation volume, choose to temporarily recompute during backpropagation to further save memory. DeepSpeed-Ulysses Sequence Parallelism DeepSpeed-Ulysses (Jacobs et al. 2023) proposes an efficient sequence parallelism scheme for ultra-long sequence training. By partitioning the input in the sequence dimension and combining two-stage all-to-all communication, it effectively reduces communication volume and activation memory, thereby supporting the training of million-token long sequence Transformer models.\nFig. 21. DeepSpeed sequence parallelism(DeepSpeed-Ulysses) design. (Image source: Jacobs et al. 2023)\nSequence Partitioning + All-to-All Communication Divide the input sequence along the sequence dimension to $P$ GPUs, and each GPU only processes a local $N/P$ sequence; before attention calculation, exchange Query ($Q$), Key ($K$), and Value ($V$) through All-to-All operation, so that each GPU obtains complete sequence information, but only calculates the assigned attention heads.\nTwo-Stage Communication Optimization\nFirst All-to-All: Perform all-to-all exchange on $Q$/$K$/$V$ before attention calculation to disperse activation calculation and reduce memory pressure per card; Second All-to-All: After attention calculation, collect the output context and remap it to local sequence partitions, which not only restores the original sequence structure but also significantly reduces the amount of communication data. Efficient Communication and Generality Using all-to-all communication, the communication volume is reduced to $O(N/P)$, which saves nearly $P$ times the bandwidth compared to the traditional All-Gather method (communication volume $O(N)$); at the same time, this scheme is suitable for dense and sparse attention and can be seamlessly integrated with ZeRO-3 memory optimization, thereby supporting efficient training of larger models and longer sequences.\nFig. 22. DeepSpeed-Ulysses vs Megatron LM. (Image source: DeepSpeed Blogs)\nIn a 64-card A100 environment, the throughput is increased by up to 2.5 times compared to Megatron-LM sequence parallelism, and longer sequences (million-level tokens) can be processed; The convergence performance is the same as the original model, and it can be easily integrated into the Megatron-DeepSpeed framework. Optimizer-Related Parallelism: ZeRO ZeRO (Zero Redundancy Optimizer) (Rajbhandari et al. 2019) is an optimizer parallelism technology designed to eliminate memory redundancy when training large models. The main memory consumption for training large models is in two parts:\nModel States: Including optimizer states (such as momentum and second-order moments of Adam), gradients, and model parameters. Mixed-precision training not only requires storing FP16 data but also needs to retain FP32 versions of parameters and states, resulting in higher memory footprint. Activations, Temporary Buffers, and Memory Fragmentation (Residual States): These data are only used once in forward and backward propagation, but they also occupy a lot of memory. To solve the memory redundancy problem, ZeRO adopts two major strategies:\nZeRO-DP (Data Parallelism): For model states, by sharding and distributing optimizer states, gradients, and parameters to multiple data parallel processes, redundancy is eliminated, and communication volume is reduced by using dynamic communication scheduling.\nZeRO-R (Residuals Optimization): For activations and temporary buffers, memory usage is optimized by using sharded activation recomputation, fixed buffer size, and real-time memory fragmentation management.\nZeRO Sharding Strategy ZeRO is divided into three stages, each stage further reduces memory redundancy on the basis of the previous stage, thus making it possible to train ultra-large models:\nZeRO-1 (Optimizer State Sharding) Principle: Shard optimizer states (such as Adam\u0026rsquo;s momentum and second-order moments) along the parameter dimension into $P$ shards ($P$ is the number of GPUs), and each GPU only stores the states corresponding to the model parameters it is responsible for. Local Update: Each GPU only updates its locally stored state and parameter shards during the parameter update phase, without additional cross-GPU communication. ZeRO-2 (Gradient Sharding) Principle: On the basis of optimizer state sharding, gradients are also sharded along the parameter dimension, and each GPU only stores the corresponding gradient shard. Each GPU calculates local gradients and uses efficient Reduce-Scatter operations to aggregate gradients and then update local parameter shards. ZeRO-3 (Parameter Sharding) Principle: On the basis of ZeRO-1 and ZeRO-2, model parameters (usually 16-bit data) are also sharded, and each GPU only stores the parameter shards corresponding to it. Parameter Collection on Demand: During forward or backward propagation, if a GPU needs complete model parameters, it collects the missing shards from other GPUs. This process is only performed when necessary to reduce communication overhead. The following figure shows the comparison of model state memory consumption per device in different stages:\nFig. 23. Comparing the per-device memory consumption of model states, with three stages of ZeRO-DP optimizations. (Image source: Rajbhandari et al. 2019)\nComparison of DeepSpeed ZeRO Sharding and Offload Strategies To better understand DeepSpeed\u0026rsquo;s ZeRO strategy, the following compares each stage and Offload scheme:\nZeRO Stage Description Memory Footprint Training Speed ZeRO-0 Pure data parallelism, no sharding, all states are fully replicated on each GPU. Highest Fastest ZeRO-1 Optimizer states are sharded only, gradients and parameters are still replicated. Higher Slightly slower than ZeRO-0 ZeRO-2 Optimizer states and gradients are sharded. Medium Slower than ZeRO-1 ZeRO-3 Optimizer states, gradients, and model parameters are sharded. Lowest Significantly slower than ZeRO-2, affected by model size and network bandwidth Offload Type Description Memory Footprint Training Speed ZeRO-1 + CPU Offload On the basis of ZeRO-1, optimizer states are offloaded to CPU memory, reducing GPU memory footprint, but relying on PCIe bandwidth and occupying CPU memory. Medium-Low Slower than ZeRO-1 ZeRO-2 + CPU Offload On the basis of ZeRO-2, optimizer states are offloaded to CPU memory, further reducing GPU memory footprint for large models, but increasing CPU-GPU data transfer. Low Slower than ZeRO-2 ZeRO-3 + CPU Offload On the basis of ZeRO-3, optimizer states and model parameters are offloaded to CPU, GPU memory footprint is the lowest, but CPU-GPU communication overhead is extremely large. Extremely Low Very Slow ZeRO-Infinity (NVMe Offload) Based on ZeRO-3, states are offloaded to NVMe devices, breaking through CPU memory limits, suitable for ultra-large models; performance is highly dependent on NVMe parallel read and write speed. Extremely LowNVMe support required Slower than ZeRO-3, but usually better than CPU Offload scheme Communication Volume and Performance Impact ZeRO-0/1/2: Mainly rely on All-Reduce for gradient synchronization, and the communication volume is relatively low.\nZeRO-3: All-Gather/All-Reduce operations are required for model parameters, and the communication volume increases significantly. Network bandwidth becomes a key bottleneck.\nOffload Strategy (CPU/NVMe): Data transmission is mainly between CPU ↔ GPU or NVMe ↔ GPU. The transmission bandwidth is much lower than the communication between GPUs, which may significantly affect the training speed, especially in ZeRO-3 scenarios.\nMulti-dimensional Parallelism Multi-dimensional Parallelism refers to the organic combination of multiple parallel technologies such as data parallelism, model parallelism, and pipeline parallelism in distributed training to fully utilize the computing resources of modern GPU clusters. Through this \u0026ldquo;3D parallelism\u0026rdquo; or \u0026ldquo;4D parallelism\u0026rdquo; strategy, not only memory efficiency can be improved, but also computational efficiency can be improved, thereby achieving efficient training of ultra-large-scale (even trillion-parameter level) models.\n3D Parallelism With the rapid improvement of the computing power of GPU clusters, training trillion-parameter models is no longer out of reach. DeepSpeed integrates data parallelism, model parallelism, and pipeline parallelism to build a \u0026ldquo;3D parallelism\u0026rdquo; strategy. This strategy mainly solves the two major challenges faced by training ultra-large models:\nMemory Efficiency: Model layers are divided into different pipeline stages, and each stage is further divided by model parallelism, reducing the memory occupied by models, optimizers, and activations. However, it should be noted that model splitting cannot be unlimited, otherwise, the communication overhead will increase significantly, which will affect computational efficiency.\nComputational Efficiency: To make the number of computing workers exceed the limitations of simple model and pipeline parallelism, and to ensure computational efficiency, DeepSpeed expands with ZeRO-DP (data parallelism based on optimizer state sharding). ZeRO-DP not only further optimizes memory usage but also allocates data parallel groups to devices with local high-bandwidth communication through topology-aware mapping, greatly reducing communication overhead.\nThe following diagram shows the overall strategy of 3D parallelism:\nFig. 24. Example 3D parallelism with 32 workers. Layers of the neural network are divided among four pipeline stages. Layers within each pipeline stage are further partitioned among four model parallel workers. Lastly, each pipeline is replicated across two data parallel instances, and ZeRO partitions the optimizer states across the data parallel replicas. (Image source: Majumder et al. 2020)\nEach parallel dimension (data, model, pipeline) is carefully mapped to fully utilize the communication bandwidth within and between nodes. Specific strategies include:\nOptimize Intra-node Communication: Since model parallelism has the largest communication overhead, model parallel groups are preferentially arranged within the same node to utilize higher intra-node bandwidth (e.g., using NVIDIA Megatron-LM\u0026rsquo;s tensor sharding method); Data Parallelism and Pipeline Parallelism: When model parallelism does not cover the entire node, data parallel groups are arranged within the same node as much as possible; pipeline parallelism can be flexibly arranged for cross-node scheduling due to its smaller communication volume. By reducing the amount of communication data in each data parallel group and increasing the parallelism of local parallel communication, the overall communication bandwidth is effectively amplified.\nFig. 25. Mapping of workers in Figure 24 to GPUs on a system with eight nodes, each with four GPUs. Coloring denotes GPUs on the same node. (Image source: Majumder et al. 2020)\n4D Parallelism To further expand the model scale, Llama3 (Grattafiori et al. 2024) adopted a 4D parallel strategy during training. It combines four parallel methods to shard the model in a more fine-grained manner, so that the model parameters, optimizer states, gradients, and activations on each GPU can be adapted to the capacity limit of high-bandwidth memory (HBM). These four parallel methods are:\nTensor Parallelism (TP): Divide a single weight tensor into multiple blocks and distribute them across different devices; Pipeline Parallelism (PP): Vertically divide the model into multiple stages, and each stage processes different micro-batches in parallel on different devices; Context Parallelism (CP): Divide the input context into multiple segments to alleviate the memory bottleneck when inputting long sequences; Data Parallelism (DP), usually using Fully Sharded Data Parallelism (FSDP): Shard models, optimizer states, and gradients, and synchronize after each training step. The following diagram shows an example of 4D parallelism implemented on 16 GPUs. The position of each GPU is represented by a vector [D1, D2, D3, D4], where each dimension corresponds to a parallel strategy. GPUs are grouped according to four dimensions [TP, CP, PP, DP], and the group size of each dimension is 2. For example, GPU0 and GPU1 belong to the same tensor parallel group; GPU0 and GPU2 belong to the same context parallel group; GPU0 and GPU4 belong to the same pipeline parallel group; GPU0 and GPU8 belong to the same data parallel group:\nFig. 26. Illustration of 4D parallelism. (Image source: Grattafiori et al. 2024)\nThrough the 4D parallel strategy, Llama3 can fully utilize the computing resources of multiple GPUs during training, while effectively reducing memory footprint and supporting the training of ultra-large-scale models.\nMemory Optimization Techniques In addition to parallel training techniques, there are many memory optimization techniques designed to help train LLMs. These designs mainly start from reducing the memory footprint of each stage in the training process.\nCPU Offloading CPU Offloading (Rhu et al. 2016) refers to a common and intuitive practice of offloading data or tensors that are temporarily not needed to the CPU when GPU memory is insufficient and loading them back to the GPU when needed. Its main purpose is to use the larger capacity of CPU memory to expand available space, so that larger-scale models can be trained even in memory-constrained environments. However, this method will bring additional data transmission overhead and usually reduce training speed, so its application has been relatively reduced in recent years.\nIdentify Offloadable Tensors: Identify tensors that are temporarily not used during training, such as model parameters, optimizer states, intermediate activations, etc. The basis for judging whether a tensor can be offloaded can be the frequency of use, life cycle, etc. of the tensor. Tensor Offloading: Move offloadable tensors from GPU memory to CPU memory. Data transmission is usually performed through the PCIe bus. Tensor Prefetching: Before needing to use tensors offloaded to CPU memory, load the tensors from CPU memory back to GPU memory in advance. Prefetching operations can be performed in parallel with GPU computing operations to reduce data loading latency. Tensor Usage: The GPU uses tensors loaded back into GPU memory for computation. Tensor Re-offloading: After the tensor is used up, if the tensor is no longer needed for a period of time, it can be offloaded to CPU memory again to release GPU memory space. ZeRO-Offload and ZeRO-Infinity are memory optimization technologies based on CPU offloading implemented in the DeepSpeed library. ZeRO-Offload offloads optimizer states to CPU memory, and ZeRO-Infinity goes further, offloading model parameters to CPU memory or even NVMe disks, breaking through the GPU memory wall and supporting the training of larger-scale models.\nThe following figure intuitively shows the memory optimization technology of Heterogeneous system:\nFig. 27. Heterogenous system illustration. (Image source: Clolossal-AI Documentation)\nActivation Recomputation/Gradient Checkpointing Activation Recomputation/Gradient Checkpointing (Chen et al. 2016) is a technology that trades computation for memory. During training, only part of the activation values are saved (e.g., the input activation values of each Transformer layer). During backpropagation, the unsaved activation values are recomputed. Activation recomputation can significantly reduce the activation memory footprint during training, especially when training deep neural networks.\nSelect Checkpoints: Select some layers in the model as checkpoints. Usually, key layers in the model are selected as checkpoints, such as the input layer of the Transformer layer. Forward Pass: During forward propagation, only the activation values of checkpoint layers are saved. For non-checkpoint layers, the activation values are immediately released after calculation and not saved. Backward Pass: During backpropagation, when it is necessary to calculate the gradient of a non-checkpoint layer, forward propagation is performed again first to calculate the activation value of the layer (recomputation), and then backward propagation is performed to calculate the gradient. For checkpoint layers, since the activation values of checkpoint layers have been saved, the saved activation values can be directly used for backpropagation without recomputation. The following is a memory cost analysis of activation recomputation. For ease of analysis, assume that the model has a total of $n$ network layers and divides them evenly into $k$ segments. In this way, each segment contains approximately $n/k$ network layers. When doing activation recomputation, we only save the activation values at the boundaries of each segment (i.e., checkpoints), and recompute the rest when needed. The following function represents the memory requirement during training:\n$$ \\text{cost-total} \\;=\\; \\max_{i=1,\\ldots,k}\\bigl[\\text{cost-of-segment}(i)\\bigr] \\;+\\; O(k) \\;=\\; O\\Bigl(\\tfrac{n}{k}\\Bigr) \\;+\\; O(k). $$Next, consider how to choose the optimal $k$ to minimize $f(k)$ given $n$:\n$$ f(k) \\;=\\; \\frac{n}{k} \\;+\\; k. $$Take the derivative of $f(k)$ with respect to $k$ and set it to 0 (only consider $k\u0026gt;0$):\n$$ f'(k) \\;=\\; -\\frac{n}{k^2} \\;+\\; 1 \\;=\\; 0 \\quad\\Longrightarrow\\quad k^2 = n \\quad\\Longrightarrow\\quad k = \\sqrt{n}. $$Substituting $k = \\sqrt{n}$, we can get the minimum memory overhead of approximately\n$$ f(\\sqrt{n}) \\;=\\; \\frac{n}{\\sqrt{n}} \\;+\\; \\sqrt{n} \\;=\\; 2\\sqrt{n}. $$Therefore, the overall peak memory requirement of this approach can be reduced to the order of $O(\\sqrt{n})$ (compared to the $O(n)$ memory of generally directly saving all activations), which is why activation recomputation technology can bring \u0026ldquo;sublinear\u0026rdquo; memory footprint. The following figure intuitively shows the effect of this trick.\nFig. 28. The memory cost of different memory saving algorithms. Sharing: Memory used by intermediate results is recycled when no longer needed. Inplace: Save the output directly into memory of an input value. (Image source: Chen et al. 2016)\nIt should be noted that activation recomputation requires additional forward recomputation in the backward propagation stage. Each segment needs to perform forward computation of $n/k$ layers. If the network is divided into $k$ segments, the total recomputation during backpropagation is approximately $k \\times \\bigl(n/k\\bigr) = n$ layers of forward operations, which is equivalent to doing approximately one more \u0026ldquo;forward propagation\u0026rdquo; in each training iteration. This is usually acceptable in LLM training because:\nCompared to quickly exhausting GPU memory and making it impossible to train large-scale models, this additional cost in computation is usually more bearable. When the model is very deep ($n$ is large), using activation recomputation technology can significantly reduce memory usage from $O(n)$ to $O(\\sqrt{n})$, making it possible to train more and deeper large models on given hardware. Mixed Precision Training Mixed Precision Training (Micikevicius al. 2017) is a technology that simultaneously uses low-precision floating-point numbers (such as FP16 or BF16) and high-precision floating-point numbers (such as FP32) during model training. Its core goal is to reduce memory footprint and accelerate training while maintaining model accuracy comparable to full-precision training as much as possible.\nModern GPUs have higher throughput and lower memory footprint in low-precision computing, thereby reducing memory access overhead and memory bandwidth requirements. Mixed-precision training can significantly improve training speed. The following figure shows the basic process of mixed-precision training in a network layer: forward and backward propagation mainly use half-precision (FP16) operations, while gradient accumulation and parameter updates use full-precision (FP32) to avoid numerical precision problems that may be caused by low-precision computing.\nFig. 29. Mixed precision training iteration for a layer. (Image source: Micikevicius al. 2017)\nMixed-precision training mainly relies on the following three key technologies:\nFull-Precision Master Copy of Weights To prevent gradients from being truncated to zero due to being too small in magnitude under FP16, a master copy of FP32 weights is maintained during training. The specific process is:\nInitialization: Use FP32 weights as the master copy of the model; Forward/Backward Propagation: Before each iteration starts, convert FP32 weights to FP16 for forward propagation and backward propagation to calculate FP16 gradients; Parameter Update: Before updating parameters, convert FP16 gradients to FP32 and use them to update the FP32 master copy. This design not only utilizes the efficiency of low-precision computing but also ensures the accuracy of parameter updates.\nLoss Scaling To avoid gradient underflow due to the limited representation range of low precision, the loss value is usually amplified before backpropagation. The specific process is:\nUse FP32 to calculate the loss $L$; Multiply the loss by a scaling factor $S$ to get $L\u0026rsquo; = L \\times S$, and then perform backpropagation to calculate FP16 gradients; Before parameter update, divide the gradient by $S$ to restore it to the true gradient. The choice of scaling factor is crucial: too small may not avoid gradient underflow, and too large may cause gradient overflow. Dynamic loss scaling technology can automatically adjust the scaling factor according to the actual situation of gradients during training.\nAs shown in the figure below, amplifying the loss makes the gradient distribution more concentrated in the higher numerical part, thereby retaining the subtle information that may be truncated under low-precision representation.\nFig. 30. The histogram of gradients in full precision. The left part up to $2^{-24}$ will be zero-ed off once the model switches to FP16. (Image source: Micikevicius al. 2017)\nArithmetic Precision Control For operations with high precision requirements (such as vector dot product and summation reduction), FP32 can be used for accumulation calculation, and then converted to FP16 for storage; for element-wise operations, FP16 or FP32 can be selected according to specific needs. Compression In the deep learning training process, intermediate results (such as activation values and gradient information), although only used once in forward propagation and once in backward propagation, often occupy a lot of memory. Considering that there is a significant time interval between two uses, data can be compressed after the first use, and then decompressed when needed later, thereby effectively reducing memory footprint.\nCompression technology is mainly applied to two scenarios:\nActivation Value Compression: Compress activation values after forward propagation and decompress before backward propagation. This is especially important for deep neural networks because activation values usually occupy a lot of memory. Gradient Compression: Compress gradients after calculating gradients in backpropagation and before gradient synchronization to reduce the amount of data for cross-GPU communication, thereby improving distributed training efficiency. Compression technology can be divided into two categories:\nLossless Compression: Methods such as Huffman coding or Lempel-Ziv algorithm are used to ensure that the decompressed data is completely consistent with the original data. However, due to the low compression rate, its memory saving effect is limited.\nLossy Compression: Algorithms such as JPEG or MPEG are used to obtain higher compression rates on the premise of allowing certain data loss. This method can significantly reduce memory footprint, but may have a certain impact on model accuracy and convergence.\nGist (Jain et al. 2018) is a memory optimization technology for activation value compression. Its core lies in using data encoding strategies to compress intermediate results, mainly including two encoding schemes:\nLayer-Specific Lossless Encoding: Design special lossless encoding schemes for specific layer structures (such as ReLU-Pool and ReLU-Conv):\nFor ReLU-Pool layers, binary encoding can be used; For ReLU-Conv layers, sparse storage and dense computation encoding are used. Aggressive Lossy Encoding: Delayed Precision Reduction (DPR) technology is used. The core idea of DPR is: activation values need to maintain high precision during forward propagation, while lower precision can be tolerated during backward propagation. Therefore, activation values are compressed to lower precision after forward propagation, and then decompressed to high precision before backward propagation.\nMemory-Efficient Optimizers Traditional optimizers (such as Adam, SGD with Momentum) need to maintain a large amount of state data (such as momentum and variance) for each model parameter during training. Their memory footprint is often comparable to or even higher than the model parameter size. For example, taking the Adam optimizer (Kingma et al. 2014) as an example, each parameter needs to store the first-order moment and the second-order moment. Adding up with the parameter itself and its gradient, the entire training process requires approximately 4 times the memory of the model weights, which poses a severe challenge to large model training.\nTo reduce memory consumption, memory-efficient optimizers are mainly designed through the following strategies:\nReduce the Number of State Variables: Only save necessary statistical information instead of complete matrices; Reduce the Precision of State Variables: Use FP16 or bfloat16 for storage; Share State Variables: Share part of the state information between multiple parameters. Adafactor Adafactor (Shazeer et al. 2018) is a memory-efficient adaptive learning rate optimizer. Unlike Adam, Adafactor does not store the complete second-order moment estimation matrix, but only stores two vectors (row and column statistics) to replace the complete second-order moment matrix, which significantly reduces memory footprint, especially suitable for scenarios where the parameter matrix has a low-rank structure.\nSM3 SM3 (Sparse Momentum for Massive Models) (Anil et al. 2019) provides a memory-efficient adaptive optimization scheme through sparse updates and state sharing.\nSparse Momentum: Only update Momentum for parameters with non-zero gradients, thereby reducing computation and storage overhead; State Sharing: To a certain extent, allow different parameters to share state variables, further reducing memory consumption; Adaptive Learning Rate: Dynamically adjust the learning rate according to the gradients of each parameter, improving the stability and convergence speed of model training. Okay, here is the English translation of the provided text about LoRA:\nLoRA LoRA (Low-Rank Adaptation) (Hu et al. 2021) proposes a method that introduces low-rank adapters alongside pre-trained weights. This enables efficient fine-tuning by adding only a small number of parameters, while preserving the original inference capabilities of the pre-trained model.\nThe figure below intuitively illustrates the principle and initialization strategy of LoRA:\nFig. 31. An illustration of LoRA. (Image source: Hu et al. 2021)\nIn the standard forward pass, the model output is\n$$ h = W_0 x, $$With LoRA introduced, the output becomes\n$$ h = W_0 x + \\Delta W x = W_0 x + B A x. $$Where:\n$A \\in \\mathbb{R}^{r \\times k}$ (Down-projection matrix): Maps the input from $k$ dimensions to a lower $r$ dimension; $B \\in \\mathbb{R}^{d \\times r}$ (Up-projection matrix): Maps the reduced-dimension features from $r$ dimensions back to the original $d$ dimensions; Input $x$: Has dimension $\\mathbb{R}^{k}$; Original weight $W_0$: Has dimension $\\mathbb{R}^{d \\times k}$, thus $W_0 x \\in \\mathbb{R}^{d}$; Assuming the pre-trained weight matrix is $$ \\mathbf{W} \\in \\mathbb{R}^{d \\times k}, $$ LoRA adds a low-rank update term to it, resulting in the new weight representation:\n$$ \\mathbf{W}' = \\mathbf{W} + \\alpha\\, \\mathbf{B}\\mathbf{A}, $$Where:\n$\\mathbf{A} \\in \\mathbb{R}^{r \\times k}$ (Down-projection matrix): Maps the input from $k$ dimensions to a lower $r$ dimension; $\\mathbf{B} \\in \\mathbb{R}^{d \\times r}$ (Up-projection matrix): Maps the reduced-dimension features from $r$ dimensions back to the original $d$ dimensions; $r \\ll \\min(d, k)$ (Low rank dimension): Typically chosen from $4$ to $16$, balancing model expressiveness with minimizing added parameters; $\\alpha$ (Scaling factor): Used to scale the low-rank update $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$, compensating for the potentially small numerical magnitude resulting from the low-rank decomposition (often set to $\\alpha = 2 \\times r$, e.g., $\\alpha = 16$ when $r = 8$). During the fine-tuning process, the original weights $\\mathbf{W}$ are frozen, and only $\\mathbf{A}$ and $\\mathbf{B}$ are updated. This significantly reduces the number of trainable and storable parameters.\nTo ensure that the update term $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$ introduced at the beginning of fine-tuning has minimal impact on the original model, the following initialization strategies are commonly used:\nInitialization of the down-projection matrix $\\mathbf{A}$\nGaussian Initialization: Set $\\mathbf{A} \\sim \\mathcal{N}(0,\\sigma^2)$ (typically with a small $\\sigma$, e.g., 0.02) to ensure the initial update is small enough not to severely disrupt the model\u0026rsquo;s output. Kaiming (He) Initialization: Kaiming initialization is a weight initialization method designed specifically for deep networks, aiming to maintain stability of forward signals and backward gradients across layers. For LoRA, ensuring a small scale (or using an appropriate scaling factor $\\alpha$) can make the initial $\\Delta \\mathbf{W}$ close to zero. Initialization of the up-projection matrix $\\mathbf{B}$\nTypically, $\\mathbf{B}$ is initialized as a zero matrix, so that initially $\\mathbf{B}\\mathbf{A} = 0$, further minimizing the impact on the original model. Training with LoRA offers the following advantages:\nParameter Efficiency: Only introduces low-rank adapter parameters, reducing the total number of parameters that need to be trained and stored. Memory and Computation Efficiency: Freezes most pre-trained weights and updates only small-scale parameters during fine-tuning, significantly reducing memory footprint and computational overhead. No Additional Inference Latency: After training, the update term $\\Delta \\mathbf{W}$ can be merged back into the original weights ($\\mathbf{W}\u0026rsquo; = \\mathbf{W} + \\alpha \\mathbf{B}\\mathbf{A}$), so no extra computation is added during the inference phase. Module Selection Flexibility: Using parameters like --lora_target or --lora-target, users can specify applying LoRA updates only to specific linear modules. Supported target modules include: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj. This design allows users to selectively fine-tune key modules based on specific task requirements, further enhancing fine-tuning efficiency and adaptability. QLoRA QLoRA (Dettmers et al. 2023) is a method for efficient fine-tuning of large-scale models based on LoRA combined with quantization ideas. Through the following three key improvements, it greatly reduces memory footprint while maintaining basically unchanged model accuracy:\n4-bit Normal Float (NF4) Quantization A block-based quantile quantization strategy is adopted to quantize the original model weights to 4 bits, thereby achieving significant storage compression with subtle loss of accuracy.\nDouble Quantization After performing quantization once on ordinary parameters, perform an additional quantization on the quantization constants to further reduce cache footprint.\nPaged Optimizer When memory usage is too high, automatically transfer part of the optimization process to CPU memory, thereby alleviating GPU memory pressure and improving scalability.\nDifferent from traditional LoRA, which only reduces the number of parameters to be fine-tuned, QLoRA also compresses all weights through 4-bit quantization, thereby maximizing the reduction of memory footprint and data transmission overhead while ensuring near-original accuracy.\nFig. 32. Different finetuning methods and their memory requirements. QLoRA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. (Image source: Dettmers et al. 2023)\nThis method can be regarded as a further extension of LoRA: LoRA improves efficiency by reducing the number of weights that need to be fine-tuned, while QLoRA, on this basis, quantizes all weights (including the un-fine-tuned part) to 4-bit precision, achieving dual compression of storage and computation in general, which is suitable for efficient fine-tuning of LLMs in resource-constrained environments.\nSummary Parallelism techniques and memory optimization strategies need to be weighed and selected according to the specific model structure, dataset size, hardware resources, and training goals. Usually, it is necessary to combine multiple technologies to effectively train large-scale models and achieve the best performance and efficiency.\nReferences [1] Weng, Lilian, and Greg Brockman. \u0026ldquo;Techniques for training large neural networks.\u0026rdquo; OpenAI Blog, 2022.\n[2] Shenggui Li, Siqi Mai. \u0026ldquo;Paradigms of Parallelism.\u0026rdquo; Colossal-AI Documentation, 2024.\n[3] Li, Shen, et al. \u0026ldquo;Pytorch distributed: Experiences on accelerating data parallel training.\u0026rdquo; arXiv preprint, 2020.\n[4] Li, Mu, et al. \u0026ldquo;Communication efficient distributed machine learning with the parameter server.\u0026rdquo; Advances in Neural Information Processing Systems 27, 2014.\n[5] Huang, Yanping, et al. \u0026ldquo;Gpipe: Efficient training of giant neural networks using pipeline parallelism.\u0026rdquo; Advances in Neural Information Processing Systems 32, 2019.\n[6] Harlap, Aaron, et al. \u0026ldquo;Pipedream: Fast and efficient pipeline parallel dnn training.\u0026rdquo; arXiv preprint, 2018.\n[7] Narayanan, Deepak, et al. \u0026ldquo;Memory-efficient pipeline-parallel dnn training.\u0026rdquo; International Conference on Machine Learning, PMLR, 2021.\n[8] Shoeybi, Mohammad, et al. \u0026ldquo;Megatron-lm: Training multi-billion parameter language models using model parallelism.\u0026rdquo; arXiv preprint, 2019.\n[9] Narayanan, Deepak, et al. \u0026ldquo;Efficient large-scale language model training on gpu clusters using megatron-lm.\u0026rdquo; Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021.\n[10] Shazeer, Noam, et al. \u0026ldquo;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\u0026rdquo; arXiv preprint, 2017.\n[11] Lepikhin, Dmitry, et al. \u0026ldquo;Gshard: Scaling giant models with conditional computation and automatic sharding.\u0026rdquo; arXiv preprint, 2020.\n[12] Fedus, William, Barret Zoph, and Noam Shazeer. \u0026ldquo;Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\u0026rdquo; Journal of Machine Learning Research 23.120, 2022: 1–39.\n[13] Zhou, Yanqi, et al. \u0026ldquo;Mixture-of-experts with expert choice routing.\u0026rdquo; Advances in Neural Information Processing Systems 35, 2022: 7103–7114.\n[14] Li, Shenggui, et al. \u0026ldquo;Sequence parallelism: Long sequence training from system perspective.\u0026rdquo; arXiv preprint, 2021.\n[15] Korthikanti, Vijay Anand, et al. \u0026ldquo;Reducing activation recomputation in large transformer models.\u0026rdquo; Proceedings of Machine Learning and Systems 5, 2023: 341–353.\n[16] Jacobs, Sam Ade, et al. \u0026ldquo;Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.\u0026rdquo; arXiv preprint, 2023.\n[17] DeepSpeed. \u0026ldquo;DeepSpeed Ulysses README.\u0026rdquo; GitHub repository.\n[18] Rajbhandari, Samyam, et al. \u0026ldquo;Zero: Memory optimizations toward training trillion parameter models.\u0026rdquo; SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, 2020.\n[19] Microsoft Research. \u0026ldquo;DeepSpeed: Extreme-scale model training for everyone.\u0026rdquo; 2020.\n[20] Dubey, Abhimanyu, et al. \u0026ldquo;The llama 3 herd of models.\u0026rdquo; arXiv preprint, 2024.\n[21] Rhu, Minsoo, et al. \u0026ldquo;vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design.\u0026rdquo; 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture(MICRO), IEEE, 2016.\n[22] Chen, Tianqi, et al. \u0026ldquo;Training deep nets with sublinear memory cost.\u0026rdquo; arXiv preprint, 2016.\n[23] Micikevicius, Paulius, et al. \u0026ldquo;Mixed precision training.\u0026rdquo; arXiv preprint, 2017.\n[24] Jain, Animesh, et al. \u0026ldquo;Gist: Efficient data encoding for deep neural network training.\u0026rdquo; 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture(ISCA), IEEE, 2018.\n[25] Kingma, Diederik P., and Jimmy Ba. \u0026ldquo;Adam: A method for stochastic optimization.\u0026rdquo; arXiv preprint, 2014.\n[26] Shazeer, Noam, and Mitchell Stern. \u0026ldquo;Adafactor: Adaptive learning rates with sublinear memory cost.\u0026rdquo; International Conference on Machine Learning, PMLR, 2018.\n[27] Ginsburg, Boris, et al. \u0026ldquo;Stochastic gradient methods with layer-wise adaptive moments for training of deep networks.\u0026rdquo; arXiv preprint, 2019.\n[28] Hu, Edward J., et al. \u0026ldquo;LoRA: Low-rank adaptation of large language models.\u0026rdquo; ICLR, 2022: 3.\n[29] Dettmers, Tim, et al. \u0026ldquo;Qlora: Efficient finetuning of quantized llms.\u0026rdquo; Advances in Neural Information Processing Systems 36, 2023: 10088–10115.\n[30] Weng, Lilian. \u0026ldquo;How to Train Really Large Models on Many GPUs?\u0026rdquo; Lil\u0026rsquo;blog, 2021.\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui.(Mar 2025). Parallelism and Memory Optimization Techniques for Training Large Models. https://syhya.github.io/posts/2025-03-01-train-llm\nOr\n@article{syhya2025train-llm, title = \u0026#34;Parallelism and Memory Optimization Techniques for Training Large Models\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Mar\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-03-01-train-llm\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-03-01-train-llm/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eRecently, the number of parameters in large models has been continuously increasing, from the initial billions to today\u0026rsquo;s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models.\u003c/p\u003e","title":"Parallelism and Memory Optimization Techniques for Training Large Models"},{"content":"This blog post introduces a streamlined alternative to RLHF called DPO. Like RLHF, DPO is designed to align model outputs with human preferences, but it stands apart with its simplicity and lower resource demands. In scenarios where project resources are limited, DPO emerges as a highly attractive and practical solution worth exploring.\nNotations Symbol \\( x \\) User input (Prompt): the question the model needs to answer \\( y \\) Model-generated response (Response / Completion): the text output by the model \\( \\pi_\\theta(y \\mid x) \\) Actor model: The trainable policy used to generate response \\(y\\); parameterized by \\(\\theta\\) \\( \\pi_{\\mathrm{ref}}(y \\mid x) \\) Reference model: The frozen SFT (Supervised Fine-Tuning) model, serving as the alignment baseline \\( r_\\phi(x,y) \\) Reward model: A reward function (with parameter \\(\\phi\\)) used to evaluate the quality of response \\(y\\) \\( V_\\psi(x) \\) Critic model: A value function (with parameter \\(\\psi\\)) used to estimate the future cumulative reward given \\(x\\) \\( \\pi^*(y \\mid x) \\) Optimal policy distribution, determined via the reference model and reward function \\( r_\\theta(x,y) \\) Reward derived from the Actor model, constructed from \\(\\pi_\\theta\\) and \\(\\pi_{\\mathrm{ref}}\\) \\(\\beta\\) Hyperparameter that controls the weight of the KL penalty or the log-ratio difference term \\(\\mathbb{D}_{\\mathrm{KL}}[P \\| Q]\\) KL divergence, a measure of the difference between probability distributions \\(P\\) and \\(Q\\) \\(\\sigma(z)\\) Sigmoid function, defined as: \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\) \\(\\log\\) Logarithm function \\(\\mathbb{E}\\) Expectation operator, used to compute the average value of a random variable \\( (y_w, y_l) \\) A pair of preference data where \\( y_w \\) is the preferred (better quality) response and \\( y_l \\) is the lesser one \\( P\\left(y_w \\succ y_l \\mid x\\right) \\) The probability that response \\( y_w \\) is preferred over \\( y_l \\) given input \\(x\\) \\( Z(x) \\) Partition function, which normalizes the probability distribution over all responses \\(y\\) \\( \\mathcal{L}_{\\mathrm{DPO}} \\) The loss function of DPO From RLHF to DPO RLHF OpenAI primarily leverages Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) to train InstructGPT (Ouyang et al., 2022), which forms the basis for LLMs (such as ChatGPT, Llama, etc.). The entire training process generally comprises the following three main steps:\nFig. 1. A diagram illustrating the three steps of InstructGPT. (Image source: Ouyang et al., 2022)\nSupervised Fine-Tuning (SFT)\nA pre-trained model is fine-tuned using a large volume of human-annotated examples, resulting in an initial model capable of understanding instructions and generating reasonable responses. This model is referred to as the reference model, \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\).\nReward Model Training\nFor simplicity, assume that for each input \\(x\\), two distinct responses are generated. In practice, multiple responses can be ranked. Two responses \\(y_w\\) (better) and \\(y_l\\) (worse) are generated for the same input \\(x\\), and human ranking provides the preference data. A reward model \\(r_\\phi(x, y)\\) is then trained on this data to predict which response aligns better with human preferences.\nPPO-Based Reinforcement Learning\nUsing feedback from the reward model \\(r_\\phi\\), the Actor model \\(\\pi_\\theta\\) is optimized via the Proximal Policy Optimization (PPO) algorithm to improve response quality. To prevent the model from deviating too far from \\(\\pi_{\\mathrm{ref}}\\), a KL penalty is added during optimization. This stage typically involves the following four models:\n\\(\\pi_\\theta\\): The model (after SFT) that is updated. \\(\\pi_{\\mathrm{ref}}\\): The frozen SFT model used as the alignment baseline. \\(r_\\phi\\): The fixed reward model for evaluating response quality. \\(V_\\psi\\): The critic model that estimates future rewards to assist the update of the Actor model. Limitations of RLHF While RLHF leverages human preference data to enhance model alignment, it comes with several inherent limitations:\nMulti-Model Training: In addition to the Actor model \\(\\pi_\\theta\\), extra models such as the reward model \\(r_\\phi\\) and the Critic model \\(V_\\psi\\) must be trained, making the overall process complex and resource-intensive. High Sampling Cost: LLMs require significant computational resources to generate text. The extensive online sampling during reinforcement learning further increases computational costs; insufficient sampling may lead to suboptimal optimization directions. Training Instability and Hyperparameter Sensitivity: PPO involves numerous hyperparameters (e.g., learning rate, sampling batch size), making tuning complex and the training process prone to instability. Alignment Tax Effect: While improving model alignment, the performance on other tasks may suffer. Introduction to DPO Fig. 2. DPO optimizes for human preferences while avoiding reinforcement learning. (Image source: Rafailov et al., 2023)\nDirect Preference Optimization (DPO) (Rafailov et al., 2023) algorithm was developed to address the above issues of RLHF. Its core idea is to convert the RLHF objective into a contrastive learning task akin to supervised fine-tuning, thereby achieving the following:\nEliminating Reward Model Training: Directly optimize the Actor model \\(\\pi_\\theta\\) using human preference data, without training a separate \\(r_\\phi\\). Removing Reinforcement Learning Sampling: Replace PPO with a contrastive loss function, reducing sampling and computational overhead. Enhancing Training Stability: The supervised learning approach is less sensitive to hyperparameters, leading to a more stable training process. Although DPO might have a lower performance ceiling compared to RLHF in terms of ultimate LLM performance improvements, it offers advantages in resource utilization, reduced implementation complexity, and training stability.\nMethod Comparison Method Training Steps Models Involved Training Approach Advantages Disadvantages RLHF Train a reward model first, then use PPO to optimize the policy \\(\\pi_\\theta\\), \\(\\pi_{\\mathrm{ref}}\\), \\(r_\\phi\\), \\(V_\\psi\\) Reinforcement learning with online sampling Fully leverages human preferences; higher performance potential Resource intensive; unstable training; hyperparameter sensitive DPO Directly train the Actor model using preference data \\(\\pi_\\theta\\), \\(\\pi_{\\mathrm{ref}}\\) Supervised-learning-like approach Simplified process; stable training; lower resource cost Performance ceiling may be lower than RLHF Mathematical Derivation of DPO RLHF Objective and the Optimal Policy Distribution In the alignment of large language models, our goal is to use RLHF to optimize model outputs. Let the input \\( x \\) be drawn from a dataset \\(\\mathcal{D}\\), and let the model generate a response \\( y \\). Denote the trainable model as \\(\\pi_\\theta(y \\mid x)\\) and the reference model as \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\) (typically the SFT model). We also introduce a reward function \\( r(x,y) \\) to measure the quality of a response. The RLHF objective can be written as\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\Big[ r(x,y) \\Big] \\;-\\; \\beta\\, \\mathbb{D}_{\\mathrm{KL}}\\Big[ \\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x) \\Big], \\tag{1} \\]where \\(\\beta\\) is a hyperparameter that balances the reward and the deviation from the reference model. Using the definition of KL divergence,\n\\[ \\mathbb{D}_{\\mathrm{KL}} \\Big[\\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\Big] = \\mathbb{E}_{y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right], \\tag{2} \\]we can rewrite Equation (1) as\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ r(x,y) - \\beta \\, \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right]. \\tag{3} \\]Converting (3) to a minimization problem and dividing by \\(\\beta\\) yields\n\\[ \\min_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} - \\frac{1}{\\beta} r(x,y) \\right]. \\tag{4} \\]Assuming there exists an optimal policy distribution \\(\\pi^*(y \\mid x)\\) that globally minimizes (4), we set\n\\[ \\pi^*(y \\mid x) \\;=\\; \\frac{1}{Z(x)} \\,\\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big), \\tag{5} \\]where the partition function \\( Z(x) \\) is defined as\n\\[ Z(x) = \\sum_{y}\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big). \\tag{6} \\] \\(Z(x)\\) sums over all possible \\(y\\) to normalize the distribution, ensuring that \\(\\pi^*(y \\mid x)\\) is a valid probability distribution. \\(Z(x)\\) is a function of \\(x\\) and is independent of the trainable Actor model \\(\\pi_\\theta\\). Taking the logarithm of (5) gives\n\\[ \\log \\pi^*(y \\mid x) = \\log \\pi_{\\mathrm{ref}}(y \\mid x) + \\frac{1}{\\beta}\\, r(x,y) - \\log Z(x), \\tag{7} \\]which can be rearranged to obtain\n\\[ r(x,y) = \\beta \\left[\\log \\frac{\\pi^*(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{8} \\]The Bradley–Terry Model To leverage pairwise preference data \\((x, y_w, y_l)\\) for training, we aim for the model to favor the higher-quality response \\( y_w \\) over the lower-quality response \\( y_l \\) for the same input \\( x \\).\nThe Bradley–Terry model is used to predict the outcomes of paired comparisons. For any two items \\( i \\) and \\( j \\), if we assign each a positive score \\( p_i \\) and \\( p_j \\), then the probability that item \\( i \\) is preferred over item \\( j \\) is\n\\[ \\Pr(i \u003e j) = \\frac{p_i}{p_i + p_j}. \\tag{9} \\]In our scenario, we set the strength parameter for each response \\( y \\) as \\( p_{y} = \\exp\\big(r(x,y)\\big) \\) (ensuring positivity). Therefore, given input \\( x \\), the probability that response \\( y_w \\) is preferred over \\( y_l \\) becomes\n\\[ P\\left(y_w \\succ y_l \\mid x\\right)=\\frac{\\exp \\big[r(x,y_w)\\big]}{\\exp \\big[r(x,y_w)\\big]+\\exp \\big[r(x,y_l)\\big]}. \\tag{10} \\]To maximize the probability that the higher-quality response \\( y_w \\) wins in every preference pair \\((x, y_w, y_l)\\) in the dataset, we design the reward model’s training objective to maximize this probability or, equivalently, to minimize the negative log-likelihood:\n\\[ L_{R}\\left(r_{\\phi}, D\\right) = -\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\left[\\log P\\left(y_w \\succ y_l \\mid x\\right)\\right], \\tag{11} \\]where the dataset is defined as\n\\[ D=\\{(x^i, y_w^i, y_l^i)\\}_{i=1}^{N}. \\tag{12} \\]Using Equations (10) and (11) along with the identity\n\\[ \\log \\frac{e^a}{e^a+e^b} = \\log\\frac{1}{1+e^{b-a}} = \\log \\sigma(a-b), \\tag{13} \\]with the Sigmoid function defined as\n\\[ \\sigma(z)=\\frac{1}{1+e^{-z}}, \\tag{14} \\]we have\n\\[ \\log P\\left(y_w \\succ y_l \\mid x\\right) = \\log \\sigma\\Big(r(x,y_w)-r(x,y_l)\\Big). \\tag{15} \\]Direct Preference Optimization Notice from Equation (8) that the reward \\( r(x,y) \\) is related to the log-ratio of the optimal policy. To avoid explicitly training a separate reward model \\(r_\\phi\\), DPO directly substitutes the trainable Actor model \\(\\pi_\\theta\\) in place of the optimal policy \\(\\pi^*\\) and represents the reward as\n\\[ r_\\theta(x,y) \\;=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{16} \\]In pairwise comparisons, for the same input \\( x \\), both responses \\( y_w \\) and \\( y_l \\) contain the same \\(\\log Z(x)\\) term; therefore, when computing the difference, this term cancels out:\n\\[ \\begin{aligned} r_\\theta(x,y_w)-r_\\theta(x,y_l) \u0026=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} + \\log Z(x)\\right] - \\beta \\left[\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} + \\log Z(x)\\right] \\\\ \u0026=\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)}. \\end{aligned} \\tag{17} \\]Substituting Equation (17) into (15) and combining with (11), we obtain the final DPO loss function:\n\\[ \\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}}) = - \\mathbb{E}_{(x,y_w,y_l) \\sim D} \\left[ \\log \\sigma\\Big( \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} -\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big) \\right]. \\tag{18} \\]This loss function is designed for the trainable Actor model \\(\\pi_\\theta\\). It distinguishes between good and bad responses by comparing the log-probability ratios (relative to the reference model \\(\\pi_{\\mathrm{ref}}\\)) for the high-quality response \\(y_w\\) and the low-quality response \\(y_l\\): when \\(\\pi_\\theta\\) assigns a much higher relative probability to \\(y_w\\) compared to \\(y_l\\), the sigmoid output approaches 1 and the loss decreases; conversely, if the relative probability for \\(y_l\\) is too high, the loss increases. The parameter \\(\\beta\\) scales the difference between these log-ratios, thereby adjusting the model’s sensitivity in distinguishing good from bad responses.\nTraining Process Data Preparation Preference Data Collection\nCollect a large amount of preference data. Each sample includes an input \\( x \\) and a pair of responses \\((y_w, y_l)\\), where \\( y_w \\) is the chosen (preferred) response and \\( y_l \\) is the rejected (lower quality) response.\nDataset Example: dpo_en_demo.json\n{ \u0026#34;conversations\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;human\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;what does abstraction mean on oop\u0026#34; } ], \u0026#34;chosen\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Abstraction in Object-Oriented Programming (OOP) is the process of exposing only the necessary information to the outside world while hiding the implementation details. It\u0026#39;s a way to simplify complex systems by focusing only on the essential aspects and encapsulating the implementation details.\\n\\nIn other words, abstraction is about hiding the internal workings of an object or system and only exposing the necessary interfaces or APIs to interact with it. This helps to reduce complexity, improve modularity, and make the system easier to understand and maintain.\\n\\nFor example, consider a car. A car has many components like engine, transmission, brakes, etc. However, when you drive a car, you don\u0026#39;t need to know how the engine works or how the transmission changes gears. All you need to know is how to operate the car\u0026#39;s controls like accelerator, brakes, and steering wheel. The car\u0026#39;s controls are an abstraction of the underlying complex systems, allowing you to interact with the car without needing to know the details of how it works.\\n\\nIn programming, abstraction is achieved by defining interfaces or abstract classes that define the essential functions or behaviors of an object without specifying how they are implemented. This allows developers to create different implementations of the same interface or abstract class, which can be swapped out without affecting the rest of the system.\\n\\nAbstraction is a fundamental concept in OOP and is used to create modular, maintainable, and flexible software systems. It helps to reduce code duplication, improve code reuse, and make software systems more understandable and easier to evolve over time.\u0026#34; }, \u0026#34;rejected\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Abstraction is a principle in object-oriented programming that refers to the process of focusing on the essential features of an object or concept without emphasizing the details that are not important for its functionality. It enables programmers to create classes that represent the objects they want to work with, and specifies only the behavior of those objects, leaving out unnecessary details. Abstraction helps to make the code more maintainable, modular, and scalable. It also improves the productivity of developers by reducing the amount of code they need to write.\u0026#34; } } Output Precomputation\nTo reduce computational overhead, pre-compute and cache the outputs of the reference model \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\).\nModel Training Training Objective\nDirectly optimize the Actor model \\(\\pi_\\theta\\) by minimizing the DPO loss \\(\\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}})\\) so that its generated responses better align with human preferences.\nTraining Steps\nSample a batch of data \\((x, y_w, y_l)\\) from the dataset.\nCompute the output probabilities of the Actor model \\(\\pi_\\theta(y \\mid x)\\).\nCalculate the loss using:\n\\[ \\mathcal{L}_{\\mathrm{DPO}} = - \\log \\sigma\\Big( \\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big). \\] Update the Actor model parameters \\(\\theta\\) via backpropagation.\nModel Inference Once training is complete, the resulting Actor model \\(\\pi_\\theta\\) can be used directly for inference. Given an input \\( x \\), the model generates responses based on the learned probability distribution. Since human preferences have been incorporated during training and the model is constrained by the reference model \\(\\pi_{\\mathrm{ref}}\\), the generated responses are not only aligned with expectations but also maintain stability in the generated text.\nSummary DPO simplifies the RLHF process into a direct supervised learning task, saving resources, enhancing training stability, and reducing implementation complexity. It serves as an efficient alternative for LLM alignment training. In practical applications, one can choose between RLHF and DPO methods based on the specific business scenario to achieve the best training results.\nReferences [1] Christiano, Paul F., et al. \u0026ldquo;Deep reinforcement learning from human preferences.\u0026rdquo; Advances in neural information processing systems 30 (2017).\n[2] Ouyang, Long, et al. \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in neural information processing systems 35 (2022): 27730-27744.\n[3] Rafailov, Rafael, et al. \u0026ldquo;Direct preference optimization: Your language model is secretly a reward model.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\nCitation Citation: When reprinting or citing the contents of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Feb 2025). LLMs Alignment: DPO.\nhttps://syhya.github.io/posts/2025-02-08-dpo\nOr\n@article{syhya2025dpo, title = \u0026#34;LLMs Alignment: DPO\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-02-08-dpo\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-02-08-dpo/","summary":"\u003cp\u003eThis blog post introduces a streamlined alternative to RLHF called DPO. Like RLHF, DPO is designed to align model outputs with human preferences, but it stands apart with its simplicity and lower resource demands. In scenarios where project resources are limited, DPO emerges as a highly attractive and practical solution worth exploring.\u003c/p\u003e\n\u003ch2 id=\"notations\"\u003eNotations\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eSymbol\u003c/th\u003e\n          \u003cth\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( x \\)\u003c/td\u003e\n          \u003ctd\u003eUser input (Prompt): the question the model needs to answer\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( y \\)\u003c/td\u003e\n          \u003ctd\u003eModel-generated response (Response / Completion): the text output by the model\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( \\pi_\\theta(y \\mid x) \\)\u003c/td\u003e\n          \u003ctd\u003eActor model: The trainable policy used to generate response \\(y\\); parameterized by \\(\\theta\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( \\pi_{\\mathrm{ref}}(y \\mid x) \\)\u003c/td\u003e\n          \u003ctd\u003eReference model: The frozen SFT (Supervised Fine-Tuning) model, serving as the alignment baseline\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( r_\\phi(x,y) \\)\u003c/td\u003e\n          \u003ctd\u003eReward model: A reward function (with parameter \\(\\phi\\)) used to evaluate the quality of response \\(y\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( V_\\psi(x) \\)\u003c/td\u003e\n          \u003ctd\u003eCritic model: A value function (with parameter \\(\\psi\\)) used to estimate the future cumulative reward given \\(x\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( \\pi^*(y \\mid x) \\)\u003c/td\u003e\n          \u003ctd\u003eOptimal policy distribution, determined via the reference model and reward function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( r_\\theta(x,y) \\)\u003c/td\u003e\n          \u003ctd\u003eReward derived from the Actor model, constructed from \\(\\pi_\\theta\\) and \\(\\pi_{\\mathrm{ref}}\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\(\\beta\\)\u003c/td\u003e\n          \u003ctd\u003eHyperparameter that controls the weight of the KL penalty or the log-ratio difference term\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\(\\mathbb{D}_{\\mathrm{KL}}[P \\| Q]\\)\u003c/td\u003e\n          \u003ctd\u003eKL divergence, a measure of the difference between probability distributions \\(P\\) and \\(Q\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\(\\sigma(z)\\)\u003c/td\u003e\n          \u003ctd\u003eSigmoid function, defined as: \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\(\\log\\)\u003c/td\u003e\n          \u003ctd\u003eLogarithm function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\(\\mathbb{E}\\)\u003c/td\u003e\n          \u003ctd\u003eExpectation operator, used to compute the average value of a random variable\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( (y_w, y_l) \\)\u003c/td\u003e\n          \u003ctd\u003eA pair of preference data where \\( y_w \\) is the preferred (better quality) response and \\( y_l \\) is the lesser one\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( P\\left(y_w \\succ y_l \\mid x\\right) \\)\u003c/td\u003e\n          \u003ctd\u003eThe probability that response \\( y_w \\) is preferred over \\( y_l \\) given input \\(x\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( Z(x) \\)\u003c/td\u003e\n          \u003ctd\u003ePartition function, which normalizes the probability distribution over all responses \\(y\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( \\mathcal{L}_{\\mathrm{DPO}} \\)\u003c/td\u003e\n          \u003ctd\u003eThe loss function of DPO\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"from-rlhf-to-dpo\"\u003eFrom RLHF to DPO\u003c/h2\u003e\n\u003ch3 id=\"rlhf\"\u003eRLHF\u003c/h3\u003e\n\u003cp\u003eOpenAI primarily leverages \u003cstrong\u003eReinforcement Learning from Human Feedback (RLHF)\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/1706.03741\"\u003eChristiano et al., 2017\u003c/a\u003e) to train InstructGPT (\u003ca href=\"https://arxiv.org/abs/2203.02155\"\u003eOuyang et al., 2022\u003c/a\u003e), which forms the basis for LLMs (such as ChatGPT, Llama, etc.). The entire training process generally comprises the following three main steps:\u003c/p\u003e","title":"LLMs Alignment: DPO"},{"content":"Introduction In deep learning, the design of network architectures significantly impacts model performance and training efficiency. As model depth increases, training deep neural networks faces numerous challenges, such as the vanishing and exploding gradient problems. To address these challenges, residual connections and various normalization methods have been introduced and are widely used in modern deep learning models. This article will first introduce residual connections and two architectures: pre-norm and post-norm. Then, it will describe four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze why current mainstream large models tend to adopt an architecture combining RMSNorm and Pre-Norm.\nResidual Connections Residual Connection is a crucial innovation in deep neural networks, forming the core of Residual Networks (ResNet) (He, et al., 2015). Residual connections are a significant architectural design aimed at mitigating the vanishing gradient problem in deep network training and facilitating information flow within the network. By introducing shortcut/skip connections, they allow information to pass directly from shallow layers to deeper layers, thereby enhancing the model\u0026rsquo;s representational capacity and training stability.\nFig. 1. Residual learning: a building block. (Image source: He, et al., 2015)\nIn a standard residual connection, the input $x_l$ undergoes a series of transformation functions $ \\text{F}(\\cdot) $ and is then added to the original input $x_l$ to form the output $x_{l+1}$:\n$$ x_{l+1} = x_l + \\text{F}(x_l) $$Where:\n$x_l$ is the input to the $l$-th layer. $\\text{F}(x_l)$ represents the residual function composed of a series of non-linear transformations (e.g., convolutional layers, fully connected layers, activation functions, etc.). $x_{l+1}$ is the output of the $(l+1)$-th layer. The structure using residual connections has several advantages:\nMitigation of Vanishing Gradients: By directly passing gradients through shortcut paths, it effectively reduces gradient decay in deep networks, making it easier to train deeper models. Facilitation of Information Flow: Shortcut paths allow information to flow more freely between network layers, helping the network learn more complex feature representations. Optimization of the Learning Process: Residual connections make the loss function surface smoother, optimizing the model\u0026rsquo;s learning process and making it easier to converge to a better solution. Improvement of Model Performance: In various deep learning tasks such as image recognition and natural language processing, models using residual connections typically exhibit superior performance. Pre-Norm vs. Post-Norm When discussing normalization methods, Pre-Norm and Post-Norm are two critical architectural design choices, particularly prominent in Transformer models. The following will detail the definitions, differences, and impacts of both on model training.\nDefinitions Fig. 2. (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer. (Image source: Xiong, et al., 2020)\nFrom the figure above, we can intuitively see that the main difference between Post-Norm and Pre-Norm lies in the position of the normalization layer:\nPost-Norm: In traditional Transformer architectures, the normalization layer (such as LayerNorm) is typically placed after the residual connection.\n$$ \\text{Post-Norm}: \\quad x_{l+1} = \\text{Norm}(x_l + \\text{F}(x_l)) $$ Pre-Norm: Places the normalization layer before the residual connection.\n$$ \\text{Pre-Norm}: \\quad x_{l+1} = x_l + \\text{F}(\\text{Norm}(x_l)) $$ Comparative Analysis Feature Post-Norm Pre-Norm Normalization Position After residual connection Before residual connection Gradient Flow May lead to vanishing or exploding gradients, especially in deep models More stable gradients, helps in training deep models Training Stability Difficult to train deep models, requires complex optimization techniques Easier to train deep models, reduces reliance on learning rate scheduling Information Transfer Retains characteristics of the original input, aiding information transfer May cause compression or loss of input feature information Model Performance Performs better in shallow models or when strong regularization is needed Performs better in deep models, improves training stability and convergence speed Implementation Complexity Relatively straightforward to implement, but training may require more tuning Simple to implement, training process is more stable The differences between Pre-Norm and Post-Norm in model training can be understood from the perspective of gradient backpropagation:\nPre-Norm: Normalization operation is performed first, allowing gradients to be passed more directly to the preceding layers during backpropagation, reducing the risk of vanishing gradients. However, this may also weaken the actual contribution of each layer, reducing the effective depth of the model.\nPost-Norm: Normalization operation is performed last, helping to maintain the stability of each layer\u0026rsquo;s output, but in deep models, gradients may decay layer by layer, leading to training difficulties.\nThe DeepNet (Wang, et al., 2022) paper indicates that Pre-Norm is effective for training extremely deep Transformer models, while Post-Norm is difficult to scale to such depths.\nNormalization Methods In deep learning, there are numerous types of normalization methods, and different methods perform differently in various application scenarios. The following will detail four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze their advantages, disadvantages, and applicable scenarios.\nBatch Normalization Batch Normalization (Ioffe, et al., 2015) aims to alleviate the Internal Covariate Shift problem by standardizing the data of each batch, making its mean 0 and variance 1. Its mathematical expression is as follows:\n$$ \\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta $$Where:\n$x_i$ is the $i$-th sample in the input vector. $\\mu_{\\text{B}}$ is the mean of the current batch: $$ \\mu_{\\text{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i $$ where $m$ is the batch size. $\\sigma_{\\text{B}}^2$ is the variance of the current batch: $$ \\sigma_{\\text{B}}^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\left(x_i - \\mu_{\\text{B}}\\right)^2 $$ $\\epsilon$ is a very small constant used to prevent division by zero. $\\gamma$ and $\\beta$ are learnable scaling and shifting parameters. Advantages:\nAccelerated Training: Accelerates the convergence speed of the model through standardization. Regularization Effect: Reduces overfitting to some extent, decreasing the reliance on regularization techniques like Dropout. Mitigation of Vanishing Gradient Problem: Helps alleviate vanishing gradients, improving the training effect of deep networks. Disadvantages:\nNot Friendly to Small Batches: When the batch size is small, the estimation of mean and variance may be unstable, affecting the normalization effect. Batch Size Dependent: Requires a large batch size to obtain good statistical estimates, limiting its use in certain application scenarios. Complex Application in Certain Network Structures: Such as Recurrent Neural Networks (RNNs), requiring special handling to adapt to the dependency of time steps. Layer Normalization Layer Normalization (Ba, et al., 2016) normalizes across the feature dimension, making the features of each sample have the same mean and variance. Its mathematical expression is as follows:\n$$ \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta $$Where:\n$x$ is the input vector. $\\mu_{\\text{L}}$ is the mean across the feature dimension: $$ \\mu_{\\text{L}} = \\frac{1}{d} \\sum_{i=1}^{d} x_i $$ where $d$ is the size of the feature dimension. $\\sigma_{\\text{L}}^2$ is the variance across the feature dimension: $$ \\sigma_{\\text{L}}^2 = \\frac{1}{d} \\sum_{i=1}^{d} \\left(x_i - \\mu_{\\text{L}}\\right)^2 $$ $\\epsilon$ is a very small constant used to prevent division by zero. $\\gamma$ and $\\beta$ are learnable scaling and shifting parameters. Advantages:\nBatch Size Independent: Suitable for scenarios with small batch sizes or dynamic batch sizes, especially performing excellently in sequence models. Applicable to Various Network Structures: Performs well in Recurrent Neural Networks (RNNs) and Transformer models. Simplified Implementation: No need to rely on batch statistics, simplifying implementation in distributed training. Disadvantages:\nHigher Computational Cost: Compared to BatchNorm, the overhead of calculating mean and variance is slightly higher. May Not Improve Training Speed as Much as BatchNorm: In some cases, the effect of LayerNorm may not be as significant as BatchNorm. Weight Normalization Weight Normalization (Salimans, et al., 2016) decouples the norm and direction of the weight vector in neural networks by reparameterizing it, thereby simplifying the optimization process and accelerating training to some extent. Its mathematical expression is as follows:\n$$ w = \\frac{g}{\\lVert v \\rVert} \\cdot v $$$$ \\text{WeightNorm}(x) = w^T x + b $$Where:\n$w$ is the reparameterized weight vector. $g$ is a learnable scalar scaling parameter. $v$ is a learnable direction vector (with the same dimension as the original $w$). $\\lVert v \\rVert$ represents the Euclidean norm of $v$. $x$ is the input vector. $b$ is the bias term. Advantages:\nSimplified Optimization Objective: Separately controlling the norm and direction of weights helps accelerate convergence. Stable Training Process: In some cases, it can reduce gradient explosion or vanishing problems. Implementation Independent of Batch Size: Unrelated to the batch size of input data, broader applicability. Disadvantages:\nImplementation Complexity: Requires reparameterization of network layers, which may bring additional implementation costs. Caution Needed When Combined with Other Normalization Methods: When used in conjunction with BatchNorm, LayerNorm, etc., debugging and experimentation are needed to determine the best combination. RMS Normalization RMS Normalization (Zhang, et al., 2019) is a simplified normalization method that normalizes by only calculating the Root Mean Square (RMS) of the input vector, thereby reducing computational overhead. Its mathematical expression is as follows:\n$$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$Where:\n$x$ is the input vector. $d$ is the size of the feature dimension. $\\epsilon$ is a very small constant used to prevent division by zero. $\\gamma$ is a learnable scaling parameter. Advantages:\nHigh Computational Efficiency: Compared to LayerNorm, which requires calculating both mean and variance, RMSNorm only needs to calculate the root mean square, reducing computational overhead. Training Stability: By normalizing the input, it improves the training stability of the model, allowing it to train stably even with larger learning rates. Resource Optimization: Reduced computational overhead helps deploy models in resource-constrained environments, improving training and inference efficiency. Simplified Implementation: RMSNorm is relatively simple to implement, making it easy to integrate and optimize in complex models, reducing the complexity of engineering implementation. Disadvantages:\nInformation Loss: Using only the root mean square for normalization may lose some information, such as mean information. Limited Applicability: In some tasks, it may not perform as well as BatchNorm or LayerNorm. Code Example You can refer to normalization.py\nComparison of Normalization Methods The following two tables compare the main characteristics of BatchNorm, LayerNorm, WeightNorm, and RMSNorm.\nBatchNorm vs. LayerNorm Feature BatchNorm (BN) LayerNorm (LN) Calculated Statistics Batch mean and variance Per-sample mean and variance Operation Dimension Normalizes across all samples in a batch Normalizes across all features for each sample Applicable Scenarios Suitable for large batch data, Convolutional Neural Networks (CNNs) Suitable for small batch or sequential data, RNNs or Transformers Batch Size Dependency Strongly dependent on batch size Independent of batch size, suitable for small batch or single-sample tasks Learnable Parameters Scaling parameter $ \\gamma $ and shifting parameter $ \\beta $ Scaling parameter $ \\gamma $ and shifting parameter $ \\beta $ Formula $ \\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta $ $ \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta $ Computational Complexity Requires calculating batch mean and variance Requires calculating per-sample mean and variance Example Usage CNN, Vision Transformers RNN, Transformer, NLP WeightNorm vs. RMSNorm Feature WeightNorm (WN) RMSNorm (RMS) Calculated Statistics Decomposes weight vector into norm and direction Root Mean Square (RMS) of each sample Operation Dimension Reparameterizes along the dimension of the weight vector Normalizes across all features for each sample Applicable Scenarios Suitable for scenarios requiring more flexible weight control or accelerated convergence Suitable for tasks requiring efficient computation, such as RNNs or Transformers Batch Size Dependency Independent of batch size, unrelated to the dimension of input data Independent of batch size, suitable for small batch or single-sample tasks Learnable Parameters Scalar scaling $g$ and direction vector $v$ Scaling parameter $ \\gamma $ Formula $ \\text{WeightNorm}(x) = w^T x + b $ $ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $ Computational Complexity Reparameterization and update of parameters, slightly higher overhead, but requires modifying network layer implementation Only needs to calculate the root mean square of each sample, computationally efficient Example Usage Fully connected layers, convolutional layers, etc., in deep networks, improving training stability and convergence speed Transformer, NLP, efficient sequence tasks Through the above comparison, it can be seen that the four normalization methods have their own advantages and disadvantages:\nBatchNorm performs excellently in large batch data and convolutional neural networks but is sensitive to small batches. LayerNorm is suitable for various batch sizes, especially effective in RNNs and Transformers. WeightNorm simplifies the optimization process and accelerates convergence to some extent by reparameterizing the weight vector. RMSNorm provides a lightweight alternative in scenarios requiring efficient computation. Why do current mainstream LLMs use Pre-Norm and RMSNorm? In recent years, with the rise of large-scale language models (LLMs) such as GPT, LLaMA, and the Qwen series, RMSNorm and Pre-Norm have become the standard choices for these models.\nAdvantages of RMSNorm Fig. 3. RMSNorm vs. LayerNorm: A Comparison of Time Consumption (Image source: Zhang, et al., 2019)\nHigher Computational Efficiency\nReduced Operations: Only needs to calculate the Root Mean Square (RMS) of the input vector, without calculating mean and variance. Faster Training Speed: In actual tests, RMSNorm significantly shortens training time (as shown in the figure, reduced from 665s to 501s), which is particularly evident in large-scale model training. More Stable Training\nAdaptable to Larger Learning Rates: While maintaining stability, it can use larger learning rates, accelerating model convergence. Maintains Expressive Power: By simplifying the normalization process with an appropriate scaling parameter $ \\gamma $, it still maintains model performance. Resource Saving\nReduced Hardware Requirements: Less computational overhead not only improves speed but also reduces the occupation of hardware resources, suitable for deployment in resource-constrained environments. Advantages of Pre-Norm Easier to Train Deep Models\nStable Gradient Propagation: Performing normalization before residual connections can effectively alleviate gradient vanishing or explosion. Reduced Reliance on Complex Optimization Techniques: Even if the model is very deep, the training process remains stable. Accelerated Model Convergence\nEfficient Gradient Flow: Pre-Norm makes it easier for gradients to propagate to preceding layers, resulting in faster overall convergence speed. Conclusion Residual connections and normalization methods play crucial roles in deep learning models. Different normalization methods and network architecture designs have their own applicable scenarios, advantages, and disadvantages. By introducing residual connections, ResNet successfully trained extremely deep networks, significantly improving model expressiveness and training efficiency. Meanwhile, normalization methods such as BatchNorm, LayerNorm, WeightNorm, and RMSNorm each offer different advantages, adapting to different application needs.\nAs model scales continue to expand, choosing appropriate normalization methods and network architecture designs becomes particularly important. RMSNorm, due to its efficient computation and good training stability, combined with the Pre-Norm architecture design, has become the preferred choice for current mainstream LLMs. This combination not only improves the training efficiency of models but also ensures training stability and performance under large-scale parameters.\nReferences [1] He, Kaiming, et al. \u0026ldquo;Deep residual learning for image recognition.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[2] Xiong, Ruibin, et al. \u0026ldquo;On layer normalization in the transformer architecture.\u0026rdquo; International Conference on Machine Learning. PMLR, 2020.\n[3] Wang, Hongyu, et al. \u0026ldquo;Deepnet: Scaling transformers to 1,000 layers.\u0026rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).\n[4] Ioffe, Sergey. \u0026ldquo;Batch normalization: Accelerating deep network training by reducing internal covariate shift.\u0026rdquo; arXiv preprint arXiv:1502.03167 (2015).\n[5] Ba, Jimmy Lei. \u0026ldquo;Layer normalization.\u0026rdquo; arXiv preprint arXiv:1607.06450 (2016).\n[6] Salimans, Tim, and Durk P. Kingma. \u0026ldquo;Weight normalization: A simple reparameterization to accelerate training of deep neural networks.\u0026rdquo; Advances in neural information processing systems 29 (2016).\n[7] Zhang, Biao, and Rico Sennrich. \u0026ldquo;Root mean square layer normalization.\u0026rdquo; Advances in Neural Information Processing Systems 32 (2019).\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Feb 2025). Normalization in Deep Learning.\nhttps://syhya.github.io/posts/2025-02-01-normalization\nOr\n@article{syhya2025normalization, title = \u0026#34;Normalization in Deep Learning\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-02-01-normalization\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-02-01-normalization/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn deep learning, the design of network architectures significantly impacts model performance and training efficiency. As model depth increases, training deep neural networks faces numerous challenges, such as the vanishing and exploding gradient problems. To address these challenges, residual connections and various normalization methods have been introduced and are widely used in modern deep learning models. This article will first introduce residual connections and two architectures: pre-norm and post-norm. Then, it will describe four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze why current mainstream large models tend to adopt an architecture combining \u003cstrong\u003eRMSNorm\u003c/strong\u003e and \u003cstrong\u003ePre-Norm\u003c/strong\u003e.\u003c/p\u003e","title":"Normalization in Deep Learning"},{"content":" Note: This article is currently being updated. The content is in draft version and may change. Please check back for the latest version.\nNotations Symbol Meaning \\(s, s', S_t, S_{t+1}\\) State, next state, state at time \\(t\\), state at time \\(t+1\\) \\(o, o_t\\) Observation, observation at time \\(t\\) \\(a, a', A_t, A_{t+1}\\) Action, next action, action at time \\(t\\), action at time \\(t+1\\) \\(r, r_t\\) Immediate reward, reward at time \\(t\\) \\(G_t\\) Return at time \\(t\\) \\(R(\\tau)\\) Return of a trajectory \\(\\tau\\) \\(\\mathcal{S}\\) Set of all possible states \\(\\mathcal{A}\\) Set of all possible actions \\(\\mathcal{R}\\) Set of all possible rewards \\(\\pi(a\\mid s), \\pi_\\theta(a\\mid s)\\) Policy (stochastic), parameterized policy \\(\\mu(s), \\mu_\\theta(s)\\) Policy (deterministic), parameterized policy \\(\\theta, \\phi, w\\) Policy or value function parameters \\(\\gamma\\) Discount factor \\(J(\\pi)\\) Expected return of policy \\(\\pi\\) \\(V_\\pi(s)\\) State-value function for policy \\(\\pi\\) \\(Q_\\pi(s,a)\\) Action-value function for policy \\(\\pi\\) \\(V_*(s)\\) Optimal state-value function \\(Q_*(s,a)\\) Optimal action-value function \\(A_\\pi(s,a)\\) Advantage function for policy \\(\\pi\\) \\(P(s'\\mid s,a)\\) Transition probability function \\(R(s,a,s')\\) Reward function \\(\\rho_0(s)\\) Start-state distribution \\(\\tau\\) Trajectory \\(D\\) Replay memory \\(\\alpha\\) Learning rate, temperature parameter (in SAC) \\(\\lambda\\) Eligibility trace parameter \\(\\epsilon\\) Exploration parameter (e.g., in \\(\\epsilon\\)-greedy), clipping parameter (in PPO) What is Reinforcement Learning? Definition Reinforcement Learning (RL) is a branch of machine learning that trains an agent to take a series of actions (\\(a_t\\)) in an environment, transitioning through different states (\\(s_t\\)) to achieve a long-terms goal.\nUnlike supervised learning, which relies on human-labeled data, RL depends on the interaction between the agent and the environment. After each action, the agent receives a reward (\\(r_t\\)) as feedback. The objective of the agent is to learn a policy $\\pi(s)$, which is a strategy for selecting actions, in order to maximize the total reward (\\(\\sum_{t=0}^{T} r_t\\)).\nApplications RL has achieved remarkable successes in various domains, including:\nGame Playing: Mastering complex games like Go (AlphaGo, AlphaGo Zero) and video games (Atari, Dota 2). Robotics: Controlling robots for tasks like navigation, manipulation, and locomotion. Autonomous Driving: Developing self-driving vehicles that can perceive their environment and make driving decisions. Resource Management: Optimizing resource allocation in areas like energy management and traffic control. Personalized Recommendations: Creating recommendation systems that adapt to user preferences over time. Policy A policy \\(\\pi\\) is the strategy an agent employs to decide which action to take in each state. It is the cornerstone of an RL agent, defining its behavior. Policies can be either deterministic or stochastic.\nDeterministic Policy \\(\\pi(s)\\): Maps each state to a single, specific action. For a given state \\(s\\), the policy \\(\\pi(s)\\) always selects the same action \\(a\\): $$ \\pi(s) = a $$ Stochastic Policy \\(\\pi(a \\mid s)\\): Provides a probability distribution over possible actions for each state. For a given state \\(s\\), \\(\\pi(a \\mid s)\\) represents the probability of choosing action \\(a\\). The agent samples an action based on this distribution: $$ \\pi(a \\mid s) = \\mathbb{P}_\\pi[A = a \\mid S = s] $$ In Deep Reinforcement Learning, policies are typically represented by sophisticated function approximators, such as neural networks. These networks, parameterized by weights \\(\\theta\\), learn to map states (or observations) to actions (or action probabilities). The parameterized policies are denoted as:\n\\(\\pi_\\theta(s)\\) for deterministic policies \\(\\pi_\\theta(a \\mid s)\\) for stochastic policies Trajectories A trajectory (also called episode) \\(\\tau\\) is a sequence of states and actions that unfold in an environment:\n\\[ \\tau = (s_0, a_0, s_1, a_1, \\ldots) \\]The initial state of the environment, \\(s_0\\), is sampled from a predefined start-state distribution, often represented as \\(\\rho_0\\):\n\\[ s_0 \\sim \\rho_0(\\cdot) \\]State transitions describe how the environment evolves from one state to the next, specifically from state \\(s_t\\) at time \\(t\\) to state \\(s_{t+1}\\) at time \\(t+1\\). These transitions are governed by the natural laws of the environment and are influenced solely by the most recent action taken \\(a_t\\). The transition dynamics can be either:\nDeterministic:\n\\[ s_{t+1} = f(s_t, a_t) \\] Stochastic:\n\\[ s_{t+1} \\sim P(\\cdot \\mid s_t, a_t) \\] Reward and Return In reinforcement learning, rewards and returns are fundamental concepts that guide an agent\u0026rsquo;s learning process by providing feedback on its actions.\nReward A reward is a scalar signal received by the agent after taking an action in a specific state. It serves as immediate feedback to indicate the desirability of the action taken. The reward function, denoted by \\( R \\), maps state-action pairs to real numbers:\n\\[ r_t = R(s_t, a_t) \\]where:\n\\( r_t \\) is the reward received at time step \\( t \\), \\( s_t \\) is the state at time step \\( t \\), \\( a_t \\) is the action taken at time step \\( t \\). The reward function encapsulates the goals of the agent by assigning higher rewards to desirable outcomes and lower (or negative) rewards to undesirable ones.\nReturn The return, often denoted by \\( G_t \\), represents the total accumulated future rewards from a specific time step onward. It quantifies the long-term benefit of actions taken by the agent. The return is defined as the sum of discounted rewards:\n\\[ G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\]where:\n\\( \\gamma \\in [0, 1) \\) is the discount factor that determines the present value of future rewards, \\( r_{t+k} \\) is the reward received \\( k \\) time steps after \\( t \\). The discount factor \\( \\gamma \\) balances the importance of immediate versus future rewards:\nA higher \\( \\gamma \\) (close to 1) makes the agent strive for long-term rewards. A lower \\( \\gamma \\) (close to 0) makes the agent prioritize immediate rewards. Why Use a Discount Factor? The discount factor \\( \\gamma \\) serves several important purposes in reinforcement learning:\nHandling Uncertainty of Future Rewards:\nHigher Uncertainty: Future rewards are often more uncertain than immediate rewards. For example, in the stock market, predicting long-term returns is more challenging due to market volatility. Preference for Immediate Benefits:\nHuman Behavior: As humans, we might prefer to enjoy rewards today rather than waiting for them years later. This preference is naturally modeled by discounting future rewards. Mathematical Convenience:\nFinite Computation: Discounting allows us to compute returns without needing to track future steps indefinitely, simplifying calculations and algorithms. Avoiding Infinite Loops:\nTermination Assurance: In environments with the possibility of infinite loops in state transitions, discounting ensures that the return remains finite, preventing issues with infinite sums. Finite Horizon Return In scenarios with a finite number of time steps \\( T \\), the return is calculated up to the terminal time step:\n\\[ G_t = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k} \\]This is common in episodic tasks where the interaction between the agent and the environment terminates after a certain number of steps.\nTransition Function and Reward Function In Reinforcement Learning (RL), a model encapsulates the agent’s representation of how the environment behaves. This model typically includes two core components: the transition function and the reward function.\nTransition Function The transition function \\( P(s' \\mid s, a) \\) specifies the probability of moving from state \\( s \\) to state \\( s' \\) after taking action \\( a \\). Formally:\n\\[ P(s' \\mid s, a) = \\mathbb{P}(S_{t+1} = s' \\mid S_t = s, A_t = a) \\] Deterministic Environment: In a deterministic setting, the transition function assigns a probability of 1 to a single specific next state and 0 to all others:\n\\[ P(s' \\mid s, a) = \\begin{cases} 1 \u0026 \\text{if } s' = f(s, a),\\\\ 0 \u0026 \\text{otherwise}. \\end{cases} \\] Stochastic Environment: In a stochastic environment, the transition function defines a probability distribution over possible next states:\n\\[ P(s' \\mid s, a) = \\text{Probability of transitioning to } s' \\text{ from } s \\text{ by taking action } a. \\] Reward Function The reward function \\( R(s, a, s') \\) specifies the immediate reward obtained after transitioning from state \\( s \\) to state \\( s' \\) via action \\( a \\). It provides essential feedback that guides the agent\u0026rsquo;s learning. Formally:\n\\[ R(s, a, s') = \\mathbb{E}\\bigl[ R_{t+1} \\mid S_t = s, A_t = a, S_{t+1} = s' \\bigr] \\]Depending on the problem, the reward function might depend on:\nState-Action-State:\n\\[ R(s, a, s') = \\text{Immediate reward after transitioning from } s \\text{ to } s' \\text{ using } a. \\] State-Action:\n\\[ R(s, a) = \\mathbb{E}\\bigl[ R_{t+1} \\mid S_t = s, A_t = a \\bigr]. \\] State Only:\n\\[ R(s) = \\mathbb{E}\\bigl[ R_{t+1} \\mid S_t = s \\bigr]. \\] Value Function Value functions quantify the expected return (sum of discounted future rewards) starting from a given state (or state-action pair). They are central to most RL methods. There are two key types:\nState-Value Function The state-value function \\( V_\\pi(s) \\) measures the expected return when starting in state \\( s \\) and following policy \\( \\pi \\) thereafter:\n\\[ V_\\pi(s) = \\mathbb{E}_\\pi\\bigl[ G_t \\mid S_t = s \\bigr] = \\mathbb{E}_\\pi\\Bigl[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\;\\big\\vert\\; S_t = s \\Bigr]. \\]Action-Value Function (Q-function) The action-value function \\( Q_\\pi(s, a) \\) measures the expected return when starting in state \\( s \\), taking action \\( a \\), and thereafter following policy \\( \\pi \\):\n\\[ Q_\\pi(s, a) = \\mathbb{E}_\\pi\\bigl[ G_t \\mid S_t = s, A_t = a \\bigr] = \\mathbb{E}_\\pi\\Bigl[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\;\\big\\vert\\; S_t = s, A_t = a \\Bigr]. \\]Advantage Function The advantage function \\( A_\\pi(s, a) \\) indicates how much better (or worse) taking action \\( a \\) in state \\( s \\) is compared to the average action under policy \\( \\pi \\). It is defined as:\n\\[ A_\\pi(s, a) = Q_\\pi(s, a) - V_\\pi(s). \\]Significance of the Advantage Function Variance Reduction: In policy gradient methods, using the advantage function can reduce the variance of gradient estimates, leading to more stable learning. Policy Improvement: It highlights which actions are better or worse than the average in a given state, providing a clearer signal for policy updates. Optimal Value and Policy The ultimate goal in RL is to find an optimal policy \\( \\pi_* \\) that maximizes the expected return in the long run. Correspondingly, we define optimal value functions.\nOptimal State-Value Function The optimal state-value function \\( V_*(s) \\) is the maximum state-value attainable at state \\( s \\) over all possible policies:\n\\[ V_*(s) = \\max_{\\pi} V_\\pi(s). \\]Optimal Action-Value Function The optimal action-value function \\( Q_*(s, a) \\) is the maximum action-value attainable for the state-action pair \\( (s, a) \\) over all possible policies:\n\\[ Q_*(s, a) = \\max_{\\pi} Q_\\pi(s, a). \\]Optimal Policy An optimal policy \\( \\pi_* \\) is any policy that achieves these optimal value functions. Formally, for all \\( s \\) and \\( a \\):\n\\[ V_{\\pi_*}(s) = V_*(s), \\quad\\text{and}\\quad Q_{\\pi_*}(s, a) = Q_*(s, a). \\]Challenges in Finding the Optimal Policy Complexity: Finding \\( \\pi_* \\), \\( V_*(s) \\), or \\( Q_*(s, a) \\) can be computationally expensive, especially for large or continuous state and action spaces. Approximation: Many RL algorithms approximate these optimal functions or directly learn an (approximately) optimal policy through techniques such as dynamic programming, Monte Carlo methods, or temporal-difference learning. A Taxonomy of RL Algorithms Now that we’ve gone through the basics of RL terminology and notation, we can delve into the richer material: the landscape of algorithms in modern RL, along with the trade-offs involved in their design.\nOverview Creating an accurate and comprehensive taxonomy of modern RL algorithms is challenging due to the modularity and complexity of these algorithms. To keep this introduction digestible, we focus on the foundational design choices in deep RL algorithms, the trade-offs these choices entail, and contextualize some prominent modern algorithms within this framework.\nKey Insights from the RL Algorithms Taxonomy The comparative table provides a structured overview of various Reinforcement Learning (RL) approaches. Below, we break down the table contents into detailed key insights to better understand the distinctions, strengths, and challenges associated with each category of RL algorithms.\n1. Model-Free RL: Policy Optimization Key Idea:\nPolicy Optimization methods focus on directly learning a parameterized policy \\( \\pi_\\theta(a \\mid s) \\) by maximizing a performance objective \\( J(\\pi) \\). These methods often incorporate learning a value function \\( V^\\pi(s) \\) to facilitate more effective policy updates.\nOn/Off-Policy Setting:\nPrimarily on-policy, meaning updates are based on data collected from the most recent policy.\nStrengths:\nStability: By directly optimizing the policy objective, these methods tend to exhibit stable and reliable convergence. Simplicity in Updates: Utilizes gradient-based approaches, making the update process conceptually straightforward. Weaknesses:\nSample Inefficiency: Requires fresh interaction data from the current policy for each update, which can be resource-intensive. High Variance: Gradient estimates can exhibit high variance, potentially slowing down the learning process. Representative Examples:\nA2C, A3C, PPO, TRPO\n2. Model-Free RL: Q-Learning Key Idea:\nQ-Learning methods aim to learn an approximate Q-function \\( Q_\\theta(s, a) \\) that estimates the optimal action-value function \\( Q^*(s, a) \\). The policy is then derived by selecting actions that maximize the Q-values.\nOn/Off-Policy Setting:\nPrimarily off-policy, allowing the use of data collected from any policy during training.\nStrengths:\nSample Efficiency: Can reuse past experiences effectively, making better use of available data. Straightforward Objective: Relies on Bellman backups, providing a clear and direct learning target. Weaknesses:\nStability Issues: Susceptible to divergence and instability, especially when combined with function approximation. Indirect Performance Optimization: Optimizes the Q-function rather than the policy directly, which can complicate the learning process. Representative Examples:\nDQN, C51, QR-DQN\n3. Model-Based RL Key Idea:\nModel-Based methods involve using or learning a model of the environment\u0026rsquo;s dynamics (state transitions and rewards) to facilitate planning or generate additional training data.\nOn/Off-Policy Setting:\nCan be on-policy or off-policy depending on the specific algorithm design.\nStrengths:\nHigh Sample Efficiency: Leveraging a model allows for planning and generating synthetic data, reducing the need for extensive real-world interactions. Forward Planning: Enables the agent to \u0026ldquo;think ahead\u0026rdquo; by simulating future states and rewards, leading to more informed decision-making. Weaknesses:\nModel Bias: Inaccuracies in the learned model can lead to suboptimal or even detrimental policy performance in the real environment. Implementation Complexity: Incorporating a model adds layers of complexity, making these methods harder to implement and tune effectively. Representative Examples:\nMBMF, MBVE, AlphaZero, World Models\n4. Hybrid / In-Between Approaches Key Idea:\nHybrid methods blend elements from policy optimization, Q-Learning, and planning. For instance, they may learn both a Q-function and a policy simultaneously or embed planning mechanisms directly into the policy structure.\nOn/Off-Policy Setting:\nVaries across different algorithms; some are off-policy, others are on-policy, and some employ a mixed approach.\nStrengths:\nBalanced Strengths: Capable of harnessing the advantages of multiple RL paradigms, such as the stability of policy optimization and the sample efficiency of Q-Learning. Enhanced Data Utilization: Can effectively reuse data while maintaining stable policy updates, leading to improved overall performance. Weaknesses:\nImplementation Complexity: Managing multiple components (e.g., separate networks for policy and value functions) increases the complexity of the algorithm. Inherited Failure Modes: Risks arising from combining different methods can lead to compounded instability or other issues from each constituent approach. Representative Examples:\nDDPG, SAC, I2A (Imagination-Augmented Agents)\nSummary of Insights Diverse Strategies: RL algorithms can be broadly categorized into model-free and model-based approaches, each with distinct methodologies and trade-offs. Hybrid methods seek to combine these strategies to leverage their respective strengths.\nPolicy Optimization vs. Q-Learning:\nPolicy Optimization offers stability and direct optimization but at the cost of sample efficiency. Q-Learning provides greater sample efficiency through data reuse but may suffer from stability issues. Model-Based Advantages and Challenges:\nWhile model-based methods can significantly enhance sample efficiency and enable forward planning, they are often hindered by the difficulty of accurately modeling complex environments and the increased complexity of implementation. Hybrid Approaches as a Middle Ground:\nBy integrating aspects of both model-free and model-based methods, hybrid algorithms aim to achieve a balance between stability, sample efficiency, and performance. However, this integration introduces additional complexity and potential points of failure. Representative Algorithms:\nUnderstanding where prominent algorithms like PPO, DQN, AlphaZero, and SAC fit within this taxonomy helps in selecting the appropriate method based on the specific requirements and constraints of the task at hand. Others Markov Decision Processes (MDPs) In reinforcement learning, the interaction between an agent and its environment is often formalized as a Markov Decision Process (MDP). MDPs provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. An MDP is defined by a 5-tuple \\(\\langle \\mathcal{S}, \\mathcal{A}, P, R, \\gamma \\rangle\\):\n\\(\\mathcal{S}\\): A set of possible states. This is the set of all possible situations the agent can be in. \\(\\mathcal{A}\\): A set of possible actions. These are the actions the agent can take in each state. \\(P(s'\\mid s,a)\\): The transition probability function. It defines the probability of transitioning to a next state \\(s'\\) from the current state \\(s\\) when action \\(a\\) is taken. This function encapsulates the environment\u0026rsquo;s dynamics. \\(R(s,a,s')\\): The reward function. It defines the reward received by the agent after transitioning from state \\(s\\) to \\(s'\\) due to action \\(a\\). This function specifies the immediate feedback from the environment. \\(\\gamma \\in [0, 1]\\): The discount factor. It is a value between 0 and 1 that discounts future rewards. A discount factor closer to 0 makes the agent prioritize immediate rewards, while a factor closer to 1 makes it value future rewards more. The defining characteristic of an MDP is the Markov property, which states that the future state and reward depend only on the current state and action, and not on the history of past states and actions. Formally, for any time step \\(t\\):\n\\[ \\mathbb{P}\\bigl[S_{t+1} \\mid S_t, A_t\\bigr] = \\mathbb{P}\\bigl[S_{t+1} \\mid S_1, A_1, S_2, A_2, \\ldots, S_t, A_t\\bigr] \\]This property simplifies the problem significantly as the agent only needs to consider the current state to make optimal decisions, without needing to remember the entire history.\nIn an MDP, the agent\u0026rsquo;s goal is to find a policy \\(\\pi\\) that maximizes the expected cumulative discounted reward, starting from some initial state distribution \\(\\rho_0(s)\\). The sequence of states, actions, and rewards generated by an agent interacting with an MDP is called a trajectory or episode:\n\\[ \\tau = \\bigl(S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \\ldots\\bigr) \\]The first state \\(S_0\\) is sampled from the start-state distribution \\(\\rho_0(\\cdot)\\). Subsequent states are determined by the transition probabilities \\(P(s'\\mid s,a)\\), and rewards are given by the reward function \\(R(s,a,s')\\). Actions \\(A_t\\) are chosen by the agent according to its policy \\(\\pi(a\\mid s)\\).\nBellman Equations Bellman equations are a set of equations that lie at the heart of dynamic programming and reinforcement learning. They decompose the value function into two parts: the immediate reward and the discounted value of the next state. These equations express a recursive relationship that value functions must satisfy. There are two main types of Bellman equations: Bellman Expectation Equations and Bellman Optimality Equations.\nBellman Expectation Equations\nBellman Expectation Equations are used for policy evaluation, i.e., calculating the value functions \\(V_\\pi(s)\\) and \\(Q_\\pi(s,a)\\) for a given policy \\(\\pi\\). They express the value of a state (or state-action pair) in terms of the expected immediate reward and the expected value of the next state, assuming the agent follows policy \\(\\pi\\). Bellman Expectation Equation for State-Value Function \\(\\bigl(V_\\pi(s)\\bigr)\\):\n\\[ V_\\pi(s) = \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_{t}=s\\bigr] \\] Expanding this expectation: \\[ V_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\Bigl(R(s, a) + \\gamma V_\\pi(s')\\Bigr) \\] Derivation:\n\\[ \\begin{aligned} V_\\pi(s) \u0026= \\mathbb{E}_{\\pi}\\bigl[G_t \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots) \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_t=s\\bigr] \\\\ \u0026= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\bigl(R(s,a) + \\gamma V_\\pi(s')\\bigr) \\end{aligned} \\] Bellman Expectation Equation for Action-Value Function \\(\\bigl(Q_\\pi(s,a)\\bigr)\\):\n\\[ Q_\\pi(s,a) = \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma Q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t=s, A_t=a\\bigr] \\] Expanding this expectation: \\[ Q_\\pi(s,a) = R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\mid s')\\, Q_\\pi(s',a') \\] Derivation:\n\\[ \\begin{aligned} Q_\\pi(s,a) \u0026= \\mathbb{E}_{\\pi}\\bigl[G_t \\mid S_t=s, A_t=a\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_t=s, A_t=a\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma \\mathbb{E}_{a' \\sim \\pi} Q_\\pi(S_{t+1}, a') \\mid S_t=s, A_t=a\\bigr] \\\\ \u0026= R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\mid s') Q_\\pi(s',a') \\end{aligned} \\] Bellman Optimality Equations\nBellman Optimality Equations specify the conditions for optimality for value functions \\(V_*(s)\\) and \\(Q_*(s,a)\\). They express the optimal value of a state (or state-action pair) in terms of the optimal values of successor states, assuming optimal actions are taken. Bellman Optimality Equation for Optimal State-Value Function \\(\\bigl(V_*(s)\\bigr)\\):\n\\[ V_*(s) = \\max_{a \\in \\mathcal{A}} \\mathbb{E}\\bigl[R_{t+1} + \\gamma V_*(S_{t+1}) \\mid S_t=s, A_t=a\\bigr] \\] Expanding this expectation: \\[ V_*(s) = \\max_{a \\in \\mathcal{A}} \\Bigl(R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a)\\, V_*(s')\\Bigr) \\] Explanation: To achieve the optimal value in state \\(s\\), we should choose the action \\(a\\) that maximizes the sum of the immediate reward \\(R(s,a)\\) and the discounted optimal value of the next state \\(V_*(s')\\).\nBellman Optimality Equation for Optimal Action-Value Function \\(\\bigl(Q_*(s,a)\\bigr)\\):\n\\[ Q_*(s,a) = \\mathbb{E}\\bigl[R_{t+1} + \\gamma \\max_{a'} Q_*(S_{t+1}, a') \\mid S_t=s, A_t=a\\bigr] \\] Expanding this expectation: \\[ Q_*(s,a) = R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a)\\, \\max_{a' \\in \\mathcal{A}} Q_*(s',a') \\] Explanation: The optimal Q-value for a state-action pair \\((s,a)\\) is the immediate reward \\(R(s,a)\\) plus the discounted maximum Q-value achievable from the next state \\(s'\\), considering all possible actions \\(a'\\) in \\(s'\\).\nThese Bellman equations form the basis for many reinforcement learning algorithms, providing a way to compute and improve value functions and policies.\n2. Fundamental Approaches Dynamic Programming Dynamic Programming (DP) provides a collection of algorithms that can be used to compute optimal policies in MDPs, given a complete model of the environment. DP methods are particularly useful when the environment is fully known, meaning we have access to the transition probabilities \\(P(s'\\mid s,a)\\) and the reward function \\(R(s,a,s')\\). DP algorithms are based on the principle of optimality and utilize Bellman equations to find optimal policies and value functions.\nPolicy Evaluation (Prediction)\nPolicy Evaluation, also known as the prediction problem, aims to compute the state-value function \\(V_\\pi(s)\\) for a given policy \\(\\pi\\). It uses the Bellman Expectation Equation for \\(V_\\pi(s)\\) iteratively. Iterative Policy Evaluation Algorithm:\nInitialize \\(V_0(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). For each iteration \\(k+1\\): For each state \\(s \\in \\mathcal{S}\\): \\[ V_{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s', r} P(s', r \\mid s, a)\\Bigl(r + \\gamma V_k(s')\\Bigr) \\] Repeat until convergence, i.e., until \\(V_{k+1}(s) \\approx V_k(s)\\) for all \\(s\\). Explanation: In each iteration, the value function for each state is updated based on the expected rewards and values of successor states, weighted by the policy and transition probabilities. This process is repeated until the value function converges, meaning the updates become very small.\nPolicy Improvement\nPolicy Improvement is the process of creating a better policy \\(\\pi'\\) from a given policy \\(\\pi\\). The idea is to act greedily with respect to the value function \\(V_\\pi\\) (or action-value function \\(Q_\\pi\\)) of the current policy. Greedy Policy Improvement: For each state \\(s \\in \\mathcal{S}\\), choose a new action \\(a'\\) that maximizes the action-value function \\(Q_\\pi(s,a)\\): \\[ \\pi'(s) = \\arg\\max_{a \\in \\mathcal{A}} Q_\\pi(s,a) \\] where \\[ Q_\\pi(s,a) = \\mathbb{E}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_t=s, A_t=a\\bigr] = \\sum_{s', r} P(s', r\\mid s, a)\\bigl(r + \\gamma V_\\pi(s')\\bigr). \\] Policy Improvement Theorem: If we improve the policy greedily with respect to \\(V_\\pi\\), the new policy \\(\\pi'\\) is guaranteed to be no worse than \\(\\pi\\), i.e., \\(V_{\\pi'}(s) \\ge V_\\pi(s)\\) for all \\(s \\in \\mathcal{S}\\). If improvement is strict for any state, then \\(\\pi'\\) is strictly better than \\(\\pi\\). Policy Iteration\nPolicy Iteration combines policy evaluation and policy improvement in an iterative process to find an optimal policy. It starts with an arbitrary policy, iteratively evaluates its value function, and then improves the policy based on this value function. Policy Iteration Algorithm: Initialization: Initialize a policy \\(\\pi_0\\) (e.g., randomly). Policy Evaluation: Compute the state-value function \\(V_{\\pi_k}\\) for the current policy \\(\\pi_k\\) using iterative policy evaluation. Policy Improvement: Create a new policy \\(\\pi_{k+1}\\) by acting greedily with respect to \\(V_{\\pi_k}\\): \\[ \\pi_{k+1}(s) = \\arg\\max_{a \\in \\mathcal{A}} Q_{\\pi_k}(s,a) \\] Iteration: Repeat steps 2 and 3 until policy improvement no longer yields a change in the policy, i.e., \\(\\pi_{k+1} = \\pi_k\\). At this point, the policy \\(\\pi_k\\) is guaranteed to be an optimal policy \\(\\pi_*\\), and \\(V_{\\pi_k} = V_*\\). Generalized Policy Iteration (GPI): The general idea of iteratively performing policy evaluation and policy improvement, which underlies many RL algorithms, including policy iteration and value iteration. Monte-Carlo Methods Monte-Carlo (MC) methods are a class of model-free reinforcement learning algorithms. Unlike Dynamic Programming, MC methods do not require a complete model of the environment. Instead, they learn directly from episodes of experience. An episode is a complete sequence of states, actions, and rewards from a start state to a terminal state. MC methods are used for both prediction (estimating value functions) and control (finding optimal policies).\nValue Estimation Monte Carlo methods estimate value functions by averaging the returns observed in actual or simulated episodes. The return \\(G_t\\) is the total discounted reward from time step \\(t\\) onwards in an episode: \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t} R_T, \\] where \\(T\\) is the terminal time step of the episode. Monte Carlo Value Estimation Algorithm:\nInitialize \\(V(s)\\) arbitrarily for all \\(s \\in \\mathcal{S}\\). Initialize \\(N(s) = 0\\) and \\(SumOfReturns(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). For each episode: Generate an episode following policy \\(\\pi\\): \\(S_1, A_1, R_2, S_2, A_2, \\ldots, S_T\\). For each state \\(S_t\\) visited in the episode: \\[ N(S_t) \\leftarrow N(S_t) + 1 \\] \\[ SumOfReturns(S_t) \\leftarrow SumOfReturns(S_t) + G_t \\] \\[ V(S_t) = \\frac{SumOfReturns(S_t)}{N(S_t)} \\] Explanation: For each episode, we calculate the return \\(G_t\\) for each state \\(S_t\\) visited in that episode. We then average these returns across all episodes to estimate \\(V(s)\\). We can use either first-visit MC, where we only consider the first visit to a state in an episode, or every-visit MC, where we consider every visit. Action-Value Function Estimation: MC methods can be extended to estimate action-value functions \\(Q_\\pi(s,a)\\) similarly, by averaging returns following each state-action pair \\((s,a)\\).\nTemporal-Difference (TD) Learning Temporal-Difference (TD) learning is a class of model-free reinforcement learning methods that learn directly from raw experience without a model of the environment, similar to Monte Carlo methods. However, TD learning has a key advantage: it can learn from incomplete episodes by bootstrapping, meaning it updates value function estimates based on other estimates, without waiting for the final outcome of an episode. TD learning is central to modern reinforcement learning algorithms.\nBootstrapping\nBootstrapping is a core concept in TD learning. It means that TD methods update their estimates based in part on other estimates. In value function learning, TD methods update the value of a state based on the estimated value of the next state. This is in contrast to Monte Carlo methods, which wait until the end of an episode to calculate the actual return and use that as the target for updates. Bootstrapping allows TD learning to be more sample-efficient and to learn online, without needing to wait for the end of episodes. Value Estimation\nThe simplest TD method for prediction is TD(0), also known as one-step TD learning. TD(0) updates the value function \\(V(S_t)\\) towards a TD target, which is an estimate of the return based on the immediate reward \\(R_{t+1}\\) and the current estimate of the value of the next state \\(V(S_{t+1})\\). TD(0) Update Rule for State-Value Function: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha \\Bigl(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\Bigr), \\] where \\(\\alpha\\) is the learning rate, controlling the step size of the update. The term \\(\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\) is called the TD error, representing the difference between the TD target and the current estimate.\nExplanation: The TD(0) update moves the value function \\(V(S_t)\\) in the direction of the TD target. If the TD error is positive, it means the current estimate \\(V(S_t)\\) is lower than the TD target, so we increase \\(V(S_t)\\). If the TD error is negative, we decrease \\(V(S_t)\\).\nSARSA: On-Policy TD Control\nSARSA (State-Action-Reward-State-Action) is an on-policy TD control algorithm for learning action-value functions \\(Q_\\pi(s,a)\\). It learns the Q-function for the policy that is being used to explore the environment. \u0026ldquo;On-policy\u0026rdquo; means that SARSA learns about the policy it is currently following. SARSA Algorithm: Initialize \\(Q(s,a)\\) arbitrarily for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}\\). For each episode: Initialize state \\(S_t\\). Choose action \\(A_t\\) using a policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy). Repeat for each step of episode: Take action \\(A_t\\), observe reward \\(R_{t+1}\\) and next state \\(S_{t+1}\\). Choose next action \\(A_{t+1}\\) using policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy). Update Q-value for state-action pair \\((S_t, A_t)\\): [ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\Bigl(R_{t+1} \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\\Bigr). ] \\(S_t \\leftarrow S_{t+1}\\), \\(A_t \\leftarrow A_{t+1}\\). Until \\(S_{t+1}\\) is terminal. Explanation: SARSA updates the Q-value for the state-action pair \\((S_t, A_t)\\) based on the reward \\(R_{t+1}\\) and the Q-value of the next state-action pair \\((S_{t+1}, A_{t+1})\\). The action \\(A_{t+1}\\) is chosen using the same policy that is being evaluated and improved, hence \u0026ldquo;on-policy\u0026rdquo;. Q-Learning: Off-Policy TD Control\nQ-Learning is an off-policy TD control algorithm. It learns an estimate of the optimal action-value function \\(Q_*(s,a)\\), independent of the policy being followed. \u0026ldquo;Off-policy\u0026rdquo; means that Q-learning can learn about an optimal policy even while following a different, possibly exploratory, policy. Q-Learning Algorithm: Initialize \\(Q(s,a)\\) arbitrarily for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}\\). For each episode: Initialize state \\(S_t\\). Repeat for each step of episode: Choose action \\(A_t\\) using a policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy). Take action \\(A_t\\), observe reward \\(R_{t+1}\\) and next state \\(S_{t+1}\\). Update Q-value for state-action pair \\((S_t, A_t)\\): [ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\Bigl(R_{t+1} \\gamma \\max_{a\u0026rsquo;} Q(S_{t+1}, a\u0026rsquo;) - Q(S_t, A_t)\\Bigr). ] \\(S_t \\leftarrow S_{t+1}\\). Until \\(S_{t+1}\\) is terminal. Explanation: Q-learning updates the Q-value for \\((S_t, A_t)\\) based on the reward \\(R_{t+1}\\) and the maximum Q-value achievable in the next state \\(S_{t+1}\\), regardless of which action is actually taken in \\(S_{t+1}\\). This \u0026ldquo;max\u0026rdquo; operation makes Q-learning off-policy because it learns about the optimal policy while potentially following a different behavior policy for exploration. Deep Q-Networks (DQN)\nDeep Q-Networks (DQN) is a groundbreaking algorithm that combines Q-learning with deep neural networks to handle high-dimensional state spaces, such as images. DQN addresses the instability and convergence issues that arise when using nonlinear function approximation (like neural networks) with bootstrapping and off-policy learning. DQN introduces two key techniques to stabilize Q-learning: Experience Replay: DQN stores transitions \\(\\bigl(s_t, a_t, r_t, s_{t+1}\\bigr)\\) in a replay memory (buffer) \\(D\\). Instead of updating Q-values online from sequential experiences, DQN samples mini-batches of transitions randomly from \\(D\\) to perform updates. This breaks the correlation between consecutive samples and smooths the data distribution over updates, improving stability. Periodically Updated Target Network: DQN uses two Q-networks: a Q-network \\(Q(s,a; \\theta)\\) with parameters \\(\\theta\\) that are being trained, and a target Q-network \\(Q(s,a; \\theta^-)\\) with parameters \\(\\theta^-\\) that are periodically updated to be the same as \\(\\theta\\) (e.g., every \\(C\\) steps) and kept frozen in between. The target network is used to compute the TD target in the Q-learning update, which stabilizes learning by reducing oscillations and divergence. DQN Loss Function: The loss function for DQN is the mean squared error between the TD target and the current Q-value: \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim U(D)}\\Bigl[\\bigl(y - Q(s,a;\\theta)\\bigr)^2\\Bigr], \\] where \\[ y = r + \\gamma \\max_{a'} Q(s',a'; \\theta^-) \\] is the TD target, and \\(\\bigl(s,a,r,s'\\bigr)\\) is a transition sampled from the replay memory \\(D\\). \\(U(D)\\) denotes uniform sampling from \\(D\\). DQN Algorithm Outline: Initialize replay memory \\(D\\) to capacity \\(N\\). Initialize Q-network \\(Q\\) with random weights \\(\\theta\\). Initialize target Q-network \\(\\hat{Q}\\) with weights \\(\\theta^- = \\theta\\). For each episode: Initialize state \\(s_1\\). For \\(t = 1, \\ldots, T\\): Select action \\(a_t\\) using \\(\\epsilon\\)-greedy policy based on \\(Q(s_t,\\cdot;\\theta)\\). Execute action \\(a_t\\), observe reward \\(r_t\\) and next state \\(s_{t+1}\\). Store transition \\(\\bigl(s_t,a_t,r_t,s_{t+1}\\bigr)\\) in \\(D\\). Sample random mini-batch of transitions from \\(D\\). Compute TD targets \\(y_j = r_j + \\gamma \\max_{a'} \\hat{Q}(s_j',a';\\theta^-)\\) (or \\(y_j = r_j\\) if episode terminates). Perform gradient descent step to minimize \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_j\\Bigl[\\bigl(y_j - Q(s_j,a_j;\\theta)\\bigr)^2\\Bigr] \\] with respect to \\(\\theta\\). Every \\(C\\) steps, reset \\(\\theta^- = \\theta\\). Combining TD and Monte-Carlo Learning n-step TD Learning\nn-step TD learning methods bridge the gap between one-step TD learning (like TD(0)) and Monte Carlo methods by looking ahead \\(n\\) steps to estimate the return. The n-step return \\(G_t^{(n)}\\) is defined as: \\[ G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n}). \\] For \\(n=1\\), \\(G_t^{(1)} = R_{t+1} + \\gamma V(S_{t+1})\\), which is the TD target. As \\(n \\to \\infty\\), \\(G_t^{(n)} \\to G_t\\), the Monte Carlo return. n-step TD Update Rule: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha \\bigl(G_t^{(n)} - V(S_t)\\bigr). \\] Benefits of n-step TD: Balances bias and variance: n-step returns have lower variance than MC returns and lower bias than 1-step TD targets. Can learn faster than MC methods and be more stable than 1-step TD methods in some environments. TD(\\(\\lambda\\))\nTD(\\(\\lambda\\)) methods generalize n-step TD learning by averaging n-step returns over all possible values of \\(n\\), weighted by a factor \\(\\lambda^{n-1}\\). The \\(\\lambda\\)-return \\(G_t^{(\\lambda)}\\) is defined as: \\[ G_t^{(\\lambda)} = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t^{(n)}, \\] where \\(\\lambda \\in [0,1]\\) is the trace-decay parameter. When \\(\\lambda = 0\\), \\(G_t^{(\\lambda)} = G_t^{(1)}\\), which reduces to TD(0). When \\(\\lambda = 1\\), \\(G_t^{(\\lambda)} = G_t^{(\\infty)} = G_t\\), which becomes the Monte Carlo return. TD(\\(\\lambda\\)) Update Rule: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha \\bigl(G_t^{(\\lambda)} - V(S_t)\\bigr). \\] Eligibility Traces: TD(\\(\\lambda\\)) can be implemented efficiently using eligibility traces, which provide a mechanism to assign credit to past states and actions for observed rewards. Eligibility traces maintain a short-term memory of visited states or state-action pairs, allowing updates to propagate back through time more efficiently than with n-step returns alone. Policy Gradient Methods Policy Gradient methods are a class of model-free reinforcement learning algorithms that directly learn and optimize the policy \\(\\pi_\\theta(a\\mid s)\\) without explicitly learning a value function (though value functions are often used to assist policy gradient methods, as in Actor-Critic methods). Policy gradient methods directly search for an optimal policy by optimizing the policy parameters \\(\\theta\\) using gradient ascent on the expected return \\(J(\\theta)\\).\nPolicy Gradient Theorem The Policy Gradient Theorem provides an analytical expression for the gradient of the performance objective \\(J(\\theta)\\) with respect to the policy parameters \\(\\theta\\). This theorem is crucial because it allows us to compute the gradient and perform gradient ascent to improve the policy. The policy gradient theorem states: \\[ \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\Bigl[\\nabla_{\\theta} \\ln \\pi(a\\mid s,\\theta)\\, Q_\\pi(s,a)\\Bigr]. \\] This equation is fundamental to policy gradient methods. It shows that the gradient of the performance objective can be estimated by averaging over trajectories sampled from the policy, where for each time step in a trajectory, we compute the product of the action-value function \\(Q_\\pi(s,a)\\) and the gradient of the log-policy \\(\\nabla_{\\theta} \\ln \\pi(a\\mid s,\\theta)\\).\nProof of Policy Gradient Theorem: (Detailed derivation already covered in the \u0026ldquo;Policy Gradient Theorem\u0026rdquo; section above.)\nAdvanced Policy Gradient Algorithms REINFORCE\nREINFORCE, also known as Monte Carlo Policy Gradient, is a basic policy gradient algorithm that directly implements the policy gradient theorem. It uses Monte Carlo estimates of the action-value function \\(Q_\\pi(s,a)\\) to update the policy parameters. In REINFORCE, the return \\(G_t\\) from an entire episode is used as an unbiased estimate of \\(Q_\\pi(S_t,A_t)\\). REINFORCE Algorithm: Initialize policy parameters \\(\\theta\\) randomly. For each episode: Generate an episode trajectory \\(\\tau = (S_1, A_1, R_2, S_2, A_2, \\ldots, S_T)\\) following policy \\(\\pi_\\theta\\). For each time step \\(t = 1, 2, \\ldots, T\\): Calculate the return \\[ G_t = \\sum_{k=0}^{T-t} \\gamma^k R_{t+k+1}. \\] Update policy parameters using gradient ascent: \\[ \\theta \\leftarrow \\theta + \\alpha\\, G_t\\, \\nabla_{\\theta} \\ln \\pi\\bigl(A_t \\mid S_t,\\theta\\bigr). \\] Explanation: REINFORCE updates the policy parameters in the direction that increases the probability of actions that led to higher returns in the episode. The gradient update is proportional to \\(G_t\\) and \\(\\nabla_{\\theta} \\ln \\pi(A_t\\mid S_t,\\theta)\\). Actor-Critic Variants\nA2C (Advantage Actor-Critic): Advantage Actor-Critic (A2C) is the synchronous, deterministic counterpart to Asynchronous Advantage Actor-Critic (A3C). A2C runs multiple agents in parallel environments and waits for all agents to complete their steps before performing a synchronized update of the global network. This synchronous update often leads to more stable and efficient learning, especially when using GPUs for training. A2C typically uses the advantage function \\(A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)\\) to reduce variance and improve the gradient signal. A3C (Asynchronous Advantage Actor-Critic): Asynchronous Advantage Actor-Critic (A3C) is a parallel, asynchronous framework where multiple independent agents (actors) run in parallel environments to collect experience and update a shared global network asynchronously. A3C is known for its efficiency in utilizing multi-core CPUs. It typically uses the advantage function \\(A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)\\) to reduce variance in policy gradient updates. Off-Policy Policy Gradient\nOff-Policy Policy Gradient methods learn a policy using data generated by a different policy, called the behavior policy, rather than the policy being optimized, called the target policy. This is in contrast to on-policy methods, where the behavior policy and target policy are the same. Off-policy methods offer several advantages: Sample Efficiency: They can reuse past experiences stored in a replay buffer, making learning more data-efficient. Exploration: They can use a behavior policy that is more exploratory than the target policy, facilitating better exploration of the environment. Flexibility: They allow learning from diverse datasets, including data collected from human experts or other agents. Importance Sampling: A key technique in off-policy policy gradient methods is importance sampling. It is used to correct for the mismatch between the distribution of data generated by the behavior policy and the distribution under the target policy. For policy gradient, importance sampling weights are used to adjust the gradient estimates. For example, in off-policy policy gradient updates, the gradient is weighted by the ratio \\(\\frac{\\pi_\\theta(a\\mid s)}{\\beta(a\\mid s)}\\), where \\(\\beta\\) is the behavior policy. Advantage Actor-Critic (A2C)\nAdvantage Actor-Critic (A2C) is the synchronous counterpart to A3C, running multiple agents in parallel environments and performing synchronized updates. It uses the advantage function to reduce variance in the policy gradient estimates, leading to more stable and efficient learning. Deterministic Policy Gradient (DPG)\nDeterministic Policy Gradient (DPG) is a policy gradient algorithm designed for continuous action spaces. Unlike stochastic policy gradient methods that learn a probability distribution over actions, DPG learns a deterministic policy \\(\\mu_\\theta(s)\\) that directly outputs a specific action for each state. DPG is based on the Deterministic Policy Gradient Theorem, which provides a way to compute the gradient of the performance objective for deterministic policies. Deterministic Policy Gradient Theorem: \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho^\\mu}\\bigl[\\nabla_a Q_\\mu(s,a)\\,\\nabla_\\theta \\mu_\\theta(s)\\bigr] \\Bigm|_{a = \\mu_\\theta(s)}, \\] where \\(\\rho^\\mu(s)\\) is the state distribution under the deterministic policy \\(\\mu_\\theta(s)\\), and \\(Q_\\mu(s,a)\\) is the action-value function for policy \\(\\mu_\\theta(s)\\). Deep Deterministic Policy Gradient (DDPG)\nDeep Deterministic Policy Gradient (DDPG) is an off-policy actor-critic algorithm that combines DPG with techniques from DQN to enable deep reinforcement learning in continuous action spaces. DDPG is essentially a deep learning version of DPG, incorporating experience replay and target networks to stabilize training. Key Features of DDPG: Actor-Critic Architecture: Uses two neural networks: an actor network \\(\\mu_\\theta(s)\\) and a critic network \\(Q_w(s,a)\\). Experience Replay: Stores transitions in a replay buffer and samples mini-batches for updates, similar to DQN. Target Networks: Uses target networks for both actor \\(\\mu_{\\theta^-}(s)\\) and critic \\(Q(s,a; w^-)\\), updated slowly by soft updates. Deterministic Policy: Learns a deterministic policy, well-suited for continuous action spaces. Exploration: Uses Ornstein-Uhlenbeck process or Gaussian noise added to actions for exploration. Distributed Distributional DDPG (D4PG)\nDistributed Distributional DDPG (D4PG) is an extension of DDPG that incorporates: Distributional Critic: Learns a distribution over returns instead of a single Q-value. N-step Returns: Uses n-step TD targets to reduce variance. Multiple Distributed Parallel Actors: Collects experience in parallel, improving data throughput. Prioritized Experience Replay (PER): Samples transitions with probabilities proportional to their TD errors. Multi-Agent DDPG (MADDPG)\nMulti-Agent DDPG (MADDPG) extends DDPG to multi-agent environments, addressing non-stationarity where multiple agents learn simultaneously. Centralized Critic, Decentralized Actors: Each agent’s critic has access to global information, while each actor only sees its local observations. Learning with Other Agents’ Policies: Helps handle changing dynamics as other agents learn. Trust Region Policy Optimization (TRPO)\nTrust Region Policy Optimization (TRPO) is an on-policy policy gradient algorithm that ensures monotonic policy improvement by constraining the size of each policy update, measured by KL divergence. TRPO Objective: \\[ \\max_{\\theta}\\,\\mathbb{E}_{s \\sim \\rho^{\\pi_{\\theta_\\text{old}}},\\,a \\sim \\pi_{\\theta_\\text{old}}} \\Bigl[\\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_\\text{old}}(a\\mid s)}\\,\\hat{A}_{\\theta_\\text{old}}(s,a)\\Bigr] \\] subject to a KL-divergence constraint \\(\\le \\delta\\). Explanation: TRPO keeps the new policy \\(\\pi_\\theta\\) within a \u0026ldquo;trust region\u0026rdquo; of the old policy \\(\\pi_{\\theta_\\text{old}}\\), preventing large, destabilizing updates. Proximal Policy Optimization (PPO)\nProximal Policy Optimization (PPO) simplifies TRPO and is more practical to implement while maintaining similar performance. PPO uses a clipped surrogate objective to bound the policy update. PPO Clipped Objective: \\[ J^{\\mathrm{CLIP}}(\\theta) = \\mathbb{E}\\Bigl[\\min\\bigl(r(\\theta)\\,\\hat{A}_{\\theta_\\text{old}}(s,a),\\,\\mathrm{clip}\\bigl(r(\\theta),1-\\epsilon,1+\\epsilon\\bigr)\\,\\hat{A}_{\\theta_\\text{old}}(s,a)\\bigr)\\Bigr], \\] where \\(r(\\theta) = \\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_\\text{old}}(a\\mid s)}\\). Explanation: By clipping the ratio \\(r(\\theta)\\), PPO avoids excessively large updates that could harm performance. Phasic Policy Gradient (PPG)\nPhasic Policy Gradient (PPG) is an on-policy algorithm that separates policy and value function updates into distinct phases, improving sample efficiency and stability. Policy Phase: Optimizes a PPO-like objective. Auxiliary Phase: Improves the value function and keeps policy from drifting too far. Benefits: More stable updates and better sample reuse compared to PPO. Actor-Critic with Experience Replay (ACER)\nActor-Critic with Experience Replay (ACER) is an off-policy actor-critic algorithm combining experience replay, Retrace for stable off-policy Q-value estimation, truncated importance weights with bias correction, and an efficient TRPO-like update. Key Ideas: Off-policy correction via truncated importance sampling. Retrace for unbiased and low-variance Q-value estimation. Trust region updates for stable learning. Actor-Critic using Kronecker-Factored Trust Region (ACKTR)\nActor-Critic using Kronecker-Factored Trust Region (ACKTR) leverages Kronecker-Factored Approximate Curvature (K-FAC) for more efficient and stable updates. K-FAC: Approximates the Fisher information matrix via Kronecker products for natural gradient updates. Natural Gradient: Helps optimize in directions that consider curvature, often improving convergence speed and stability. Soft Actor-Critic (SAC)\nSoft Actor-Critic (SAC) is an off-policy actor-critic algorithm featuring maximum entropy RL. It maximizes both return and policy entropy, encouraging exploration and robustness. Maximum Entropy Objective: \\[ J(\\pi) = \\sum_{t=1}^{T} \\mathbb{E}\\bigl[r(s_t,a_t) + \\alpha\\,\\mathcal{H}\\bigl(\\pi_\\theta(\\cdot\\mid s_t)\\bigr)\\bigr], \\] where \\(\\alpha\\) is a temperature parameter. Soft Q-function and Soft Value Function: Incorporates entropy terms into the Bellman backup. Automatic Temperature Adjustment: \\(\\alpha\\) can be learned to balance exploration and exploitation. Twin Delayed DDPG (TD3)\nTwin Delayed DDPG (TD3) is an improvement over DDPG that addresses overestimation bias and improves stability. Clipped Double Q-learning: Uses two critics and takes the minimum of both Q-values to reduce overestimation. Delayed Policy Updates: Updates the policy less frequently than the critics. Target Policy Smoothing: Adds noise to target actions for smoother Q-value estimates. Stein Variational Policy Gradient (SVPG)\nStein Variational Policy Gradient (SVPG) uses Stein Variational Gradient Descent (SVGD) to maintain an ensemble of policy \u0026ldquo;particles,\u0026rdquo; encouraging diversity. SVGD: Iteratively updates a set of particles to approximate a target distribution. Kernel Function: Encourages particles to spread out, improving exploration. Importance Weighted Actor-Learner Architectures (IMPALA)\nImportance Weighted Actor-Learner Architectures (IMPALA) is a highly scalable RL framework with decoupled actors and learners, using V-trace to handle off-policy corrections. Decoupled Architecture: Multiple actors generate data in parallel, sending trajectories to a central learner. V-trace: Corrects off-policy data from actors running slightly outdated policies. High Throughput: Achieves efficient large-scale training in complex environments. ","permalink":"https://syhya.github.io/posts/rl-introduction/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: This article \u003cstrong\u003eis currently being updated\u003c/strong\u003e. The content is in \u003cstrong\u003edraft version\u003c/strong\u003e and may change. Please check back for the latest version.\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"notations\"\u003eNotations\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003eSymbol\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eMeaning\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(s, s', S_t, S_{t+1}\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eState, next state, state at time \\(t\\), state at time \\(t+1\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(o, o_t\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eObservation, observation at time \\(t\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(a, a', A_t, A_{t+1}\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eAction, next action, action at time \\(t\\), action at time \\(t+1\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(r, r_t\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eImmediate reward, reward at time \\(t\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(G_t\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eReturn at time \\(t\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(R(\\tau)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eReturn of a trajectory \\(\\tau\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\mathcal{S}\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eSet of all possible states\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\mathcal{A}\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eSet of all possible actions\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\mathcal{R}\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eSet of all possible rewards\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\pi(a\\mid s), \\pi_\\theta(a\\mid s)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ePolicy (stochastic), parameterized policy\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\mu(s), \\mu_\\theta(s)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ePolicy (deterministic), parameterized policy\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\theta, \\phi, w\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ePolicy or value function parameters\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\gamma\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eDiscount factor\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(J(\\pi)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eExpected return of policy \\(\\pi\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(V_\\pi(s)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eState-value function for policy \\(\\pi\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(Q_\\pi(s,a)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eAction-value function for policy \\(\\pi\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(V_*(s)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eOptimal state-value function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(Q_*(s,a)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eOptimal action-value function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(A_\\pi(s,a)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eAdvantage function for policy \\(\\pi\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(P(s'\\mid s,a)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eTransition probability function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(R(s,a,s')\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eReward function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\rho_0(s)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eStart-state distribution\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\tau\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eTrajectory\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(D\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eReplay memory\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\alpha\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eLearning rate, temperature parameter (in SAC)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\lambda\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eEligibility trace parameter\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\epsilon\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eExploration parameter (e.g., in \\(\\epsilon\\)-greedy), clipping parameter (in PPO)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"what-is-reinforcement-learning\"\u003eWhat is Reinforcement Learning?\u003c/h2\u003e\n\u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/posts/rl-introduction/image.png\"\u003e\u003c/p\u003e","title":"Deep Reinforcement Learning (Ongoing Updates)"},{"content":"DeepSeek AI recently released DeepSeek-R1 (DeepSeek-AI, 2025), whose reasoning performance on multiple benchmarks approaches the level of OpenAI\u0026rsquo;s o1 (OpenAI, 2024), marking a significant step for the open-source community in successfully replicating o1. Relevant code for R1 can be found in the huggingface\u0026rsquo;s attempt to open-source replication project open-r1. While previous research has often relied on massive amounts of supervised data to enhance the performance of Large Language Models (LLMs), the success of DeepSeek-R1 and its earlier experiment, DeepSeek-R1-Zero, powerfully demonstrates the potential of purely large-scale reinforcement learning in improving the reasoning capabilities of LLMs. This success reinforces the profound insight proposed by Richard Sutton in \u0026ldquo;The Bitter Lesson\u0026rdquo;:\nOne thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning. (Richard Sutton, 2019)\nNotations The following table lists the mathematical symbols used in this article to facilitate your reading.\nSymbol Meaning \\( q \\) or \\( Q \\) Question, user input or instruction \\( o \\) or \\( O \\) Output, model-generated text response or answer \\( t \\) Token index, indicating the position of the \\( t \\)-th token in the output text \\( o_t \\) The \\( t \\)-th token in the output text \\( o \\) \\( o_{\u0026lt;t} \\) The sequence of tokens in the output text \\( o \\) preceding the \\( t \\)-th token \\( \u0026#124;o\u0026#124; \\) Length of the output text \\( o \\), usually referring to the number of tokens \\( G \\) Output group size, in the GRPO algorithm, the number of outputs sampled for each question \\( \\pi_\\theta, \\pi_{\\theta_{\\text{old}}}, \\pi_{\\text{ref}}, \\pi_{\\text{sft}} \\) Policy models and their variants, used to generate text outputs or as reference models \\( A_t, A_i \\) Advantage function and relative advantage value \\( \\varepsilon \\) Clipping hyperparameter, used to limit the range of the importance sampling, ensuring the stability of policy updates \\( \\beta \\) Regularization coefficient, used to control the weight of the KL divergence penalty term in the objective function \\( \\mathbb{D}_{KL} \\) KL divergence, a measure of the difference between two probability distributions, used to constrain the distance between the new policy and the reference policy \\( \\mathcal{J}, \\mathcal{L} \\) Objective function and loss function \\( \\mathbb{E} \\) Expectation, representing the average value of a random variable, in the objective function, it represents the average over sample data \\( P_{\\text{sft}}(Q, O) \\) Distribution of the SFT dataset, representing the joint probability distribution of question \\( Q \\) and output \\( O \\) in the \\( SFT \\) dataset \\( P_{\\text{sft}}(Q) \\) Distribution of questions in the SFT dataset, representing the marginal probability distribution of question \\( Q \\) in the \\( SFT \\) dataset \\( \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\) Conditional probability of the policy model generating the \\( t \\)-th token \\( o_t \\) given the question \\( q \\) and previously generated tokens \\( o_{\u0026lt;t} \\) \\( \\mathbb{I}(o) \\) Indicator function that determines whether the output \\( o \\) is of high quality, 1 if high quality, 0 otherwise \\( r(o) \\) Reward function, a function that evaluates the quality of the model output \\( o \\) \\( r_i \\) Reward value of the \\( i \\)-th output \\( \\nabla_{\\theta} \\) Gradient operator, representing the gradient of a function with respect to model parameters \\( \\theta \\) \\( \\mathcal{N}(\\mu, 1) \\) Normal distribution with mean \\( \\mu \\) and standard deviation 1 \\( \\binom{a}{b} \\) Binomial coefficient, representing the number of combinations of choosing \\( b \\) elements from \\( a \\) elements \\( r(o) = \\frac{\\pi_{\\text{ref}}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\) Probability ratio, the ratio of the probability of generating output \\( o \\) by the reference model to the current policy model Training Process Overview The training of the DeepSeek-R1 series models is a multi-stage process aimed at building LLMs with superior reasoning and general language capabilities. The entire training process starts from the DeepSeek-V3 (DeepSeek-AI, 2024) model and iteratively optimizes it to obtain different versions of the DeepSeek-R1 model.\nFig. 1. DeepSeek R1 Training Pipeline. (Image source: Harris Chan\u0026rsquo;s Tweet)\nAs shown in Figure 1, the DeepSeek-R1 training process is clearly displayed and mainly divided into the following key stages:\nBase Model and Initial Fine-tuning: The starting point of the process is the DeepSeek-V3 Base model. First, SFT technology is used to initially train the base model on cold-start long-text CoT data, endowing the model with preliminary reasoning abilities.\nReinforcing Reasoning Ability: Based on SFT, a reasoning-oriented reinforcement learning method, specifically the Group Relative Policy Optimization (GRPO) algorithm, combined with rule-based rewards and CoT language consistency rewards, is used to further enhance the model\u0026rsquo;s reasoning ability.\nReasoning Data Generation and Rejection Sampling: Using reasoning prompts and rejection sampling techniques, and leveraging rules and the DeepSeek-V3 model to judge data quality, high-quality reasoning data is generated.\nNon-Reasoning Data Generation: Using the CoT prompting method, the DeepSeek-V3 model is used for data augmentation to generate non-reasoning data, which is combined with the original SFT data to improve the model\u0026rsquo;s general language capabilities.\nDistillation: Combining reasoning data and non-reasoning data for distillation training. Through SFT, the capabilities of DeepSeek-V3 are transferred to a series of smaller models (Qwen and Llama series), resulting in the DeepSeek-R1-Distill series models.\nFinal Model Fine-tuning: The DeepSeek-V3 model is fine-tuned again with SFT and reinforcement learning. In the reinforcement learning stage, reasoning and preference rewards are adopted, and diverse training prompts are used, ultimately resulting in the DeepSeek-R1 model.\nDeepSeek-R1-Zero: Trained directly on DeepSeek-V3 Base using the GRPO algorithm, serving as a comparative baseline for other models.\nNext, this blog post will delve into the key technologies and methods in the DeepSeek-R1 training process.\nDeepSeek-R1-Zero PPO Proximal Policy Optimization (PPO) (Schulman et al. 2017) algorithm is a classic algorithm widely used in reinforcement learning. In the InstructGPT (Ouyang et al. 2022) paper, it was proven to be an effective and stable method for training LLMs in the reinforcement learning fine-tuning stage.\nThe core idea of reinforcement learning is to allow an agent to learn through interaction with an environment, maximizing cumulative rewards through trial and error. In the LLM scenario, the model itself is the agent, and the \u0026ldquo;environment\u0026rdquo; can be understood as the questions raised by users and the expected ways of answering. The policy \\( \\pi_\\theta \\) represents the agent\u0026rsquo;s behavior guidelines, i.e., given an input (e.g., question \\( q \\)), the policy will output an action (e.g., generate text \\( o \\)). The policy \\( \\pi_\\theta \\) is usually parameterized by a neural network model, and the training objective is to find the optimal parameters \\( \\theta \\) so that the policy can generate high-quality outputs.\nThe Actor-Critic framework is a commonly used architecture in reinforcement learning, and PPO also belongs to the Actor-Critic algorithm family. The Actor-Critic framework includes two core components:\nActor (Policy Model): Responsible for learning the policy \\( \\pi_\\theta \\), i.e., how to choose actions (generate text) based on the current state (e.g., user question). Critic (Value Model): Responsible for evaluating the quality of the Actor\u0026rsquo;s policy, usually achieved by learning a value function \\( V(s) \\) or \\( Q(s, a) \\). The value function predicts the expected value of cumulative rewards that can be obtained in the future given a state \\( s \\) (or state-action pair \\( (s, a) \\)). The goal of PPO is to improve the policy model (Actor) so that it can generate higher quality outputs, while using the value model (Critic) to stabilize the training process. PPO updates the policy model \\( \\pi_{\\theta} \\) by maximizing the following objective function:\n\\[ \\mathcal{J}_{PPO}(\\theta) = \\mathbb{E}\\!\\Biggl[ \\min\\Bigl( \\frac{\\pi_\\theta(a\\!\\mid\\!s)}{\\pi_{\\theta_{\\text{old}}}(a\\!\\mid\\!s)}A_t,\\, \\operatorname{clip}\\Bigl( \\frac{\\pi_\\theta(a\\!\\mid\\!s)}{\\pi_{\\theta_{\\text{old}}}(a\\!\\mid\\!s)}, 1-\\varepsilon,\\, 1+\\varepsilon \\Bigr)A_t \\Bigr) \\Biggr] \\]Parameter Description:\nExpectation \\( \\mathbb{E}[\\cdot] \\): Represents the average over samples. In actual training, we sample a batch of data (e.g., user questions and model-generated answers) and then calculate the average objective function value for this batch of data.\nImportance Sampling: Measures the probability ratio of the current policy \\( \\pi_\\theta \\) to the old policy \\( \\pi_{\\theta_{\\text{old}}} \\) on action \\( a \\). PPO adopts the idea of proximal policy update, limiting the magnitude of each policy update to avoid excessive policy changes that lead to training instability.\nAdvantage Function \\( A_t \\): Evaluates the advantage of taking action \\( a \\) in state \\( s \\) relative to the average level. The advantage function is usually estimated by the Critic model (value network), and can be Advantage Estimation or Generalized Advantage Estimation (GAE) and other methods. The larger the advantage function \\( A_t \\), the better the current action \\( a \\), and the policy model should increase the probability of taking this action.\nclip: One of the core mechanisms of PPO, which can essentially be seen as a penalty function, used to limit the range of the importance sampling between \\( [1-\\varepsilon, 1+\\varepsilon] \\), where \\( \\varepsilon \\) is a hyperparameter (usually set to 0.2). The clipping operation prevents excessive policy update steps and improves training stability.\nThe clip function penalizes excessively large or small policy update magnitudes by limiting the importance sampling.\nWhen the importance sampling exceeds the range of \\( [1-\\varepsilon, 1+\\varepsilon] \\), the clip function will limit it within this range, thereby reducing the gain (or reducing the loss) of the objective function. For positive updates (\\( A_t \u003e 0 \\)): If the importance sampling is too large (exceeds \\( 1+\\varepsilon \\)), clip will limit it to \\( 1+\\varepsilon \\), reducing the actual update magnitude and penalizing overly aggressive policy improvements. For negative updates (\\( A_t \u003c 0 \\)): If the importance sampling is too small (less than \\( 1-\\varepsilon \\)), clip will limit it to \\( 1-\\varepsilon \\), also limiting the update magnitude and avoiding drastic changes in policy. The objective function takes the minimum value between clip before and clip after, ensuring that when the importance sampling is out of range, PPO will penalize policy updates, ensuring the \u0026ldquo;conservatism\u0026rdquo; of policy updates.\nIn the actual optimization process, we usually define the PPO loss function \\( \\mathcal{L}_{PPO}(\\theta) \\) as the negative value of the objective function, and maximize the objective function by minimizing the loss:\n\\[ \\mathcal{L}_{PPO}(\\theta) = -\\,\\mathcal{J}_{PPO}(\\theta). \\]The PPO algorithm, due to its characteristics of being simple and effective, and relatively stable, has become one of the benchmark algorithms in the field of reinforcement learning and has achieved success in various tasks, including reinforcement learning fine-tuning of LLMs. PPO is generally considered more stable than earlier methods such as TRPO, but its specific application in large models still requires careful hyperparameter tuning. In large-scale language model scenarios, if the value network and policy network are completely separated and of comparable size, it will inevitably bring more computational and memory overhead. To solve these problems, the DeepSeek team proposed Group Relative Policy Optimization (GRPO) algorithm.\nGRPO Group Relative Policy Optimization (GRPO) (Shao, et al. 2024) is an efficient and stable reinforcement learning algorithm specifically designed by the DeepSeek team for training LLMs like DeepSeek-R1-Zero. GRPO\u0026rsquo;s core innovation lies in abandoning the dependence on an independent value network (critic model) in the traditional Actor-Critic framework, reducing computational costs and improving training stability. Broadly speaking, GRPO can be regarded as an Actor-Only reinforcement learning method.\nGRPO is inspired by the idea of relative evaluation. In many practical scenarios, we are often better at judging the relative quality among a group of things than giving absolute value evaluations. For example, when evaluating a group of student assignments, teachers may find it easier to compare the merits of different assignments than to give each assignment an absolute score. GRPO introduces this idea of relative evaluation into reinforcement learning, using in-group relative scoring to build a baseline, completely replacing the dependence on value networks.\nSpecifically, for each question \\( q \\), GRPO samples a set of outputs \\( \\{o_1, o_2, \\ldots, o_G\\} \\) from the old policy \\( \\pi_{\\theta_{\\text{old}}} \\), forming an output group. Then, the policy model \\( \\pi_{\\theta} \\) is updated by maximizing the following objective function:\n\\[ \\begin{aligned} \\mathcal{J}_{GRPO}(\\theta) \u0026 = \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O \\mid q)\\right] \\\\ \u0026 \\quad \\frac{1}{G} \\sum_{i=1}^G \\Biggl( \\min\\biggl( \\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)} \\,A_i,\\, \\operatorname{clip}\\Bigl( \\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)}, 1-\\varepsilon,\\, 1+\\varepsilon \\Bigr)\\,A_i \\biggr) \\;-\\;\\beta\\,\\mathbb{D}_{KL}\\bigl(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\bigr) \\Biggr), \\end{aligned} \\]Similar to the PPO objective function, the GRPO objective function also includes the importance sampling and clip to ensure the stability of policy updates. The differences are:\nRelative Advantage Value \\( A_i \\): GRPO uses the relative advantage value \\( A_i \\) instead of the advantage function \\( A_t \\) in PPO. The relative advantage value \\( A_i \\) is calculated based on in-group rewards, without the need for value network estimation. KL Divergence Penalty Term \\( \\mathbb{D}_{KL}\\bigl(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\bigr) \\): To further constrain policy updates, GRPO introduces a KL divergence penalty term, limiting the difference between the new policy \\( \\pi_\\theta \\) and the reference policy \\( \\pi_{\\text{ref}} \\) from being too large. Fig. 2. The comparison of PPO and GRPO. (Image source: DeepSeek-AI, 2024)\nFrom Figure 2 above, we can see that the core innovation of GRPO lies in the calculation method of the relative advantage value \\( A_i \\). Unlike PPO, GRPO does not rely on an independent value network, but directly uses in-group rewards for relative evaluation. For each output group \\( \\{o_1, o_2, \\ldots, o_G\\} \\), GRPO first obtains the reward values \\( \\{r_1, r_2, \\ldots, r_G\\} \\) corresponding to each output. Then, the relative advantage value \\( A_i \\) is calculated according to the following formula:\n\\[ A_i = \\frac{\\,r_i \\;-\\; \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})\\,}{ \\text{std}\\bigl(\\{r_1, r_2, \\ldots, r_G\\}\\bigr)}. \\]The relative advantage value \\( A_i \\) is obtained by standardizing the in-group rewards \\( \\{r_1, r_2, \\ldots, r_G\\} \\), with zero mean and unit variance, better reflecting the relative merits of each output within the group.\nGRPO adopts the method of relative evaluation, which has the following advantages:\nNo need to train a value network: Avoids the computational overhead and instability caused by training a large-scale value network. Reduces variance in value estimation: Relative evaluation focuses on the relative merits of outputs within the group, rather than absolute values, reducing estimation variance and improving training stability. More consistent with the comparative nature of reward models: Reward models are usually trained based on comparative data, and GRPO\u0026rsquo;s relative evaluation method is more consistent with this. More suitable for credit assignment in sequence generation tasks: Even if the reward is sparse, GRPO can learn effectively because it focuses on the relative quality between outputs in the same group. Schulman Unbiased Estimator KL divergence \\( \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) \\) measures the information loss of policy \\( \\pi_\\theta \\) relative to the reference policy \\( \\pi_{\\text{ref}} \\), and its standard definition is:\n\\[ \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) = \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\log \\frac{\\pi_\\theta(o \\mid q)}{\\pi_{\\text{ref}}(o \\mid q)} \\right]. \\]As mentioned earlier, directly calculating the above expectation in practice faces challenges. To solve this problem, GRPO adopts the Schulman unbiased estimator (Schulman, 2020). Unlike the KL divergence penalty term that may be used in formula, we use the following unbiased estimator to estimate the KL divergence between \\( \\pi_\\theta \\) and \\( \\pi_{ref} \\):\n$$ \\mathbb{D}_{K L}\\left[\\pi_{\\theta}| | \\pi_{r e f}\\right]=\\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}{\\pi_{\\theta}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}{\\pi_{\\theta}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}-1. $$To understand the advantages of this estimator, we first mathematically derive its unbiasedness.\nUnbiasedness Proof To simplify the notation, let \\( r(o) = \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\). Then the Schulman estimator can be written as:\n\\[ \\hat{D}_{KL}(o) = r(o) - \\log r(o) - 1. \\]We need to prove that when \\( o \\) is sampled from \\( \\pi_\\theta(\\cdot|q) \\), the expectation of \\( \\hat{D}_{KL}(o) \\) is equal to the true KL divergence \\( \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) \\).\n\\[ \\begin{aligned} \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} [\\hat{D}_{KL}(o)] \u0026= \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} [r(o) - \\log r(o) - 1] \\\\ \u0026= \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - 1 \\right] \\\\ \u0026= \\sum_{o} \\pi_\\theta(o \\mid q) \\left[ \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - 1 \\right] \\quad (\\text{Discrete case, integral for continuous case}) \\\\ \u0026= \\sum_{o} \\left[ \\pi_{ref}(o \\mid q) - \\pi_\\theta(o \\mid q) \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\pi_\\theta(o \\mid q) \\right] \\\\ \u0026= \\underbrace{\\sum_{o} \\pi_{ref}(o \\mid q)}_{=1} - \\underbrace{\\sum_{o} \\pi_\\theta(o \\mid q) \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)}}_{=-\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})} - \\underbrace{\\sum_{o} \\pi_\\theta(o \\mid q)}_{=1} \\\\ \u0026= 1 - (-\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})) - 1 \\\\ \u0026= \\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref}). \\end{aligned} \\]Therefore, we have proven that \\( \\hat{D}_{KL}(o) \\) is an unbiased estimator of \\( \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) \\).\nComparison of Three KL Divergence Estimators To intuitively understand the differences between the three estimators, the following table lists their mathematical expressions, where \\( r(o) = \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\):\nEstimator Mathematical Expression Main Features k1 (Naive Estimator) \\( \\hat{D}_{KL}^{(k1)}(o) = \\log \\frac{\\pi_\\theta(o \\mid q)}{\\pi_{ref}(o \\mid q)} = \\log \\frac{1}{r(o)} \\) Simple and direct, corresponds to the definition of KL divergence; high variance, large fluctuations in estimation results. k2 (Squared Log-Ratio Estimator) \\( \\hat{D}_{KL}^{(k2)}(o) = \\frac{1}{2} (\\log r(o))^2 \\) Uses the square of the log-ratio, always positive, reduces variance; introduces bias, especially when distribution differences are large. k3 (Schulman Unbiased Estimator) \\( \\hat{D}_{KL}^{(k3)}(o) = r(o) - \\log r(o) - 1 \\) Combines the ratio \\( r(o) \\) and the log-ratio \\( \\log r(o) \\); unbiased, low variance, stable estimation. k1 (Naive Estimator): Unbiased, simple and direct, but with high variance, leading to unstable estimation results. k2 (Squared Log-Ratio Estimator): Reduces variance, but introduces bias, especially when the distribution difference is large, the bias is significant. k3 (Schulman Unbiased Estimator): Combines unbiasedness and low variance, providing stable estimation results. Experimental Results To evaluate the performance of the three KL divergence estimators, we conducted numerical experiments, and the results are shown in the table below. In the experiment, the distribution \\( q = \\mathcal{N}(0, 1) \\) was fixed, and the mean \\( \\mu \\) of the distribution \\( p = \\mathcal{N}(\\mu, 1) \\) was changed to control the true KL divergence \\( \\mathbb{D}_{KL}(p \\| q) \\). Monte Carlo estimation was performed using 500 million samples, and the experiment was repeated to obtain stable results.\nExperimental code can be found at unbiased_kl_divergence.py\nTrue KL Divergence Estimator Average Estimated Value Standard Deviation Relative Bias (%) 0.005 k1 0.005 0.1 0.0387 0.005 k2 0.005 0.0071 0.2415 0.005 k3 0.005 0.0071 -0.0082 0.125 k1 0.125 0.5 -0.0389 0.125 k2 0.1328 0.1875 6.2500 0.125 k3 0.125 0.1845 0.0072 0.5 k1 0.5 1 -0.0018 0.5 k2 0.625 0.866 25.0004 0.5 k3 0.5 0.8478 0.0052 Naive Estimator (k1):\nUnbiasedness: The average estimated value is highly consistent with the true KL divergence, and the relative bias is close to 0%. Variance: The standard deviation is higher than k3 and increases with the true KL divergence, leading to unstable estimation results. Squared Log-Ratio Estimator (k2):\nUnbiasedness: There is a certain bias, and the bias increases significantly with the increase of the true KL divergence (for example, when the true KL is 0.5, the relative bias reaches 25%). Variance: The variance is lower at lower true KL divergence, but the overall performance is unstable. Schulman Unbiased Estimator (k3):\nUnbiasedness: The experimental results show that the relative bias is extremely small, almost 0%, which verifies its unbiasedness. Variance: The standard deviation is significantly lower than k1, and compared with k1, it shows lower variance under all KL divergences, especially when the KL divergence is low, the advantage is significant. Advantages Summary Unbiasedness: Both theoretical and experimental results show that k3 is an unbiased estimator, which can accurately reflect the true KL divergence. Positive Definiteness: The estimated value is always non-negative, which is consistent with the nature of KL divergence. Lower Variance: Compared with k1, k3 significantly reduces the estimation variance and provides more stable estimation results, especially when the KL divergence is small, the performance is outstanding. The Schulman unbiased estimator \\( \\hat{D}_{KL}^{(k3)}(o) = r(o) - \\log r(o) - 1 \\) provides an estimation method for KL divergence that combines unbiasedness and low variance. Its unbiasedness ensures the accuracy of the estimation, and the lower variance improves the stability of the estimation, especially suitable for reinforcement learning scenarios that require stable gradient signals, such as policy optimization. Based on these advantages, the GRPO algorithm chooses to use k3 as an estimator to penalize policy deviation, thereby ensuring the stability of the training process and the performance of the final policy.\nIn actual optimization, the GRPO loss function \\( \\mathcal{L}_{GRPO}(\\theta) \\) is defined as the negative value of the objective function \\( \\mathcal{J}_{GRPO}(\\theta) \\), and the objective function \\( \\mathcal{J}_{GRPO}(\\theta) \\) is maximized by minimizing the loss function \\( \\mathcal{L}_{GRPO}(\\theta) \\):\n\\[ \\mathcal{L}_{GRPO}(\\theta) = -\\,\\mathcal{J}_{GRPO}(\\theta) \\]Comparison of PPO and GRPO To more clearly understand the similarities and differences between PPO and GRPO, the following table compares the two algorithms:\nFeature PPO GRPO Actor-Critic or Not Yes Yes (broadly considered Actor-Only) Value Network Needed Requires an independent value network (Critic) No independent value network required Advantage Estimation Estimates absolute advantage value through a value network Relatively evaluates relative advantage value through in-group rewards Computational Cost Higher, requires training a value network Lower, no need to train a value network Training Stability Relatively good, but value network training may introduce instability Better, avoids instability from value network training Algorithm Complexity Relatively complex, needs to maintain and update policy and value networks Relatively simple, only needs to maintain and update the policy network Applicable Scenarios Widely applicable to various RL tasks, including fine-tuning of small to medium-sized language models Especially suitable for RL fine-tuning of large-scale language models, focusing on efficiency and stability Credit Assignment Relies on value network for temporal difference learning to handle credit assignment issues Relies on final rewards and in-group relative evaluation, can also be assisted by intermediate rewards Variance Issue Value network estimation may introduce variance In-group relative advantage estimation may have variance under small group sizes, which can be mitigated by increasing group size, etc. As can be seen from the table, PPO is a general and powerful reinforcement learning algorithm, but its mechanism of training a value network brings additional computational burden and potential instability in LLMs scenarios. GRPO cleverly avoids the need for a value network by introducing in-group relative scoring, significantly reducing computational costs and improving training stability while ensuring performance. This makes GRPO an ideal choice for training LLMs like DeepSeek-R1-Zero when training resources are limited.\nCode Generation Evaluation Metrics Code generation employs more rigorous testing methods. The code generated by the model is executed through a compiler, and multiple unit tests are performed using predefined test cases to determine the correctness of the code. Commonly used evaluation metrics include pass@k (Chen et al. 2021) and cons@N (OpenAI, 2024).\npass@k: Measures the probability that at least one sample out of k code samples generated by the model can pass all predefined test cases.\nBiased Estimation Formula for pass@k \\[ \\text{Simplified pass@k} = \\frac{1}{P} \\sum_{i=1}^{P} C_i \\] Where, for each problem \\(i\\), \\(C_i\\) is defined as: \\[ C_i = \\begin{cases} 1 \u0026 \\text{if at least one of the k generated samples is correct} \\\\ 0 \u0026 \\text{if all k generated samples are incorrect} \\end{cases} \\]Parameter Description:\n\\( P \\): Total number of problems evaluated. \\( C_i \\): For the \\( i \\)-th problem, \\( C_i = 1 \\) if at least one of the \\( k \\) generated samples is correct, otherwise \\( C_i = 0 \\). \\( \\sum_{i=1}^{P} C_i \\): Represents the total number of problems \u0026ldquo;solved\u0026rdquo; among all \\( P \\) problems. \\( \\frac{1}{P} \\sum_{i=1}^{P} C_i \\): Represents the proportion of \u0026ldquo;solved\u0026rdquo; problems, i.e., accuracy. Formula Meaning: This simplified method directly calculates the proportion of problems for which at least one sample is correct after generating k samples. Although this method provides a biased estimate of pass@k, which may slightly overestimate the true value, it is very commonly used in practice because it is intuitive, easy to calculate, and can provide a reasonable approximation of model performance when the sample size is large enough. Especially in industrial and rapid evaluation scenarios, this simplified method is very practical.\nHowever, LLMs are affected by parameters such as temperature, top_p, top_k, and repetition_penalty during reasoning decoding. These parameters can make code generation results random and diverse, and if the parameters are set too randomly when sample K is relatively small, it will affect the evaluation results of pass@k. Therefore, using an unbiased estimation method can more accurately reflect the true performance of the model.\nUnbiased Estimation Formula for pass@k \\[ \\text { pass @ } k:=\\underset{\\text { Problems }}{\\mathbb{E}}\\left[1-\\frac{\\binom{n-c}{k}}{\\binom{n}{k}}\\right] \\]Parameter Description:\n\\( n \\): Total number of code samples generated for each problem. \\( c \\): Number of correct samples among the \\( n \\) samples that can pass all unit tests. \\( k \\): Parameter \\( k \\) in the pass@\\(k\\) metric, indicating the number of generated samples we consider. \\( \\binom{a}{b} \\): Represents the binomial coefficient, calculating the number of combinations of choosing \\( b \\) elements from \\( a \\) elements. \\( \\underset{\\text { Problems }}{\\mathbb{E}} \\): Represents the expected value (average value) over all evaluation problems. Formula Meaning:\nThe formula actually calculates the probability of having at least one correct sample. The formula \\( \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\) calculates the probability of randomly selecting \\( k \\) samples from the generated \\( n \\) samples, and none of these \\( k \\) samples are correct. We subtract this probability from 1 to get the probability of randomly selecting \\( k \\) samples from \\( n \\) samples, and at least one of these \\( k \\) samples is correct, which is the meaning of the pass@\\(k\\) metric. This formula provides an unbiased estimate of pass@k, which is more suitable for scenarios requiring precise evaluation such as academic research. In actual calculations, a sample size \\( n \\) much larger than \\( k \\) is usually generated (for example, \\( n=200 \\), \\( k \\leq 100 \\) is used in papers) to more stably estimate pass@\\(k\\). Simplified Product Form of pass@k For easier numerical calculation, the original formula can also be converted into the following product form, which is still an unbiased estimate and can avoid numerical overflow problems:\n\\[ \\text { pass @ } k = \\underset{\\text { Problems }}{\\mathbb{E}}\\left[1 - \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i}\\right] \\]Derivation Process:\nThe opposite of having at least one correct sample is that all k samples are incorrect. Therefore, pass@k is equal to 1 minus the probability that all k samples are incorrect.\nConsider the scenario of sampling without replacement. Assume we draw \\( k \\) samples from \\( n \\) samples, and we want to calculate the probability that all \\( k \\) samples are incorrect. There are a total of \\( n \\) samples, of which \\( n-c \\) are incorrect.\nWhen drawing for the first time, the probability of drawing an incorrect sample is \\( \\frac{n-c}{n} \\).\nGiven that an incorrect sample was drawn in the first draw, when drawing for the second time, among the remaining \\( n-1 \\) samples, there are \\( n-c-1 \\) incorrect samples. Therefore, the conditional probability of still drawing an incorrect sample for the second time is \\( \\frac{n-c-1}{n-1} \\).\nBy analogy, when drawing for the \\( i \\)-th time ( \\( i \\) from 1 to \\( k \\)), given that incorrect samples were drawn in the previous \\( i-1 \\) times, the conditional probability of still drawing an incorrect sample for the \\( i \\)-th time is \\( \\frac{n-c-(i-1)}{n-(i-1)} = \\frac{n-c-i+1}{n-i+1} \\). To align with the index \\( i=0 \\) in the formula, we change the index to range from \\( i=0 \\) to \\( k-1 \\), then when drawing for the \\( (i+1) \\)-th time ( \\( i \\) from 0 to \\( k-1 \\)), the conditional probability is \\( \\frac{n-c-i}{n-i} \\).\nMultiply these conditional probabilities of \\( k \\) draws to get the probability that all \\( k \\) samples are incorrect:\n\\[ P(\\text{all k samples are incorrect}) = \\frac{n-c}{n} \\times \\frac{n-c-1}{n-1} \\times \\cdots \\times \\frac{n-c-k+1}{n-k+1} = \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i} \\] Finally, the simplified formula for pass@k is:\n\\[ \\text { pass @ } k = \\underset{\\text { Problems }}{\\mathbb{E}}\\left[1 - \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i}\\right] \\] This product form formula avoids directly calculating binomial coefficients that may be numerically large, is easier to understand and numerically calculate, especially in programming implementation, it can be multiplied term by term to effectively prevent numerical overflow.\ncons@N cons@N: By generating N samples and selecting the answer with the highest frequency as the final answer, the accuracy of this answer is evaluated. In the evaluation of DeepSeek-R1-Zero, cons@64 was used, i.e., 64 samples were generated, and the answer that appeared most frequently among them was taken as the final answer for evaluation.\n\\[ \\text{cons@N} = \\frac{1}{P} \\sum_{i=1}^{P} \\mathbb{I}(\\text{ConsensusAnswer}_i \\text{ is correct}) \\]Parameter Description:\n\\( P \\): Total number of problems evaluated. \\( \\text{ConsensusAnswer}_i \\): Consensus answer obtained through majority voting. \\( \\mathbb{I}(\\text{ConsensusAnswer}_i \\text{ is correct}) \\): Indicator function, 1 if the consensus answer is correct, 0 otherwise. Formula Meaning: Calculate the proportion of consensus answers that are correct among all evaluation problems. By increasing the number of generated samples \\( N \\) and adopting a majority voting strategy, the cons@N metric can more stably and reliably evaluate the average performance of the model. In cases where the model\u0026rsquo;s generated results have a certain degree of randomness, this metric can verify the consistency and accuracy of the model\u0026rsquo;s output.\nReward Model Reward models are crucial in the development of LLMs, mainly used in the following key stages:\nReinforcement Learning from Human Feedback: In the Reinforcement Learning from Human Feedback (RLHF) process, reward models are used to evaluate the quality of model-generated results and provide reward signals for subsequent reinforcement learning.\nKey Tool for Rejection Sampling: In the rejection sampling process, reward models score a large number of candidate results and filter out high-quality samples for SFT. Rejection sampling is an important method for automated sample engineering, and reward models are its core component.\nDiscriminator in Business Scenarios: In practical applications, reward models serve as discriminators or validators of LLM output results, evaluating the quality of generated results. Only results with scores exceeding a preset threshold will be output, otherwise, regeneration or degradation processing will be performed to improve the reliability and safety of the output.\nORM vs PRM Fig. 3. Outcome reward vs Process reward. (Image source: Zeng et al. 2024)\nCurrent reward models are mainly divided into two paradigms: Outcome Reward Model (ORM) and Process Reward Model (PRM). Figure 3 above intuitively shows the difference between these two reward models. The following table also compares the main characteristics of these two models:\nFeature ORM PRM Definition Holistically scores the complete result generated by the model Provides fine-grained scoring for each step or stage during content generation Main Advantages Simple and direct, easy to implement Comprehensive evaluation of overall results Provides more refined reward signals Helps guide each step of the model generation process Main Disadvantages High variance, large fluctuations in estimation results Lack of feedback during the process More complex to train and apply May introduce bias, especially when distribution differences are large Applicable Scenarios Tasks requiring overall evaluation of generated results Tasks requiring fine-grained control of the generation process, such as step-by-step reasoning or complex generation tasks Ability to Avoid Reward Hacking Medium, depends on the accuracy of overall scoring Lower, can cheat by optimizing rewards for each step rather than overall performance Training Complexity Lower, no need for additional supervision of the generation process Higher, needs to score at each step of generation, increasing computational and data requirements Explainability High, scoring is based on the final result Lower, scoring involves multiple steps of the generation process, difficult to fully understand the scoring basis for each step To train DeepSeek-R1-Zero, the DeepSeek team chose ORM instead of PRM. This choice is based on the following considerations:\nAvoiding Reward Hacking PRM is prone to being exploited by agents in large-scale RL training, leading to reward hacking (Gao et al. 2022). Models may adopt \u0026ldquo;shortcuts\u0026rdquo; to maximize rewards instead of improving reasoning ability. Rule-based reward systems effectively avoid reward hacking problems through clear and interpretable rules.\nRule-based reward systems may be difficult to cover all types of questions when the problem scenario is complex or creative answers are required, and rule design may have loopholes that can be exploited by the model.\nReducing Training Complexity Training PRM requires a lot of computing resources and data, increasing the complexity of the training process. Rule-based reward systems, on the other hand, do not require additional training, and rules can be directly applied once determined, simplifying the training process. Rule-based reward systems are particularly suitable for tasks with automatic scoring or clear objectives, such as math problems, LeetCode programming problems, and tasks with clear output format requirements. For open-ended dialogue or creative tasks, it may be necessary to combine human feedback or trained reward models.\nReward Mechanism The reward system of DeepSeek-R1-Zero adopts a dual reward mechanism, which is automatically evaluated through predefined rules to ensure the efficiency and real-time nature of the evaluation process. This system mainly includes the following two types of rewards:\n1. Accuracy Reward\nDefinition: Measures the correctness of the model output result, which is the most critical part of the reward system. Implementation Method: Different verification methods are adopted according to different task types: Math Problems: Verify whether the final answer is consistent with the standard answer. Code Generation: Execute the code generated by the model through a compiler, and use preset unit test cases for multiple tests to determine the correctness of the code. Purpose: Guide the model to generate accurate and reliable output results. 2. Format Reward\nDefinition: A reward mechanism introduced to improve the readability and structure of model output, facilitating subsequent analysis and evaluation. Evaluation Method: Automatically evaluated by a predefined rule system during reinforcement learning training. Purpose: Encourage the model to generate structured output, such as including the thinking process and the final answer, making it easier to understand and analyze. The reward function \\( r(o) \\) of DeepSeek-R1-Zero consists of a weighted sum of accuracy reward and format reward:\n$$ r(o) = r_{\\text{accuracy}}(o) + \\lambda \\cdot r_{\\text{format_effective}}(o) $$Where, the effective format reward \\( r_{\\text{format_effective}}(o) \\) is calculated as follows:\n$$ r_{\\text{format_effective}}(o) = \\begin{cases} r_{\\text{format}}(o) \u0026 \\text{if the basic format of } o \\text{ meets the requirements} \\\\ 0 \u0026 \\text{if the basic format of } o \\text{ does not meet the requirements} \\end{cases} $$The basic format reward \\( r_{\\text{format}}(o) \\) is graded according to the degree of compliance with the format specification:\n$$ r_{\\text{format}}(o) = \\begin{cases} R_{\\text{format_full}} \u0026 \\text{if the format of } o \\text{ fully complies with the specification} \\\\ R_{\\text{format_partial}} \u0026 \\text{if the format of } o \\text{ partially complies with the specification} \\\\ 0 \u0026 \\text{if the format of } o \\text{ does not comply with the specification} \\end{cases} $$Training Template To guide the base model to follow specific instructions, the DeepSeek team designed a concise and effective training template. This template requires the model to first generate a reasoning process (placed between \u0026lt;think\u0026gt; and \u0026lt;/think\u0026gt; tags), and then provide the final answer (placed between \u0026lt;answer\u0026gt; and \u0026lt;/answer\u0026gt; tags). This structured format not only ensures the readability of the output but also allows researchers to clearly observe the model\u0026rsquo;s reasoning process during RL training, thereby more accurately assessing the model\u0026rsquo;s learning progress.\nRole Prompt Content Assistant Reply User prompt (question posed by the user) Assistant: \u0026lt;think\u0026gt; Reasoning process \u0026lt;/think\u0026gt; \u0026lt;answer\u0026gt; Answer \u0026lt;/answer\u0026gt; \u0026lt;think\u0026gt; and \u0026lt;/think\u0026gt; (thinking process tags): Used to wrap the model\u0026rsquo;s intermediate reasoning steps, clearly showing the model\u0026rsquo;s thinking process, facilitating understanding of the model\u0026rsquo;s reasoning logic and error analysis. \u0026lt;answer\u0026gt; and \u0026lt;/answer\u0026gt; (final answer tags): Used to wrap the model\u0026rsquo;s final answer, facilitating automated extraction of the answer portion for efficient evaluation and subsequent processing. Evaluation Process Accuracy Assessment: Evaluates whether the model output \\(o\\) answer is correct, calculating the accuracy reward \\(r_{\\text{accuracy}}(o)\\). Basic Format Check: Checks if the output \\(o\\) basic format meets predefined requirements, such as whether it contains necessary tags \u0026lt;think\u0026gt; and \u0026lt;answer\u0026gt;, and whether the tags are correctly closed and nested. Effective Format Reward Determination: Basic format does not comply: Effective format reward \\(r_{\\text{format_effective}}(o) = 0\\). Basic format complies: Further evaluates the degree of format standardization, calculating the basic format reward \\(r_{\\text{format}}(o)\\). Final Reward Calculation: Combines the accuracy reward \\(r_{\\text{accuracy}}(o)\\) and effective format reward \\(r_{\\text{format_effective}}(o)\\) through linear weighting to obtain the final reward \\(r(o)\\). By combining accuracy rewards and format rewards, DeepSeek-R1-Zero\u0026rsquo;s reward system not only focuses on the correctness of model outputs but also emphasizes the structure and readability of output results. This enables the model not only to provide correct answers but also to demonstrate its thinking process, making it more like an intelligent agent with reasoning capabilities rather than just a simple answer output machine.\nBenchmark Testing Fig. 4. Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. (Image source: DeepSeek-AI, 2025)\nFigure 4 shows the performance of different models on multiple benchmark tests. In the AIME 2024 benchmark test, the DeepSeek-R1-Zero model achieved a pass@1 score of 71.0%, with a cons@64 score of 86.7%, comparable to the OpenAI o1-0912 model.\nSelf-Reflection Fig. 5. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)\nFigure 5 shows that as the training process deepened, DeepSeek-R1-Zero gradually emerged with self-evolution capabilities. The model not only could self-reflect to revisit and reevaluate previous reasoning steps but also actively explore alternative solution paths. Additionally, the model could adaptively increase the number of tokens generated during the reasoning process based on the complexity of the problem, allowing for longer, more in-depth thinking time. Overall, this dynamic and spontaneous behavior significantly enhanced the model\u0026rsquo;s reasoning ability, enabling it to solve more complex and challenging tasks more efficiently and accurately.\nFig. 6. An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. (Image source: DeepSeek-AI, 2025)\nFigure 6 shows an \u0026ldquo;aha moment\u0026rdquo; experienced by the DeepSeek-R1-Zero model. At this stage, the model essentially realized during the reasoning process that its previous thinking path was incorrect, quickly adjusted its thinking direction, and ultimately arrived at the correct answer. This phenomenon also indicates that the model has developed a certain self-reflection capability during the reasoning process.\nDeepSeek-R1 Training Process To further improve model performance based on DeepSeek-R1-Zero, the DeepSeek team adopted a multi-stage training strategy and incorporated cold-start data into the training process. The training process of DeepSeek-R1 mainly includes the following four stages, reflecting the progressive path from initial policy initialization to comprehensive capability improvement:\nCold Start: Using high-quality long Chain-of-Thought (CoT) data, perform preliminary SFT on the DeepSeek-V3-Base base model to lay the foundation for subsequent reinforcement learning.\nReasoning-Oriented RL: Based on the cold-start model, apply reinforcement learning algorithms, focusing on enhancing the model\u0026rsquo;s ability in reasoning-intensive tasks.\nRejection Sampling \u0026amp; SFT: Filter high-quality reasoning data through rejection sampling technology, and combine it with non-reasoning data for SFT to further improve the model\u0026rsquo;s reasoning ability and general ability.\nAll-Scenario RL: Comprehensively consider reasoning and non-reasoning tasks, and conduct the second stage of reinforcement learning to align the model with human preferences and improve performance in a wider range of scenarios.\nCold Start In the training process of DeepSeek-R1, the cold start stage is crucial, like the igniter of an engine, laying a solid foundation for the subsequent complex reinforcement learning process. SFT is the core technology of the cold start stage.\nTraining Objective The objective of the cold start stage is clear and critical: using high-quality Chain-of-Thought (CoT) data to perform preliminary fine-tuning on the DeepSeek-V3-Base base model. This fine-tuning aims to quickly endow the model with the following core capabilities:\nPreliminary Reasoning Ability: Guide the model to learn to imitate the human reasoning process, laying the foundation for more complex reasoning. Good Text Generation Quality: Ensure the fluency and naturalness of the text output by the model, improving the user experience. These CoT data are like the model\u0026rsquo;s \u0026ldquo;starting fuel\u0026rdquo;, helping the model quickly grasp human reasoning patterns and providing good policy initialization for subsequent reinforcement learning, effectively avoiding the inefficiency and instability of RL training starting from scratch in the early stage.\nData Construction To construct high-quality cold-start data, the DeepSeek team conducted multi-faceted explorations and finally integrated the following efficient methods:\nFew-shot Prompting: Using a small number of high-quality examples to guide the model to generate longer, deeper, and more logical CoT data. Model Generation + Reflection Verification: Directly prompt the model to generate answers, and add reflection and verification links to ensure the quality of answers and the correctness of reasoning. Optimize R1-Zero Output: Collect the output of the DeepSeek-R1-Zero model, and improve the readability and overall quality of the data through manual annotation and optimization. Through the above strategies, the DeepSeek team accumulated thousands of high-quality cold-start data, and used this as a basis to fine-tune DeepSeek-V3-Base as a solid starting point for reinforcement learning.\nAdvantages of Cold Start Compared to directly using DeepSeek-R1-Zero as a starting point, cold-start data brings several significant advantages, laying a better foundation for subsequent training:\nImproved Readability:\nThe output of DeepSeek-R1-Zero has readability challenges, such as language mixing, lack of structured format, etc. Cold-start data is specially designed with a more readable output mode, including: Adding Summary: Add a refined summary at the end of the reply to quickly extract core conclusions. Filtering Bad Replies: Remove unfriendly or low-quality replies to ensure data purity. Structured Output Format: Adopt the | special_token | \u0026lt;reasoning_process\u0026gt; | special_token | \u0026lt;summary\u0026gt; format to clearly present the reasoning process and summary. Enhanced Performance:\nBy carefully designing data patterns that incorporate human prior knowledge, the DeepSeek team observed a significant improvement in model performance compared to R1-Zero. This further verifies that iterative training is an effective path to improve the performance of reasoning models. Superior Policy Initialization:\nThe core of SFT in the cold start stage is policy initialization. Policy initialization is a key step in building Reasoning LLMs, such as the OpenAI o1 series. By learning high-quality CoT data, the model initially grasps human reasoning patterns and has the ability to generate structured reasoning processes, laying a solid foundation for subsequent reinforcement learning training and avoiding the dilemma of starting exploration from scratch. SFT The core objective of Supervised Fine-tuning (SFT) is to fine-tune the model on supervised labeled data so that its predictions are as close as possible to the true labels. This aims to improve the model\u0026rsquo;s ability in specific tasks and instruction execution.\nLoss Function The training objective of SFT is to minimize the difference between model predictions and true labels. The loss function usually adopts Cross-Entropy Loss, also known as Negative Log-Likelihood, to measure the difference between the model\u0026rsquo;s predicted token distribution and the true token distribution. To balance the contributions of output sequences of different lengths, we usually normalize the loss function to the average loss per token.\nThe loss function formula is as follows:\n\\[ \\mathcal{L}_{SFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\log \\pi_\\theta\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]Parameter Description:\n\\( \\mathcal{L}_{SFT}(\\theta) \\): SFT loss function, minimized by adjusting model parameters \\( \\theta \\). \\( \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}[\\cdot] \\): Expectation over the SFT dataset distribution \\( P_{sft}(Q, O) \\). \\( P_{sft}(Q, O) \\): SFT dataset distribution, \\( q \\) represents the question (Query), and \\( o \\) represents the corresponding standard answer output (Output). \\( (q, o) \\): Question-answer pair sampled from the SFT dataset. \\( |o| \\): Token length of the standard answer output. \\( o_t \\): The \\( t \\)-th token of the standard answer output. \\( o_{\u0026lt;t} \\): The first \\( t-1 \\) tokens of the standard answer output. \\( \\pi_\\theta\\left(o_t \\mid q, o_{\u0026lt;t} \\right) \\): Given the question \\( q \\) and the preceding text \\( o_{\u0026lt;t} \\), the probability of the model predicting token \\( o_t \\). \\( \\frac{1}{|o|} \\): Length normalization factor, dividing the total loss by the output sequence length to get the average loss per token. The SFT loss function aims to penalize deviations between model predictions and standard answers. For a given question \\( q \\) and standard answer \\( o \\), the loss function calculates the probability \\( \\pi_\\theta(o_t | q, o_{\u0026lt;t} ) \\) of the model predicting each token \\( o_t \\) in the answer \\( o \\). By dividing by the output length \\( |o| \\), the loss function is normalized to the average negative log-likelihood per token.\nWhen the model accurately predicts the standard answer token, \\( \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\approx 1 \\), \\( \\log \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\approx 0 \\), and the loss value is close to the minimum. When the model prediction deviates from the standard answer, \\( \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\) is smaller, \\( \\log \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\) is negative and has a larger absolute value, and the loss value increases. The process of minimizing the SFT loss function is the process of making the model learn to generate text as similar as possible to the standard answers in the training dataset. From the perspective of negative log-likelihood, the goal is to find the optimal model parameters \\( \\theta \\) to maximize the probability of the model generating the answer \\( o \\) in the training data, which is equivalent to minimizing the negative log-likelihood of generating the answer \\( o \\). High-quality CoT data contains human preferences for reasoning and results, so SFT can also be regarded as a process of making the model learn and fit human reasoning preferences.\nGradient The gradient of the SFT loss function is used to guide model parameter updates to reduce the loss value. The gradient of the loss function with respect to the model parameters \\( \\theta \\) is:\n\\[ \\nabla_{\\theta} \\mathcal{L}_{SFT} = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]Parameter Description:\n\\( \\nabla_{\\theta} \\mathcal{L}_{SFT} \\): Gradient of the SFT loss function with respect to parameter \\( \\theta \\), indicating the direction in which the loss function value decreases fastest. \\( \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right) \\): Gradient of the token probability logarithm \\( \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right) \\) with respect to parameter \\( \\theta \\). \\( \\frac{1}{|o|} \\): Length normalization factor, consistent with the loss function, the gradient is also the gradient of the average loss per token. When actually calculating the gradient, stochastic gradient descent algorithm is usually used to update the model parameters along the gradient descent direction, gradually minimizing the loss function and improving the accuracy of the model in generating standard answers.\nGradient Coefficient\nIn the SFT stage, the gradient coefficient is usually set to 1, which means that all training samples contribute equally to the update of model parameters. The model learns each example equally, striving to minimize the average loss over the entire dataset.\nData Source and Human Preference Data Source: The SFT dataset mainly consists of high-quality long Chain-of-Thought (CoT) examples, representing the \u0026ldquo;standard answers\u0026rdquo; that the model is expected to learn, used to guide the minimization of the loss function. Data may come from manual annotation or generation by more powerful models. Refer to the SFT dataset OpenO1-SFT of the Open-o1 project, which contains long CoT replies. Human Preference: In the SFT stage, human selection can be regarded as an implicit reward function. High-quality CoT data reflects human expectations for model reasoning and output. By learning these data, the model minimizes the deviation from human expected output, thereby fitting human preferences. Reasoning-Oriented Reinforcement Learning After cold-start fine-tuning, the DeepSeek team further improved the model\u0026rsquo;s ability in reasoning-intensive tasks (such as coding, mathematics, science, and logical reasoning) through reinforcement learning (RL). The core of this stage is to maximize the reward function, guiding the model to learn more effective reasoning strategies.\nReward Function To solve the problem of CoT language mixing during reasoning, the DeepSeek team introduced language consistency reward and combined it with task reward to form the total reward function:\n\\[ r(o) = r_{\\text{task}}(o) + \\alpha \\cdot r_{\\text{lang_consistency}}(o) \\]Parameter Description:\n\\( r(o) \\): Total reward function, the goal of RL training is to maximize this function. \\( r_{\\text{task}}(o) \\): Task reward based on task completion, measuring the accuracy of model reasoning. \\( r_{\\text{lang_consistency}}(o) \\): Language consistency reward, measuring the language purity of CoT output. \\( \\alpha \\): Hyperparameter, balancing the weights of task reward and language consistency reward. The total reward function is the weighted sum of task reward and language consistency reward. Maximizing \\( r(o) \\) drives the model to improve reasoning accuracy while maintaining the language consistency of CoT output. The role of \\( \\alpha \\) is to adjust the model\u0026rsquo;s emphasis on language consistency.\nTraining Objective By maximizing the above reward function, the DeepSeek team conducted RL training on the model after cold-start fine-tuning, optimizing model parameters to obtain higher reward values in reasoning tasks, and ultimately improving reasoning ability.\nRFT Rejection Sampling Fine-tuning (RFT) aims to improve the general ability of the model by refining training data. Its core idea is to minimize the selective loss function, guiding the model to learn the generation patterns of high-quality outputs.\nLoss Function RFT adopts a rejection sampling strategy to distinguish the generation and selection processes of reasoning data and non-reasoning data, and constructs a high-quality SFT dataset. The training objective is to minimize the following loss function:\n\\[ \\mathcal{L}_{RFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q) \\times \\pi_{sft}(O \\mid q)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\mathbb{I}(o) \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]Where, the indicator function \\( \\mathbb{I}(o) \\) is defined as:\n\\[ \\mathbb{I}(o) = \\begin{cases} 1, \u0026 \\text{if output } o \\text{ is judged to be high quality} \\\\ 0, \u0026 \\text{otherwise} \\end{cases} \\]Parameter Description:\n\\( \\mathcal{L}_{RFT}(\\theta) \\): RFT loss function. \\( P_{sft}(Q) \\): Distribution of question \\( q \\). \\( \\pi_{sft}(O \\mid q) \\): Conditional probability distribution of the SFT model generating output \\( O \\) given question \\( q \\). \\( \\mathbb{I}(o) \\): Indicator function, used to select high-quality answers. It is 1 when output \\( o \\) is judged to be high quality, and 0 otherwise. The RFT loss function is based on cross-entropy loss, and selectively learns high-quality outputs through the indicator function \\( \\mathbb{I}(o) \\):\nHigh-quality output (\\( \\mathbb{I}(o) = 1 \\)): The loss function degenerates into standard cross-entropy loss, and the model updates parameters based on the negative log-likelihood of high-quality answers, minimizing the difference between model predictions and high-quality answers. Low-quality output (\\( \\mathbb{I}(o) = 0 \\)): The loss function is zero, and low-quality answers do not participate in parameter updates. RFT guides the model to focus on learning the generation patterns of high-quality answers by minimizing the loss function, achieving selective learning.\nData Generation High-quality data (reasoning data): Generate candidate answers through the RL model, use a reward model (or DeepSeek-V3 model) to score, and reject sample to retain high-score answers. SFT data (non-reasoning data): Reuse the SFT dataset of DeepSeek-V3 and its generation process. Training Process Use the high-quality dataset obtained by rejection sampling to perform SFT on the DeepSeek-V3-Base model, minimize the RFT loss function, and improve the model\u0026rsquo;s reasoning and general abilities.\nRFT iteratively refines data and retrains the model, expecting the model to learn higher quality data patterns in each iteration, and finally converge to a high-quality output model. In the iterative process, the training data distribution \\( P_{sft}(Q, O) \\) gradually focuses on high-quality data, enabling the model to continuously improve its ability to generate high-quality outputs in the process of loss minimization.\nOnRFT Online Rejection Sampling Fine-tuning (OnRFT) has a similar objective to RFT, both aiming to learn high-quality output patterns by minimizing the selective loss function. The main difference between OnRFT and RFT is the data sampling method, and the loss function form is consistent with RFT. The gradient of the OnRFT loss function is:\n\\[ \\nabla_{\\theta} \\mathcal{L}_{OnRFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q) \\times \\pi_{\\theta}(O \\mid q)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\mathbb{I}(o) \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]Parameter Description:\n\\( \\nabla_{\\theta} \\mathcal{L}_{OnRFT} \\): Gradient of the OnRFT loss function with respect to model parameter \\( \\theta \\), indicating the direction of loss function decrease. \\( \\pi_{\\theta}(O \\mid q) \\): Conditional probability distribution of the current training model generating output \\( O \\) given question \\( q \\). Comparison of RFT and OnRFT The table below briefly compares the main differences between RFT and OnRFT.\nFeature RFT OnRFT Data Generation Method Offline Online Data Generation Model SFT model \\( \\pi_{sft} \\) Current training model \\( \\pi_{\\theta} \\) Rejection Sampling Data Source Pre-generated SFT dataset Real-time data generation during training Data Loop Separated Online loop Loss Function Mechanism Selective cross-entropy loss, selects high-quality output for learning Selective cross-entropy loss, selects high-quality output for learning Training Data Distribution Change Gradually focuses on high-quality data Dynamic change, fits current model capability All-Scenario Reinforcement Learning To further align with human preferences, the DeepSeek team conducted the second stage of RL, aiming to improve the model\u0026rsquo;s Helpfulness and Harmlessness while maximizing the reward function, and also taking into account reasoning ability. This stage still uses maximizing the reward function to guide model training, but the design of the reward function is more complex to reflect multi-dimensional optimization goals.\nThe RL training in this stage combines:\nDiverse Prompt Distribution: Covers a wider range of scenarios, including reasoning and general tasks. Multi-objective Reward Signals: Reasoning Data: Follows the rule-based task reward, focusing on reasoning accuracy. Maximize task reward to guide the model to minimize reasoning errors. General Data: Uses a reward model to capture human preferences for helpfulness and harmlessness. The goal of the reward model is to learn human preferences and output reward signals consistent with human preferences. The goal of RL training is to maximize the reward value given by the reward model, thereby indirectly minimizing the deviation between model output and human preferences. Distillation To transfer the powerful reasoning ability of DeepSeek-R1 to more efficient small models, the DeepSeek team adopted Distillation (Hinton et al. 2015) technology. The distillation process mainly includes the following steps:\nData Generation: Use the trained DeepSeek-R1 model to generate about 800,000 high-quality reasoning data. These data not only include reasoning-intensive tasks (such as math problems, programming problems), but also cover general tasks (such as question answering, dialogue) to ensure the diversity and coverage of distillation data.\nModel Fine-tuning: Use the generated 800,000 high-quality reasoning data to fine-tune small dense models. Distillation experiments selected Qwen and Llama series models as Student models, covering multiple model scales from 1.5B to 70B parameters to explore the effect of distillation technology under different model scales. The selected Student models include Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct.\nPerformance Evaluation: Conduct a comprehensive performance evaluation of the distilled models in multiple reasoning-related benchmarks. The evaluation results are intended to verify whether distillation technology can effectively transfer the reasoning ability of large models to small models, and to investigate whether the reasoning ability of distilled small models can reach or even exceed the level of large models.\nKL Divergence Distillation In addition to directly using the text output generated by the Teacher model as pseudo-labels for SFT distillation, a more rigorous method is to also consider the token probability distribution \\( \\pi_{\\text{teacher}} \\) generated by the Teacher model. KL divergence distillation is a commonly used method, which not only allows the Student model to learn the text output of the Teacher model, but also learns the token probability distribution of the Teacher model. By minimizing the KL divergence between the output probability distributions of the Student model and the Teacher model, the knowledge of the Teacher model can be more fully transferred to the Student model. However, in actual engineering, directly using the text output of the Teacher model as pseudo-labels for SFT distillation can usually achieve sufficiently good results and is simpler to implement.\nExperimental Results The experimental results are shown in Figure 6:\nFig. 6. Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. (Image source: DeepSeek-AI, 2025)\nThe experimental results indicate that this direct SFT distillation method can substantially enhance the inference capabilities of smaller models. Notably, on various benchmark tests excluding CodeForces, the distilled Llama-3.3-70B-Instruct model outperforms OpenAI-o1-mini. Achieving such remarkable improvements solely through SFT distillation from a larger base model demonstrates the strong potential of this approach for future research and applications.\nDiscussion DeepSeek-R1, based on a multi-stage training framework, explores a simplified path for Reasoning Model training technology, mainly including the following points:\nLinearized Thinking Process: CoT Replaces MCTS\nTraditional reinforcement learning AI, such as Go and chess, once relied on Monte Carlo Tree Search (MCTS). DeepSeek-R1 and other models explore the use of autoregressive chain-of-thought methods to simplify the reasoning process, gradually abandoning computationally complex MCTS. CoT decomposes complex reasoning into linear steps, and the model reasons step by step like solving a problem, rather than the exhaustive search of MCTS. This linearized thinking reduces computational complexity, is more in line with human thinking habits, and makes it easier for models to learn complex reasoning strategies. Eliminating Independent Value Networks: Simplifying RL Architecture\nTraditional reinforcement learning (such as PPO) usually requires independent policy networks and value networks. DeepSeek-R1 and other studies have found that strengthened policy networks or simplified value evaluation methods (such as GRPO\u0026rsquo;s in-group relative scoring) can replace independent value networks. This simplifies the RL training architecture, reduces resource requirements, and improves efficiency. It shows that the policy network of LLMs already has strong value evaluation capabilities, and no additional value network is needed. Focusing on Outcome Rewards: Minimizing Reward Signals\nDeepSeek-R1 adopts a simpler ORM reward strategy, mainly focusing on the accuracy reward of the final result, weakening the reward for intermediate reasoning steps. This strategy is inspired by AlphaZero (Silver et al. 2017), which only focuses on winning or losing. For Reasoning Models, outcome rewards may be more effective than PRM, which can help models learn \u0026ldquo;ways of thinking\u0026rdquo; more naturally and reduce cumbersome step-by-step supervision. Increasing Thinking Time: Model Spontaneously Emerges Deep Thinking\nDeepSeek-R1-Zero training shows the ability to spontaneously increase thinking time. As training deepens, the model adaptively allocates more \u0026ldquo;thinking time\u0026rdquo; according to the complexity of the question, generating longer reasoning sequences. This increase in \u0026ldquo;thinking time\u0026rdquo; is an emergent behavior of the model in RL training. Increasing thinking time reflects the model\u0026rsquo;s deeper exploration and optimization of the thinking process. Complex problems require more reasoning steps to find answers. The self-evolution ability of DeepSeek-R1-Zero confirms the potential of reinforcement learning in improving model reasoning ability. Summary The success of DeepSeek-R1 demonstrates the great potential of RL in improving the reasoning ability of LLMs. The GRPO algorithm adopted by DeepSeek-R1 is superior to PPO and DPO in terms of computational efficiency, optimization stability, and reward robustness, and reduces training resource consumption by simplifying the model architecture. DeepSeek-R1 provides a path worth referencing for open-source Reasoning Model replication of o1.\nReferences [1] Guo, Daya, et al. \u0026ldquo;Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\u0026rdquo; arXiv preprint arXiv:2501.12948 (2025).\n[2] OpenAI. \u0026ldquo;Introducing OpenAI o1\u0026rdquo; OpenAI, 2024.\n[3] Jaech, Aaron, et al. \u0026ldquo;OpenAI o1 system card.\u0026rdquo; arXiv preprint arXiv:2412.16720 (2024).\n[4] Open-r1. HuggingFace, 2024.\n[5] Sutton, Richard. \u0026ldquo;The bitter lesson.\u0026rdquo; Incomplete Ideas (blog) 13.1 (2019): 38.\n[6] Liu A, et al. \u0026ldquo;Deepseek-v3 technical report.\u0026rdquo; arXiv preprint arXiv:2412.19437 (2024).\n[7] Schulman, John, et al. \u0026ldquo;Proximal policy optimization algorithms.\u0026rdquo; arXiv preprint arXiv:1707.06347 (2017).\n[8] Ouyang, Long, et al. \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in neural information processing systems 35 (2022): 27730-27744.\n[9] Shao, Zhihong, et al. \u0026ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\u0026rdquo; arXiv preprint arXiv:2402.03300 (2024).\n[10] J. Schulman. Approximating kl divergence, 2020.\n[11] Gao, Leo, John Schulman, and Jacob Hilton. \u0026ldquo;Scaling laws for reward model overoptimization.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n[12] Chen, Mark, et al. \u0026ldquo;Evaluating large language models trained on code.\u0026rdquo; arXiv preprint arXiv:2107.03374 (2021).\n[13] Learning to Reason with LLMs. OpenAI, 2024.\n[14] AMC. Mathematical Association of America (MAA), 2024.\n[15] Open-O1. Open-Source O1, 2024.\n[16] Zeng, Zhiyuan, et al. \u0026ldquo;Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective.\u0026rdquo; arXiv preprint arXiv:2412.14135 (2024).\n[17] Hinton, Geoffrey. \u0026ldquo;Distilling the Knowledge in a Neural Network.\u0026rdquo; arXiv preprint arXiv:1503.02531 (2015).\n[18] Silver, David, et al. \u0026ldquo;Mastering chess and shogi by self-play with a general reinforcement learning algorithm.\u0026rdquo; arXiv preprint arXiv:1712.01815 (2017).\nCitation Citation: Please indicate the original author and source when reprinting or citing the content of this article.\nCited as:\nYue Shui. (Jan 2025). OpenAI o1 Replication Progress: DeepSeek-R1. https://syhya.github.io/posts/2025-01-27-deepseek-r1\nOr\n@article{syhya2025deepseekr1, title = \u0026#34;OpenAI o1 Replication Progress: DeepSeek-R1\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-27-deepseek-r1\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-01-27-deepseek-r1/","summary":"\u003cp\u003eDeepSeek AI recently released \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/2501.12948\"\u003eDeepSeek-AI, 2025\u003c/a\u003e), whose reasoning performance on multiple benchmarks approaches the level of OpenAI\u0026rsquo;s o1 (\u003ca href=\"https://openai.com/o1/\"\u003eOpenAI, 2024\u003c/a\u003e), marking a significant step for the open-source community in successfully replicating o1. Relevant code for R1 can be found in the huggingface\u0026rsquo;s attempt to open-source replication project \u003ca href=\"https://github.com/huggingface/open-r1\"\u003eopen-r1\u003c/a\u003e. While previous research has often relied on massive amounts of supervised data to enhance the performance of Large Language Models (LLMs), the success of DeepSeek-R1 and its earlier experiment, DeepSeek-R1-Zero, powerfully demonstrates the potential of purely large-scale reinforcement learning in improving the reasoning capabilities of LLMs. This success reinforces the profound insight proposed by Richard Sutton in \u0026ldquo;The Bitter Lesson\u0026rdquo;:\u003c/p\u003e","title":"OpenAI o1 Replication Progress: DeepSeek-R1"},{"content":"Background The Transformer (Vaswani et al., 2017) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (Devlin et al., 2018) which uses only the encoder, GPT (Radford et al., 2018) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (Touvron et al., 2023) and GPT-4 (OpenAI et al., 2024), most of which adopt a decoder-only architecture.\nNotations Symbol Meaning \\(B\\) Batch Size \\(S\\) Sequence Length \\(d\\) Hidden Dimension / Model Size \\(H\\) Number of Heads in Multi-Head Attention \\(G\\) Number of Groups, used for Grouped-Query Attention (GQA) \\(d_{\\text{head}} = \\frac{d}{H}\\) Dimension of each attention head \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) Input sequence, with batch size \\(B\\), sequence length \\(S\\), and hidden dimension \\(d\\) \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{B \\times S \\times d}\\) Query, Key, and Value matrices after linear transformation \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) Trainable linear projection matrices for generating \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) respectively \\(W_O \\in \\mathbb{R}^{d \\times d}\\) Trainable linear projection matrix for mapping multi-head/grouped attention outputs back to dimension \\(d\\) \\(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h \\in \\mathbb{R}^{B \\times S \\times d_{\\text{head}}}\\) Query, Key, and Value sub-matrices for the \\(h\\)-th attention head \\(\\mathbf{K}^*, \\mathbf{V}^*\\) Shared \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) obtained by averaging or merging all heads\u0026rsquo; \\(\\mathbf{K}_h, \\mathbf{V}_h\\) in Multi-Query Attention (MQA) \\(\\mathbf{q}, \\mathbf{k}\\in \\mathbb{R}^{d_{\\text{head}}}\\) Single query and key vectors used in mathematical derivations (Central Limit Theorem) in Scaled Dot-Product Attention Attention Mechanism in Transformers The core of the Transformer model is the Self-Attention Mechanism, which allows the model to dynamically focus on different parts of the sequence when processing sequential data. Specifically, given an input sequence \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) (batch size \\(B\\), sequence length \\(S\\), hidden dimension \\(d\\)), the Transformer projects it into queries (\\(\\mathbf{Q}\\)), keys (\\(\\mathbf{K}\\)), and values (\\(\\mathbf{V}\\)) through three linear layers:\n\\[ \\mathbf{Q} = \\mathbf{X} W_Q, \\quad \\mathbf{K} = \\mathbf{X} W_K, \\quad \\mathbf{V} = \\mathbf{X} W_V \\]where \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) are trainable weight matrices. MHA enhances the model\u0026rsquo;s representational capacity by splitting these projections into multiple heads, each responsible for different subspace representations.\nThere are various forms of attention mechanisms, and the Transformer relies on Scaled Dot-Product Attention: given query matrix \\(\\mathbf{Q}\\), key matrix \\(\\mathbf{K}\\), and value matrix \\(\\mathbf{V}\\), the output is a weighted sum of the value vectors, where each weight is determined by the dot product of the query with the corresponding key:\n\\[ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V} \\] Fig. 1. Scaled Dot-Product Attention. (Image source: Vaswani et al., 2017)\nMulti-Head Attention (MHA) Multi-Head Attention (MHA) splits \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) into multiple heads, each with independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\), thereby increasing the model\u0026rsquo;s capacity and flexibility:\n\\[ \\text{MHA}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]where each head is computed as:\n\\[ \\text{head}_h = \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q}_h \\mathbf{K}_h^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V}_h \\] Fig. 2. Multi-Head Attention. (Image source: Vaswani et al., 2017)\nBenefits of Using Multi-Head Attention Capturing Diverse Features: A single-head attention mechanism can only focus on one type of feature or pattern in the input sequence, whereas MHA can simultaneously focus on different features or patterns across multiple attention heads, enabling the model to understand the input data more comprehensively. Enhanced Expressive Power: Each attention head can learn different representations, enhancing the model\u0026rsquo;s expressive power. Different heads can focus on different parts or relationships within the input sequence, helping the model capture complex dependencies more effectively. Improved Stability and Performance: MHA reduces noise and instability from individual attention heads by averaging or combining multiple heads, thereby improving the model\u0026rsquo;s stability and performance. Parallel Computation: MHA allows for parallel computation since each attention head\u0026rsquo;s calculations are independent. This boosts computational efficiency, especially when using hardware accelerators like GPUs or TPUs. Softmax in Scaled Dot-Product Attention The softmax function transforms a vector \\(\\mathbf{z} = [z_1, z_2, \\dots, z_n]\\) into a probability distribution \\(\\mathbf{y} = [y_1, y_2, \\dots, y_n]\\) defined as:\n\\[ y_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)} \\quad \\text{for} \\quad i = 1, 2, \\dots, n \\]In the attention mechanism, the softmax function is used to convert the scaled dot product \\(\\tfrac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\) into attention weights:\n\\[ \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr) = \\Bigl[ \\frac{\\exp\\Bigl(\\frac{Q_1 \\cdot K_1}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_1 \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)}, \\dots, \\frac{\\exp\\Bigl(\\frac{Q_S \\cdot K_S}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_S \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)} \\Bigr] \\]In the Transformer\u0026rsquo;s attention mechanism, the scaling factor \\(\\sqrt{d_{\\text{head}}}\\) in the scaled dot-product attention formula ensures that the dot product results do not become excessively large as the vector dimension increases before applying softmax. This is primarily for the following reasons:\nPreventing Gradient Vanishing: Scaling the attention scores avoids overly large inputs to the softmax function, preventing gradients from vanishing during backpropagation.\nNumerical Stability: Scaling ensures that the input range to the softmax function remains reasonable, avoiding extreme values that could lead to numerical instability and overflow issues, especially when the vector dimensions are large. Without scaling, the dot product results could cause the softmax\u0026rsquo;s exponential function to produce excessively large values.\nMathematical Explanation: Suppose vectors \\(\\mathbf{q}\\) and \\(\\mathbf{k}\\) have independent and identically distributed components with mean 0 and variance 1. Their dot product \\(\\mathbf{q} \\cdot \\mathbf{k}\\) has a mean of 0 and a variance of \\(d_{\\text{head}}\\). To prevent the dot product\u0026rsquo;s variance from increasing with the dimension \\(d_{\\text{head}}\\), it is scaled by \\(\\frac{1}{\\sqrt{d_{\\text{head}}}}\\). This scaling ensures that the variance of the scaled dot product remains 1, independent of \\(d_{\\text{head}}\\).\nAccording to statistical principles, dividing a random variable by a constant scales its variance by the inverse square of that constant. Therefore, the scaling factor \\(\\tfrac{1}{\\sqrt{d_{\\text{head}}}}\\) effectively controls the magnitude of the attention scores, enhancing numerical stability. The detailed derivation is as follows:\nAssume \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^{d_{\\text{head}}}\\) with independent and identically distributed components, mean 0, and variance 1. Their dot product is:\n\\[ \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d_{\\text{head}}} q_i k_i \\]By the Central Limit Theorem, for large \\(d_{\\text{head}}\\), the dot product \\(\\mathbf{q} \\cdot \\mathbf{k}\\) approximately follows a normal distribution with mean 0 and variance \\(d_{\\text{head}}\\):\n\\[ \\mathbf{q} \\cdot \\mathbf{k} \\sim \\mathcal{N}(0, d_{\\text{head}}) \\]To achieve unit variance in the scaled dot product, we divide by \\(\\sqrt{d_{\\text{head}}}\\):\n\\[ \\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}} \\;\\sim\\; \\mathcal{N}\\!\\Bigl(0, \\frac{d_{\\text{head}}}{d_{\\text{head}}}\\Bigr) = \\mathcal{N}(0, 1) \\]Thus, the scaled dot product \\(\\tfrac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}}\\) maintains a variance of 1, independent of \\(d_{\\text{head}}\\). This scaling operation keeps the dot product within a stable range, preventing the softmax function from encountering numerical instability due to excessively large or small input values.\nMulti-Query Attention (MQA) Multi-Query Attention (MQA) (Shazeer, 2019) significantly reduces memory bandwidth requirements by allowing all query heads to share the same set of keys (\\(\\mathbf{K}\\)) and values (\\(\\mathbf{V}\\)). Specifically, if we average all \\(\\mathbf{K}_h\\) and \\(\\mathbf{V}_h\\) from traditional MHA as follows:\n\\[ \\mathbf{K}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{K}_h, \\quad \\mathbf{V}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{V}_h, \\]where \\(H\\) is the number of query heads, and \\(\\mathbf{K}_h\\) and \\(\\mathbf{V}_h\\) are the keys and values for the \\(h\\)-th head, respectively. During inference, each head only needs to use the same \\(\\mathbf{K}^*\\) and \\(\\mathbf{V}^*\\), significantly reducing memory bandwidth usage. Finally, all head outputs are concatenated and projected back to the output space:\n\\[ \\text{MQA}(\\mathbf{Q}, \\mathbf{K}^*, \\mathbf{V}^*) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]Since keys and values are consolidated into a single set, MQA inference is faster but may limit the model\u0026rsquo;s expressive capacity and performance in certain scenarios.\nGrouped-Query Attention (GQA) Grouped-Query Attention (GQA) (Ainslie, 2023) serves as a compromise between MHA and MQA. It divides query heads into multiple groups, allowing each group to share a set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads, thereby balancing inference speed and model performance. Each group contains \\(\\frac{H}{G}\\) query heads and shares one set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads. The specific process is as follows:\nProjection: Project the input \\(\\mathbf{X}\\) into \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) via linear transformations. Grouped Query: After splitting \\(\\mathbf{Q}\\) into \\(H\\) heads, further divide these heads into \\(G\\) groups. Grouped Key/Value: Split \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) into \\(G\\) groups, with each group sharing a set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Within-Group Attention: Perform attention calculations for each group\u0026rsquo;s \\(\\mathbf{Q}\\) with the shared \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Concatenate Outputs: Concatenate the attention results from all groups along the channel dimension and project them through a linear layer to obtain the final output. Relationship Between the Three Attention Methods Fig. 3. Overview of grouped-query method. (Image source: Ainslie et al., 2023)\nFigure 3 intuitively illustrates the relationship between the three attention mechanisms: MHA maintains independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) for each query head; MQA allows all query heads to share the same set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\); GQA strikes a balance by sharing \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) within groups.\nWhen \\(G=1\\): All query heads share the same set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). In this case, GQA degenerates into MQA.\nNumber of \\(\\mathbf{K}/\\mathbf{V}\\) Heads: 1 Model Behavior: All heads use the same \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) for attention calculations, significantly reducing memory bandwidth requirements. When \\(G=H\\): Each query head has its own independent set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). In this case, GQA degenerates into MHA.\nNumber of \\(\\mathbf{K}/\\mathbf{V}\\) Heads: \\(H\\) Model Behavior: Each head uses completely independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\), maintaining the high model capacity and performance of MHA. By adjusting the number of groups \\(G\\), GQA allows flexible switching between MHA and MQA, achieving a balance between maintaining high model performance and improving inference speed.\nImplementation Code Snippet Below is a simple PyTorch implementation of MHA 、MQA和 GQA. For GQA, two approaches are demonstrated: broadcasting and repeating.\nAdditionally, note that in the actual implementation of LLaMA3, GQA incorporates KV Cache for optimization. To keep the example concise, this part is omitted in the code below. For more comprehensive details, you can refer to the official source code in model.py.\nMHA Code Snippet multi_head_attention.py\nimport math import torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # (nums_head * head_dim = hidden_dim) assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(dropout_rate) # Define linear projection layers self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): # x has shape: (batch_size, seq_len, hidden_dim) batch_size, seq_len, _ = x.size() # Q, K, V each has shape: (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) K = self.k_proj(x) V = self.v_proj(x) # Reshaping from (batch_size, seq_len, hidden_dim) to (batch_size, seq_len, nums_head, head_dim) # Then transpose to (batch_size, nums_head, seq_len, head_dim) # q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3) # [Another approach to do it] q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) k = K.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Matrix multiplication: (batch_size, nums_head, seq_len, head_dim) * (batch_size, nums_head, head_dim, seq_len) # Resulting shape: (batch_size, nums_head, seq_len, seq_len) # Note that the scaling factor uses head_dim, not hidden_dim. attention_val = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) print(f\u0026#34;attention_mask shape is {attention_mask.size()}\u0026#34;) if attention_mask is not None: # If attention_mask is provided, it should have shape (batch_size, nums_head, seq_len, seq_len). assert attention_val.size() == attention_mask.size() attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # Apply softmax along the last dimension to get attention weights. attention_weight = torch.softmax(attention_val, dim=-1) # Dropout on attention weights attention_weight = self.dropout(attention_weight) # Multiply attention weights with V: # (batch_size, nums_head, seq_len, seq_len) * (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v # Transpose back: (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) # # Note: The transpose operation changes the dimension ordering but does not change the memory layout, # resulting in a non-contiguous tensor. The contiguous() method makes the tensor contiguous in memory, # allowing subsequent view or reshape operations without error. output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) # output = output_mid.permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.hidden_dim) # # [Another approach to do it] output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 # attention_mask has shape: (batch_size, nums_head, seq_len, seq_len). # Here we use a lower-triangular mask to simulate causal masking. attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_head_attention = MultiHeadAttention(hidden_dim=hidden_dim, nums_head=nums_head) x_forward = multi_head_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) MQA Code Snippet multi_query_attention.py\nimport torch import torch.nn as nn import math class MultiQueryAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(p=dropout) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # For kv, project: hidden_dim -\u0026gt; head_dim self.k_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.v_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() Q, K, V = self.q_proj(x), self.k_proj(x), self.v_proj(x) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Broadcast k and v to match q\u0026#39;s dimensions for attention computation # k -\u0026gt; (batch_size, 1, seq_len, head_dim) # v -\u0026gt; (batch_size, 1, seq_len, head_dim) k = K.unsqueeze(1) v = V.unsqueeze(1) # (batch_size, head_num, seq_len, head_dim) * (batch_size, 1, head_dim, seq_len) # -\u0026gt; (batch_size, head_num, seq_len, seq_len) attention_val = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) attention_weight = torch.softmax(attention_val, dim=-1) print(f\u0026#34;attention_weight is {attention_weight}\u0026#34;) attention_weight = self.dropout(attention_weight) # (batch_size, head_num, seq_len, seq_len) * (batch_size, 1, seq_len, head_dim) # -\u0026gt; (batch_size, head_num, seq_len, head_dim) output_tmp = attention_weight @ v # -\u0026gt; (batch_size, seq_len, head_num, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_query_attention = MultiQueryAttention(hidden_dim=hidden_dim, nums_head=nums_head, dropout=0.2) x_forward = multi_query_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) GQA Code Snippet group_query_attention.py\nimport math import torch import torch.nn as nn class GQABroadcast(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # Total number of Q heads (H) self.nums_kv_head = nums_kv_head # Number of K, V heads (G, groups) assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head # Number of Q heads per group self.q_heads_per_group = nums_head // nums_kv_head self.dropout = nn.Dropout(dropout_rate) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # Projection output dimensions for K, V = nums_kv_head * head_dim self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask= None): batch_size, seq_len, _ = x.size() Q = self.q_proj(x) # (batch_size, seq_len, hidden_dim) K = self.k_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) V = self.v_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) # Q: (batch_size, seq_len, hidden_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2).contiguous() q = q.view(batch_size, self.nums_kv_head, self.q_heads_per_group, seq_len, self.head_dim) # K, V: (batch_size, seq_len, nums_kv_head * head_dim) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim # -\u0026gt; (batch_size, nums_kv_head, 1, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) # q: (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) * (batch_size, nums_kv_head, 1, head_dim, seq_len) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_val = q @ k.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) * (batch_size, nums_kv_head, 1, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) output_tmp = attention_weight @ v # (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = output_tmp.view(batch_size, self.nums_head, seq_len, self.head_dim) # (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) -\u0026gt; (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output class GQARepeat(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head self.nums_kv_head = nums_kv_head assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head self.q_head_per_group = nums_head // nums_kv_head self.q_proj = nn.Linear(hidden_dim, nums_head * self.head_dim) self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) self.dropout = nn.Dropout(dropout_rate) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() # (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) # (batch_size, seq_len, nums_kv_head * self.head_dim) K = self.k_proj(x) V = self.v_proj(x) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) # (batch_size, nums_head, seq_len, head_dim) k_repeat = k.repeat_interleave(self.q_head_per_group, dim=1) v_repeat = v.repeat_interleave(self.q_head_per_group, dim=1) # (batch_size, nums_head, seq_len, seq_len) attention_val = q @ k_repeat.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#39;-inf\u0026#39;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v_repeat # (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 16) batch_size, seq_len, hidden_dim = x.size() nums_head = 8 head_dim = hidden_dim // nums_head nums_kv_head = 4 q_heads_per_group = nums_head // nums_kv_head # v1: Broadcast # attention_mask_v1 has shape: (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_mask_v1 = torch.tril(torch.ones(batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len)) gqa_broadcast = GQABroadcast(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v1 = gqa_broadcast.forward(x, attention_mask=attention_mask_v1) # print(x_forward_v1) print(x_forward_v1.size()) # v2: Repeat # attention_mask_v2 has shape: (batch_size, nums_head, seq_len, seq_len) attention_mask_v2 = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) gqa_repeat = GQARepeat(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v2 = gqa_repeat.forward(x, attention_mask=attention_mask_v2) # print(x_forward_v2) print(x_forward_v2.size()) Time and Space Complexity Analysis Note: The following discussion focuses on the computational complexity of a single forward pass. In training, one must also account for backward pass and parameter updates, which rely on the intermediate activations stored during the forward pass. The additional computation to calculate gradients and maintain partial derivatives usually makes the total training cost (both computation and memory usage) significantly higher—often multiple times the forward-pass cost.\nWhen analyzing different attention mechanisms (MHA, MQA, GQA), our main focus is on their time complexity and space complexity during the forward pass of either self-attention or cross-attention. Even though their implementation details (e.g., whether \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) are shared) can differ, the overall computational cost and memory usage are roughly on the same order of magnitude.\nAssume that each position in the sequence produces its own representations of query \\(\\mathbf{Q}\\), key \\(\\mathbf{K}\\), and value \\(\\mathbf{V}\\). After splitting by batch size and number of heads, their shapes can be written as:\n\\[ \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\;\\in\\; \\mathbb{R}^{B \\times H \\times S \\times d_{\\text{head}}} \\]Time Complexity Analysis General Time Complexity of Matrix Multiplication For two matrices \\(\\mathbf{A}\\) of shape \\(m \\times n\\) and \\(\\mathbf{B}\\) of shape \\(n \\times p\\), the complexity of computing the product \\(\\mathbf{A}\\mathbf{B}\\) is typically expressed as:\n\\[ \\mathcal{O}(m \\times n \\times p) \\]In attention-related computations, this formula is frequently used to analyze \\(\\mathbf{Q}\\mathbf{K}^\\top\\) and the multiplication of attention scores by \\(\\mathbf{V}\\).\nMain Steps and Complexity in Self-Attention Dot Product (\\(\\mathbf{Q}\\mathbf{K}^\\top\\))\nShape of \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nShape of \\(\\mathbf{K}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nConsequently, the result of \\(\\mathbf{Q}\\mathbf{K}^\\top\\) has shape \\(B \\times H \\times S \\times S\\)\nThe calculation can be viewed as \\(S \\times S\\) dot products for each head in each batch. Each dot product involves \\(d_{\\text{head}}\\) multiply-add operations.\nHence, its time complexity is:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S \\times S \\times d_{\\text{head}}\\bigr) \\;=\\; \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] Softmax Operation\nApplied element-wise to the attention score matrix of shape \\(B \\times H \\times S \\times S\\)\nSoftmax entails computing exponentials and performing normalization on each element. The complexity is approximately:\n\\[ \\mathcal{O}(\\text{number of elements}) = \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) \\] Compared with the matrix multiplication above, this step’s dependency on \\(d_{\\text{head}}\\) is negligible and is thus often considered a smaller overhead.\nWeighted Averaging (Multiplying Attention Scores with \\(\\mathbf{V}\\))\nShape of \\(\\mathbf{V}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nShape of the attention score matrix: \\(B \\times H \\times S \\times S\\)\nMultiplying each position’s attention scores by the corresponding \\(\\mathbf{V}\\) vector yields an output of shape \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nIts time complexity is analogous to that of \\(\\mathbf{Q}\\mathbf{K}^\\top\\):\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] Combining these three steps, the dominant costs come from the two matrix multiplications, each of complexity \\(\\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}})\\). Therefore, for a single full forward pass, the total complexity can be denoted as:\n\\[ \\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}}) = \\mathcal{O}(B \\times S^2 \\times d) \\]Here, we use \\(d_{\\text{head}} = \\frac{d}{H}\\).\nTime Complexity in Incremental Decoding/Inference with KV Cache Fig. 4. KV cache example. (Image source: Efficient NLP YouTube Channel)\nAs depicted in Figure 4, incremental decoding (especially in autoregressive generation) often employs a KV Cache to store previously computed \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Thus, one does not have to recalculate keys and values at each new time step. With every new token generated (i.e., a new time step), the following operations are performed:\nCompute \\(\\mathbf{Q}\\) for the New Token (and corresponding \\(\\mathbf{K}, \\mathbf{V}\\))\nIf only the projection weights are retained, then generating the new \\(\\mathbf{Q}\\) vector and the local \\(\\mathbf{K}, \\mathbf{V}\\) involves \\(\\mathcal{O}(d^2)\\) parameters, but this overhead is small as it is only for a single token. Perform Attention with the Existing KV Cache\nThe KV Cache stores all previous \\(\\mathbf{K}, \\mathbf{V}\\) vectors, with shape:\n\\[ B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} \\]Here, \\(S_{\\text{past}}\\) is the length of the already-generated sequence.\nThe new \\(\\mathbf{Q}\\) has shape \\(B \\times H \\times 1 \\times d_{\\text{head}}\\). Hence, computing the attention scores for the new token:\n\\[ \\mathbf{Q}\\mathbf{K}^\\top : \\; \\mathcal{O}\\bigl(B \\times H \\times 1 \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] Similarly, multiplying these scores by \\(\\mathbf{V}\\) has the same order:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] Update the KV Cache\nAppend the newly generated \\(\\mathbf{K}, \\mathbf{V}\\) to the cache, so they can be used at the subsequent time step. This merely requires a concatenation or append operation, which primarily grows the memory usage rather than incurring high compute. Thus, for incremental decoding, each new token involves:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] in computation, instead of the \\(S \\times S\\) scale for each forward pass. If one aims to generate \\(S\\) tokens in total, the cumulative complexity (under ideal conditions) becomes:\n\\[ \\sum_{k=1}^{S} \\mathcal{O}\\bigl(B \\times H \\times k \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] which is the same order as the one-shot computation. The difference is that incremental decoding computes one token at a time, thus requiring lower temporary memory usage per step and avoiding a full \\(S \\times S\\) attention score matrix at once.\nSummary of Time Complexity MHA (Multi-Head Attention): Multiple heads, each computing its own \\(\\mathbf{K}, \\mathbf{V}\\). MQA (Multi-Query Attention): Multiple heads share \\(\\mathbf{K}, \\mathbf{V}\\). GQA (Grouped Query Attention): The \\(H\\) heads are divided into \\(G\\) groups, each group sharing a single \\(\\mathbf{K}, \\mathbf{V}\\). Regardless of whether we use MHA, MQA, or GQA, in a full forward pass (or the forward portion during training), the main matrix multiplications have roughly the same complexity:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) \\]On the other hand, in incremental inference with KV Cache, the per-token complexity decreases to\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] but one must maintain and update the KV Cache over multiple decoding steps.\nSpace Complexity Analysis Space complexity encompasses both model parameters (weights) and intermediate activations needed during the forward pass—particularly the attention score matrices, weighted outputs, and potential KV Cache.\nModel Parameter Scale Parameters for the Linear Projection Layers\nProjecting the input vector of dimension \\(d\\) into \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\):\n\\[ \\underbrace{d \\times d}_{\\text{Q projection}} + \\underbrace{d \\times d}_{\\text{K projection}} + \\underbrace{d \\times d}_{\\text{V projection}} = 3d^2 \\] These parameters may be split among heads, but the total remains \\(\\mathcal{O}(d^2)\\), independent of the number of heads \\(H\\).\nOutput Merging Layer\nAfter concatenating multiple heads, there is typically another \\(d \\times d\\) linear layer to project the concatenated outputs back into dimension \\(d\\). This is also \\(\\mathcal{O}(d^2)\\).\nTherefore, combining these yields:\n\\[ 3d^2 + d^2 = 4d^2 \\] which remains \\(\\mathcal{O}(d^2)\\).\nIntermediate Activations for the Forward Pass During training or a full forward pass, the following key tensors often need to be stored:\nAttention Score Matrix\nShape: \\(B \\times H \\times S \\times S\\). Regardless of MHA, MQA, or GQA, each head (or group) computes \\(\\mathbf{Q}\\mathbf{K}^\\top\\) for the attention scores, yielding:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) \\] Weighted Output\nShape: \\(B \\times H \\times S \\times d_{\\text{head}}\\), corresponding to the contextual vectors after weighting \\(\\mathbf{V}\\). Its size is:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S \\times d\\bigr) \\] Storage of \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) for Backprop\nIn backward propagation, we need the forward outputs (or intermediate gradients). If explicitly stored, their shapes and scales are usually:\nMHA (Multi-Head Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) MQA (Multi-Query Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\) (shared): \\(B \\times S \\times d\\) GQA (Grouped Query Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\) (shared by group): \\(B \\times G \\times S \\times d_{\\text{head}}\\), where \\(G \\times d_{\\text{head}} = d\\). Space Usage in Incremental Decoding (KV Cache) In inference with incremental decoding, a KV Cache is typically used to store all previously computed keys and values, thus avoiding repeated computation for past tokens. The structure is generally as follows:\nKV Cache Dimensions (MHA example):\n\\[ \\mathbf{K}, \\mathbf{V} : B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} \\] As the generated sequence length \\(S_{\\text{past}}\\) grows, the cache usage increases linearly.\nPer-Step Attention Score Matrix:\nEach new token only requires a score matrix of shape:\n\\[ B \\times H \\times 1 \\times S_{\\text{past}} \\]which is much smaller than the \\(B \\times H \\times S \\times S\\) matrix used during training.\nTherefore, in incremental decoding, large temporary activations—such as the \\(S \\times S\\) score matrix—are not needed; however, the KV Cache itself (size \\(\\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}})\\)) must be maintained and grows along with the sequence length.\nCombined Space Complexity Training / Full Forward\nThe main activations (attention scores + weighted outputs + explicit storage of Q,K,V) add up to:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) \\]For large \\(S\\), the \\(\\mathcal{O}(B \\times H \\times S^2)\\) term tends to dominate.\nInference / Incremental Decoding (KV Cache)\nThere is no need for the full \\(S^2\\) attention matrix, but a KV Cache of size\n\\[ \\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}) \\] must be stored. This grows linearly with the decoding steps \\(S_{\\text{past}}\\).\nMeanwhile, the per-step attention matrix is only \\(B \\times H \\times 1 \\times S_{\\text{past}}\\), significantly smaller than the \\(\\mathcal{O}(S^2)\\) scenario in training.\nConclusions and Comparisons Time Complexity\nFor self-attention—whether using MHA, MQA, or GQA—in a full forward pass (which also applies to the forward portion during training), the principal matrix multiplications remain:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) \\] In incremental inference (KV Cache), each new token only requires\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\]but the KV Cache must be updated and maintained throughout the decoding sequence.\nSpace Complexity\nModel Parameters: All three attention mechanisms (MHA, MQA, GQA) reside in \\(\\mathcal{O}(d^2)\\) parameter space.\nIntermediate Activations (Training / Full Forward): Dominated by the attention score matrix and weighted outputs:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) \\] Incremental Decoding (KV Cache): Saves on the \\(\\mathcal{O}(S^2)\\) score matrix cost but requires\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] of storage for the KV Cache, which increases linearly with \\(S_{\\text{past}}\\).\nBenefits of MQA / GQA\nAlthough from a high-level perspective, MHA, MQA, and GQA share similar asymptotic complexities when \\(S\\) is large, MQA and GQA can achieve improved efficiency in practice due to key/value sharing (or partial sharing) which can reduce memory bandwidth demands and improve cache locality. Consequently, in real-world systems, they often deliver better speed and memory performance. The table below summarizes the main differences among MHA, MQA, and GQA attention mechanisms:\nFeature MHA MQA GQA Number of K/V Heads Same as number of heads (\\(H\\)) Single K/V head Number of groups (\\(G\\)), one K/V head per group Inference Time Slower Fastest Faster, but slightly slower than MQA Memory Bandwidth Requirement Highest, \\(H\\) times K/V loading Lowest, only one K/V head Between MHA and MQA, \\(G\\) times K/V loading Model Capacity Highest Lowest Moderate, depending on the number of groups \\(G\\) Performance Best Slightly lower than MHA Close to MHA, significantly better than MQA Uptraining Requirement None High, requires more stability and tuning Lower, GQA models stabilize after minimal uptraining Applicable Scenarios Applications with high performance requirements but insensitive to inference speed Scenarios requiring extremely fast inference with lower model performance demands Applications needing a balance between inference speed and model performance In summary, from a theoretical standpoint, all three attention mechanisms (MHA, MQA, GQA) share \\(\\mathcal{O}(B \\times S^2 \\times d)\\) complexity in a full pass and \\(\\mathcal{O}(B \\times S_{\\text{past}} \\times d)\\) per-step complexity in incremental decoding.\nExperimental Results Performance Testing This experiment was conducted on an environment equipped with dual NVIDIA RTX 4090 GPUs using data parallelism (DP), evenly splitting the batch size across both GPUs. The experiment only tested the performance of the forward pass, including average latency time (Time_mean, unit: ms) and peak memory usage (Peak_Mem_mean, unit: MB), to evaluate the resource requirements and efficiency of different attention mechanisms (MHA, MQA, and GQA) during the inference phase. You can get the source code in benchmark_attention.py.\nThe tests were based on Llama3 8B hyperparameters. Fig. 5. Overview of the key hyperparameters of Llama 3. (Image source: Grattafiori et al., 2024)\nThe main configuration parameters are as follows:\nTotal Layers: 32 layers. Hidden Layer Dimension: 4096. Total Number of Multi-Head Attention Heads: 32. Different Group Configurations (nums_kv_head): 32 (MHA), 1 (MQA), 8 (GQA-8). Experimental Results This section primarily introduces the experimental performance of MHA, MQA, and GQA-8 under different sequence lengths (512, 1024, and 1536), including data on latency and memory usage. For ease of comparison, the table below presents the specific test results for the three attention mechanisms.\nModel Size Method nums_kv_head Seq Length Time_mean (ms) Peak_Mem_mean (MB) Llama3 8B GQA-8 8 512 40.8777 2322.328 Llama3 8B MHA 32 512 53.0167 2706.375 Llama3 8B MQA 1 512 37.3592 2210.314 Llama3 8B GQA-8 8 1024 85.5850 6738.328 Llama3 8B MQA 1 1024 80.8002 6570.314 Llama3 8B MHA 32 1024 102.0514 7314.375 Llama3 8B GQA-8 8 1536 147.5949 13586.328 Llama3 8B MHA 32 1536 168.8142 14354.375 Llama3 8B MQA 1 1536 141.5059 13362.314 Fig. 6. Average Time Benchmark.\nFig. 7. Average Peak Memory Benchmark.\nIn scenarios sensitive to memory and time overheads, MQA and GQA-8 are more efficient choices, with MQA performing the best but potentially lacking in model performance capabilities; GQA-8 achieves a good balance between efficiency and performance.\nGQA Paper Experimental Results Inference Performance Fig. 8. Inference time and performance comparison. (Image source: Ainslie et al., 2023)\nFig. 9. Additional Experimental Results. (Image source: Ainslie et al., 2023)\nThe experimental results show that:\nInference Speed:\nMHA-XXL\u0026rsquo;s inference time is significantly higher than MHA-Large, primarily due to its larger number of heads and model size. Compared to MHA-XXL, MQA-XXL and GQA-8-XXL reduce inference time to approximately 1/6 and 1/5, respectively. Performance:\nMHA-XXL performs best across all tasks but has longer inference times. MQA-XXL has an advantage in inference speed, with average scores only slightly lower than MHA-XXL. GQA-8-XXL has inference speed close to MQA-XXL but nearly matches MHA-XXL in performance, demonstrating the efficiency and superiority of GQA. Checkpoint Conversion Fig. 10. Ablation Study on Checkpoint Conversion Methods. (Image source: Ainslie et al., 2023)\nFigure 10 compares the performance of different methods for checkpoint conversion. The mean pooling method performs best in retaining model information, followed by selecting the first head, while random initialization performs the worst. Mean pooling effectively integrates information from multiple \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads, maintaining model performance.\nUptraining Ratio Fig. 11. Ablation Study on Uptraining Ratios. (Image source: Ainslie et al., 2023)\nFigure 11 shows how performance varies with uptraining proportion for T5 XXL with MQA and GQA.\nGQA: Even with only conversion (no uptraining), GQA already has certain performance. As the uptraining ratio increases, performance continues to improve. MQA: Requires at least a 5% uptraining ratio to achieve practical performance, and as the ratio increases, performance gains tend to plateau. Effect of Number of GQA Groups on Inference Speed Fig. 12. Effect of the Number of GQA Groups on Inference Speed. (Image source: Ainslie et al., 2023)\nFigure 12 demonstrates that as the number of groups increases, inference time slightly rises, but it still maintains a significant speed advantage over MHA. Choosing an appropriate number of groups, such as 8, can achieve a good balance between speed and performance. Figure 3 also shows that models ranging from 7B to 405B parameters in Llama3 adopt 8 as the number of groups (key/value heads = 8).\nOther Optimization Methods In addition to optimizing the attention mechanism, researchers have proposed various methods to enhance the inference and training efficiency of Transformer models:\nLoRA (Hu et al., 2021): Achieves efficient parameter fine-tuning by adding low-rank matrices to the pretrained model\u0026rsquo;s weight matrices. Flash Attention (Dao et al., 2022): Reduces memory and computational overhead by optimizing attention calculations. Quantization Techniques: LLM.int8 (Dettmers et al., 2022) and GPTQ (Frantar et al., 2022) reduce memory usage and computational costs by lowering the precision of model weights and activations. Model Distillation (Hinton et al., 2015): Reduces model size by training smaller models to mimic the behavior of larger models. Speculative Sampling (Chen et al., 2023): Enhances generation efficiency through parallel generation and filtering. Key Takeaways Uptraining methods can effectively utilize existing MHA model checkpoints. By performing a small amount of additional training, they can transform these into more efficient MQA or GQA models, significantly reducing training costs. Grouped-Query Attention (GQA) strikes a good balance between inference efficiency and model performance, making it especially suitable for applications requiring both high-efficiency inference and high performance. Experimental results demonstrate that GQA can significantly improve inference speed while maintaining performance comparable to MHA models, making it suitable for large-scale model deployment and real-time applications. References [1] Vaswani A. Attention is all you need [J]. Advances in Neural Information Processing Systems, 2017.\n[2] Devlin J. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] Radford A. Improving Language Understanding by Generative Pre-Training [J]. 2018.\n[4] Touvron H, Lavril T, Izacard G, et al. LLaMA: Open and Efficient Foundation Language Models [J]. arXiv preprint arXiv:2302.13971, 2023.\n[5] Achiam J, Adler S, Agarwal S, et al. GPT-4 Technical Report [J]. arXiv preprint arXiv:2303.08774, 2023.\n[6] Shazeer N. Fast Transformer Decoding: One Write-Head is All You Need [J]. arXiv preprint arXiv:1911.02150, 2019.\n[7] Ainslie J, Lee-Thorp J, de Jong M, et al. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints [J]. arXiv preprint arXiv:2305.13245, 2023.\n[8] Hu E J, Shen Y, Wallis P, et al. LoRA: Low-Rank Adaptation of Large Language Models [J]. arXiv preprint arXiv:2106.09685, 2021.\n[9] Dettmers T, Lewis M, Belkada Y, et al. GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale [J]. Advances in Neural Information Processing Systems, 2022, 35: 30318-30332.\n[10] Frantar E, Ashkboos S, Hoefler T, et al. GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers [J]. arXiv preprint arXiv:2210.17323, 2022.\n[11] Hinton G. Distilling the Knowledge in a Neural Network [J]. arXiv preprint arXiv:1503.02531, 2015.\n[12] Chen C, Borgeaud S, Irving G, et al. Accelerating Large Language Model Decoding with Speculative Sampling [J]. arXiv preprint arXiv:2302.01318, 2023.\nCitation Citation: To reproduce or cite the content of this article, please acknowledge the original author and source.\nCited as:\nYue Shui. (Jan 2025). Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA.\nhttps://syhya.github.io/posts/2025-01-16-group-query-attention/\nOr\n@article{syhya2025gqa, title = \u0026#34;Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-16-group-query-attention/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-01-16-group-query-attention/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eThe Transformer (\u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eVaswani et al., 2017\u003c/a\u003e) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (\u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eDevlin et al., 2018\u003c/a\u003e) which uses only the encoder, GPT (\u003ca href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\"\u003eRadford et al., 2018\u003c/a\u003e) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (\u003ca href=\"https://arxiv.org/abs/2302.13971\"\u003eTouvron et al., 2023\u003c/a\u003e) and GPT-4 (\u003ca href=\"https://arxiv.org/abs/2303.08774\"\u003eOpenAI et al., 2024\u003c/a\u003e), most of which adopt a decoder-only architecture.\u003c/p\u003e","title":"Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA"},{"content":"Background With the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.\nBased on my work experience, this article summarizes how to build LLMs equipped with specific domain knowledge by leveraging data preparation, model training, deployment, evaluation, and continuous iteration on top of existing general models.\nWhy Inject Domain Knowledge into the Foundational LLMs? Challenge 1: Limited Domain Knowledge Existing pre-trained models (such as GPT-4 and Llama 3) are primarily trained on general-purpose corpora, lacking in-depth understanding of niche languages or proprietary domains. This deficiency leads to subpar performance when the models handle programming code.\nChallenge 2: Data Security and Compliance When enterprises handle sensitive data, they must adhere to strict data sovereignty and compliance requirements. Uploading proprietary business data to third-party cloud services poses security risks, necessitating data processing and model training within local environments.\nChallenge 3: Limitations of OpenAI Fine-Tuning Mainstream commercial APIs for fine-tuning are typically basic and struggle to achieve deep alignment and optimization. For highly customized domain models, such approaches often fail to meet the required specifications.\nTwo Approaches of Injecting Knowledge In practical projects, the common methods for injecting domain knowledge into base models include Fine-Tuning and Retrieval-Augmented Generation (RAG). The following sections provide a detailed comparison of these methods to aid in selecting the most suitable strategy.\nMethod Comparison Fine-Tuning Core Concept\nThrough continued pre-training, supervised fine-tuning, and preference alignment, directly update the model parameters to enable it to master domain-specific knowledge and task patterns.\nTechnical Details\nContinued Pre-Training (CPT): Continue pre-training the base model on a large volume of domain-specific unsupervised data. Supervised Fine-Tuning (SFT): Perform supervised fine-tuning using high-quality labeled data. Preference Alignment (DPO): Optimize model outputs based on user feedback. Parameter Tuning Methods: Utilize full-parameter fine-tuning or combine with PEFT methods like LoRA to freeze some parameters and add adapters. Advantages\nDeep Customization: Updating the internal weights of the model enables a profound understanding of domain knowledge. No External Retrieval Dependency: Inference does not require additional knowledge bases, reducing latency and total token consumption. Enhanced Overall Performance: Significantly outperforms general models in domain-specific tasks. Disadvantages\nHigh Computational Cost: Requires substantial computational resources for training, especially during the CPT phase. Long Training Cycles: From data preparation to model training and optimization, the process is time-consuming. Catastrophic Forgetting: The model may forget its original general capabilities while learning new knowledge. Retrieval-Augmented Generation (RAG) Core Concept\nBuild a domain-specific knowledge base and retrieve relevant documents during inference to assist the model in generating more accurate responses without directly altering model parameters.\nTechnical Details\nData Processing: Preprocess domain documents by chunking them based on size and overlap. Vectorization: Embedding text chunks as vectors using embedding models and storing them in a Vector Store for retrieval. Retrieval: During inference, retrieve relevant documents through similarity search to provide contextual information or few-shot examples to the base model. Advantages\nPreserves General Capabilities: Model parameters remain unchanged, retaining general language abilities. Quick Updates: The knowledge base can be dynamically updated without retraining the model. Computational Efficiency: Avoids large-scale training, saving computational resources. Disadvantages\nDependence on Knowledge Base Quality: The quality of retrieved documents directly impacts response quality. Inference Speed: The retrieval process may increase inference latency and require more tokens. Limited Knowledge Coverage: The model’s internal knowledge is still restricted by the base model’s pre-training data. Models and Training Resources Base Models Taking the Llama 3 series as an example, it features the following characteristics:\nParameter Scale\nThe Llama 3 series includes models ranging from 1B to 405B parameters, widely supporting multilingual processing, code generation, reasoning, as well as visual and textual tasks. Smaller models (1B and 3B) are specially optimized for edge and mobile devices, supporting up to 128K context windows, efficiently handling local tasks such as summary generation, instruction execution, and text rewriting.\nMultimodal Capabilities\nLlama 3\u0026rsquo;s visual models (11B and 90B parameters) outperform many closed models in image understanding tasks and support multimodal processing of images, videos, and audio. All models support fine-tuning, facilitating customized development for specific domains.\nOpen Source and Community Support\nLlama 3 series models and their weights are released in open-source form and can be accessed via llama.com and the Hugging Face platform, providing convenient access and application support for developers.\nDataset Restrictions\nAlthough the Llama 3 models are released as open-source, the datasets used for their training are not open-sourced. Therefore, strictly speaking, Llama 3 is not entirely open-source. This limitation may pose challenges in addressing catastrophic forgetting, as obtaining data sets identical to the original training data is difficult.\nTraining Resources Training large language models requires robust computational resources and efficient distributed training frameworks.\nHardware Resources\nGPU Clusters: NVIDIA A100 or H100 GPUs are recommended, with configurations of 4 or 8 GPUs connected via NVLink or InfiniBand to enhance communication bandwidth. Storage Resources: High-performance SSDs (e.g., NVMe) to support fast data read and write operations. Software Frameworks\nDistributed Training Frameworks: DeepSpeed, Megatron-LM, among others, support large-scale model training. Inference Frameworks: vLLM, ollama, etc., optimize inference speed and resource utilization. Parallel Strategies\nData Parallelism (DP): Suitable when the model fits on a single GPU, implemented via DeepSpeed\u0026rsquo;s ZeRO Stage 0. Model Parallelism (MP), Pipeline Parallelism (PP), and Tensor Parallelism (TP): When the model cannot fit on a single GPU, optimize using ZeRO Stage 1, 2, or 3, or employ ZeRO-Infinity to offload parts of parameters and optimizer states to CPU or NVMe. DeepSpeed ZeRO Sharding Strategies Comparison ZeRO Stage Sharding Strategies ZeRO Stage Description GPU Memory Usage Training Speed ZeRO-0 Pure data parallelism without any sharding. All optimizer states, gradients, and parameters are fully replicated on each GPU. Highest Fastest ZeRO-1 Shards optimizer states (e.g., momentum and second moments), reducing GPU memory usage, but gradients and parameters remain data parallel. High Slightly slower than ZeRO-0 ZeRO-2 Shards optimizer states and gradients, further reducing GPU memory usage based on ZeRO-1. Medium Slower than ZeRO-1 ZeRO-3 Shards optimizer states, gradients, and model parameters, achieving the lowest GPU memory usage, suitable for extremely large models. Requires parameter broadcasting (All-Gather/All-Reduce) during forward/backward passes, significantly increasing communication overhead. Low Significantly slower than ZeRO-2, depends on model size and network bandwidth Offload Strategies Offload Type Description GPU Memory Usage Training Speed ZeRO-1 + CPU Offload Extends ZeRO-1 by offloading optimizer states to CPU memory, further reducing GPU memory usage but necessitating CPU-GPU data transfer, relying on PCIe bandwidth, and occupying CPU memory. Medium-low Slower than ZeRO-1, affected by CPU performance and PCIe bandwidth ZeRO-2 + CPU Offload Extends ZeRO-2 by offloading optimizer states to CPU memory, further reducing GPU memory usage for larger models but increasing CPU-GPU data transfer overhead. Lower Slower than ZeRO-2, affected by CPU performance and PCIe bandwidth ZeRO-3 + CPU Offload Extends ZeRO-3 by offloading optimizer states and model parameters to CPU, achieving minimal GPU memory usage but with extremely high CPU-GPU communication volume and CPU bandwidth significantly lower than GPU-GPU communication. Extremely Low Very Slow ZeRO-Infinity (NVMe Offload) Based on ZeRO-3, offloads optimizer states, gradients, and parameters to NVMe, breaking CPU memory limits and suitable for ultra-large-scale models; performance highly depends on NVMe parallel read/write speeds. Extremely LowRequires NVMe support Slower than ZeRO-3 but generally faster than ZeRO-3 + CPU Offload, can achieve better throughput if NVMe bandwidth is sufficient Communication Volume and Performance Impact ZeRO-0/1/2:\nCommunication is primarily gradient synchronization using All-Reduce operations, resulting in relatively low communication volume.\nZeRO-3:\nRequires All-Gather/All-Reduce operations for model parameters, significantly increasing communication volume. Network bandwidth becomes a critical bottleneck, and parameter broadcasting during forward/backward passes further exacerbates communication load.\nCPU Offload (ZeRO-1/2/3 + CPU):\nOffloads optimizer states or parameters to CPU, reducing GPU memory usage. Communication volume mainly arises from CPU \u0026lt;-\u0026gt; GPU data transfers, which have much lower bandwidth compared to GPU-GPU communication, easily causing performance bottlenecks, especially in ZeRO-3 scenarios. NVMe Offload (ZeRO-Infinity):\nFurther offloads to NVMe based on ZeRO-3, overcoming CPU memory limitations to support ultra-large-scale models. Performance heavily relies on NVMe I/O bandwidth and parallelism. If NVMe speed is sufficiently high, it typically outperforms CPU Offload; however, performance may suffer in scenarios with weak I/O performance or high latency. Hardware and Configuration Impact Hardware Constraints:\nPCIe Bandwidth, Network Bandwidth, NVMe I/O, etc., significantly impact Offload performance. Optimal strategies should be selected based on the hardware environment. Additional Notes:\nCPU Offload utilizes CPU memory and transfers data via PCIe; NVMe Offload saves states on NVMe devices. NVMe Offload generally outperforms CPU Offload when NVMe I/O performance is adequate, but care must be taken to avoid performance bottlenecks caused by insufficient I/O performance. Reference to Official Documentation:\nIt is recommended to consult the DeepSpeed official documentation for the latest and most accurate configuration parameters and performance tuning advice. Data Preparation: The Core of Training Success Data quality directly determines model performance. Data preparation includes data collection, cleaning, deduplication, categorization and balancing, anonymization, and other steps.\nPre-Training Data Data Sources Public Datasets: Such as the-stack-v2, Common Crawl, etc. Enterprise Proprietary Data: Internal documents, code repositories, business logs, etc. Web Crawlers: Collect domain-relevant web content using crawling technologies. Data Scale It is recommended to use at least hundreds of millions to billions of tokens to ensure the model can thoroughly learn domain knowledge. When data volume is insufficient, model performance may be limited. Data augmentation methods are suggested to supplement the data. Data Processing Data Preprocessing\nUniform Formatting: Process large volumes of unlabeled corpora from multiple data sources to ensure consistent formatting. It is recommended to use efficient storage formats like Parquet to improve data reading and processing efficiency. Data Deduplication\nDetection Methods: Use algorithms such as MinHash, SimHash, or cosine similarity for approximate duplicate detection. Granularity of Processing: Choose to deduplicate at the sentence, paragraph, or document level, adjusting flexibly based on task requirements. Similarity Threshold: Set a reasonable similarity threshold (e.g., 0.9) to remove texts with duplication above the threshold, ensuring data diversity. Data Cleaning\nText Filtering: Remove garbled text, spelling errors, and low-quality text by combining rule-based methods and model scorers (e.g., BERT/RoBERTa). Formatting Processing: Prefer using JSON format to handle data, ensuring the accuracy of special formats like code, Markdown, and LaTeX. Data Anonymization\nPrivacy Protection: Anonymize or remove sensitive information such as names, phone numbers, emails, passwords, etc., to ensure data compliance. Filtering Non-Compliant Content: Remove data blocks containing illegal, pornographic, or racially discriminatory content. Data Mixing and Balancing\nProportion Control: For example, combine 70% domain-specific data with 30% general data to prevent the model from forgetting general capabilities. Task Types: Ensure the data includes various task types such as code generation, Q\u0026amp;A dialogue, document summarization, multi-turn conversations, and mathematical reasoning. Data Sequencing\nProgressive Guidance: Use Curriculum Learning to start training with simple, clean data and gradually introduce more complex or noisy data, optimizing the model\u0026rsquo;s learning efficiency and convergence path. Semantic Coherence: Utilize In-Context Pretraining techniques to concatenate semantically similar documents, enhancing contextual consistency and improving the model\u0026rsquo;s depth of semantic understanding and generalization ability. Supervised Fine-Tuning Data Data Format Adopt Alpaca or Vicuna styles, such as single-turn and multi-turn dialogues structured as [instruction, input, output].\nScale: From thousands to hundreds of thousands, depending on project requirements and computational resources. Quality: Ensure high-quality and diverse data to prevent the model from learning errors or biases. Data Construction During the data construction process, we first collect daily business data and collaboratively build foundational questions with business experts. Subsequently, we use large language models for data augmentation to enhance data diversity and robustness. The specific data augmentation strategies are as follows:\nData Augmentation Strategies Diverse Expressions\nRewrite existing data using large language models through synonym replacement and syntactic transformations to increase data diversity.\nRobustness Enhancement\nCreate prompts containing spelling errors, mixed languages, and other input variations to simulate real-world scenarios while ensuring high-quality generated answers.\nKnowledge Distillation\nUtilize large language models like GPT-4 and Claude for knowledge distillation to generate Q\u0026amp;A pairs that meet requirements.\nComplex Task Design\nManually design high-quality data for complex scenarios (e.g., multi-turn dialogues, logical reasoning) to cover the model\u0026rsquo;s capability boundaries.\nData Generation Pipeline\nBuild an automated data generation pipeline that integrates data generation, filtering, formatting, and validation to improve overall efficiency.\nKey Points Task Type Annotation: Clearly annotate each data entry with its task type to facilitate subsequent fine-grained analysis and tuning. Multi-Turn Dialogues and Topic Switching: Construct data that captures contextual coherence and topic transitions in multi-turn dialogues to ensure the model learns the ability to handle topic switching and maintain contextual relevance. Chain of Thought (CoT) Strategy: For classification and reasoning tasks, generate procedural answers using CoT to improve accuracy. Data Flywheel: Continuously collect real user queries after deployment, iterating data based on real needs; regularly clean the data to ensure quality and diversity. Preference Data Data Format Triple Structure: [prompt, chosen answer, rejected answer] Annotation Details: Multi-Model Sampling: Generate answers using multiple models at different training stages or with different data ratios to increase data diversity. Editing and Optimization: Annotators can make slight modifications to the chosen answers to ensure answer quality. Sampling Strategies Multi-Model Sampling: Deploy multiple versions of the model to generate diverse answers for the same prompt. Comparative Annotation: Use manual or automated systems to compare generated answers and select superior answer pairs. Key Points Data Diversity and Coverage: Ensure preference data covers various scenarios and tasks to prevent the model from underperforming in specific contexts. High-Quality Annotation: The quality of preference data directly affects the model\u0026rsquo;s alignment, requiring accurate and consistent annotations. Training Process A complete training process for a domain-specific large language model typically includes Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO) as the three main steps, ultimately achieving model deployment and continuous optimization.\nComparison of Three Methods Training Method Overview Training Method Main Objective Data Requirements Typical Application Scenarios Continued Pre-Training (CPT) Continue pre-training on large-scale unsupervised corpora to inject new domain knowledge Large amounts of unlabeled text (at least hundreds of millions to billions of tokens) Supplementing domain knowledge, such as specialized texts in law, medicine, finance, etc. Supervised Fine-Tuning (SFT) Fine-tune on supervised labeled data to strengthen specific tasks and instruction execution capabilities Customized labeled data (instruction/dialog pairs), ranging from thousands to hundreds of thousands Various specific tasks, such as code generation, Q\u0026amp;A, text rewriting, complex instruction execution, etc. Direct Preference Optimization (DPO) Optimize model outputs to align with human preferences using preference data (chosen vs. rejected) Preference data: [prompt, chosen, rejected](relatively smaller scale) Aligning with human feedback, such as response style, compliance, safety, etc. Advantages and Challenges Continued Pre-Training (CPT) Advantages:\nBetter domain coverage, comprehensively enhancing the model\u0026rsquo;s understanding and generation capabilities in specific domains. No need for additional manual annotation. Challenges/Limitations:\nRequires a large volume of high-quality domain data. High training costs, necessitating massive computational power and time. May introduce domain biases, necessitating careful handling of data quality and distribution. Supervised Fine-Tuning (SFT) Advantages:\nQuickly acquires task execution capabilities. Significantly improves accuracy in specific scenarios. Challenges/Limitations:\nHigh data annotation costs. Requires careful selection of labeled data to avoid overfitting. Fine-tuning may weaken the model\u0026rsquo;s generality. Direct Preference Optimization (DPO) Advantages:\nNo need to train a separate Reward Model. Requires less data and computational resources to achieve similar or better results compared to PPO. Challenges/Limitations:\nRequires reliable preference annotations. Continues to need more preference data for complex and diverse scenarios. Easily constrained by the distribution of preference data. General Training Tips and Technical Details When performing CPT, SFT, and DPO, there are numerous general training tips and technical details. The following sections uniformly describe these general aspects for better understanding and application.\nData Processing and Preparation Data Quality: Regardless of CPT, SFT, or DPO, data quality is crucial. Ensure data accuracy, unambiguity, and diversity. Data Formatting: Consistent data formats simplify the training process. For example, using JSON or other structured formats to store training data. Data Augmentation: Increase data diversity and improve the model\u0026rsquo;s generalization ability through methods like LLM rewriting and optimization. Learning Rate and Optimization Learning Rate Settings: Typically use a smaller learning rate than during pre-training, such as reducing from 3e-4 to 3e-5, depending on the task and data volume. Learning Rate Scheduling: Use warm-up strategies (e.g., linearly increasing for the first 10% of steps), followed by linear decay or cosine annealing to ensure a smooth training process. Optimizer Selection: Choose suitable optimizers based on model size and hardware resources, such as AdamW. Training Strategies Full-Parameter Fine-Tuning: When resources permit, prioritize full-parameter fine-tuning to ensure the model fully captures new knowledge. Parameter-Efficient Fine-Tuning (PEFT): Methods like LoRA are suitable for scenarios with limited computational resources by freezing some parameters and adding adapters for efficient fine-tuning. Mixed Precision Training: Use bf16 or fp16 on supported GPUs to reduce memory usage and increase training speed. Training Stability: Employ techniques such as gradient clipping, regularization, dropout, and weight decay to prevent gradient explosion and model overfitting. Flash Attention: Utilize Flash Attention to optimize the computation efficiency of the attention mechanism, enhancing training speed and reducing memory usage. Monitoring and Tuning Convergence Monitoring: Continuously monitor loss curves on training and validation sets to ensure the model is converging properly. Adjust learning rates and other hyperparameters as needed. Checkpoint: Regularly save checkpoints to prevent loss of all training progress due to unexpected interruptions. Early Stopping: Prevent model overfitting by stopping training at an appropriate time and saving the best model state. Model Evaluation: Conduct periodic evaluations during training to ensure model performance meets expectations. Continued Pre-Training (CPT) Objective Inject new domain knowledge into the base model by continuing pre-training on a large volume of domain-specific unsupervised data, enhancing the model\u0026rsquo;s understanding and generation capabilities in the specific domain.\nTraining Tips Streaming Data Loading\nImplement streaming data loading to dynamically read data during training, preventing memory overflows and training interruptions. Full-Parameter Fine-Tuning\nTypically, update all model parameters during training to ensure comprehensive knowledge acquisition. Compared to parameter-efficient fine-tuning methods (e.g., LoRA), full-parameter fine-tuning offers better domain knowledge injection, especially when computational resources are abundant. It is recommended to prioritize full-parameter fine-tuning under such conditions. Supervised Fine-Tuning (SFT) Objective Enhance the model\u0026rsquo;s practicality and accuracy by training it on high-quality labeled data to perform specific tasks such as code generation, code repair, and complex instruction execution.\nTraining Tips Number of Epochs\nTypically, 1 to 4 epochs are sufficient to observe significant effects when data volume is adequate. If data volume is insufficient, consider increasing the number of epochs while being mindful of overfitting risks. Data augmentation is recommended in such cases. Data Augmentation and Diversity\nEnsure training data covers a variety of task types and instruction expressions to improve the model\u0026rsquo;s generalization ability. Include multi-turn dialogues and robustness data to enhance the model\u0026rsquo;s capability to handle real user scenarios. Direct Preference Optimization (DPO) Objective Optimize model outputs to better align with human expectations and needs, including response style, safety, and readability, by leveraging user feedback and preference data.\nCharacteristics of DPO Direct Optimization\nDoes not require training a separate Reward Model. Instead, directly performs contrastive learning on (chosen, rejected) data pairs.\nEfficiency\nCompared to PPO, DPO requires less data and computational resources to achieve similar or better results.\nDynamic Adaptation\nThe model can immediately adapt whenever new data is available without the need to retrain a Reward Model.\nTraining Tips Collecting Preference Data\nDeploy multiple models at different training stages or with different data ratios to generate diverse responses. Annotate chosen and rejected answer pairs through manual or automated means to ensure data diversity and quality. Contrastive Learning\nOptimize model parameters by maximizing the probability of chosen answers and minimizing the probability of rejected answers. Iterative Optimization\nContinuously collect user feedback, generate new preference data, and perform iterative training to gradually enhance model performance. Implement a data flywheel mechanism to achieve ongoing model evolution and optimization. Common Issues and Solutions Repetitive Outputs\nIssue: The model generates repetitive content, continuously printing without stopping.\nSolutions:\nData Deduplication and Cleaning: Ensure training data does not contain a large amount of repetitive content. Check EOT (End-of-Token) Settings: Prevent the model from continuously generating without stopping. Align via SFT/DPO: Optimize model output quality. Adjust Decoding Strategies: Increase parameters like top_k, repetition penalty, and temperature. Catastrophic Forgetting\nIssue: The model forgets its original general capabilities during fine-tuning, effectively overfitting to the new dataset and causing excessive changes to the original model parameter space.\nSolutions:\nMix in Some General Data: Maintain the model’s general capabilities. Lower the Learning Rate: Reduce the impact on existing knowledge. Increase Dropout Rate and Weight Decay: Prevent overfitting. Use Parameter-Efficient Fine-Tuning Methods like LoRA: Avoid large-scale parameter updates. Utilize RAG Assistance: Combine with external knowledge bases to enhance model performance. Chat Vector: Quickly inject conversational and general capabilities into the model through simple arithmetic operations on model weights. Insufficient Understanding of Entity Relationships and Reasoning Paths\nIssue: The model struggles to correctly understand complex entity relationships and reasoning paths.\nSolutions:\nIntroduce Chain-of-Thought (CoT) Data and Enhanced Reasoning Training: Improve the model\u0026rsquo;s capabilities through step-by-step reasoning training, combined with Reinforcement Fine-Tuning and o1/o3 training methods. Expand Training Data Coverage: Incorporate more diverse scenarios containing complex entity relationships and reasoning paths. Combine with Knowledge Graph Modeling: Use GraphRAG to strengthen the model\u0026rsquo;s understanding and reasoning abilities regarding entity relationships. Model Deployment and Evaluation Deployment Inference Frameworks\nollama: Local inference deployment based on llama.cpp, enabling quick startups. vLLM: Optimized for high concurrency and inference throughput in multi-user scenarios. Quantization: Quantize the model to 8-bit or 4-bit to further reduce inference costs and improve deployment efficiency. Integrate RAG \u0026amp; Agents\nRAG: Combine with a vector knowledge base to retrieve relevant documents or code snippets in real-time, assisting the model in generating more accurate responses. Agents: Utilize Function Calls or multi-turn dialogue mechanisms to enable the model to invoke external tools or perform complex reasoning, enhancing interactivity and practicality. Langgraph: Encapsulate RAG and multi-agent workflows to build customized dialogue systems or automated code generation platforms. Evaluation Evaluation Metrics\nCPT Phase: Use domain-specific test sets to evaluate perplexity (PPL) or cross-entropy, measuring the model\u0026rsquo;s mastery of new knowledge. SFT/DPO Phase: Human or Model Evaluation: Assess the accuracy, coherence, readability, and safety of responses through human ratings or automated tools. Code Generation: Build a large-scale unit test set to evaluate the pass@k metric, measuring the correctness rate of code generation. General Capabilities: Test the model on common benchmarks (e.g., MMLU, CMMLU) to ensure minimal performance degradation on general tasks. Decoding Hyperparameters\nConsistency: Maintain consistent decoding parameters such as top_k, top_p, temperature, and max_new_tokens during evaluation to ensure comparability of results. Grid Search: When computational resources permit, evaluate different combinations of decoding parameters to select the optimal configuration. Data Flywheel and Continuous Iteration Data Flywheel Mechanism\nReal-Time Collection of User Logs\nCollect real user prompts and generated responses online, covering diverse usage scenarios and task types. Automated or Manual Annotation\nAnnotate collected user prompts and responses with preferences, generating new (chosen, rejected) data pairs. Iterative Training\nIncorporate newly generated preference data into the next round of SFT/DPO training to continuously optimize response quality and user experience. Robustness Data\nInclude data with spelling errors, mixed languages, vague instructions, etc., to enhance the model’s robustness and ability to handle real-world scenarios. Continuous Optimization\nFeedback Loop: Utilize user feedback to continuously improve training data and model performance, achieving self-optimization and evolution of the model. Multi-Model Collaboration: Deploy multiple versions of the model to generate diverse responses, enhancing the model\u0026rsquo;s comprehensive capabilities through contrastive learning. Integrating Intent Recognition and Multi-Agent Reasoning Use an intent classification model to allow the large model to determine the category of user input intent. Based on the mapping between intent categories and context types, supervise the reasoning path, and then perform multi-way retrieval based on the reasoning path. Provide this information to the trained model to generate the final result.\nConclusion Through the combination of Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO), it is possible to effectively inject domain-specific knowledge into base large models, constructing closed-source LLMs capable of efficiently solving business problems. The key steps are as follows:\nData Preparation\nHigh-quality data collection, cleaning, deduplication, and categorization to ensure data diversity and accuracy. Implement data anonymization strategies to protect privacy and ensure compliance. Model Training\nUse CPT to inject domain knowledge, SFT to learn specific task patterns, and DPO to optimize model outputs to align with human preferences and safety. Leverage efficient parallel training frameworks and hyperparameter tuning techniques to enhance training efficiency and resource utilization. Deployment and Evaluation\nEmploy efficient inference frameworks, integrating RAG and Agents for knowledge enhancement and functional extension. Conduct multi-dimensional evaluations to ensure the model performs as expected at each stage. Continuous Iteration\nBuild a data flywheel to continuously collect user feedback and optimize training data and model performance. Integrate RAG and Agents to achieve ongoing improvement and expansion of model capabilities. Ultimately, through a systematic process and technical measures, it is possible to construct an AI system with not only profound domain knowledge but also the flexibility to handle complex business requirements over its lifecycle.\nReferences DeepSpeed Megatron-LM ollama vLLM GraphRAG The Llama 3 Herd of Models ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages Evaluating Large Language Models Trained on Code Direct Preference Optimization: Your Language Model is Secretly a Reward Model Citation Citation: To reproduce or cite the content of this article, please acknowledge the original author and source.\nCited as:\nYue Shui. (Jan 2025). Building Domain-Specific LLMs.\nhttps://syhya.github.io/posts/2025-01-05-build-domain-llm\nOr\n@article{syhya2024domainllm, title = \u0026#34;Building Domain-Specific LLMs\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-05-build-domain-llm/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-01-05-domain-llm-training/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eWith the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.\u003c/p\u003e","title":"Building Domain-Specific LLMs"},{"content":"Rent a GPU or Buy Your Own? Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\nAdvantages of Renting GPUs:\nNo high upfront hardware costs Elastic scalability according to project needs Cloud vendors typically provide data compliance and security assurances, reducing hardware maintenance concerns Advantages of Buying GPUs:\nLower total cost if used extensively over the long term Higher privacy and control for in-house data and models Hardware can be upgraded or adjusted at any time, offering more flexible deployment Personal Suggestions\nIf you have a limited budget or are just starting out, use Colab, Kaggle, or cloud-based GPU solutions first. Once computing needs and privacy requirements increase, consider building your own multi-GPU server or renting a multi-node, multi-GPU cluster. Background In September 2023, to continue my exploration and research on Large Language Models (LLMs) outside of work, I assembled a dual RTX 4090 personal AI server. It has been running for nearly a year, and here are some observations:\nNoise: Placed under my desk, the fans can get quite loud under full load. However, noise levels are acceptable during inference or moderate loads. Inference Performance: With a total of 48GB of VRAM, and by using 4-bit quantization, it can handle 70B-level models (e.g., Llama 70B, Qwen 72B). Training Performance: By using DeepSpeed with distributed and offload techniques (ZeRO-3 + CPU offload), I can finetune around 34B parameter models (e.g., CodeLlama 34B). Cost-Effectiveness: For personal or small-team experiments and small-to-medium scale model training, this configuration is quite practical. However, full-scale training of very large models (hundreds of billions of parameters) still requires more professional GPUs (e.g., multi-GPU A100 or H100 clusters). Below is an illustration of VRAM requirements for various model sizes and training approaches :\nFig. 1. Hardware Requirement. (Image source: LLaMA-Factory)\nAssembly Strategy \u0026amp; Configuration Details The total budget is around 40,000 RMB (~6,000 USD). The final build list is as follows (for reference only):\nComponent Model Price (RMB) GPU RTX 4090 * 2 25098 Motherboard + CPU AMD R9 7900X + MSI MPG X670E CARBON 5157.55 Memory Corsair 48GB * 2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + Samsung 990PRO 4TB 4587 PSU Corsair AX1600i 2699 Fans Phanteks T30 120mm P * 6 1066.76 CPU Cooler Thermalright FC140 BLACK 419 Chassis Phanteks 620PC Full Tower 897.99 GPU Riser Cable Phanteks FL60 PCI-E4.0 *16 399 Total: ~ 42,723.3 RMB\nGPU Selection For large-scale model research, floating-point performance (TFLOPS) and VRAM capacity are the most critical metrics. Professional GPUs (A100, H100, etc.) boast higher VRAM and NVLink support but can easily cost hundreds of thousands of RMB, which is prohibitive for most individual users. According to Tim Dettmers, the RTX 4090 offers excellent performance-per-dollar and supports features like BF16 and Flash Attention, making it a high-value option.\nCooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling Cooling Method Advantages Disadvantages Best For Blower Fan Compact form factor; good for multi-GPU setups Loud noise, generally weaker cooling Server racks, dense multi-GPU deployments Air-Cooling Good balance of performance and noise; easy upkeep Cards are often large, require space Home or personal research (with enough space) Liquid-Cooling Excellent cooling, quieter under full load Risk of leaks, higher cost Extreme quiet needs or heavy overclocking Home Setup Recommendation: Air-cooled GPUs are typically the best balance of cooling efficiency, noise, and maintenance. They are generally more user-friendly compared to blower or liquid-cooled alternatives.\nCPU \u0026amp; Motherboard In deep learning scenarios, the CPU handles data preprocessing, pipeline scheduling, and multi-process/thread management, ensuring high-throughput, low-latency data delivery to the GPUs. Thus, the CPU’s main requirements include sufficient PCIe lanes and robust multi-threaded performance.\nIntel: 13th/14th Generation i9 (e.g., 13900K) offers 20 primary PCIe lanes, enough for dual GPUs at x8 + x8. AMD: The Ryzen 7000/9000 series (e.g., 7950X) provides 28 (24 usable) PCIe lanes and also supports dual GPUs at x8 + x8, with enough bandwidth for multiple M.2 SSDs. MSI MPG X670E CARBON Motherboard Expandability: Supports PCIe 5.0 and DDR5 memory, offering ample future-proofing. Stability: High-quality power delivery, ensuring stable operation for both CPU and multiple GPUs. Rich I/O: Supports multiple M.2 SSDs and USB4 for various usage scenarios. AMD Ryzen 9 7900X Highlights Cores \u0026amp; Threads: 12 cores, 24 threads, excellent for data preprocessing and multitasking in deep learning workloads. PCIe Bandwidth: Provides 28 (24 usable) PCIe 5.0 lanes to support dual GPUs at x8 + x8 while also powering high-speed M.2 SSDs. Power Efficiency: Built on the Zen 4 architecture, delivering outstanding performance-to-power ratio for high-performance computing tasks. Key Motherboard Considerations Physical Layout RTX 4090 cards are huge, often occupying multiple slots. Confirm the board can hold two such cards simultaneously. If space or thermal conflicts arise, use a riser cable for vertical placement of the second GPU. PCIe Lane Splitting Ensure the motherboard can run two PCIe 4.0 x8 + x8 slots. Avoid a setup ending up as x16 + x2, which severely limits the second GPU’s bandwidth and can lead to a significant performance bottleneck, especially critical in large model training. Expandability With two GPUs installed, you still need enough M.2 slots and external ports for additional storage or peripherals. After evaluating expandability, performance, and cost-effectiveness, I chose the AMD Ryzen 9 7900X paired with the MSI MPG X670E CARBON. A GPU riser cable resolved the slot conflicts caused by the thickness of dual RTX 4090s.\nBIOS Setup Recommendations Memory Optimization Enable XMP/EXPO (Intel/AMD) to boost memory clock speeds and bandwidth. Overclocking If additional performance is needed, enable PBO (Precision Boost Overdrive) or Intel Performance Tuning and monitor system stability. Thermals \u0026amp; Stability Avoid extreme overclocking and keep temperatures under control to maintain system stability. Memory During deep learning training, large amounts of system memory are used for data loading and optimizer states (especially in multi-GPU scenarios with Zero-stage parallelism). Aim for at least 2× the total GPU VRAM capacity. This build uses 48GB * 2 (96GB in total), sufficient for everyday multitasking and distributed training without frequent swapping.\nStorage Prefer M.2 NVMe SSDs: They offer superior read/write performance, critical for loading large model weights, caching intermediate files, and storing training logs. Capacity ≥ 2TB: Model files continue to grow in size. 2TB can fill up quickly; consider 4TB or more depending on your needs. SSD Brands: Samsung, SK Hynix, and Western Digital have reliable high-end product lines. Power Supply Dual RTX 4090s can draw 900W–1000W under full load, and you also need overhead for the CPU, motherboard, and storage. Generally, 1,500W+ Platinum or Titanium PSUs are recommended to ensure stable power delivery and mitigate voltage fluctuations.\nI opted for a Corsair AX1600i (digital PSU), which provides real-time power monitoring and sufficient headroom.\nCooling \u0026amp; Fans I chose an air-cooling setup:\nCPU Cooler: Thermalright FC140, a dual-tower air cooler offering solid thermal performance with relatively low noise. Case Fans: Phanteks T30 120mm * 6, maintaining slightly positive internal pressure to ensure proper airflow across the GPUs and power delivery modules. For prolonged high-load training (such as distributed training of large models), chassis airflow management and adequate fans are crucial. Monitor CPU, GPU, and VRM temperatures regularly and adjust fan curves as needed.\nAdvanced Cooling\nFor even quieter operation, consider a Hybrid or partial liquid-cooling solution, along with finely tuned fan curves. Regularly cleaning dust filters, using dust guards, and refreshing thermal compound can also help improve thermals and stability. Chassis Because the RTX 4090 is massive, and two of them stacked requires ample internal space and airflow, a full-tower chassis is recommended for better cable routing and thermal design. I went with the Phanteks 620PC, which offers ample space and built-in cable management.\nBelow is a picture of the built computer:\nFig. 2. Computer\nSystem \u0026amp; Software Environment Operating System: Linux (e.g., Ubuntu 22.04 LTS) is highly recommended due to better compatibility and support for CUDA, NVIDIA drivers, and popular deep learning frameworks. The general setup flow:\nOS Installation: Ubuntu or another Linux distribution. NVIDIA Driver Installation: Make sure nvidia-smi detects both 4090 GPUs correctly:\nFig. 3. nvidia-smi Output\nCUDA Toolkit: Verify via nvcc -V:\nFig. 4. nvcc -V Output\ncuDNN: Ensure your deep learning frameworks can leverage GPU-accelerated kernels for convolution, RNN, etc. Framework Testing: Use PyTorch, TensorFlow, or JAX to confirm basic inference and training functionality. Docker Containerization: With nvidia-container-toolkit, containers can directly access GPU resources, eliminating host-environment conflicts. For multi-node, multi-GPU setups, consider Kubernetes, Ray, or Slurm for cluster scheduling and resource management. Recommended Tools \u0026amp; Frameworks Training Frameworks\nLLaMA-Factory: Offers user-friendly packaging for large language model training and inference. Great for beginners. DeepSpeed: Provides distributed training for large models, with multiple parallelization strategies and optimizations. Megatron-LM: NVIDIA’s official large-scale language model training framework, ideal for multi-node, multi-GPU scenarios. Monitoring \u0026amp; Visualization\nWeights \u0026amp; Biases or TensorBoard: Real-time monitoring of loss, learning rate, etc., with remote-friendly UI. Inference Tools\nollama: Based on llama.cpp, easy local inference setup. vLLM: Optimized for high-concurrency, multi-user scenarios, delivering superior throughput. Framework ollama vLLM Function Simple local LLM deployment High-concurrency / high-throughput LLM Concurrent Performance drops as concurrency increases Handles higher concurrency with better TPS 16 Threads ~17s/req ~9s/req Throughput Slower token generation speeds ~2× faster token generation Max Concur. Performance deteriorates over 32 threads Remains stable under large concurrency Use Cases Personal or low-traffic apps Enterprise or multi-user high concurrency WebUI\nOpen-WebUI: A user-friendly, web-based solution that integrates multiple AI backends (ollama, OpenAI API, etc.), handy for rapid prototyping and visualization. Advanced Suggestions Development \u0026amp; Debugging Efficiency\nUse SSH for remote development, and create custom Docker images to reduce setup overhead. Quantization \u0026amp; Pruning\nTechniques like 4-bit or 8-bit quantization and pruning can reduce model size and VRAM usage, thereby improving inference performance. Mixed-Precision Training\nSwitch to BF16 or FP16 training to accelerate training speed, combined with gradient scaling (GradScaler) to maintain numerical stability. CPU Coordination\nEnhance data loading using multi-threading, multi-processing, or RAM disks for streaming large pretraining datasets. Multi-Node Cluster Deployment\nConnect nodes via InfiniBand or high-speed Ethernet and use Kubernetes for efficient resource scheduling. Summary With the above configuration and methodology, I successfully built a dual RTX 4090 deep learning workstation. It excels at inference and small to medium scale fine-tuning scenarios—ideal for individuals or small teams focusing on LLM research or application development. This setup strikes a balance between cost-effectiveness and flexibility. However, if you plan to train massive models (hundreds of billions of parameters) in a full-parameter regime, you will still need more GPUs (e.g., multi-GPU A100 or H100 clusters).\nFrom personal experience, dual 4090 GPUs provide sufficient performance within a reasonable budget, meeting the majority of small-to-medium-scale R\u0026amp;D needs—a solid option for qualified individuals or teams to consider.\nReferences Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe lane specs AMD R5 7600X PCIe lane specs MSI MPG X670E CARBON Specifications nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI Copyright \u0026amp; Citation Disclaimer: The configurations, prices, and recommendations in this article are for technical discussion and research purposes only. Actual purchases and deployments should be carefully evaluated based on individual budgets and requirements. The author assumes no liability for any direct or indirect consequences resulting from following or adapting the information provided herein.\nCitation: When reposting or referencing this content, please credit the original author and source.\nCited as:\nYue Shui. (Dec 2024). Building a Home Deep Learning Rig with Dual RTX 4090 GPUs. https://syhya.github.io/posts/2024-12-21-build-gpu-server\nOr\n@article{syhya2024build, title = \u0026#34;Building a Home Deep Learning Rig with Dual RTX 4090 GPUs\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Dec\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2024-12-21-build-gpu-server/\u0026#34; ","permalink":"https://syhya.github.io/posts/2024-12-21-build-gpu-server/","summary":"\u003ch2 id=\"rent-a-gpu-or-buy-your-own\"\u003eRent a GPU or Buy Your Own?\u003c/h2\u003e\n\u003cp\u003eBefore setting up a deep learning environment, consider \u003cstrong\u003eusage duration\u003c/strong\u003e, \u003cstrong\u003ebudget\u003c/strong\u003e, \u003cstrong\u003edata privacy\u003c/strong\u003e, and \u003cstrong\u003emaintenance overhead\u003c/strong\u003e. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\u003c/p\u003e","title":"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs"},{"content":"Abstract The stock market is a crucial component of the financial market. In recent years, with its vigorous development, research on stock price prediction and quantitative investment strategies has attracted scholars from various fields. With the advancement of Artificial Intelligence (AI) and Machine Learning (ML) in recent years, researchers have shifted from traditional statistical models to AI algorithms. Particularly after the deep learning boom, neural networks have achieved remarkable results in stock price prediction and quantitative investment strategy research. The objective of deep learning is to learn multi-level features, constructing abstract high-level features by combining low-level ones, thereby mining the distributed feature representations of data. This approach enables complex nonlinear modeling to accomplish prediction tasks. Recurrent Neural Networks (RNNs) have been widely applied to sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, leading many researchers to use RNNs for stock price prediction. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The advent of Long Short-Term Memory (LSTM) networks addressed this problem, followed by variants such as Gated Recurrent Units (GRUs), Peephole LSTMs, and Bidirectional LSTMs (BiLSTMs). Traditional stock prediction models often overlook temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully leverages the contextual relationships in both forward and backward temporal directions of time series data. It also avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies.\nThe first part of this paper\u0026rsquo;s experiments utilizes stock data from China\u0026rsquo;s Shanghai Pudong Development Bank and the US\u0026rsquo;s IBM to establish stock prediction models using LSTM, GRU, and BiLSTM. By comparing the prediction results of these three deep learning models, it is found that the BiLSTM model outperforms the others for both datasets, demonstrating better prediction accuracy. The second part uses A-share market-wide stock data and first employs a LightGBM model to screen 50 factors, selecting the top 10 most important factors. Subsequently, a BiLSTM model is used to select and combine these factors to establish a quantitative investment strategy. Empirical analysis and backtesting of this strategy reveal that it outperforms the market benchmark index, indicating the practical application value of the BiLSTM model in stock price prediction and quantitative investment.\nKeywords: Quantitative Investment; Deep Learning; Neural Network Model; Multi-Factor Stock Selection; BiLSTM\nChapter 1 Introduction 1.1 Research Background and Significance 1.1.1 Research Background Emerging in the 1970s, quantitative investment gradually entered the vision of investors, initiating a new revolution that changed the landscape of portfolio management previously dominated by passive management and the efficient market hypothesis. The efficient market hypothesis posits that under market efficiency, stock prices reflect all market information, and no excess returns exist. Passive investment management, based on the belief that markets are efficient, focuses more on asset classes, with the most common approach being purchasing index funds and tracking published index performance. In contrast, active investment management relies primarily on investors\u0026rsquo; subjective judgments of the market and individual stocks. By applying mathematical models to the financial domain and using available public data to evaluate stocks, active managers construct portfolios to achieve returns. Quantitative investment, through statistical processing of vast historical data to uncover investment opportunities and avoid subjective factors, has gained increasing popularity among investors. Since the rise of quantitative investment, people have gradually utilized various technologies for stock price prediction to better establish quantitative investment strategies. Early domestic and international scholars adopted statistical methods for modeling and predicting stock prices, such as exponential smoothing, multiple regression, and Autoregressive Moving Average (ARMA) models. However, due to the multitude of factors influencing the stock market and the large volume of data, stock prediction is highly challenging, and the prediction effectiveness of various statistical models has been unsatisfactory.\nIn recent years, the continuous development of machine learning, deep learning, and neural network technologies has provided technical support for stock price prediction and the construction of quantitative strategies. Many scholars have achieved new breakthroughs using methods like Random Forest, Neural Networks, Support Vector Machines, and Convolutional Neural Networks. The ample historical data in the stock market, coupled with diverse technological support, provides favorable conditions for stock price prediction and the construction of quantitative strategies.\n1.1.2 Research Significance From the perspective of the long-term development of the national economic system and financial markets, research on stock price prediction models and quantitative investment strategies is indispensable. China started relatively late, with a less mature financial market, fewer financial instruments, and lower market efficiency. However, in recent years, the country has gradually relaxed policies and vigorously developed the financial market, providing a \u0026ldquo;breeding ground\u0026rdquo; for the development of quantitative investment. Developing quantitative investment and emerging financial technologies can offer China\u0026rsquo;s financial market an opportunity for a \u0026ldquo;curve overtaking\u0026rdquo;. Furthermore, the stock price index, as a crucial economic indicator, serves as a barometer for China\u0026rsquo;s economic development.\nFrom the perspective of individual and institutional investors, constructing stock price prediction models and quantitative investment strategy models enhances market efficiency. Individual investors often lack comprehensive professional knowledge, and their investment behaviors can be somewhat blind. Developing relevant models to provide references can reduce the probability of judgment errors and change the relatively disadvantaged position of individual investors in the capital market. For institutional investors, rational and objective models, combined with personal experience, improve the accuracy of decision-making and provide new directions for investment behaviors.\nIn summary, considering China\u0026rsquo;s current development status, this paper\u0026rsquo;s selection of individual stocks for stock price prediction models and A-share market-wide stocks for quantitative strategy research holds significant practical research value.\n1.2 Literature Review White (1988)$^{[1]}$ used a Backpropagation (BP) neural network to predict the daily returns of IBM stock. However, due to the BP neural network model\u0026rsquo;s susceptibility to gradient explosion, the model could not converge to a global minimum, thus failing to achieve accurate predictions.\nKimoto (1990)$^{[2]}$ developed a system for predicting the Tokyo Stock Exchange Prices Index (TOPIX) using modular neural network technology. This system not only successfully predicted TOPIX but also achieved a certain level of profitability through stock trading simulations based on the prediction results.\nG. Peter Zhang (2003)$^{[3]}$ conducted a comparative study on the performance of Autoregressive Integrated Moving Average (ARIMA) models and Artificial Neural Network (ANN) models in time series forecasting. The results showed that ANN models significantly outperformed ARIMA models in terms of time series prediction accuracy.\nRyo Akita (2016)$^{[4]}$ selected the Consumer Price Index (CPI), Price-to-Earnings ratio (P/E ratio), and various events reported in newspapers as features, and constructed a financial time series prediction model using paragraph vectors and LSTM networks. Using actual data from fifty listed companies on the Tokyo Stock Exchange, the effectiveness of this model in predicting stock opening prices was verified.\nKunihiro Miyazaki (2017)$^{[5]}$ constructed a model for predicting the rise and fall of the Topix Core 30 index and its constituent stocks by extracting daily stock chart images and 30-minute stock price data. The study compared multiple models, including Logistic Regression (LR), Random Forest (RF), Multilayer Perceptron (MLP), LSTM, CNN, PCA-CNN, and CNN-LSTM. The results indicated that LSTM had the best prediction performance, CNN performed generally, but hybrid models combining CNN and LSTM could improve prediction accuracy.\nTaewook Kim (2019)$^{[6]}$ proposed an LSTM-CNN hybrid model to combine features from both stock price time series and stock price image data representations to predict the stock price of the S\u0026amp;P 500 index. The study showed that the LSTM-CNN model outperformed single models in stock price prediction, and this prediction had practical significance for constructing quantitative investment strategies.\n1.3 Innovations of the Paper This paper has the following innovations in stock price prediction:\nData from both the domestic A-share market (Shanghai Pudong Development Bank) and the international US stock market (IBM) are used for research, avoiding the limitations of single-market studies. Traditional BP models have never considered temporal factors, or like LSTM models, only consider unidirectional temporal relationships. Therefore, this paper uses the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions of time series data and avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies. Empirical evidence, compared with LSTM and GRU models, demonstrates its ability to improve prediction accuracy. The stock price prediction model is trained using multiple stock features, including opening price, closing price, highest price, and trading volume. Compared to single-feature prediction of stock closing prices, this approach is theoretically more accurate and can better compare the prediction effectiveness of LSTM, GRU, and BiLSTM for stocks. This paper has the following innovations in quantitative strategy research:\nInstead of using common market factors, this paper uses multiple price-volume factors obtained through Genetic Programming (GP) algorithms and artificial data mining. LightGBM model is used to screen 50 factors, selecting the top 10 most important factors. Traditional quantitative investment models generally use LSTM and CNN models to construct quantitative investment strategies. This paper uses A-share market-wide data and employs a BiLSTM model to select and combine factors to establish a quantitative investment strategy. Backtesting and empirical analysis of this strategy show that it outperforms the market benchmark index (CSI All Share), demonstrating the practical application value of the BiLSTM model in stock price prediction and quantitative investment. 1.4 Research Framework of the Paper This paper conducts research on stock price prediction and quantitative strategies based on deep learning algorithms. The specific research framework of this paper is shown in Fig. 1:\nFig. 1. Research Framework.\nThe specific research framework of this paper is as follows:\nChapter 1 is the introduction. This chapter first introduces the research significance and background of stock price prediction and quantitative strategy research. Then, it reviews the current research status, followed by an explanation of the innovations of this paper compared to existing research, and finally, a brief description of the research framework of this paper.\nChapter 2 is about related theoretical foundations. This chapter introduces the basic theories of deep learning models and quantitative stock selection involved in this research. The deep learning model section sequentially introduces four deep learning models: RNN, LSTM, GRU, and BiLSTM, with a focus on the internal structure of the LSTM model. The quantitative stock selection theory section sequentially introduces the Mean-Variance Model, Capital Asset Pricing Model, Arbitrage Pricing Theory, Multi-Factor Model, Fama-French Three-Factor Model, and Five-Factor Model. This section introduces the history of multi-factor quantitative stock selection from the development context of various financial theories and models.\nChapter 3 is a comparative study of LSTM, GRU, and BiLSTM in stock price prediction. This chapter first introduces the datasets of domestic and international stocks used in the experiment, and then performs data preprocessing steps of normalization and data partitioning. It then describes the network structures, model compilation, and hyperparameter settings of the LSTM, GRU, and BiLSTM models used in this chapter, and conducts experiments to obtain experimental results. Finally, the experimental results are analyzed, and a summary of this chapter is provided.\nChapter 4 is a study on a quantitative investment model based on LightGBM-BiLSTM. This chapter first outlines the experimental steps, and then introduces the stock data and factor data used in the experiment. Subsequently, factors are processed sequentially through missing value handling, outlier removal, factor standardization, and factor neutralization to obtain cleaned factors. Then, LightGBM and BiLSTM are used for factor selection and factor combination, respectively. Finally, a quantitative strategy is constructed based on the obtained model, and backtesting is performed on the quantitative strategy.\nChapter 5 is the conclusion and future directions. This chapter summarizes the main research content of this paper on stock price prediction and quantitative investment strategies. Based on the existing shortcomings of the current research, future research directions are proposed.\nChapter 2 Related Theoretical Foundations 2.1 Deep Learning Models 2.1.1 RNN Recurrent Neural Networks (RNNs) are widely used for sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, hence many previous studies have used RNNs to predict stock prices. RNNs employ a very simple chain structure of repeating modules, such as a single tanh layer. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The emergence of LSTM solved this problem. Fig. 2 is an RNN structure diagram.\nFig. 2. RNN Structure Diagram. (Image source: Understanding LSTM Networks)\n2.1.2 LSTM Long Short-Term Memory (LSTM) networks are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter \u0026amp; Schmidhuber (1997)$^{[7]}$ and have been refined and popularized by many in subsequent work. Due to their unique design structure, LSTMs are relatively insensitive to gap lengths and solve the gradient vanishing and explosion problems of traditional RNNs. Compared to traditional RNNs and other time series models like Hidden Markov Models (HMMs), LSTMs can better handle and predict important events in time series with very long intervals and delays. Therefore, LSTMs are widely used in machine translation and speech recognition.\nLSTMs are explicitly designed to avoid long-term dependency problems. All recurrent neural networks have the form of a chain of repeating modules of neural networks, but LSTM improves the structure of RNN. Instead of a single neural network layer, LSTM uses a four-layer structure that interacts in a special way.\nFig. 3. LSTM Structure Diagram 1. (Image source: Understanding LSTM Networks)\nFig. 4. LSTM Structure Diagram 2. (Image source: Understanding LSTM Networks)\nAs shown in Fig. 3, black lines are used to represent the transmission of an output vector from one node to the input vector of another node. A neural network layer is a processing module with a $\\sigma$ activation function or a tanh activation function; pointwise operation represents element-wise multiplication between vectors; vector transfer indicates the direction of information flow; concatenate and copy are represented by two black lines merging together and two black lines separating, respectively, to indicate information merging and information copying.\nBelow, we take LSTM as an example to explain its structure in detail.\nForget Gate Fig. 5. Forget Gate Calculation (Image source: Understanding LSTM Networks)\n$$ f_{t} = \\sigma\\left(W_{f} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{f}\\right) $$Parameter Description:\n$h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $\\sigma$: Sigmoid activation function $W_{f}$: Weight matrix for the forget gate $b_{f}$: Bias vector parameter for the forget gate The first step, as shown in Fig. 5, is to decide what information to discard from the cell state. This process is calculated by the sigmoid function to obtain the value of $f_{t}$ (the range of $f_{t}$ is between 0 and 1, where 0 means completely block, and 1 means completely pass through) to determine whether the cell state $C_{t-1}$ is passed through or partially passed through.\nInput Gate Fig. 6. Input Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} i_{t} \u0026= \\sigma\\left(W_{i} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{i}\\right) \\\\ \\tilde{C}_{t} \u0026= \\tanh\\left(W_{C} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{C}\\right) \\end{aligned} $$Parameter Description:\n$h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $\\sigma$: Sigmoid activation function $W_{i}$: Weight matrix for the input gate $b_{i}$: Bias vector parameter for the input gate $W_{C}$: Weight matrix for the cell state $b_{C}$: Bias vector parameter for the cell state $\\tanh$: tanh activation function The second step, as shown in Fig. 6, uses a sigmoid function to calculate what information we want to store in the cell state. Next, a $\\tanh$ layer creates a candidate vector $\\tilde{C}_{t}$, which will be added to the cell state.\nFig. 7. Current Cell State Calculation (Image source: Understanding LSTM Networks)\n$$ C_{t} = f_{t} * C_{t-1} + i_{t} * \\tilde{C}_{t} $$Parameter Description:\n$C_{t-1}$: Cell state from the previous time step $\\tilde{C}_{t}$: Temporary cell state $i_{t}$: Value of the input gate $f_{t}$: Value of the forget gate The third step, as shown in Fig. 7, calculates the current cell state $C_t$ by combining the effects of the forget gate and the input gate.\nThe forget gate $f_t$ weights the previous cell state $C_{t-1}$ to discard unnecessary information. The input gate $i_t$ weights the candidate cell state $\\tilde{C}_t$ to decide how much new information to introduce. Finally, the two parts are added together to update and derive the current cell state $C_t$. Output Gate Fig. 8. Output Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} o_{t} \u0026= \\sigma\\left(W_{o} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{o}\\right) \\\\ h_{t} \u0026= o_{t} * \\tanh\\left(C_{t}\\right) \\end{aligned} $$Parameter Description:\n$o_{t}$: Value of the output gate $\\sigma$: Sigmoid activation function $W_{o}$: Weight matrix for the output gate $h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $b_{o}$: Bias vector parameter for the output gate $h_{t}$: Output at the current time step $\\tanh$: tanh activation function $C_{t}$: Cell state at the current time step The fourth step, as shown in Fig. 8, uses a sigmoid function to calculate the value of the output gate. Finally, the cell state $C_{t}$ at this time step is processed by a tanh activation function and multiplied by the value of the output gate $o_{t}$ to obtain the output $h_{t}$ at the current time step.\n2.1.3 GRU K. Cho (2014)$^{[8]}$ proposed the Gated Recurrent Unit (GRU). GRU is mainly simplified and adjusted based on LSTM, merging the original forget gate, input gate, and output gate of LSTM into an update gate and a reset gate. In addition, GRU also merges the cell state and hidden state, thereby reducing the complexity of the model while still achieving performance comparable to LSTM in some tasks.\nThis model can save a lot of time when the training dataset is relatively large and shows better performance on some smaller and less frequent datasets$^{[9][10]}$.\nFig. 9. GRU Structure Diagram (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} z_{t} \u0026= \\sigma\\left(W_{z} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ r_{t} \u0026= \\sigma\\left(W_{r} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ \\tilde{h}_{t} \u0026= \\tanh\\left(W \\cdot \\left[r_{t} * h_{t-1}, x_{t}\\right]\\right) \\\\ h_{t} \u0026= \\left(1 - z_{t}\\right) * h_{t-1} + z_{t} * \\tilde{h}_{t} \\end{aligned} $$Parameter Description:\n$z_{t}$: Value of the update gate $r_{t}$: Value of the reset gate $W_{z}$: Weight matrix for the update gate $W_{r}$: Weight matrix for the reset gate $\\tilde{h}_{t}$: Temporary output 2.1.4 BiLSTM Bidirectional Long Short-Term Memory (BiLSTM) networks are formed by combining a forward LSTM and a backward LSTM. The BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions of time series data, enabling it to learn information with long-term dependencies. Compared to unidirectional LSTM, it can better consider the reverse impact of future data. Fig. 10 is a BiLSTM structure diagram.\nFig. 10. BiLSTM Structure Diagram. (Image source: Baeldung)\n2.2 Quantitative Stock Selection Theory 2.2.1 Mean-Variance Model Quantitative stock selection strategies originated in the 1950s. Markowitz (1952)$^{[11]}$ proposed the Mean-Variance Model. This model not only laid the foundation for modern portfolio theory, quantifying investment risk, but also established a specific model describing risk and expected return. It broke away from the previous situation of qualitative analysis of investment portfolios without quantitative analysis, successfully introducing mathematical models into the field of financial investment.\n$$ \\begin{aligned} \\mathrm{E}\\left(R_{p}\\right) \u0026= \\sum_{i=1}^{n} w_{i} \\mathrm{E}\\left(R_{i}\\right) \\\\ \\sigma_{p}^{2} \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\operatorname{Cov}\\left(R_{i}, R_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\sigma_{i} \\sigma_{j} \\rho_{ij} \\\\ \\sigma_{i} \u0026= \\sqrt{\\operatorname{Var}\\left(R_{i}\\right)}, \\quad \\rho_{ij} = \\operatorname{Corr}\\left(R_{i}, R_{j}\\right) \\end{aligned} $$$$ \\min \\sigma_{p}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} \\mathrm{E}\\left(R_{i}\\right) w_{i} = \\mu_{p}, \\quad \\sum_{i=1}^{n} w_{i} = 1 $$$$ \\begin{aligned} \\Omega \u0026= \\begin{pmatrix} \\sigma_{11} \u0026 \\cdots \u0026 \\sigma_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\sigma_{n1} \u0026 \\cdots \u0026 \\sigma_{nn} \\end{pmatrix} = \\begin{pmatrix} \\operatorname{Var}\\left(R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Cov}\\left(R_{1}, R_{n}\\right) \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\operatorname{Cov}\\left(R_{n}, R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Var}\\left(R_{n}\\right) \\end{pmatrix} \\\\ \\Omega^{-1} \u0026= \\begin{pmatrix} v_{11} \u0026 \\cdots \u0026 v_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ v_{n1} \u0026 \\cdots \u0026 v_{nn} \\end{pmatrix} \\\\ w_{i} \u0026= \\frac{1}{D}\\left(\\mu_{p} \\sum_{j=1}^{n} v_{ij}\\left(C \\mathrm{E}\\left(R_{j}\\right) - A\\right) + \\sum_{j=1}^{n} v_{ij}\\left(B - A \\mathrm{E}\\left(R_{j}\\right)\\right)\\right), \\quad i = 1, \\ldots, n \\end{aligned} $$$$ \\begin{aligned} A \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{j}\\right), \\quad B = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{i}\\right) \\mathrm{E}\\left(R_{j}\\right), \\quad C = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij}, \\quad D = BC - A^{2} \u003e 0 \\\\ \\sigma_{p}^{2} \u0026= \\frac{C \\mu_{p}^{2} - 2A \\mu_{p} + B}{D} \\end{aligned} $$Where:\n$\\mathrm{E}\\left(R_{p}\\right)$ and $\\mu_{p}$ are the expected return of portfolio $p$ $\\mathrm{E}\\left(R_{i}\\right)$ is the expected return of asset $i$ $\\sigma_{i}$ is the standard deviation of asset $i$ $\\sigma_{j}$ is the standard deviation of asset $j$ $w_{i}$ is the proportion of asset $i$ in the portfolio $\\sigma_{p}^{2}$ is the variance of portfolio $p$ $\\rho_{ij}$ is the correlation coefficient between asset $i$ and asset $j$ Using the above formulas$^{[12]}$, we can construct an investment portfolio that minimizes non-systematic risk under a certain expected rate of return.\n2.2.2 Capital Asset Pricing Model William Sharpe (1964)$^{[13]}$, John Lintner (1965)$^{[14]}$, and Jan Mossin (1966)$^{[15]}$ proposed the Capital Asset Pricing Model (CAPM). This model posits that the expected return of an asset is related to its risk measure, the $\\beta$ value. Through a simple linear relationship, this model links the expected return of an asset to market risk, making Markowitz (1952)$^{[11]}$\u0026rsquo;s portfolio selection theory more relevant to the real world, and laying a theoretical foundation for the establishment of multi-factor stock selection models.\nAccording to the Capital Asset Pricing Model, for a given asset $i$, the relationship between its expected return and the expected return of the market portfolio can be expressed as:\n$$ E\\left(r_{i}\\right) = r_{f} + \\beta_{im}\\left[E\\left(r_{m}\\right) - r_{f}\\right] $$Where:\n$E\\left(r_{i}\\right)$ is the expected return of asset $i$ $r_{f}$ is the risk-free rate $\\beta_{im}$ (Beta) is the systematic risk coefficient of asset $i$, $\\beta_{im} = \\frac{\\operatorname{Cov}\\left(r_{i}, r_{m}\\right)}{\\operatorname{Var}\\left(r_{m}\\right)}$ $E\\left(r_{m}\\right)$ is the expected return of the market portfolio $m$ $E\\left(r_{m}\\right) - r_{f}$ is the market risk premium 2.2.3 Arbitrage Pricing Theory and Multi-Factor Model Ross (1976)$^{[16]}$ proposed the Arbitrage Pricing Theory (APT). This theory argues that arbitrage behavior is the decisive factor in forming market equilibrium prices. By introducing a series of factors in the return formation process to construct linear correlations, it overcomes the limitations of the Capital Asset Pricing Model (CAPM) and provides important theoretical guidance for subsequent researchers.\nArbitrage Pricing Theory is considered the theoretical basis of the Multi-Factor Model (MFM), an important component of asset pricing models, and one of the cornerstones of asset pricing theory. The general form of the multi-factor model is:\n$$ r_{j} = a_{j} + \\lambda_{j1} f_{1} + \\lambda_{j2} f_{2} + \\cdots + \\lambda_{jn} f_{n} + \\delta_{j} $$Where:\n$r_{j}$ is the return of asset $j$ $a_{j}$ is a constant for asset $j$ $f_{n}$ is the systematic factor $\\lambda_{jn}$ is the factor loading $\\delta_{j}$ is the random error 2.2.4 Fama-French Three-Factor Model and Five-Factor Model Fama (1992) and French (1992)$^{[17]}$ used a combination of cross-sectional regression and time series methods and found that the $\\beta$ value of the stock market could not explain the differences in returns of different stocks, while market capitalization, book-to-market ratio, and price-to-earnings ratio of listed companies could significantly explain the differences in stock returns. They argued that excess returns are compensation for risk factors not reflected by $\\beta$ in CAPM, and thus proposed the Fama-French Three-Factor Model. The three factors are:\nMarket Risk Premium Factor (Market Risk Premium)\nRepresents the overall systematic risk of the market, i.e., the difference between the expected return of the market portfolio and the risk-free rate. Measures the excess return investors expect for bearing systematic risk (risk that cannot be eliminated through diversification). Calculated as: $$ \\text{Market Risk Premium} = E(R_m) - R_f $$ where $E(R_m)$ is the expected market return, and $R_f$ is the risk-free rate. Size Factor (SMB: Small Minus Big)\nRepresents the return difference between small-cap stocks and large-cap stocks. Small-cap stocks are generally riskier, but historical data shows that their expected returns tend to be higher than those of large-cap stocks. Calculated as: $$ SMB = R_{\\text{Small}} - R_{\\text{Big}} $$ reflecting the market\u0026rsquo;s compensation for the additional risk premium of small-cap stocks. Value Factor (HML: High Minus Low)\nReflects the return difference between high book-to-market ratio stocks (i.e., \u0026ldquo;value stocks\u0026rdquo;) and low book-to-market ratio stocks (i.e., \u0026ldquo;growth stocks\u0026rdquo;). Stocks with high book-to-market ratios are usually priced lower (undervalued by the market), but may achieve higher returns in the long run. Calculated as: $$ HML = R_{\\text{High}} - R_{\\text{Low}} $$ Stocks with low book-to-market ratios may be overvalued due to overly optimistic market expectations. This model concretizes the factors in the APT model and concludes that investing in small-cap, high-growth stocks has the characteristics of high risk and high return. The Fama-French Three-Factor Model is widely used in the analysis and practice of modern investment behavior.\nSubsequently, Fama (2015) and French (2015)$^{[18]}$ extended the three-factor model, adding the following two factors:\nProfitability Factor (RMW: Robust Minus Weak)\nReflects the return difference between highly profitable companies and less profitable companies. Companies with strong profitability (high ROE, net profit margin) are more likely to provide stable and higher returns. Calculated as: $$ RMW = R_{\\text{Robust}} - R_{\\text{Weak}} $$ Investment Factor (CMA: Conservative Minus Aggressive)\nReflects the return difference between conservative investment companies and aggressive investment companies. Aggressive companies (rapidly expanding, high capital expenditure) are usually accompanied by greater operational risks, while conservative companies (relatively stable capital expenditure) show higher stability and returns. Calculated as: $$ CMA = R_{\\text{Conservative}} - R_{\\text{Aggressive}} $$ The Fama-French Three-Factor Model formula is:\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\epsilon_i $$The Fama-French Five-Factor Model formula is:\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\beta_{i,RMW} \\cdot RMW + \\beta_{i,CMA} \\cdot CMA + \\epsilon_i $$Where:\n$R_i$: Expected return of stock $i$ $R_f$: Risk-free rate of return $R_m$: Expected return of the market portfolio $R_m - R_f$: Market risk premium factor $SMB$: Return of small-cap stocks minus large-cap stocks $HML$: Return of high book-to-market ratio stocks minus low book-to-market ratio stocks $RMW$: Return of high profitability stocks minus low profitability stocks $CMA$: Return of conservative investment stocks minus aggressive investment stocks $\\beta_{i,*}$: Sensitivity of stock $i$ to the corresponding factor $\\epsilon_i$: Regression residual 2.2.5 Model Comparison Table The following table summarizes the core content and factor sources of the Mean-Variance Model, Capital Asset Pricing Model (CAPM), Arbitrage Pricing Theory (APT), and Fama-French Models:\nModel Core Content Factor Source Mean-Variance Model Foundation of portfolio theory, optimizes portfolio through expected returns and covariance matrix. Expected returns and covariance matrix of assets in portfolio Capital Asset Pricing Model (CAPM) Explains asset returns through market risk factor ($\\beta$), laying the theoretical foundation for multi-factor models. Market factor $\\beta$ Arbitrage Pricing Theory (APT) Multi-factor framework, allows multiple economic variables to explain asset returns, e.g., inflation rate, interest rate. Multiple factors (macroeconomic variables, e.g., inflation rate, interest rate) Fama-French Three-Factor Model Adds size factor and book-to-market ratio factor, improving the explanatory power of asset returns. Market factor, SMB (size factor), HML (book-to-market ratio factor) Fama-French Five-Factor Model Adds profitability factor and investment factor on the basis of the three-factor model, further improving asset pricing model. Market factor, SMB, HML, RMW (profitability factor), CMA (investment factor) The following table summarizes the advantages and disadvantages of these models:\nModel Advantages Disadvantages Mean-Variance Model Provides a systematic portfolio optimization method, laying the foundation for modern investment theory. Only optimizes for return and variance, does not explicitly specify the source of risk compensation. Capital Asset Pricing Model (CAPM) Simple and easy to use, explains return differences through market risk, provides a theoretical basis for multi-factor models. Assumes a single factor (market risk) determines returns, ignores other systematic risk factors. Arbitrage Pricing Theory (APT) Allows multiple factors to explain asset returns, reduces reliance on single-factor assumptions, more flexible. Does not specify concrete factors, lower operability, only provides a framework. Fama-French Three-Factor Model Significantly improves the explanatory power of asset returns by adding size factor and book-to-market ratio factor. Ignores other factors such as profitability and investment behavior. Fama-French Five-Factor Model More comprehensively captures key variables affecting asset returns by adding profitability factor and investment factor on the basis of the three-factor model. Higher model complexity, high data requirements, may still miss some potential factors. Chapter 3 Comparative Study of LSTM, GRU, and BiLSTM in Stock Price Prediction 3.1 Introduction to Experimental Data Many scholars, both domestically and internationally, focus their research on their own country\u0026rsquo;s stock indices, with relatively less research on individual stocks in different markets. Furthermore, few studies compare LSTM, GRU, and BiLSTM models directly. Therefore, this paper selects Shanghai Pudong Development Bank (SPD Bank, code 600000) in the domestic A-share market and International Business Machines Corporation (IBM) in the US stock market for research. This approach allows for a more accurate comparison of the three models used. For SPD Bank, stock data from January 1, 2008, to December 31, 2020, is used, totaling 3114 valid data points, sourced from the Tushare financial big data platform. We select six features from this dataset for the experiment: date, open price, close price, high price, low price, and volume. For the SPD Bank dataset, all five features except date (used as a time series index) are used as independent variables. For IBM, stock data from January 2, 1990, to November 15, 2018, is used, totaling 7278 valid data points, sourced from Yahoo Finance. We select seven features from this dataset for the experiment: date, open price, high price, low price, close price, adjusted close price (Adj Close), and volume. For the IBM dataset, all six features except date (used as a time series index) are used as independent variables. In this experiment, the closing price (close) is chosen as the variable to be predicted. Tables 3.1.1 and 3.1.2 show partial data from the two datasets.\n3.1.1 Partial Display of SPD Bank Dataset date open close high low volume code 2008-01-02 9.007 9.101 9.356 8.805 131583.90 600000 2008-01-03 9.007 8.645 9.101 8.426 211346.56 600000 2008-01-04 8659 9.009 9.111 8.501 139249.67 600000 2008-01-07 8.970 9.515 9.593 8.953 228043.01 600000 2008-01-08 9.551 9.583 9.719 9.517 161255.31 600000 2008-01-09 9.583 9.663 9.772 9.432 102510.92 600000 2008-01-10 9.701 9.680 9.836 9.602 217966.25 600000 2008-01-11 9.670 10.467 10.532 9.670 231544.21 600000 2008-01-14 10.367 10.059 10.433 10.027 142918.39 600000 2008-01-15 10.142 10.051 10.389 10.006 161221.52 600000 Data Source: Tushare\n3.1.2 Partial Display of IBM Dataset Date Open High Low Close Adj Close Volume 1990-01-02 23.6875 24.5313 23.6250 24.5000 6.590755 7041600 1990-01-03 24.6875 24.8750 24.5938 24.7188 6.649599 9464000 1990-01-04 24.7500 25.0938 24.7188 25.0000 6.725261 9674800 1990-01-05 24.9688 25.4063 24.8750 24.9375 6.708448 7570000 1990-01-08 24.8125 25.2188 24.8125 25.0938 6.750481 4625200 1990-01-09 25.1250 25.3125 24.8438 24.8438 6.683229 7048000 1990-01-10 24.8750 25.0000 24.6563 24.7500 6.658009 5945600 1990-01-11 24.8750 25.0938 24.8438 24.9688 6.716855 5905600 1990-01-12 24.6563 24.8125 24.4063 24.4688 6.582347 5390800 1990-01-15 24.4063 24.5938 24.3125 24.5313 6.599163 4035600 Data Source: Yahoo Finance\n3.2 Experimental Data Preprocessing 3.2.1 Data Normalization In the experiment, there are differences in units and magnitudes among various features. For example, the magnitude difference between stock prices and trading volume is huge, which will affect the final prediction results of our experiment. Therefore, we use the MinMaxScaler method from the sklearn.preprocessing library to scale the features of the data to between 0 and 1. This can not only improve the model accuracy but also increase the model convergence speed. The normalization formula is:\n$$ x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)} $$where $x^{\\prime}$ is the normalized data, $x$ is the original data, $\\min (x)$ is the minimum value of the original dataset, and $\\max (x)$ is the maximum value of the original dataset. After obtaining the prediction results in our experimental process, we also need to denormalize the data before we can perform stock price prediction and model evaluation.\n3.2.2 Data Partitioning Here, the entire experimental datasets of SPD Bank and IBM are input respectively, and the timestep of the recurrent kernel is set to 60 for both, with the number of input features per timestep being 5 and 6, respectively. This allows inputting data from the previous 60 trading days to predict the closing price on the 61st day. This makes our dataset meet the input requirements of the three neural network models to be compared later, which are the number of samples, the number of recurrent kernel unfolding steps, and the number of input features per timestep. Then, we divide the normalized SPD Bank dataset into training, validation, and test sets in a ratio of 2488:311:255. The normalized IBM dataset is divided into training, validation, and test sets in a ratio of 6550:364:304. We partition out a validation set here to facilitate adjusting the hyperparameters of the models to optimize each model before comparison.\n3.3 Model Network Structure The network structures of each model set in this paper through a large number of repeated experiments are shown in the table below. The default tanh and linear activation functions of recurrent neural networks are used between layers, and Dropout is added to prevent overfitting. The dropout rate is set to 0.2. The number of neurons in each recurrent layer of LSTM and GRU is 50, and the number of neurons in the recurrent layer of BiLSTM is 100. Each model of LSTM, GRU, and BiLSTM adopts four layers of LSTM, GRU, BiLSTM, and one fully connected layer, with a Dropout set between each network layer.\n3.3.1 LSTM Network Structure for IBM Layer(type) Output Shape Param# lstm_1 (LSTM) (None, 60, 50) 11400 dropout_1 (Dropout) (None, 60, 50) 0 lstm_2 (LSTM) (None, 60, 50) 20200 dropout_2 (Dropout) (None, 60, 50) 0 lstm_3 (LSTM) (None, 60, 50) 20200 dropout_3 (Dropout) (None, 60, 50) 0 lstm_4 (LSTM) (None, 50) 20200 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params: 72,051\nTrainable params: 72,051\nNon-trainable params: 0\n3.3.2 GRU Network Structure for IBM Layer(type) Output Shape Param# gru_1 (GRU) (None, 60, 50) 8550 dropout_1 (Dropout) (None, 60, 50) 0 gru_2 (GRU) (None, 60, 50) 15150 dropout_2 (Dropout) (None, 60, 50) 0 gru_3 (GRU) (None, 60, 50) 15150 dropout_3 (Dropout) (None, 60, 50) 0 gru_4 (GRU) (None, 50) 15150 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params: 54,051\nTrainable params: 54,051\nNon-trainable params: 0\n3.3.3 BiLSTM Network Structure for IBM Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 60, 100) 22800 dropout_1 (Dropout) (None, 60, 100) 0 bidirectional_2 (Bidirection) (None, 60, 100) 60400 dropout_2 (Dropout) (None, 60, 100) 0 bidirectional_3 (Bidirection) (None, 60, 100) 60400 dropout_3 (Dropout) (None, 60, 100) 0 bidirectional_4 (Bidirection) (None, 100) 60400 dropout_4 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 1) 101 Total params: 204,101\nTrainable params: 204,101\nNon-trainable params: 0\n3.4 Model Compilation and Hyperparameter Settings In this paper, after continuous hyperparameter tuning with the goal of minimizing the loss function on the validation set, the following hyperparameters are selected for the three models of SPD Bank: epochs=100, batch_size=32; and for the three models of IBM: epochs=50, batch_size=32. The optimizer used is Adaptive Moment Estimation (Adam)$^{[19]}$. The default values in its keras package are used, i.e., lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, and decay=0.0. The loss function is Mean Squared Error (MSE).\nParameter Explanation:\nlr: Learning rate beta_1: Exponential decay rate for the first moment estimate beta_2: Exponential decay rate for the second moment estimate epsilon: Fuzz factor decay: Learning rate decay value after each update 3.5 Experimental Results and Analysis First, let\u0026rsquo;s briefly introduce the evaluation metrics used for the models. The calculation formulas are as follows:\nMean Squared Error (MSE): $$ M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2} $$ Root Mean Squared Error (RMSE): $$ R M S E=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}} $$ Mean Absolute Error (MAE): $$ M A E=\\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-\\hat{Y}_{i}\\right| $$ \\( R^2 \\) (R Squared): $$ \\begin{gathered} \\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} \\\\ R^{2}=1-\\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\end{gathered} $$Where: $n$ is the number of samples, $Y_{i}$ is the actual closing price of the stock, $\\hat{Y}_{i}$ is the predicted closing price of the stock, and $\\bar{Y}$ is the average closing price of the stock. The smaller the MSE, RMSE, and MAE, the more accurate the model. The larger the \\( R^2 \\), the better the goodness of fit of the model coefficients.\n3.5.1 Experimental Results for SPD Bank LSTM GRU BiLSTM MSE 0.059781 0.069323 0.056454 RMSE 0.244501 0.263292 0.237601 MAE 0.186541 0.202665 0.154289 R-squared 0.91788 0.896214 0.929643 Comparing the evaluation metrics of the three models, we can find that on the SPD Bank test set, the MSE, RMSE, and MAE of the BiLSTM model are smaller than those of the LSTM and GRU models, while the R-Squared is larger than those of the LSTM and GRU models. By comparing RMSE, we find that BiLSTM has a 2.90% performance improvement over LSTM and a 10.81% performance improvement over GRU on the validation set.\n3.5.2 Experimental Results for IBM LSTM GRU BiLSTM MSE 18.01311 12.938584 11.057501 RMSE 4.244186 3.597024 3.325282 MAE 3.793223 3.069033 2.732075 R-squared 0.789453 0.851939 0.883334 Comparing the evaluation metrics of the three models, we can find that on the IBM test set, the MSE, RMSE, and MAE of the BiLSTM model are smaller than those of the LSTM and GRU models, while the R-Squared is larger than those of the LSTM and GRU models. By comparing RMSE, we find that BiLSTM has a 27.63% performance improvement over LSTM and an 8.17% performance improvement over GRU on the validation set.\n3.6 Chapter Summary This chapter first introduced the SPD Bank and IBM datasets and the features used in the experiment. Then, it performed preprocessing steps of data normalization and data partitioning on the datasets. It also detailed the network structures and hyperparameters of the LSTM, GRU, and BiLSTM models used in the experiment. Finally, it obtained the loss function images and a series of fitting graphs for each model. By comparing multiple evaluation metrics and fitting images of the models, it is concluded that the BiLSTM model can better predict stock prices, laying a foundation for our next chapter\u0026rsquo;s research on the LightGBM-BiLSTM quantitative investment strategy.\nChapter 4 Research on Quantitative Investment Model Based on LightGBM-BiLSTM 4.1 Experimental Steps Fig. 11. LightGBM-BiLSTM Diagram.\nAs shown in Fig. 11, this experiment first selects 50 factors from the factor library. Then, it performs factor cleaning steps of outlier removal, standardization, and missing value imputation on the factors sequentially. Next, the LightGBM model is used for factor selection, and the top ten factors with the highest importance are selected as the factors for this cross-sectional selection. Subsequently, a BiLSTM model is used to establish a multi-factor model, and finally, backtesting analysis is performed.\n4.2 Experimental Data The market data used in this paper comes from Tushare. The specific features of the dataset are shown in the table below.\n4.2.1 Features Included in the Stock Dataset Name Type Description ts_code str Stock code trade_date str Trading date open float Open price high float High price low float Low price close float Close price pre_close float Previous close price change float Change amount pct_chg float Change percentage (unadjusted) vol float Volume (in hands) amount float Turnover (in thousands of CNY) The A-share market-wide daily dataset contains 5,872,309 rows of data, i.e., 5,872,309 samples. As shown in Table 4.2.1, the A-share market-wide daily dataset has the following 11 features, in order: stock code (ts_code), trading date (trade_date), open price (open), high price (high), low price (low), close price (close), previous close price (pre_close), change amount (change), turnover rate (turnover_rate), turnover amount (amount), total market value (total_mv), and adjustment factor (adj_factor).\n4.2.2 Partial Display of A-Share Market-Wide Daily Dataset ts_code trade_date open high low close pre_close change vol amount 600613.SH 20120104 8.20 8.20 7.84 7.86 8.16 -0.30 4762.98 3854.1000 600690.SH 20120104 9.00 9.17 8.78 8.78 8.93 -0.15 142288.41 127992.6050 300277.SZ 20120104 22.90 22.98 20.81 20.88 22.68 -1.80 12212.39 26797.1370 002403.SZ 20120104 8.87 8.90 8.40 8.40 8.84 -0.441 10331.97 9013.4317 300179.SZ 20120104 19.99 20.32 19.20 19.50 19.96 -0.46 1532.31 3008.0594 600000.SH 20120104 8.54 8.56 8.39 8.41 8.49 -0.08 342013.79 290229.5510 300282.SZ 20120104 22.90 23.33 21.02 21.02 23.35 -2.33 38408.60 86216.2356 002319.SZ 20120104 9.74 9.95 9.38 9.41 9.73 -0.32 4809.74 4671.4803 601991.SH 20120104 5.17 5.39 5.12 5.25 5.16 0.09 145268.38 76547.7490 000780.SZ 20120104 10.42 10.49 10.00 10.00 10.30 -0.30 20362.30 20830.1761 [5872309 rows x 11 columns]\nThe CSI All Share daily dataset contains 5,057 rows of data, i.e., 5,057 samples. As shown in Table 4.2.2, the CSI All Share daily dataset has the following 7 features, in order: trading date (trade_date), open price (open), high price (high), low price (low), close price (close), volume (volume), and previous close price (pre_close).\n4.2.3 Partial Display of CSI All Share Daily Dataset trade_date open high low close volume pre_close 2006-11-24 1564.3560 1579.3470 1549.9790 1576.1530 7.521819e+09 1567.0910 2006-11-27 1574.1130 1598.7440 1574.1130 1598.7440 7.212786e+09 1581.1530 2006-11-28 1597.7200 1604.7190 1585.3620 1596.8400 7.025637e+09 1598.7440 2006-11-29 1575.3030 1620.2870 1575.3030 1617.9880 7.250354e+09 1596.8400 2006-11-30 1621.4280 1657.3230 1621.4280 1657.3230 9.656888e+09 1617.9880 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 2020-11-11 5477.8870 5493.5867 5422.9110 5425.8017 5.604086e+10 5494.1042 2020-11-12 5439.2296 5454.3452 5413.9659 5435.1379 4.594251e+10 5425.8017 2020-11-13 5418.2953 5418.3523 5364.2031 5402.7702 4.688916e+10 5435.1379 2020-11-16 5422.3565 5456.7264 5391.9232 5456.7264 5.593672e+10 5402.7702 2020-11-17 5454.0696 5454.0696 5395.6052 5428.0765 5.857009e+10 5456.7264 [5057 rows x 7 columns]\nTable 4.2.4 below shows partial data of the original factors. After sequentially going through the four factor cleaning steps of missing value imputation, outlier removal, factor standardization, and factor neutralization mentioned above, partial data of the cleaned factors are obtained as shown in Table 4.2.5.\n4.2.4 Original Factor Data trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 4.2.5 Cleaned Factor Data sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.2.6 Factor Data Construction of Price-Volume Factors This paper uses the following method to construct price-volume factors. There are two basic elements for constructing price-volume factors: first, basic fields, and second, operators. As shown in Table 4.2.1, basic fields include daily frequency high price, low price, open price, close price, previous day\u0026rsquo;s close price, volume, change percentage, turnover rate, turnover amount, total market value, and adjustment factor.\n4.2.7 Basic Field Table No. Field Name Meaning high High Price Highest price in intraday transactions low Low Price Lowest price in intraday transactions open Open Price Price at which the call auction concludes close Close Price Price of the last transaction of the day pre_close Previous Close Price Price of the last transaction of the previous day vol Volume Number of shares traded throughout the day pct_chg Change Percentage Percentage change of the security for the day turnover_rate Turnover Rate Turnover rate of the security for the day amount Turnover Amount Total value of transactions for the day total_mv Total Market Value Total value of the stock, calculated by total shares outstanding multiplied by the current stock price adj_factor Adjustment Factor Ratio for adjusting for dividends and splits This paper obtains the operator list shown in the table below through the basic operator set provided by gplearn and some self-defined special operators.\n4.2.8 Operator List Operator Name Definition add(x, y) Sum \\( x + y\\); element-wise operation \\(\\operatorname{div}(x, y)\\) Division \\( x / y\\); element-wise operation \\(\\operatorname{mul}(x, y)\\) Multiplication \\( x \\cdot y\\); element-wise operation \\(\\operatorname{sub}(x, y)\\) Subtraction \\( x - y\\); element-wise operation neg(x) Negative \\(-x\\); element-wise operation \\(\\log(x)\\) Logarithm \\(\\log(x)\\); element-wise operation max(x, y) Maximum Larger value between \\(x\\) and \\(y\\); element-wise operation \\(\\min(x, y)\\) Minimum Smaller value between \\(x\\) and \\(y\\); element-wise operation delta_d(x) d-day Difference Current day\u0026rsquo;s \\(x\\) value minus \\(x\\) value \\(d\\) days ago; time series operation delay_d(x) d-day Delay \\(x\\) value \\(d\\) days ago; time series operation Corr_d(x, y) d-day Correlation Correlation between \\(x\\) values and \\(y\\) values over \\(d\\) days; time series operation Max_d(x) d-day Maximum Maximum value of \\(x\\) over \\(d\\) days; time series operation Min_d(x) d-day Minimum Minimum value of \\(x\\) over \\(d\\) days; time series operation sort_d(x) d-day Rank Rank of \\(x\\) values over \\(d\\) days; time series operation Argsortmin_d(x) d-day Minimum Position Position of the minimum value of \\(x\\) over \\(d\\) days; time series operation Argsortmax_d(x) d-day Maximum Position Position of the maximum value of \\(x\\) over \\(d\\) days; time series operation \\(\\operatorname{inv}(x)\\) Inverse \\( 1 / x\\); element-wise operation Std_d(x) d-day Standard Deviation Standard deviation of \\(x\\) values over \\(d\\) days; time series operation abs(x) Absolute Value \\(\\lvert x\\rvert\\); element-wise operation 4.2.9 Genetic Programming The core idea of Genetic Programming (GP) is to use evolutionary algorithms to automatically \u0026ldquo;evolve\u0026rdquo; factor expressions with strong predictive power in the vast search space composed of operators and basic fields. For factor mining in this paper, the main goal of GP is to search and find those factors that can better predict future stock returns from all possible expressions that can be combined from the basic fields in Table 4.2.7 and the operators in Table 4.2.8. The core process of GP can be divided into the following steps:\nInitialization Define Operator Set and Basic Fields\nOperator set (operators) as shown in Table 4.2.8, including operations such as addition, subtraction, multiplication, division, logarithm, absolute value, delay, moving maximum/minimum, moving correlation coefficient, etc. Basic fields (terminals) as shown in Table 4.2.7, including open price, close price, high price, low price, volume, adjustment factor, etc. These operators and basic fields can be regarded as \u0026ldquo;nodes\u0026rdquo; in the factor expression tree, where basic fields are leaf nodes (terminal nodes), and operators are internal nodes. Randomly Generate Initial Population\nIn the initialization phase, based on the given operator set and field set, a series of factor expressions (which can be represented as several syntax trees or expression trees) are randomly \u0026ldquo;spliced\u0026rdquo; to form an initial population. For example, it may randomly generate \\[ \\text{Factor 1}: \\mathrm{Max\\_5}\\bigl(\\mathrm{add}(\\mathrm{vol}, \\mathrm{close})\\bigr), \\quad \\text{Factor 2}: \\mathrm{sub}\\bigl(\\mathrm{adj\\_factor}, \\mathrm{neg}(\\mathrm{turnover\\_rate})\\bigr), \\dots \\] Each factor expression will correspond to an individual. Fitness Function Measure Factor\u0026rsquo;s Predictive Ability\nFor each expression (individual), we need to evaluate its predictive ability for future returns or other objectives. Specifically, we can calculate the correlation coefficient (IC) or a more comprehensive indicator IR (Information Ratio) between the next period\u0026rsquo;s stock return \\( r^{T+1} \\) and the current period\u0026rsquo;s factor exposure \\( x_k^T \\) to measure it. Set Objective\nIf we want the factor to have a higher correlation (IC), we can set the fitness function to \\(\\lvert \\rho(x_k^T, r^{T+1})\\rvert\\); If we want the factor to have a higher IR, we can set the fitness function to the IR value. The higher the factor IC or IR, the higher the \u0026ldquo;fitness\u0026rdquo; of the expression. Therefore, we usually set: \\[ \\text{Fitness} \\bigl(F(x)\\bigr) \\;=\\; \\begin{cases} \\lvert \\rho(x_k^T, r^{T+1})\\rvert \\quad \u0026\\text{(Maximize IC)},\\\\[6pt] \\mathrm{IR}(x_k^T) \\quad \u0026\\text{(Maximize IR)}. \\end{cases} \\] where \\(\\rho(\\cdot)\\) represents the correlation coefficient, and \\(\\mathrm{IR}(\\cdot)\\) is the IR indicator.\nSelection, Crossover, and Mutation Selection\nBased on the results of the fitness function, expressions with high factor fitness are \u0026ldquo;retained\u0026rdquo; or \u0026ldquo;bred\u0026rdquo;, while expressions with lower fitness are eliminated. This is similar to \u0026ldquo;survival of the fittest\u0026rdquo; in biological evolution. Crossover\nRandomly select a part of the \u0026ldquo;nodes\u0026rdquo; of several expressions with higher fitness (parents) for exchange, so as to obtain new expressions (offspring). In the expression tree structure, subtree A and subtree B can be interchanged to generate new offspring expressions. For example, if a subtree of expression tree \\(\\mathrm{FactorA}\\) is exchanged with the corresponding subtree of expression tree \\(\\mathrm{FactorB}\\), two new expressions are generated. Mutation\nRandomly change some nodes of the expression with a certain probability, such as: Replacing the operator of the node (for example, changing \\(\\mathrm{add}\\) to \\(\\mathrm{sub}\\)), Replacing the basic field of the terminal node (for example, changing \\(\\mathrm{vol}\\) to \\(\\mathrm{close}\\)), Or randomly changing operation parameters (such as moving window length, smoothing factor, etc.). Mutation can increase the diversity of the population and avoid premature convergence or falling into local optima. Iterative Evolution Iterative Execution\nRepeatedly execute selection, crossover, and mutation operations for multiple generations. Each generation produces a new population of factor expressions and evaluates their fitness. Convergence and Termination\nWhen evolution reaches a predetermined stopping condition (such as the number of iterations, fitness threshold, etc.), the algorithm terminates. Usually, we will select several factor expressions with higher final fitness and regard them as the evolution results. Mathematical Representation: Searching for Optimal Factor Expressions Abstracting the above process into the following formula, the factor search objective can be simply expressed as:\n\\[ F(x) \\;=\\; \\mathrm{GP}\\bigl(\\{\\text{operators}\\}, \\{\\text{terminals}\\}\\bigr), \\] indicating that a function \\(F(x)\\) is searched through the GP algorithm on a given operator set (operators) and basic field set (terminals). From the perspective of optimization, we hope to find:\n\\[ \\max_{F} \\bigl\\lvert \\rho(F^T, r^{T+1}) \\bigr\\rvert \\quad \\text{or} \\quad \\max_{F} \\; \\mathrm{IR}\\bigl(F\\bigr), \\] where\n\\(\\rho(\\cdot)\\) represents the correlation coefficient (IC) between the factor and the next period\u0026rsquo;s return, \\(\\mathrm{IR}(\\cdot)\\) represents the IR indicator of the factor. In practical applications, we will give a backtesting period, score the candidate factors of each generation (IC/IR evaluation), and continuously \u0026ldquo;evolve\u0026rdquo; better factors through the iterative process of selection, crossover, and mutation.\nThrough the above steps, we can finally automatically mine a batch of factor expressions that have strong predictive power for future returns and good robustness (such as higher IR) in the vast search space of operator combinations and basic field combinations.\n4.2.10 Partially Mined Factors Factor Name Definition 0 Max＿25(add(turnover_rate, vol)) 1 Max＿30(vol) 2 Max＿25(turnover_rate) 3 Max＿35(add(vol, close)) 4 Max＿30(turnover_rate) 5 sub(Min＿20(neg(pre_close)), div(vol, adj_factor)) 6 Max＿60(max(vol, adj_factor)) 7 Max＿50(amount) 8 div(vol, neg(close)) 9 min(ArgSortMin＿25(pre_close), neg(vol)) 10 neg(max(vol, turnover_rate)) 11 mul(amount, neg(turnover_rate)) 12 inv(add(ArgSortMax＿40(change), inv(pct_chg))) 13 Std＿40(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))) 14 div(log(total_mv),amount) 15 div(neg(Max＿5(amount)), Min＿20(ArgSort＿60(high))) 16 Corr＿30(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))), add(log(Max＿10(pre_close)), high)) 17 ArgSort＿60(neg(turnover_rate)) \u0026hellip; \u0026hellip; These factors are all obtained by combining from the operator list (Table 4.2.8) and the basic field list (Table 4.2.7) through genetic programming and have different mathematical expressions.\nFactor Validity Test After we get the mined factors, we need to test the validity of the factors. Common test indicators are Information Coefficient (IC) and Information Ratio (IR).\nInformation Coefficient (IC) describes the linear correlation between the next period\u0026rsquo;s return rate of the selected stocks and the current period\u0026rsquo;s factor exposure, which can reflect the robustness of the factor in predicting returns. Information Ratio (IR) is the ratio of the mean of excess returns to the standard deviation of excess returns. The information ratio is similar to the Sharpe ratio. The main difference is that the Sharpe ratio uses the risk-free return as a benchmark, while the information ratio uses a risk index as a benchmark. The Sharpe ratio helps to determine the absolute return of a portfolio, and the information ratio helps to determine the relative return of a portfolio. After we calculate the IC, we can calculate the IR based on the IC value. When the IR is greater than 0.5, the factor has a strong ability to stably obtain excess returns. In actual calculation, the \\( \\mathrm{IC} \\) value of factor \\(k\\) generally refers to the correlation coefficient between the exposure \\( x_k^T \\) of factor \\(k\\) in period \\(T\\) of the selected stocks and the return rate \\( r^{T+1} \\) of the selected stocks in period \\(T+1\\); the \\( \\mathrm{IR} \\) value of factor \\(k\\) is the mean of the \\( \\mathrm{IC} \\) of factor \\(k\\) divided by the standard deviation of the \\( \\mathrm{IC} \\) of factor \\(k\\). The calculation formulas are as follows:\n$$ \\begin{gathered} I C=\\rho_{x_{k}^{T}, r^{T+1}}=\\frac{\\operatorname{cov}\\left(x_{k}^{T}, r^{T+1}\\right)}{\\sigma_{x_{k}^{T}} \\sigma_{r^{T+1}}}=\\frac{\\mathrm{E}\\left(x_{k}^{T} * r^{T+1}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right) \\mathrm{E}\\left(r^{T+1}\\right)}{\\sqrt{\\mathrm{E}\\left(\\left(x_{k}^{T}\\right)^{2}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right)^{2}} \\cdot \\sqrt{\\mathrm{E}\\left(\\left(r^{T+1}\\right)^{2}\\right)-\\mathrm{E}\\left(r^{T+1}\\right)^{2}}} \\\\ I R=\\frac{\\overline{I C}}{\\sigma_{I C}} \\end{gathered} $$Where:\n$x_{k}^{T}$: the exposure of the selected stock to factor $k$ in period $T$ $r^{T+1}$: the return of the selected stock in period $T+1$ $\\overline{IC}$: the mean of the Information Coefficient (IC) This paper uses IR to judge the quality of factors. Through \u0026ldquo;screening\u0026rdquo; a large number of different combinations of operators and basic data and IC and IR, this paper obtains the 50 price-volume factors selected in this paper. After IR testing, the table shown in the figure below is obtained by sorting IR from high to low. From the table below, we can see that the IRs of the selected 50 price-volume factors are all greater than 0.5, indicating that these factors have a strong ability to stably obtain excess returns.\n4.2.11 Factor IR Test Table Factor Name IR Factor Name IR 0 3.11 25 2.73 1 2.95 26 2.71 2 2.95 27 2.70 3 2.95 28 2.69 4 2.95 29 2.69 5 2.94 30 2.69 6 2.94 31 2.68 7 2.94 32 2.68 8 2.93 33 2.68 9 2.93 34 2.68 10 2.93 35 2.67 11 2.92 36 2.67 12 2.91 37 2.66 13 2.89 38 2.65 14 2.86 39 2.65 15 2.83 40 2.65 16 2.83 41 2.65 17 2.83 42 2.64 18 2.79 43 2.63 19 2.78 44 2.63 20 2.78 45 2.62 21 2.76 46 2.62 22 2.75 47 2.62 It can be seen from this table that among the screened factors, the IRs of all factors are greater than 0.5, which has a strong and stable ability to obtain excess returns.\n4.3 Factor Cleaning 4.3.1 Factor Missing Value Handling and Outlier Removal Methods for handling missing values of factors include case deletion, mean imputation, regression imputation, and other methods. This paper adopts a relatively simple mean imputation method to handle missing values, that is, using the average value of the factor to replace the missing data. Methods for factor outlier removal include median outlier removal, percentile outlier removal, and $3 \\sigma$ outlier removal. This paper uses the $3 \\sigma$ outlier removal method. This method uses the $3 \\sigma$ principle in statistics to convert outlier factors that are more than three standard deviations away from the mean of the factor to a position that is just three standard deviations away from the mean. The specific calculation formula is as follows:\n$$ X_i^{\\prime}= \\begin{cases} \\bar{X}+3 \\sigma \u0026 \\text{if } X_i \u003e \\bar{X} + 3 \\sigma \\\\ \\bar{X}-3 \\sigma \u0026 \\text{if } X_i \u003c \\bar{X} - 3 \\sigma \\\\ X_i \u0026 \\text{if } \\bar{X} - 3 \\sigma \u003c X_i \u003c \\bar{X} + 3 \\sigma \\end{cases} $$Where:\n$X_{i}$: Value of the factor before processing $\\bar{X}$: Mean of the factor sequence $\\sigma$: Standard deviation of the factor sequence $X_{i}^{\\prime}$: Value of the factor after outlier removal 4.3.2 Factor Standardization In this experiment, multiple factors are selected, and the dimensions of each factor are not completely the same. For the convenience of comparison and regression, we also need to standardize the factors. Currently, common specific standardization methods include Min-Max standardization, Z-score standardization, and Decimal scaling standardization. This paper chooses the Z-score standardization method. The data is standardized through the mean and standard deviation of the original data. The processed data conforms to the standard normal distribution, that is, the mean is 0 and the standard deviation is 1. The standardized numerical value is positive or negative, and a standard normal distribution curve is obtained.\nThe Z-score standardization formula used in this paper is as follows:\n$$ \\tilde{x}=\\frac{x_{i}-u}{\\sigma} $$Where:\n$x_{i}$: Original value of the factor $u$: Mean of the factor sequence $\\sigma$: Standard deviation of the factor sequence $\\tilde{x}$: Standardized factor value 4.3.3 Factor Neutralization Factor neutralization is to eliminate the influence of other factors on our selected factors, so that the stocks selected by our quantitative investment strategy portfolio are more dispersed, rather than concentrated in specific industries or market capitalization stocks. It can better share the risk of the investment portfolio and solve the problem of factor multicollinearity. Market capitalization and industry are the two main independent variables that affect stock returns. Therefore, in the process of factor cleaning, the influence of market capitalization and industry must also be considered. In this empirical study, we adopt the method of only including industry factors and including market factors in industry factors. The single-factor regression model for factors is shown in formula (31). We take the residual term of the following regression model as the new factor value after factor neutralization.\n$$ \\tilde{r}_{j}^{t}=\\sum_{s=1}^{s} X_{j s}^{t} \\tilde{f}_{s}^{t}+X_{j k}^{t} \\tilde{f}_{k}^{t}+\\tilde{u}_{j}^{t} $$Where:\n$\\tilde{r}_{j}^{t}$: Return rate of stock $j$ in period $t$ $X_{j s}^{t}$: Exposure of stock $j$ in industry $s$ in period $t$ $\\tilde{f}_{s}^{t}$: Return rate of the industry in period $t$ $X_{j k}^{t}$: Exposure of stock $j$ on factor $k$ in period $t$ $\\tilde{f}_{k}^{t}$: Return rate of factor $k$ in period $t$ $\\tilde{u}_j^t$: A $0-1$ dummy variable, that is, if stock $j$ belongs to industry $s$, the exposure is 1, otherwise it is 0 In this paper, the industry to which a company belongs is not proportionally split, that is, stock $j$ can only belong to a specific industry $s$, the exposure in industry $s$ is 1, and the exposure in all other industries is 0. This paper uses the Shenwan Hongyuan industry classification standard. The specific classifications are sequentially: agriculture, forestry, animal husbandry and fishery, mining, chemical industry, steel, nonferrous metals, electronic components, household appliances, food and beverage, textile and apparel, light industry manufacturing, pharmaceutical and biological, public utilities, transportation, real estate, commercial trade, catering and tourism, comprehensive, building materials, building decoration, electrical equipment, national defense and military industry, computer, media, communication, banking, non-banking finance, automobile, and mechanical equipment, a total of 28 categories. The table below shows the historical market chart of Shenwan Index Level 1 industries on February 5, 2021.\n4.3.3.1 Historical Market Chart of Shenwan Index Level 1 Industries on February 5, 2021 Index Code Index Name Release Date Open Index High Index Low Index Close Index Volume (100 Million Hands) Turnover (100 Million CNY) Change (%) 801010 Agriculture, Forestry, Animal Husbandry and Fishery 2021/2/5 0:00 4111.43 4271.09 4072.53 4081.81 15.81 307.82 -0.3 801020 Mining 2021/2/5 0:00 2344.62 2357.33 2288.97 2289.41 18.06 115.6 -2.25 801030 Chemical Industry 2021/2/5 0:00 4087.77 4097.59 3910.67 3910.67 55.78 778.85 -3.95 801040 Steel 2021/2/5 0:00 2253.78 2268.17 2243.48 2250.81 11.61 48.39 -1.02 801050 Nonferrous Metals 2021/2/5 0:00 4212.1 4250.59 4035.99 4036.74 45.41 593.92 -4.43 801080 Electronic Components 2021/2/5 0:00 4694.8 4694.8 4561.95 4561.95 52.67 850.79 -2.78 801110 Household Appliances 2021/2/5 0:00 10033.82 10171.26 9968.93 10096.83 8.55 149.18 0.83 801120 Food and Beverage 2021/2/5 0:00 30876.33 31545.02 30649.57 30931.69 11.32 657.11 0.47 801130 Textile and Apparel 2021/2/5 0:00 1614.48 1633.89 1604.68 1607.63 6.28 57.47 -0.39 801140 Light Industry Manufacturing 2021/2/5 0:00 2782.07 2791.88 2735.48 2737.24 15.28 176.16 -1.35 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Data Source: Shenwan Hongyuan\nThe table below is partial data of the original factors. After sequentially going through the four factor cleaning steps of missing value imputation, factor outlier removal, factor standardization, and factor neutralization mentioned above, partial data of the cleaned factors are obtained as shown in the table.\n4.3.3.2 Original Factor Data trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 4.3.3.3 Cleaned Factor Data sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.4 Factor Selection Based on LightGBM 4.4.1 GBDT Gradient Boosting Decision Tree (GBDT), proposed by Friedman (2001)$^{[20]}$, is an iterative regression decision tree. Its main idea is to optimize the model by gradually adding weak classifiers (usually decision trees), so that the overall model can minimize the loss function. The GBDT model can be expressed as:\n$$ \\hat{y} = \\sum_{m=1}^{M} \\gamma_m h_m(\\mathbf{x}) $$Where:\n\\( M \\) is the number of iterations, \\( \\gamma_m \\) is the weight of the \\( m \\)-th weak classifier, \\( h_m(\\mathbf{x}) \\) is the \\( m \\)-th decision tree model. The training process of GBDT minimizes the loss function by gradually fitting the negative gradient direction. The specific update formula is:\n$$ \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{N} L\\left(y_i, \\hat{y}_{i}^{(m-1)} + \\gamma h_m(\\mathbf{x}_i)\\right) $$Where, \\( L \\) is the loss function, \\( y_i \\) is the true value, and \\( \\hat{y}_{i}^{(m-1)} \\) is the predicted value after the \\( (m-1) \\)-th iteration.\n4.4.2 LightGBM Light Gradient Boosting Machine (LightGBM)$^{[21]}$ is an efficient framework for implementing the GBDT algorithm, initially developed by Microsoft as a free and open-source distributed gradient boosting framework. LightGBM is based on decision tree algorithms and is widely used in ranking, classification, and other machine learning tasks. Its development focuses on performance and scalability. Its main advantages include high-efficiency parallel training, faster training speed, lower memory consumption, better accuracy, and support for distributed computing and fast processing of massive data$^{[22]}$.\nThe core algorithm of LightGBM is based on the following optimization objective:\n$$ L = \\sum_{i=1}^{N} l(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(h_m) $$Where, \\( l \\) is the loss function, and \\( \\Omega \\) is the regularization term, used to control model complexity, usually expressed as:\n$$ \\Omega(h_m) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 $$Here, \\( T \\) is the number of leaves in the tree, \\( w_j \\) is the weight of the \\( j \\)-th leaf, and \\( \\gamma \\) and \\( \\lambda \\) are regularization parameters.\nLightGBM uses technologies such as Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB), which significantly improve training efficiency and model performance.\nIn this study, the loss function used during training is Mean Squared Error (MSE), which is defined as:\n$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$Where, \\( y \\) is the true return rate, \\( \\hat{y} \\) is the return rate predicted by the model, and \\( N \\) is the number of samples.\n4.4.3 Algorithm Flow The specific algorithm flow in this section is as follows:\nData Preparation: Use one year\u0026rsquo;s worth of 50 factor data for each stock (A-share market-wide data) and historical future one-month returns as features.\nModel Training: Use Grid Search to optimize the hyperparameters of the LightGBM model and train the model to predict the future one-month return rate. The model training flow is shown in Fig. 4.12.\n$$ \\text{Parameter Optimization:} \\quad \\theta^* = \\arg\\min_\\theta \\sum_{i=1}^{N} L(y_i, \\hat{y}_i(\\theta)) $$Where, \\( \\theta \\) represents the set of model hyperparameters, and \\( \\theta^* \\) is the optimal parameter.\nFactor Importance Calculation: Use LightGBM\u0026rsquo;s feature_importances_ method to calculate the feature importance of each factor. Feature importance is mainly measured by two indicators:\nSplit: The number of times the feature is used for splitting in all trees. Gain: The total gain brought by the feature in all splits (i.e., the amount of reduction in the loss function). The feature importance of a factor can be expressed as:\n$$ \\text{Importance}_{\\text{split}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\mathbb{I}(f \\text{ is used for splitting the } j \\text{-th leaf node}) $$$$ \\text{Importance}_{\\text{gain}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\Delta L_{m,j} \\cdot \\mathbb{I}(f \\text{ is used for splitting the } j \\text{-th leaf node}) $$Where, \\( \\mathbb{I} \\) is the indicator function, and \\( \\Delta L_{m,j} \\) is the reduction in loss brought by factor \\( f \\) in the \\( j \\)-th split of the \\( m \\)-th tree.\nFactor Screening: Sort according to the factor importance calculated by the model, and select the top ten factors with the highest importance as the factors used in this cross-sectional analysis. The importance of the selected factors is shown in Table 4.4.4.\n4.4.4 Partial Ranking of Selected Factor Importance importance feature_name trade_date 35 factor_35 2010-08-11 27 factor_27 2010-08-11 33 factor_33 2010-08-11 20 factor_20 2010-08-11 24 factor_24 2010-08-11 45 factor_45 2010-08-11 37 factor_37 2010-08-11 49 factor_49 2010-08-11 19 factor_19 2010-08-11 47 factor_47 2010-08-11 22 factor_22 2010-09-09 20 factor_20 2010-09-09 30 factor_30 2010-09-09 24 factor_24 2010-09-09 4.4.5 Code Implementation Snippet The following is a code snippet used in the training process for factor selection.\nfeature_choice def feature_choice( self, days=21, is_local=False ): if is_local: feature_info = pd.read_hdf(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;)) else: factors = self.get_env().query_data(Factors_Data) factors = factors[ factors[COM_DATE] \u0026gt;= \u0026#39;2010-01-01\u0026#39; ] trade_list = list(set(factors[COM_DATE])) trade_list.sort() if len(trade_list) % days == 0: n = int(len(trade_list) / days) - 7 else: n = int(len(trade_list) / days) - 6 feature_info = pd.DataFrame() begin_index = 147 feature = list(factors.columns) feature.remove(COM_SEC) feature.remove(COM_DATE) feature.remove(Ret) for i in range(n): end_date = days * i + begin_index - 21 begin_date = days * i trade_date = days * i + begin_index print(trade_list[trade_date]) train_data = factors[ (factors[COM_DATE] \u0026lt;= trade_list[end_date]) \u0026amp; (factors[COM_DATE] \u0026gt;= trade_list[begin_date]) ] model = lgb.LGBMRegressor() model.fit(train_data[feature], train_data[Ret]) feature_info_cell = pd.DataFrame(columns=Info_Fields) feature_info_cell[Importance] = model.feature_importances_ feature_info_cell[Feature_Name] = model.feature_name_ feature_info_cell = feature_info_cell.sort_values(by=Importance).tail(10) feature_info_cell[COM_DATE] = trade_list[trade_date] feature_info = pd.concat( [feature_info, feature_info_cell], axis=0 ) h = pd.HDFStore(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;), \u0026#39;w\u0026#39;) h[\u0026#39;data\u0026#39;] = feature_info h.close() self.get_env().add_data(feature_info, Feature_Info) pass Through the above process, LightGBM is used to efficiently screen out the factors that have the greatest impact on predicting future returns, thereby improving the predictive ability and interpretability of the model.\n4.5 Factor Combination Based on BiLSTM This section uses BiLSTM for factor combination. The specific principle of BiLSTM has been introduced in Chapter 2, and will not be repeated here. First, let\u0026rsquo;s introduce the specific network structure of the model. The network structure of BiLSTM set in this paper through a large number of repeated experiments is shown in Table 4.5.1. The default tanh and linear activation functions of recurrent neural networks are used between layers. Dropout is added to prevent overfitting, but if Dropout uses an excessively large dropout rate, underfitting will occur. Therefore, the dropout rate of Dropout is set to 0.01. The number of neurons in the BiLSTM recurrent layer of the final model is 100. A BiLSTM layer and three fully connected layers are used, and a Dropout is set between the BiLSTM layer and the first fully connected layer.\n4.5.1 BiLSTM Network Structure Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 100) 24400 dropout_1 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 256) 25856 dropout_2 (Dropout) (None, 256) 0 dense_2 (Dense) (None, 64) 16448 dense_3 (Dense) (None, 1) 0 Total params: 66,769\nTrainable params: 66,769\nNon-trainable params: 0\nBecause the amount of data used in this experiment is large, epochs=400 and batch_size=1024 are selected. The loss function of the model is Mean Squared Error (MSE). The optimizer used is Stochastic Gradient Descent (SGD). Stochastic gradient descent has three advantages over gradient descent (GD): it can more effectively use information when information is redundant, and the early iteration effect is excellent, which is suitable for processing large-sample data $^{[23]}$. Since the amount of training data in this experiment is large, if SGD is used, only one sample is used for iteration each time, and the training speed is very fast, which can greatly reduce the time spent on training. The default values in its keras package are used, i.e., lr=0.01, momentum=0.0, decay=0.0, and nesterov=False.\nParameter Explanation:\nlr: Learning rate momentum: Momentum parameter decay: Learning rate decay value after each update nesterov: Determine whether to use Nesterov momentum 4.5.2 Algorithm Flow The specific algorithm flow in this section is as follows:\nUse A-share market-wide data of 10 factors (factors selected by LightGBM) and historical future one-month returns for each stock for one year as features. Take the future one-month return rate of each stock per year as the prediction target, and use BiLSTM for training, as shown in Fig. 12. Fig. 12. Rolling Window\nThe real-time factor data of out-of-sample data for one month is passed through the trained BiLSTM model to obtain the real-time expected return rate of each stock for the next month. The return rate is shown in Table 4.11. 4.5.3 Partial Stock Predicted Return Rate Table sec_code trade_date y_hat 000001.SZ 2011/5/26 0.0424621 000002.SZ 2011/5/26 -0.1632174 000004.SZ 2011/5/26 -0.0642319 000005.SZ 2011/5/26 0.08154649 000006.SZ 2011/5/26 0.00093213 000007.SZ 2011/5/26 -0.073218 000008.SZ 2011/5/26 -0.0464256 000009.SZ 2011/5/26 -0.091549 000010.SZ 2011/5/26 0.08154649 000011.SZ 2011/5/26 -0.1219943 000012.SZ 2011/5/26 -0.1448984 000014.SZ 2011/5/26 0.09038845 000016.SZ 2011/5/26 -0.11225 4.5.4 Code Implementation Snippet The following is a code snippet used in the training process for building the BiLSTM training network.\nbuild_net_blstm def build_net_blstm(self): model = ks.Sequential() model.add( ks.layers.Bidirectional(ks.layers.LSTM( 50 ),input_shape=(11,10)) ) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(256)) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(64)) model.add(ks.layers.Dense(1)) model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;mse\u0026#39;) model.summary() self.set_model(model) 4.6 Quantitative Strategy and Strategy Backtesting 4.6.1 Backtesting Metrics First, let\u0026rsquo;s introduce some common backtesting metrics for strategies. Evaluation metrics include Total Rate of Return, Annualized Rate of Return, Annualized volatility, Sharpe ratio, Maximum Drawdown (MDD), Annualized turnover rate, and Annualized transaction cost rate. It is assumed that the stock market is open for 252 days a year, the risk-free rate is defaulted to 0.035, and the commission fee is defaulted to 0.002.\nTotal Rate of Return: Under the same other indicators, the larger the cumulative rate of return, the better the strategy, and the more it can bring greater returns. The formula is as follows: $$ \\text{Total Rate of Return} = r_{p} = \\frac{P_{1} - P_{0}}{P_{0}} $$$P_{1}$: Total value of final stocks and cash\n$P_{0}$: Total value of initial stocks and cash\nAnnualized Rate of Return: It is to convert the cumulative total rate of return into a geometric average rate of return on an annual basis. Under the same other indicators, the larger the annualized rate of return, the better the strategy. The formula is as follows: $$ \\text{Annualized Rate of Return} = R_{p} = \\left(1 + r_{p}\\right)^{\\frac{252}{t}} - 1 $$$r_{p}$: Cumulative rate of return\n$t$: Number of days the investment strategy is executed\nAnnualized volatility: Defined as the standard deviation of the logarithmic value of the annual return rate of the object asset. Annualized volatility is used to measure the risk of a strategy. The greater the volatility, the higher the risk of the strategy. The formula is as follows: $$ \\begin{aligned} \\text{Annualized volatility} = \\sigma_{p} \u0026= \\sqrt{\\frac{252}{t-1} \\sum_{i=1}^{t}\\left(r_{d} - \\bar{r}_{d}\\right)^{2}} \\\\ \\bar{r}_{d} \u0026= \\frac{1}{t} \\sum_{i=1}^{t} r_{d_{i}} \\end{aligned} $$$r_{d_{i}}$: Daily return rate on the $i$-th day\n$\\bar{r}_{d}$: Average daily return rate\n$t$: Number of days the investment strategy is executed\nSharpe ratio: Proposed by Sharpe (1966)$^{[24]}$. It represents the excess return obtained by investors for bearing an extra unit of risk$^{[25]}$. Here is the calculation formula for the annualized Sharpe ratio: $$ S = \\frac{R_{p} - R_{f}}{\\sigma_{p}} $$$R_{p}$: Annualized rate of return\n$R_{f}$: Risk-free rate of return\n$\\sigma_{p}$: Annualized volatility\nMaximum Drawdown (MDD): Indicates the maximum value of the return rate drawdown when the total value of stocks and cash of our strategy portfolio reaches the lowest point during the operation period. Maximum drawdown is used to measure the most extreme possible loss situation of the strategy. $$ MDD = \\frac{\\max \\left(V_{x} - V_{y}\\right)}{V_{x}} $$$V_{x}$ and $V_{y}$ are the total value of stocks and cash of the strategy portfolio on day $x$ and day $y$ respectively, and $x \u0026lt; y$.\nAnnualized turnover rate: Used to measure the frequency of buying and selling stocks in the investment portfolio. The larger the value, the more frequent the portfolio turnover and the greater the transaction cost. $$ \\text{change} = \\frac{N \\times 252}{t} $$$t$: Number of days the investment strategy is executed\n$N$: Total number of buy and sell transactions\nAnnualized transaction cost rate: Used to measure the transaction cost of the investment portfolio strategy. The larger the value, the higher the transaction cost. $$ c = \\left(1 + \\text{commison}\\right)^{\\text{change}} - 1 $$change: Annualized turnover rate\ncommison: Commission fee\n4.6.2 Strategy and Backtesting Results The quantitative trading strategy in this paper adopts position switching every month (i.e., the rebalancing period is 28 trading days). Each time, the strategy adopts an equal-weight stock holding method to buy the 25 stocks with the highest expected return rate predicted by BiLSTM and sell the originally held stocks. The backtesting time and rules in this paper are as follows:\nBacktesting Time: From January 2012 to October 2020. Backtesting Stock Pool: All A-shares, excluding Special Treatment (ST) stocks. Transaction Fee: A brokerage commission of 0.2% is paid when buying, and a brokerage commission of 0.2% is paid when selling. If the commission for a single transaction is less than 5 CNY, the brokerage charges 5 CNY. Buying and Selling Rules: Stocks that hit the upper limit on the opening day cannot be bought, and stocks that hit the lower limit cannot be sold. 4.6.2.1 Strategy Backtesting Results Cumulative Return Annualized Return Annualized Volatility Sharpe Ratio Max Drawdown Annualized Turnover Rate Annualized Transaction Cost Rate Strategy 701.00% 29.18% 33.44% 0.77 51.10% 51.10% 11.35% Benchmark 110.40% 9.70% 26.01% 0.24 58.49% 58.49% 0.00% Fig. 22. Net Profit Curve\nThe backtesting results are shown in the table and Fig. 22 above. My strategy adopts the LightGBM-BiLSTM quantitative strategy introduced in this chapter. The benchmark uses the CSI All Share (000985). From the results above, it can be seen that the cumulative return of this strategy is 701.00%, which is much higher than the benchmark\u0026rsquo;s 110.40%; the annualized return is 29.18%, which is much higher than the benchmark\u0026rsquo;s 9.70%; and the Sharpe ratio is 0.77, which is higher than the benchmark\u0026rsquo;s 0.24. These three backtesting indicators show that the LightGBM-BiLSTM quantitative strategy can indeed bring greater returns to investors. The annualized volatility of this strategy is 33.44%, which is greater than the benchmark\u0026rsquo;s 26.01%, and the maximum drawdown is 51.10%, which is less than the benchmark\u0026rsquo;s 58.49%. These two backtesting indicators show that the LightGBM-BiLSTM quantitative strategy has certain risks, especially it is difficult to resist the impact of systemic risks. The annualized turnover rate is 11.35%, and the annualized transaction cost rate is 2.29%, indicating that our strategy is not a high-frequency trading strategy and the transaction cost is small. It can be seen from the return curve chart that the return rate of the LightGBM-BiLSTM quantitative strategy in the first two years is not much different from the benchmark, and there is no special advantage. However, from around April 2015, the return rate of the LightGBM-BiLSTM quantitative strategy is significantly better than the benchmark\u0026rsquo;s return rate. Overall, the return rate of this LightGBM-BiLSTM quantitative strategy is very considerable, but there are still certain risks.\nChapter 5 Conclusion and Future Directions 5.1 Conclusion This paper first introduced the research background and significance of stock price prediction and quantitative strategy research based on deep learning, and then introduced the domestic and international research status of stock price prediction and quantitative investment strategies respectively. Then, the innovations and research framework of this paper were explained. Then, in the chapter on related theoretical foundations, this paper briefly introduced the deep learning models and the development history of quantitative investment used in this paper. The basic structure, basic principles, and characteristics of the three models LSTM, GRU, and BiLSTM are mainly introduced.\nSubsequently, this paper used the daily frequency data of SPD Bank and IBM, and preprocessed the data through a series of data processing processes and feature extraction. Then, the specific network structure and hyperparameter settings of the three models LSTM, GRU, and BiLSTM were introduced. Then, we used LSTM, GRU, and BiLSTM to predict the closing prices of the two stocks and compare the model evaluations. The experimental results show that for both stocks, the BiLSTM prediction effect is more accurate.\nFinally, in order to further illustrate the application value of BiLSTM in finance, this paper constructed a quantitative investment model based on LightGBM-BiLSTM. Stocks in the entire A-share market and multiple factors were selected for factor cleaning, factor selection based on LightGBM, and factor combination based on LSTM. Then, we constructed a certain investment strategy and compared it with the benchmark holding CSI All Share through evaluation indicators such as cumulative return rate, annualized return rate, annualized volatility, and Sharpe ratio. Through comparison, it was found that the LightGBM-BiLSTM quantitative investment model can bring better returns, indicating the effectiveness of using deep learning to build quantitative investment strategies.\n5.2 Future Directions Although this paper compares the effects of LSTM, GRU, and BiLSTM models in predicting stock closing prices and achieves certain results based on the LightGBM-BiLSTM quantitative investment strategy, there are still some shortcomings in this paper\u0026rsquo;s research. Combining the research results of this paper, the following research and improvements can be further carried out:\nDiversification of Prediction Targets: In terms of predicting stock prices, this paper selects the stock closing price as the prediction target. Although this result is the most intuitive, the Random Walk Hypothesis (RWH) proposed by Bachelier (1900)$^{[26]}$ believes that stock prices follow a random walk and are unpredictable. Although many behavioral economists have since proved that this view is not entirely correct, it also shows that simply predicting stock closing prices is not so strong in terms of difficulty and interpretability $^{[27][28]}$. Therefore, stock volatility prediction, stock price increase/decrease judgment, and stock return rate prediction can be selected as future research directions. Diversified Model Comparison: In terms of predicting stock prices, this paper compares the three recurrent neural network models LSTM, GRU, and BiLSTM and shows that BiLSTM has better prediction effect, but there is still a lack of comparative research with more different models. Therefore, future in-depth research can be conducted on comparisons with Autoregressive Integrated Moving Average (ARIMA), Convolutional Neural Networks (CNNs), Deep Neural Networks (DNNs), CNN-LSTM, Transformer, TimeGPT, and other single or composite models. Factor Diversification: The factors used in this paper to construct quantitative investment strategies are all technical price-volume factors, and the types of factors are single. In the future, different types of factors such as financial factors, sentiment factors, and growth factors can be selected to improve the performance of the strategy. At the same time, future research can also appropriately add timing strategies to increase positions when predicting that the market will rise and reduce positions when predicting that the market will fall to earn beta (\\(\\beta\\)) returns. Investment Portfolio Optimization: The factor combination process in this paper is still imperfect. In the future, quadratic programming methods can be used to optimize the investment portfolio. High-Frequency Trading Strategy Research: The quantitative investment strategy method in this paper adopts a low-frequency trading strategy. In the future, stock tick data can be used to study high-frequency strategies and ultra-high-frequency strategies. References [1] White, H. “Economic prediction using neural networks: The case of IBM daily stock returns.” Proc. of ICNN. 1988, 2: 451-458.\n[2] Kimoto, T., Asakawa, K., Yoda, M., et al. “Stock market prediction system with modular neural networks.” Proc. of 1990 IJCNN International Joint Conference on Neural Networks. IEEE, 1990: 1-6.\n[3] Zhang, G. P. “Time series forecasting using a hybrid ARIMA and neural network model.” Neurocomputing. 2003, 50: 159-175.\n[4] Akita, R., Yoshihara, A., Matsubara, T., et al. “Deep learning for stock prediction using numerical and textual information.” Proc. of 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS). IEEE, 2016: 1-6.\n[5] 宮崎邦洋, 松尾豊. “Deep Learning を用いた株価予測の分析.” 人工知能学会全国大会論文集 第31回全国大会. 一般社団法人 人工知能学会, 2017: 2D3OS19a3-2D3OS19a3.\n[6] Kim, T., Kim, H. Y. “Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data.” PLoS ONE. 2019, 14(2): e0212320.\n[7] Hochreiter, S., Schmidhuber, J. “Long short-term memory.” Neural Computation. 1997, 9(8): 1735-1780.\n[8] Cho, K., Van Merriënboer, B., Gulcehre, C., et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078. 2014.\n[9] Chung, J., Gulcehre, C., Cho, K. H., et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” arXiv preprint arXiv:1412.3555. 2014.\n[10] Gruber, N., Jockisch, A. “Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?” Frontiers in Artificial Intelligence. 2020, 3(40): 1-6.\n[11] Markowitz, H. “Portfolio Selection.” The Journal of Finance. 1952, 7(1): 77-91. doi:10.2307/2975974.\n[12] Merton, R. C. “An analytic derivation of the efficient portfolio frontier.” Journal of Financial and Quantitative Analysis. 1972: 1851-1872.\n[13] Sharpe, W. F. “Capital asset prices: A theory of market equilibrium under conditions of risk.” The Journal of Finance. 1964, 19(3): 425-442.\n[14] Lintner, J. “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.” Review of Economics and Statistics. 1965, 47(1): 13-37.\n[15] Mossin, J. “Equilibrium in a capital asset market.” Econometrica: Journal of the Econometric Society. 1966: 768-783.\n[16] Ross, S. A. “The arbitrage theory of capital asset pricing.” Journal of Economic Theory. 1976, 13(3): 341-60.\n[17] Fama, E. F., French, K. R. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics. 1993, 33(1): 3-56.\n[18] Fama, E. F., French, K. R. “A five-factor asset pricing model.” Journal of Financial Economics. 2015, 116(1): 1-22.\n[19] Kingma, D. P., Ba, J. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980. 2014.\n[20] Friedman, J. H. “Greedy function approximation: A gradient boosting machine.” Annals of Statistics. 2001: 1189-1232.\n[21] Kopitar, L., Kocbek, P., Cilar, L., et al. “Early detection of type 2 diabetes mellitus using machine learning-based prediction models.” Scientific Reports. 2020, 10(1): 1-12.\n[22] Ke, G., Meng, Q., Finley, T., et al. “Lightgbm: A highly efficient gradient boosting decision tree.” Advances in Neural Information Processing Systems. 2017, 30: 3146-3154.\n[23] Bottou, L., Curtis, F. E., Nocedal, J. “Optimization methods for large-scale machine learning.” SIAM Review. 2018, 60(2): 223-311.\n[24] Sharpe, W. F. “Mutual fund performance.” The Journal of Business. 1966, 39(1): 119-138.\n[25] Sharpe, W. F. “The sharpe ratio.” Journal of Portfolio Management. 1994, 21(1): 49-58.\n[26] Bachelier, L. “Théorie de la spéculation.” Annales Scientifiques de l\u0026rsquo;École Normale Supérieure. 1900, 17: 21-86.\n[27] Fromlet, H. “Behavioral finance-theory and practical application: Systematic analysis of departures from the homo oeconomicus paradigm are essential for realistic financial research and analysis.” Business Economics. 2001: 63-69.\n[28] Lo, A. W. “The adaptive markets hypothesis.” The Journal of Portfolio Management. 2004, 30(5): 15-29.\nReference Blog Colah\u0026rsquo;s Blog. (2015, August 27). Understanding LSTM Networks. Citation Citation: For reprint or citation of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Apr 2021). Stock Price Prediction and Quantitative Strategy Based on Deep Learning. https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\nOr\n@article{syhya2021stockprediction, title = \u0026#34;Stock Price Prediction and Quantitative Strategy Based on Deep Learning\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2021\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/","summary":"\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eThe stock market is a crucial component of the financial market. In recent years, with its vigorous development, research on stock price prediction and quantitative investment strategies has attracted scholars from various fields.  With the advancement of Artificial Intelligence (AI) and Machine Learning (ML) in recent years, researchers have shifted from traditional statistical models to AI algorithms. Particularly after the deep learning boom, neural networks have achieved remarkable results in stock price prediction and quantitative investment strategy research. The objective of deep learning is to learn multi-level features, constructing abstract high-level features by combining low-level ones, thereby mining the distributed feature representations of data. This approach enables complex nonlinear modeling to accomplish prediction tasks. Recurrent Neural Networks (RNNs) have been widely applied to sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, leading many researchers to use RNNs for stock price prediction. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The advent of Long Short-Term Memory (LSTM) networks addressed this problem, followed by variants such as Gated Recurrent Units (GRUs), Peephole LSTMs, and Bidirectional LSTMs (BiLSTMs). Traditional stock prediction models often overlook temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully leverages the contextual relationships in both forward and backward temporal directions of time series data. It also avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies.\u003c/p\u003e","title":"Stock Price Prediction and Quantitative Strategy Based on Deep Learning"}]