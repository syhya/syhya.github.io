[{"content":"Background Recently, the number of parameters in large models has been continuously increasing, from the initial billions to today\u0026rsquo;s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models.\nTraining Challenges of Large Models Explosive Growth in Parameter Scale With the continuous pursuit of model capacity and performance, the number of parameters in neural networks is growing exponentially. Today, models ranging from millions to billions, hundreds of billions, and even trillions of parameters are emerging. For example, Llama 3.1 405B has approximately 405 billion parameters, while it is rumored that GPT-4 may have as many as 1.7 trillion parameters. This massive parameter scale has led to a sharp increase in computing and memory demands, bringing unprecedented pressure to the training process.\nSoaring Computational Complexity The rapid increase in the number of parameters directly leads to a significant increase in overall computational complexity. Training a large model once may take weeks or even months. Even with large-scale high-performance GPU clusters, the training cycle is still unsatisfactory, severely restricting model iteration speed and research efficiency.\nIncreasingly Prominent Memory Bottleneck In addition to storing massive model parameters, large models must also save intermediate activations, gradient information, and optimizer states during training. This data poses a huge challenge to GPU memory. Even with high-end GPUs equipped with A100, H100 (80GB memory), H200 (141GB memory), or GB200 (384GB memory), single-card memory is often insufficient to meet the needs of models with hundreds of billions or even trillions of parameters, leading to frequent \u0026ldquo;Out of Memory (OOM)\u0026rdquo; errors.\nCommunication Overhead Becomes a Bottleneck In multi-GPU distributed training environments, inter-node communication is frequently required for data synchronization (such as gradient aggregation). As the model size and the number of GPUs increase, this communication volume rises sharply. Even in high-bandwidth networks, All-Reduce operations to transmit massive amounts of data consume a significant amount of time, becoming one of the main bottlenecks of overall parallel efficiency.\nTraining Stability Challenges Ultra-large-scale models are more prone to gradient vanishing or gradient explosion problems during training, leading to unstable training processes and difficulty in convergence. Although mixed-precision training can accelerate training and reduce memory footprint to some extent, it may also introduce new numerical stability issues, requiring researchers to invest more effort in detailed tuning.\nNecessity of Distributed Training Faced with the above challenges, distributed training technology has become a key solution to support the training of large models. By splitting training tasks and distributing them to multiple GPUs or computing nodes, distributed training can fully utilize parallel computing and cluster memory resources, thereby breaking through the limitations of a single GPU. The main advantages are reflected in the following aspects:\nBreaking Through the Computing Power Limit of a Single GPU The computing power of a single GPU is ultimately limited and cannot cope with the massive computing demands of trillion-parameter models. With data parallelism and model parallelism techniques, training tasks can be evenly distributed to multiple GPUs, thereby significantly shortening the overall training time.\nOvercoming the Memory Bottleneck of a Single GPU By distributing model parameters, intermediate activations, and optimizer states across the memory of multiple GPUs, distributed training effectively expands the available memory capacity. Typical technologies such as ZeRO, through sharding data processing, make the training of ultra-large-scale models possible.\nAccelerating Model Iteration and R\u0026amp;D Cycle The high parallelism of distributed training makes it possible to complete training tasks that originally required weeks or even months in just a few days, thereby greatly improving the model iteration speed and enabling new architectures and strategies to be verified and applied more quickly.\nSupporting Exploration of Larger-Scale Models Distributed training provides a solid foundation for exploring larger-scale and more complex neural network architectures. It is with this technical support that trillion-parameter models (such as Switch Transformer) can be successfully trained and put into practical applications.\nImproving the Robustness and Scalability of Training Systems Distributed systems have excellent fault tolerance. When a GPU node fails, other nodes can quickly take over the task, ensuring that the training process is not interrupted. At the same time, the cluster size can be flexibly expanded or reduced according to specific needs, meeting the training requirements of different scale models.\nParallel Training The following figure intuitively shows the differences between various parallel training strategies. Different colors represent different model layers (e.g., three layers), and dashed lines distinguish different GPUs. From left to right are data parallelism, model parallelism (including pipeline parallelism and tensor parallelism), and expert parallelism (MoE).\nFig. 1. An illustration of various parallelism strategies on a three-layer model. Each color refers to one layer and dashed lines separate different GPUs. (Image source: OpenAI Blog, 2022)\nData Parallelism The complete model is copied to each GPU, and the dataset is divided into different batches and distributed to each GPU for parallel computation. Finally, the gradients of all GPUs are aggregated during parameter updates.\nModel Parallelism The model is divided and distributed across different GPUs, with each GPU responsible for computing only a part of the model. It can be further divided into the following two categories:\nPipeline Parallelism: The model is split layer-wise (vertically), with different GPUs responsible for different layers. Micro-batches are passed through the pipeline to execute forward and backward computations in parallel. Tensor Parallelism: Large-scale tensor operations (such as large matrix multiplications) within a layer are split horizontally. Each GPU performs part of the computation in parallel and aggregates the results when necessary. Expert Parallelism Through a gating strategy, each input sample only passes through a subset of experts (sub-networks), thus distributing the entire model across different GPUs by \u0026ldquo;expert modules\u0026rdquo;. Commonly used in Mixture-of-Experts (MOE) structures, it can achieve ultra-large parameter scales but only activate a portion of experts during inference/training.\nBelow, I will elaborate on various parallel methods.\nData Parallelism Fig. 2. Data Parallelism. (Image source: Clolossal-AI Documentation)\nIn deep learning training, Data Parallelism (DP) is the most commonly used parallel strategy. Its core idea is:\nReplicate Model Parameters: Place a complete copy of the model parameters on each computing device (usually a GPU). Partition Training Data: Divide the large-scale dataset into multiple subsets along the sample dimension. Different subsets are assigned to different GPUs for processing. Local Forward and Backward Propagation: Each GPU independently computes the loss and corresponding local gradients. Gradient/Parameter Synchronization: Aggregate the gradients from each GPU and update the model parameters, ensuring that the model replicas on all GPUs remain consistent after each iteration. The following shows the data parallelism workflow:\nDataset Partitioning Divide the training dataset $D$ into $N$ non-overlapping subsets ${D_1, D_2, \\dots, D_N}$, where $N$ is the number of GPUs. Usually, it is ensured that the size of each subset is similar to achieve load balancing.\nModel Replication Replicate a complete copy of the model parameters $\\theta$ on each GPU. At the beginning of training, these parameters are the same on each GPU.\nData Distribution Distribute subset $D_i$ to the $i$-th GPU, allowing it to be stored locally and used for subsequent calculations.\nLocal Forward Propagation Each GPU performs forward propagation based on its local data subset $D_i$ to obtain the local loss $L_i(\\theta, D_i)$.\nLocal Backward Propagation Each GPU performs backward propagation based on the local loss $L_i$ to calculate the local gradient\n$$ g_i = \\nabla_{\\theta} L_i(\\theta, D_i). $$ Gradient Synchronization Gradient synchronization (usually All-Reduce) is performed between GPUs to aggregate all local gradients ${g_1, g_2, \\ldots, g_N}$ to obtain the global average gradient\n$$ \\bar{g} = \\frac{1}{N} \\sum_{i=1}^{N} g_i. $$ Parameter Update Each GPU uses the global average gradient $\\bar{g}$ to update its local model parameters:\n$$ \\theta \\leftarrow \\theta - \\eta \\cdot \\bar{g}, $$where $\\eta$ is the learning rate.\nIterative Loop Repeat steps 4-7 until the model converges or reaches the preset number of training epochs.\nBulk Synchronous Parallel vs. Asynchronous Parallel In step 6 \u0026ldquo;Gradient Synchronization\u0026rdquo; above, how and when to perform \u0026ldquo;synchronization\u0026rdquo; is one of the important factors affecting the performance and convergence behavior of data parallelism. It is generally divided into the following two categories:\nBulk Synchronous Parallel (BSP) is the most common and easiest to understand synchronization mode in data parallelism. Its characteristics can be summarized as \u0026ldquo;globally synchronizing gradients and updating parameters once after each mini-batch iteration\u0026rdquo;. The specific process is:\nLocal Computation: Each GPU performs forward and backward propagation based on its data subset $D_i$ to obtain the local gradient $g_i$. Global Communication: All GPUs synchronize (e.g., through All-Reduce) to calculate $\\bar{g}$. Parameter Update: Each node uses $\\bar{g}$ to update its local parameter replica $\\theta$. Wait and Next Iteration: All nodes complete the above operations before entering the next iteration. Asynchronous Parallel (ASP) aims to get rid of the global synchronization point of BSP and allow each node to perform calculations and parameter updates independently. Its typical implementation is the asynchronous push-pull process under the \u0026ldquo;Parameter Server (PS)\u0026rdquo; architecture:\nEach node calculates the gradient $g_i$ locally, and then pushes it to the parameter server; Once the parameter server receives the gradient, it immediately updates the global model parameters; Other nodes will pull down the latest parameters when they need them to continue the next step of calculation. BSP vs. ASP The following table summarizes the main differences between synchronous and asynchronous parallelism in a data parallel environment:\nComparison Dimension Synchronous Parallel (BSP) Asynchronous Parallel (ASP) Parameter Update Timing Global synchronization once per mini-batch or after a certain number of iterations Each node updates parameters independently, without needing to keep the same timestep as others Convergence Stability High. The gradients used are the latest, the convergence path is controllable and easy to analyze Lower. Stale gradients exist, convergence rate and stability may be affected Communication Requirements Highly dependent on All-Reduce, all nodes need to wait and exchange data during synchronization Each node asynchronously pushes/pulls to the parameter server, communication is more flexible, but the parameter server may become a bottleneck Hardware Resource Utilization If there are slow nodes or network delays, other nodes need to wait, and resource utilization may be reduced No need to wait for slow nodes, computing resources can be used efficiently Implementation Complexity Relatively low, mainstream frameworks (PyTorch DDP, Horovod, etc.) have built-in support Relatively higher, parameter server and other components are required, more synchronization logic and data consistency need to be handled Applicable Scenarios Homogeneous hardware, good network bandwidth, pursuit of higher convergence quality Heterogeneous hardware, unstable or low bandwidth network, need for extremely high throughput and tolerance for certain convergence risks Typical Implementations PyTorch DDP, TensorFlow MirroredStrategy Parameter Server architecture (MXNet, TensorFlow ParameterServer mode, etc.) Recommendation: In actual projects, start with simple synchronous parallelism (BSP), and use PyTorch DDP or similar tools for multi-GPU training. If the network environment is heterogeneous, there are many nodes, or the task requires extremely high throughput, you can try asynchronous parallelism (ASP) or parameter server solutions, and cooperate with Gradient Accumulation to balance bandwidth and update frequency.\nGradient Accumulation When the batch size is large or communication becomes the main bottleneck, Gradient Accumulation can be used to reduce the synchronization frequency. Its core idea is:\nContinuously calculate the local gradients of multiple mini-batches and accumulate them in the local accumulation buffer; When the number of accumulated mini-batches reaches $K$, trigger a global gradient synchronization and parameter update. Let $g_j$ be the gradient of the $j$-th mini-batch, then in an \u0026ldquo;accumulation cycle\u0026rdquo;, we get\n$$ G = \\sum_{j=1}^{K} g_j. $$Then update with learning rate $\\eta$:\n$$ \\theta \\leftarrow \\theta - \\eta \\cdot G. $$Since gradient synchronization is no longer performed for each mini-batch, but once every $K$ accumulated mini-batches, the communication overhead can be significantly reduced. However, the reduced parameter update frequency may also slow down the training convergence speed, and a trade-off between throughput and convergence performance is needed.\nDistributed Data Parallel Distributed Data Parallel (DDP) is a highly optimized implementation of BSP in PyTorch v1.5 (Li et al. 2020), which facilitates data parallelism for single-machine multi-GPU and even multi-machine multi-GPU. Its main optimizations include:\nGradient Bucketing: Divide model parameters into multiple \u0026ldquo;buckets\u0026rdquo;; when backpropagation is performed, once all gradients in a bucket are calculated, an All-Reduce for that bucket is immediately initiated, instead of waiting for all gradients to be calculated before synchronizing at once. Communication and Computation Overlap: DDP uses asynchronous communication and non-blocking operations to overlap gradient synchronization (communication) with forward propagation and backward propagation (computation) as much as possible, thereby reducing communication overhead. This overlap strategy improves overall parallel efficiency. Gradient Accumulation: DDP can also be easily combined with gradient accumulation. Combined use, by increasing the gradient update interval for each synchronization, reduces the synchronization frequency. In large-scale distributed training, this helps to further reduce communication overhead and improve training efficiency. Fig. 3. Pseudo code for Pytorch DDP. (Image source: Li et al. 2020)\nRing All-Reduce In a multi-GPU (especially single-machine multi-GPU) environment, if there is high-speed interconnect (such as NVLink, PCIe switch, etc.), Ring All-Reduce can be used to significantly reduce communication overhead. The idea is:\nOrganize $k$ nodes into a ring and divide the gradient vector into $k$ parts equally. In the \u0026ldquo;summation phase\u0026rdquo;, each node sends a part of its local gradient to the next node and adds it to the received gradient; after several rounds of this process, each node will hold the complete \u0026ldquo;aggregated\u0026rdquo; gradient. In the \u0026ldquo;broadcast phase\u0026rdquo;, the final result is distributed to all nodes along the ring. Ideally, the communication cost of Ring All-Reduce is approximately independent of the number of nodes (can be regarded as $\\mathcal{O}(1)$), which is very suitable for gradient synchronization in a multi-GPU environment. It is a core communication mode widely used in libraries such as Horovod and NCCL.\nParameter Server When the cluster scale expands to multi-machine multi-GPU, simple single-point aggregation (such as a central server) is often difficult to support parallel training of massive data. Parameter Server (PS) (Li, et al., 2014) is a typical architecture designed for scalable distributed training:\nParameter Sharding: Split model parameters in the form of key-value pairs. Different PS nodes only manage parameters of specific shards. push-pull Semantics: After the computing node obtains the gradient locally, it pushes it to the corresponding PS; after the PS updates the parameters of the shard, the computing node can pull down the latest version when needed for the next step of calculation. Flexible Fault Tolerance and Expansion: By adding or removing PS nodes, capacity can be flexibly expanded in terms of bandwidth or computing needs; backup and fault tolerance strategies can also be implemented on PS. This PS + Worker mode can combine data parallelism and model parallelism simultaneously, splitting ultra-large models and storing them on multiple PSs, and performing distributed training on ultra-large data. PS itself can also be split and merged according to the load situation to form a more complex hierarchical topology.\nModel Parallelism Model Parallelism (MP) is a parallel method that splits the model itself across multiple computing devices (GPUs) for training. When the model parameter size exceeds the memory capacity of a single GPU, model parallelism becomes a necessary choice. Model parallelism is mainly divided into two types: Pipeline Parallelism and Tensor Parallelism.\nNaive Model Parallelism and Bubble Problem\nFig. 4. A naive model parallelism setup where the model is vertically split into 4 partitions. Data is processed by one worker at a time due to sequential dependency, leading to large “bubbles” of idle time. (Image source: Huang et al. 2018)\nNaive model parallelism implementation, which simply divides the model layer by layer and executes it sequentially on different GPUs, will encounter a serious \u0026ldquo;bubble\u0026rdquo; problem. Due to the dependencies between layers, when one GPU is processing a certain stage of a data sample, other GPUs may be idle, waiting for the output of the previous GPU or the input of the next GPU. This GPU idle time is called \u0026ldquo;bubble\u0026rdquo;, which seriously reduces the efficiency of pipeline parallelism.\nWhere $F_i$ represents the forward propagation of Stage $i$, and $B_i$ represents the backward propagation of Stage $i$. It can be seen that in naive pipeline parallelism, only one GPU is working most of the time, and other GPUs are idle, resulting in low efficiency.\nReasons for the bubble problem:\nInter-layer dependency: There is a sequential dependency between the layers of the neural network. The calculation of the next layer must depend on the output of the previous layer. Sequential execution: Naive model parallelism executes layer by layer in order, which prevents GPUs from working in full parallelism. Pipeline Parallelism Fig. 5. Pipeline Parallelism. (Image source: Clolossal-AI Documentation)\nPipeline Parallelism (PP) divides the model layer by layer into multiple stages, and each stage is assigned to a GPU. Data is passed between different GPUs like a pipeline. The output of the previous GPU serves as the input of the next GPU. Pipeline parallelism aims to improve the efficiency of model parallel training and reduce GPU idle time.\nGPipe GPipe (Huang et al. 2018) is an efficient pipeline parallel training system proposed by Google, which aims to solve the bubble problem of naive pipeline parallelism. The core idea of GPipe is to divide a mini-batch into multiple micro-batches and use synchronous gradient aggregation to alleviate the bubble problem and improve pipeline efficiency.\nFig. 6. Illustration of pipeline parallelism in GPipe with 4 microbatches and 4 partitions. GPipe aggregates and updates gradients across devices synchronously at the end of every batch. (Image source: Huang et al. 2018)\nThe following is the GPipe scheduling strategy:\nMicro-batch Partitioning: Divide a mini-batch into $m$ micro-batches. The size of each micro-batch after partitioning is $1/m$ of the original mini-batch. Pipeline Stage Partitioning: Divide the model layer by layer into $d$ stages, and assign each stage to a GPU. Pipeline Execution: Process each micro-batch in sequence, performing forward and backward propagation in the pipeline. The specific process is as follows: Forward Propagation: For each micro-batch, perform forward propagation sequentially on Stage 1, Stage 2, \u0026hellip;, Stage $d$. The output of Stage $i$ serves as the input of Stage $i+1$. Backward Propagation: When the forward propagation of all micro-batches is completed, backward propagation begins. For each micro-batch, perform backward propagation sequentially on Stage $d$, Stage $d-1$, \u0026hellip;, Stage $1$. The gradient of Stage $i$ serves as the input of Stage $i-1$. Synchronous Gradient Aggregation: After the backward propagation of all micro-batches is completed, aggregate the gradients of all micro-batches (e.g., averaging) to obtain the global average gradient. Parameter Update: Each GPU uses the global average gradient to update its local model parameters. GPipe Bubble Ratio Formula Assuming that the forward and backward propagation time of each micro-batch is 1 unit, the pipeline depth is $d$, and the number of micro-batches is $m$, the bubble ratio of GPipe is:\n$$ \\text{Bubble Ratio} = 1 - \\frac{2md}{(2m + 2(d-1))d} = \\frac{d-1}{m+d-1} $$When the number of micro-batches $m$ is much larger than the pipeline depth $d$ ($m \\gg d$), the bubble ratio approaches 0, and the pipeline efficiency is close to linear acceleration. The GPipe paper points out that when $m \u0026gt; 4d$, the bubble overhead can be almost ignored (in the case of activation recomputation). Therefore, there are the following benefits:\nReduce Bubbles: GPipe significantly reduces the bubble problem of naive pipeline parallelism through micro-batch partitioning and pipeline scheduling, improving GPU utilization and training efficiency. Synchronous Gradient Aggregation: GPipe adopts synchronous gradient aggregation, which ensures the synchronicity of the training process and good model convergence. Linear Acceleration Potential: When the number of micro-batches is large enough, GPipe can achieve near-linear acceleration. PipeDream Fig. 7. Illustration of 1F1B microbatch scheduling in PipeDream. (Image source: Harlap et al. 2018)\nPipeDream (Harlap et al. 2018) is another efficient pipeline parallel training system. It adopts the 1F1B (1-Forward-1-Backward) scheduling strategy and introduces Weight Stashing technology to further reduce bubbles, improve pipeline efficiency, and solve the weight version inconsistency problem that may be caused by 1F1B scheduling.\nThe core idea of PipeDream\u0026rsquo;s 1F1B scheduling strategy is that each GPU (Stage) alternately performs forward propagation and backward propagation, working in parallel as much as possible to reduce GPU idle time. The specific process is as follows:\nMicro-batch Partitioning: Divide a mini-batch into $m$ micro-batches. Pipeline Stage Partitioning: Divide the model layer by layer into $d$ stages, and assign each stage to a GPU. 1F1B Scheduling Execution: Each GPU takes turns to perform forward propagation and backward propagation. Weight Stashing Since forward propagation and backward propagation may use different versions of model weights in 1F1B scheduling, it will cause weight version inconsistency problems, affecting the correctness and convergence of training. PipeDream introduces Weight Stashing technology to solve this problem. The core idea of weight stashing is that each GPU maintains multiple versions of model weights and ensures that forward propagation and backward propagation use the same version of weights.\nWeight Stashing Implementation:\nVersion Management: Each GPU maintains a weight version queue to store multiple versions of model weights. Version Selection: When performing forward propagation, select the latest weight version. When performing backward propagation, select the same weight version as the corresponding forward propagation. Version Update: After completing backward propagation of all micro-batches in a mini-batch, update the model weights and generate a new weight version. To further optimize the memory usage of PipeDream, especially in terms of weight stashing, PipeDream has derived two memory optimization variants: PipeDream-flush and PipeDream-2BW.\nPipeDream-flush Fig. 8. Illustration of pipeline scheduling in PipeDream-flush. (Image source: Narayanan et al. 2020)\nPipeDream-flush periodically performs global synchronous pipeline flushing on the basis of PipeDream, similar to GPipe\u0026rsquo;s synchronous gradient aggregation. By periodically flushing, PipeDream-flush can greatly reduce the memory space required for weight stashing, only needing to maintain a single version of model weights, but it will sacrifice a small amount of throughput.\nPipeDream-2BW PipeDream-2BW (Double-Buffered Weights) maintains two versions of model weights, namely \u0026ldquo;double-buffered weights\u0026rdquo;. It updates the model version every $k$ micro-batches, where $k$ is greater than the pipeline depth $d$ ($k \u0026gt; d$). The newly updated model version does not immediately completely replace the old version, because there may still be some remaining backward propagation operations that depend on the old version. With double-buffered weights, PipeDream-2BW can reduce the memory overhead of weight stashing to only maintaining two versions of model weights, significantly reducing memory footprint.\nFig. 9. Illustration of pipeline scheduling in PipeDream-2BW. (Image source: Narayanan et al. 2020)\nThe PipeDream-2BW strategy has the following advantages:\nLower Bubble Overhead: The 1F1B scheduling strategy can further reduce bubbles compared to GPipe, improving GPU utilization and training efficiency. Weight Stashing Solves Version Consistency: Weight stashing technology ensures that forward propagation and backward propagation use the same version of weights, solving the weight version inconsistency problem that may be caused by 1F1B scheduling. Memory Optimization Variants: PipeDream-flush and PipeDream-2BW further optimize memory usage, reduce the memory overhead of weight stashing, and make pipeline parallelism more suitable for memory-constrained scenarios. Tensor Parallelism Tensor Parallelism (TP) is a parallel method that splits tensors in the model (usually weight matrices) along specific dimensions and distributes the split shards to different GPUs for computation. Tensor parallelism has the following advantages:\nBreaking Through Single GPU Memory Limits: Tensor parallelism can distribute model parameters across multiple GPUs, breaking through the memory capacity limit of a single GPU and supporting the training of larger-scale models. Intra-layer Parallelism: Tensor parallelism can achieve parallelization within model layers, such as parallel computation of matrix multiplication operations, improving computational efficiency. Combination with Data Parallelism and Pipeline Parallelism: Tensor parallelism can be combined with other parallel technologies such as data parallelism and pipeline parallelism to form multi-dimensional parallel strategies, further improving training efficiency and scalability. Megatron-LM Megatron-LM (Shoeybi et al. 2019) is a system proposed by NVIDIA for training ultra-large language models. It adopts tensor parallelism technology to parallelize matrix multiplication operations within Transformer model layers, including matrix multiplications in self-attention and MLP.\nFig. 10. Illustration of tensor parallelism for key transformer components proposed in Megatron-LM. (Image source: Shoeybi et al. 2019)\nThe MLP layer of Transformer usually contains two linear layers. The calculation of the first linear layer can be expressed as $Y = \\text{GeLU}(XA)$, where $X$ is the input matrix, $A$ is the weight matrix, and GeLU is the activation function. Megatron-LM splits the weight matrix $A$ along the column dimension into $P$ shards $[A_1, A_2, \u0026hellip;, A_P]$, where $P$ is the number of GPUs. Each GPU $i$ is responsible for storing and computing the weight shard $A_i$.\nTensor Parallel Computation Process of MLP Layer:\n$$ \\begin{aligned} \\text { Split } A \u0026 =\\left[A_1, A_2\\right] \\\\ Y \u0026 =\\operatorname{GeLU}(X A) \\\\ {\\left[Y_1, Y_2\\right] } \u0026 =\\left[\\operatorname{GeLU}\\left(X A_1\\right), \\operatorname{GeLU}\\left(X A_2\\right)\\right] \\end{aligned} $$ Weight Sharding: Split the weight matrix $A$ along the column dimension into $P$ shards $[A_1, A_2, \u0026hellip;, A_P]$ and assign shard $A_i$ to GPU $i$. Local Matrix Multiplication: Each GPU $i$ uses the input matrix $X$ and weight shard $A_i$ to perform matrix multiplication to obtain the local output $Y_i = \\text{GeLU}(XA_i)$. Global Concatenation (All-Gather): All GPUs use All-Gather operation to concatenate the local outputs ${Y_1, Y_2, \u0026hellip;, Y_P}$ into a complete output matrix $Y = [Y_1, Y_2, \u0026hellip;, Y_P]$. Tensor Parallelism of Self-Attention Layer\nMegatron-LM also performs tensor parallel sharding on the Query ($Q$), Key ($K$), Value ($V$) weight matrices in the Transformer\u0026rsquo;s self-attention layer, and performs corresponding local matrix multiplication and global concatenation operations to achieve tensor parallelism of the self-attention layer. The calculation formula of the self-attention layer is:\n$$ \\text{Attention}(X, Q, K, V) = \\text{softmax}\\left(\\frac{(XQ)(XK)^T}{\\sqrt{d_k}}\\right)XV $$PTD-P PTD-P (Pipeline, Tensor, and Data Parallelism) (Narayanan et al. 2021) is a multi-dimensional parallel strategy that combines pipeline parallelism, tensor parallelism, and data parallelism. PTD-P aims to fully utilize the advantages of various parallel technologies to improve the efficiency and scalability of training ultra-large models.\nFeatures of PTD-P:\nMulti-dimensional Parallel Combination: PTD-P uses pipeline parallelism, tensor parallelism, and data parallelism simultaneously, which can parallelize the training process from multiple dimensions. Interleaved 1F1B Scheduling: PTD-P adopts the interleaved 1F1B scheduling strategy. Unlike traditional pipeline parallelism, it divides the model into multiple discontinuous layer blocks (model chunks) and assigns multiple layer blocks to each GPU. This scheduling strategy can further reduce bubbles and improve pipeline efficiency. Flexible Parallel Configuration: PTD-P allows users to flexibly configure the combination of various parallel technologies according to the model structure and hardware resources. For example, tensor parallelism and data parallelism can be used alone, or pipeline parallelism, tensor parallelism, and data parallelism can be used simultaneously. Traditional pipeline parallelism usually divides the model into continuous layer blocks, and each GPU is responsible for a continuous layer block. PTD-P\u0026rsquo;s interleaved 1F1B scheduling divides the model into multiple discontinuous layer blocks. For example, GPU 1 is responsible for layers 1, 2, 9, 10, GPU 2 is responsible for layers 3, 4, 11, 12, and so on. Each GPU is responsible for multiple discontinuous layer blocks, which can more effectively utilize GPU resources and reduce bubble overhead.\nFig. 11.(Top) Default 1F1B pipeline schedule as in PipeDream-flush.(Bottom) Interleaved 1F1B pipeline schedule. First model chunks are in dark colors and second chunks are in light colors. (Image source: Narayanan et al. 2021)\nMixture-of-Experts Model Mixture-of-Experts (MoE) (Shazeer et al., 2017) is a sparsely activated model that significantly increases the model\u0026rsquo;s parameter size and performance without significantly increasing the computational cost by combining multiple independent \u0026ldquo;expert\u0026rdquo; networks and a gating network. The core idea of MoE is Sparse Activation, that is, for each input sample, only a part of the expert networks are activated, rather than the entire model. This method not only improves computational efficiency but also enhances the model\u0026rsquo;s expressive ability, making it perform well in LLMs.\nMoE\u0026rsquo;s design inspiration comes from Ensemble learning, a technology that decomposes complex tasks into multiple subtasks and completes them collaboratively by different models. In MoE, these \u0026ldquo;subtasks\u0026rdquo; are processed by multiple independent expert networks, and the gating network is responsible for dynamically selecting the most suitable experts based on the characteristics of the input sample. This division of labor and cooperation mechanism is similar to an expert team in human society: experts in different fields provide professional opinions for specific problems, and finally, a comprehensive result is obtained.\nFig. 12. Illustration of a mixture-of-experts(MoE) layer. Only 2 out of experts are selected and activated by the gating network. (Image source: Shazeer et al., 2017)\nCore Components of MoE A typical MoE contains the following components:\nExpert Networks: A set of independent neural networks ${E_1, E_2, \u0026hellip;, E_n}$. Each expert network $E_i$ can be any type of neural network, such as FFN, CNN, RNN, etc. The number of expert networks $n$ can be very large, such as dozens, hundreds, or even thousands. Gating Network: A trainable neural network $G$ used to learn a probability distribution based on the input sample $x$ to determine which experts to activate. The input of the gating network is the input sample $x$, and the output is an $n$-dimensional probability vector $p = G(x) = [p_1, p_2, \u0026hellip;, p_n]$, where $p_i$ represents the probability of activating expert $E_i$. Expert Output Aggregation: According to the output probability distribution of the gating network, the outputs of the activated expert networks are weighted and summed to obtain the final output $y$ of the MoE layer. Noisy Top-k Gating To achieve sparse activation and ensure balanced expert usage, MoE usually adopts Noisy Top-k Gating as the gating mechanism. This method guarantees computational efficiency and avoids uneven expert load through the introduction of noise and top-k selection. The detailed workflow is as follows:\nGating Score Calculation: For an input sample $x$, the gating network first calculates the gating score $H^{(i)}(x)$ for each expert. This score consists of two parts: linear transformation and noise term, as shown in the formula:\n$$ H^{(i)}(x) =(x W_g)^{(i)} + \\epsilon \\cdot \\text{softplus}\\left((x W_{\\text{noise}})^{(i)} \\right), \\quad \\epsilon \\sim \\mathcal{N}(0, 1) $$ Parameter Description: $W_g \\in \\mathbb{R}^{d \\times n}$: Trainable weight matrix of the gating network, where $d$ is the input feature dimension and $n$ is the number of experts. $W_{\\text{noise}} \\in \\mathbb{R}^{d \\times n}$: Weight matrix used to generate noise. $\\epsilon \\sim \\mathcal{N}(0, 1)$: Standard Gaussian noise, increasing gating randomness. $\\text{softplus}(x) = \\log(1 + e^x)$: Smooth activation function to ensure that the noise is non-negative. The introduction of noise avoids the gating network always selecting fixed experts and enhances the robustness and diversity of the model.\nTop-k Selection: After calculating the gating score vector $H(x) = [H^{(1)}(x), H^{(2)}(x), \\dots, H^{(n)}(x)]$, the gating network selects the top $k$ experts with the largest values (usually $k \\ll n$). This step is implemented by the $\\text{topk}(v, k)$ function:\n$$ \\text{topk}^{(i)}(v, k) = \\begin{cases} v^{(i)} \u0026 \\text{if } v^{(i)} \\text{ is in the top } k \\text{ elements of } v \\\\ -\\infty \u0026 \\text{otherwise} \\end{cases} $$Setting the scores of non-Top-k experts to $-\\infty$ ensures that the probabilities of these experts in the subsequent softmax operation are 0, achieving sparsity.\nSoftmax Normalization: Perform softmax normalization on the gating scores of the Top-k experts to obtain a sparse probability distribution $G(x)$:\n$$ G(x) = \\text{softmax}\\left( \\text{topk}(H(x), k) \\right) $$Only the probabilities of the Top-k experts are non-zero, and the rest are 0. For example, if $n=100, k=2$, then the probabilities of 98 experts are 0.\nWeighted Summation: Weight and sum the outputs of the Top-k experts according to the probabilities to obtain the output of the MoE layer:\n$$ y = \\sum_{i=1}^{n} G^{(i)}(x) E_i(x) $$Since only $k$ experts are activated, the amount of calculation is much lower than activating all $n$ experts.\nAuxiliary Loss To prevent the gating network from being overly biased towards a few experts, MoE introduces Auxiliary Loss (Shazeer et al., 2017) to encourage all experts to be used evenly. A common method is based on the square of the Coefficient of Variation (CV) of expert usage rate:\n$$ \\mathcal{L}_{\\text{aux}} = w_{\\text{aux}} \\cdot \\text{CV}\\left( \\sum_{x \\in X} G(x) \\right)^2 $$ Parameter Description:\n$X$: Input samples of a mini-batch. $\\sum_{x \\in X} G(x)$: Statistics on the number of times each expert is activated in a mini-batch. $\\text{CV}$: The ratio of standard deviation to mean, measuring the uniformity of expert usage distribution. $w_{\\text{aux}}$: Weight of auxiliary loss, which needs to be adjusted manually. Function: By minimizing $\\mathcal{L}_{\\text{aux}}$, the model optimizes the balance of expert selection and avoids some experts being overused while others are idle.\nGShard GShard (Lepikhin et al., 2020) mainly shards the MoE layer, distributing the expert networks ${E_1, E_2, \u0026hellip;, E_n}$ in the MoE layer to multiple TPU devices. For example, if there are $P$ TPU devices, the expert networks can be divided into $P$ groups, and each group of expert networks is assigned to a TPU device. Other layers of the Transformer model (such as self-attention layer, LayerNorm layer) are replicated on all TPU devices.\nImproved Gating Mechanism of GShard:\nGShard has made some improvements on the basis of Noisy Top-k Gating to improve the performance and stability of the gating mechanism:\nExpert Capacity: To avoid expert overload, GShard introduces expert capacity limits. Each expert network has a capacity limit, indicating the maximum number of tokens it can process. If a token is routed to an expert network that has reached its capacity limit, the token will be marked as \u0026ldquo;overflowed\u0026rdquo;, and the gating output will be set to a zero vector, indicating that the token will not be routed to any expert network.\nLocal Group Dispatching: To improve gating efficiency, GShard groups tokens and enforces expert capacity limits at the group level. For example, divide the tokens in a mini-batch into multiple local groups, each local group containing a certain number of tokens. The gating network selects the top-k expert networks for each local group and ensures that the number of tokens processed by each expert network in a local group does not exceed its capacity limit.\nAuxiliary Loss: GShard also uses an auxiliary loss function to balance expert load. Different from the auxiliary loss of the original MoE model, GShard\u0026rsquo;s auxiliary loss aims to minimize the mean square error of the proportion of data routed to each expert network, which more directly measures the degree of expert load balance.\nRandom Routing: To increase the randomness of routing, GShard introduces a random routing mechanism when selecting the top-k expert networks. In addition to selecting the best top-k expert networks, GShard also randomly selects suboptimal expert networks with a certain probability to increase the diversity of expert networks and improve the generalization ability of the model.\nBelow is the core algorithm flow of GShard:\nFig. 13. Pseudo code of the group-level top-2 gating mechanism with auxiliary loss in GShard. (Image source: Lepikhin et al., 2020)\nSwitch Transformer Switch Transformer (Fedus et al. 2021) is a MoE model proposed by Google with a parameter size of trillions. Its core innovation is to replace the dense feed-forward network (FFN) layer in the Transformer model with a sparse Switch FFN layer. Unlike GShard\u0026rsquo;s Top-2 Gating, Switch Transformer only routes each input token to one expert network, which has higher sparsity and further reduces computational costs, making it possible to train trillion-parameter models. It encourages token routing to be more balanced among $N$ experts. The auxiliary loss of Switch Transformer is based on the cumulative product of the actual routing ratio and the predicted routing probability. The specific formula is as follows:\n$$ \\text{loss} = \\alpha \\cdot N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ Parameter Description: $N$: Total number of experts.\n$f_i$: The proportion of tokens routed to the $i$-th expert, defined as:\n$$ f_i = \\frac{1}{T} \\sum_{x \\in B} 1\\{\\text{argmax } p(x) = i\\} $$ $P_i$: The routing probability of the $i$-th expert predicted by the gating network, defined as:\n$$ P_i = \\frac{1}{T} \\sum_{x \\in B} p_i(x) $$ $T$: Total number of tokens in batch $B$.\n$\\alpha$: Weight hyperparameter of auxiliary loss, usually set to $10^{-2}$.\nBy minimizing $\\text{loss}$, the model makes the actual routing ratio $f_i$ consistent with the predicted probability $P_i$, thereby indirectly promoting load balancing between experts and avoiding some experts being idle.\nFig. 14. Switch transformer. The sparse switch FFN layer is in the blue boxes. (Image source: Fedus et al. 2021)\nSwitch Router Mechanism:\nRouting Prediction: For an input token $x$, Switch Router predicts the routing probability $p_i = G^{(i)}(x)$ of each expert network, where $i = 1, 2, \u0026hellip;, n$, and n is the number of expert networks.\nExpert Selection: Select the expert network with the highest routing probability as the best expert network. Switch Transformer adopts the Top-1 routing strategy, that is, each token is only routed to the expert network with the highest routing probability.\nToken Routing: Route the input token $x$ to the selected best expert network for processing.\nTraining Stability Optimization of Switch Transformer:\nTo improve the training stability of Switch Transformer, the paper proposes the following optimization strategies:\nSelective Precision Using FP32 precision inside the routing function can improve training stability and avoid additional overhead caused by FP32 tensor communication. Specifically, the calculation process of Switch Router uses FP32 throughout, and the final result is converted to FP16 to balance efficiency and precision.\nSmaller Initialization It is recommended to adjust the weight initialization scale parameter $s$ of Transformer from 1 to 0.1. A smaller initialization scale helps to alleviate the risk of gradient explosion in the early stage of training, thereby improving overall training stability. The specific implementation is to sample from a truncated normal distribution with a mean of 0 and a standard deviation of $\\sqrt{s/n}$ (where $n$ is the number of input units).\nHigher Expert Dropout Using a higher dropout rate (e.g., 0.4) in the expert FFN layer, while maintaining a lower dropout rate (e.g., 0.1) in non-expert layers, this setting can effectively prevent overfitting and thus enhance the generalization ability of the model. The experimental results in the figure below show that the model performs best when the dropout rate of the expert layer is set to 0.4 on tasks such as GLUE, CNNDM, SQuAD, and SuperGLUE.\nFig. 15. Fine-tuning regularization results. A sweep of dropout rates while fine-tuning Switch Transformer models pre-trained on 34B tokens of the C4 data set(higher numbers are better). (Image source: Fedus et al. 2021)\nThe Switch Transformers paper uses the following figure to intuitively show how different parallel technologies split model weights and data:\nFig. 16. An illustration of various parallelism strategies on how(Top) model weights and(Bottom) data are split over multiple GPU cores. In the top row, each color denotes a unique weight matrix. In the bottom row, different colors indicate different sets of tokens. (Image source: Fedus et al. 2021)\nExpert Choice Expert Choice (EC) (Zhou et al. 2022) is a routing strategy opposite to token choice routing (such as GShard\u0026rsquo;s top-2 or Switch Transformer\u0026rsquo;s top-1). In token choice routing, each token selects top-k experts from all experts for routing; while in expert choice routing, each expert selects top-k tokens from all tokens for processing. This method aims to solve the problems of load imbalance and token waste in token choice routing, and significantly improve training efficiency. The following is the specific calculation process:\nCalculate Token-to-Expert Affinity Score\nFor an input matrix $X \\in \\mathbb{R}^{n \\times d}$, the process of calculating the token-to-expert affinity score matrix $S \\in \\mathbb{R}^{n \\times e}$ is:\n$$ S = \\text{softmax}(X \\cdot W_g), \\quad \\text{where } W_g \\in \\mathbb{R}^{d \\times e}. $$ Here, $W_g$ is the gating weight matrix, and $e$ is the number of experts.\nExpert Selects Tokens\nEach expert selects top-k tokens from all tokens for processing. Top-k selection is performed on $S^T$:\n$$ G, I = \\text{top-}k(S^T, k), $$to get:\nGating matrix $G \\in \\mathbb{R}^{e \\times k}$: Records the routing weights corresponding to the tokens selected by the experts, where $G[i, j]$ represents the weight of the $j$-th token selected by expert $i$; Token index matrix $I \\in \\mathbb{R}^{e \\times k}$: Represents the index of the token selected by each expert in the input. One-hot Encoding\nConvert the token index matrix $I$ into a one-hot encoding matrix $P \\in \\mathbb{R}^{e \\times k \\times n}$ for subsequent calculations:\n$$ P = \\operatorname{one}-\\operatorname{hot}(I) $$ Construct Gated FFN Layer Input\nFor each expert $i$, the input of its gated FFN layer is:\n$$ (P \\cdot X) \\in \\mathbb{R}^{e \\times k \\times d}. $$ EC controls the sparsity of the model by regularizing and limiting the number of experts to which each token is routed. A common regularization target is as follows:\n$$ \\begin{aligned} \u0026 \\max_{A} \\langle S^{\\top}, A \\rangle + \\lambda H(A) \\\\ \u0026 \\text{s.t. } \\forall i: \\sum_{j'} A[i, j'] = k, \\quad \\forall j: \\sum_{i'} A[i', j] \\leq b, \\quad \\forall i,j: 0 \\leq A[i, j] \\leq 1, \\end{aligned} $$In the optimization problem considered, a matrix $A$ is defined, and the element in the $i$-th row and $j$-th column indicates whether the $i$-th expert has selected the $j$-th token (value 0 or 1). Since this optimization problem is complex to solve, the paper uses the Dykstra algorithm (to obtain an approximate solution through multiple iterations) to solve it.\nThe parameter $b$ is usually determined by the total number of tokens $n$ in the batch and the capacity factor, where the capacity factor represents the average number of experts used by each token. Most experiments use a higher capacity factor. The experimental results show that even when the capacity is reduced, EC (Expert Choice) still performs better than traditional top-1 token choice routing, although capped expert choice slightly reduces fine-tuning performance.\nThe advantages of EC are mainly reflected in the following two aspects:\nPerfect Load Balancing: Each expert processes a fixed number of $k$ tokens, thus avoiding the problem of some experts being overloaded while others are idle, achieving ideal load balancing. Higher Training Efficiency: Experiments show that EC can improve the training convergence speed by about 2 times, which is more efficient than traditional token choice routing. However, EC also has the following limitations:\nBatch Size Requirements: Since EC has high requirements for batch size, it is not suitable for scenarios with smaller batch sizes. Autoregressive Generation Limitations: In autoregressive text generation tasks, EC\u0026rsquo;s top-k selection cannot be implemented because future tokens cannot be predicted, so it is not suitable for such tasks. Sequence Parallelism Sequence Parallelism (SP) is a parallelization strategy proposed for long sequence models (such as Transformer). By partitioning the input in the sequence dimension, it greatly reduces activation memory footprint and improves training efficiency. It is often used in combination with data parallelism, tensor parallelism, or pipeline parallelism, and is especially suitable for processing ultra-long text or other sequence data.\nColossal-AI Sequence Parallelism Fig. 17. The overall architecture of the proposed sequence parallelism and existing parallel approaches. For sequence parallelism, Device 1 and Device 2 share the same trainable parameters. (Image source: Li, et al. 2021)\nThe computational complexity and memory overhead of self-attention are proportional to the square of the sequence length $s$, $O(s^2)$. Long sequence data will increase the intermediate activation memory usage, thus limiting the training capacity of the device. Colossal-AI sequence parallelism (Li, et al. 2021) proposes splitting ultra-long sequences to multiple cards from a system perspective. The specific solution steps are as follows.\nSequence Chunking Divide the input sequence into several chunks, each chunk is saved and computed by different GPUs; therefore, each card only needs to store the activation of its corresponding sequence chunk, avoiding single-card memory explosion. Ring Communication + Self-Attention Propose Ring Self-Attention (RSA) mechanism: each GPU first calculates local attention, and then sequentially transmits (ring structure) Key/Value chunks to adjacent GPUs. After multiple iterations, it is guaranteed that each GPU can obtain global sequence information. Combination with Other Parallel Methods Not restricted by hyperparameters such as the number of attention heads and layers, it can be combined with data parallelism, tensor parallelism, pipeline parallelism and other technologies to jointly break through the sequence length limit of large-scale models. Fig. 18. Ring Self-Attention. (Image source: Li, et al. 2021)\nMegatron-LM Sequence Parallelism Megatron-LM (Shoeybi et al. 2019) originally used tensor parallelism to share part of the activation values, but the activation values of operations such as LayerNorm and Dropout in Transformer still need to be completely saved on a single card, and the memory consumption is still huge. Therefore, NVIDIA proposed Megatron-LM sequence parallelism (Korthikanti, et al. 2022) to split these activation values in the sequence dimension, greatly reducing the footprint.\nFig. 19. Transformer layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)\nFig. 20. MLP layer with tensor and sequence parallelism. (Image source: Korthikanti, et al. 2022)\nSequence Dimension Splitting For activations that are difficult to split in the tensor dimension, such as LayerNorm and Dropout, divide them along the sequence dimension, so that each GPU only processes a part of the sequence\u0026rsquo;s nonlinear operations. Tensor Parallelism is Still Retained Linear operations such as Attention and MLP continue to use tensor parallelism; the activations of sequence parallelism need to perform corresponding All-Gather or Reduce-Scatter before and after to exchange data. Selective Activation Recomputation For some operations with small computational load but large activation volume, choose to temporarily recompute during backpropagation to further save memory. DeepSpeed-Ulysses Sequence Parallelism DeepSpeed-Ulysses (Jacobs et al. 2023) proposes an efficient sequence parallelism scheme for ultra-long sequence training. By partitioning the input in the sequence dimension and combining two-stage all-to-all communication, it effectively reduces communication volume and activation memory, thereby supporting the training of million-token long sequence Transformer models.\nFig. 21. DeepSpeed sequence parallelism(DeepSpeed-Ulysses) design. (Image source: Jacobs et al. 2023)\nSequence Partitioning + All-to-All Communication Divide the input sequence along the sequence dimension to $P$ GPUs, and each GPU only processes a local $N/P$ sequence; before attention calculation, exchange Query ($Q$), Key ($K$), and Value ($V$) through All-to-All operation, so that each GPU obtains complete sequence information, but only calculates the assigned attention heads.\nTwo-Stage Communication Optimization\nFirst All-to-All: Perform all-to-all exchange on $Q$/$K$/$V$ before attention calculation to disperse activation calculation and reduce memory pressure per card; Second All-to-All: After attention calculation, collect the output context and remap it to local sequence partitions, which not only restores the original sequence structure but also significantly reduces the amount of communication data. Efficient Communication and Generality Using all-to-all communication, the communication volume is reduced to $O(N/P)$, which saves nearly $P$ times the bandwidth compared to the traditional All-Gather method (communication volume $O(N)$); at the same time, this scheme is suitable for dense and sparse attention and can be seamlessly integrated with ZeRO-3 memory optimization, thereby supporting efficient training of larger models and longer sequences.\nFig. 22. DeepSpeed-Ulysses vs Megatron LM. (Image source: DeepSpeed Blogs)\nIn a 64-card A100 environment, the throughput is increased by up to 2.5 times compared to Megatron-LM sequence parallelism, and longer sequences (million-level tokens) can be processed; The convergence performance is the same as the original model, and it can be easily integrated into the Megatron-DeepSpeed framework. Optimizer-Related Parallelism: ZeRO ZeRO (Zero Redundancy Optimizer) (Rajbhandari et al. 2019) is an optimizer parallelism technology designed to eliminate memory redundancy when training large models. The main memory consumption for training large models is in two parts:\nModel States: Including optimizer states (such as momentum and second-order moments of Adam), gradients, and model parameters. Mixed-precision training not only requires storing FP16 data but also needs to retain FP32 versions of parameters and states, resulting in higher memory footprint. Activations, Temporary Buffers, and Memory Fragmentation (Residual States): These data are only used once in forward and backward propagation, but they also occupy a lot of memory. To solve the memory redundancy problem, ZeRO adopts two major strategies:\nZeRO-DP (Data Parallelism): For model states, by sharding and distributing optimizer states, gradients, and parameters to multiple data parallel processes, redundancy is eliminated, and communication volume is reduced by using dynamic communication scheduling.\nZeRO-R (Residuals Optimization): For activations and temporary buffers, memory usage is optimized by using sharded activation recomputation, fixed buffer size, and real-time memory fragmentation management.\nZeRO Sharding Strategy ZeRO is divided into three stages, each stage further reduces memory redundancy on the basis of the previous stage, thus making it possible to train ultra-large models:\nZeRO-1 (Optimizer State Sharding) Principle: Shard optimizer states (such as Adam\u0026rsquo;s momentum and second-order moments) along the parameter dimension into $P$ shards ($P$ is the number of GPUs), and each GPU only stores the states corresponding to the model parameters it is responsible for. Local Update: Each GPU only updates its locally stored state and parameter shards during the parameter update phase, without additional cross-GPU communication. ZeRO-2 (Gradient Sharding) Principle: On the basis of optimizer state sharding, gradients are also sharded along the parameter dimension, and each GPU only stores the corresponding gradient shard. Each GPU calculates local gradients and uses efficient Reduce-Scatter operations to aggregate gradients and then update local parameter shards. ZeRO-3 (Parameter Sharding) Principle: On the basis of ZeRO-1 and ZeRO-2, model parameters (usually 16-bit data) are also sharded, and each GPU only stores the parameter shards corresponding to it. Parameter Collection on Demand: During forward or backward propagation, if a GPU needs complete model parameters, it collects the missing shards from other GPUs. This process is only performed when necessary to reduce communication overhead. The following figure shows the comparison of model state memory consumption per device in different stages:\nFig. 23. Comparing the per-device memory consumption of model states, with three stages of ZeRO-DP optimizations. (Image source: Rajbhandari et al. 2019)\nComparison of DeepSpeed ZeRO Sharding and Offload Strategies To better understand DeepSpeed\u0026rsquo;s ZeRO strategy, the following compares each stage and Offload scheme:\nZeRO Stage Description Memory Footprint Training Speed ZeRO-0 Pure data parallelism, no sharding, all states are fully replicated on each GPU. Highest Fastest ZeRO-1 Optimizer states are sharded only, gradients and parameters are still replicated. Higher Slightly slower than ZeRO-0 ZeRO-2 Optimizer states and gradients are sharded. Medium Slower than ZeRO-1 ZeRO-3 Optimizer states, gradients, and model parameters are sharded. Lowest Significantly slower than ZeRO-2, affected by model size and network bandwidth Offload Type Description Memory Footprint Training Speed ZeRO-1 + CPU Offload On the basis of ZeRO-1, optimizer states are offloaded to CPU memory, reducing GPU memory footprint, but relying on PCIe bandwidth and occupying CPU memory. Medium-Low Slower than ZeRO-1 ZeRO-2 + CPU Offload On the basis of ZeRO-2, optimizer states are offloaded to CPU memory, further reducing GPU memory footprint for large models, but increasing CPU-GPU data transfer. Low Slower than ZeRO-2 ZeRO-3 + CPU Offload On the basis of ZeRO-3, optimizer states and model parameters are offloaded to CPU, GPU memory footprint is the lowest, but CPU-GPU communication overhead is extremely large. Extremely Low Very Slow ZeRO-Infinity (NVMe Offload) Based on ZeRO-3, states are offloaded to NVMe devices, breaking through CPU memory limits, suitable for ultra-large models; performance is highly dependent on NVMe parallel read and write speed. Extremely LowNVMe support required Slower than ZeRO-3, but usually better than CPU Offload scheme Communication Volume and Performance Impact ZeRO-0/1/2: Mainly rely on All-Reduce for gradient synchronization, and the communication volume is relatively low.\nZeRO-3: All-Gather/All-Reduce operations are required for model parameters, and the communication volume increases significantly. Network bandwidth becomes a key bottleneck.\nOffload Strategy (CPU/NVMe): Data transmission is mainly between CPU ↔ GPU or NVMe ↔ GPU. The transmission bandwidth is much lower than the communication between GPUs, which may significantly affect the training speed, especially in ZeRO-3 scenarios.\nMulti-dimensional Parallelism Multi-dimensional Parallelism refers to the organic combination of multiple parallel technologies such as data parallelism, model parallelism, and pipeline parallelism in distributed training to fully utilize the computing resources of modern GPU clusters. Through this \u0026ldquo;3D parallelism\u0026rdquo; or \u0026ldquo;4D parallelism\u0026rdquo; strategy, not only memory efficiency can be improved, but also computational efficiency can be improved, thereby achieving efficient training of ultra-large-scale (even trillion-parameter level) models.\n3D Parallelism With the rapid improvement of the computing power of GPU clusters, training trillion-parameter models is no longer out of reach. DeepSpeed integrates data parallelism, model parallelism, and pipeline parallelism to build a \u0026ldquo;3D parallelism\u0026rdquo; strategy. This strategy mainly solves the two major challenges faced by training ultra-large models:\nMemory Efficiency: Model layers are divided into different pipeline stages, and each stage is further divided by model parallelism, reducing the memory occupied by models, optimizers, and activations. However, it should be noted that model splitting cannot be unlimited, otherwise, the communication overhead will increase significantly, which will affect computational efficiency.\nComputational Efficiency: To make the number of computing workers exceed the limitations of simple model and pipeline parallelism, and to ensure computational efficiency, DeepSpeed expands with ZeRO-DP (data parallelism based on optimizer state sharding). ZeRO-DP not only further optimizes memory usage but also allocates data parallel groups to devices with local high-bandwidth communication through topology-aware mapping, greatly reducing communication overhead.\nThe following diagram shows the overall strategy of 3D parallelism:\nFig. 24. Example 3D parallelism with 32 workers. Layers of the neural network are divided among four pipeline stages. Layers within each pipeline stage are further partitioned among four model parallel workers. Lastly, each pipeline is replicated across two data parallel instances, and ZeRO partitions the optimizer states across the data parallel replicas. (Image source: Majumder et al. 2020)\nEach parallel dimension (data, model, pipeline) is carefully mapped to fully utilize the communication bandwidth within and between nodes. Specific strategies include:\nOptimize Intra-node Communication: Since model parallelism has the largest communication overhead, model parallel groups are preferentially arranged within the same node to utilize higher intra-node bandwidth (e.g., using NVIDIA Megatron-LM\u0026rsquo;s tensor sharding method); Data Parallelism and Pipeline Parallelism: When model parallelism does not cover the entire node, data parallel groups are arranged within the same node as much as possible; pipeline parallelism can be flexibly arranged for cross-node scheduling due to its smaller communication volume. By reducing the amount of communication data in each data parallel group and increasing the parallelism of local parallel communication, the overall communication bandwidth is effectively amplified.\nFig. 25. Mapping of workers in Figure 24 to GPUs on a system with eight nodes, each with four GPUs. Coloring denotes GPUs on the same node. (Image source: Majumder et al. 2020)\n4D Parallelism To further expand the model scale, Llama3 (Grattafiori et al., 2024) adopted a 4D parallel strategy during training. It combines four parallel methods to shard the model in a more fine-grained manner, so that the model parameters, optimizer states, gradients, and activations on each GPU can be adapted to the capacity limit of high-bandwidth memory (HBM). These four parallel methods are:\nTensor Parallelism (TP): Divide a single weight tensor into multiple blocks and distribute them across different devices; Pipeline Parallelism (PP): Vertically divide the model into multiple stages, and each stage processes different micro-batches in parallel on different devices; Context Parallelism (CP): Divide the input context into multiple segments to alleviate the memory bottleneck when inputting long sequences; Data Parallelism (DP), usually using Fully Sharded Data Parallelism (FSDP): Shard models, optimizer states, and gradients, and synchronize after each training step. The following diagram shows an example of 4D parallelism implemented on 16 GPUs. The position of each GPU is represented by a vector [D1, D2, D3, D4], where each dimension corresponds to a parallel strategy. GPUs are grouped according to four dimensions [TP, CP, PP, DP], and the group size of each dimension is 2. For example, GPU0 and GPU1 belong to the same tensor parallel group; GPU0 and GPU2 belong to the same context parallel group; GPU0 and GPU4 belong to the same pipeline parallel group; GPU0 and GPU8 belong to the same data parallel group:\nFig. 26. Illustration of 4D parallelism. (Image source: Grattafiori et al., 2024)\nThrough the 4D parallel strategy, Llama3 can fully utilize the computing resources of multiple GPUs during training, while effectively reducing memory footprint and supporting the training of ultra-large-scale models.\nMemory Optimization Techniques In addition to parallel training techniques, there are many memory optimization techniques designed to help train LLMs. These designs mainly start from reducing the memory footprint of each stage in the training process.\nCPU Offloading CPU Offloading (Rhu et al. 2016) refers to a common and intuitive practice of offloading data or tensors that are temporarily not needed to the CPU when GPU memory is insufficient and loading them back to the GPU when needed. Its main purpose is to use the larger capacity of CPU memory to expand available space, so that larger-scale models can be trained even in memory-constrained environments. However, this method will bring additional data transmission overhead and usually reduce training speed, so its application has been relatively reduced in recent years.\nIdentify Offloadable Tensors: Identify tensors that are temporarily not used during training, such as model parameters, optimizer states, intermediate activations, etc. The basis for judging whether a tensor can be offloaded can be the frequency of use, life cycle, etc. of the tensor. Tensor Offloading: Move offloadable tensors from GPU memory to CPU memory. Data transmission is usually performed through the PCIe bus. Tensor Prefetching: Before needing to use tensors offloaded to CPU memory, load the tensors from CPU memory back to GPU memory in advance. Prefetching operations can be performed in parallel with GPU computing operations to reduce data loading latency. Tensor Usage: The GPU uses tensors loaded back into GPU memory for computation. Tensor Re-offloading: After the tensor is used up, if the tensor is no longer needed for a period of time, it can be offloaded to CPU memory again to release GPU memory space. ZeRO-Offload and ZeRO-Infinity are memory optimization technologies based on CPU offloading implemented in the DeepSpeed library. ZeRO-Offload offloads optimizer states to CPU memory, and ZeRO-Infinity goes further, offloading model parameters to CPU memory or even NVMe disks, breaking through the GPU memory wall and supporting the training of larger-scale models.\nThe following figure intuitively shows the memory optimization technology of Heterogeneous system:\nFig. 27. Heterogenous system illustration. (Image source: Clolossal-AI Documentation)\nActivation Recomputation/Gradient Checkpointing Activation Recomputation/Gradient Checkpointing (Chen et al. 2016) is a technology that trades computation for memory. During training, only part of the activation values are saved (e.g., the input activation values of each Transformer layer). During backpropagation, the unsaved activation values are recomputed. Activation recomputation can significantly reduce the activation memory footprint during training, especially when training deep neural networks.\nSelect Checkpoints: Select some layers in the model as checkpoints. Usually, key layers in the model are selected as checkpoints, such as the input layer of the Transformer layer. Forward Pass: During forward propagation, only the activation values of checkpoint layers are saved. For non-checkpoint layers, the activation values are immediately released after calculation and not saved. Backward Pass: During backpropagation, when it is necessary to calculate the gradient of a non-checkpoint layer, forward propagation is performed again first to calculate the activation value of the layer (recomputation), and then backward propagation is performed to calculate the gradient. For checkpoint layers, since the activation values of checkpoint layers have been saved, the saved activation values can be directly used for backpropagation without recomputation. The following is a memory cost analysis of activation recomputation. For ease of analysis, assume that the model has a total of $n$ network layers and divides them evenly into $k$ segments. In this way, each segment contains approximately $n/k$ network layers. When doing activation recomputation, we only save the activation values at the boundaries of each segment (i.e., checkpoints), and recompute the rest when needed. The following function represents the memory requirement during training:\n$$ \\text{cost-total} \\;=\\; \\max_{i=1,\\ldots,k}\\bigl[\\text{cost-of-segment}(i)\\bigr] \\;+\\; O(k) \\;=\\; O\\Bigl(\\tfrac{n}{k}\\Bigr) \\;+\\; O(k). $$Next, consider how to choose the optimal $k$ to minimize $f(k)$ given $n$:\n$$ f(k) \\;=\\; \\frac{n}{k} \\;+\\; k. $$Take the derivative of $f(k)$ with respect to $k$ and set it to 0 (only consider $k\u0026gt;0$):\n$$ f'(k) \\;=\\; -\\frac{n}{k^2} \\;+\\; 1 \\;=\\; 0 \\quad\\Longrightarrow\\quad k^2 = n \\quad\\Longrightarrow\\quad k = \\sqrt{n}. $$Substituting $k = \\sqrt{n}$, we can get the minimum memory overhead of approximately\n$$ f(\\sqrt{n}) \\;=\\; \\frac{n}{\\sqrt{n}} \\;+\\; \\sqrt{n} \\;=\\; 2\\sqrt{n}. $$Therefore, the overall peak memory requirement of this approach can be reduced to the order of $O(\\sqrt{n})$ (compared to the $O(n)$ memory of generally directly saving all activations), which is why activation recomputation technology can bring \u0026ldquo;sublinear\u0026rdquo; memory footprint. The following figure intuitively shows the effect of this trick.\nFig. 28. The memory cost of different memory saving algorithms. Sharing: Memory used by intermediate results is recycled when no longer needed. Inplace: Save the output directly into memory of an input value. (Image source: Chen et al. 2016)\nIt should be noted that activation recomputation requires additional forward recomputation in the backward propagation stage. Each segment needs to perform forward computation of $n/k$ layers. If the network is divided into $k$ segments, the total recomputation during backpropagation is approximately $k \\times \\bigl(n/k\\bigr) = n$ layers of forward operations, which is equivalent to doing approximately one more \u0026ldquo;forward propagation\u0026rdquo; in each training iteration. This is usually acceptable in LLM training because:\nCompared to quickly exhausting GPU memory and making it impossible to train large-scale models, this additional cost in computation is usually more bearable. When the model is very deep ($n$ is large), using activation recomputation technology can significantly reduce memory usage from $O(n)$ to $O(\\sqrt{n})$, making it possible to train more and deeper large models on given hardware. Mixed Precision Training Mixed Precision Training (Micikevicius al. 2017) is a technology that simultaneously uses low-precision floating-point numbers (such as FP16 or BF16) and high-precision floating-point numbers (such as FP32) during model training. Its core goal is to reduce memory footprint and accelerate training while maintaining model accuracy comparable to full-precision training as much as possible.\nModern GPUs have higher throughput and lower memory footprint in low-precision computing, thereby reducing memory access overhead and memory bandwidth requirements. Mixed-precision training can significantly improve training speed. The following figure shows the basic process of mixed-precision training in a network layer: forward and backward propagation mainly use half-precision (FP16) operations, while gradient accumulation and parameter updates use full-precision (FP32) to avoid numerical precision problems that may be caused by low-precision computing.\nFig. 29. Mixed precision training iteration for a layer. (Image source: Micikevicius al. 2017)\nMixed-precision training mainly relies on the following three key technologies:\nFull-Precision Master Copy of Weights To prevent gradients from being truncated to zero due to being too small in magnitude under FP16, a master copy of FP32 weights is maintained during training. The specific process is:\nInitialization: Use FP32 weights as the master copy of the model; Forward/Backward Propagation: Before each iteration starts, convert FP32 weights to FP16 for forward propagation and backward propagation to calculate FP16 gradients; Parameter Update: Before updating parameters, convert FP16 gradients to FP32 and use them to update the FP32 master copy. This design not only utilizes the efficiency of low-precision computing but also ensures the accuracy of parameter updates.\nLoss Scaling To avoid gradient underflow due to the limited representation range of low precision, the loss value is usually amplified before backpropagation. The specific process is:\nUse FP32 to calculate the loss $L$; Multiply the loss by a scaling factor $S$ to get $L\u0026rsquo; = L \\times S$, and then perform backpropagation to calculate FP16 gradients; Before parameter update, divide the gradient by $S$ to restore it to the true gradient. The choice of scaling factor is crucial: too small may not avoid gradient underflow, and too large may cause gradient overflow. Dynamic loss scaling technology can automatically adjust the scaling factor according to the actual situation of gradients during training.\nAs shown in the figure below, amplifying the loss makes the gradient distribution more concentrated in the higher numerical part, thereby retaining the subtle information that may be truncated under low-precision representation.\nFig. 30. The histogram of gradients in full precision. The left part up to $2^{-24}$ will be zero-ed off once the model switches to FP16. (Image source: Micikevicius al. 2017)\nArithmetic Precision Control For operations with high precision requirements (such as vector dot product and summation reduction), FP32 can be used for accumulation calculation, and then converted to FP16 for storage; for element-wise operations, FP16 or FP32 can be selected according to specific needs. Compression In the deep learning training process, intermediate results (such as activation values and gradient information), although only used once in forward propagation and once in backward propagation, often occupy a lot of memory. Considering that there is a significant time interval between two uses, data can be compressed after the first use, and then decompressed when needed later, thereby effectively reducing memory footprint.\nCompression technology is mainly applied to two scenarios:\nActivation Value Compression: Compress activation values after forward propagation and decompress before backward propagation. This is especially important for deep neural networks because activation values usually occupy a lot of memory. Gradient Compression: Compress gradients after calculating gradients in backpropagation and before gradient synchronization to reduce the amount of data for cross-GPU communication, thereby improving distributed training efficiency. Compression technology can be divided into two categories:\nLossless Compression: Methods such as Huffman coding or Lempel-Ziv algorithm are used to ensure that the decompressed data is completely consistent with the original data. However, due to the low compression rate, its memory saving effect is limited.\nLossy Compression: Algorithms such as JPEG or MPEG are used to obtain higher compression rates on the premise of allowing certain data loss. This method can significantly reduce memory footprint, but may have a certain impact on model accuracy and convergence.\nGist (Jain et al. 2018) is a memory optimization technology for activation value compression. Its core lies in using data encoding strategies to compress intermediate results, mainly including two encoding schemes:\nLayer-Specific Lossless Encoding: Design special lossless encoding schemes for specific layer structures (such as ReLU-Pool and ReLU-Conv):\nFor ReLU-Pool layers, binary encoding can be used; For ReLU-Conv layers, sparse storage and dense computation encoding are used. Aggressive Lossy Encoding: Delayed Precision Reduction (DPR) technology is used. The core idea of DPR is: activation values need to maintain high precision during forward propagation, while lower precision can be tolerated during backward propagation. Therefore, activation values are compressed to lower precision after forward propagation, and then decompressed to high precision before backward propagation.\nMemory-Efficient Optimizers Traditional optimizers (such as Adam, SGD with Momentum) need to maintain a large amount of state data (such as momentum and variance) for each model parameter during training. Their memory footprint is often comparable to or even higher than the model parameter size. For example, taking the Adam optimizer (Kingma et al. 2014) as an example, each parameter needs to store the first-order moment and the second-order moment. Adding up with the parameter itself and its gradient, the entire training process requires approximately 4 times the memory of the model weights, which poses a severe challenge to large model training.\nTo reduce memory consumption, memory-efficient optimizers are mainly designed through the following strategies:\nReduce the Number of State Variables: Only save necessary statistical information instead of complete matrices; Reduce the Precision of State Variables: Use FP16 or bfloat16 for storage; Share State Variables: Share part of the state information between multiple parameters. Adafactor Adafactor (Shazeer et al. 2018) is a memory-efficient adaptive learning rate optimizer. Unlike Adam, Adafactor does not store the complete second-order moment estimation matrix, but only stores two vectors (row and column statistics) to replace the complete second-order moment matrix, which significantly reduces memory footprint, especially suitable for scenarios where the parameter matrix has a low-rank structure.\nSM3 SM3 (Sparse Momentum for Massive Models) (Anil et al. 2019) provides a memory-efficient adaptive optimization scheme through sparse updates and state sharing.\nSparse Momentum: Only update Momentum for parameters with non-zero gradients, thereby reducing computation and storage overhead; State Sharing: To a certain extent, allow different parameters to share state variables, further reducing memory consumption; Adaptive Learning Rate: Dynamically adjust the learning rate according to the gradients of each parameter, improving the stability and convergence speed of model training. LoRA LoRA (Low-Rank Adaptation) (Hu et al., 2021) proposes to introduce low-rank adapters next to pre-trained weights to achieve efficient fine-tuning by adding a small number of parameters without interfering with the original inference ability of the pre-trained model.\nThe following figure intuitively shows the principle and initialization strategy of LoRA:\nFig. 31. An illustration of LoRA. (Image source: Hu et al., 2021)\nAssuming that the pre-trained weight matrix is $ \\mathbf{W} \\in \\mathbb{R}^{d \\times k} $. LoRA adds a low-rank update term $ \\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A} $ to it to obtain new weights:\n$$ \\mathbf{W}' = \\mathbf{W} + \\alpha\\, \\mathbf{B}\\mathbf{A}, $$where:\n$ \\mathbf{A} \\in \\mathbb{R}^{r \\times d} $ is the dimensionality reduction matrix; $ \\mathbf{B} \\in \\mathbb{R}^{k \\times r} $ is the dimensionality increase matrix; $ r \\ll \\min(d,k) $ is the low-rank dimension, generally taking values of 1, 2, 4, or 8; $ \\alpha $ is an adjustable scaling factor. During fine-tuning, the original weight $ \\mathbf{W} $ is frozen and unchanged, and only $ \\mathbf{A} $ and $ \\mathbf{B} $ are updated. Since $ r $ is much smaller than $ d $ or $ k $, the number of parameters that need to be trained is greatly reduced.\nTo ensure that the impact of the introduced $ \\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A} $ on the original model is as small as possible in the early stage of fine-tuning, $ \\Delta \\mathbf{W} \\approx 0 $ is required. Common practices are as follows:\nInitialization of Dimensionality Reduction Matrix $ \\mathbf{A} $\nGaussian Initialization Let $ \\mathbf{A} \\sim \\mathcal{N}(0,\\sigma^2) $ (generally $ \\sigma $ takes a smaller value, such as 0.02). This can ensure that the initial update amount is small and does not seriously interfere with the model output. Kaiming (He) Initialization Kaiming initialization is a weight initialization method specially designed for deep networks. Its goal is to maintain the stability of forward signals and backward gradients between network layers. For LoRA, as long as it is ensured that the scale is small (or with a suitable scaling factor), the initial $ \\Delta \\mathbf{W} $ can be made closer to zero. Initialization of Dimensionality Increase Matrix $ \\mathbf{B} $\nUsually, $ \\mathbf{B} $ is initialized as a zero matrix, so that $ \\mathbf{B}\\mathbf{A} = 0 $ initially, further ensuring that the impact on the original model is minimal. Training with LoRA has the following advantages:\nParameter Efficiency: Only low-rank adapter parameters are introduced, reducing the total number of parameters that need to be trained and stored. Memory and Computational Efficiency: Most pre-trained weights are frozen, and only small-scale parameters are updated during fine-tuning, significantly reducing memory footprint and computing power overhead. No Additional Inference Latency: After training is completed, the update term $ \\Delta \\mathbf{W} $ can be merged back into the original weights, which will not increase the amount of calculation in the inference stage. QLoRA QLoRA (Dettmers et al., 2023) is a method for efficient fine-tuning of large-scale models based on LoRA combined with quantization ideas. Through the following three key improvements, it greatly reduces memory footprint while maintaining basically unchanged model accuracy:\n4-bit Normal Float (NF4) Quantization A block-based quantile quantization strategy is adopted to quantize the original model weights to 4 bits, thereby achieving significant storage compression with subtle loss of accuracy.\nDouble Quantization After performing quantization once on ordinary parameters, perform an additional quantization on the quantization constants to further reduce cache footprint.\nPaged Optimizer When memory usage is too high, automatically transfer part of the optimization process to CPU memory, thereby alleviating GPU memory pressure and improving scalability.\nDifferent from traditional LoRA, which only reduces the number of parameters to be fine-tuned, QLoRA also compresses all weights through 4-bit quantization, thereby maximizing the reduction of memory footprint and data transmission overhead while ensuring near-original accuracy.\nFig. 32. Different finetuning methods and their memory requirements. QLoRA improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. (Image source: Dettmers et al., 2023)\nThis method can be regarded as a further extension of LoRA: LoRA improves efficiency by reducing the number of weights that need to be fine-tuned, while QLoRA, on this basis, quantizes all weights (including the un-fine-tuned part) to 4-bit precision, achieving dual compression of storage and computation in general, which is suitable for efficient fine-tuning of LLMs in resource-constrained environments.\nSummary Parallel techniques and memory optimization strategies need to be weighed and selected according to the specific model structure, dataset size, hardware resources, and training goals. Usually, it is necessary to combine multiple technologies to effectively train large-scale models and achieve the best performance and efficiency.\nReferences [1] Weng, Lilian, and Greg Brockman. \u0026ldquo;Techniques for training large neural networks.\u0026rdquo; OpenAI Blog, 2022.\n[2] Shenggui Li, Siqi Mai. \u0026ldquo;Paradigms of Parallelism.\u0026rdquo; Colossal-AI Documentation, 2024.\n[3] Li, Shen, et al. \u0026ldquo;Pytorch distributed: Experiences on accelerating data parallel training.\u0026rdquo; arXiv preprint, 2020.\n[4] Li, Mu, et al. \u0026ldquo;Communication efficient distributed machine learning with the parameter server.\u0026rdquo; Advances in Neural Information Processing Systems 27, 2014.\n[5] Huang, Yanping, et al. \u0026ldquo;Gpipe: Efficient training of giant neural networks using pipeline parallelism.\u0026rdquo; Advances in Neural Information Processing Systems 32, 2019.\n[6] Harlap, Aaron, et al. \u0026ldquo;Pipedream: Fast and efficient pipeline parallel dnn training.\u0026rdquo; arXiv preprint, 2018.\n[7] Narayanan, Deepak, et al. \u0026ldquo;Memory-efficient pipeline-parallel dnn training.\u0026rdquo; International Conference on Machine Learning, PMLR, 2021.\n[8] Shoeybi, Mohammad, et al. \u0026ldquo;Megatron-lm: Training multi-billion parameter language models using model parallelism.\u0026rdquo; arXiv preprint, 2019.\n[9] Narayanan, Deepak, et al. \u0026ldquo;Efficient large-scale language model training on gpu clusters using megatron-lm.\u0026rdquo; Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021.\n[10] Shazeer, Noam, et al. \u0026ldquo;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\u0026rdquo; arXiv preprint, 2017.\n[11] Lepikhin, Dmitry, et al. \u0026ldquo;Gshard: Scaling giant models with conditional computation and automatic sharding.\u0026rdquo; arXiv preprint, 2020.\n[12] Fedus, William, Barret Zoph, and Noam Shazeer. \u0026ldquo;Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\u0026rdquo; Journal of Machine Learning Research 23.120, 2022: 1–39.\n[13] Zhou, Yanqi, et al. \u0026ldquo;Mixture-of-experts with expert choice routing.\u0026rdquo; Advances in Neural Information Processing Systems 35, 2022: 7103–7114.\n[14] Li, Shenggui, et al. \u0026ldquo;Sequence parallelism: Long sequence training from system perspective.\u0026rdquo; arXiv preprint, 2021.\n[15] Korthikanti, Vijay Anand, et al. \u0026ldquo;Reducing activation recomputation in large transformer models.\u0026rdquo; Proceedings of Machine Learning and Systems 5, 2023: 341–353.\n[16] Jacobs, Sam Ade, et al. \u0026ldquo;Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.\u0026rdquo; arXiv preprint, 2023.\n[17] DeepSpeed. \u0026ldquo;DeepSpeed Ulysses README.\u0026rdquo; GitHub repository.\n[18] Rajbhandari, Samyam, et al. \u0026ldquo;Zero: Memory optimizations toward training trillion parameter models.\u0026rdquo; SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, 2020.\n[19] Microsoft Research. \u0026ldquo;DeepSpeed: Extreme-scale model training for everyone.\u0026rdquo; 2020.\n[20] Dubey, Abhimanyu, et al. \u0026ldquo;The llama 3 herd of models.\u0026rdquo; arXiv preprint, 2024.\n[21] Rhu, Minsoo, et al. \u0026ldquo;vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design.\u0026rdquo; 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture(MICRO), IEEE, 2016.\n[22] Chen, Tianqi, et al. \u0026ldquo;Training deep nets with sublinear memory cost.\u0026rdquo; arXiv preprint, 2016.\n[23] Micikevicius, Paulius, et al. \u0026ldquo;Mixed precision training.\u0026rdquo; arXiv preprint, 2017.\n[24] Jain, Animesh, et al. \u0026ldquo;Gist: Efficient data encoding for deep neural network training.\u0026rdquo; 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture(ISCA), IEEE, 2018.\n[25] Kingma, Diederik P., and Jimmy Ba. \u0026ldquo;Adam: A method for stochastic optimization.\u0026rdquo; arXiv preprint, 2014.\n[26] Shazeer, Noam, and Mitchell Stern. \u0026ldquo;Adafactor: Adaptive learning rates with sublinear memory cost.\u0026rdquo; International Conference on Machine Learning, PMLR, 2018.\n[27] Ginsburg, Boris, et al. \u0026ldquo;Stochastic gradient methods with layer-wise adaptive moments for training of deep networks.\u0026rdquo; arXiv preprint, 2019.\n[28] Hu, Edward J., et al. \u0026ldquo;LoRA: Low-rank adaptation of large language models.\u0026rdquo; ICLR, 2022: 3.\n[29] Dettmers, Tim, et al. \u0026ldquo;Qlora: Efficient finetuning of quantized llms.\u0026rdquo; Advances in Neural Information Processing Systems 36, 2023: 10088–10115.\n[30] Weng, Lilian. \u0026ldquo;How to Train Really Large Models on Many GPUs?\u0026rdquo; Lil\u0026rsquo;blog, 2021.\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui.(Mar 2025). Parallel and Memory Optimization Techniques for Training Large Models. https://syhya.github.io/posts/2025-03-01-train-llm\nOr\n@article{syhya2025train-llm, title = \u0026#34;Parallel and Memory Optimization Techniques for Training Large Models\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Mar\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-03-01-train-llm\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-03-01-train-llm/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eRecently, the number of parameters in large models has been continuously increasing, from the initial billions to today\u0026rsquo;s hundreds of billions or even trillions. While large models have brought unprecedented application effects, they have also triggered a series of severe challenges in computing resources, memory management, and training stability. Therefore, this blog summarizes some commonly used distributed parallel training and memory management techniques, hoping to help everyone better train and optimize large models.\u003c/p\u003e","title":"Parallel and Memory Optimization Techniques for Training Large Models"},{"content":"This blog post introduces a streamlined alternative to RLHF called DPO. Like RLHF, DPO is designed to align model outputs with human preferences, but it stands apart with its simplicity and lower resource demands. In scenarios where project resources are limited, DPO emerges as a highly attractive and practical solution worth exploring.\nNotations Symbol \\( x \\) User input (Prompt): the question the model needs to answer \\( y \\) Model-generated response (Response / Completion): the text output by the model \\( \\pi_\\theta(y \\mid x) \\) Actor model: The trainable policy used to generate response \\(y\\); parameterized by \\(\\theta\\) \\( \\pi_{\\mathrm{ref}}(y \\mid x) \\) Reference model: The frozen SFT (Supervised Fine-Tuning) model, serving as the alignment baseline \\( r_\\phi(x,y) \\) Reward model: A reward function (with parameter \\(\\phi\\)) used to evaluate the quality of response \\(y\\) \\( V_\\psi(x) \\) Critic model: A value function (with parameter \\(\\psi\\)) used to estimate the future cumulative reward given \\(x\\) \\( \\pi^*(y \\mid x) \\) Optimal policy distribution, determined via the reference model and reward function \\( r_\\theta(x,y) \\) Reward derived from the Actor model, constructed from \\(\\pi_\\theta\\) and \\(\\pi_{\\mathrm{ref}}\\) \\(\\beta\\) Hyperparameter that controls the weight of the KL penalty or the log-ratio difference term \\(\\mathbb{D}_{\\mathrm{KL}}[P \\| Q]\\) KL divergence, a measure of the difference between probability distributions \\(P\\) and \\(Q\\) \\(\\sigma(z)\\) Sigmoid function, defined as: \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\) \\(\\log\\) Logarithm function \\(\\mathbb{E}\\) Expectation operator, used to compute the average value of a random variable \\( (y_w, y_l) \\) A pair of preference data where \\( y_w \\) is the preferred (better quality) response and \\( y_l \\) is the lesser one \\( P\\left(y_w \\succ y_l \\mid x\\right) \\) The probability that response \\( y_w \\) is preferred over \\( y_l \\) given input \\(x\\) \\( Z(x) \\) Partition function, which normalizes the probability distribution over all responses \\(y\\) \\( \\mathcal{L}_{\\mathrm{DPO}} \\) The loss function of DPO From RLHF to DPO RLHF OpenAI primarily leverages Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017) to train InstructGPT (Ouyang et al., 2022), which forms the basis for LLMs (such as ChatGPT, Llama, etc.). The entire training process generally comprises the following three main steps:\nFig. 1. A diagram illustrating the three steps of InstructGPT. (Image source: Ouyang et al., 2022)\nSupervised Fine-Tuning (SFT)\nA pre-trained model is fine-tuned using a large volume of human-annotated examples, resulting in an initial model capable of understanding instructions and generating reasonable responses. This model is referred to as the reference model, \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\).\nReward Model Training\nFor simplicity, assume that for each input \\(x\\), two distinct responses are generated. In practice, multiple responses can be ranked. Two responses \\(y_w\\) (better) and \\(y_l\\) (worse) are generated for the same input \\(x\\), and human ranking provides the preference data. A reward model \\(r_\\phi(x, y)\\) is then trained on this data to predict which response aligns better with human preferences.\nPPO-Based Reinforcement Learning\nUsing feedback from the reward model \\(r_\\phi\\), the Actor model \\(\\pi_\\theta\\) is optimized via the Proximal Policy Optimization (PPO) algorithm to improve response quality. To prevent the model from deviating too far from \\(\\pi_{\\mathrm{ref}}\\), a KL penalty is added during optimization. This stage typically involves the following four models:\n\\(\\pi_\\theta\\): The model (after SFT) that is updated. \\(\\pi_{\\mathrm{ref}}\\): The frozen SFT model used as the alignment baseline. \\(r_\\phi\\): The fixed reward model for evaluating response quality. \\(V_\\psi\\): The critic model that estimates future rewards to assist the update of the Actor model. Limitations of RLHF While RLHF leverages human preference data to enhance model alignment, it comes with several inherent limitations:\nMulti-Model Training: In addition to the Actor model \\(\\pi_\\theta\\), extra models such as the reward model \\(r_\\phi\\) and the Critic model \\(V_\\psi\\) must be trained, making the overall process complex and resource-intensive. High Sampling Cost: LLMs require significant computational resources to generate text. The extensive online sampling during reinforcement learning further increases computational costs; insufficient sampling may lead to suboptimal optimization directions. Training Instability and Hyperparameter Sensitivity: PPO involves numerous hyperparameters (e.g., learning rate, sampling batch size), making tuning complex and the training process prone to instability. Alignment Tax Effect: While improving model alignment, the performance on other tasks may suffer. Introduction to DPO Fig. 2. DPO optimizes for human preferences while avoiding reinforcement learning. (Image source: Rafailov et al., 2023)\nDirect Preference Optimization (DPO) (Rafailov et al., 2023) algorithm was developed to address the above issues of RLHF. Its core idea is to convert the RLHF objective into a contrastive learning task akin to supervised fine-tuning, thereby achieving the following:\nEliminating Reward Model Training: Directly optimize the Actor model \\(\\pi_\\theta\\) using human preference data, without training a separate \\(r_\\phi\\). Removing Reinforcement Learning Sampling: Replace PPO with a contrastive loss function, reducing sampling and computational overhead. Enhancing Training Stability: The supervised learning approach is less sensitive to hyperparameters, leading to a more stable training process. Although DPO might have a lower performance ceiling compared to RLHF in terms of ultimate LLM performance improvements, it offers advantages in resource utilization, reduced implementation complexity, and training stability.\nMethod Comparison Method Training Steps Models Involved Training Approach Advantages Disadvantages RLHF Train a reward model first, then use PPO to optimize the policy \\(\\pi_\\theta\\), \\(\\pi_{\\mathrm{ref}}\\), \\(r_\\phi\\), \\(V_\\psi\\) Reinforcement learning with online sampling Fully leverages human preferences; higher performance potential Resource intensive; unstable training; hyperparameter sensitive DPO Directly train the Actor model using preference data \\(\\pi_\\theta\\), \\(\\pi_{\\mathrm{ref}}\\) Supervised-learning-like approach Simplified process; stable training; lower resource cost Performance ceiling may be lower than RLHF Mathematical Derivation of DPO RLHF Objective and the Optimal Policy Distribution In the alignment of large language models, our goal is to use RLHF to optimize model outputs. Let the input \\( x \\) be drawn from a dataset \\(\\mathcal{D}\\), and let the model generate a response \\( y \\). Denote the trainable model as \\(\\pi_\\theta(y \\mid x)\\) and the reference model as \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\) (typically the SFT model). We also introduce a reward function \\( r(x,y) \\) to measure the quality of a response. The RLHF objective can be written as\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\Big[ r(x,y) \\Big] \\;-\\; \\beta\\, \\mathbb{D}_{\\mathrm{KL}}\\Big[ \\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x) \\Big], \\tag{1} \\]where \\(\\beta\\) is a hyperparameter that balances the reward and the deviation from the reference model. Using the definition of KL divergence,\n\\[ \\mathbb{D}_{\\mathrm{KL}} \\Big[\\pi(y \\mid x) \\,\\|\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\Big] = \\mathbb{E}_{y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right], \\tag{2} \\]we can rewrite Equation (1) as\n\\[ \\max_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ r(x,y) - \\beta \\, \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} \\right]. \\tag{3} \\]Converting (3) to a minimization problem and dividing by \\(\\beta\\) yields\n\\[ \\min_{\\pi} \\; \\mathbb{E}_{x \\sim \\mathcal{D},\\, y \\sim \\pi(y \\mid x)} \\left[ \\log \\frac{\\pi(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} - \\frac{1}{\\beta} r(x,y) \\right]. \\tag{4} \\]Assuming there exists an optimal policy distribution \\(\\pi^*(y \\mid x)\\) that globally minimizes (4), we set\n\\[ \\pi^*(y \\mid x) \\;=\\; \\frac{1}{Z(x)} \\,\\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big), \\tag{5} \\]where the partition function \\( Z(x) \\) is defined as\n\\[ Z(x) = \\sum_{y}\\, \\pi_{\\mathrm{ref}}(y \\mid x)\\, \\exp\\!\\Big(\\frac{1}{\\beta} \\, r(x,y)\\Big). \\tag{6} \\] \\(Z(x)\\) sums over all possible \\(y\\) to normalize the distribution, ensuring that \\(\\pi^*(y \\mid x)\\) is a valid probability distribution. \\(Z(x)\\) is a function of \\(x\\) and is independent of the trainable Actor model \\(\\pi_\\theta\\). Taking the logarithm of (5) gives\n\\[ \\log \\pi^*(y \\mid x) = \\log \\pi_{\\mathrm{ref}}(y \\mid x) + \\frac{1}{\\beta}\\, r(x,y) - \\log Z(x), \\tag{7} \\]which can be rearranged to obtain\n\\[ r(x,y) = \\beta \\left[\\log \\frac{\\pi^*(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{8} \\]The Bradley–Terry Model To leverage pairwise preference data \\((x, y_w, y_l)\\) for training, we aim for the model to favor the higher-quality response \\( y_w \\) over the lower-quality response \\( y_l \\) for the same input \\( x \\).\nThe Bradley–Terry model is used to predict the outcomes of paired comparisons. For any two items \\( i \\) and \\( j \\), if we assign each a positive score \\( p_i \\) and \\( p_j \\), then the probability that item \\( i \\) is preferred over item \\( j \\) is\n\\[ \\Pr(i \u003e j) = \\frac{p_i}{p_i + p_j}. \\tag{9} \\]In our scenario, we set the strength parameter for each response \\( y \\) as \\( p_{y} = \\exp\\big(r(x,y)\\big) \\) (ensuring positivity). Therefore, given input \\( x \\), the probability that response \\( y_w \\) is preferred over \\( y_l \\) becomes\n\\[ P\\left(y_w \\succ y_l \\mid x\\right)=\\frac{\\exp \\big[r(x,y_w)\\big]}{\\exp \\big[r(x,y_w)\\big]+\\exp \\big[r(x,y_l)\\big]}. \\tag{10} \\]To maximize the probability that the higher-quality response \\( y_w \\) wins in every preference pair \\((x, y_w, y_l)\\) in the dataset, we design the reward model’s training objective to maximize this probability or, equivalently, to minimize the negative log-likelihood:\n\\[ L_{R}\\left(r_{\\phi}, D\\right) = -\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\left[\\log P\\left(y_w \\succ y_l \\mid x\\right)\\right], \\tag{11} \\]where the dataset is defined as\n\\[ D=\\{(x^i, y_w^i, y_l^i)\\}_{i=1}^{N}. \\tag{12} \\]Using Equations (10) and (11) along with the identity\n\\[ \\log \\frac{e^a}{e^a+e^b} = \\log\\frac{1}{1+e^{b-a}} = \\log \\sigma(a-b), \\tag{13} \\]with the Sigmoid function defined as\n\\[ \\sigma(z)=\\frac{1}{1+e^{-z}}, \\tag{14} \\]we have\n\\[ \\log P\\left(y_w \\succ y_l \\mid x\\right) = \\log \\sigma\\Big(r(x,y_w)-r(x,y_l)\\Big). \\tag{15} \\]Direct Preference Optimization Notice from Equation (8) that the reward \\( r(x,y) \\) is related to the log-ratio of the optimal policy. To avoid explicitly training a separate reward model \\(r_\\phi\\), DPO directly substitutes the trainable Actor model \\(\\pi_\\theta\\) in place of the optimal policy \\(\\pi^*\\) and represents the reward as\n\\[ r_\\theta(x,y) \\;=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_{\\mathrm{ref}}(y \\mid x)} + \\log Z(x)\\right]. \\tag{16} \\]In pairwise comparisons, for the same input \\( x \\), both responses \\( y_w \\) and \\( y_l \\) contain the same \\(\\log Z(x)\\) term; therefore, when computing the difference, this term cancels out:\n\\[ \\begin{aligned} r_\\theta(x,y_w)-r_\\theta(x,y_l) \u0026=\\; \\beta \\left[\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} + \\log Z(x)\\right] - \\beta \\left[\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} + \\log Z(x)\\right] \\\\ \u0026=\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)}. \\end{aligned} \\tag{17} \\]Substituting Equation (17) into (15) and combining with (11), we obtain the final DPO loss function:\n\\[ \\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}}) = - \\mathbb{E}_{(x,y_w,y_l) \\sim D} \\left[ \\log \\sigma\\Big( \\beta \\,\\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} -\\; \\beta \\,\\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big) \\right]. \\tag{18} \\]This loss function is designed for the trainable Actor model \\(\\pi_\\theta\\). It distinguishes between good and bad responses by comparing the log-probability ratios (relative to the reference model \\(\\pi_{\\mathrm{ref}}\\)) for the high-quality response \\(y_w\\) and the low-quality response \\(y_l\\): when \\(\\pi_\\theta\\) assigns a much higher relative probability to \\(y_w\\) compared to \\(y_l\\), the sigmoid output approaches 1 and the loss decreases; conversely, if the relative probability for \\(y_l\\) is too high, the loss increases. The parameter \\(\\beta\\) scales the difference between these log-ratios, thereby adjusting the model’s sensitivity in distinguishing good from bad responses.\nTraining Process Data Preparation Preference Data Collection\nCollect a large amount of preference data. Each sample includes an input \\( x \\) and a pair of responses \\((y_w, y_l)\\), where \\( y_w \\) is the chosen (preferred) response and \\( y_l \\) is the rejected (lower quality) response.\nDataset Example: dpo_en_demo.json\n{ \u0026#34;conversations\u0026#34;: [ { \u0026#34;from\u0026#34;: \u0026#34;human\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;what does abstraction mean on oop\u0026#34; } ], \u0026#34;chosen\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Abstraction in Object-Oriented Programming (OOP) is the process of exposing only the necessary information to the outside world while hiding the implementation details. It\u0026#39;s a way to simplify complex systems by focusing only on the essential aspects and encapsulating the implementation details.\\n\\nIn other words, abstraction is about hiding the internal workings of an object or system and only exposing the necessary interfaces or APIs to interact with it. This helps to reduce complexity, improve modularity, and make the system easier to understand and maintain.\\n\\nFor example, consider a car. A car has many components like engine, transmission, brakes, etc. However, when you drive a car, you don\u0026#39;t need to know how the engine works or how the transmission changes gears. All you need to know is how to operate the car\u0026#39;s controls like accelerator, brakes, and steering wheel. The car\u0026#39;s controls are an abstraction of the underlying complex systems, allowing you to interact with the car without needing to know the details of how it works.\\n\\nIn programming, abstraction is achieved by defining interfaces or abstract classes that define the essential functions or behaviors of an object without specifying how they are implemented. This allows developers to create different implementations of the same interface or abstract class, which can be swapped out without affecting the rest of the system.\\n\\nAbstraction is a fundamental concept in OOP and is used to create modular, maintainable, and flexible software systems. It helps to reduce code duplication, improve code reuse, and make software systems more understandable and easier to evolve over time.\u0026#34; }, \u0026#34;rejected\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;gpt\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Abstraction is a principle in object-oriented programming that refers to the process of focusing on the essential features of an object or concept without emphasizing the details that are not important for its functionality. It enables programmers to create classes that represent the objects they want to work with, and specifies only the behavior of those objects, leaving out unnecessary details. Abstraction helps to make the code more maintainable, modular, and scalable. It also improves the productivity of developers by reducing the amount of code they need to write.\u0026#34; } } Output Precomputation\nTo reduce computational overhead, pre-compute and cache the outputs of the reference model \\(\\pi_{\\mathrm{ref}}(y \\mid x)\\).\nModel Training Training Objective\nDirectly optimize the Actor model \\(\\pi_\\theta\\) by minimizing the DPO loss \\(\\mathcal{L}_{\\mathrm{DPO}}(\\pi_\\theta; \\pi_{\\mathrm{ref}})\\) so that its generated responses better align with human preferences.\nTraining Steps\nSample a batch of data \\((x, y_w, y_l)\\) from the dataset.\nCompute the output probabilities of the Actor model \\(\\pi_\\theta(y \\mid x)\\).\nCalculate the loss using:\n\\[ \\mathcal{L}_{\\mathrm{DPO}} = - \\log \\sigma\\Big( \\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\mathrm{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\mathrm{ref}}(y_l \\mid x)} \\Big). \\] Update the Actor model parameters \\(\\theta\\) via backpropagation.\nModel Inference Once training is complete, the resulting Actor model \\(\\pi_\\theta\\) can be used directly for inference. Given an input \\( x \\), the model generates responses based on the learned probability distribution. Since human preferences have been incorporated during training and the model is constrained by the reference model \\(\\pi_{\\mathrm{ref}}\\), the generated responses are not only aligned with expectations but also maintain stability in the generated text.\nSummary DPO simplifies the RLHF process into a direct supervised learning task, saving resources, enhancing training stability, and reducing implementation complexity. It serves as an efficient alternative for LLM alignment training. In practical applications, one can choose between RLHF and DPO methods based on the specific business scenario to achieve the best training results.\nReferences [1] Christiano, Paul F., et al. \u0026ldquo;Deep reinforcement learning from human preferences.\u0026rdquo; Advances in neural information processing systems 30 (2017).\n[2] Ouyang, Long, et al. \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in neural information processing systems 35 (2022): 27730-27744.\n[3] Rafailov, Rafael, et al. \u0026ldquo;Direct preference optimization: Your language model is secretly a reward model.\u0026rdquo; Advances in Neural Information Processing Systems 36 (2024).\nCitation Citation: When reprinting or citing the contents of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Feb 2025). LLMs Alignment: DPO.\nhttps://syhya.github.io/posts/2025-02-08-dpo\nOr\n@article{syhya2025dpo, title = \u0026#34;LLMs Alignment: DPO\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-02-08-dpo\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-02-08-dpo/","summary":"\u003cp\u003eThis blog post introduces a streamlined alternative to RLHF called DPO. Like RLHF, DPO is designed to align model outputs with human preferences, but it stands apart with its simplicity and lower resource demands. In scenarios where project resources are limited, DPO emerges as a highly attractive and practical solution worth exploring.\u003c/p\u003e\n\u003ch2 id=\"notations\"\u003eNotations\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eSymbol\u003c/th\u003e\n          \u003cth\u003e\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( x \\)\u003c/td\u003e\n          \u003ctd\u003eUser input (Prompt): the question the model needs to answer\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( y \\)\u003c/td\u003e\n          \u003ctd\u003eModel-generated response (Response / Completion): the text output by the model\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( \\pi_\\theta(y \\mid x) \\)\u003c/td\u003e\n          \u003ctd\u003eActor model: The trainable policy used to generate response \\(y\\); parameterized by \\(\\theta\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( \\pi_{\\mathrm{ref}}(y \\mid x) \\)\u003c/td\u003e\n          \u003ctd\u003eReference model: The frozen SFT (Supervised Fine-Tuning) model, serving as the alignment baseline\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( r_\\phi(x,y) \\)\u003c/td\u003e\n          \u003ctd\u003eReward model: A reward function (with parameter \\(\\phi\\)) used to evaluate the quality of response \\(y\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( V_\\psi(x) \\)\u003c/td\u003e\n          \u003ctd\u003eCritic model: A value function (with parameter \\(\\psi\\)) used to estimate the future cumulative reward given \\(x\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( \\pi^*(y \\mid x) \\)\u003c/td\u003e\n          \u003ctd\u003eOptimal policy distribution, determined via the reference model and reward function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( r_\\theta(x,y) \\)\u003c/td\u003e\n          \u003ctd\u003eReward derived from the Actor model, constructed from \\(\\pi_\\theta\\) and \\(\\pi_{\\mathrm{ref}}\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\(\\beta\\)\u003c/td\u003e\n          \u003ctd\u003eHyperparameter that controls the weight of the KL penalty or the log-ratio difference term\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\(\\mathbb{D}_{\\mathrm{KL}}[P \\| Q]\\)\u003c/td\u003e\n          \u003ctd\u003eKL divergence, a measure of the difference between probability distributions \\(P\\) and \\(Q\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\(\\sigma(z)\\)\u003c/td\u003e\n          \u003ctd\u003eSigmoid function, defined as: \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\(\\log\\)\u003c/td\u003e\n          \u003ctd\u003eLogarithm function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\(\\mathbb{E}\\)\u003c/td\u003e\n          \u003ctd\u003eExpectation operator, used to compute the average value of a random variable\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( (y_w, y_l) \\)\u003c/td\u003e\n          \u003ctd\u003eA pair of preference data where \\( y_w \\) is the preferred (better quality) response and \\( y_l \\) is the lesser one\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( P\\left(y_w \\succ y_l \\mid x\\right) \\)\u003c/td\u003e\n          \u003ctd\u003eThe probability that response \\( y_w \\) is preferred over \\( y_l \\) given input \\(x\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( Z(x) \\)\u003c/td\u003e\n          \u003ctd\u003ePartition function, which normalizes the probability distribution over all responses \\(y\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\\( \\mathcal{L}_{\\mathrm{DPO}} \\)\u003c/td\u003e\n          \u003ctd\u003eThe loss function of DPO\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"from-rlhf-to-dpo\"\u003eFrom RLHF to DPO\u003c/h2\u003e\n\u003ch3 id=\"rlhf\"\u003eRLHF\u003c/h3\u003e\n\u003cp\u003eOpenAI primarily leverages \u003cstrong\u003eReinforcement Learning from Human Feedback (RLHF)\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/1706.03741\"\u003eChristiano et al., 2017\u003c/a\u003e) to train InstructGPT (\u003ca href=\"https://arxiv.org/abs/2203.02155\"\u003eOuyang et al., 2022\u003c/a\u003e), which forms the basis for LLMs (such as ChatGPT, Llama, etc.). The entire training process generally comprises the following three main steps:\u003c/p\u003e","title":"LLMs Alignment: DPO"},{"content":"Introduction In deep learning, the design of network architectures significantly impacts model performance and training efficiency. As model depth increases, training deep neural networks faces numerous challenges, such as the vanishing and exploding gradient problems. To address these challenges, residual connections and various normalization methods have been introduced and are widely used in modern deep learning models. This article will first introduce residual connections and two architectures: pre-norm and post-norm. Then, it will describe four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze why current mainstream large models tend to adopt an architecture combining RMSNorm and Pre-Norm.\nResidual Connections Residual Connection is a crucial innovation in deep neural networks, forming the core of Residual Networks (ResNet) (He, et al., 2015). Residual connections are a significant architectural design aimed at mitigating the vanishing gradient problem in deep network training and facilitating information flow within the network. By introducing shortcut/skip connections, they allow information to pass directly from shallow layers to deeper layers, thereby enhancing the model\u0026rsquo;s representational capacity and training stability.\nFig. 1. Residual learning: a building block. (Image source: He, et al., 2015)\nIn a standard residual connection, the input $x_l$ undergoes a series of transformation functions $ \\text{F}(\\cdot) $ and is then added to the original input $x_l$ to form the output $x_{l+1}$:\n$$ x_{l+1} = x_l + \\text{F}(x_l) $$Where:\n$x_l$ is the input to the $l$-th layer. $\\text{F}(x_l)$ represents the residual function composed of a series of non-linear transformations (e.g., convolutional layers, fully connected layers, activation functions, etc.). $x_{l+1}$ is the output of the $(l+1)$-th layer. The structure using residual connections has several advantages:\nMitigation of Vanishing Gradients: By directly passing gradients through shortcut paths, it effectively reduces gradient decay in deep networks, making it easier to train deeper models. Facilitation of Information Flow: Shortcut paths allow information to flow more freely between network layers, helping the network learn more complex feature representations. Optimization of the Learning Process: Residual connections make the loss function surface smoother, optimizing the model\u0026rsquo;s learning process and making it easier to converge to a better solution. Improvement of Model Performance: In various deep learning tasks such as image recognition and natural language processing, models using residual connections typically exhibit superior performance. Pre-Norm vs. Post-Norm When discussing normalization methods, Pre-Norm and Post-Norm are two critical architectural design choices, particularly prominent in Transformer models. The following will detail the definitions, differences, and impacts of both on model training.\nDefinitions Fig. 2. (a) Post-LN Transformer layer; (b) Pre-LN Transformer layer. (Image source: Xiong, et al., 2020)\nFrom the figure above, we can intuitively see that the main difference between Post-Norm and Pre-Norm lies in the position of the normalization layer:\nPost-Norm: In traditional Transformer architectures, the normalization layer (such as LayerNorm) is typically placed after the residual connection.\n$$ \\text{Post-Norm}: \\quad x_{l+1} = \\text{Norm}(x_l + \\text{F}(x_l)) $$ Pre-Norm: Places the normalization layer before the residual connection.\n$$ \\text{Pre-Norm}: \\quad x_{l+1} = x_l + \\text{F}(\\text{Norm}(x_l)) $$ Comparative Analysis Feature Post-Norm Pre-Norm Normalization Position After residual connection Before residual connection Gradient Flow May lead to vanishing or exploding gradients, especially in deep models More stable gradients, helps in training deep models Training Stability Difficult to train deep models, requires complex optimization techniques Easier to train deep models, reduces reliance on learning rate scheduling Information Transfer Retains characteristics of the original input, aiding information transfer May cause compression or loss of input feature information Model Performance Performs better in shallow models or when strong regularization is needed Performs better in deep models, improves training stability and convergence speed Implementation Complexity Relatively straightforward to implement, but training may require more tuning Simple to implement, training process is more stable The differences between Pre-Norm and Post-Norm in model training can be understood from the perspective of gradient backpropagation:\nPre-Norm: Normalization operation is performed first, allowing gradients to be passed more directly to the preceding layers during backpropagation, reducing the risk of vanishing gradients. However, this may also weaken the actual contribution of each layer, reducing the effective depth of the model.\nPost-Norm: Normalization operation is performed last, helping to maintain the stability of each layer\u0026rsquo;s output, but in deep models, gradients may decay layer by layer, leading to training difficulties.\nThe DeepNet (Wang, et al., 2022) paper indicates that Pre-Norm is effective for training extremely deep Transformer models, while Post-Norm is difficult to scale to such depths.\nNormalization Methods In deep learning, there are numerous types of normalization methods, and different methods perform differently in various application scenarios. The following will detail four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze their advantages, disadvantages, and applicable scenarios.\nBatch Normalization Batch Normalization (Ioffe, et al., 2015) aims to alleviate the Internal Covariate Shift problem by standardizing the data of each batch, making its mean 0 and variance 1. Its mathematical expression is as follows:\n$$ \\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta $$Where:\n$x_i$ is the $i$-th sample in the input vector. $\\mu_{\\text{B}}$ is the mean of the current batch: $$ \\mu_{\\text{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i $$ where $m$ is the batch size. $\\sigma_{\\text{B}}^2$ is the variance of the current batch: $$ \\sigma_{\\text{B}}^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\left(x_i - \\mu_{\\text{B}}\\right)^2 $$ $\\epsilon$ is a very small constant used to prevent division by zero. $\\gamma$ and $\\beta$ are learnable scaling and shifting parameters. Advantages:\nAccelerated Training: Accelerates the convergence speed of the model through standardization. Regularization Effect: Reduces overfitting to some extent, decreasing the reliance on regularization techniques like Dropout. Mitigation of Vanishing Gradient Problem: Helps alleviate vanishing gradients, improving the training effect of deep networks. Disadvantages:\nNot Friendly to Small Batches: When the batch size is small, the estimation of mean and variance may be unstable, affecting the normalization effect. Batch Size Dependent: Requires a large batch size to obtain good statistical estimates, limiting its use in certain application scenarios. Complex Application in Certain Network Structures: Such as Recurrent Neural Networks (RNNs), requiring special handling to adapt to the dependency of time steps. Layer Normalization Layer Normalization (Ba, et al., 2016) normalizes across the feature dimension, making the features of each sample have the same mean and variance. Its mathematical expression is as follows:\n$$ \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta $$Where:\n$x$ is the input vector. $\\mu_{\\text{L}}$ is the mean across the feature dimension: $$ \\mu_{\\text{L}} = \\frac{1}{d} \\sum_{i=1}^{d} x_i $$ where $d$ is the size of the feature dimension. $\\sigma_{\\text{L}}^2$ is the variance across the feature dimension: $$ \\sigma_{\\text{L}}^2 = \\frac{1}{d} \\sum_{i=1}^{d} \\left(x_i - \\mu_{\\text{L}}\\right)^2 $$ $\\epsilon$ is a very small constant used to prevent division by zero. $\\gamma$ and $\\beta$ are learnable scaling and shifting parameters. Advantages:\nBatch Size Independent: Suitable for scenarios with small batch sizes or dynamic batch sizes, especially performing excellently in sequence models. Applicable to Various Network Structures: Performs well in Recurrent Neural Networks (RNNs) and Transformer models. Simplified Implementation: No need to rely on batch statistics, simplifying implementation in distributed training. Disadvantages:\nHigher Computational Cost: Compared to BatchNorm, the overhead of calculating mean and variance is slightly higher. May Not Improve Training Speed as Much as BatchNorm: In some cases, the effect of LayerNorm may not be as significant as BatchNorm. Weight Normalization Weight Normalization (Salimans, et al., 2016) decouples the norm and direction of the weight vector in neural networks by reparameterizing it, thereby simplifying the optimization process and accelerating training to some extent. Its mathematical expression is as follows:\n$$ w = \\frac{g}{\\lVert v \\rVert} \\cdot v $$$$ \\text{WeightNorm}(x) = w^T x + b $$Where:\n$w$ is the reparameterized weight vector. $g$ is a learnable scalar scaling parameter. $v$ is a learnable direction vector (with the same dimension as the original $w$). $\\lVert v \\rVert$ represents the Euclidean norm of $v$. $x$ is the input vector. $b$ is the bias term. Advantages:\nSimplified Optimization Objective: Separately controlling the norm and direction of weights helps accelerate convergence. Stable Training Process: In some cases, it can reduce gradient explosion or vanishing problems. Implementation Independent of Batch Size: Unrelated to the batch size of input data, broader applicability. Disadvantages:\nImplementation Complexity: Requires reparameterization of network layers, which may bring additional implementation costs. Caution Needed When Combined with Other Normalization Methods: When used in conjunction with BatchNorm, LayerNorm, etc., debugging and experimentation are needed to determine the best combination. RMS Normalization RMS Normalization (Zhang, et al., 2019) is a simplified normalization method that normalizes by only calculating the Root Mean Square (RMS) of the input vector, thereby reducing computational overhead. Its mathematical expression is as follows:\n$$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $$Where:\n$x$ is the input vector. $d$ is the size of the feature dimension. $\\epsilon$ is a very small constant used to prevent division by zero. $\\gamma$ is a learnable scaling parameter. Advantages:\nHigh Computational Efficiency: Compared to LayerNorm, which requires calculating both mean and variance, RMSNorm only needs to calculate the root mean square, reducing computational overhead. Training Stability: By normalizing the input, it improves the training stability of the model, allowing it to train stably even with larger learning rates. Resource Optimization: Reduced computational overhead helps deploy models in resource-constrained environments, improving training and inference efficiency. Simplified Implementation: RMSNorm is relatively simple to implement, making it easy to integrate and optimize in complex models, reducing the complexity of engineering implementation. Disadvantages:\nInformation Loss: Using only the root mean square for normalization may lose some information, such as mean information. Limited Applicability: In some tasks, it may not perform as well as BatchNorm or LayerNorm. Code Example You can refer to normalization.py\nComparison of Normalization Methods The following two tables compare the main characteristics of BatchNorm, LayerNorm, WeightNorm, and RMSNorm.\nBatchNorm vs. LayerNorm Feature BatchNorm (BN) LayerNorm (LN) Calculated Statistics Batch mean and variance Per-sample mean and variance Operation Dimension Normalizes across all samples in a batch Normalizes across all features for each sample Applicable Scenarios Suitable for large batch data, Convolutional Neural Networks (CNNs) Suitable for small batch or sequential data, RNNs or Transformers Batch Size Dependency Strongly dependent on batch size Independent of batch size, suitable for small batch or single-sample tasks Learnable Parameters Scaling parameter $ \\gamma $ and shifting parameter $ \\beta $ Scaling parameter $ \\gamma $ and shifting parameter $ \\beta $ Formula $ \\text{BatchNorm}(x_i) = \\gamma \\cdot \\frac{x_i - \\mu_{\\text{B}}}{\\sqrt{\\sigma_{\\text{B}}^2 + \\epsilon}} + \\beta $ $ \\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu_{\\text{L}}}{\\sqrt{\\sigma_{\\text{L}}^2 + \\epsilon}} + \\beta $ Computational Complexity Requires calculating batch mean and variance Requires calculating per-sample mean and variance Example Usage CNN, Vision Transformers RNN, Transformer, NLP WeightNorm vs. RMSNorm Feature WeightNorm (WN) RMSNorm (RMS) Calculated Statistics Decomposes weight vector into norm and direction Root Mean Square (RMS) of each sample Operation Dimension Reparameterizes along the dimension of the weight vector Normalizes across all features for each sample Applicable Scenarios Suitable for scenarios requiring more flexible weight control or accelerated convergence Suitable for tasks requiring efficient computation, such as RNNs or Transformers Batch Size Dependency Independent of batch size, unrelated to the dimension of input data Independent of batch size, suitable for small batch or single-sample tasks Learnable Parameters Scalar scaling $g$ and direction vector $v$ Scaling parameter $ \\gamma $ Formula $ \\text{WeightNorm}(x) = w^T x + b $ $ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}} \\cdot \\gamma $ Computational Complexity Reparameterization and update of parameters, slightly higher overhead, but requires modifying network layer implementation Only needs to calculate the root mean square of each sample, computationally efficient Example Usage Fully connected layers, convolutional layers, etc., in deep networks, improving training stability and convergence speed Transformer, NLP, efficient sequence tasks Through the above comparison, it can be seen that the four normalization methods have their own advantages and disadvantages:\nBatchNorm performs excellently in large batch data and convolutional neural networks but is sensitive to small batches. LayerNorm is suitable for various batch sizes, especially effective in RNNs and Transformers. WeightNorm simplifies the optimization process and accelerates convergence to some extent by reparameterizing the weight vector. RMSNorm provides a lightweight alternative in scenarios requiring efficient computation. Why do current mainstream LLMs use Pre-Norm and RMSNorm? In recent years, with the rise of large-scale language models (LLMs) such as GPT, LLaMA, and the Qwen series, RMSNorm and Pre-Norm have become the standard choices for these models.\nAdvantages of RMSNorm Fig. 3. RMSNorm vs. LayerNorm: A Comparison of Time Consumption (Image source: Zhang, et al., 2019)\nHigher Computational Efficiency\nReduced Operations: Only needs to calculate the Root Mean Square (RMS) of the input vector, without calculating mean and variance. Faster Training Speed: In actual tests, RMSNorm significantly shortens training time (as shown in the figure, reduced from 665s to 501s), which is particularly evident in large-scale model training. More Stable Training\nAdaptable to Larger Learning Rates: While maintaining stability, it can use larger learning rates, accelerating model convergence. Maintains Expressive Power: By simplifying the normalization process with an appropriate scaling parameter $ \\gamma $, it still maintains model performance. Resource Saving\nReduced Hardware Requirements: Less computational overhead not only improves speed but also reduces the occupation of hardware resources, suitable for deployment in resource-constrained environments. Advantages of Pre-Norm Easier to Train Deep Models\nStable Gradient Propagation: Performing normalization before residual connections can effectively alleviate gradient vanishing or explosion. Reduced Reliance on Complex Optimization Techniques: Even if the model is very deep, the training process remains stable. Accelerated Model Convergence\nEfficient Gradient Flow: Pre-Norm makes it easier for gradients to propagate to preceding layers, resulting in faster overall convergence speed. Conclusion Residual connections and normalization methods play crucial roles in deep learning models. Different normalization methods and network architecture designs have their own applicable scenarios, advantages, and disadvantages. By introducing residual connections, ResNet successfully trained extremely deep networks, significantly improving model expressiveness and training efficiency. Meanwhile, normalization methods such as BatchNorm, LayerNorm, WeightNorm, and RMSNorm each offer different advantages, adapting to different application needs.\nAs model scales continue to expand, choosing appropriate normalization methods and network architecture designs becomes particularly important. RMSNorm, due to its efficient computation and good training stability, combined with the Pre-Norm architecture design, has become the preferred choice for current mainstream LLMs. This combination not only improves the training efficiency of models but also ensures training stability and performance under large-scale parameters.\nReferences [1] He, Kaiming, et al. \u0026ldquo;Deep residual learning for image recognition.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n[2] Xiong, Ruibin, et al. \u0026ldquo;On layer normalization in the transformer architecture.\u0026rdquo; International Conference on Machine Learning. PMLR, 2020.\n[3] Wang, Hongyu, et al. \u0026ldquo;Deepnet: Scaling transformers to 1,000 layers.\u0026rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence (2024).\n[4] Ioffe, Sergey. \u0026ldquo;Batch normalization: Accelerating deep network training by reducing internal covariate shift.\u0026rdquo; arXiv preprint arXiv:1502.03167 (2015).\n[5] Ba, Jimmy Lei. \u0026ldquo;Layer normalization.\u0026rdquo; arXiv preprint arXiv:1607.06450 (2016).\n[6] Salimans, Tim, and Durk P. Kingma. \u0026ldquo;Weight normalization: A simple reparameterization to accelerate training of deep neural networks.\u0026rdquo; Advances in neural information processing systems 29 (2016).\n[7] Zhang, Biao, and Rico Sennrich. \u0026ldquo;Root mean square layer normalization.\u0026rdquo; Advances in Neural Information Processing Systems 32 (2019).\nCitation Citation: When reprinting or citing the content of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Feb 2025). Normalization in Deep Learning.\nhttps://syhya.github.io/posts/2025-02-01-normalization\nOr\n@article{syhya2025normalization, title = \u0026#34;Normalization in Deep Learning\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-02-01-normalization\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-02-01-normalization/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn deep learning, the design of network architectures significantly impacts model performance and training efficiency. As model depth increases, training deep neural networks faces numerous challenges, such as the vanishing and exploding gradient problems. To address these challenges, residual connections and various normalization methods have been introduced and are widely used in modern deep learning models. This article will first introduce residual connections and two architectures: pre-norm and post-norm. Then, it will describe four common normalization methods: Batch Normalization, Layer Normalization, Weight Normalization, and RMS Normalization, and analyze why current mainstream large models tend to adopt an architecture combining \u003cstrong\u003eRMSNorm\u003c/strong\u003e and \u003cstrong\u003ePre-Norm\u003c/strong\u003e.\u003c/p\u003e","title":"Normalization in Deep Learning"},{"content":" Note: This article is currently being updated. The content is in draft version and may change. Please check back for the latest version.\nNotations Symbol Meaning \\(s, s', S_t, S_{t+1}\\) State, next state, state at time \\(t\\), state at time \\(t+1\\) \\(o, o_t\\) Observation, observation at time \\(t\\) \\(a, a', A_t, A_{t+1}\\) Action, next action, action at time \\(t\\), action at time \\(t+1\\) \\(r, r_t\\) Immediate reward, reward at time \\(t\\) \\(G_t\\) Return at time \\(t\\) \\(R(\\tau)\\) Return of a trajectory \\(\\tau\\) \\(\\mathcal{S}\\) Set of all possible states \\(\\mathcal{A}\\) Set of all possible actions \\(\\mathcal{R}\\) Set of all possible rewards \\(\\pi(a\\mid s), \\pi_\\theta(a\\mid s)\\) Policy (stochastic), parameterized policy \\(\\mu(s), \\mu_\\theta(s)\\) Policy (deterministic), parameterized policy \\(\\theta, \\phi, w\\) Policy or value function parameters \\(\\gamma\\) Discount factor \\(J(\\pi)\\) Expected return of policy \\(\\pi\\) \\(V_\\pi(s)\\) State-value function for policy \\(\\pi\\) \\(Q_\\pi(s,a)\\) Action-value function for policy \\(\\pi\\) \\(V_*(s)\\) Optimal state-value function \\(Q_*(s,a)\\) Optimal action-value function \\(A_\\pi(s,a)\\) Advantage function for policy \\(\\pi\\) \\(P(s'\\mid s,a)\\) Transition probability function \\(R(s,a,s')\\) Reward function \\(\\rho_0(s)\\) Start-state distribution \\(\\tau\\) Trajectory \\(D\\) Replay memory \\(\\alpha\\) Learning rate, temperature parameter (in SAC) \\(\\lambda\\) Eligibility trace parameter \\(\\epsilon\\) Exploration parameter (e.g., in \\(\\epsilon\\)-greedy), clipping parameter (in PPO) What is Reinforcement Learning? Definition Reinforcement Learning (RL) is a branch of machine learning that trains an agent to take a series of actions (\\(a_t\\)) in an environment, transitioning through different states (\\(s_t\\)) to achieve a long-term3 goal.\nUnlike supervised learning, which relies on human-labeled data, RL depends on the interaction between the agent and the environment. After each action, the agent receives a reward (\\(r_t\\)) as feedback. The objective of the agent is to learn a policy $\\pi(s)$, which is a strategy for selecting actions, in order to maximize the total reward (\\(\\sum_{t=0}^{T} r_t\\)).\nApplications RL has achieved remarkable successes in various domains, including:\nGame Playing: Mastering complex games like Go (AlphaGo, AlphaGo Zero) and video games (Atari, Dota 2). Robotics: Controlling robots for tasks like navigation, manipulation, and locomotion. Autonomous Driving: Developing self-driving vehicles that can perceive their environment and make driving decisions. Resource Management: Optimizing resource allocation in areas like energy management and traffic control. Personalized Recommendations: Creating recommendation systems that adapt to user preferences over time. Policy A policy \\(\\pi\\) is the strategy an agent employs to decide which action to take in each state. It is the cornerstone of an RL agent, defining its behavior. Policies can be either deterministic or stochastic.\n$$ \\pi(s) = a $$ $$ \\pi(a \\mid s) = \\mathbb{P}_\\pi[A = a \\mid S = s] $$ In Deep Reinforcement Learning, policies are typically represented by sophisticated function approximators, such as neural networks. These networks, parameterized by weights \\(\\theta\\), learn to map states (or observations) to actions (or action probabilities). The parameterized policies are denoted as:\n\\(\\pi_\\theta(s)\\) for deterministic policies \\(\\pi_\\theta(a \\mid s)\\) for stochastic policies Trajectories A trajectory (also called episode) \\(\\tau\\) is a sequence of states and actions that unfold in an environment:\n\\[ \\tau = (s_0, a_0, s_1, a_1, \\ldots) \\]The initial state of the environment, \\(s_0\\), is sampled from a predefined start-state distribution, often represented as \\(\\rho_0\\):\n\\[ s_0 \\sim \\rho_0(\\cdot) \\]State transitions describe how the environment evolves from one state to the next, specifically from state \\(s_t\\) at time \\(t\\) to state \\(s_{t+1}\\) at time \\(t+1\\). These transitions are governed by the natural laws of the environment and are influenced solely by the most recent action taken \\(a_t\\). The transition dynamics can be either:\nDeterministic:\n\\[ s_{t+1} = f(s_t, a_t) \\] Stochastic:\n\\[ s_{t+1} \\sim P(\\cdot \\mid s_t, a_t) \\] Reward and Return In reinforcement learning, rewards and returns are fundamental concepts that guide an agent\u0026rsquo;s learning process by providing feedback on its actions.\nReward A reward is a scalar signal received by the agent after taking an action in a specific state. It serves as immediate feedback to indicate the desirability of the action taken. The reward function, denoted by \\( R \\), maps state-action pairs to real numbers:\n\\[ r_t = R(s_t, a_t) \\]where:\n\\( r_t \\) is the reward received at time step \\( t \\), \\( s_t \\) is the state at time step \\( t \\), \\( a_t \\) is the action taken at time step \\( t \\). The reward function encapsulates the goals of the agent by assigning higher rewards to desirable outcomes and lower (or negative) rewards to undesirable ones.\nReturn The return, often denoted by \\( G_t \\), represents the total accumulated future rewards from a specific time step onward. It quantifies the long-term benefit of actions taken by the agent. The return is defined as the sum of discounted rewards:\n\\[ G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} \\]where:\n\\( \\gamma \\in [0, 1) \\) is the discount factor that determines the present value of future rewards, \\( r_{t+k} \\) is the reward received \\( k \\) time steps after \\( t \\). The discount factor \\( \\gamma \\) balances the importance of immediate versus future rewards:\nA higher \\( \\gamma \\) (close to 1) makes the agent strive for long-term rewards. A lower \\( \\gamma \\) (close to 0) makes the agent prioritize immediate rewards. Why Use a Discount Factor? The discount factor \\( \\gamma \\) serves several important purposes in reinforcement learning:\nHandling Uncertainty of Future Rewards:\nHigher Uncertainty: Future rewards are often more uncertain than immediate rewards. For example, in the stock market, predicting long-term returns is more challenging due to market volatility. Preference for Immediate Benefits:\nHuman Behavior: As humans, we might prefer to enjoy rewards today rather than waiting for them years later. This preference is naturally modeled by discounting future rewards. Mathematical Convenience:\nFinite Computation: Discounting allows us to compute returns without needing to track future steps indefinitely, simplifying calculations and algorithms. Avoiding Infinite Loops:\nTermination Assurance: In environments with the possibility of infinite loops in state transitions, discounting ensures that the return remains finite, preventing issues with infinite sums. Finite Horizon Return In scenarios with a finite number of time steps \\( T \\), the return is calculated up to the terminal time step:\n\\[ G_t = \\sum_{k=0}^{T-t-1} \\gamma^k r_{t+k} \\]This is common in episodic tasks where the interaction between the agent and the environment terminates after a certain number of steps.\nTransition Function and Reward Function In Reinforcement Learning (RL), a model encapsulates the agent’s representation of how the environment behaves. This model typically includes two core components: the transition function and the reward function.\nTransition Function The transition function \\( P(s' \\mid s, a) \\) specifies the probability of moving from state \\( s \\) to state \\( s' \\) after taking action \\( a \\). Formally:\n\\[ P(s' \\mid s, a) = \\mathbb{P}(S_{t+1} = s' \\mid S_t = s, A_t = a) \\] Deterministic Environment: In a deterministic setting, the transition function assigns a probability of 1 to a single specific next state and 0 to all others:\n\\[ P(s' \\mid s, a) = \\begin{cases} 1 \u0026 \\text{if } s' = f(s, a),\\\\ 0 \u0026 \\text{otherwise}. \\end{cases} \\] Stochastic Environment: In a stochastic environment, the transition function defines a probability distribution over possible next states:\n\\[ P(s' \\mid s, a) = \\text{Probability of transitioning to } s' \\text{ from } s \\text{ by taking action } a. \\] Reward Function The reward function \\( R(s, a, s') \\) specifies the immediate reward obtained after transitioning from state \\( s \\) to state \\( s' \\) via action \\( a \\). It provides essential feedback that guides the agent\u0026rsquo;s learning. Formally:\n\\[ R(s, a, s') = \\mathbb{E}\\bigl[ R_{t+1} \\mid S_t = s, A_t = a, S_{t+1} = s' \\bigr] \\]Depending on the problem, the reward function might depend on:\nState-Action-State:\n\\[ R(s, a, s') = \\text{Immediate reward after transitioning from } s \\text{ to } s' \\text{ using } a. \\] State-Action:\n\\[ R(s, a) = \\mathbb{E}\\bigl[ R_{t+1} \\mid S_t = s, A_t = a \\bigr]. \\] State Only:\n\\[ R(s) = \\mathbb{E}\\bigl[ R_{t+1} \\mid S_t = s \\bigr]. \\] Value Function Value functions quantify the expected return (sum of discounted future rewards) starting from a given state (or state-action pair). They are central to most RL methods. There are two key types:\nState-Value Function The state-value function \\( V_\\pi(s) \\) measures the expected return when starting in state \\( s \\) and following policy \\( \\pi \\) thereafter:\n\\[ V_\\pi(s) = \\mathbb{E}_\\pi\\bigl[ G_t \\mid S_t = s \\bigr] = \\mathbb{E}_\\pi\\Bigl[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\;\\big\\vert\\; S_t = s \\Bigr]. \\]Action-Value Function (Q-function) The action-value function \\( Q_\\pi(s, a) \\) measures the expected return when starting in state \\( s \\), taking action \\( a \\), and thereafter following policy \\( \\pi \\):\n\\[ Q_\\pi(s, a) = \\mathbb{E}_\\pi\\bigl[ G_t \\mid S_t = s, A_t = a \\bigr] = \\mathbb{E}_\\pi\\Bigl[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\;\\big\\vert\\; S_t = s, A_t = a \\Bigr]. \\]Advantage Function The advantage function \\( A_\\pi(s, a) \\) indicates how much better (or worse) taking action \\( a \\) in state \\( s \\) is compared to the average action under policy \\( \\pi \\). It is defined as:\n\\[ A_\\pi(s, a) = Q_\\pi(s, a) - V_\\pi(s). \\]Significance of the Advantage Function Variance Reduction: In policy gradient methods, using the advantage function can reduce the variance of gradient estimates, leading to more stable learning. Policy Improvement: It highlights which actions are better or worse than the average in a given state, providing a clearer signal for policy updates. Optimal Value and Policy The ultimate goal in RL is to find an optimal policy \\( \\pi_* \\) that maximizes the expected return in the long run. Correspondingly, we define optimal value functions.\nOptimal State-Value Function The optimal state-value function \\( V_*(s) \\) is the maximum state-value attainable at state \\( s \\) over all possible policies:\n\\[ V_*(s) = \\max_{\\pi} V_\\pi(s). \\]Optimal Action-Value Function The optimal action-value function \\( Q_*(s, a) \\) is the maximum action-value attainable for the state-action pair \\( (s, a) \\) over all possible policies:\n\\[ Q_*(s, a) = \\max_{\\pi} Q_\\pi(s, a). \\]Optimal Policy An optimal policy \\( \\pi_* \\) is any policy that achieves these optimal value functions. Formally, for all \\( s \\) and \\( a \\):\n\\[ V_{\\pi_*}(s) = V_*(s), \\quad\\text{and}\\quad Q_{\\pi_*}(s, a) = Q_*(s, a). \\]Challenges in Finding the Optimal Policy Complexity: Finding \\( \\pi_* \\), \\( V_*(s) \\), or \\( Q_*(s, a) \\) can be computationally expensive, especially for large or continuous state and action spaces. Approximation: Many RL algorithms approximate these optimal functions or directly learn an (approximately) optimal policy through techniques such as dynamic programming, Monte Carlo methods, or temporal-difference learning. A Taxonomy of RL Algorithms Now that we’ve gone through the basics of RL terminology and notation, we can delve into the richer material: the landscape of algorithms in modern RL, along with the trade-offs involved in their design.\nOverview Creating an accurate and comprehensive taxonomy of modern RL algorithms is challenging due to the modularity and complexity of these algorithms. To keep this introduction digestible, we focus on the foundational design choices in deep RL algorithms, the trade-offs these choices entail, and contextualize some prominent modern algorithms within this framework.\nKey Insights from the RL Algorithms Taxonomy The comparative table provides a structured overview of various Reinforcement Learning (RL) approaches. Below, we break down the table contents into detailed key insights to better understand the distinctions, strengths, and challenges associated with each category of RL algorithms.\n1. Model-Free RL: Policy Optimization Key Idea:\nPolicy Optimization methods focus on directly learning a parameterized policy \\( \\pi_\\theta(a \\mid s) \\) by maximizing a performance objective \\( J(\\pi) \\). These methods often incorporate learning a value function \\( V^\\pi(s) \\) to facilitate more effective policy updates.\nOn/Off-Policy Setting:\nPrimarily on-policy, meaning updates are based on data collected from the most recent policy.\nStrengths:\nStability: By directly optimizing the policy objective, these methods tend to exhibit stable and reliable convergence. Simplicity in Updates: Utilizes gradient-based approaches, making the update process conceptually straightforward. Weaknesses:\nSample Inefficiency: Requires fresh interaction data from the current policy for each update, which can be resource-intensive. High Variance: Gradient estimates can exhibit high variance, potentially slowing down the learning process. Representative Examples:\nA2C, A3C, PPO, TRPO\n2. Model-Free RL: Q-Learning Key Idea:\nQ-Learning methods aim to learn an approximate Q-function \\( Q_\\theta(s, a) \\) that estimates the optimal action-value function \\( Q^*(s, a) \\). The policy is then derived by selecting actions that maximize the Q-values.\nOn/Off-Policy Setting:\nPrimarily off-policy, allowing the use of data collected from any policy during training.\nStrengths:\nSample Efficiency: Can reuse past experiences effectively, making better use of available data. Straightforward Objective: Relies on Bellman backups, providing a clear and direct learning target. Weaknesses:\nStability Issues: Susceptible to divergence and instability, especially when combined with function approximation. Indirect Performance Optimization: Optimizes the Q-function rather than the policy directly, which can complicate the learning process. Representative Examples:\nDQN, C51, QR-DQN\n3. Model-Based RL Key Idea:\nModel-Based methods involve using or learning a model of the environment\u0026rsquo;s dynamics (state transitions and rewards) to facilitate planning or generate additional training data.\nOn/Off-Policy Setting:\nCan be on-policy or off-policy depending on the specific algorithm design.\nStrengths:\nHigh Sample Efficiency: Leveraging a model allows for planning and generating synthetic data, reducing the need for extensive real-world interactions. Forward Planning: Enables the agent to \u0026ldquo;think ahead\u0026rdquo; by simulating future states and rewards, leading to more informed decision-making. Weaknesses:\nModel Bias: Inaccuracies in the learned model can lead to suboptimal or even detrimental policy performance in the real environment. Implementation Complexity: Incorporating a model adds layers of complexity, making these methods harder to implement and tune effectively. Representative Examples:\nMBMF, MBVE, AlphaZero, World Models\n4. Hybrid / In-Between Approaches Key Idea:\nHybrid methods blend elements from policy optimization, Q-Learning, and planning. For instance, they may learn both a Q-function and a policy simultaneously or embed planning mechanisms directly into the policy structure.\nOn/Off-Policy Setting:\nVaries across different algorithms; some are off-policy, others are on-policy, and some employ a mixed approach.\nStrengths:\nBalanced Strengths: Capable of harnessing the advantages of multiple RL paradigms, such as the stability of policy optimization and the sample efficiency of Q-Learning. Enhanced Data Utilization: Can effectively reuse data while maintaining stable policy updates, leading to improved overall performance. Weaknesses:\nImplementation Complexity: Managing multiple components (e.g., separate networks for policy and value functions) increases the complexity of the algorithm. Inherited Failure Modes: Risks arising from combining different methods can lead to compounded instability or other issues from each constituent approach. Representative Examples:\nDDPG, SAC, I2A (Imagination-Augmented Agents)\nSummary of Insights Diverse Strategies: RL algorithms can be broadly categorized into model-free and model-based approaches, each with distinct methodologies and trade-offs. Hybrid methods seek to combine these strategies to leverage their respective strengths.\nPolicy Optimization vs. Q-Learning:\nPolicy Optimization offers stability and direct optimization but at the cost of sample efficiency. Q-Learning provides greater sample efficiency through data reuse but may suffer from stability issues. Model-Based Advantages and Challenges:\nWhile model-based methods can significantly enhance sample efficiency and enable forward planning, they are often hindered by the difficulty of accurately modeling complex environments and the increased complexity of implementation. Hybrid Approaches as a Middle Ground:\nBy integrating aspects of both model-free and model-based methods, hybrid algorithms aim to achieve a balance between stability, sample efficiency, and performance. However, this integration introduces additional complexity and potential points of failure. Representative Algorithms:\nUnderstanding where prominent algorithms like PPO, DQN, AlphaZero, and SAC fit within this taxonomy helps in selecting the appropriate method based on the specific requirements and constraints of the task at hand. Others Markov Decision Processes (MDPs) In reinforcement learning, the interaction between an agent and its environment is often formalized as a Markov Decision Process (MDP). MDPs provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. An MDP is defined by a 5-tuple \\(\\langle \\mathcal{S}, \\mathcal{A}, P, R, \\gamma \\rangle\\):\n\\(\\mathcal{S}\\): A set of possible states. This is the set of all possible situations the agent can be in. \\(\\mathcal{A}\\): A set of possible actions. These are the actions the agent can take in each state. \\(P(s'\\mid s,a)\\): The transition probability function. It defines the probability of transitioning to a next state \\(s'\\) from the current state \\(s\\) when action \\(a\\) is taken. This function encapsulates the environment\u0026rsquo;s dynamics. \\(R(s,a,s')\\): The reward function. It defines the reward received by the agent after transitioning from state \\(s\\) to \\(s'\\) due to action \\(a\\). This function specifies the immediate feedback from the environment. \\(\\gamma \\in [0, 1]\\): The discount factor. It is a value between 0 and 1 that discounts future rewards. A discount factor closer to 0 makes the agent prioritize immediate rewards, while a factor closer to 1 makes it value future rewards more. The defining characteristic of an MDP is the Markov property, which states that the future state and reward depend only on the current state and action, and not on the history of past states and actions. Formally, for any time step \\(t\\):\n\\[ \\mathbb{P}\\bigl[S_{t+1} \\mid S_t, A_t\\bigr] = \\mathbb{P}\\bigl[S_{t+1} \\mid S_1, A_1, S_2, A_2, \\ldots, S_t, A_t\\bigr] \\]This property simplifies the problem significantly as the agent only needs to consider the current state to make optimal decisions, without needing to remember the entire history.\nIn an MDP, the agent\u0026rsquo;s goal is to find a policy \\(\\pi\\) that maximizes the expected cumulative discounted reward, starting from some initial state distribution \\(\\rho_0(s)\\). The sequence of states, actions, and rewards generated by an agent interacting with an MDP is called a trajectory or episode:\n\\[ \\tau = \\bigl(S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, \\ldots\\bigr) \\]The first state \\(S_0\\) is sampled from the start-state distribution \\(\\rho_0(\\cdot)\\). Subsequent states are determined by the transition probabilities \\(P(s'\\mid s,a)\\), and rewards are given by the reward function \\(R(s,a,s')\\). Actions \\(A_t\\) are chosen by the agent according to its policy \\(\\pi(a\\mid s)\\).\nBellman Equations Bellman equations are a set of equations that lie at the heart of dynamic programming and reinforcement learning. They decompose the value function into two parts: the immediate reward and the discounted value of the next state. These equations express a recursive relationship that value functions must satisfy. There are two main types of Bellman equations: Bellman Expectation Equations and Bellman Optimality Equations.\nBellman Expectation Equations\nBellman Expectation Equations are used for policy evaluation, i.e., calculating the value functions \\(V_\\pi(s)\\) and \\(Q_\\pi(s,a)\\) for a given policy \\(\\pi\\). They express the value of a state (or state-action pair) in terms of the expected immediate reward and the expected value of the next state, assuming the agent follows policy \\(\\pi\\). Bellman Expectation Equation for State-Value Function \\(\\bigl(V_\\pi(s)\\bigr)\\):\n\\[ V_\\pi(s) = \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_{t}=s\\bigr] \\]\\[ V_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\Bigl(R(s, a) + \\gamma V_\\pi(s')\\Bigr) \\] Derivation:\n\\[ \\begin{aligned} V_\\pi(s) \u0026= \\mathbb{E}_{\\pi}\\bigl[G_t \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma (R_{t+2} + \\gamma R_{t+3} + \\cdots) \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma G_{t+1} \\mid S_t=s\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_t=s\\bigr] \\\\ \u0026= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\bigl(R(s,a) + \\gamma V_\\pi(s')\\bigr) \\end{aligned} \\] Bellman Expectation Equation for Action-Value Function \\(\\bigl(Q_\\pi(s,a)\\bigr)\\):\n\\[ Q_\\pi(s,a) = \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma Q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t=s, A_t=a\\bigr] \\]\\[ Q_\\pi(s,a) = R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\mid s')\\, Q_\\pi(s',a') \\] Derivation:\n\\[ \\begin{aligned} Q_\\pi(s,a) \u0026= \\mathbb{E}_{\\pi}\\bigl[G_t \\mid S_t=s, A_t=a\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_t=s, A_t=a\\bigr] \\\\ \u0026= \\mathbb{E}_{\\pi}\\bigl[R_{t+1} + \\gamma \\mathbb{E}_{a' \\sim \\pi} Q_\\pi(S_{t+1}, a') \\mid S_t=s, A_t=a\\bigr] \\\\ \u0026= R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\mid s') Q_\\pi(s',a') \\end{aligned} \\] Bellman Optimality Equations\nBellman Optimality Equations specify the conditions for optimality for value functions \\(V_*(s)\\) and \\(Q_*(s,a)\\). They express the optimal value of a state (or state-action pair) in terms of the optimal values of successor states, assuming optimal actions are taken. Bellman Optimality Equation for Optimal State-Value Function \\(\\bigl(V_*(s)\\bigr)\\):\n\\[ V_*(s) = \\max_{a \\in \\mathcal{A}} \\mathbb{E}\\bigl[R_{t+1} + \\gamma V_*(S_{t+1}) \\mid S_t=s, A_t=a\\bigr] \\]\\[ V_*(s) = \\max_{a \\in \\mathcal{A}} \\Bigl(R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a)\\, V_*(s')\\Bigr) \\] Explanation: To achieve the optimal value in state \\(s\\), we should choose the action \\(a\\) that maximizes the sum of the immediate reward \\(R(s,a)\\) and the discounted optimal value of the next state \\(V_*(s')\\).\nBellman Optimality Equation for Optimal Action-Value Function \\(\\bigl(Q_*(s,a)\\bigr)\\):\n\\[ Q_*(s,a) = \\mathbb{E}\\bigl[R_{t+1} + \\gamma \\max_{a'} Q_*(S_{t+1}, a') \\mid S_t=s, A_t=a\\bigr] \\]\\[ Q_*(s,a) = R(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a)\\, \\max_{a' \\in \\mathcal{A}} Q_*(s',a') \\] Explanation: The optimal Q-value for a state-action pair \\((s,a)\\) is the immediate reward \\(R(s,a)\\) plus the discounted maximum Q-value achievable from the next state \\(s'\\), considering all possible actions \\(a'\\) in \\(s'\\).\nThese Bellman equations form the basis for many reinforcement learning algorithms, providing a way to compute and improve value functions and policies.\n2. Fundamental Approaches Dynamic Programming Dynamic Programming (DP) provides a collection of algorithms that can be used to compute optimal policies in MDPs, given a complete model of the environment. DP methods are particularly useful when the environment is fully known, meaning we have access to the transition probabilities \\(P(s'\\mid s,a)\\) and the reward function \\(R(s,a,s')\\). DP algorithms are based on the principle of optimality and utilize Bellman equations to find optimal policies and value functions.\nPolicy Evaluation (Prediction)\nPolicy Evaluation, also known as the prediction problem, aims to compute the state-value function \\(V_\\pi(s)\\) for a given policy \\(\\pi\\). It uses the Bellman Expectation Equation for \\(V_\\pi(s)\\) iteratively. Iterative Policy Evaluation Algorithm:\nInitialize \\(V_0(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). For each iteration \\(k+1\\): For each state \\(s \\in \\mathcal{S}\\): \\[ V_{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\sum_{s', r} P(s', r \\mid s, a)\\Bigl(r + \\gamma V_k(s')\\Bigr) \\] Repeat until convergence, i.e., until \\(V_{k+1}(s) \\approx V_k(s)\\) for all \\(s\\). Explanation: In each iteration, the value function for each state is updated based on the expected rewards and values of successor states, weighted by the policy and transition probabilities. This process is repeated until the value function converges, meaning the updates become very small.\nPolicy Improvement\nPolicy Improvement is the process of creating a better policy \\(\\pi'\\) from a given policy \\(\\pi\\). The idea is to act greedily with respect to the value function \\(V_\\pi\\) (or action-value function \\(Q_\\pi\\)) of the current policy. Greedy Policy Improvement: For each state \\(s \\in \\mathcal{S}\\), choose a new action \\(a'\\) that maximizes the action-value function \\(Q_\\pi(s,a)\\): \\[ \\pi'(s) = \\arg\\max_{a \\in \\mathcal{A}} Q_\\pi(s,a) \\] where \\[ Q_\\pi(s,a) = \\mathbb{E}\\bigl[R_{t+1} + \\gamma V_\\pi(S_{t+1}) \\mid S_t=s, A_t=a\\bigr] = \\sum_{s', r} P(s', r\\mid s, a)\\bigl(r + \\gamma V_\\pi(s')\\bigr). \\] Policy Improvement Theorem: If we improve the policy greedily with respect to \\(V_\\pi\\), the new policy \\(\\pi'\\) is guaranteed to be no worse than \\(\\pi\\), i.e., \\(V_{\\pi'}(s) \\ge V_\\pi(s)\\) for all \\(s \\in \\mathcal{S}\\). If improvement is strict for any state, then \\(\\pi'\\) is strictly better than \\(\\pi\\). Policy Iteration\nPolicy Iteration combines policy evaluation and policy improvement in an iterative process to find an optimal policy. It starts with an arbitrary policy, iteratively evaluates its value function, and then improves the policy based on this value function. Policy Iteration Algorithm: Initialization: Initialize a policy \\(\\pi_0\\) (e.g., randomly). Policy Evaluation: Compute the state-value function \\(V_{\\pi_k}\\) for the current policy \\(\\pi_k\\) using iterative policy evaluation. Policy Improvement: Create a new policy \\(\\pi_{k+1}\\) by acting greedily with respect to \\(V_{\\pi_k}\\): \\[ \\pi_{k+1}(s) = \\arg\\max_{a \\in \\mathcal{A}} Q_{\\pi_k}(s,a) \\] Iteration: Repeat steps 2 and 3 until policy improvement no longer yields a change in the policy, i.e., \\(\\pi_{k+1} = \\pi_k\\). At this point, the policy \\(\\pi_k\\) is guaranteed to be an optimal policy \\(\\pi_*\\), and \\(V_{\\pi_k} = V_*\\). Generalized Policy Iteration (GPI): The general idea of iteratively performing policy evaluation and policy improvement, which underlies many RL algorithms, including policy iteration and value iteration. Monte-Carlo Methods Monte-Carlo (MC) methods are a class of model-free reinforcement learning algorithms. Unlike Dynamic Programming, MC methods do not require a complete model of the environment. Instead, they learn directly from episodes of experience. An episode is a complete sequence of states, actions, and rewards from a start state to a terminal state. MC methods are used for both prediction (estimating value functions) and control (finding optimal policies).\nValue Estimation Monte Carlo methods estimate value functions by averaging the returns observed in actual or simulated episodes. The return \\(G_t\\) is the total discounted reward from time step \\(t\\) onwards in an episode: \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t} R_T, \\] where \\(T\\) is the terminal time step of the episode. Monte Carlo Value Estimation Algorithm:\nInitialize \\(V(s)\\) arbitrarily for all \\(s \\in \\mathcal{S}\\). Initialize \\(N(s) = 0\\) and \\(SumOfReturns(s) = 0\\) for all \\(s \\in \\mathcal{S}\\). For each episode: Generate an episode following policy \\(\\pi\\): \\(S_1, A_1, R_2, S_2, A_2, \\ldots, S_T\\). For each state \\(S_t\\) visited in the episode: \\[ N(S_t) \\leftarrow N(S_t) + 1 \\] \\[ SumOfReturns(S_t) \\leftarrow SumOfReturns(S_t) + G_t \\] \\[ V(S_t) = \\frac{SumOfReturns(S_t)}{N(S_t)} \\] Explanation: For each episode, we calculate the return \\(G_t\\) for each state \\(S_t\\) visited in that episode. We then average these returns across all episodes to estimate \\(V(s)\\). We can use either first-visit MC, where we only consider the first visit to a state in an episode, or every-visit MC, where we consider every visit. Action-Value Function Estimation: MC methods can be extended to estimate action-value functions \\(Q_\\pi(s,a)\\) similarly, by averaging returns following each state-action pair \\((s,a)\\).\nTemporal-Difference (TD) Learning Temporal-Difference (TD) learning is a class of model-free reinforcement learning methods that learn directly from raw experience without a model of the environment, similar to Monte Carlo methods. However, TD learning has a key advantage: it can learn from incomplete episodes by bootstrapping, meaning it updates value function estimates based on other estimates, without waiting for the final outcome of an episode. TD learning is central to modern reinforcement learning algorithms.\nBootstrapping\nBootstrapping is a core concept in TD learning. It means that TD methods update their estimates based in part on other estimates. In value function learning, TD methods update the value of a state based on the estimated value of the next state. This is in contrast to Monte Carlo methods, which wait until the end of an episode to calculate the actual return and use that as the target for updates. Bootstrapping allows TD learning to be more sample-efficient and to learn online, without needing to wait for the end of episodes. Value Estimation\nThe simplest TD method for prediction is TD(0), also known as one-step TD learning. TD(0) updates the value function \\(V(S_t)\\) towards a TD target, which is an estimate of the return based on the immediate reward \\(R_{t+1}\\) and the current estimate of the value of the next state \\(V(S_{t+1})\\). \\[ V(S_t) \\leftarrow V(S_t) + \\alpha \\Bigl(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\Bigr), \\] where \\(\\alpha\\) is the learning rate, controlling the step size of the update. The term \\(\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\\) is called the TD error, representing the difference between the TD target and the current estimate.\nExplanation: The TD(0) update moves the value function \\(V(S_t)\\) in the direction of the TD target. If the TD error is positive, it means the current estimate \\(V(S_t)\\) is lower than the TD target, so we increase \\(V(S_t)\\). If the TD error is negative, we decrease \\(V(S_t)\\).\nSARSA: On-Policy TD Control\nSARSA (State-Action-Reward-State-Action) is an on-policy TD control algorithm for learning action-value functions \\(Q_\\pi(s,a)\\). It learns the Q-function for the policy that is being used to explore the environment. \u0026ldquo;On-policy\u0026rdquo; means that SARSA learns about the policy it is currently following. SARSA Algorithm: Initialize \\(Q(s,a)\\) arbitrarily for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}\\). For each episode: Initialize state \\(S_t\\). Choose action \\(A_t\\) using a policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy). Repeat for each step of episode: Take action \\(A_t\\), observe reward \\(R_{t+1}\\) and next state \\(S_{t+1}\\). Choose next action \\(A_{t+1}\\) using policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy). Update Q-value for state-action pair \\((S_t, A_t)\\): [ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\Bigl(R_{t+1} \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\\Bigr). ] \\(S_t \\leftarrow S_{t+1}\\), \\(A_t \\leftarrow A_{t+1}\\). Until \\(S_{t+1}\\) is terminal. Explanation: SARSA updates the Q-value for the state-action pair \\((S_t, A_t)\\) based on the reward \\(R_{t+1}\\) and the Q-value of the next state-action pair \\((S_{t+1}, A_{t+1})\\). The action \\(A_{t+1}\\) is chosen using the same policy that is being evaluated and improved, hence \u0026ldquo;on-policy\u0026rdquo;. Q-Learning: Off-Policy TD Control\nQ-Learning is an off-policy TD control algorithm. It learns an estimate of the optimal action-value function \\(Q_*(s,a)\\), independent of the policy being followed. \u0026ldquo;Off-policy\u0026rdquo; means that Q-learning can learn about an optimal policy even while following a different, possibly exploratory, policy. Q-Learning Algorithm: Initialize \\(Q(s,a)\\) arbitrarily for all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}\\). For each episode: Initialize state \\(S_t\\). Repeat for each step of episode: Choose action \\(A_t\\) using a policy derived from \\(Q\\) (e.g., \\(\\epsilon\\)-greedy). Take action \\(A_t\\), observe reward \\(R_{t+1}\\) and next state \\(S_{t+1}\\). Update Q-value for state-action pair \\((S_t, A_t)\\): [ Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\Bigl(R_{t+1} \\gamma \\max_{a\u0026rsquo;} Q(S_{t+1}, a\u0026rsquo;) - Q(S_t, A_t)\\Bigr). ] \\(S_t \\leftarrow S_{t+1}\\). Until \\(S_{t+1}\\) is terminal. Explanation: Q-learning updates the Q-value for \\((S_t, A_t)\\) based on the reward \\(R_{t+1}\\) and the maximum Q-value achievable in the next state \\(S_{t+1}\\), regardless of which action is actually taken in \\(S_{t+1}\\). This \u0026ldquo;max\u0026rdquo; operation makes Q-learning off-policy because it learns about the optimal policy while potentially following a different behavior policy for exploration. Deep Q-Networks (DQN)\nDeep Q-Networks (DQN) is a groundbreaking algorithm that combines Q-learning with deep neural networks to handle high-dimensional state spaces, such as images. DQN addresses the instability and convergence issues that arise when using nonlinear function approximation (like neural networks) with bootstrapping and off-policy learning. DQN introduces two key techniques to stabilize Q-learning: Experience Replay: DQN stores transitions \\(\\bigl(s_t, a_t, r_t, s_{t+1}\\bigr)\\) in a replay memory (buffer) \\(D\\). Instead of updating Q-values online from sequential experiences, DQN samples mini-batches of transitions randomly from \\(D\\) to perform updates. This breaks the correlation between consecutive samples and smooths the data distribution over updates, improving stability. Periodically Updated Target Network: DQN uses two Q-networks: a Q-network \\(Q(s,a; \\theta)\\) with parameters \\(\\theta\\) that are being trained, and a target Q-network \\(Q(s,a; \\theta^-)\\) with parameters \\(\\theta^-\\) that are periodically updated to be the same as \\(\\theta\\) (e.g., every \\(C\\) steps) and kept frozen in between. The target network is used to compute the TD target in the Q-learning update, which stabilizes learning by reducing oscillations and divergence. DQN Loss Function: The loss function for DQN is the mean squared error between the TD target and the current Q-value: \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim U(D)}\\Bigl[\\bigl(y - Q(s,a;\\theta)\\bigr)^2\\Bigr], \\] where \\[ y = r + \\gamma \\max_{a'} Q(s',a'; \\theta^-) \\] is the TD target, and \\(\\bigl(s,a,r,s'\\bigr)\\) is a transition sampled from the replay memory \\(D\\). \\(U(D)\\) denotes uniform sampling from \\(D\\). DQN Algorithm Outline: Initialize replay memory \\(D\\) to capacity \\(N\\). Initialize Q-network \\(Q\\) with random weights \\(\\theta\\). Initialize target Q-network \\(\\hat{Q}\\) with weights \\(\\theta^- = \\theta\\). For each episode: Initialize state \\(s_1\\). For \\(t = 1, \\ldots, T\\): Select action \\(a_t\\) using \\(\\epsilon\\)-greedy policy based on \\(Q(s_t,\\cdot;\\theta)\\). Execute action \\(a_t\\), observe reward \\(r_t\\) and next state \\(s_{t+1}\\). Store transition \\(\\bigl(s_t,a_t,r_t,s_{t+1}\\bigr)\\) in \\(D\\). Sample random mini-batch of transitions from \\(D\\). Compute TD targets \\(y_j = r_j + \\gamma \\max_{a'} \\hat{Q}(s_j',a';\\theta^-)\\) (or \\(y_j = r_j\\) if episode terminates). Perform gradient descent step to minimize \\[ \\mathcal{L}(\\theta) = \\mathbb{E}_j\\Bigl[\\bigl(y_j - Q(s_j,a_j;\\theta)\\bigr)^2\\Bigr] \\] with respect to \\(\\theta\\). Every \\(C\\) steps, reset \\(\\theta^- = \\theta\\). Combining TD and Monte-Carlo Learning n-step TD Learning\nn-step TD learning methods bridge the gap between one-step TD learning (like TD(0)) and Monte Carlo methods by looking ahead \\(n\\) steps to estimate the return. The n-step return \\(G_t^{(n)}\\) is defined as: \\[ G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n}). \\] For \\(n=1\\), \\(G_t^{(1)} = R_{t+1} + \\gamma V(S_{t+1})\\), which is the TD target. As \\(n \\to \\infty\\), \\(G_t^{(n)} \\to G_t\\), the Monte Carlo return. n-step TD Update Rule: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha \\bigl(G_t^{(n)} - V(S_t)\\bigr). \\] Benefits of n-step TD: Balances bias and variance: n-step returns have lower variance than MC returns and lower bias than 1-step TD targets. Can learn faster than MC methods and be more stable than 1-step TD methods in some environments. TD(\\(\\lambda\\))\nTD(\\(\\lambda\\)) methods generalize n-step TD learning by averaging n-step returns over all possible values of \\(n\\), weighted by a factor \\(\\lambda^{n-1}\\). The \\(\\lambda\\)-return \\(G_t^{(\\lambda)}\\) is defined as: \\[ G_t^{(\\lambda)} = (1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} G_t^{(n)}, \\] where \\(\\lambda \\in [0,1]\\) is the trace-decay parameter. When \\(\\lambda = 0\\), \\(G_t^{(\\lambda)} = G_t^{(1)}\\), which reduces to TD(0). When \\(\\lambda = 1\\), \\(G_t^{(\\lambda)} = G_t^{(\\infty)} = G_t\\), which becomes the Monte Carlo return. TD(\\(\\lambda\\)) Update Rule: \\[ V(S_t) \\leftarrow V(S_t) + \\alpha \\bigl(G_t^{(\\lambda)} - V(S_t)\\bigr). \\] Eligibility Traces: TD(\\(\\lambda\\)) can be implemented efficiently using eligibility traces, which provide a mechanism to assign credit to past states and actions for observed rewards. Eligibility traces maintain a short-term memory of visited states or state-action pairs, allowing updates to propagate back through time more efficiently than with n-step returns alone. Policy Gradient Methods Policy Gradient methods are a class of model-free reinforcement learning algorithms that directly learn and optimize the policy \\(\\pi_\\theta(a\\mid s)\\) without explicitly learning a value function (though value functions are often used to assist policy gradient methods, as in Actor-Critic methods). Policy gradient methods directly search for an optimal policy by optimizing the policy parameters \\(\\theta\\) using gradient ascent on the expected return \\(J(\\theta)\\).\nPolicy Gradient Theorem \\[ \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\Bigl[\\nabla_{\\theta} \\ln \\pi(a\\mid s,\\theta)\\, Q_\\pi(s,a)\\Bigr]. \\] This equation is fundamental to policy gradient methods. It shows that the gradient of the performance objective can be estimated by averaging over trajectories sampled from the policy, where for each time step in a trajectory, we compute the product of the action-value function \\(Q_\\pi(s,a)\\) and the gradient of the log-policy \\(\\nabla_{\\theta} \\ln \\pi(a\\mid s,\\theta)\\).\nProof of Policy Gradient Theorem: (Detailed derivation already covered in the \u0026ldquo;Policy Gradient Theorem\u0026rdquo; section above.)\nAdvanced Policy Gradient Algorithms REINFORCE\nREINFORCE, also known as Monte Carlo Policy Gradient, is a basic policy gradient algorithm that directly implements the policy gradient theorem. It uses Monte Carlo estimates of the action-value function \\(Q_\\pi(s,a)\\) to update the policy parameters. In REINFORCE, the return \\(G_t\\) from an entire episode is used as an unbiased estimate of \\(Q_\\pi(S_t,A_t)\\). REINFORCE Algorithm: Initialize policy parameters \\(\\theta\\) randomly. For each episode: Generate an episode trajectory \\(\\tau = (S_1, A_1, R_2, S_2, A_2, \\ldots, S_T)\\) following policy \\(\\pi_\\theta\\). For each time step \\(t = 1, 2, \\ldots, T\\): Calculate the return \\[ G_t = \\sum_{k=0}^{T-t} \\gamma^k R_{t+k+1}. \\] Update policy parameters using gradient ascent: \\[ \\theta \\leftarrow \\theta + \\alpha\\, G_t\\, \\nabla_{\\theta} \\ln \\pi\\bigl(A_t \\mid S_t,\\theta\\bigr). \\] Explanation: REINFORCE updates the policy parameters in the direction that increases the probability of actions that led to higher returns in the episode. The gradient update is proportional to \\(G_t\\) and \\(\\nabla_{\\theta} \\ln \\pi(A_t\\mid S_t,\\theta)\\). Actor-Critic Variants\nA2C (Advantage Actor-Critic): Advantage Actor-Critic (A2C) is the synchronous, deterministic counterpart to Asynchronous Advantage Actor-Critic (A3C). A2C runs multiple agents in parallel environments and waits for all agents to complete their steps before performing a synchronized update of the global network. This synchronous update often leads to more stable and efficient learning, especially when using GPUs for training. A2C typically uses the advantage function \\(A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)\\) to reduce variance and improve the gradient signal. A3C (Asynchronous Advantage Actor-Critic): Asynchronous Advantage Actor-Critic (A3C) is a parallel, asynchronous framework where multiple independent agents (actors) run in parallel environments to collect experience and update a shared global network asynchronously. A3C is known for its efficiency in utilizing multi-core CPUs. It typically uses the advantage function \\(A_\\pi(s,a) = Q_\\pi(s,a) - V_\\pi(s)\\) to reduce variance in policy gradient updates. Off-Policy Policy Gradient\nOff-Policy Policy Gradient methods learn a policy using data generated by a different policy, called the behavior policy, rather than the policy being optimized, called the target policy. This is in contrast to on-policy methods, where the behavior policy and target policy are the same. Off-policy methods offer several advantages: Sample Efficiency: They can reuse past experiences stored in a replay buffer, making learning more data-efficient. Exploration: They can use a behavior policy that is more exploratory than the target policy, facilitating better exploration of the environment. Flexibility: They allow learning from diverse datasets, including data collected from human experts or other agents. Importance Sampling: A key technique in off-policy policy gradient methods is importance sampling. It is used to correct for the mismatch between the distribution of data generated by the behavior policy and the distribution under the target policy. For policy gradient, importance sampling weights are used to adjust the gradient estimates. For example, in off-policy policy gradient updates, the gradient is weighted by the ratio \\(\\frac{\\pi_\\theta(a\\mid s)}{\\beta(a\\mid s)}\\), where \\(\\beta\\) is the behavior policy. Advantage Actor-Critic (A2C)\nAdvantage Actor-Critic (A2C) is the synchronous counterpart to A3C, running multiple agents in parallel environments and performing synchronized updates. It uses the advantage function to reduce variance in the policy gradient estimates, leading to more stable and efficient learning. Deterministic Policy Gradient (DPG)\nDeterministic Policy Gradient (DPG) is a policy gradient algorithm designed for continuous action spaces. Unlike stochastic policy gradient methods that learn a probability distribution over actions, DPG learns a deterministic policy \\(\\mu_\\theta(s)\\) that directly outputs a specific action for each state. DPG is based on the Deterministic Policy Gradient Theorem, which provides a way to compute the gradient of the performance objective for deterministic policies. Deterministic Policy Gradient Theorem: \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho^\\mu}\\bigl[\\nabla_a Q_\\mu(s,a)\\,\\nabla_\\theta \\mu_\\theta(s)\\bigr] \\Bigm|_{a = \\mu_\\theta(s)}, \\] where \\(\\rho^\\mu(s)\\) is the state distribution under the deterministic policy \\(\\mu_\\theta(s)\\), and \\(Q_\\mu(s,a)\\) is the action-value function for policy \\(\\mu_\\theta(s)\\). Deep Deterministic Policy Gradient (DDPG)\nDeep Deterministic Policy Gradient (DDPG) is an off-policy actor-critic algorithm that combines DPG with techniques from DQN to enable deep reinforcement learning in continuous action spaces. DDPG is essentially a deep learning version of DPG, incorporating experience replay and target networks to stabilize training. Key Features of DDPG: Actor-Critic Architecture: Uses two neural networks: an actor network \\(\\mu_\\theta(s)\\) and a critic network \\(Q_w(s,a)\\). Experience Replay: Stores transitions in a replay buffer and samples mini-batches for updates, similar to DQN. Target Networks: Uses target networks for both actor \\(\\mu_{\\theta^-}(s)\\) and critic \\(Q(s,a; w^-)\\), updated slowly by soft updates. Deterministic Policy: Learns a deterministic policy, well-suited for continuous action spaces. Exploration: Uses Ornstein-Uhlenbeck process or Gaussian noise added to actions for exploration. Distributed Distributional DDPG (D4PG)\nDistributed Distributional DDPG (D4PG) is an extension of DDPG that incorporates: Distributional Critic: Learns a distribution over returns instead of a single Q-value. N-step Returns: Uses n-step TD targets to reduce variance. Multiple Distributed Parallel Actors: Collects experience in parallel, improving data throughput. Prioritized Experience Replay (PER): Samples transitions with probabilities proportional to their TD errors. Multi-Agent DDPG (MADDPG)\nMulti-Agent DDPG (MADDPG) extends DDPG to multi-agent environments, addressing non-stationarity where multiple agents learn simultaneously. Centralized Critic, Decentralized Actors: Each agent’s critic has access to global information, while each actor only sees its local observations. Learning with Other Agents’ Policies: Helps handle changing dynamics as other agents learn. Trust Region Policy Optimization (TRPO)\nTrust Region Policy Optimization (TRPO) is an on-policy policy gradient algorithm that ensures monotonic policy improvement by constraining the size of each policy update, measured by KL divergence. TRPO Objective: \\[ \\max_{\\theta}\\,\\mathbb{E}_{s \\sim \\rho^{\\pi_{\\theta_\\text{old}}},\\,a \\sim \\pi_{\\theta_\\text{old}}} \\Bigl[\\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_\\text{old}}(a\\mid s)}\\,\\hat{A}_{\\theta_\\text{old}}(s,a)\\Bigr] \\] subject to a KL-divergence constraint \\(\\le \\delta\\). Explanation: TRPO keeps the new policy \\(\\pi_\\theta\\) within a \u0026ldquo;trust region\u0026rdquo; of the old policy \\(\\pi_{\\theta_\\text{old}}\\), preventing large, destabilizing updates. Proximal Policy Optimization (PPO)\nProximal Policy Optimization (PPO) simplifies TRPO and is more practical to implement while maintaining similar performance. PPO uses a clipped surrogate objective to bound the policy update. PPO Clipped Objective: \\[ J^{\\mathrm{CLIP}}(\\theta) = \\mathbb{E}\\Bigl[\\min\\bigl(r(\\theta)\\,\\hat{A}_{\\theta_\\text{old}}(s,a),\\,\\mathrm{clip}\\bigl(r(\\theta),1-\\epsilon,1+\\epsilon\\bigr)\\,\\hat{A}_{\\theta_\\text{old}}(s,a)\\bigr)\\Bigr], \\] where \\(r(\\theta) = \\frac{\\pi_\\theta(a\\mid s)}{\\pi_{\\theta_\\text{old}}(a\\mid s)}\\). Explanation: By clipping the ratio \\(r(\\theta)\\), PPO avoids excessively large updates that could harm performance. Phasic Policy Gradient (PPG)\nPhasic Policy Gradient (PPG) is an on-policy algorithm that separates policy and value function updates into distinct phases, improving sample efficiency and stability. Policy Phase: Optimizes a PPO-like objective. Auxiliary Phase: Improves the value function and keeps policy from drifting too far. Benefits: More stable updates and better sample reuse compared to PPO. Actor-Critic with Experience Replay (ACER)\nActor-Critic with Experience Replay (ACER) is an off-policy actor-critic algorithm combining experience replay, Retrace for stable off-policy Q-value estimation, truncated importance weights with bias correction, and an efficient TRPO-like update. Key Ideas: Off-policy correction via truncated importance sampling. Retrace for unbiased and low-variance Q-value estimation. Trust region updates for stable learning. Actor-Critic using Kronecker-Factored Trust Region (ACKTR)\nActor-Critic using Kronecker-Factored Trust Region (ACKTR) leverages Kronecker-Factored Approximate Curvature (K-FAC) for more efficient and stable updates. K-FAC: Approximates the Fisher information matrix via Kronecker products for natural gradient updates. Natural Gradient: Helps optimize in directions that consider curvature, often improving convergence speed and stability. Soft Actor-Critic (SAC)\nSoft Actor-Critic (SAC) is an off-policy actor-critic algorithm featuring maximum entropy RL. It maximizes both return and policy entropy, encouraging exploration and robustness. Maximum Entropy Objective: \\[ J(\\pi) = \\sum_{t=1}^{T} \\mathbb{E}\\bigl[r(s_t,a_t) + \\alpha\\,\\mathcal{H}\\bigl(\\pi_\\theta(\\cdot\\mid s_t)\\bigr)\\bigr], \\] where \\(\\alpha\\) is a temperature parameter. Soft Q-function and Soft Value Function: Incorporates entropy terms into the Bellman backup. Automatic Temperature Adjustment: \\(\\alpha\\) can be learned to balance exploration and exploitation. Twin Delayed DDPG (TD3)\nTwin Delayed DDPG (TD3) is an improvement over DDPG that addresses overestimation bias and improves stability. Clipped Double Q-learning: Uses two critics and takes the minimum of both Q-values to reduce overestimation. Delayed Policy Updates: Updates the policy less frequently than the critics. Target Policy Smoothing: Adds noise to target actions for smoother Q-value estimates. Stein Variational Policy Gradient (SVPG)\nStein Variational Policy Gradient (SVPG) uses Stein Variational Gradient Descent (SVGD) to maintain an ensemble of policy \u0026ldquo;particles,\u0026rdquo; encouraging diversity. SVGD: Iteratively updates a set of particles to approximate a target distribution. Kernel Function: Encourages particles to spread out, improving exploration. Importance Weighted Actor-Learner Architectures (IMPALA)\nImportance Weighted Actor-Learner Architectures (IMPALA) is a highly scalable RL framework with decoupled actors and learners, using V-trace to handle off-policy corrections. Decoupled Architecture: Multiple actors generate data in parallel, sending trajectories to a central learner. V-trace: Corrects off-policy data from actors running slightly outdated policies. High Throughput: Achieves efficient large-scale training in complex environments. ","permalink":"https://syhya.github.io/posts/rl-introduciton/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: This article \u003cstrong\u003eis currently being updated\u003c/strong\u003e. The content is in \u003cstrong\u003edraft version\u003c/strong\u003e and may change. Please check back for the latest version.\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"notations\"\u003eNotations\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003eSymbol\u003c/th\u003e\n          \u003cth style=\"text-align: left\"\u003eMeaning\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(s, s', S_t, S_{t+1}\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eState, next state, state at time \\(t\\), state at time \\(t+1\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(o, o_t\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eObservation, observation at time \\(t\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(a, a', A_t, A_{t+1}\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eAction, next action, action at time \\(t\\), action at time \\(t+1\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(r, r_t\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eImmediate reward, reward at time \\(t\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(G_t\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eReturn at time \\(t\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(R(\\tau)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eReturn of a trajectory \\(\\tau\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\mathcal{S}\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eSet of all possible states\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\mathcal{A}\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eSet of all possible actions\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\mathcal{R}\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eSet of all possible rewards\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\pi(a\\mid s), \\pi_\\theta(a\\mid s)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ePolicy (stochastic), parameterized policy\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\mu(s), \\mu_\\theta(s)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ePolicy (deterministic), parameterized policy\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\theta, \\phi, w\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003ePolicy or value function parameters\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\gamma\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eDiscount factor\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(J(\\pi)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eExpected return of policy \\(\\pi\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(V_\\pi(s)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eState-value function for policy \\(\\pi\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(Q_\\pi(s,a)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eAction-value function for policy \\(\\pi\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(V_*(s)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eOptimal state-value function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(Q_*(s,a)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eOptimal action-value function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(A_\\pi(s,a)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eAdvantage function for policy \\(\\pi\\)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(P(s'\\mid s,a)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eTransition probability function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(R(s,a,s')\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eReward function\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\rho_0(s)\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eStart-state distribution\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\tau\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eTrajectory\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(D\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eReplay memory\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\alpha\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eLearning rate, temperature parameter (in SAC)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\lambda\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eEligibility trace parameter\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e\\(\\epsilon\\)\u003c/td\u003e\n          \u003ctd style=\"text-align: left\"\u003eExploration parameter (e.g., in \\(\\epsilon\\)-greedy), clipping parameter (in PPO)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2 id=\"what-is-reinforcement-learning\"\u003eWhat is Reinforcement Learning?\u003c/h2\u003e\n\u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n\u003cp\u003e\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/posts/rl-introduciton/image.png\"\u003e\u003c/p\u003e","title":"Deep Reinforcement Learning (Ongoing Updates)"},{"content":"DeepSeek AI recently released DeepSeek-R1 (DeepSeek-AI, 2025), whose reasoning performance on multiple benchmarks approaches the level of OpenAI\u0026rsquo;s o1 (OpenAI, 2024), marking a significant step for the open-source community in successfully replicating o1. Relevant code for R1 can be found in the huggingface\u0026rsquo;s attempt to open-source replication project open-r1. While previous research has often relied on massive amounts of supervised data to enhance the performance of Large Language Models (LLMs), the success of DeepSeek-R1 and its earlier experiment, DeepSeek-R1-Zero, powerfully demonstrates the potential of purely large-scale reinforcement learning in improving the reasoning capabilities of LLMs. This success reinforces the profound insight proposed by Richard Sutton in \u0026ldquo;The Bitter Lesson\u0026rdquo;:\nOne thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning. (Richard Sutton, 2019)\nNotations The following table lists the mathematical symbols used in this article to facilitate your reading.\nSymbol Meaning \\( q \\) or \\( Q \\) Question, user input or instruction \\( o \\) or \\( O \\) Output, model-generated text response or answer \\( t \\) Token index, indicating the position of the \\( t \\)-th token in the output text \\( o_t \\) The \\( t \\)-th token in the output text \\( o \\) \\( o_{\u0026lt;t} \\) The sequence of tokens in the output text \\( o \\) preceding the \\( t \\)-th token \\( \u0026#124;o\u0026#124; \\) Length of the output text \\( o \\), usually referring to the number of tokens \\( G \\) Output group size, in the GRPO algorithm, the number of outputs sampled for each question \\( \\pi_\\theta, \\pi_{\\theta_{\\text{old}}}, \\pi_{\\text{ref}}, \\pi_{\\text{sft}} \\) Policy models and their variants, used to generate text outputs or as reference models \\( A_t, A_i \\) Advantage function and relative advantage value \\( \\varepsilon \\) Clipping hyperparameter, used to limit the range of the importance sampling, ensuring the stability of policy updates \\( \\beta \\) Regularization coefficient, used to control the weight of the KL divergence penalty term in the objective function \\( \\mathbb{D}_{KL} \\) KL divergence, a measure of the difference between two probability distributions, used to constrain the distance between the new policy and the reference policy \\( \\mathcal{J}, \\mathcal{L} \\) Objective function and loss function \\( \\mathbb{E} \\) Expectation, representing the average value of a random variable, in the objective function, it represents the average over sample data \\( P_{\\text{sft}}(Q, O) \\) Distribution of the SFT dataset, representing the joint probability distribution of question \\( Q \\) and output \\( O \\) in the \\( SFT \\) dataset \\( P_{\\text{sft}}(Q) \\) Distribution of questions in the SFT dataset, representing the marginal probability distribution of question \\( Q \\) in the \\( SFT \\) dataset \\( \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\) Conditional probability of the policy model generating the \\( t \\)-th token \\( o_t \\) given the question \\( q \\) and previously generated tokens \\( o_{\u0026lt;t} \\) \\( \\mathbb{I}(o) \\) Indicator function that determines whether the output \\( o \\) is of high quality, 1 if high quality, 0 otherwise \\( r(o) \\) Reward function, a function that evaluates the quality of the model output \\( o \\) \\( r_i \\) Reward value of the \\( i \\)-th output \\( \\nabla_{\\theta} \\) Gradient operator, representing the gradient of a function with respect to model parameters \\( \\theta \\) \\( \\mathcal{N}(\\mu, 1) \\) Normal distribution with mean \\( \\mu \\) and standard deviation 1 \\( \\binom{a}{b} \\) Binomial coefficient, representing the number of combinations of choosing \\( b \\) elements from \\( a \\) elements \\( r(o) = \\frac{\\pi_{\\text{ref}}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\) Probability ratio, the ratio of the probability of generating output \\( o \\) by the reference model to the current policy model Training Process Overview The training of the DeepSeek-R1 series models is a multi-stage process aimed at building LLMs with superior reasoning and general language capabilities. The entire training process starts from the DeepSeek-V3 (DeepSeek-AI, 2024) model and iteratively optimizes it to obtain different versions of the DeepSeek-R1 model.\nFig. 1. DeepSeek R1 Training Pipeline. (Image source: Harris Chan\u0026rsquo;s Tweet)\nAs shown in Figure 1, the DeepSeek-R1 training process is clearly displayed and mainly divided into the following key stages:\nBase Model and Initial Fine-tuning: The starting point of the process is the DeepSeek-V3 Base model. First, SFT technology is used to initially train the base model on cold-start long-text CoT data, endowing the model with preliminary reasoning abilities.\nReinforcing Reasoning Ability: Based on SFT, a reasoning-oriented reinforcement learning method, specifically the Group Relative Policy Optimization (GRPO) algorithm, combined with rule-based rewards and CoT language consistency rewards, is used to further enhance the model\u0026rsquo;s reasoning ability.\nReasoning Data Generation and Rejection Sampling: Using reasoning prompts and rejection sampling techniques, and leveraging rules and the DeepSeek-V3 model to judge data quality, high-quality reasoning data is generated.\nNon-Reasoning Data Generation: Using the CoT prompting method, the DeepSeek-V3 model is used for data augmentation to generate non-reasoning data, which is combined with the original SFT data to improve the model\u0026rsquo;s general language capabilities.\nDistillation: Combining reasoning data and non-reasoning data for distillation training. Through SFT, the capabilities of DeepSeek-V3 are transferred to a series of smaller models (Qwen and Llama series), resulting in the DeepSeek-R1-Distill series models.\nFinal Model Fine-tuning: The DeepSeek-V3 model is fine-tuned again with SFT and reinforcement learning. In the reinforcement learning stage, reasoning and preference rewards are adopted, and diverse training prompts are used, ultimately resulting in the DeepSeek-R1 model.\nDeepSeek-R1-Zero: Trained directly on DeepSeek-V3 Base using the GRPO algorithm, serving as a comparative baseline for other models.\nNext, this blog post will delve into the key technologies and methods in the DeepSeek-R1 training process.\nDeepSeek-R1-Zero PPO Proximal Policy Optimization (PPO) (Schulman et al., 2017) algorithm is a classic algorithm widely used in reinforcement learning. In the InstructGPT (Ouyang et al., 2022) paper, it was proven to be an effective and stable method for training LLMs in the reinforcement learning fine-tuning stage.\nThe core idea of reinforcement learning is to allow an agent to learn through interaction with an environment, maximizing cumulative rewards through trial and error. In the LLM scenario, the model itself is the agent, and the \u0026ldquo;environment\u0026rdquo; can be understood as the questions raised by users and the expected ways of answering. The policy \\( \\pi_\\theta \\) represents the agent\u0026rsquo;s behavior guidelines, i.e., given an input (e.g., question \\( q \\)), the policy will output an action (e.g., generate text \\( o \\)). The policy \\( \\pi_\\theta \\) is usually parameterized by a neural network model, and the training objective is to find the optimal parameters \\( \\theta \\) so that the policy can generate high-quality outputs.\nThe Actor-Critic framework is a commonly used architecture in reinforcement learning, and PPO also belongs to the Actor-Critic algorithm family. The Actor-Critic framework includes two core components:\nActor (Policy Model): Responsible for learning the policy \\( \\pi_\\theta \\), i.e., how to choose actions (generate text) based on the current state (e.g., user question). Critic (Value Model): Responsible for evaluating the quality of the Actor\u0026rsquo;s policy, usually achieved by learning a value function \\( V(s) \\) or \\( Q(s, a) \\). The value function predicts the expected value of cumulative rewards that can be obtained in the future given a state \\( s \\) (or state-action pair \\( (s, a) \\)). The goal of PPO is to improve the policy model (Actor) so that it can generate higher quality outputs, while using the value model (Critic) to stabilize the training process. PPO updates the policy model \\( \\pi_{\\theta} \\) by maximizing the following objective function:\n\\[ \\mathcal{J}_{PPO}(\\theta) = \\mathbb{E}\\!\\Biggl[ \\min\\Bigl( \\frac{\\pi_\\theta(a\\!\\mid\\!s)}{\\pi_{\\theta_{\\text{old}}}(a\\!\\mid\\!s)}A_t,\\, \\operatorname{clip}\\Bigl( \\frac{\\pi_\\theta(a\\!\\mid\\!s)}{\\pi_{\\theta_{\\text{old}}}(a\\!\\mid\\!s)}, 1-\\varepsilon,\\, 1+\\varepsilon \\Bigr)A_t \\Bigr) \\Biggr] \\]Parameter Description:\nExpectation \\( \\mathbb{E}[\\cdot] \\): Represents the average over samples. In actual training, we sample a batch of data (e.g., user questions and model-generated answers) and then calculate the average objective function value for this batch of data.\nImportance Sampling: Measures the probability ratio of the current policy \\( \\pi_\\theta \\) to the old policy \\( \\pi_{\\theta_{\\text{old}}} \\) on action \\( a \\). PPO adopts the idea of proximal policy update, limiting the magnitude of each policy update to avoid excessive policy changes that lead to training instability.\nAdvantage Function \\( A_t \\): Evaluates the advantage of taking action \\( a \\) in state \\( s \\) relative to the average level. The advantage function is usually estimated by the Critic model (value network), and can be Advantage Estimation or Generalized Advantage Estimation (GAE) and other methods. The larger the advantage function \\( A_t \\), the better the current action \\( a \\), and the policy model should increase the probability of taking this action.\nclip: One of the core mechanisms of PPO, which can essentially be seen as a penalty function, used to limit the range of the importance sampling between \\( [1-\\varepsilon, 1+\\varepsilon] \\), where \\( \\varepsilon \\) is a hyperparameter (usually set to 0.2). The clipping operation prevents excessive policy update steps and improves training stability.\nThe clip function penalizes excessively large or small policy update magnitudes by limiting the importance sampling.\nWhen the importance sampling exceeds the range of \\( [1-\\varepsilon, 1+\\varepsilon] \\), the clip function will limit it within this range, thereby reducing the gain (or reducing the loss) of the objective function. For positive updates (\\( A_t \u003e 0 \\)): If the importance sampling is too large (exceeds \\( 1+\\varepsilon \\)), clip will limit it to \\( 1+\\varepsilon \\), reducing the actual update magnitude and penalizing overly aggressive policy improvements. For negative updates (\\( A_t \u003c 0 \\)): If the importance sampling is too small (less than \\( 1-\\varepsilon \\)), clip will limit it to \\( 1-\\varepsilon \\), also limiting the update magnitude and avoiding drastic changes in policy. The objective function takes the minimum value between clip before and clip after, ensuring that when the importance sampling is out of range, PPO will penalize policy updates, ensuring the \u0026ldquo;conservatism\u0026rdquo; of policy updates.\nIn the actual optimization process, we usually define the PPO loss function \\( \\mathcal{L}_{PPO}(\\theta) \\) as the negative value of the objective function, and maximize the objective function by minimizing the loss:\n\\[ \\mathcal{L}_{PPO}(\\theta) = -\\,\\mathcal{J}_{PPO}(\\theta). \\]The PPO algorithm, due to its characteristics of being simple and effective, and relatively stable, has become one of the benchmark algorithms in the field of reinforcement learning and has achieved success in various tasks, including reinforcement learning fine-tuning of LLMs. PPO is generally considered more stable than earlier methods such as TRPO, but its specific application in large models still requires careful hyperparameter tuning. In large-scale language model scenarios, if the value network and policy network are completely separated and of comparable size, it will inevitably bring more computational and memory overhead. To solve these problems, the DeepSeek team proposed Group Relative Policy Optimization (GRPO) algorithm.\nGRPO Group Relative Policy Optimization (GRPO) (Shao, et al., 2024) is an efficient and stable reinforcement learning algorithm specifically designed by the DeepSeek team for training LLMs like DeepSeek-R1-Zero. GRPO\u0026rsquo;s core innovation lies in abandoning the dependence on an independent value network (critic model) in the traditional Actor-Critic framework, reducing computational costs and improving training stability. Broadly speaking, GRPO can be regarded as an Actor-Only reinforcement learning method.\nGRPO is inspired by the idea of relative evaluation. In many practical scenarios, we are often better at judging the relative quality among a group of things than giving absolute value evaluations. For example, when evaluating a group of student assignments, teachers may find it easier to compare the merits of different assignments than to give each assignment an absolute score. GRPO introduces this idea of relative evaluation into reinforcement learning, using in-group relative scoring to build a baseline, completely replacing the dependence on value networks.\nSpecifically, for each question \\( q \\), GRPO samples a set of outputs \\( \\{o_1, o_2, \\ldots, o_G\\} \\) from the old policy \\( \\pi_{\\theta_{\\text{old}}} \\), forming an output group. Then, the policy model \\( \\pi_{\\theta} \\) is updated by maximizing the following objective function:\n\\[ \\begin{aligned} \\mathcal{J}_{GRPO}(\\theta) \u0026 = \\mathbb{E}\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O \\mid q)\\right] \\\\ \u0026 \\quad \\frac{1}{G} \\sum_{i=1}^G \\Biggl( \\min\\biggl( \\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)} \\,A_i,\\, \\operatorname{clip}\\Bigl( \\frac{\\pi_\\theta(o_i \\mid q)}{\\pi_{\\theta_{\\text{old}}}(o_i \\mid q)}, 1-\\varepsilon,\\, 1+\\varepsilon \\Bigr)\\,A_i \\biggr) \\;-\\;\\beta\\,\\mathbb{D}_{KL}\\bigl(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\bigr) \\Biggr), \\end{aligned} \\]Similar to the PPO objective function, the GRPO objective function also includes the importance sampling and clip to ensure the stability of policy updates. The differences are:\nRelative Advantage Value \\( A_i \\): GRPO uses the relative advantage value \\( A_i \\) instead of the advantage function \\( A_t \\) in PPO. The relative advantage value \\( A_i \\) is calculated based on in-group rewards, without the need for value network estimation. KL Divergence Penalty Term \\( \\mathbb{D}_{KL}\\bigl(\\pi_\\theta \\,\\|\\, \\pi_{\\text{ref}}\\bigr) \\): To further constrain policy updates, GRPO introduces a KL divergence penalty term, limiting the difference between the new policy \\( \\pi_\\theta \\) and the reference policy \\( \\pi_{\\text{ref}} \\) from being too large. Fig. 2. The comparison of PPO and GRPO. (Image source: DeepSeek-AI, 2024)\nFrom Figure 2 above, we can see that the core innovation of GRPO lies in the calculation method of the relative advantage value \\( A_i \\). Unlike PPO, GRPO does not rely on an independent value network, but directly uses in-group rewards for relative evaluation. For each output group \\( \\{o_1, o_2, \\ldots, o_G\\} \\), GRPO first obtains the reward values \\( \\{r_1, r_2, \\ldots, r_G\\} \\) corresponding to each output. Then, the relative advantage value \\( A_i \\) is calculated according to the following formula:\n\\[ A_i = \\frac{\\,r_i \\;-\\; \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})\\,}{ \\text{std}\\bigl(\\{r_1, r_2, \\ldots, r_G\\}\\bigr)}. \\]The relative advantage value \\( A_i \\) is obtained by standardizing the in-group rewards \\( \\{r_1, r_2, \\ldots, r_G\\} \\), with zero mean and unit variance, better reflecting the relative merits of each output within the group.\nGRPO adopts the method of relative evaluation, which has the following advantages:\nNo need to train a value network: Avoids the computational overhead and instability caused by training a large-scale value network. Reduces variance in value estimation: Relative evaluation focuses on the relative merits of outputs within the group, rather than absolute values, reducing estimation variance and improving training stability. More consistent with the comparative nature of reward models: Reward models are usually trained based on comparative data, and GRPO\u0026rsquo;s relative evaluation method is more consistent with this. More suitable for credit assignment in sequence generation tasks: Even if the reward is sparse, GRPO can learn effectively because it focuses on the relative quality between outputs in the same group. Schulman Unbiased Estimator KL divergence \\( \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) \\) measures the information loss of policy \\( \\pi_\\theta \\) relative to the reference policy \\( \\pi_{\\text{ref}} \\), and its standard definition is:\n\\[ \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) = \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\log \\frac{\\pi_\\theta(o \\mid q)}{\\pi_{\\text{ref}}(o \\mid q)} \\right]. \\]As mentioned earlier, directly calculating the above expectation in practice faces challenges. To solve this problem, GRPO adopts the Schulman unbiased estimator (Schulman, 2020). Unlike the KL divergence penalty term that may be used in formula, we use the following unbiased estimator to estimate the KL divergence between \\( \\pi_\\theta \\) and \\( \\pi_{ref} \\):\n$$ \\mathbb{D}_{K L}\\left[\\pi_{\\theta}| | \\pi_{r e f}\\right]=\\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}{\\pi_{\\theta}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}-\\log \\frac{\\pi_{r e f}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}{\\pi_{\\theta}\\left(o_{i, t} \\mid q, o_{i,\u0026lt;t}\\right)}-1. $$To understand the advantages of this estimator, we first mathematically derive its unbiasedness.\nUnbiasedness Proof To simplify the notation, let \\( r(o) = \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\). Then the Schulman estimator can be written as:\n\\[ \\hat{D}_{KL}(o) = r(o) - \\log r(o) - 1. \\]We need to prove that when \\( o \\) is sampled from \\( \\pi_\\theta(\\cdot|q) \\), the expectation of \\( \\hat{D}_{KL}(o) \\) is equal to the true KL divergence \\( \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) \\).\n\\[ \\begin{aligned} \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} [\\hat{D}_{KL}(o)] \u0026= \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} [r(o) - \\log r(o) - 1] \\\\ \u0026= \\mathbb{E}_{o \\sim \\pi_\\theta(\\cdot|q)} \\left[ \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - 1 \\right] \\\\ \u0026= \\sum_{o} \\pi_\\theta(o \\mid q) \\left[ \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - 1 \\right] \\quad (\\text{Discrete case, integral for continuous case}) \\\\ \u0026= \\sum_{o} \\left[ \\pi_{ref}(o \\mid q) - \\pi_\\theta(o \\mid q) \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} - \\pi_\\theta(o \\mid q) \\right] \\\\ \u0026= \\underbrace{\\sum_{o} \\pi_{ref}(o \\mid q)}_{=1} - \\underbrace{\\sum_{o} \\pi_\\theta(o \\mid q) \\log \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)}}_{=-\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})} - \\underbrace{\\sum_{o} \\pi_\\theta(o \\mid q)}_{=1} \\\\ \u0026= 1 - (-\\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref})) - 1 \\\\ \u0026= \\mathbb{D}_{KL}(\\pi_\\theta || \\pi_{ref}). \\end{aligned} \\]Therefore, we have proven that \\( \\hat{D}_{KL}(o) \\) is an unbiased estimator of \\( \\mathbb{D}_{KL}\\left(\\pi_\\theta \\|\\pi_{\\text{ref}}\\right) \\).\nComparison of Three KL Divergence Estimators To intuitively understand the differences between the three estimators, the following table lists their mathematical expressions, where \\( r(o) = \\frac{\\pi_{ref}(o \\mid q)}{\\pi_\\theta(o \\mid q)} \\):\nEstimator Mathematical Expression Main Features k1 (Naive Estimator) \\( \\hat{D}_{KL}^{(k1)}(o) = \\log \\frac{\\pi_\\theta(o \\mid q)}{\\pi_{ref}(o \\mid q)} = \\log \\frac{1}{r(o)} \\) Simple and direct, corresponds to the definition of KL divergence; high variance, large fluctuations in estimation results. k2 (Squared Log-Ratio Estimator) \\( \\hat{D}_{KL}^{(k2)}(o) = \\frac{1}{2} (\\log r(o))^2 \\) Uses the square of the log-ratio, always positive, reduces variance; introduces bias, especially when distribution differences are large. k3 (Schulman Unbiased Estimator) \\( \\hat{D}_{KL}^{(k3)}(o) = r(o) - \\log r(o) - 1 \\) Combines the ratio \\( r(o) \\) and the log-ratio \\( \\log r(o) \\); unbiased, low variance, stable estimation. k1 (Naive Estimator): Unbiased, simple and direct, but with high variance, leading to unstable estimation results. k2 (Squared Log-Ratio Estimator): Reduces variance, but introduces bias, especially when the distribution difference is large, the bias is significant. k3 (Schulman Unbiased Estimator): Combines unbiasedness and low variance, providing stable estimation results. Experimental Results To evaluate the performance of the three KL divergence estimators, we conducted numerical experiments, and the results are shown in the table below. In the experiment, the distribution \\( q = \\mathcal{N}(0, 1) \\) was fixed, and the mean \\( \\mu \\) of the distribution \\( p = \\mathcal{N}(\\mu, 1) \\) was changed to control the true KL divergence \\( \\mathbb{D}_{KL}(p \\| q) \\). Monte Carlo estimation was performed using 500 million samples, and the experiment was repeated to obtain stable results.\nExperimental code can be found at unbiased_kl_divergence.py\nTrue KL Divergence Estimator Average Estimated Value Standard Deviation Relative Bias (%) 0.005 k1 0.005 0.1 0.0387 0.005 k2 0.005 0.0071 0.2415 0.005 k3 0.005 0.0071 -0.0082 0.125 k1 0.125 0.5 -0.0389 0.125 k2 0.1328 0.1875 6.2500 0.125 k3 0.125 0.1845 0.0072 0.5 k1 0.5 1 -0.0018 0.5 k2 0.625 0.866 25.0004 0.5 k3 0.5 0.8478 0.0052 Naive Estimator (k1):\nUnbiasedness: The average estimated value is highly consistent with the true KL divergence, and the relative bias is close to 0%. Variance: The standard deviation is higher than k3 and increases with the true KL divergence, leading to unstable estimation results. Squared Log-Ratio Estimator (k2):\nUnbiasedness: There is a certain bias, and the bias increases significantly with the increase of the true KL divergence (for example, when the true KL is 0.5, the relative bias reaches 25%). Variance: The variance is lower at lower true KL divergence, but the overall performance is unstable. Schulman Unbiased Estimator (k3):\nUnbiasedness: The experimental results show that the relative bias is extremely small, almost 0%, which verifies its unbiasedness. Variance: The standard deviation is significantly lower than k1, and compared with k1, it shows lower variance under all KL divergences, especially when the KL divergence is low, the advantage is significant. Advantages Summary Unbiasedness: Both theoretical and experimental results show that k3 is an unbiased estimator, which can accurately reflect the true KL divergence. Positive Definiteness: The estimated value is always non-negative, which is consistent with the nature of KL divergence. Lower Variance: Compared with k1, k3 significantly reduces the estimation variance and provides more stable estimation results, especially when the KL divergence is small, the performance is outstanding. The Schulman unbiased estimator \\( \\hat{D}_{KL}^{(k3)}(o) = r(o) - \\log r(o) - 1 \\) provides an estimation method for KL divergence that combines unbiasedness and low variance. Its unbiasedness ensures the accuracy of the estimation, and the lower variance improves the stability of the estimation, especially suitable for reinforcement learning scenarios that require stable gradient signals, such as policy optimization. Based on these advantages, the GRPO algorithm chooses to use k3 as an estimator to penalize policy deviation, thereby ensuring the stability of the training process and the performance of the final policy.\nIn actual optimization, the GRPO loss function \\( \\mathcal{L}_{GRPO}(\\theta) \\) is defined as the negative value of the objective function \\( \\mathcal{J}_{GRPO}(\\theta) \\), and the objective function \\( \\mathcal{J}_{GRPO}(\\theta) \\) is maximized by minimizing the loss function \\( \\mathcal{L}_{GRPO}(\\theta) \\):\n\\[ \\mathcal{L}_{GRPO}(\\theta) = -\\,\\mathcal{J}_{GRPO}(\\theta) \\]Comparison of PPO and GRPO To more clearly understand the similarities and differences between PPO and GRPO, the following table compares the two algorithms:\nFeature PPO GRPO Actor-Critic or Not Yes Yes (broadly considered Actor-Only) Value Network Needed Requires an independent value network (Critic) No independent value network required Advantage Estimation Estimates absolute advantage value through a value network Relatively evaluates relative advantage value through in-group rewards Computational Cost Higher, requires training a value network Lower, no need to train a value network Training Stability Relatively good, but value network training may introduce instability Better, avoids instability from value network training Algorithm Complexity Relatively complex, needs to maintain and update policy and value networks Relatively simple, only needs to maintain and update the policy network Applicable Scenarios Widely applicable to various RL tasks, including fine-tuning of small to medium-sized language models Especially suitable for RL fine-tuning of large-scale language models, focusing on efficiency and stability Credit Assignment Relies on value network for temporal difference learning to handle credit assignment issues Relies on final rewards and in-group relative evaluation, can also be assisted by intermediate rewards Variance Issue Value network estimation may introduce variance In-group relative advantage estimation may have variance under small group sizes, which can be mitigated by increasing group size, etc. As can be seen from the table, PPO is a general and powerful reinforcement learning algorithm, but its mechanism of training a value network brings additional computational burden and potential instability in LLMs scenarios. GRPO cleverly avoids the need for a value network by introducing in-group relative scoring, significantly reducing computational costs and improving training stability while ensuring performance. This makes GRPO an ideal choice for training LLMs like DeepSeek-R1-Zero when training resources are limited.\nCode Generation Evaluation Metrics Code generation employs more rigorous testing methods. The code generated by the model is executed through a compiler, and multiple unit tests are performed using predefined test cases to determine the correctness of the code. Commonly used evaluation metrics include pass@k (Chen et al., 2021) and cons@N (OpenAI, 2024).\npass@k: Measures the probability that at least one sample out of k code samples generated by the model can pass all predefined test cases.\nBiased Estimation Formula for pass@k \\[ \\text{Simplified pass@k} = \\frac{1}{P} \\sum_{i=1}^{P} C_i \\]\\[ C_i = \\begin{cases} 1 \u0026 \\text{if at least one of the k generated samples is correct} \\\\ 0 \u0026 \\text{if all k generated samples are incorrect} \\end{cases} \\]Parameter Description:\n\\( P \\): Total number of problems evaluated. \\( C_i \\): For the \\( i \\)-th problem, \\( C_i = 1 \\) if at least one of the \\( k \\) generated samples is correct, otherwise \\( C_i = 0 \\). \\( \\sum_{i=1}^{P} C_i \\): Represents the total number of problems \u0026ldquo;solved\u0026rdquo; among all \\( P \\) problems. \\( \\frac{1}{P} \\sum_{i=1}^{P} C_i \\): Represents the proportion of \u0026ldquo;solved\u0026rdquo; problems, i.e., accuracy. Formula Meaning: This simplified method directly calculates the proportion of problems for which at least one sample is correct after generating k samples. Although this method provides a biased estimate of pass@k, which may slightly overestimate the true value, it is very commonly used in practice because it is intuitive, easy to calculate, and can provide a reasonable approximation of model performance when the sample size is large enough. Especially in industrial and rapid evaluation scenarios, this simplified method is very practical.\nHowever, LLMs are affected by parameters such as temperature, top_p, top_k, and repetition_penalty during reasoning decoding. These parameters can make code generation results random and diverse, and if the parameters are set too randomly when sample K is relatively small, it will affect the evaluation results of pass@k. Therefore, using an unbiased estimation method can more accurately reflect the true performance of the model.\nUnbiased Estimation Formula for pass@k \\[ \\text { pass @ } k:=\\underset{\\text { Problems }}{\\mathbb{E}}\\left[1-\\frac{\\binom{n-c}{k}}{\\binom{n}{k}}\\right] \\]Parameter Description:\n\\( n \\): Total number of code samples generated for each problem. \\( c \\): Number of correct samples among the \\( n \\) samples that can pass all unit tests. \\( k \\): Parameter \\( k \\) in the pass@\\(k\\) metric, indicating the number of generated samples we consider. \\( \\binom{a}{b} \\): Represents the binomial coefficient, calculating the number of combinations of choosing \\( b \\) elements from \\( a \\) elements. \\( \\underset{\\text { Problems }}{\\mathbb{E}} \\): Represents the expected value (average value) over all evaluation problems. Formula Meaning:\nThe formula actually calculates the probability of having at least one correct sample. The formula \\( \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\) calculates the probability of randomly selecting \\( k \\) samples from the generated \\( n \\) samples, and none of these \\( k \\) samples are correct. We subtract this probability from 1 to get the probability of randomly selecting \\( k \\) samples from \\( n \\) samples, and at least one of these \\( k \\) samples is correct, which is the meaning of the pass@\\(k\\) metric. This formula provides an unbiased estimate of pass@k, which is more suitable for scenarios requiring precise evaluation such as academic research. In actual calculations, a sample size \\( n \\) much larger than \\( k \\) is usually generated (for example, \\( n=200 \\), \\( k \\leq 100 \\) is used in papers) to more stably estimate pass@\\(k\\). Simplified Product Form of pass@k For easier numerical calculation, the original formula can also be converted into the following product form, which is still an unbiased estimate and can avoid numerical overflow problems:\n\\[ \\text { pass @ } k = \\underset{\\text { Problems }}{\\mathbb{E}}\\left[1 - \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i}\\right] \\]Derivation Process:\nThe opposite of having at least one correct sample is that all k samples are incorrect. Therefore, pass@k is equal to 1 minus the probability that all k samples are incorrect.\nConsider the scenario of sampling without replacement. Assume we draw \\( k \\) samples from \\( n \\) samples, and we want to calculate the probability that all \\( k \\) samples are incorrect. There are a total of \\( n \\) samples, of which \\( n-c \\) are incorrect.\nWhen drawing for the first time, the probability of drawing an incorrect sample is \\( \\frac{n-c}{n} \\).\nGiven that an incorrect sample was drawn in the first draw, when drawing for the second time, among the remaining \\( n-1 \\) samples, there are \\( n-c-1 \\) incorrect samples. Therefore, the conditional probability of still drawing an incorrect sample for the second time is \\( \\frac{n-c-1}{n-1} \\).\nBy analogy, when drawing for the \\( i \\)-th time ( \\( i \\) from 1 to \\( k \\)), given that incorrect samples were drawn in the previous \\( i-1 \\) times, the conditional probability of still drawing an incorrect sample for the \\( i \\)-th time is \\( \\frac{n-c-(i-1)}{n-(i-1)} = \\frac{n-c-i+1}{n-i+1} \\). To align with the index \\( i=0 \\) in the formula, we change the index to range from \\( i=0 \\) to \\( k-1 \\), then when drawing for the \\( (i+1) \\)-th time ( \\( i \\) from 0 to \\( k-1 \\)), the conditional probability is \\( \\frac{n-c-i}{n-i} \\).\nMultiply these conditional probabilities of \\( k \\) draws to get the probability that all \\( k \\) samples are incorrect:\n\\[ P(\\text{all k samples are incorrect}) = \\frac{n-c}{n} \\times \\frac{n-c-1}{n-1} \\times \\cdots \\times \\frac{n-c-k+1}{n-k+1} = \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i} \\] Finally, the simplified formula for pass@k is:\n\\[ \\text { pass @ } k = \\underset{\\text { Problems }}{\\mathbb{E}}\\left[1 - \\prod_{i=0}^{k-1} \\frac{n-c-i}{n-i}\\right] \\] This product form formula avoids directly calculating binomial coefficients that may be numerically large, is easier to understand and numerically calculate, especially in programming implementation, it can be multiplied term by term to effectively prevent numerical overflow.\ncons@N cons@N: By generating N samples and selecting the answer with the highest frequency as the final answer, the accuracy of this answer is evaluated. In the evaluation of DeepSeek-R1-Zero, cons@64 was used, i.e., 64 samples were generated, and the answer that appeared most frequently among them was taken as the final answer for evaluation.\n\\[ \\text{cons@N} = \\frac{1}{P} \\sum_{i=1}^{P} \\mathbb{I}(\\text{ConsensusAnswer}_i \\text{ is correct}) \\]Parameter Description:\n\\( P \\): Total number of problems evaluated. \\( \\text{ConsensusAnswer}_i \\): Consensus answer obtained through majority voting. \\( \\mathbb{I}(\\text{ConsensusAnswer}_i \\text{ is correct}) \\): Indicator function, 1 if the consensus answer is correct, 0 otherwise. Formula Meaning: Calculate the proportion of consensus answers that are correct among all evaluation problems. By increasing the number of generated samples \\( N \\) and adopting a majority voting strategy, the cons@N metric can more stably and reliably evaluate the average performance of the model. In cases where the model\u0026rsquo;s generated results have a certain degree of randomness, this metric can verify the consistency and accuracy of the model\u0026rsquo;s output.\nReward Model Reward models are crucial in the development of LLMs, mainly used in the following key stages:\nReinforcement Learning from Human Feedback: In the Reinforcement Learning from Human Feedback (RLHF) process, reward models are used to evaluate the quality of model-generated results and provide reward signals for subsequent reinforcement learning.\nKey Tool for Rejection Sampling: In the rejection sampling process, reward models score a large number of candidate results and filter out high-quality samples for SFT. Rejection sampling is an important method for automated sample engineering, and reward models are its core component.\nDiscriminator in Business Scenarios: In practical applications, reward models serve as discriminators or validators of LLM output results, evaluating the quality of generated results. Only results with scores exceeding a preset threshold will be output, otherwise, regeneration or degradation processing will be performed to improve the reliability and safety of the output.\nORM vs PRM Fig. 3. Outcome reward vs Process reward. (Image source: Zeng et al., 2024)\nCurrent reward models are mainly divided into two paradigms: Outcome Reward Model (ORM) and Process Reward Model (PRM). Figure 3 above intuitively shows the difference between these two reward models. The following table also compares the main characteristics of these two models:\nFeature ORM PRM Definition Holistically scores the complete result generated by the model Provides fine-grained scoring for each step or stage during content generation Main Advantages Simple and direct, easy to implement Comprehensive evaluation of overall results Provides more refined reward signals Helps guide each step of the model generation process Main Disadvantages High variance, large fluctuations in estimation results Lack of feedback during the process More complex to train and apply May introduce bias, especially when distribution differences are large Applicable Scenarios Tasks requiring overall evaluation of generated results Tasks requiring fine-grained control of the generation process, such as step-by-step reasoning or complex generation tasks Ability to Avoid Reward Hacking Medium, depends on the accuracy of overall scoring Lower, can cheat by optimizing rewards for each step rather than overall performance Training Complexity Lower, no need for additional supervision of the generation process Higher, needs to score at each step of generation, increasing computational and data requirements Explainability High, scoring is based on the final result Lower, scoring involves multiple steps of the generation process, difficult to fully understand the scoring basis for each step Adopting ORM To train DeepSeek-R1-Zero, the DeepSeek team chose ORM instead of PRM. This choice is based on the following considerations:\nAvoiding Reward Hacking PRM is prone to being exploited by agents in large-scale RL training, leading to reward hacking (Gao et al., 2022). Models may adopt \u0026ldquo;shortcuts\u0026rdquo; to maximize rewards instead of improving reasoning ability. Rule-based reward systems effectively avoid reward hacking problems through clear and interpretable rules.\nRule-based reward systems may be difficult to cover all types of questions when the problem scenario is complex or creative answers are required, and rule design may have loopholes that can be exploited by the model.\nReducing Training Complexity Training PRM requires a lot of computing resources and data, increasing the complexity of the training process. Rule-based reward systems, on the other hand, do not require additional training, and rules can be directly applied once determined, simplifying the training process. Rule-based reward systems are particularly suitable for tasks with automatic scoring or clear objectives, such as math problems, LeetCode programming problems, and tasks with clear output format requirements. For open-ended dialogue or creative tasks, it may be necessary to combine human feedback or trained reward models.\nReward Mechanism The reward system of DeepSeek-R1-Zero adopts a dual reward mechanism, which is automatically evaluated through predefined rules to ensure the efficiency and real-time nature of the evaluation process. This system mainly includes the following two types of rewards:\n1. Accuracy Reward\nDefinition: Measures the correctness of the model output result, which is the most critical part of the reward system. Implementation Method: Different verification methods are adopted according to different task types: Math Problems: Verify whether the final answer is consistent with the standard answer. Code Generation: Execute the code generated by the model through a compiler, and use preset unit test cases for multiple tests to determine the correctness of the code. Purpose: Guide the model to generate accurate and reliable output results. 2. Format Reward\nDefinition: A reward mechanism introduced to improve the readability and structure of model output, facilitating subsequent analysis and evaluation. Evaluation Method: Automatically evaluated by a predefined rule system during reinforcement learning training. Purpose: Encourage the model to generate structured output, such as including the thinking process and the final answer, making it easier to understand and analyze. The reward function \\( r(o) \\) of DeepSeek-R1-Zero consists of a weighted sum of accuracy reward and format reward:\n$$ r(o) = r_{\\text{accuracy}}(o) + \\lambda \\cdot r_{\\text{format_effective}}(o) $$Where, the effective format reward \\( r_{\\text{format_effective}}(o) \\) is calculated as follows:\n$$ r_{\\text{format_effective}}(o) = \\begin{cases} r_{\\text{format}}(o) \u0026 \\text{if the basic format of } o \\text{ meets the requirements} \\\\ 0 \u0026 \\text{if the basic format of } o \\text{ does not meet the requirements} \\end{cases} $$The basic format reward \\( r_{\\text{format}}(o) \\) is graded according to the degree of compliance with the format specification:\n$$ r_{\\text{format}}(o) = \\begin{cases} R_{\\text{format_full}} \u0026 \\text{if the format of } o \\text{ fully complies with the specification} \\\\ R_{\\text{format_partial}} \u0026 \\text{if the format of } o \\text{ partially complies with the specification} \\\\ 0 \u0026 \\text{if the format of } o \\text{ does not comply with the specification} \\end{cases} $$Experimental Process Training Template To guide the base model to follow specified instructions, the DeepSeek team designed a concise and effective training template. This template requires the model to first generate the reasoning process (placed between \u0026lt;think\u0026gt; and \u0026lt;/think\u0026gt; tags), and then provide the final answer (placed between \u0026lt;answer\u0026gt; and \u0026lt;/answer\u0026gt; tags). This structured format not only ensures the readability of the output, but also allows researchers to clearly observe the model\u0026rsquo;s reasoning process during RL training, thereby more accurately assessing the model\u0026rsquo;s learning progress.\nRole Prompt Content Assistant Reply User prompt (user question) Assistant: \u0026lt;think\u0026gt; Reasoning Process \u0026lt;/think\u0026gt; \u0026lt;answer\u0026gt; Answer \u0026lt;/answer\u0026gt; \u0026lt;think\u0026gt; and \u0026lt;/think\u0026gt; (Thinking Process Tags): Used to wrap the model\u0026rsquo;s intermediate reasoning steps, clearly showing the model\u0026rsquo;s thinking process, facilitating understanding of the model\u0026rsquo;s reasoning logic and error analysis. \u0026lt;answer\u0026gt; and \u0026lt;/answer\u0026gt; (Final Answer Tags): Used to wrap the model\u0026rsquo;s final answer, facilitating program automation to extract the answer part for efficient evaluation and subsequent processing. Evaluation Process Accuracy Evaluation: Evaluate whether the answer of the model output \\( o \\) is correct, and calculate the accuracy reward \\( r_{\\text{accuracy}}(o) \\). Basic Format Check: Check whether the basic format of the output \\( o \\) meets predefined requirements, such as whether it contains necessary tags \u0026lt;think\u0026gt; and \u0026lt;answer\u0026gt;, and whether the tags are correctly closed and nested. Effective Format Reward Judgment: Basic format does not comply: Effective format reward \\( r_{\\text{format_effective}}(o) = 0 \\). Basic format complies: Further evaluate the degree of format specification compliance, and calculate the basic format reward \\( r_{\\text{format}}(o) \\). Final Reward Calculation: Linearly weight and sum the accuracy reward \\( r_{\\text{accuracy}}(o) \\) and the effective format reward \\( r_{\\text{format_effective}}(o) \\) to obtain the final reward \\( r(o) \\). By combining accuracy reward and format reward, the reward system of DeepSeek-R1-Zero not only focuses on the correctness of the model output, but also emphasizes the structuredness and readability of the output results. This enables the model to not only give correct answers, but also show its thinking process, making it more like an intelligent agent with reasoning ability, rather than just a simple answer output machine.\nExperimental Results Fig. 4. Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related benchmarks. (Image source: DeepSeek-AI, 2025)\nFigure 4 shows the performance of different models on multiple benchmarks. In the AIME 2024 benchmark, the pass@1 score of the DeepSeek-R1-Zero model reached 71.0%, and the cons@64 score was 86.7%, comparable to the OpenAI o1-0912 model.\nFig. 5. The average response length of DeepSeek-R1-Zero on the training set during the RL process. (Image source: DeepSeek-AI, 2025)\nFigure 5 shows that as training deepens, the DeepSeek-R1-Zero model exhibits the ability of spontaneous self-evolution. The model dynamically allocates \u0026ldquo;thinking time\u0026rdquo; according to the complexity of the question. For more complex questions, it spontaneously generates longer reasoning chains for deeper thinking. This adaptive adjustment of \u0026ldquo;thinking time\u0026rdquo; is not artificially set, but an emergent behavior of the model in the RL training process, fully reflecting the autonomous improvement of the model\u0026rsquo;s reasoning ability driven by reinforcement learning.\nDeepSeek-R1 Training Process To further improve model performance based on DeepSeek-R1-Zero, the DeepSeek team adopted a multi-stage training strategy and incorporated cold-start data into the training process. The training process of DeepSeek-R1 mainly includes the following four stages, reflecting the progressive path from initial policy initialization to comprehensive capability improvement:\nCold Start: Using high-quality long Chain-of-Thought (CoT) data, perform preliminary SFT on the DeepSeek-V3-Base base model to lay the foundation for subsequent reinforcement learning.\nReasoning-Oriented RL: Based on the cold-start model, apply reinforcement learning algorithms, focusing on enhancing the model\u0026rsquo;s ability in reasoning-intensive tasks.\nRejection Sampling \u0026amp; SFT: Filter high-quality reasoning data through rejection sampling technology, and combine it with non-reasoning data for SFT to further improve the model\u0026rsquo;s reasoning ability and general ability.\nAll-Scenario RL: Comprehensively consider reasoning and non-reasoning tasks, and conduct the second stage of reinforcement learning to align the model with human preferences and improve performance in a wider range of scenarios.\nCold Start In the training process of DeepSeek-R1, the cold start stage is crucial, like the igniter of an engine, laying a solid foundation for the subsequent complex reinforcement learning process. SFT is the core technology of the cold start stage.\nTraining Objective The objective of the cold start stage is clear and critical: using high-quality Chain-of-Thought (CoT) data to perform preliminary fine-tuning on the DeepSeek-V3-Base base model. This fine-tuning aims to quickly endow the model with the following core capabilities:\nPreliminary Reasoning Ability: Guide the model to learn to imitate the human reasoning process, laying the foundation for more complex reasoning. Good Text Generation Quality: Ensure the fluency and naturalness of the text output by the model, improving the user experience. These CoT data are like the model\u0026rsquo;s \u0026ldquo;starting fuel\u0026rdquo;, helping the model quickly grasp human reasoning patterns and providing good policy initialization for subsequent reinforcement learning, effectively avoiding the inefficiency and instability of RL training starting from scratch in the early stage.\nData Construction To construct high-quality cold-start data, the DeepSeek team conducted multi-faceted explorations and finally integrated the following efficient methods:\nFew-shot Prompting: Using a small number of high-quality examples to guide the model to generate longer, deeper, and more logical CoT data. Model Generation + Reflection Verification: Directly prompt the model to generate answers, and add reflection and verification links to ensure the quality of answers and the correctness of reasoning. Optimize R1-Zero Output: Collect the output of the DeepSeek-R1-Zero model, and improve the readability and overall quality of the data through manual annotation and optimization. Through the above strategies, the DeepSeek team accumulated thousands of high-quality cold-start data, and used this as a basis to fine-tune DeepSeek-V3-Base as a solid starting point for reinforcement learning.\nAdvantages of Cold Start Compared to directly using DeepSeek-R1-Zero as a starting point, cold-start data brings several significant advantages, laying a better foundation for subsequent training:\nImproved Readability:\nThe output of DeepSeek-R1-Zero has readability challenges, such as language mixing, lack of structured format, etc. Cold-start data is specially designed with a more readable output mode, including: Adding Summary: Add a refined summary at the end of the reply to quickly extract core conclusions. Filtering Bad Replies: Remove unfriendly or low-quality replies to ensure data purity. Structured Output Format: Adopt the | special_token | \u0026lt;reasoning_process\u0026gt; | special_token | \u0026lt;summary\u0026gt; format to clearly present the reasoning process and summary. Enhanced Performance:\nBy carefully designing data patterns that incorporate human prior knowledge, the DeepSeek team observed a significant improvement in model performance compared to R1-Zero. This further verifies that iterative training is an effective path to improve the performance of reasoning models. Superior Policy Initialization:\nThe core of SFT in the cold start stage is policy initialization. Policy initialization is a key step in building Reasoning LLMs, such as the OpenAI o1 series. By learning high-quality CoT data, the model initially grasps human reasoning patterns and has the ability to generate structured reasoning processes, laying a solid foundation for subsequent reinforcement learning training and avoiding the dilemma of starting exploration from scratch. SFT The core objective of Supervised Fine-tuning (SFT) is to fine-tune the model on supervised labeled data so that its predictions are as close as possible to the true labels. This aims to improve the model\u0026rsquo;s ability in specific tasks and instruction execution.\nLoss Function The training objective of SFT is to minimize the difference between model predictions and true labels. The loss function usually adopts Cross-Entropy Loss, also known as Negative Log-Likelihood, to measure the difference between the model\u0026rsquo;s predicted token distribution and the true token distribution. To balance the contributions of output sequences of different lengths, we usually normalize the loss function to the average loss per token.\nThe loss function formula is as follows:\n\\[ \\mathcal{L}_{SFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\log \\pi_\\theta\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]Parameter Description:\n\\( \\mathcal{L}_{SFT}(\\theta) \\): SFT loss function, minimized by adjusting model parameters \\( \\theta \\). \\( \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}[\\cdot] \\): Expectation over the SFT dataset distribution \\( P_{sft}(Q, O) \\). \\( P_{sft}(Q, O) \\): SFT dataset distribution, \\( q \\) represents the question (Query), and \\( o \\) represents the corresponding standard answer output (Output). \\( (q, o) \\): Question-answer pair sampled from the SFT dataset. \\( |o| \\): Token length of the standard answer output. \\( o_t \\): The \\( t \\)-th token of the standard answer output. \\( o_{\u0026lt;t} \\): The first \\( t-1 \\) tokens of the standard answer output. \\( \\pi_\\theta\\left(o_t \\mid q, o_{\u0026lt;t} \\right) \\): Given the question \\( q \\) and the preceding text \\( o_{\u0026lt;t} \\), the probability of the model predicting token \\( o_t \\). \\( \\frac{1}{|o|} \\): Length normalization factor, dividing the total loss by the output sequence length to get the average loss per token. The SFT loss function aims to penalize deviations between model predictions and standard answers. For a given question \\( q \\) and standard answer \\( o \\), the loss function calculates the probability \\( \\pi_\\theta(o_t | q, o_{\u0026lt;t} ) \\) of the model predicting each token \\( o_t \\) in the answer \\( o \\). By dividing by the output length \\( |o| \\), the loss function is normalized to the average negative log-likelihood per token.\nWhen the model accurately predicts the standard answer token, \\( \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\approx 1 \\), \\( \\log \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\approx 0 \\), and the loss value is close to the minimum. When the model prediction deviates from the standard answer, \\( \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\) is smaller, \\( \\log \\pi_\\theta(o_t \\mid q, o_{\u0026lt;t} ) \\) is negative and has a larger absolute value, and the loss value increases. The process of minimizing the SFT loss function is the process of making the model learn to generate text as similar as possible to the standard answers in the training dataset. From the perspective of negative log-likelihood, the goal is to find the optimal model parameters \\( \\theta \\) to maximize the probability of the model generating the answer \\( o \\) in the training data, which is equivalent to minimizing the negative log-likelihood of generating the answer \\( o \\). High-quality CoT data contains human preferences for reasoning and results, so SFT can also be regarded as a process of making the model learn and fit human reasoning preferences.\nGradient The gradient of the SFT loss function is used to guide model parameter updates to reduce the loss value. The gradient of the loss function with respect to the model parameters \\( \\theta \\) is:\n\\[ \\nabla_{\\theta} \\mathcal{L}_{SFT} = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q, O)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]Parameter Description:\n\\( \\nabla_{\\theta} \\mathcal{L}_{SFT} \\): Gradient of the SFT loss function with respect to parameter \\( \\theta \\), indicating the direction in which the loss function value decreases fastest. \\( \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right) \\): Gradient of the token probability logarithm \\( \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right) \\) with respect to parameter \\( \\theta \\). \\( \\frac{1}{|o|} \\): Length normalization factor, consistent with the loss function, the gradient is also the gradient of the average loss per token. When actually calculating the gradient, stochastic gradient descent algorithm is usually used to update the model parameters along the gradient descent direction, gradually minimizing the loss function and improving the accuracy of the model in generating standard answers.\nGradient Coefficient\nIn the SFT stage, the gradient coefficient is usually set to 1, which means that all training samples contribute equally to the update of model parameters. The model learns each example equally, striving to minimize the average loss over the entire dataset.\nData Source and Human Preference Data Source: The SFT dataset mainly consists of high-quality long Chain-of-Thought (CoT) examples, representing the \u0026ldquo;standard answers\u0026rdquo; that the model is expected to learn, used to guide the minimization of the loss function. Data may come from manual annotation or generation by more powerful models. Refer to the SFT dataset OpenO1-SFT of the Open-o1 project, which contains long CoT replies. Human Preference: In the SFT stage, human selection can be regarded as an implicit reward function. High-quality CoT data reflects human expectations for model reasoning and output. By learning these data, the model minimizes the deviation from human expected output, thereby fitting human preferences. Reasoning-Oriented Reinforcement Learning After cold-start fine-tuning, the DeepSeek team further improved the model\u0026rsquo;s ability in reasoning-intensive tasks (such as coding, mathematics, science, and logical reasoning) through reinforcement learning (RL). The core of this stage is to maximize the reward function, guiding the model to learn more effective reasoning strategies.\nReward Function To solve the problem of CoT language mixing during reasoning, the DeepSeek team introduced language consistency reward and combined it with task reward to form the total reward function:\n\\[ r(o) = r_{\\text{task}}(o) + \\alpha \\cdot r_{\\text{lang_consistency}}(o) \\]Parameter Description:\n\\( r(o) \\): Total reward function, the goal of RL training is to maximize this function. \\( r_{\\text{task}}(o) \\): Task reward based on task completion, measuring the accuracy of model reasoning. \\( r_{\\text{lang_consistency}}(o) \\): Language consistency reward, measuring the language purity of CoT output. \\( \\alpha \\): Hyperparameter, balancing the weights of task reward and language consistency reward. The total reward function is the weighted sum of task reward and language consistency reward. Maximizing \\( r(o) \\) drives the model to improve reasoning accuracy while maintaining the language consistency of CoT output. The role of \\( \\alpha \\) is to adjust the model\u0026rsquo;s emphasis on language consistency.\nTraining Objective By maximizing the above reward function, the DeepSeek team conducted RL training on the model after cold-start fine-tuning, optimizing model parameters to obtain higher reward values in reasoning tasks, and ultimately improving reasoning ability.\nRFT Rejection Sampling Fine-tuning (RFT) aims to improve the general ability of the model by refining training data. Its core idea is to minimize the selective loss function, guiding the model to learn the generation patterns of high-quality outputs.\nLoss Function RFT adopts a rejection sampling strategy to distinguish the generation and selection processes of reasoning data and non-reasoning data, and constructs a high-quality SFT dataset. The training objective is to minimize the following loss function:\n\\[ \\mathcal{L}_{RFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q) \\times \\pi_{sft}(O \\mid q)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\mathbb{I}(o) \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]Where, the indicator function \\( \\mathbb{I}(o) \\) is defined as:\n\\[ \\mathbb{I}(o) = \\begin{cases} 1, \u0026 \\text{if output } o \\text{ is judged to be high quality} \\\\ 0, \u0026 \\text{otherwise} \\end{cases} \\]Parameter Description:\n\\( \\mathcal{L}_{RFT}(\\theta) \\): RFT loss function. \\( P_{sft}(Q) \\): Distribution of question \\( q \\). \\( \\pi_{sft}(O \\mid q) \\): Conditional probability distribution of the SFT model generating output \\( O \\) given question \\( q \\). \\( \\mathbb{I}(o) \\): Indicator function, used to select high-quality answers. It is 1 when output \\( o \\) is judged to be high quality, and 0 otherwise. The RFT loss function is based on cross-entropy loss, and selectively learns high-quality outputs through the indicator function \\( \\mathbb{I}(o) \\):\nHigh-quality output (\\( \\mathbb{I}(o) = 1 \\)): The loss function degenerates into standard cross-entropy loss, and the model updates parameters based on the negative log-likelihood of high-quality answers, minimizing the difference between model predictions and high-quality answers. Low-quality output (\\( \\mathbb{I}(o) = 0 \\)): The loss function is zero, and low-quality answers do not participate in parameter updates. RFT guides the model to focus on learning the generation patterns of high-quality answers by minimizing the loss function, achieving selective learning.\nData Generation High-quality data (reasoning data): Generate candidate answers through the RL model, use a reward model (or DeepSeek-V3 model) to score, and reject sample to retain high-score answers. SFT data (non-reasoning data): Reuse the SFT dataset of DeepSeek-V3 and its generation process. Training Process Use the high-quality dataset obtained by rejection sampling to perform SFT on the DeepSeek-V3-Base model, minimize the RFT loss function, and improve the model\u0026rsquo;s reasoning and general abilities.\nRFT iteratively refines data and retrains the model, expecting the model to learn higher quality data patterns in each iteration, and finally converge to a high-quality output model. In the iterative process, the training data distribution \\( P_{sft}(Q, O) \\) gradually focuses on high-quality data, enabling the model to continuously improve its ability to generate high-quality outputs in the process of loss minimization.\nOnRFT Online Rejection Sampling Fine-tuning (OnRFT) has a similar objective to RFT, both aiming to learn high-quality output patterns by minimizing the selective loss function. The main difference between OnRFT and RFT is the data sampling method, and the loss function form is consistent with RFT. The gradient of the OnRFT loss function is:\n\\[ \\nabla_{\\theta} \\mathcal{L}_{OnRFT}(\\theta) = - \\mathbb{E}_{(q, o) \\sim P_{sft}(Q) \\times \\pi_{\\theta}(O \\mid q)}\\left[\\frac{1}{|o|} \\sum_{t=1}^{|o|} \\mathbb{I}(o) \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(o_t \\mid q, o_{\u0026lt;t} \\right)\\right] \\]Parameter Description:\n\\( \\nabla_{\\theta} \\mathcal{L}_{OnRFT} \\): Gradient of the OnRFT loss function with respect to model parameter \\( \\theta \\), indicating the direction of loss function decrease. \\( \\pi_{\\theta}(O \\mid q) \\): Conditional probability distribution of the current training model generating output \\( O \\) given question \\( q \\). Comparison of RFT and OnRFT The table below briefly compares the main differences between RFT and OnRFT.\nFeature RFT OnRFT Data Generation Method Offline Online Data Generation Model SFT model \\( \\pi_{sft} \\) Current training model \\( \\pi_{\\theta} \\) Rejection Sampling Data Source Pre-generated SFT dataset Real-time data generation during training Data Loop Separated Online loop Loss Function Mechanism Selective cross-entropy loss, selects high-quality output for learning Selective cross-entropy loss, selects high-quality output for learning Training Data Distribution Change Gradually focuses on high-quality data Dynamic change, fits current model capability All-Scenario Reinforcement Learning To further align with human preferences, the DeepSeek team conducted the second stage of RL, aiming to improve the model\u0026rsquo;s Helpfulness and Harmlessness while maximizing the reward function, and also taking into account reasoning ability. This stage still uses maximizing the reward function to guide model training, but the design of the reward function is more complex to reflect multi-dimensional optimization goals.\nThe RL training in this stage combines:\nDiverse Prompt Distribution: Covers a wider range of scenarios, including reasoning and general tasks. Multi-objective Reward Signals: Reasoning Data: Follows the rule-based task reward, focusing on reasoning accuracy. Maximize task reward to guide the model to minimize reasoning errors. General Data: Uses a reward model to capture human preferences for helpfulness and harmlessness. The goal of the reward model is to learn human preferences and output reward signals consistent with human preferences. The goal of RL training is to maximize the reward value given by the reward model, thereby indirectly minimizing the deviation between model output and human preferences. Distillation To transfer the powerful reasoning ability of DeepSeek-R1 to more efficient small models, the DeepSeek team adopted Distillation (Hinton et al., 2015) technology. The distillation process mainly includes the following steps:\nData Generation: Use the trained DeepSeek-R1 model to generate about 800,000 high-quality reasoning data. These data not only include reasoning-intensive tasks (such as math problems, programming problems), but also cover general tasks (such as question answering, dialogue) to ensure the diversity and coverage of distillation data.\nModel Fine-tuning: Use the generated 800,000 high-quality reasoning data to fine-tune small dense models. Distillation experiments selected Qwen and Llama series models as Student models, covering multiple model scales from 1.5B to 70B parameters to explore the effect of distillation technology under different model scales. The selected Student models include Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-14B, Qwen2.5-32B, Llama-3.1-8B, and Llama-3.3-70B-Instruct.\nPerformance Evaluation: Conduct a comprehensive performance evaluation of the distilled models in multiple reasoning-related benchmarks. The evaluation results are intended to verify whether distillation technology can effectively transfer the reasoning ability of large models to small models, and to investigate whether the reasoning ability of distilled small models can reach or even exceed the level of large models.\nKL Divergence Distillation In addition to directly using the text output generated by the Teacher model as pseudo-labels for SFT distillation, a more rigorous method is to also consider the token probability distribution \\( \\pi_{\\text{teacher}} \\) generated by the Teacher model. KL divergence distillation is a commonly used method, which not only allows the Student model to learn the text output of the Teacher model, but also learns the token probability distribution of the Teacher model. By minimizing the KL divergence between the output probability distributions of the Student model and the Teacher model, the knowledge of the Teacher model can be more fully transferred to the Student model. However, in actual engineering, directly using the text output of the Teacher model as pseudo-labels for SFT distillation can usually achieve sufficiently good results and is simpler to implement.\nExperimental Results The experimental results are shown in Figure 6:\nFig. 6. Comparison of DeepSeek-R1 distilled models and other comparable models on reasoning-related benchmarks. (Image source: DeepSeek-AI, 2025)\nThe experimental results indicate that this direct SFT distillation method can substantially enhance the inference capabilities of smaller models. Notably, on various benchmark tests excluding CodeForces, the distilled Llama-3.3-70B-Instruct model outperforms OpenAI-o1-mini. Achieving such remarkable improvements solely through SFT distillation from a larger base model demonstrates the strong potential of this approach for future research and applications.\nDiscussion DeepSeek-R1, based on a multi-stage training framework, explores a simplified path for Reasoning Model training technology, mainly including the following points:\nLinearized Thinking Process: CoT Replaces MCTS\nTraditional reinforcement learning AI, such as Go and chess, once relied on Monte Carlo Tree Search (MCTS). DeepSeek-R1 and other models explore the use of autoregressive chain-of-thought methods to simplify the reasoning process, gradually abandoning computationally complex MCTS. CoT decomposes complex reasoning into linear steps, and the model reasons step by step like solving a problem, rather than the exhaustive search of MCTS. This linearized thinking reduces computational complexity, is more in line with human thinking habits, and makes it easier for models to learn complex reasoning strategies. Eliminating Independent Value Networks: Simplifying RL Architecture\nTraditional reinforcement learning (such as PPO) usually requires independent policy networks and value networks. DeepSeek-R1 and other studies have found that strengthened policy networks or simplified value evaluation methods (such as GRPO\u0026rsquo;s in-group relative scoring) can replace independent value networks. This simplifies the RL training architecture, reduces resource requirements, and improves efficiency. It shows that the policy network of LLMs already has strong value evaluation capabilities, and no additional value network is needed. Focusing on Outcome Rewards: Minimizing Reward Signals\nDeepSeek-R1 adopts a simpler ORM reward strategy, mainly focusing on the accuracy reward of the final result, weakening the reward for intermediate reasoning steps. This strategy is inspired by AlphaZero (Silver et al., 2017), which only focuses on winning or losing. For Reasoning Models, outcome rewards may be more effective than PRM, which can help models learn \u0026ldquo;ways of thinking\u0026rdquo; more naturally and reduce cumbersome step-by-step supervision. Increasing Thinking Time: Model Spontaneously Emerges Deep Thinking\nDeepSeek-R1-Zero training shows the ability to spontaneously increase thinking time. As training deepens, the model adaptively allocates more \u0026ldquo;thinking time\u0026rdquo; according to the complexity of the question, generating longer reasoning sequences. This increase in \u0026ldquo;thinking time\u0026rdquo; is an emergent behavior of the model in RL training. Increasing thinking time reflects the model\u0026rsquo;s deeper exploration and optimization of the thinking process. Complex problems require more reasoning steps to find answers. The self-evolution ability of DeepSeek-R1-Zero confirms the potential of reinforcement learning in improving model reasoning ability. Summary The success of DeepSeek-R1 demonstrates the great potential of RL in improving the reasoning ability of LLMs. The GRPO algorithm adopted by DeepSeek-R1 is superior to PPO and DPO in terms of computational efficiency, optimization stability, and reward robustness, and reduces training resource consumption by simplifying the model architecture. DeepSeek-R1 provides a path worth referencing for open-source Reasoning Model replication of o1.\nReferences [1] OpenAI o1. OpenAI, 2024. (OpenAI O1 official introduction page)\n[2] Jaech, Aaron, et al. \u0026ldquo;OpenAI o1 system card.\u0026rdquo; arXiv preprint arXiv:2412.16720 (2024).\n[3] Open-r1. GitHub, 2024. (Open-r1 open source project GitHub repository)\n[4] Sutton, Richard. \u0026ldquo;The bitter lesson.\u0026rdquo; Incomplete Ideas (blog) 13.1 (2019): 38.\n[5] Liu A, et al. \u0026ldquo;Deepseek-v3 technical report.\u0026rdquo; arXiv preprint arXiv:2412.19437 (2024).\n[6] Schulman, John, et al. \u0026ldquo;Proximal policy optimization algorithms.\u0026rdquo; arXiv preprint arXiv:1707.06347 (2017).\n[7] Ouyang, Long, et al. \u0026ldquo;Training language models to follow instructions with human feedback.\u0026rdquo; Advances in neural information processing systems 35 (2022): 27730-27744.\n[8] Shao, Zhihong, et al. \u0026ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\u0026rdquo; arXiv preprint arXiv:2402.03300 (2024).\n[9] J. Schulman. Approximating kl divergence, 2020.\n[10] Gao, Leo, John Schulman, and Jacob Hilton. \u0026ldquo;Scaling laws for reward model overoptimization.\u0026rdquo; International Conference on Machine Learning. PMLR, 2023.\n[11] Chen, Mark, et al. \u0026ldquo;Evaluating large language models trained on code.\u0026rdquo; arXiv preprint arXiv:2107.03374 (2021).\n[12] Learning to Reason with LLMs. OpenAI, 2024. (OpenAI blog post about LLM reasoning ability)\n[13] AMC. Mathematical Association of America (MAA), 2024. (American Mathematics Competitions AMC official website)\n[14] Open-O1. GitHub, 2024. (Open-O1 open source project GitHub repository)\n[15] Zeng, Zhiyuan, et al. \u0026ldquo;Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective.\u0026rdquo; arXiv preprint arXiv:2412.14135 (2024).\n[16] Hinton, Geoffrey. \u0026ldquo;Distilling the Knowledge in a Neural Network.\u0026rdquo; arXiv preprint arXiv:1503.02531 (2015).\n[17] Silver, David, et al. \u0026ldquo;Mastering chess and shogi by self-play with a general reinforcement learning algorithm.\u0026rdquo; arXiv preprint arXiv:1712.01815 (2017).\nCitation Citation: Please indicate the original author and source when reprinting or citing the content of this article.\nCited as:\nYue Shui. (Jan 2025). o1 Replication Progress: DeepSeek-R1. https://syhya.github.io/posts/2025-01-27-deepseek-r1\nOr\n@article{syhya2025deepseekr1, title = \u0026#34;o1 Replication Progress: DeepSeek-R1\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-27-deepseek-r1\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-01-27-deepseek-r1/","summary":"\u003cp\u003eDeepSeek AI recently released \u003cstrong\u003eDeepSeek-R1\u003c/strong\u003e (\u003ca href=\"https://arxiv.org/abs/2501.12948\"\u003eDeepSeek-AI, 2025\u003c/a\u003e), whose reasoning performance on multiple benchmarks approaches the level of OpenAI\u0026rsquo;s o1 (\u003ca href=\"https://openai.com/o1/\"\u003eOpenAI, 2024\u003c/a\u003e), marking a significant step for the open-source community in successfully replicating o1. Relevant code for R1 can be found in the huggingface\u0026rsquo;s attempt to open-source replication project \u003ca href=\"https://github.com/huggingface/open-r1\"\u003eopen-r1\u003c/a\u003e. While previous research has often relied on massive amounts of supervised data to enhance the performance of Large Language Models (LLMs), the success of DeepSeek-R1 and its earlier experiment, DeepSeek-R1-Zero, powerfully demonstrates the potential of purely large-scale reinforcement learning in improving the reasoning capabilities of LLMs. This success reinforces the profound insight proposed by Richard Sutton in \u0026ldquo;The Bitter Lesson\u0026rdquo;:\u003c/p\u003e","title":"o1 Replication Progress: DeepSeek-R1"},{"content":"Background The Transformer (Vaswani et al., 2017) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (Devlin et al., 2018) which uses only the encoder, GPT (Radford et al., 2018) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (Touvron et al., 2023) and GPT-4 (OpenAI et al., 2024), most of which adopt a decoder-only architecture.\nNotations Symbol Meaning \\(B\\) Batch Size \\(S\\) Sequence Length \\(d\\) Hidden Dimension / Model Size \\(H\\) Number of Heads in Multi-Head Attention \\(G\\) Number of Groups, used for Grouped-Query Attention (GQA) \\(d_{\\text{head}} = \\frac{d}{H}\\) Dimension of each attention head \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) Input sequence, with batch size \\(B\\), sequence length \\(S\\), and hidden dimension \\(d\\) \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{B \\times S \\times d}\\) Query, Key, and Value matrices after linear transformation \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) Trainable linear projection matrices for generating \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) respectively \\(W_O \\in \\mathbb{R}^{d \\times d}\\) Trainable linear projection matrix for mapping multi-head/grouped attention outputs back to dimension \\(d\\) \\(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h \\in \\mathbb{R}^{B \\times S \\times d_{\\text{head}}}\\) Query, Key, and Value sub-matrices for the \\(h\\)-th attention head \\(\\mathbf{K}^*, \\mathbf{V}^*\\) Shared \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) obtained by averaging or merging all heads\u0026rsquo; \\(\\mathbf{K}_h, \\mathbf{V}_h\\) in Multi-Query Attention (MQA) \\(\\mathbf{q}, \\mathbf{k}\\in \\mathbb{R}^{d_{\\text{head}}}\\) Single query and key vectors used in mathematical derivations (Central Limit Theorem) in Scaled Dot-Product Attention Attention Mechanism in Transformers The core of the Transformer model is the Self-Attention Mechanism, which allows the model to dynamically focus on different parts of the sequence when processing sequential data. Specifically, given an input sequence \\(\\mathbf{X} \\in \\mathbb{R}^{B \\times S \\times d}\\) (batch size \\(B\\), sequence length \\(S\\), hidden dimension \\(d\\)), the Transformer projects it into queries (\\(\\mathbf{Q}\\)), keys (\\(\\mathbf{K}\\)), and values (\\(\\mathbf{V}\\)) through three linear layers:\n\\[ \\mathbf{Q} = \\mathbf{X} W_Q, \\quad \\mathbf{K} = \\mathbf{X} W_K, \\quad \\mathbf{V} = \\mathbf{X} W_V \\]where \\(W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}\\) are trainable weight matrices. MHA enhances the model\u0026rsquo;s representational capacity by splitting these projections into multiple heads, each responsible for different subspace representations.\nThere are various forms of attention mechanisms, and the Transformer relies on Scaled Dot-Product Attention: given query matrix \\(\\mathbf{Q}\\), key matrix \\(\\mathbf{K}\\), and value matrix \\(\\mathbf{V}\\), the output is a weighted sum of the value vectors, where each weight is determined by the dot product of the query with the corresponding key:\n\\[ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V} \\] Fig. 1. Scaled Dot-Product Attention. (Image source: Vaswani et al., 2017)\nMulti-Head Attention (MHA) Multi-Head Attention (MHA) splits \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) into multiple heads, each with independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\), thereby increasing the model\u0026rsquo;s capacity and flexibility:\n\\[ \\text{MHA}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]where each head is computed as:\n\\[ \\text{head}_h = \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h) = \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q}_h \\mathbf{K}_h^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr)\\,\\mathbf{V}_h \\] Fig. 2. Multi-Head Attention. (Image source: Vaswani et al., 2017)\nBenefits of Using Multi-Head Attention Capturing Diverse Features: A single-head attention mechanism can only focus on one type of feature or pattern in the input sequence, whereas MHA can simultaneously focus on different features or patterns across multiple attention heads, enabling the model to understand the input data more comprehensively. Enhanced Expressive Power: Each attention head can learn different representations, enhancing the model\u0026rsquo;s expressive power. Different heads can focus on different parts or relationships within the input sequence, helping the model capture complex dependencies more effectively. Improved Stability and Performance: MHA reduces noise and instability from individual attention heads by averaging or combining multiple heads, thereby improving the model\u0026rsquo;s stability and performance. Parallel Computation: MHA allows for parallel computation since each attention head\u0026rsquo;s calculations are independent. This boosts computational efficiency, especially when using hardware accelerators like GPUs or TPUs. Softmax in Scaled Dot-Product Attention The softmax function transforms a vector \\(\\mathbf{z} = [z_1, z_2, \\dots, z_n]\\) into a probability distribution \\(\\mathbf{y} = [y_1, y_2, \\dots, y_n]\\) defined as:\n\\[ y_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)} \\quad \\text{for} \\quad i = 1, 2, \\dots, n \\]In the attention mechanism, the softmax function is used to convert the scaled dot product \\(\\tfrac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\) into attention weights:\n\\[ \\text{softmax}\\!\\Bigl(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_{\\text{head}}}}\\Bigr) = \\Bigl[ \\frac{\\exp\\Bigl(\\frac{Q_1 \\cdot K_1}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_1 \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)}, \\dots, \\frac{\\exp\\Bigl(\\frac{Q_S \\cdot K_S}{\\sqrt{d_{\\text{head}}}}\\Bigr)}{\\sum_{j=1}^{S} \\exp\\Bigl(\\frac{Q_S \\cdot K_j}{\\sqrt{d_{\\text{head}}}}\\Bigr)} \\Bigr] \\]In the Transformer\u0026rsquo;s attention mechanism, the scaling factor \\(\\sqrt{d_{\\text{head}}}\\) in the scaled dot-product attention formula ensures that the dot product results do not become excessively large as the vector dimension increases before applying softmax. This is primarily for the following reasons:\nPreventing Gradient Vanishing: Scaling the attention scores avoids overly large inputs to the softmax function, preventing gradients from vanishing during backpropagation.\nNumerical Stability: Scaling ensures that the input range to the softmax function remains reasonable, avoiding extreme values that could lead to numerical instability and overflow issues, especially when the vector dimensions are large. Without scaling, the dot product results could cause the softmax\u0026rsquo;s exponential function to produce excessively large values.\nMathematical Explanation: Suppose vectors \\(\\mathbf{q}\\) and \\(\\mathbf{k}\\) have independent and identically distributed components with mean 0 and variance 1. Their dot product \\(\\mathbf{q} \\cdot \\mathbf{k}\\) has a mean of 0 and a variance of \\(d_{\\text{head}}\\). To prevent the dot product\u0026rsquo;s variance from increasing with the dimension \\(d_{\\text{head}}\\), it is scaled by \\(\\frac{1}{\\sqrt{d_{\\text{head}}}}\\). This scaling ensures that the variance of the scaled dot product remains 1, independent of \\(d_{\\text{head}}\\).\nAccording to statistical principles, dividing a random variable by a constant scales its variance by the inverse square of that constant. Therefore, the scaling factor \\(\\tfrac{1}{\\sqrt{d_{\\text{head}}}}\\) effectively controls the magnitude of the attention scores, enhancing numerical stability. The detailed derivation is as follows:\nAssume \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^{d_{\\text{head}}}\\) with independent and identically distributed components, mean 0, and variance 1. Their dot product is:\n\\[ \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d_{\\text{head}}} q_i k_i \\]By the Central Limit Theorem, for large \\(d_{\\text{head}}\\), the dot product \\(\\mathbf{q} \\cdot \\mathbf{k}\\) approximately follows a normal distribution with mean 0 and variance \\(d_{\\text{head}}\\):\n\\[ \\mathbf{q} \\cdot \\mathbf{k} \\sim \\mathcal{N}(0, d_{\\text{head}}) \\]To achieve unit variance in the scaled dot product, we divide by \\(\\sqrt{d_{\\text{head}}}\\):\n\\[ \\frac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}} \\;\\sim\\; \\mathcal{N}\\!\\Bigl(0, \\frac{d_{\\text{head}}}{d_{\\text{head}}}\\Bigr) = \\mathcal{N}(0, 1) \\]Thus, the scaled dot product \\(\\tfrac{\\mathbf{q} \\cdot \\mathbf{k}}{\\sqrt{d_{\\text{head}}}}\\) maintains a variance of 1, independent of \\(d_{\\text{head}}\\). This scaling operation keeps the dot product within a stable range, preventing the softmax function from encountering numerical instability due to excessively large or small input values.\nMulti-Query Attention (MQA) Multi-Query Attention (MQA) (Shazeer, 2019) significantly reduces memory bandwidth requirements by allowing all query heads to share the same set of keys (\\(\\mathbf{K}\\)) and values (\\(\\mathbf{V}\\)). Specifically, if we average all \\(\\mathbf{K}_h\\) and \\(\\mathbf{V}_h\\) from traditional MHA as follows:\n\\[ \\mathbf{K}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{K}_h, \\quad \\mathbf{V}^* = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{V}_h, \\]where \\(H\\) is the number of query heads, and \\(\\mathbf{K}_h\\) and \\(\\mathbf{V}_h\\) are the keys and values for the \\(h\\)-th head, respectively. During inference, each head only needs to use the same \\(\\mathbf{K}^*\\) and \\(\\mathbf{V}^*\\), significantly reducing memory bandwidth usage. Finally, all head outputs are concatenated and projected back to the output space:\n\\[ \\text{MQA}(\\mathbf{Q}, \\mathbf{K}^*, \\mathbf{V}^*) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_H)\\, W_O \\]Since keys and values are consolidated into a single set, MQA inference is faster but may limit the model\u0026rsquo;s expressive capacity and performance in certain scenarios.\nGrouped-Query Attention (GQA) Grouped-Query Attention (GQA) (Ainslie, 2023) serves as a compromise between MHA and MQA. It divides query heads into multiple groups, allowing each group to share a set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads, thereby balancing inference speed and model performance. Each group contains \\(\\frac{H}{G}\\) query heads and shares one set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads. The specific process is as follows:\nProjection: Project the input \\(\\mathbf{X}\\) into \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) via linear transformations. Grouped Query: After splitting \\(\\mathbf{Q}\\) into \\(H\\) heads, further divide these heads into \\(G\\) groups. Grouped Key/Value: Split \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) into \\(G\\) groups, with each group sharing a set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Within-Group Attention: Perform attention calculations for each group\u0026rsquo;s \\(\\mathbf{Q}\\) with the shared \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Concatenate Outputs: Concatenate the attention results from all groups along the channel dimension and project them through a linear layer to obtain the final output. Relationship Between the Three Attention Methods Fig. 3. Overview of grouped-query method. (Image source: Ainslie et al., 2023)\nFigure 3 intuitively illustrates the relationship between the three attention mechanisms: MHA maintains independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) for each query head; MQA allows all query heads to share the same set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\); GQA strikes a balance by sharing \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) within groups.\nWhen \\(G=1\\): All query heads share the same set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). In this case, GQA degenerates into MQA.\nNumber of \\(\\mathbf{K}/\\mathbf{V}\\) Heads: 1 Model Behavior: All heads use the same \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) for attention calculations, significantly reducing memory bandwidth requirements. When \\(G=H\\): Each query head has its own independent set of \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). In this case, GQA degenerates into MHA.\nNumber of \\(\\mathbf{K}/\\mathbf{V}\\) Heads: \\(H\\) Model Behavior: Each head uses completely independent \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\), maintaining the high model capacity and performance of MHA. By adjusting the number of groups \\(G\\), GQA allows flexible switching between MHA and MQA, achieving a balance between maintaining high model performance and improving inference speed.\nImplementation Code Snippet Below is a simple PyTorch implementation of MHA 、MQA和 GQA. For GQA, two approaches are demonstrated: broadcasting and repeating.\nAdditionally, note that in the actual implementation of LLaMA3, GQA incorporates KV Cache for optimization. To keep the example concise, this part is omitted in the code below. For more comprehensive details, you can refer to the official source code in model.py.\nMHA Code Snippet multi_head_attention.py\nimport math import torch import torch.nn as nn class MultiHeadAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # (nums_head * head_dim = hidden_dim) assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(dropout_rate) # Define linear projection layers self.q_proj = nn.Linear(hidden_dim, hidden_dim) self.k_proj = nn.Linear(hidden_dim, hidden_dim) self.v_proj = nn.Linear(hidden_dim, hidden_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): # x has shape: (batch_size, seq_len, hidden_dim) batch_size, seq_len, _ = x.size() # Q, K, V each has shape: (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) K = self.k_proj(x) V = self.v_proj(x) # Reshaping from (batch_size, seq_len, hidden_dim) to (batch_size, seq_len, nums_head, head_dim) # Then transpose to (batch_size, nums_head, seq_len, head_dim) # q_state = Q.view(batch_size, seq_len, self.head_num, self.head_dim).permute(0, 2, 1, 3) # [Another approach to do it] q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) k = K.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Matrix multiplication: (batch_size, nums_head, seq_len, head_dim) * (batch_size, nums_head, head_dim, seq_len) # Resulting shape: (batch_size, nums_head, seq_len, seq_len) # Note that the scaling factor uses head_dim, not hidden_dim. attention_val = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) print(f\u0026#34;attention_mask shape is {attention_mask.size()}\u0026#34;) if attention_mask is not None: # If attention_mask is provided, it should have shape (batch_size, nums_head, seq_len, seq_len). assert attention_val.size() == attention_mask.size() attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # Apply softmax along the last dimension to get attention weights. attention_weight = torch.softmax(attention_val, dim=-1) # Dropout on attention weights attention_weight = self.dropout(attention_weight) # Multiply attention weights with V: # (batch_size, nums_head, seq_len, seq_len) * (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v # Transpose back: (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) # # Note: The transpose operation changes the dimension ordering but does not change the memory layout, # resulting in a non-contiguous tensor. The contiguous() method makes the tensor contiguous in memory, # allowing subsequent view or reshape operations without error. output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) # output = output_mid.permute(0, 2, 1, 3).reshpae(batch_size, seq_len, self.hidden_dim) # # [Another approach to do it] output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 # attention_mask has shape: (batch_size, nums_head, seq_len, seq_len). # Here we use a lower-triangular mask to simulate causal masking. attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_head_attention = MultiHeadAttention(hidden_dim=hidden_dim, nums_head=nums_head) x_forward = multi_head_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) MQA Code Snippet multi_query_attention.py\nimport torch import torch.nn as nn import math class MultiQueryAttention(nn.Module): def __init__(self, hidden_dim, nums_head, dropout=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head assert hidden_dim % nums_head == 0 self.head_dim = hidden_dim // nums_head self.dropout = nn.Dropout(p=dropout) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # For kv, project: hidden_dim -\u0026gt; head_dim self.k_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.v_proj = nn.Linear(hidden_dim, self.head_dim * 1) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() Q, K, V = self.q_proj(x), self.k_proj(x), self.v_proj(x) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # Broadcast k and v to match q\u0026#39;s dimensions for attention computation # k -\u0026gt; (batch_size, 1, seq_len, head_dim) # v -\u0026gt; (batch_size, 1, seq_len, head_dim) k = K.unsqueeze(1) v = V.unsqueeze(1) # (batch_size, head_num, seq_len, head_dim) * (batch_size, 1, head_dim, seq_len) # -\u0026gt; (batch_size, head_num, seq_len, seq_len) attention_val = (q @ k.transpose(-1, -2)) / math.sqrt(self.head_dim) print(f\u0026#34;attention_val shape is {attention_val.size()}\u0026#34;) if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) attention_weight = torch.softmax(attention_val, dim=-1) print(f\u0026#34;attention_weight is {attention_weight}\u0026#34;) attention_weight = self.dropout(attention_weight) # (batch_size, head_num, seq_len, seq_len) * (batch_size, 1, seq_len, head_dim) # -\u0026gt; (batch_size, head_num, seq_len, head_dim) output_tmp = attention_weight @ v # -\u0026gt; (batch_size, seq_len, head_num, head_dim) # -\u0026gt; (batch_size, seq_len, hidden_dim) output_tmp = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_tmp) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 4) batch_size, seq_len, hidden_dim = x.size() nums_head = 2 attention_mask = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) print(attention_mask) multi_query_attention = MultiQueryAttention(hidden_dim=hidden_dim, nums_head=nums_head, dropout=0.2) x_forward = multi_query_attention.forward(x, attention_mask=attention_mask) print(x_forward) print(x_forward.size()) GQA Code Snippet group_query_attention.py\nimport math import torch import torch.nn as nn class GQABroadcast(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head # Total number of Q heads (H) self.nums_kv_head = nums_kv_head # Number of K, V heads (G, groups) assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head # Number of Q heads per group self.q_heads_per_group = nums_head // nums_kv_head self.dropout = nn.Dropout(dropout_rate) self.q_proj = nn.Linear(hidden_dim, hidden_dim) # Projection output dimensions for K, V = nums_kv_head * head_dim self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) def forward(self, x, attention_mask= None): batch_size, seq_len, _ = x.size() Q = self.q_proj(x) # (batch_size, seq_len, hidden_dim) K = self.k_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) V = self.v_proj(x) # (batch_size, seq_len, nums_kv_head * head_dim) # Q: (batch_size, seq_len, hidden_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2).contiguous() q = q.view(batch_size, self.nums_kv_head, self.q_heads_per_group, seq_len, self.head_dim) # K, V: (batch_size, seq_len, nums_kv_head * head_dim) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim # -\u0026gt; (batch_size, nums_kv_head, 1, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2).unsqueeze(2) # q: (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) * (batch_size, nums_kv_head, 1, head_dim, seq_len) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_val = q @ k.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#34;-inf\u0026#34;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) * (batch_size, nums_kv_head, 1, seq_len, head_dim) # -\u0026gt; (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) output_tmp = attention_weight @ v # (batch_size, nums_kv_head, q_heads_per_group, seq_len, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) output_tmp = output_tmp.view(batch_size, self.nums_head, seq_len, self.head_dim) # (batch_size, nums_head, seq_len, head_dim) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) -\u0026gt; (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output class GQARepeat(nn.Module): \u0026#34;\u0026#34;\u0026#34; Group Query Attention (GQA) implementation: By configuring `nums_kv_head` (G, the number of groups), this module supports: - When nums_kv_head == nums_head: Multi-Head Attention (MHA) - When nums_kv_head == 1: Multi-Query Attention (MQA) - When 1 \u0026lt; nums_kv_head \u0026lt; nums_head: Generic Grouped Query Attention (GQA) \u0026#34;\u0026#34;\u0026#34; def __init__(self, hidden_dim, nums_head, nums_kv_head, dropout_rate=0.1): super().__init__() self.hidden_dim = hidden_dim self.nums_head = nums_head self.nums_kv_head = nums_kv_head assert hidden_dim % nums_head == 0 assert nums_head % nums_kv_head == 0 self.head_dim = hidden_dim // nums_head self.q_head_per_group = nums_head // nums_kv_head self.q_proj = nn.Linear(hidden_dim, nums_head * self.head_dim) self.k_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.v_proj = nn.Linear(hidden_dim, nums_kv_head * self.head_dim) self.output_proj = nn.Linear(hidden_dim, hidden_dim) self.dropout = nn.Dropout(dropout_rate) def forward(self, x, attention_mask=None): batch_size, seq_len, _ = x.size() # (batch_size, seq_len, hidden_dim) Q = self.q_proj(x) # (batch_size, seq_len, nums_kv_head * self.head_dim) K = self.k_proj(x) V = self.v_proj(x) # -\u0026gt; (batch_size, seq_len, nums_head, head_dim) # -\u0026gt; (batch_size, nums_head, seq_len, head_dim) q = Q.view(batch_size, seq_len, self.nums_head, self.head_dim).transpose(1, 2) # -\u0026gt; (batch_size, seq_len, nums_kv_head, head_dim) # -\u0026gt; (batch_size, nums_kv_head, seq_len, head_dim) k = K.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) v = V.view(batch_size, seq_len, self.nums_kv_head, self.head_dim).transpose(1, 2) # (batch_size, nums_head, seq_len, head_dim) k_repeat = k.repeat_interleave(self.q_head_per_group, dim=1) v_repeat = v.repeat_interleave(self.q_head_per_group, dim=1) # (batch_size, nums_head, seq_len, seq_len) attention_val = q @ k_repeat.transpose(-1, -2) / math.sqrt(self.head_dim) # mask if attention_mask is not None: attention_val = torch.masked_fill(attention_val, attention_mask == 0, float(\u0026#39;-inf\u0026#39;)) # softmax attention_weight = torch.softmax(attention_val, dim=-1) # dropout attention_weight = self.dropout(attention_weight) # (batch_size, nums_head, seq_len, head_dim) output_tmp = attention_weight @ v_repeat # (batch_size, seq_len, hidden_dim) output_concat = output_tmp.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim) output = self.output_proj(output_concat) return output if __name__ == \u0026#34;__main__\u0026#34;: x = torch.randn(2, 3, 16) batch_size, seq_len, hidden_dim = x.size() nums_head = 8 head_dim = hidden_dim // nums_head nums_kv_head = 4 q_heads_per_group = nums_head // nums_kv_head # v1: Boardcast # attention_mask_v1 has shape: (batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len) attention_mask_v1 = torch.tril(torch.ones(batch_size, nums_kv_head, q_heads_per_group, seq_len, seq_len)) gqa_boradcast = GQABroadcast(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v1 = gqa_boradcast.forward(x, attention_mask=attention_mask_v1) # print(x_forward_v1) print(x_forward_v1.size()) # v2: Repeat # attention_mask_v2 has shape: (batch_size, nums_head, seq_len, seq_len) attention_mask_v2 = torch.tril(torch.ones(batch_size, nums_head, seq_len, seq_len)) gqa_repeat = GQARepeat(hidden_dim=hidden_dim, nums_head=nums_head, nums_kv_head=nums_kv_head, dropout_rate=0.1) x_forward_v2 = gqa_repeat.forward(x, attention_mask=attention_mask_v2) # print(x_forward_v2) print(x_forward_v2.size()) Time and Space Complexity Analysis Note: The following discussion focuses on the computational complexity of a single forward pass. In training, one must also account for backward pass and parameter updates, which rely on the intermediate activations stored during the forward pass. The additional computation to calculate gradients and maintain partial derivatives usually makes the total training cost (both computation and memory usage) significantly higher—often multiple times the forward-pass cost.\nWhen analyzing different attention mechanisms (MHA, MQA, GQA), our main focus is on their time complexity and space complexity during the forward pass of either self-attention or cross-attention. Even though their implementation details (e.g., whether \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) are shared) can differ, the overall computational cost and memory usage are roughly on the same order of magnitude.\nAssume that each position in the sequence produces its own representations of query \\(\\mathbf{Q}\\), key \\(\\mathbf{K}\\), and value \\(\\mathbf{V}\\). After splitting by batch size and number of heads, their shapes can be written as:\n\\[ \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\;\\in\\; \\mathbb{R}^{B \\times H \\times S \\times d_{\\text{head}}} \\]Time Complexity Analysis General Time Complexity of Matrix Multiplication For two matrices \\(\\mathbf{A}\\) of shape \\(m \\times n\\) and \\(\\mathbf{B}\\) of shape \\(n \\times p\\), the complexity of computing the product \\(\\mathbf{A}\\mathbf{B}\\) is typically expressed as:\n\\[ \\mathcal{O}(m \\times n \\times p) \\]In attention-related computations, this formula is frequently used to analyze \\(\\mathbf{Q}\\mathbf{K}^\\top\\) and the multiplication of attention scores by \\(\\mathbf{V}\\).\nMain Steps and Complexity in Self-Attention Dot Product (\\(\\mathbf{Q}\\mathbf{K}^\\top\\))\nShape of \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nShape of \\(\\mathbf{K}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nConsequently, the result of \\(\\mathbf{Q}\\mathbf{K}^\\top\\) has shape \\(B \\times H \\times S \\times S\\)\nThe calculation can be viewed as \\(S \\times S\\) dot products for each head in each batch. Each dot product involves \\(d_{\\text{head}}\\) multiply-add operations.\nHence, its time complexity is:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S \\times S \\times d_{\\text{head}}\\bigr) \\;=\\; \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] Softmax Operation\nApplied element-wise to the attention score matrix of shape \\(B \\times H \\times S \\times S\\)\nSoftmax entails computing exponentials and performing normalization on each element. The complexity is approximately:\n\\[ \\mathcal{O}(\\text{number of elements}) = \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) \\] Compared with the matrix multiplication above, this step’s dependency on \\(d_{\\text{head}}\\) is negligible and is thus often considered a smaller overhead.\nWeighted Averaging (Multiplying Attention Scores with \\(\\mathbf{V}\\))\nShape of \\(\\mathbf{V}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nShape of the attention score matrix: \\(B \\times H \\times S \\times S\\)\nMultiplying each position’s attention scores by the corresponding \\(\\mathbf{V}\\) vector yields an output of shape \\(B \\times H \\times S \\times d_{\\text{head}}\\)\nIts time complexity is analogous to that of \\(\\mathbf{Q}\\mathbf{K}^\\top\\):\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] Combining these three steps, the dominant costs come from the two matrix multiplications, each of complexity \\(\\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}})\\). Therefore, for a single full forward pass, the total complexity can be denoted as:\n\\[ \\mathcal{O}(B \\times H \\times S^2 \\times d_{\\text{head}}) = \\mathcal{O}(B \\times S^2 \\times d) \\]Here, we use \\(d_{\\text{head}} = \\frac{d}{H}\\).\nTime Complexity in Incremental Decoding/Inference with KV Cache Fig. 4. KV cache example. (Image source: Efficient NLP YouTube Channel)\nAs depicted in Figure 4, incremental decoding (especially in autoregressive generation) often employs a KV Cache to store previously computed \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\). Thus, one does not have to recalculate keys and values at each new time step. With every new token generated (i.e., a new time step), the following operations are performed:\nCompute \\(\\mathbf{Q}\\) for the New Token (and corresponding \\(\\mathbf{K}, \\mathbf{V}\\))\nIf only the projection weights are retained, then generating the new \\(\\mathbf{Q}\\) vector and the local \\(\\mathbf{K}, \\mathbf{V}\\) involves \\(\\mathcal{O}(d^2)\\) parameters, but this overhead is small as it is only for a single token. Perform Attention with the Existing KV Cache\nThe KV Cache stores all previous \\(\\mathbf{K}, \\mathbf{V}\\) vectors, with shape:\n\\[ B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} \\]Here, \\(S_{\\text{past}}\\) is the length of the already-generated sequence.\nThe new \\(\\mathbf{Q}\\) has shape \\(B \\times H \\times 1 \\times d_{\\text{head}}\\). Hence, computing the attention scores for the new token:\n\\[ \\mathbf{Q}\\mathbf{K}^\\top : \\; \\mathcal{O}\\bigl(B \\times H \\times 1 \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] Similarly, multiplying these scores by \\(\\mathbf{V}\\) has the same order:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] Update the KV Cache\nAppend the newly generated \\(\\mathbf{K}, \\mathbf{V}\\) to the cache, so they can be used at the subsequent time step. This merely requires a concatenation or append operation, which primarily grows the memory usage rather than incurring high compute. Thus, for incremental decoding, each new token involves:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] in computation, instead of the \\(S \\times S\\) scale for each forward pass. If one aims to generate \\(S\\) tokens in total, the cumulative complexity (under ideal conditions) becomes:\n\\[ \\sum_{k=1}^{S} \\mathcal{O}\\bigl(B \\times H \\times k \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) \\] which is the same order as the one-shot computation. The difference is that incremental decoding computes one token at a time, thus requiring lower temporary memory usage per step and avoiding a full \\(S \\times S\\) attention score matrix at once.\nSummary of Time Complexity MHA (Multi-Head Attention): Multiple heads, each computing its own \\(\\mathbf{K}, \\mathbf{V}\\). MQA (Multi-Query Attention): Multiple heads share \\(\\mathbf{K}, \\mathbf{V}\\). GQA (Grouped Query Attention): The \\(H\\) heads are divided into \\(G\\) groups, each group sharing a single \\(\\mathbf{K}, \\mathbf{V}\\). Regardless of whether we use MHA, MQA, or GQA, in a full forward pass (or the forward portion during training), the main matrix multiplications have roughly the same complexity:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) \\]On the other hand, in incremental inference with KV Cache, the per-token complexity decreases to\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] but one must maintain and update the KV Cache over multiple decoding steps.\nSpace Complexity Analysis Space complexity encompasses both model parameters (weights) and intermediate activations needed during the forward pass—particularly the attention score matrices, weighted outputs, and potential KV Cache.\nModel Parameter Scale Parameters for the Linear Projection Layers\nProjecting the input vector of dimension \\(d\\) into \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\):\n\\[ \\underbrace{d \\times d}_{\\text{Q projection}} + \\underbrace{d \\times d}_{\\text{K projection}} + \\underbrace{d \\times d}_{\\text{V projection}} = 3d^2 \\] These parameters may be split among heads, but the total remains \\(\\mathcal{O}(d^2)\\), independent of the number of heads \\(H\\).\nOutput Merging Layer\nAfter concatenating multiple heads, there is typically another \\(d \\times d\\) linear layer to project the concatenated outputs back into dimension \\(d\\). This is also \\(\\mathcal{O}(d^2)\\).\nTherefore, combining these yields:\n\\[ 3d^2 + d^2 = 4d^2 \\] which remains \\(\\mathcal{O}(d^2)\\).\nIntermediate Activations for the Forward Pass During training or a full forward pass, the following key tensors often need to be stored:\nAttention Score Matrix\nShape: \\(B \\times H \\times S \\times S\\). Regardless of MHA, MQA, or GQA, each head (or group) computes \\(\\mathbf{Q}\\mathbf{K}^\\top\\) for the attention scores, yielding:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2\\bigr) \\] Weighted Output\nShape: \\(B \\times H \\times S \\times d_{\\text{head}}\\), corresponding to the contextual vectors after weighting \\(\\mathbf{V}\\). Its size is:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S \\times d\\bigr) \\] Storage of \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}\\) for Backprop\nIn backward propagation, we need the forward outputs (or intermediate gradients). If explicitly stored, their shapes and scales are usually:\nMHA (Multi-Head Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) MQA (Multi-Query Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\) (shared): \\(B \\times S \\times d\\) GQA (Grouped Query Attention) \\(\\mathbf{Q}\\): \\(B \\times H \\times S \\times d_{\\text{head}}\\) \\(\\mathbf{K}, \\mathbf{V}\\) (shared by group): \\(B \\times G \\times S \\times d_{\\text{head}}\\), where \\(G \\times d_{\\text{head}} = d\\). Space Usage in Incremental Decoding (KV Cache) In inference with incremental decoding, a KV Cache is typically used to store all previously computed keys and values, thus avoiding repeated computation for past tokens. The structure is generally as follows:\nKV Cache Dimensions (MHA example):\n\\[ \\mathbf{K}, \\mathbf{V} : B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}} \\] As the generated sequence length \\(S_{\\text{past}}\\) grows, the cache usage increases linearly.\nPer-Step Attention Score Matrix:\nEach new token only requires a score matrix of shape:\n\\[ B \\times H \\times 1 \\times S_{\\text{past}} \\]which is much smaller than the \\(B \\times H \\times S \\times S\\) matrix used during training.\nTherefore, in incremental decoding, large temporary activations—such as the \\(S \\times S\\) score matrix—are not needed; however, the KV Cache itself (size \\(\\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}})\\)) must be maintained and grows along with the sequence length.\nCombined Space Complexity Training / Full Forward\nThe main activations (attention scores + weighted outputs + explicit storage of Q,K,V) add up to:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) \\]For large \\(S\\), the \\(\\mathcal{O}(B \\times H \\times S^2)\\) term tends to dominate.\nInference / Incremental Decoding (KV Cache)\nThere is no need for the full \\(S^2\\) attention matrix, but a KV Cache of size\n\\[ \\mathcal{O}(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}) \\] must be stored. This grows linearly with the decoding steps \\(S_{\\text{past}}\\).\nMeanwhile, the per-step attention matrix is only \\(B \\times H \\times 1 \\times S_{\\text{past}}\\), significantly smaller than the \\(\\mathcal{O}(S^2)\\) scenario in training.\nConclusions and Comparisons Time Complexity\nFor self-attention—whether using MHA, MQA, or GQA—in a full forward pass (which also applies to the forward portion during training), the principal matrix multiplications remain:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 \\times d_{\\text{head}}\\bigr) = \\mathcal{O}\\bigl(B \\times S^2 \\times d\\bigr) \\] In incremental inference (KV Cache), each new token only requires\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\]but the KV Cache must be updated and maintained throughout the decoding sequence.\nSpace Complexity\nModel Parameters: All three attention mechanisms (MHA, MQA, GQA) reside in \\(\\mathcal{O}(d^2)\\) parameter space.\nIntermediate Activations (Training / Full Forward): Dominated by the attention score matrix and weighted outputs:\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S^2 + B \\times S \\times d\\bigr) \\] Incremental Decoding (KV Cache): Saves on the \\(\\mathcal{O}(S^2)\\) score matrix cost but requires\n\\[ \\mathcal{O}\\bigl(B \\times H \\times S_{\\text{past}} \\times d_{\\text{head}}\\bigr) \\] of storage for the KV Cache, which increases linearly with \\(S_{\\text{past}}\\).\nBenefits of MQA / GQA\nAlthough from a high-level perspective, MHA, MQA, and GQA share similar asymptotic complexities when \\(S\\) is large, MQA and GQA can achieve improved efficiency in practice due to key/value sharing (or partial sharing) which can reduce memory bandwidth demands and improve cache locality. Consequently, in real-world systems, they often deliver better speed and memory performance. The table below summarizes the main differences among MHA, MQA, and GQA attention mechanisms:\nFeature MHA MQA GQA Number of K/V Heads Same as number of heads (\\(H\\)) Single K/V head Number of groups (\\(G\\)), one K/V head per group Inference Time Slower Fastest Faster, but slightly slower than MQA Memory Bandwidth Requirement Highest, \\(H\\) times K/V loading Lowest, only one K/V head Between MHA and MQA, \\(G\\) times K/V loading Model Capacity Highest Lowest Moderate, depending on the number of groups \\(G\\) Performance Best Slightly lower than MHA Close to MHA, significantly better than MQA Uptraining Requirement None High, requires more stability and tuning Lower, GQA models stabilize after minimal uptraining Applicable Scenarios Applications with high performance requirements but insensitive to inference speed Scenarios requiring extremely fast inference with lower model performance demands Applications needing a balance between inference speed and model performance In summary, from a theoretical standpoint, all three attention mechanisms (MHA, MQA, GQA) share \\(\\mathcal{O}(B \\times S^2 \\times d)\\) complexity in a full pass and \\(\\mathcal{O}(B \\times S_{\\text{past}} \\times d)\\) per-step complexity in incremental decoding.\nExperimental Results Performance Testing This experiment was conducted on an environment equipped with dual NVIDIA RTX 4090 GPUs using data parallelism (DP), evenly splitting the batch size across both GPUs. The experiment only tested the performance of the forward pass, including average latency time (Time_mean, unit: ms) and peak memory usage (Peak_Mem_mean, unit: MB), to evaluate the resource requirements and efficiency of different attention mechanisms (MHA, MQA, and GQA) during the inference phase. You can get the source code in benchmark_attention.py.\nThe tests were based on Llama3 8B hyperparameters. Fig. 5. Overview of the key hyperparameters of Llama 3. (Image source: Grattafiori et al., 2024)\nThe main configuration parameters are as follows:\nTotal Layers: 32 layers. Hidden Layer Dimension: 4096. Total Number of Multi-Head Attention Heads: 32. Different Group Configurations (nums_kv_head): 32 (MHA), 1 (MQA), 8 (GQA-8). Experimental Results This section primarily introduces the experimental performance of MHA, MQA, and GQA-8 under different sequence lengths (512, 1024, and 1536), including data on latency and memory usage. For ease of comparison, the table below presents the specific test results for the three attention mechanisms.\nModel Size Method nums_kv_head Seq Length Time_mean (ms) Peak_Mem_mean (MB) Llama3 8B GQA-8 8 512 40.8777 2322.328 Llama3 8B MHA 32 512 53.0167 2706.375 Llama3 8B MQA 1 512 37.3592 2210.314 Llama3 8B GQA-8 8 1024 85.5850 6738.328 Llama3 8B MQA 1 1024 80.8002 6570.314 Llama3 8B MHA 32 1024 102.0514 7314.375 Llama3 8B GQA-8 8 1536 147.5949 13586.328 Llama3 8B MHA 32 1536 168.8142 14354.375 Llama3 8B MQA 1 1536 141.5059 13362.314 Fig. 6. Average Time Benchmark.\nFig. 7. Average Peak Memory Benchmark.\nIn scenarios sensitive to memory and time overheads, MQA and GQA-8 are more efficient choices, with MQA performing the best but potentially lacking in model performance capabilities; GQA-8 achieves a good balance between efficiency and performance.\nGQA Paper Experimental Results Inference Performance Fig. 8. Inference time and performance comparison. (Image source: Ainslie et al., 2023)\nFig. 9. Additional Experimental Results. (Image source: Ainslie et al., 2023)\nThe experimental results show that:\nInference Speed:\nMHA-XXL\u0026rsquo;s inference time is significantly higher than MHA-Large, primarily due to its larger number of heads and model size. Compared to MHA-XXL, MQA-XXL and GQA-8-XXL reduce inference time to approximately 1/6 and 1/5, respectively. Performance:\nMHA-XXL performs best across all tasks but has longer inference times. MQA-XXL has an advantage in inference speed, with average scores only slightly lower than MHA-XXL. GQA-8-XXL has inference speed close to MQA-XXL but nearly matches MHA-XXL in performance, demonstrating the efficiency and superiority of GQA. Checkpoint Conversion Fig. 10. Ablation Study on Checkpoint Conversion Methods. (Image source: Ainslie et al., 2023)\nFigure 10 compares the performance of different methods for checkpoint conversion. The mean pooling method performs best in retaining model information, followed by selecting the first head, while random initialization performs the worst. Mean pooling effectively integrates information from multiple \\(\\mathbf{K}\\) and \\(\\mathbf{V}\\) heads, maintaining model performance.\nUptraining Ratio Fig. 11. Ablation Study on Uptraining Ratios. (Image source: Ainslie et al., 2023)\nFigure 11 shows how performance varies with uptraining proportion for T5 XXL with MQA and GQA.\nGQA: Even with only conversion (no uptraining), GQA already has certain performance. As the uptraining ratio increases, performance continues to improve. MQA: Requires at least a 5% uptraining ratio to achieve practical performance, and as the ratio increases, performance gains tend to plateau. Effect of Number of GQA Groups on Inference Speed Fig. 12. Effect of the Number of GQA Groups on Inference Speed. (Image source: Ainslie et al., 2023)\nFigure 12 demonstrates that as the number of groups increases, inference time slightly rises, but it still maintains a significant speed advantage over MHA. Choosing an appropriate number of groups, such as 8, can achieve a good balance between speed and performance. Figure 3 also shows that models ranging from 7B to 405B parameters in Llama3 adopt 8 as the number of groups (key/value heads = 8).\nOther Optimization Methods In addition to optimizing the attention mechanism, researchers have proposed various methods to enhance the inference and training efficiency of Transformer models:\nLoRA (Hu et al., 2021): Achieves efficient parameter fine-tuning by adding low-rank matrices to the pretrained model\u0026rsquo;s weight matrices. Flash Attention (Dao et al., 2022): Reduces memory and computational overhead by optimizing attention calculations. Quantization Techniques: LLM.int8 (Dettmers et al., 2022) and GPTQ (Frantar et al., 2022) reduce memory usage and computational costs by lowering the precision of model weights and activations. Model Distillation (Hinton et al., 2015): Reduces model size by training smaller models to mimic the behavior of larger models. Speculative Sampling (Chen et al., 2023): Enhances generation efficiency through parallel generation and filtering. Key Takeaways Uptraining methods can effectively utilize existing MHA model checkpoints. By performing a small amount of additional training, they can transform these into more efficient MQA or GQA models, significantly reducing training costs. Grouped-Query Attention (GQA) strikes a good balance between inference efficiency and model performance, making it especially suitable for applications requiring both high-efficiency inference and high performance. Experimental results demonstrate that GQA can significantly improve inference speed while maintaining performance comparable to MHA models, making it suitable for large-scale model deployment and real-time applications. References [1] Vaswani A. Attention is all you need [J]. Advances in Neural Information Processing Systems, 2017.\n[2] Devlin J. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [J]. arXiv preprint arXiv:1810.04805, 2018.\n[3] Radford A. Improving Language Understanding by Generative Pre-Training [J]. 2018.\n[4] Touvron H, Lavril T, Izacard G, et al. LLaMA: Open and Efficient Foundation Language Models [J]. arXiv preprint arXiv:2302.13971, 2023.\n[5] Achiam J, Adler S, Agarwal S, et al. GPT-4 Technical Report [J]. arXiv preprint arXiv:2303.08774, 2023.\n[6] Shazeer N. Fast Transformer Decoding: One Write-Head is All You Need [J]. arXiv preprint arXiv:1911.02150, 2019.\n[7] Ainslie J, Lee-Thorp J, de Jong M, et al. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints [J]. arXiv preprint arXiv:2305.13245, 2023.\n[8] Hu E J, Shen Y, Wallis P, et al. LoRA: Low-Rank Adaptation of Large Language Models [J]. arXiv preprint arXiv:2106.09685, 2021.\n[9] Dettmers T, Lewis M, Belkada Y, et al. GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale [J]. Advances in Neural Information Processing Systems, 2022, 35: 30318-30332.\n[10] Frantar E, Ashkboos S, Hoefler T, et al. GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers [J]. arXiv preprint arXiv:2210.17323, 2022.\n[11] Hinton G. Distilling the Knowledge in a Neural Network [J]. arXiv preprint arXiv:1503.02531, 2015.\n[12] Chen C, Borgeaud S, Irving G, et al. Accelerating Large Language Model Decoding with Speculative Sampling [J]. arXiv preprint arXiv:2302.01318, 2023.\nCitation Citation: To reproduce or cite the content of this article, please acknowledge the original author and source.\nCited as:\nYue Shui. (Jan 2025). Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA.\nhttps://syhya.github.io/posts/2025-01-16-group-query-attention/\nOr\n@article{syhya2025gqa, title = \u0026#34;Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-16-group-query-attention/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-01-16-group-query-attention/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eThe Transformer (\u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eVaswani et al., 2017\u003c/a\u003e) is a model based on the encoder-decoder architecture. This model has demonstrated outstanding performance in the field of natural language processing (NLP), leading to a series of optimized models based on it, such as BERT (\u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eDevlin et al., 2018\u003c/a\u003e) which uses only the encoder, GPT (\u003ca href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\"\u003eRadford et al., 2018\u003c/a\u003e) series which uses only the decoder, and subsequent large language models (LLMs) like LLaMA (\u003ca href=\"https://arxiv.org/abs/2302.13971\"\u003eTouvron et al., 2023\u003c/a\u003e) and GPT-4 (\u003ca href=\"https://arxiv.org/abs/2303.08774\"\u003eOpenAI et al., 2024\u003c/a\u003e), most of which adopt a decoder-only architecture.\u003c/p\u003e","title":"Attention Mechanisms in Transformers: Comparing MHA, MQA, and GQA"},{"content":"Background With the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.\nBased on my work experience, this article summarizes how to build LLMs equipped with specific domain knowledge by leveraging data preparation, model training, deployment, evaluation, and continuous iteration on top of existing general models.\nWhy Inject Domain Knowledge into the Foundational LLMs? Challenge 1: Limited Domain Knowledge Existing pre-trained models (such as GPT-4 and Llama 3) are primarily trained on general-purpose corpora, lacking in-depth understanding of niche languages or proprietary domains. This deficiency leads to subpar performance when the models handle programming code.\nChallenge 2: Data Security and Compliance When enterprises handle sensitive data, they must adhere to strict data sovereignty and compliance requirements. Uploading proprietary business data to third-party cloud services poses security risks, necessitating data processing and model training within local environments.\nChallenge 3: Limitations of OpenAI Fine-Tuning Mainstream commercial APIs for fine-tuning are typically basic and struggle to achieve deep alignment and optimization. For highly customized domain models, such approaches often fail to meet the required specifications.\nTwo Approaches of Injecting Knowledge In practical projects, the common methods for injecting domain knowledge into base models include Fine-Tuning and Retrieval-Augmented Generation (RAG). The following sections provide a detailed comparison of these methods to aid in selecting the most suitable strategy.\nMethod Comparison Fine-Tuning Core Concept\nThrough continued pre-training, supervised fine-tuning, and preference alignment, directly update the model parameters to enable it to master domain-specific knowledge and task patterns.\nTechnical Details\nContinued Pre-Training (CPT): Continue pre-training the base model on a large volume of domain-specific unsupervised data. Supervised Fine-Tuning (SFT): Perform supervised fine-tuning using high-quality labeled data. Preference Alignment (DPO): Optimize model outputs based on user feedback. Parameter Tuning Methods: Utilize full-parameter fine-tuning or combine with PEFT methods like LoRA to freeze some parameters and add adapters. Advantages\nDeep Customization: Updating the internal weights of the model enables a profound understanding of domain knowledge. No External Retrieval Dependency: Inference does not require additional knowledge bases, reducing latency and total token consumption. Enhanced Overall Performance: Significantly outperforms general models in domain-specific tasks. Disadvantages\nHigh Computational Cost: Requires substantial computational resources for training, especially during the CPT phase. Long Training Cycles: From data preparation to model training and optimization, the process is time-consuming. Catastrophic Forgetting: The model may forget its original general capabilities while learning new knowledge. Retrieval-Augmented Generation (RAG) Core Concept\nBuild a domain-specific knowledge base and retrieve relevant documents during inference to assist the model in generating more accurate responses without directly altering model parameters.\nTechnical Details\nData Processing: Preprocess domain documents by chunking them based on size and overlap. Vectorization: Embedding text chunks as vectors using embedding models and storing them in a Vector Store for retrieval. Retrieval: During inference, retrieve relevant documents through similarity search to provide contextual information or few-shot examples to the base model. Advantages\nPreserves General Capabilities: Model parameters remain unchanged, retaining general language abilities. Quick Updates: The knowledge base can be dynamically updated without retraining the model. Computational Efficiency: Avoids large-scale training, saving computational resources. Disadvantages\nDependence on Knowledge Base Quality: The quality of retrieved documents directly impacts response quality. Inference Speed: The retrieval process may increase inference latency and require more tokens. Limited Knowledge Coverage: The model’s internal knowledge is still restricted by the base model’s pre-training data. Models and Training Resources Base Models Taking the Llama 3 series as an example, it features the following characteristics:\nParameter Scale\nThe Llama 3 series includes models ranging from 1B to 405B parameters, widely supporting multilingual processing, code generation, reasoning, as well as visual and textual tasks. Smaller models (1B and 3B) are specially optimized for edge and mobile devices, supporting up to 128K context windows, efficiently handling local tasks such as summary generation, instruction execution, and text rewriting.\nMultimodal Capabilities\nLlama 3\u0026rsquo;s visual models (11B and 90B parameters) outperform many closed models in image understanding tasks and support multimodal processing of images, videos, and audio. All models support fine-tuning, facilitating customized development for specific domains.\nOpen Source and Community Support\nLlama 3 series models and their weights are released in open-source form and can be accessed via llama.com and the Hugging Face platform, providing convenient access and application support for developers.\nDataset Restrictions\nAlthough the Llama 3 models are released as open-source, the datasets used for their training are not open-sourced. Therefore, strictly speaking, Llama 3 is not entirely open-source. This limitation may pose challenges in addressing catastrophic forgetting, as obtaining data sets identical to the original training data is difficult.\nTraining Resources Training large language models requires robust computational resources and efficient distributed training frameworks.\nHardware Resources\nGPU Clusters: NVIDIA A100 or H100 GPUs are recommended, with configurations of 4 or 8 GPUs connected via NVLink or InfiniBand to enhance communication bandwidth. Storage Resources: High-performance SSDs (e.g., NVMe) to support fast data read and write operations. Software Frameworks\nDistributed Training Frameworks: DeepSpeed, Megatron-LM, among others, support large-scale model training. Inference Frameworks: vLLM, ollama, etc., optimize inference speed and resource utilization. Parallel Strategies\nData Parallelism (DP): Suitable when the model fits on a single GPU, implemented via DeepSpeed\u0026rsquo;s ZeRO Stage 0. Model Parallelism (MP), Pipeline Parallelism (PP), and Tensor Parallelism (TP): When the model cannot fit on a single GPU, optimize using ZeRO Stage 1, 2, or 3, or employ ZeRO-Infinity to offload parts of parameters and optimizer states to CPU or NVMe. DeepSpeed ZeRO Sharding Strategies Comparison ZeRO Stage Sharding Strategies ZeRO Stage Description GPU Memory Usage Training Speed ZeRO-0 Pure data parallelism without any sharding. All optimizer states, gradients, and parameters are fully replicated on each GPU. Highest Fastest ZeRO-1 Shards optimizer states (e.g., momentum and second moments), reducing GPU memory usage, but gradients and parameters remain data parallel. High Slightly slower than ZeRO-0 ZeRO-2 Shards optimizer states and gradients, further reducing GPU memory usage based on ZeRO-1. Medium Slower than ZeRO-1 ZeRO-3 Shards optimizer states, gradients, and model parameters, achieving the lowest GPU memory usage, suitable for extremely large models. Requires parameter broadcasting (All-Gather/All-Reduce) during forward/backward passes, significantly increasing communication overhead. Low Significantly slower than ZeRO-2, depends on model size and network bandwidth Offload Strategies Offload Type Description GPU Memory Usage Training Speed ZeRO-1 + CPU Offload Extends ZeRO-1 by offloading optimizer states to CPU memory, further reducing GPU memory usage but necessitating CPU-GPU data transfer, relying on PCIe bandwidth, and occupying CPU memory. Medium-low Slower than ZeRO-1, affected by CPU performance and PCIe bandwidth ZeRO-2 + CPU Offload Extends ZeRO-2 by offloading optimizer states to CPU memory, further reducing GPU memory usage for larger models but increasing CPU-GPU data transfer overhead. Lower Slower than ZeRO-2, affected by CPU performance and PCIe bandwidth ZeRO-3 + CPU Offload Extends ZeRO-3 by offloading optimizer states and model parameters to CPU, achieving minimal GPU memory usage but with extremely high CPU-GPU communication volume and CPU bandwidth significantly lower than GPU-GPU communication. Extremely Low Very Slow ZeRO-Infinity (NVMe Offload) Based on ZeRO-3, offloads optimizer states, gradients, and parameters to NVMe, breaking CPU memory limits and suitable for ultra-large-scale models; performance highly depends on NVMe parallel read/write speeds. Extremely LowRequires NVMe support Slower than ZeRO-3 but generally faster than ZeRO-3 + CPU Offload, can achieve better throughput if NVMe bandwidth is sufficient Communication Volume and Performance Impact ZeRO-0/1/2:\nCommunication is primarily gradient synchronization using All-Reduce operations, resulting in relatively low communication volume.\nZeRO-3:\nRequires All-Gather/All-Reduce operations for model parameters, significantly increasing communication volume. Network bandwidth becomes a critical bottleneck, and parameter broadcasting during forward/backward passes further exacerbates communication load.\nCPU Offload (ZeRO-1/2/3 + CPU):\nOffloads optimizer states or parameters to CPU, reducing GPU memory usage. Communication volume mainly arises from CPU \u0026lt;-\u0026gt; GPU data transfers, which have much lower bandwidth compared to GPU-GPU communication, easily causing performance bottlenecks, especially in ZeRO-3 scenarios. NVMe Offload (ZeRO-Infinity):\nFurther offloads to NVMe based on ZeRO-3, overcoming CPU memory limitations to support ultra-large-scale models. Performance heavily relies on NVMe I/O bandwidth and parallelism. If NVMe speed is sufficiently high, it typically outperforms CPU Offload; however, performance may suffer in scenarios with weak I/O performance or high latency. Hardware and Configuration Impact Hardware Constraints:\nPCIe Bandwidth, Network Bandwidth, NVMe I/O, etc., significantly impact Offload performance. Optimal strategies should be selected based on the hardware environment. Additional Notes:\nCPU Offload utilizes CPU memory and transfers data via PCIe; NVMe Offload saves states on NVMe devices. NVMe Offload generally outperforms CPU Offload when NVMe I/O performance is adequate, but care must be taken to avoid performance bottlenecks caused by insufficient I/O performance. Reference to Official Documentation:\nIt is recommended to consult the DeepSpeed official documentation for the latest and most accurate configuration parameters and performance tuning advice. Data Preparation: The Core of Training Success Data quality directly determines model performance. Data preparation includes data collection, cleaning, deduplication, categorization and balancing, anonymization, and other steps.\nPre-Training Data Data Sources Public Datasets: Such as the-stack-v2, Common Crawl, etc. Enterprise Proprietary Data: Internal documents, code repositories, business logs, etc. Web Crawlers: Collect domain-relevant web content using crawling technologies. Data Scale It is recommended to use at least hundreds of millions to billions of tokens to ensure the model can thoroughly learn domain knowledge. When data volume is insufficient, model performance may be limited. Data augmentation methods are suggested to supplement the data. Data Processing Data Preprocessing\nUniform Formatting: Process large volumes of unlabeled corpora from multiple data sources to ensure consistent formatting. It is recommended to use efficient storage formats like Parquet to improve data reading and processing efficiency. Data Deduplication\nDetection Methods: Use algorithms such as MinHash, SimHash, or cosine similarity for approximate duplicate detection. Granularity of Processing: Choose to deduplicate at the sentence, paragraph, or document level, adjusting flexibly based on task requirements. Similarity Threshold: Set a reasonable similarity threshold (e.g., 0.9) to remove texts with duplication above the threshold, ensuring data diversity. Data Cleaning\nText Filtering: Remove garbled text, spelling errors, and low-quality text by combining rule-based methods and model scorers (e.g., BERT/RoBERTa). Formatting Processing: Prefer using JSON format to handle data, ensuring the accuracy of special formats like code, Markdown, and LaTeX. Data Anonymization\nPrivacy Protection: Anonymize or remove sensitive information such as names, phone numbers, emails, passwords, etc., to ensure data compliance. Filtering Non-Compliant Content: Remove data blocks containing illegal, pornographic, or racially discriminatory content. Data Mixing and Balancing\nProportion Control: For example, combine 70% domain-specific data with 30% general data to prevent the model from forgetting general capabilities. Task Types: Ensure the data includes various task types such as code generation, Q\u0026amp;A dialogue, document summarization, multi-turn conversations, and mathematical reasoning. Data Sequencing\nProgressive Guidance: Use Curriculum Learning to start training with simple, clean data and gradually introduce more complex or noisy data, optimizing the model\u0026rsquo;s learning efficiency and convergence path. Semantic Coherence: Utilize In-Context Pretraining techniques to concatenate semantically similar documents, enhancing contextual consistency and improving the model\u0026rsquo;s depth of semantic understanding and generalization ability. Supervised Fine-Tuning Data Data Format Adopt Alpaca or Vicuna styles, such as single-turn and multi-turn dialogues structured as [instruction, input, output].\nScale: From thousands to hundreds of thousands, depending on project requirements and computational resources. Quality: Ensure high-quality and diverse data to prevent the model from learning errors or biases. Data Construction During the data construction process, we first collect daily business data and collaboratively build foundational questions with business experts. Subsequently, we use large language models for data augmentation to enhance data diversity and robustness. The specific data augmentation strategies are as follows:\nData Augmentation Strategies Diverse Expressions\nRewrite existing data using large language models through synonym replacement and syntactic transformations to increase data diversity.\nRobustness Enhancement\nCreate prompts containing spelling errors, mixed languages, and other input variations to simulate real-world scenarios while ensuring high-quality generated answers.\nKnowledge Distillation\nUtilize large language models like GPT-4 and Claude for knowledge distillation to generate Q\u0026amp;A pairs that meet requirements.\nComplex Task Design\nManually design high-quality data for complex scenarios (e.g., multi-turn dialogues, logical reasoning) to cover the model\u0026rsquo;s capability boundaries.\nData Generation Pipeline\nBuild an automated data generation pipeline that integrates data generation, filtering, formatting, and validation to improve overall efficiency.\nKey Points Task Type Annotation: Clearly annotate each data entry with its task type to facilitate subsequent fine-grained analysis and tuning. Multi-Turn Dialogues and Topic Switching: Construct data that captures contextual coherence and topic transitions in multi-turn dialogues to ensure the model learns the ability to handle topic switching and maintain contextual relevance. Chain of Thought (CoT) Strategy: For classification and reasoning tasks, generate procedural answers using CoT to improve accuracy. Data Flywheel: Continuously collect real user queries after deployment, iterating data based on real needs; regularly clean the data to ensure quality and diversity. Preference Data Data Format Triple Structure: [prompt, chosen answer, rejected answer] Annotation Details: Multi-Model Sampling: Generate answers using multiple models at different training stages or with different data ratios to increase data diversity. Editing and Optimization: Annotators can make slight modifications to the chosen answers to ensure answer quality. Sampling Strategies Multi-Model Sampling: Deploy multiple versions of the model to generate diverse answers for the same prompt. Comparative Annotation: Use manual or automated systems to compare generated answers and select superior answer pairs. Key Points Data Diversity and Coverage: Ensure preference data covers various scenarios and tasks to prevent the model from underperforming in specific contexts. High-Quality Annotation: The quality of preference data directly affects the model\u0026rsquo;s alignment, requiring accurate and consistent annotations. Training Process A complete training process for a domain-specific large language model typically includes Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO) as the three main steps, ultimately achieving model deployment and continuous optimization.\nComparison of Three Methods Training Method Overview Training Method Main Objective Data Requirements Typical Application Scenarios Continued Pre-Training (CPT) Continue pre-training on large-scale unsupervised corpora to inject new domain knowledge Large amounts of unlabeled text (at least hundreds of millions to billions of tokens) Supplementing domain knowledge, such as specialized texts in law, medicine, finance, etc. Supervised Fine-Tuning (SFT) Fine-tune on supervised labeled data to strengthen specific tasks and instruction execution capabilities Customized labeled data (instruction/dialog pairs), ranging from thousands to hundreds of thousands Various specific tasks, such as code generation, Q\u0026amp;A, text rewriting, complex instruction execution, etc. Direct Preference Optimization (DPO) Optimize model outputs to align with human preferences using preference data (chosen vs. rejected) Preference data: [prompt, chosen, rejected](relatively smaller scale) Aligning with human feedback, such as response style, compliance, safety, etc. Advantages and Challenges Continued Pre-Training (CPT) Advantages:\nBetter domain coverage, comprehensively enhancing the model\u0026rsquo;s understanding and generation capabilities in specific domains. No need for additional manual annotation. Challenges/Limitations:\nRequires a large volume of high-quality domain data. High training costs, necessitating massive computational power and time. May introduce domain biases, necessitating careful handling of data quality and distribution. Supervised Fine-Tuning (SFT) Advantages:\nQuickly acquires task execution capabilities. Significantly improves accuracy in specific scenarios. Challenges/Limitations:\nHigh data annotation costs. Requires careful selection of labeled data to avoid overfitting. Fine-tuning may weaken the model\u0026rsquo;s generality. Direct Preference Optimization (DPO) Advantages:\nNo need to train a separate Reward Model. Requires less data and computational resources to achieve similar or better results compared to PPO. Challenges/Limitations:\nRequires reliable preference annotations. Continues to need more preference data for complex and diverse scenarios. Easily constrained by the distribution of preference data. General Training Tips and Technical Details When performing CPT, SFT, and DPO, there are numerous general training tips and technical details. The following sections uniformly describe these general aspects for better understanding and application.\nData Processing and Preparation Data Quality: Regardless of CPT, SFT, or DPO, data quality is crucial. Ensure data accuracy, unambiguity, and diversity. Data Formatting: Consistent data formats simplify the training process. For example, using JSON or other structured formats to store training data. Data Augmentation: Increase data diversity and improve the model\u0026rsquo;s generalization ability through methods like LLM rewriting and optimization. Learning Rate and Optimization Learning Rate Settings: Typically use a smaller learning rate than during pre-training, such as reducing from 3e-4 to 3e-5, depending on the task and data volume. Learning Rate Scheduling: Use warm-up strategies (e.g., linearly increasing for the first 10% of steps), followed by linear decay or cosine annealing to ensure a smooth training process. Optimizer Selection: Choose suitable optimizers based on model size and hardware resources, such as AdamW. Training Strategies Full-Parameter Fine-Tuning: When resources permit, prioritize full-parameter fine-tuning to ensure the model fully captures new knowledge. Parameter-Efficient Fine-Tuning (PEFT): Methods like LoRA are suitable for scenarios with limited computational resources by freezing some parameters and adding adapters for efficient fine-tuning. Mixed Precision Training: Use bf16 or fp16 on supported GPUs to reduce memory usage and increase training speed. Training Stability: Employ techniques such as gradient clipping, regularization, dropout, and weight decay to prevent gradient explosion and model overfitting. Flash Attention: Utilize Flash Attention to optimize the computation efficiency of the attention mechanism, enhancing training speed and reducing memory usage. Monitoring and Tuning Convergence Monitoring: Continuously monitor loss curves on training and validation sets to ensure the model is converging properly. Adjust learning rates and other hyperparameters as needed. Checkpoint: Regularly save checkpoints to prevent loss of all training progress due to unexpected interruptions. Early Stopping: Prevent model overfitting by stopping training at an appropriate time and saving the best model state. Model Evaluation: Conduct periodic evaluations during training to ensure model performance meets expectations. Continued Pre-Training (CPT) Objective Inject new domain knowledge into the base model by continuing pre-training on a large volume of domain-specific unsupervised data, enhancing the model\u0026rsquo;s understanding and generation capabilities in the specific domain.\nTraining Tips Streaming Data Loading\nImplement streaming data loading to dynamically read data during training, preventing memory overflows and training interruptions. Full-Parameter Fine-Tuning\nTypically, update all model parameters during training to ensure comprehensive knowledge acquisition. Compared to parameter-efficient fine-tuning methods (e.g., LoRA), full-parameter fine-tuning offers better domain knowledge injection, especially when computational resources are abundant. It is recommended to prioritize full-parameter fine-tuning under such conditions. Supervised Fine-Tuning (SFT) Objective Enhance the model\u0026rsquo;s practicality and accuracy by training it on high-quality labeled data to perform specific tasks such as code generation, code repair, and complex instruction execution.\nTraining Tips Number of Epochs\nTypically, 1 to 4 epochs are sufficient to observe significant effects when data volume is adequate. If data volume is insufficient, consider increasing the number of epochs while being mindful of overfitting risks. Data augmentation is recommended in such cases. Data Augmentation and Diversity\nEnsure training data covers a variety of task types and instruction expressions to improve the model\u0026rsquo;s generalization ability. Include multi-turn dialogues and robustness data to enhance the model\u0026rsquo;s capability to handle real user scenarios. Direct Preference Optimization (DPO) Objective Optimize model outputs to better align with human expectations and needs, including response style, safety, and readability, by leveraging user feedback and preference data.\nCharacteristics of DPO Direct Optimization\nDoes not require training a separate Reward Model. Instead, directly performs contrastive learning on (chosen, rejected) data pairs.\nEfficiency\nCompared to PPO, DPO requires less data and computational resources to achieve similar or better results.\nDynamic Adaptation\nThe model can immediately adapt whenever new data is available without the need to retrain a Reward Model.\nTraining Tips Collecting Preference Data\nDeploy multiple models at different training stages or with different data ratios to generate diverse responses. Annotate chosen and rejected answer pairs through manual or automated means to ensure data diversity and quality. Contrastive Learning\nOptimize model parameters by maximizing the probability of chosen answers and minimizing the probability of rejected answers. Iterative Optimization\nContinuously collect user feedback, generate new preference data, and perform iterative training to gradually enhance model performance. Implement a data flywheel mechanism to achieve ongoing model evolution and optimization. Common Issues and Solutions Repetitive Outputs\nIssue: The model generates repetitive content, continuously printing without stopping.\nSolutions:\nData Deduplication and Cleaning: Ensure training data does not contain a large amount of repetitive content. Check EOT (End-of-Token) Settings: Prevent the model from continuously generating without stopping. Align via SFT/DPO: Optimize model output quality. Adjust Decoding Strategies: Increase parameters like top_k, repetition penalty, and temperature. Catastrophic Forgetting\nIssue: The model forgets its original general capabilities during fine-tuning, effectively overfitting to the new dataset and causing excessive changes to the original model parameter space.\nSolutions:\nMix in Some General Data: Maintain the model’s general capabilities. Lower the Learning Rate: Reduce the impact on existing knowledge. Increase Dropout Rate and Weight Decay: Prevent overfitting. Use Parameter-Efficient Fine-Tuning Methods like LoRA: Avoid large-scale parameter updates. Utilize RAG Assistance: Combine with external knowledge bases to enhance model performance. Chat Vector: Quickly inject conversational and general capabilities into the model through simple arithmetic operations on model weights. Insufficient Understanding of Entity Relationships and Reasoning Paths\nIssue: The model struggles to correctly understand complex entity relationships and reasoning paths.\nSolutions:\nIntroduce Chain-of-Thought (CoT) Data and Enhanced Reasoning Training: Improve the model\u0026rsquo;s capabilities through step-by-step reasoning training, combined with Reinforcement Fine-Tuning and o1/o3 training methods. Expand Training Data Coverage: Incorporate more diverse scenarios containing complex entity relationships and reasoning paths. Combine with Knowledge Graph Modeling: Use GraphRAG to strengthen the model\u0026rsquo;s understanding and reasoning abilities regarding entity relationships. Model Deployment and Evaluation Deployment Inference Frameworks\nollama: Local inference deployment based on llama.cpp, enabling quick startups. vLLM: Optimized for high concurrency and inference throughput in multi-user scenarios. Quantization: Quantize the model to 8-bit or 4-bit to further reduce inference costs and improve deployment efficiency. Integrate RAG \u0026amp; Agents\nRAG: Combine with a vector knowledge base to retrieve relevant documents or code snippets in real-time, assisting the model in generating more accurate responses. Agents: Utilize Function Calls or multi-turn dialogue mechanisms to enable the model to invoke external tools or perform complex reasoning, enhancing interactivity and practicality. Langgraph: Encapsulate RAG and multi-agent workflows to build customized dialogue systems or automated code generation platforms. Evaluation Evaluation Metrics\nCPT Phase: Use domain-specific test sets to evaluate perplexity (PPL) or cross-entropy, measuring the model\u0026rsquo;s mastery of new knowledge. SFT/DPO Phase: Human or Model Evaluation: Assess the accuracy, coherence, readability, and safety of responses through human ratings or automated tools. Code Generation: Build a large-scale unit test set to evaluate the pass@k metric, measuring the correctness rate of code generation. General Capabilities: Test the model on common benchmarks (e.g., MMLU, CMMLU) to ensure minimal performance degradation on general tasks. Decoding Hyperparameters\nConsistency: Maintain consistent decoding parameters such as top_k, top_p, temperature, and max_new_tokens during evaluation to ensure comparability of results. Grid Search: When computational resources permit, evaluate different combinations of decoding parameters to select the optimal configuration. Data Flywheel and Continuous Iteration Data Flywheel Mechanism\nReal-Time Collection of User Logs\nCollect real user prompts and generated responses online, covering diverse usage scenarios and task types. Automated or Manual Annotation\nAnnotate collected user prompts and responses with preferences, generating new (chosen, rejected) data pairs. Iterative Training\nIncorporate newly generated preference data into the next round of SFT/DPO training to continuously optimize response quality and user experience. Robustness Data\nInclude data with spelling errors, mixed languages, vague instructions, etc., to enhance the model’s robustness and ability to handle real-world scenarios. Continuous Optimization\nFeedback Loop: Utilize user feedback to continuously improve training data and model performance, achieving self-optimization and evolution of the model. Multi-Model Collaboration: Deploy multiple versions of the model to generate diverse responses, enhancing the model\u0026rsquo;s comprehensive capabilities through contrastive learning. Integrating Intent Recognition and Multi-Agent Reasoning Use an intent classification model to allow the large model to determine the category of user input intent. Based on the mapping between intent categories and context types, supervise the reasoning path, and then perform multi-way retrieval based on the reasoning path. Provide this information to the trained model to generate the final result.\nConclusion Through the combination of Continued Pre-Training (CPT) → Supervised Fine-Tuning (SFT) → Direct Preference Optimization (DPO), it is possible to effectively inject domain-specific knowledge into base large models, constructing closed-source LLMs capable of efficiently solving business problems. The key steps are as follows:\nData Preparation\nHigh-quality data collection, cleaning, deduplication, and categorization to ensure data diversity and accuracy. Implement data anonymization strategies to protect privacy and ensure compliance. Model Training\nUse CPT to inject domain knowledge, SFT to learn specific task patterns, and DPO to optimize model outputs to align with human preferences and safety. Leverage efficient parallel training frameworks and hyperparameter tuning techniques to enhance training efficiency and resource utilization. Deployment and Evaluation\nEmploy efficient inference frameworks, integrating RAG and Agents for knowledge enhancement and functional extension. Conduct multi-dimensional evaluations to ensure the model performs as expected at each stage. Continuous Iteration\nBuild a data flywheel to continuously collect user feedback and optimize training data and model performance. Integrate RAG and Agents to achieve ongoing improvement and expansion of model capabilities. Ultimately, through a systematic process and technical measures, it is possible to construct an AI system with not only profound domain knowledge but also the flexibility to handle complex business requirements over its lifecycle.\nReferences DeepSpeed Megatron-LM ollama vLLM GraphRAG The Llama 3 Herd of Models ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages Evaluating Large Language Models Trained on Code Direct Preference Optimization: Your Language Model is Secretly a Reward Model Citation Citation: To reproduce or cite the content of this article, please acknowledge the original author and source.\nCited as:\nYue Shui. (Jan 2025). Building Domain-Specific LLMs.\nhttps://syhya.github.io/posts/2025-01-05-build-domain-llm\nOr\n@article{syhya2024domainllm, title = \u0026#34;Building Domain-Specific LLMs\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2025-01-05-build-domain-llm/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2025-01-05-domain-llm-training/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eWith the widespread application of Large Language Models (LLMs) across various industries, enterprises and research teams face an urgent need to adapt general-purpose models to specific domains. Foundational LLMs often fail to meet deep domain-specific requirements when handling specialized tasks. For example, in the application of closed-source programming languages, existing open-source models lack sufficient understanding of their syntax and semantics, leading to poor performance in tasks such as code generation and error correction. Therefore, injecting domain knowledge and training dedicated LLMs has become a key step in enhancing development efficiency and code quality.\u003c/p\u003e","title":"Building Domain-Specific LLMs"},{"content":"Rent a GPU or Buy Your Own? Before setting up a deep learning environment, consider usage duration, budget, data privacy, and maintenance overhead. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\nAdvantages of Renting GPUs:\nNo high upfront hardware costs Elastic scalability according to project needs Cloud vendors typically provide data compliance and security assurances, reducing hardware maintenance concerns Advantages of Buying GPUs:\nLower total cost if used extensively over the long term Higher privacy and control for in-house data and models Hardware can be upgraded or adjusted at any time, offering more flexible deployment Personal Suggestions\nIf you have a limited budget or are just starting out, use Colab, Kaggle, or cloud-based GPU solutions first. Once computing needs and privacy requirements increase, consider building your own multi-GPU server or renting a multi-node, multi-GPU cluster. Background In September 2023, to continue my exploration and research on Large Language Models (LLMs) outside of work, I assembled a dual RTX 4090 personal AI server. It has been running for nearly a year, and here are some observations:\nNoise: Placed under my desk, the fans can get quite loud under full load. However, noise levels are acceptable during inference or moderate loads. Inference Performance: With a total of 48GB of VRAM, and by using 4-bit quantization, it can handle 70B-level models (e.g., Llama 70B, Qwen 72B). Training Performance: By using DeepSpeed with distributed and offload techniques (ZeRO-3 + CPU offload), I can finetune around 34B parameter models (e.g., CodeLlama 34B). Cost-Effectiveness: For personal or small-team experiments and small-to-medium scale model training, this configuration is quite practical. However, full-scale training of very large models (hundreds of billions of parameters) still requires more professional GPUs (e.g., multi-GPU A100 or H100 clusters). Below is an illustration of VRAM requirements for various model sizes and training approaches :\nFig. 1. Hardware Requirement. (Image source: LLaMA-Factory)\nAssembly Strategy \u0026amp; Configuration Details The total budget is around 40,000 RMB (~6,000 USD). The final build list is as follows (for reference only):\nComponent Model Price (RMB) GPU RTX 4090 * 2 25098 Motherboard + CPU AMD R9 7900X + MSI MPG X670E CARBON 5157.55 Memory Corsair 48GB * 2 (DDR5 5600) 2399 SSD SOLIDIGM 944 PRO 2TB *2 + Samsung 990PRO 4TB 4587 PSU Corsair AX1600i 2699 Fans Phanteks T30 120mm P * 6 1066.76 CPU Cooler Thermalright FC140 BLACK 419 Chassis Phanteks 620PC Full Tower 897.99 GPU Riser Cable Phanteks FL60 PCI-E4.0 *16 399 Total: ~ 42,723.3 RMB\nGPU Selection For large-scale model research, floating-point performance (TFLOPS) and VRAM capacity are the most critical metrics. Professional GPUs (A100, H100, etc.) boast higher VRAM and NVLink support but can easily cost hundreds of thousands of RMB, which is prohibitive for most individual users. According to Tim Dettmers, the RTX 4090 offers excellent performance-per-dollar and supports features like BF16 and Flash Attention, making it a high-value option.\nCooling Options: Blower vs. Air-Cooling vs. Liquid-Cooling Cooling Method Advantages Disadvantages Best For Blower Fan Compact form factor; good for multi-GPU setups Loud noise, generally weaker cooling Server racks, dense multi-GPU deployments Air-Cooling Good balance of performance and noise; easy upkeep Cards are often large, require space Home or personal research (with enough space) Liquid-Cooling Excellent cooling, quieter under full load Risk of leaks, higher cost Extreme quiet needs or heavy overclocking Home Setup Recommendation: Air-cooled GPUs are typically the best balance of cooling efficiency, noise, and maintenance. They are generally more user-friendly compared to blower or liquid-cooled alternatives.\nCPU \u0026amp; Motherboard In deep learning scenarios, the CPU handles data preprocessing, pipeline scheduling, and multi-process/thread management, ensuring high-throughput, low-latency data delivery to the GPUs. Thus, the CPU’s main requirements include sufficient PCIe lanes and robust multi-threaded performance.\nIntel: 13th/14th Generation i9 (e.g., 13900K) offers 20 primary PCIe lanes, enough for dual GPUs at x8 + x8. AMD: The Ryzen 7000/9000 series (e.g., 7950X) provides 28 (24 usable) PCIe lanes and also supports dual GPUs at x8 + x8, with enough bandwidth for multiple M.2 SSDs. MSI MPG X670E CARBON Motherboard Expandability: Supports PCIe 5.0 and DDR5 memory, offering ample future-proofing. Stability: High-quality power delivery, ensuring stable operation for both CPU and multiple GPUs. Rich I/O: Supports multiple M.2 SSDs and USB4 for various usage scenarios. AMD Ryzen 9 7900X Highlights Cores \u0026amp; Threads: 12 cores, 24 threads, excellent for data preprocessing and multitasking in deep learning workloads. PCIe Bandwidth: Provides 28 (24 usable) PCIe 5.0 lanes to support dual GPUs at x8 + x8 while also powering high-speed M.2 SSDs. Power Efficiency: Built on the Zen 4 architecture, delivering outstanding performance-to-power ratio for high-performance computing tasks. Key Motherboard Considerations Physical Layout RTX 4090 cards are huge, often occupying multiple slots. Confirm the board can hold two such cards simultaneously. If space or thermal conflicts arise, use a riser cable for vertical placement of the second GPU. PCIe Lane Splitting Ensure the motherboard can run two PCIe 4.0 x8 + x8 slots. Avoid a setup ending up as x16 + x2, which severely limits the second GPU’s bandwidth and can lead to a significant performance bottleneck, especially critical in large model training. Expandability With two GPUs installed, you still need enough M.2 slots and external ports for additional storage or peripherals. After evaluating expandability, performance, and cost-effectiveness, I chose the AMD Ryzen 9 7900X paired with the MSI MPG X670E CARBON. A GPU riser cable resolved the slot conflicts caused by the thickness of dual RTX 4090s.\nBIOS Setup Recommendations Memory Optimization Enable XMP/EXPO (Intel/AMD) to boost memory clock speeds and bandwidth. Overclocking If additional performance is needed, enable PBO (Precision Boost Overdrive) or Intel Performance Tuning and monitor system stability. Thermals \u0026amp; Stability Avoid extreme overclocking and keep temperatures under control to maintain system stability. Memory During deep learning training, large amounts of system memory are used for data loading and optimizer states (especially in multi-GPU scenarios with Zero-stage parallelism). Aim for at least 2× the total GPU VRAM capacity. This build uses 48GB * 2 (96GB in total), sufficient for everyday multitasking and distributed training without frequent swapping.\nStorage Prefer M.2 NVMe SSDs: They offer superior read/write performance, critical for loading large model weights, caching intermediate files, and storing training logs. Capacity ≥ 2TB: Model files continue to grow in size. 2TB can fill up quickly; consider 4TB or more depending on your needs. SSD Brands: Samsung, SK Hynix, and Western Digital have reliable high-end product lines. Power Supply Dual RTX 4090s can draw 900W–1000W under full load, and you also need overhead for the CPU, motherboard, and storage. Generally, 1,500W+ Platinum or Titanium PSUs are recommended to ensure stable power delivery and mitigate voltage fluctuations.\nI opted for a Corsair AX1600i (digital PSU), which provides real-time power monitoring and sufficient headroom.\nCooling \u0026amp; Fans I chose an air-cooling setup:\nCPU Cooler: Thermalright FC140, a dual-tower air cooler offering solid thermal performance with relatively low noise. Case Fans: Phanteks T30 120mm * 6, maintaining slightly positive internal pressure to ensure proper airflow across the GPUs and power delivery modules. For prolonged high-load training (such as distributed training of large models), chassis airflow management and adequate fans are crucial. Monitor CPU, GPU, and VRM temperatures regularly and adjust fan curves as needed.\nAdvanced Cooling\nFor even quieter operation, consider a Hybrid or partial liquid-cooling solution, along with finely tuned fan curves. Regularly cleaning dust filters, using dust guards, and refreshing thermal compound can also help improve thermals and stability. Chassis Because the RTX 4090 is massive, and two of them stacked requires ample internal space and airflow, a full-tower chassis is recommended for better cable routing and thermal design. I went with the Phanteks 620PC, which offers ample space and built-in cable management.\nBelow is a picture of the built computer:\nFig. 2. Computer\nSystem \u0026amp; Software Environment Operating System: Linux (e.g., Ubuntu 22.04 LTS) is highly recommended due to better compatibility and support for CUDA, NVIDIA drivers, and popular deep learning frameworks. The general setup flow:\nOS Installation: Ubuntu or another Linux distribution. NVIDIA Driver Installation: Make sure nvidia-smi detects both 4090 GPUs correctly:\nFig. 3. nvidia-smi Output\nCUDA Toolkit: Verify via nvcc -V:\nFig. 4. nvcc -V Output\ncuDNN: Ensure your deep learning frameworks can leverage GPU-accelerated kernels for convolution, RNN, etc. Framework Testing: Use PyTorch, TensorFlow, or JAX to confirm basic inference and training functionality. Docker Containerization: With nvidia-container-toolkit, containers can directly access GPU resources, eliminating host-environment conflicts. For multi-node, multi-GPU setups, consider Kubernetes, Ray, or Slurm for cluster scheduling and resource management. Recommended Tools \u0026amp; Frameworks Training Frameworks\nLLaMA-Factory: Offers user-friendly packaging for large language model training and inference. Great for beginners. DeepSpeed: Provides distributed training for large models, with multiple parallelization strategies and optimizations. Megatron-LM: NVIDIA’s official large-scale language model training framework, ideal for multi-node, multi-GPU scenarios. Monitoring \u0026amp; Visualization\nWeights \u0026amp; Biases or TensorBoard: Real-time monitoring of loss, learning rate, etc., with remote-friendly UI. Inference Tools\nollama: Based on llama.cpp, easy local inference setup. vLLM: Optimized for high-concurrency, multi-user scenarios, delivering superior throughput. Framework ollama vLLM Function Simple local LLM deployment High-concurrency / high-throughput LLM Concurrent Performance drops as concurrency increases Handles higher concurrency with better TPS 16 Threads ~17s/req ~9s/req Throughput Slower token generation speeds ~2× faster token generation Max Concur. Performance deteriorates over 32 threads Remains stable under large concurrency Use Cases Personal or low-traffic apps Enterprise or multi-user high concurrency WebUI\nOpen-WebUI: A user-friendly, web-based solution that integrates multiple AI backends (ollama, OpenAI API, etc.), handy for rapid prototyping and visualization. Advanced Suggestions Development \u0026amp; Debugging Efficiency\nUse SSH for remote development, and create custom Docker images to reduce setup overhead. Quantization \u0026amp; Pruning\nTechniques like 4-bit or 8-bit quantization and pruning can reduce model size and VRAM usage, thereby improving inference performance. Mixed-Precision Training\nSwitch to BF16 or FP16 training to accelerate training speed, combined with gradient scaling (GradScaler) to maintain numerical stability. CPU Coordination\nEnhance data loading using multi-threading, multi-processing, or RAM disks for streaming large pretraining datasets. Multi-Node Cluster Deployment\nConnect nodes via InfiniBand or high-speed Ethernet and use Kubernetes for efficient resource scheduling. Summary With the above configuration and methodology, I successfully built a dual RTX 4090 deep learning workstation. It excels at inference and small to medium scale fine-tuning scenarios—ideal for individuals or small teams focusing on LLM research or application development. This setup strikes a balance between cost-effectiveness and flexibility. However, if you plan to train massive models (hundreds of billions of parameters) in a full-parameter regime, you will still need more GPUs (e.g., multi-GPU A100 or H100 clusters).\nFrom personal experience, dual 4090 GPUs provide sufficient performance within a reasonable budget, meeting the majority of small-to-medium-scale R\u0026amp;D needs—a solid option for qualified individuals or teams to consider.\nReferences Tim Dettmers: Which GPU for Deep Learning? (2023) Intel 14900K PCIe lane specs AMD R5 7600X PCIe lane specs MSI MPG X670E CARBON Specifications nvidia-container-toolkit LLaMA-Factory DeepSpeed Megatron-LM ollama vLLM Ollama vs VLLM: Which Tool Handles AI Models Better? Open-WebUI Copyright \u0026amp; Citation Disclaimer: The configurations, prices, and recommendations in this article are for technical discussion and research purposes only. Actual purchases and deployments should be carefully evaluated based on individual budgets and requirements. The author assumes no liability for any direct or indirect consequences resulting from following or adapting the information provided herein.\nCitation: When reposting or referencing this content, please credit the original author and source.\nCited as:\nYue Shui. (Dec 2024). Building a Home Deep Learning Rig with Dual RTX 4090 GPUs. https://syhya.github.io/posts/2024-12-21-build-gpu-server\nOr\n@article{syhya2024build, title = \u0026#34;Building a Home Deep Learning Rig with Dual RTX 4090 GPUs\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Dec\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2024-12-21-build-gpu-server/\u0026#34; ","permalink":"https://syhya.github.io/posts/2024-12-21-build-gpu-server/","summary":"\u003ch2 id=\"rent-a-gpu-or-buy-your-own\"\u003eRent a GPU or Buy Your Own?\u003c/h2\u003e\n\u003cp\u003eBefore setting up a deep learning environment, consider \u003cstrong\u003eusage duration\u003c/strong\u003e, \u003cstrong\u003ebudget\u003c/strong\u003e, \u003cstrong\u003edata privacy\u003c/strong\u003e, and \u003cstrong\u003emaintenance overhead\u003c/strong\u003e. If you have long-term needs (e.g., over a year) and require strict data security, building your own GPU server often provides lower overall costs and a more controllable environment. On the other hand, for short-term projects or when data privacy is not critical, renting cloud GPUs (e.g., Azure, AWS, GCP) or using free platforms (Colab, Kaggle) offers greater flexibility.\u003c/p\u003e","title":"Building a Home Deep Learning Rig with Dual RTX 4090 GPUs"},{"content":"Abstract The stock market is a crucial component of the financial market. In recent years, with its vigorous development, research on stock price prediction and quantitative investment strategies has attracted scholars from various fields. With the advancement of Artificial Intelligence (AI) and Machine Learning (ML) in recent years, researchers have shifted from traditional statistical models to AI algorithms. Particularly after the deep learning boom, neural networks have achieved remarkable results in stock price prediction and quantitative investment strategy research. The objective of deep learning is to learn multi-level features, constructing abstract high-level features by combining low-level ones, thereby mining the distributed feature representations of data. This approach enables complex nonlinear modeling to accomplish prediction tasks. Recurrent Neural Networks (RNNs) have been widely applied to sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, leading many researchers to use RNNs for stock price prediction. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The advent of Long Short-Term Memory (LSTM) networks addressed this problem, followed by variants such as Gated Recurrent Units (GRUs), Peephole LSTMs, and Bidirectional LSTMs (BiLSTMs). Traditional stock prediction models often overlook temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully leverages the contextual relationships in both forward and backward temporal directions of time series data. It also avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies.\nThe first part of this paper\u0026rsquo;s experiments utilizes stock data from China\u0026rsquo;s Shanghai Pudong Development Bank and the US\u0026rsquo;s IBM to establish stock prediction models using LSTM, GRU, and BiLSTM. By comparing the prediction results of these three deep learning models, it is found that the BiLSTM model outperforms the others for both datasets, demonstrating better prediction accuracy. The second part uses A-share market-wide stock data and first employs a LightGBM model to screen 50 factors, selecting the top 10 most important factors. Subsequently, a BiLSTM model is used to select and combine these factors to establish a quantitative investment strategy. Empirical analysis and backtesting of this strategy reveal that it outperforms the market benchmark index, indicating the practical application value of the BiLSTM model in stock price prediction and quantitative investment.\nKeywords: Quantitative Investment; Deep Learning; Neural Network Model; Multi-Factor Stock Selection; BiLSTM\nChapter 1 Introduction 1.1 Research Background and Significance 1.1.1 Research Background Emerging in the 1970s, quantitative investment gradually entered the vision of investors, initiating a new revolution that changed the landscape of portfolio management previously dominated by passive management and the efficient market hypothesis. The efficient market hypothesis posits that under market efficiency, stock prices reflect all market information, and no excess returns exist. Passive investment management, based on the belief that markets are efficient, focuses more on asset classes, with the most common approach being purchasing index funds and tracking published index performance. In contrast, active investment management relies primarily on investors\u0026rsquo; subjective judgments of the market and individual stocks. By applying mathematical models to the financial domain and using available public data to evaluate stocks, active managers construct portfolios to achieve returns. Quantitative investment, through statistical processing of vast historical data to uncover investment opportunities and avoid subjective factors, has gained increasing popularity among investors. Since the rise of quantitative investment, people have gradually utilized various technologies for stock price prediction to better establish quantitative investment strategies. Early domestic and international scholars adopted statistical methods for modeling and predicting stock prices, such as exponential smoothing, multiple regression, and Autoregressive Moving Average (ARMA) models. However, due to the multitude of factors influencing the stock market and the large volume of data, stock prediction is highly challenging, and the prediction effectiveness of various statistical models has been unsatisfactory.\nIn recent years, the continuous development of machine learning, deep learning, and neural network technologies has provided technical support for stock price prediction and the construction of quantitative strategies. Many scholars have achieved new breakthroughs using methods like Random Forest, Neural Networks, Support Vector Machines, and Convolutional Neural Networks. The ample historical data in the stock market, coupled with diverse technological support, provides favorable conditions for stock price prediction and the construction of quantitative strategies.\n1.1.2 Research Significance From the perspective of the long-term development of the national economic system and financial markets, research on stock price prediction models and quantitative investment strategies is indispensable. China started relatively late, with a less mature financial market, fewer financial instruments, and lower market efficiency. However, in recent years, the country has gradually relaxed policies and vigorously developed the financial market, providing a \u0026ldquo;breeding ground\u0026rdquo; for the development of quantitative investment. Developing quantitative investment and emerging financial technologies can offer China\u0026rsquo;s financial market an opportunity for a \u0026ldquo;curve overtaking\u0026rdquo;. Furthermore, the stock price index, as a crucial economic indicator, serves as a barometer for China\u0026rsquo;s economic development.\nFrom the perspective of individual and institutional investors, constructing stock price prediction models and quantitative investment strategy models enhances market efficiency. Individual investors often lack comprehensive professional knowledge, and their investment behaviors can be somewhat blind. Developing relevant models to provide references can reduce the probability of judgment errors and change the relatively disadvantaged position of individual investors in the capital market. For institutional investors, rational and objective models, combined with personal experience, improve the accuracy of decision-making and provide new directions for investment behaviors.\nIn summary, considering China\u0026rsquo;s current development status, this paper\u0026rsquo;s selection of individual stocks for stock price prediction models and A-share market-wide stocks for quantitative strategy research holds significant practical research value.\n1.2 Literature Review White (1988)$^{[1]}$ used a Backpropagation (BP) neural network to predict the daily returns of IBM stock. However, due to the BP neural network model\u0026rsquo;s susceptibility to gradient explosion, the model could not converge to a global minimum, thus failing to achieve accurate predictions.\nKimoto (1990)$^{[2]}$ developed a system for predicting the Tokyo Stock Exchange Prices Index (TOPIX) using modular neural network technology. This system not only successfully predicted TOPIX but also achieved a certain level of profitability through stock trading simulations based on the prediction results.\nG. Peter Zhang (2003)$^{[3]}$ conducted a comparative study on the performance of Autoregressive Integrated Moving Average (ARIMA) models and Artificial Neural Network (ANN) models in time series forecasting. The results showed that ANN models significantly outperformed ARIMA models in terms of time series prediction accuracy.\nRyo Akita (2016)$^{[4]}$ selected the Consumer Price Index (CPI), Price-to-Earnings ratio (P/E ratio), and various events reported in newspapers as features, and constructed a financial time series prediction model using paragraph vectors and LSTM networks. Using actual data from fifty listed companies on the Tokyo Stock Exchange, the effectiveness of this model in predicting stock opening prices was verified.\nKunihiro Miyazaki (2017)$^{[5]}$ constructed a model for predicting the rise and fall of the Topix Core 30 index and its constituent stocks by extracting daily stock chart images and 30-minute stock price data. The study compared multiple models, including Logistic Regression (LR), Random Forest (RF), Multilayer Perceptron (MLP), LSTM, CNN, PCA-CNN, and CNN-LSTM. The results indicated that LSTM had the best prediction performance, CNN performed generally, but hybrid models combining CNN and LSTM could improve prediction accuracy.\nTaewook Kim (2019)$^{[6]}$ proposed an LSTM-CNN hybrid model to combine features from both stock price time series and stock price image data representations to predict the stock price of the S\u0026amp;P 500 index. The study showed that the LSTM-CNN model outperformed single models in stock price prediction, and this prediction had practical significance for constructing quantitative investment strategies.\n1.3 Innovations of the Paper This paper has the following innovations in stock price prediction:\nData from both the domestic A-share market (Shanghai Pudong Development Bank) and the international US stock market (IBM) are used for research, avoiding the limitations of single-market studies. Traditional BP models have never considered temporal factors, or like LSTM models, only consider unidirectional temporal relationships. Therefore, this paper uses the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions of time series data and avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies. Empirical evidence, compared with LSTM and GRU models, demonstrates its ability to improve prediction accuracy. The stock price prediction model is trained using multiple stock features, including opening price, closing price, highest price, and trading volume. Compared to single-feature prediction of stock closing prices, this approach is theoretically more accurate and can better compare the prediction effectiveness of LSTM, GRU, and BiLSTM for stocks. This paper has the following innovations in quantitative strategy research:\nInstead of using common market factors, this paper uses multiple price-volume factors obtained through Genetic Programming (GP) algorithms and artificial data mining. LightGBM model is used to screen 50 factors, selecting the top 10 most important factors. Traditional quantitative investment models generally use LSTM and CNN models to construct quantitative investment strategies. This paper uses A-share market-wide data and employs a BiLSTM model to select and combine factors to establish a quantitative investment strategy. Backtesting and empirical analysis of this strategy show that it outperforms the market benchmark index (CSI All Share), demonstrating the practical application value of the BiLSTM model in stock price prediction and quantitative investment. 1.4 Research Framework of the Paper This paper conducts research on stock price prediction and quantitative strategies based on deep learning algorithms. The specific research framework of this paper is shown in Fig. 1:\nFig. 1. Research Framework.\nThe specific research framework of this paper is as follows:\nChapter 1 is the introduction. This chapter first introduces the research significance and background of stock price prediction and quantitative strategy research. Then, it reviews the current research status, followed by an explanation of the innovations of this paper compared to existing research, and finally, a brief description of the research framework of this paper.\nChapter 2 is about related theoretical foundations. This chapter introduces the basic theories of deep learning models and quantitative stock selection involved in this research. The deep learning model section sequentially introduces four deep learning models: RNN, LSTM, GRU, and BiLSTM, with a focus on the internal structure of the LSTM model. The quantitative stock selection theory section sequentially introduces the Mean-Variance Model, Capital Asset Pricing Model, Arbitrage Pricing Theory, Multi-Factor Model, Fama-French Three-Factor Model, and Five-Factor Model. This section introduces the history of multi-factor quantitative stock selection from the development context of various financial theories and models.\nChapter 3 is a comparative study of LSTM, GRU, and BiLSTM in stock price prediction. This chapter first introduces the datasets of domestic and international stocks used in the experiment, and then performs data preprocessing steps of normalization and data partitioning. It then describes the network structures, model compilation, and hyperparameter settings of the LSTM, GRU, and BiLSTM models used in this chapter, and conducts experiments to obtain experimental results. Finally, the experimental results are analyzed, and a summary of this chapter is provided.\nChapter 4 is a study on a quantitative investment model based on LightGBM-BiLSTM. This chapter first outlines the experimental steps, and then introduces the stock data and factor data used in the experiment. Subsequently, factors are processed sequentially through missing value handling, outlier removal, factor standardization, and factor neutralization to obtain cleaned factors. Then, LightGBM and BiLSTM are used for factor selection and factor combination, respectively. Finally, a quantitative strategy is constructed based on the obtained model, and backtesting is performed on the quantitative strategy.\nChapter 5 is the conclusion and future directions. This chapter summarizes the main research content of this paper on stock price prediction and quantitative investment strategies. Based on the existing shortcomings of the current research, future research directions are proposed.\nChapter 2 Related Theoretical Foundations 2.1 Deep Learning Models 2.1.1 RNN Recurrent Neural Networks (RNNs) are widely used for sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, hence many previous studies have used RNNs to predict stock prices. RNNs employ a very simple chain structure of repeating modules, such as a single tanh layer. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The emergence of LSTM solved this problem. Fig. 2 is an RNN structure diagram.\nFig. 2. RNN Structure Diagram. (Image source: Understanding LSTM Networks)\n2.1.2 LSTM Long Short-Term Memory (LSTM) networks are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter \u0026amp; Schmidhuber (1997)$^{[7]}$ and have been refined and popularized by many in subsequent work. Due to their unique design structure, LSTMs are relatively insensitive to gap lengths and solve the gradient vanishing and explosion problems of traditional RNNs. Compared to traditional RNNs and other time series models like Hidden Markov Models (HMMs), LSTMs can better handle and predict important events in time series with very long intervals and delays. Therefore, LSTMs are widely used in machine translation and speech recognition.\nLSTMs are explicitly designed to avoid long-term dependency problems. All recurrent neural networks have the form of a chain of repeating modules of neural networks, but LSTM improves the structure of RNN. Instead of a single neural network layer, LSTM uses a four-layer structure that interacts in a special way.\nFig. 3. LSTM Structure Diagram 1. (Image source: Understanding LSTM Networks)\nFig. 4. LSTM Structure Diagram 2. (Image source: Understanding LSTM Networks)\nAs shown in Fig. 3, black lines are used to represent the transmission of an output vector from one node to the input vector of another node. A neural network layer is a processing module with a $\\sigma$ activation function or a tanh activation function; pointwise operation represents element-wise multiplication between vectors; vector transfer indicates the direction of information flow; concatenate and copy are represented by two black lines merging together and two black lines separating, respectively, to indicate information merging and information copying.\nBelow, we take LSTM as an example to explain its structure in detail.\nForget Gate Fig. 5. Forget Gate Calculation (Image source: Understanding LSTM Networks)\n$$ f_{t} = \\sigma\\left(W_{f} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{f}\\right) $$Parameter Description:\n$h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $\\sigma$: Sigmoid activation function $W_{f}$: Weight matrix for the forget gate $b_{f}$: Bias vector parameter for the forget gate The first step, as shown in Fig. 5, is to decide what information to discard from the cell state. This process is calculated by the sigmoid function to obtain the value of $f_{t}$ (the range of $f_{t}$ is between 0 and 1, where 0 means completely block, and 1 means completely pass through) to determine whether the cell state $C_{t-1}$ is passed through or partially passed through.\nInput Gate Fig. 6. Input Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} i_{t} \u0026= \\sigma\\left(W_{i} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{i}\\right) \\\\ \\tilde{C}_{t} \u0026= \\tanh\\left(W_{C} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{C}\\right) \\end{aligned} $$Parameter Description:\n$h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $\\sigma$: Sigmoid activation function $W_{i}$: Weight matrix for the input gate $b_{i}$: Bias vector parameter for the input gate $W_{C}$: Weight matrix for the cell state $b_{C}$: Bias vector parameter for the cell state $\\tanh$: tanh activation function The second step, as shown in Fig. 6, uses a sigmoid function to calculate what information we want to store in the cell state. Next, a $\\tanh$ layer creates a candidate vector $\\tilde{C}_{t}$, which will be added to the cell state.\nFig. 7. Current Cell State Calculation (Image source: Understanding LSTM Networks)\n$$ C_{t} = f_{t} * C_{t-1} + i_{t} * \\tilde{C}_{t} $$Parameter Description:\n$C_{t-1}$: Cell state from the previous time step $\\tilde{C}_{t}$: Temporary cell state $i_{t}$: Value of the input gate $f_{t}$: Value of the forget gate The third step, as shown in Fig. 7, calculates the current cell state $C_t$ by combining the effects of the forget gate and the input gate.\nThe forget gate $f_t$ weights the previous cell state $C_{t-1}$ to discard unnecessary information. The input gate $i_t$ weights the candidate cell state $\\tilde{C}_t$ to decide how much new information to introduce. Finally, the two parts are added together to update and derive the current cell state $C_t$. Output Gate Fig. 8. Output Gate Calculation (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} o_{t} \u0026= \\sigma\\left(W_{o} \\cdot \\left[h_{t-1}, x_{t}\\right] + b_{o}\\right) \\\\ h_{t} \u0026= o_{t} * \\tanh\\left(C_{t}\\right) \\end{aligned} $$Parameter Description:\n$o_{t}$: Value of the output gate $\\sigma$: Sigmoid activation function $W_{o}$: Weight matrix for the output gate $h_{t-1}$: Output from the previous time step $x_{t}$: Input at the current time step $b_{o}$: Bias vector parameter for the output gate $h_{t}$: Output at the current time step $\\tanh$: tanh activation function $C_{t}$: Cell state at the current time step The fourth step, as shown in Fig. 8, uses a sigmoid function to calculate the value of the output gate. Finally, the cell state $C_{t}$ at this time step is processed by a tanh activation function and multiplied by the value of the output gate $o_{t}$ to obtain the output $h_{t}$ at the current time step.\n2.1.3 GRU K. Cho (2014)$^{[8]}$ proposed the Gated Recurrent Unit (GRU). GRU is mainly simplified and adjusted based on LSTM, merging the original forget gate, input gate, and output gate of LSTM into an update gate and a reset gate. In addition, GRU also merges the cell state and hidden state, thereby reducing the complexity of the model while still achieving performance comparable to LSTM in some tasks.\nThis model can save a lot of time when the training dataset is relatively large and shows better performance on some smaller and less frequent datasets$^{[9][10]}$.\nFig. 9. GRU Structure Diagram (Image source: Understanding LSTM Networks)\n$$ \\begin{aligned} z_{t} \u0026= \\sigma\\left(W_{z} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ r_{t} \u0026= \\sigma\\left(W_{r} \\cdot \\left[h_{t-1}, x_{t}\\right]\\right) \\\\ \\tilde{h}_{t} \u0026= \\tanh\\left(W \\cdot \\left[r_{t} * h_{t-1}, x_{t}\\right]\\right) \\\\ h_{t} \u0026= \\left(1 - z_{t}\\right) * h_{t-1} + z_{t} * \\tilde{h}_{t} \\end{aligned} $$Parameter Description:\n$z_{t}$: Value of the update gate $r_{t}$: Value of the reset gate $W_{z}$: Weight matrix for the update gate $W_{r}$: Weight matrix for the reset gate $\\tilde{h}_{t}$: Temporary output 2.1.4 BiLSTM Bidirectional Long Short-Term Memory (BiLSTM) networks are formed by combining a forward LSTM and a backward LSTM. The BiLSTM model fully utilizes the contextual relationships in both forward and backward temporal directions of time series data, enabling it to learn information with long-term dependencies. Compared to unidirectional LSTM, it can better consider the reverse impact of future data. Fig. 10 is a BiLSTM structure diagram.\nFig. 10. BiLSTM Structure Diagram. (Image source: Baeldung)\n2.2 Quantitative Stock Selection Theory 2.2.1 Mean-Variance Model Quantitative stock selection strategies originated in the 1950s. Markowitz (1952)$^{[11]}$ proposed the Mean-Variance Model. This model not only laid the foundation for modern portfolio theory, quantifying investment risk, but also established a specific model describing risk and expected return. It broke away from the previous situation of qualitative analysis of investment portfolios without quantitative analysis, successfully introducing mathematical models into the field of financial investment.\n$$ \\begin{aligned} \\mathrm{E}\\left(R_{p}\\right) \u0026= \\sum_{i=1}^{n} w_{i} \\mathrm{E}\\left(R_{i}\\right) \\\\ \\sigma_{p}^{2} \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\operatorname{Cov}\\left(R_{i}, R_{j}\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} w_{i} w_{j} \\sigma_{i} \\sigma_{j} \\rho_{ij} \\\\ \\sigma_{i} \u0026= \\sqrt{\\operatorname{Var}\\left(R_{i}\\right)}, \\quad \\rho_{ij} = \\operatorname{Corr}\\left(R_{i}, R_{j}\\right) \\end{aligned} $$$$ \\min \\sigma_{p}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} \\mathrm{E}\\left(R_{i}\\right) w_{i} = \\mu_{p}, \\quad \\sum_{i=1}^{n} w_{i} = 1 $$$$ \\begin{aligned} \\Omega \u0026= \\begin{pmatrix} \\sigma_{11} \u0026 \\cdots \u0026 \\sigma_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\sigma_{n1} \u0026 \\cdots \u0026 \\sigma_{nn} \\end{pmatrix} = \\begin{pmatrix} \\operatorname{Var}\\left(R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Cov}\\left(R_{1}, R_{n}\\right) \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\operatorname{Cov}\\left(R_{n}, R_{1}\\right) \u0026 \\cdots \u0026 \\operatorname{Var}\\left(R_{n}\\right) \\end{pmatrix} \\\\ \\Omega^{-1} \u0026= \\begin{pmatrix} v_{11} \u0026 \\cdots \u0026 v_{1n} \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ v_{n1} \u0026 \\cdots \u0026 v_{nn} \\end{pmatrix} \\\\ w_{i} \u0026= \\frac{1}{D}\\left(\\mu_{p} \\sum_{j=1}^{n} v_{ij}\\left(C \\mathrm{E}\\left(R_{j}\\right) - A\\right) + \\sum_{j=1}^{n} v_{ij}\\left(B - A \\mathrm{E}\\left(R_{j}\\right)\\right)\\right), \\quad i = 1, \\ldots, n \\end{aligned} $$$$ \\begin{aligned} A \u0026= \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{j}\\right), \\quad B = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij} \\mathrm{E}\\left(R_{i}\\right) \\mathrm{E}\\left(R_{j}\\right), \\quad C = \\sum_{i=1}^{n} \\sum_{j=1}^{n} v_{ij}, \\quad D = BC - A^{2} \u003e 0 \\\\ \\sigma_{p}^{2} \u0026= \\frac{C \\mu_{p}^{2} - 2A \\mu_{p} + B}{D} \\end{aligned} $$Where:\n$\\mathrm{E}\\left(R_{p}\\right)$ and $\\mu_{p}$ are the expected return of portfolio $p$ $\\mathrm{E}\\left(R_{i}\\right)$ is the expected return of asset $i$ $\\sigma_{i}$ is the standard deviation of asset $i$ $\\sigma_{j}$ is the standard deviation of asset $j$ $w_{i}$ is the proportion of asset $i$ in the portfolio $\\sigma_{p}^{2}$ is the variance of portfolio $p$ $\\rho_{ij}$ is the correlation coefficient between asset $i$ and asset $j$ Using the above formulas$^{[12]}$, we can construct an investment portfolio that minimizes non-systematic risk under a certain expected rate of return.\n2.2.2 Capital Asset Pricing Model William Sharpe (1964)$^{[13]}$, John Lintner (1965)$^{[14]}$, and Jan Mossin (1966)$^{[15]}$ proposed the Capital Asset Pricing Model (CAPM). This model posits that the expected return of an asset is related to its risk measure, the $\\beta$ value. Through a simple linear relationship, this model links the expected return of an asset to market risk, making Markowitz (1952)$^{[11]}$\u0026rsquo;s portfolio selection theory more relevant to the real world, and laying a theoretical foundation for the establishment of multi-factor stock selection models.\nAccording to the Capital Asset Pricing Model, for a given asset $i$, the relationship between its expected return and the expected return of the market portfolio can be expressed as:\n$$ E\\left(r_{i}\\right) = r_{f} + \\beta_{im}\\left[E\\left(r_{m}\\right) - r_{f}\\right] $$Where:\n$E\\left(r_{i}\\right)$ is the expected return of asset $i$ $r_{f}$ is the risk-free rate $\\beta_{im}$ (Beta) is the systematic risk coefficient of asset $i$, $\\beta_{im} = \\frac{\\operatorname{Cov}\\left(r_{i}, r_{m}\\right)}{\\operatorname{Var}\\left(r_{m}\\right)}$ $E\\left(r_{m}\\right)$ is the expected return of the market portfolio $m$ $E\\left(r_{m}\\right) - r_{f}$ is the market risk premium 2.2.3 Arbitrage Pricing Theory and Multi-Factor Model Ross (1976)$^{[16]}$ proposed the Arbitrage Pricing Theory (APT). This theory argues that arbitrage behavior is the decisive factor in forming market equilibrium prices. By introducing a series of factors in the return formation process to construct linear correlations, it overcomes the limitations of the Capital Asset Pricing Model (CAPM) and provides important theoretical guidance for subsequent researchers.\nArbitrage Pricing Theory is considered the theoretical basis of the Multi-Factor Model (MFM), an important component of asset pricing models, and one of the cornerstones of asset pricing theory. The general form of the multi-factor model is:\n$$ r_{j} = a_{j} + \\lambda_{j1} f_{1} + \\lambda_{j2} f_{2} + \\cdots + \\lambda_{jn} f_{n} + \\delta_{j} $$Where:\n$r_{j}$ is the return of asset $j$ $a_{j}$ is a constant for asset $j$ $f_{n}$ is the systematic factor $\\lambda_{jn}$ is the factor loading $\\delta_{j}$ is the random error 2.2.4 Fama-French Three-Factor Model and Five-Factor Model Fama (1992) and French (1992)$^{[17]}$ used a combination of cross-sectional regression and time series methods and found that the $\\beta$ value of the stock market could not explain the differences in returns of different stocks, while market capitalization, book-to-market ratio, and price-to-earnings ratio of listed companies could significantly explain the differences in stock returns. They argued that excess returns are compensation for risk factors not reflected by $\\beta$ in CAPM, and thus proposed the Fama-French Three-Factor Model. The three factors are:\nMarket Risk Premium Factor (Market Risk Premium)\nRepresents the overall systematic risk of the market, i.e., the difference between the expected return of the market portfolio and the risk-free rate. Measures the excess return investors expect for bearing systematic risk (risk that cannot be eliminated through diversification). Calculated as: $$ \\text{Market Risk Premium} = E(R_m) - R_f $$ where $E(R_m)$ is the expected market return, and $R_f$ is the risk-free rate. Size Factor (SMB: Small Minus Big)\nRepresents the return difference between small-cap stocks and large-cap stocks. Small-cap stocks are generally riskier, but historical data shows that their expected returns tend to be higher than those of large-cap stocks. Calculated as: $$ SMB = R_{\\text{Small}} - R_{\\text{Big}} $$ reflecting the market\u0026rsquo;s compensation for the additional risk premium of small-cap stocks. Value Factor (HML: High Minus Low)\nReflects the return difference between high book-to-market ratio stocks (i.e., \u0026ldquo;value stocks\u0026rdquo;) and low book-to-market ratio stocks (i.e., \u0026ldquo;growth stocks\u0026rdquo;). Stocks with high book-to-market ratios are usually priced lower (undervalued by the market), but may achieve higher returns in the long run. Calculated as: $$ HML = R_{\\text{High}} - R_{\\text{Low}} $$ Stocks with low book-to-market ratios may be overvalued due to overly optimistic market expectations. This model concretizes the factors in the APT model and concludes that investing in small-cap, high-growth stocks has the characteristics of high risk and high return. The Fama-French Three-Factor Model is widely used in the analysis and practice of modern investment behavior.\nSubsequently, Fama (2015) and French (2015)$^{[18]}$ extended the three-factor model, adding the following two factors:\nProfitability Factor (RMW: Robust Minus Weak)\nReflects the return difference between highly profitable companies and less profitable companies. Companies with strong profitability (high ROE, net profit margin) are more likely to provide stable and higher returns. Calculated as: $$ RMW = R_{\\text{Robust}} - R_{\\text{Weak}} $$ Investment Factor (CMA: Conservative Minus Aggressive)\nReflects the return difference between conservative investment companies and aggressive investment companies. Aggressive companies (rapidly expanding, high capital expenditure) are usually accompanied by greater operational risks, while conservative companies (relatively stable capital expenditure) show higher stability and returns. Calculated as: $$ CMA = R_{\\text{Conservative}} - R_{\\text{Aggressive}} $$ The Fama-French Three-Factor Model formula is:\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\epsilon_i $$The Fama-French Five-Factor Model formula is:\n$$ R_i - R_f = \\alpha_i + \\beta_{i,m} \\cdot (R_m - R_f) + \\beta_{i,SMB} \\cdot SMB + \\beta_{i,HML} \\cdot HML + \\beta_{i,RMW} \\cdot RMW + \\beta_{i,CMA} \\cdot CMA + \\epsilon_i $$Where:\n$R_i$: Expected return of stock $i$ $R_f$: Risk-free rate of return $R_m$: Expected return of the market portfolio $R_m - R_f$: Market risk premium factor $SMB$: Return of small-cap stocks minus large-cap stocks $HML$: Return of high book-to-market ratio stocks minus low book-to-market ratio stocks $RMW$: Return of high profitability stocks minus low profitability stocks $CMA$: Return of conservative investment stocks minus aggressive investment stocks $\\beta_{i,*}$: Sensitivity of stock $i$ to the corresponding factor $\\epsilon_i$: Regression residual 2.2.5 Model Comparison Table The following table summarizes the core content and factor sources of the Mean-Variance Model, Capital Asset Pricing Model (CAPM), Arbitrage Pricing Theory (APT), and Fama-French Models:\nModel Core Content Factor Source Mean-Variance Model Foundation of portfolio theory, optimizes portfolio through expected returns and covariance matrix. Expected returns and covariance matrix of assets in portfolio Capital Asset Pricing Model (CAPM) Explains asset returns through market risk factor ($\\beta$), laying the theoretical foundation for multi-factor models. Market factor $\\beta$ Arbitrage Pricing Theory (APT) Multi-factor framework, allows multiple economic variables to explain asset returns, e.g., inflation rate, interest rate. Multiple factors (macroeconomic variables, e.g., inflation rate, interest rate) Fama-French Three-Factor Model Adds size factor and book-to-market ratio factor, improving the explanatory power of asset returns. Market factor, SMB (size factor), HML (book-to-market ratio factor) Fama-French Five-Factor Model Adds profitability factor and investment factor on the basis of the three-factor model, further improving asset pricing model. Market factor, SMB, HML, RMW (profitability factor), CMA (investment factor) The following table summarizes the advantages and disadvantages of these models:\nModel Advantages Disadvantages Mean-Variance Model Provides a systematic portfolio optimization method, laying the foundation for modern investment theory. Only optimizes for return and variance, does not explicitly specify the source of risk compensation. Capital Asset Pricing Model (CAPM) Simple and easy to use, explains return differences through market risk, provides a theoretical basis for multi-factor models. Assumes a single factor (market risk) determines returns, ignores other systematic risk factors. Arbitrage Pricing Theory (APT) Allows multiple factors to explain asset returns, reduces reliance on single-factor assumptions, more flexible. Does not specify concrete factors, lower operability, only provides a framework. Fama-French Three-Factor Model Significantly improves the explanatory power of asset returns by adding size factor and book-to-market ratio factor. Ignores other factors such as profitability and investment behavior. Fama-French Five-Factor Model More comprehensively captures key variables affecting asset returns by adding profitability factor and investment factor on the basis of the three-factor model. Higher model complexity, high data requirements, may still miss some potential factors. Chapter 3 Comparative Study of LSTM, GRU, and BiLSTM in Stock Price Prediction 3.1 Introduction to Experimental Data Many scholars, both domestically and internationally, focus their research on their own country\u0026rsquo;s stock indices, with relatively less research on individual stocks in different markets. Furthermore, few studies compare LSTM, GRU, and BiLSTM models directly. Therefore, this paper selects Shanghai Pudong Development Bank (SPD Bank, code 600000) in the domestic A-share market and International Business Machines Corporation (IBM) in the US stock market for research. This approach allows for a more accurate comparison of the three models used. For SPD Bank, stock data from January 1, 2008, to December 31, 2020, is used, totaling 3114 valid data points, sourced from the Tushare financial big data platform. We select six features from this dataset for the experiment: date, open price, close price, high price, low price, and volume. For the SPD Bank dataset, all five features except date (used as a time series index) are used as independent variables. For IBM, stock data from January 2, 1990, to November 15, 2018, is used, totaling 7278 valid data points, sourced from Yahoo Finance. We select seven features from this dataset for the experiment: date, open price, high price, low price, close price, adjusted close price (Adj Close), and volume. For the IBM dataset, all six features except date (used as a time series index) are used as independent variables. In this experiment, the closing price (close) is chosen as the variable to be predicted. Tables 3.1.1 and 3.1.2 show partial data from the two datasets.\n3.1.1 Partial Display of SPD Bank Dataset date open close high low volume code 2008-01-02 9.007 9.101 9.356 8.805 131583.90 600000 2008-01-03 9.007 8.645 9.101 8.426 211346.56 600000 2008-01-04 8659 9.009 9.111 8.501 139249.67 600000 2008-01-07 8.970 9.515 9.593 8.953 228043.01 600000 2008-01-08 9.551 9.583 9.719 9.517 161255.31 600000 2008-01-09 9.583 9.663 9.772 9.432 102510.92 600000 2008-01-10 9.701 9.680 9.836 9.602 217966.25 600000 2008-01-11 9.670 10.467 10.532 9.670 231544.21 600000 2008-01-14 10.367 10.059 10.433 10.027 142918.39 600000 2008-01-15 10.142 10.051 10.389 10.006 161221.52 600000 Data Source: Tushare\n3.1.2 Partial Display of IBM Dataset Date Open High Low Close Adj Close Volume 1990-01-02 23.6875 24.5313 23.6250 24.5000 6.590755 7041600 1990-01-03 24.6875 24.8750 24.5938 24.7188 6.649599 9464000 1990-01-04 24.7500 25.0938 24.7188 25.0000 6.725261 9674800 1990-01-05 24.9688 25.4063 24.8750 24.9375 6.708448 7570000 1990-01-08 24.8125 25.2188 24.8125 25.0938 6.750481 4625200 1990-01-09 25.1250 25.3125 24.8438 24.8438 6.683229 7048000 1990-01-10 24.8750 25.0000 24.6563 24.7500 6.658009 5945600 1990-01-11 24.8750 25.0938 24.8438 24.9688 6.716855 5905600 1990-01-12 24.6563 24.8125 24.4063 24.4688 6.582347 5390800 1990-01-15 24.4063 24.5938 24.3125 24.5313 6.599163 4035600 Data Source: Yahoo Finance\n3.2 Experimental Data Preprocessing 3.2.1 Data Normalization In the experiment, there are differences in units and magnitudes among various features. For example, the magnitude difference between stock prices and trading volume is huge, which will affect the final prediction results of our experiment. Therefore, we use the MinMaxScaler method from the sklearn.preprocessing library to scale the features of the data to between 0 and 1. This can not only improve the model accuracy but also increase the model convergence speed. The normalization formula is:\n$$ x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)} $$where $x^{\\prime}$ is the normalized data, $x$ is the original data, $\\min (x)$ is the minimum value of the original dataset, and $\\max (x)$ is the maximum value of the original dataset. After obtaining the prediction results in our experimental process, we also need to denormalize the data before we can perform stock price prediction and model evaluation.\n3.2.2 Data Partitioning Here, the entire experimental datasets of SPD Bank and IBM are input respectively, and the timestep of the recurrent kernel is set to 60 for both, with the number of input features per timestep being 5 and 6, respectively. This allows inputting data from the previous 60 trading days to predict the closing price on the 61st day. This makes our dataset meet the input requirements of the three neural network models to be compared later, which are the number of samples, the number of recurrent kernel unfolding steps, and the number of input features per timestep. Then, we divide the normalized SPD Bank dataset into training, validation, and test sets in a ratio of 2488:311:255. The normalized IBM dataset is divided into training, validation, and test sets in a ratio of 6550:364:304. We partition out a validation set here to facilitate adjusting the hyperparameters of the models to optimize each model before comparison.\n3.3 Model Network Structure The network structures of each model set in this paper through a large number of repeated experiments are shown in the table below. The default tanh and linear activation functions of recurrent neural networks are used between layers, and Dropout is added to prevent overfitting. The dropout rate is set to 0.2. The number of neurons in each recurrent layer of LSTM and GRU is 50, and the number of neurons in the recurrent layer of BiLSTM is 100. Each model of LSTM, GRU, and BiLSTM adopts four layers of LSTM, GRU, BiLSTM, and one fully connected layer, with a Dropout set between each network layer.\n3.3.1 LSTM Network Structure for IBM Layer(type) Output Shape Param# lstm_1 (LSTM) (None, 60, 50) 11400 dropout_1 (Dropout) (None, 60, 50) 0 lstm_2 (LSTM) (None, 60, 50) 20200 dropout_2 (Dropout) (None, 60, 50) 0 lstm_3 (LSTM) (None, 60, 50) 20200 dropout_3 (Dropout) (None, 60, 50) 0 lstm_4 (LSTM) (None, 50) 20200 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params: 72,051\nTrainable params: 72,051\nNon-trainable params: 0\n3.3.2 GRU Network Structure for IBM Layer(type) Output Shape Param# gru_1 (GRU) (None, 60, 50) 8550 dropout_1 (Dropout) (None, 60, 50) 0 gru_2 (GRU) (None, 60, 50) 15150 dropout_2 (Dropout) (None, 60, 50) 0 gru_3 (GRU) (None, 60, 50) 15150 dropout_3 (Dropout) (None, 60, 50) 0 gru_4 (GRU) (None, 50) 15150 dropout_4 (Dropout) (None, 50) 0 dense_1 (Dense) (None, 1) 51 Total params: 54,051\nTrainable params: 54,051\nNon-trainable params: 0\n3.3.3 BiLSTM Network Structure for IBM Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 60, 100) 22800 dropout_1 (Dropout) (None, 60, 100) 0 bidirectional_2 (Bidirection) (None, 60, 100) 60400 dropout_2 (Dropout) (None, 60, 100) 0 bidirectional_3 (Bidirection) (None, 60, 100) 60400 dropout_3 (Dropout) (None, 60, 100) 0 bidirectional_4 (Bidirection) (None, 100) 60400 dropout_4 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 1) 101 Total params: 204,101\nTrainable params: 204,101\nNon-trainable params: 0\n3.4 Model Compilation and Hyperparameter Settings In this paper, after continuous hyperparameter tuning with the goal of minimizing the loss function on the validation set, the following hyperparameters are selected for the three models of SPD Bank: epochs=100, batch_size=32; and for the three models of IBM: epochs=50, batch_size=32. The optimizer used is Adaptive Moment Estimation (Adam)$^{[19]}$. The default values in its keras package are used, i.e., lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, and decay=0.0. The loss function is Mean Squared Error (MSE).\nParameter Explanation:\nlr: Learning rate beta_1: Exponential decay rate for the first moment estimate beta_2: Exponential decay rate for the second moment estimate epsilon: Fuzz factor decay: Learning rate decay value after each update 3.5 Experimental Results and Analysis First, let\u0026rsquo;s briefly introduce the evaluation metrics used for the models. The calculation formulas are as follows:\nMean Squared Error (MSE): $$ M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2} $$ Root Mean Squared Error (RMSE): $$ R M S E=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}} $$ Mean Absolute Error (MAE): $$ M A E=\\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-\\hat{Y}_{i}\\right| $$ \\( R^2 \\) (R Squared): $$ \\begin{gathered} \\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} \\\\ R^{2}=1-\\frac{\\sum_{i=1}^{n}\\left(Y_{i}-\\hat{Y}_{i}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} \\end{gathered} $$Where: $n$ is the number of samples, $Y_{i}$ is the actual closing price of the stock, $\\hat{Y}_{i}$ is the predicted closing price of the stock, and $\\bar{Y}$ is the average closing price of the stock. The smaller the MSE, RMSE, and MAE, the more accurate the model. The larger the \\( R^2 \\), the better the goodness of fit of the model coefficients.\n3.5.1 Experimental Results for SPD Bank LSTM GRU BiLSTM MSE 0.059781 0.069323 0.056454 RMSE 0.244501 0.263292 0.237601 MAE 0.186541 0.202665 0.154289 R-squared 0.91788 0.896214 0.929643 Comparing the evaluation metrics of the three models, we can find that on the SPD Bank test set, the MSE, RMSE, and MAE of the BiLSTM model are smaller than those of the LSTM and GRU models, while the R-Squared is larger than those of the LSTM and GRU models. By comparing RMSE, we find that BiLSTM has a 2.90% performance improvement over LSTM and a 10.81% performance improvement over GRU on the validation set.\n3.5.2 Experimental Results for IBM LSTM GRU BiLSTM MSE 18.01311 12.938584 11.057501 RMSE 4.244186 3.597024 3.325282 MAE 3.793223 3.069033 2.732075 R-squared 0.789453 0.851939 0.883334 Comparing the evaluation metrics of the three models, we can find that on the IBM test set, the MSE, RMSE, and MAE of the BiLSTM model are smaller than those of the LSTM and GRU models, while the R-Squared is larger than those of the LSTM and GRU models. By comparing RMSE, we find that BiLSTM has a 27.63% performance improvement over LSTM and an 8.17% performance improvement over GRU on the validation set.\n3.6 Chapter Summary This chapter first introduced the SPD Bank and IBM datasets and the features used in the experiment. Then, it performed preprocessing steps of data normalization and data partitioning on the datasets. It also detailed the network structures and hyperparameters of the LSTM, GRU, and BiLSTM models used in the experiment. Finally, it obtained the loss function images and a series of fitting graphs for each model. By comparing multiple evaluation metrics and fitting images of the models, it is concluded that the BiLSTM model can better predict stock prices, laying a foundation for our next chapter\u0026rsquo;s research on the LightGBM-BiLSTM quantitative investment strategy.\nChapter 4 Research on Quantitative Investment Model Based on LightGBM-BiLSTM 4.1 Experimental Steps Fig. 11. LightGBM-BiLSTM Diagram.\nAs shown in Fig. 11, this experiment first selects 50 factors from the factor library. Then, it performs factor cleaning steps of outlier removal, standardization, and missing value imputation on the factors sequentially. Next, the LightGBM model is used for factor selection, and the top ten factors with the highest importance are selected as the factors for this cross-sectional selection. Subsequently, a BiLSTM model is used to establish a multi-factor model, and finally, backtesting analysis is performed.\n4.2 Experimental Data The market data used in this paper comes from Tushare. The specific features of the dataset are shown in the table below.\n4.2.1 Features Included in the Stock Dataset Name Type Description ts_code str Stock code trade_date str Trading date open float Open price high float High price low float Low price close float Close price pre_close float Previous close price change float Change amount pct_chg float Change percentage (unadjusted) vol float Volume (in hands) amount float Turnover (in thousands of CNY) The A-share market-wide daily dataset contains 5,872,309 rows of data, i.e., 5,872,309 samples. As shown in Table 4.2.1, the A-share market-wide daily dataset has the following 11 features, in order: stock code (ts_code), trading date (trade_date), open price (open), high price (high), low price (low), close price (close), previous close price (pre_close), change amount (change), turnover rate (turnover_rate), turnover amount (amount), total market value (total_mv), and adjustment factor (adj_factor).\n4.2.2 Partial Display of A-Share Market-Wide Daily Dataset ts_code trade_date open high low close pre_close change vol amount 600613.SH 20120104 8.20 8.20 7.84 7.86 8.16 -0.30 4762.98 3854.1000 600690.SH 20120104 9.00 9.17 8.78 8.78 8.93 -0.15 142288.41 127992.6050 300277.SZ 20120104 22.90 22.98 20.81 20.88 22.68 -1.80 12212.39 26797.1370 002403.SZ 20120104 8.87 8.90 8.40 8.40 8.84 -0.441 10331.97 9013.4317 300179.SZ 20120104 19.99 20.32 19.20 19.50 19.96 -0.46 1532.31 3008.0594 600000.SH 20120104 8.54 8.56 8.39 8.41 8.49 -0.08 342013.79 290229.5510 300282.SZ 20120104 22.90 23.33 21.02 21.02 23.35 -2.33 38408.60 86216.2356 002319.SZ 20120104 9.74 9.95 9.38 9.41 9.73 -0.32 4809.74 4671.4803 601991.SH 20120104 5.17 5.39 5.12 5.25 5.16 0.09 145268.38 76547.7490 000780.SZ 20120104 10.42 10.49 10.00 10.00 10.30 -0.30 20362.30 20830.1761 [5872309 rows x 11 columns]\nThe CSI All Share daily dataset contains 5,057 rows of data, i.e., 5,057 samples. As shown in Table 4.2.2, the CSI All Share daily dataset has the following 7 features, in order: trading date (trade_date), open price (open), high price (high), low price (low), close price (close), volume (volume), and previous close price (pre_close).\n4.2.3 Partial Display of CSI All Share Daily Dataset trade_date open high low close volume pre_close 2006-11-24 1564.3560 1579.3470 1549.9790 1576.1530 7.521819e+09 1567.0910 2006-11-27 1574.1130 1598.7440 1574.1130 1598.7440 7.212786e+09 1581.1530 2006-11-28 1597.7200 1604.7190 1585.3620 1596.8400 7.025637e+09 1598.7440 2006-11-29 1575.3030 1620.2870 1575.3030 1617.9880 7.250354e+09 1596.8400 2006-11-30 1621.4280 1657.3230 1621.4280 1657.3230 9.656888e+09 1617.9880 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 2020-11-11 5477.8870 5493.5867 5422.9110 5425.8017 5.604086e+10 5494.1042 2020-11-12 5439.2296 5454.3452 5413.9659 5435.1379 4.594251e+10 5425.8017 2020-11-13 5418.2953 5418.3523 5364.2031 5402.7702 4.688916e+10 5435.1379 2020-11-16 5422.3565 5456.7264 5391.9232 5456.7264 5.593672e+10 5402.7702 2020-11-17 5454.0696 5454.0696 5395.6052 5428.0765 5.857009e+10 5456.7264 [5057 rows x 7 columns]\nTable 4.2.4 below shows partial data of the original factors. After sequentially going through the four factor cleaning steps of missing value imputation, outlier removal, factor standardization, and factor neutralization mentioned above, partial data of the cleaned factors are obtained as shown in Table 4.2.5.\n4.2.4 Original Factor Data trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 4.2.5 Cleaned Factor Data sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.2.6 Factor Data Construction of Price-Volume Factors This paper uses the following method to construct price-volume factors. There are two basic elements for constructing price-volume factors: first, basic fields, and second, operators. As shown in Table 4.2.1, basic fields include daily frequency high price, low price, open price, close price, previous day\u0026rsquo;s close price, volume, change percentage, turnover rate, turnover amount, total market value, and adjustment factor.\n4.2.7 Basic Field Table No. Field Name Meaning high High Price Highest price in intraday transactions low Low Price Lowest price in intraday transactions open Open Price Price at which the call auction concludes close Close Price Price of the last transaction of the day pre_close Previous Close Price Price of the last transaction of the previous day vol Volume Number of shares traded throughout the day pct_chg Change Percentage Percentage change of the security for the day turnover_rate Turnover Rate Turnover rate of the security for the day amount Turnover Amount Total value of transactions for the day total_mv Total Market Value Total value of the stock, calculated by total shares outstanding multiplied by the current stock price adj_factor Adjustment Factor Ratio for adjusting for dividends and splits This paper obtains the operator list shown in the table below through the basic operator set provided by gplearn and some self-defined special operators.\n4.2.8 Operator List Operator Name Definition add(x, y) Sum \\( x + y\\); element-wise operation \\(\\operatorname{div}(x, y)\\) Division \\( x / y\\); element-wise operation \\(\\operatorname{mul}(x, y)\\) Multiplication \\( x \\cdot y\\); element-wise operation \\(\\operatorname{sub}(x, y)\\) Subtraction \\( x - y\\); element-wise operation neg(x) Negative \\(-x\\); element-wise operation \\(\\log(x)\\) Logarithm \\(\\log(x)\\); element-wise operation max(x, y) Maximum Larger value between \\(x\\) and \\(y\\); element-wise operation \\(\\min(x, y)\\) Minimum Smaller value between \\(x\\) and \\(y\\); element-wise operation delta_d(x) d-day Difference Current day\u0026rsquo;s \\(x\\) value minus \\(x\\) value \\(d\\) days ago; time series operation delay_d(x) d-day Delay \\(x\\) value \\(d\\) days ago; time series operation Corr_d(x, y) d-day Correlation Correlation between \\(x\\) values and \\(y\\) values over \\(d\\) days; time series operation Max_d(x) d-day Maximum Maximum value of \\(x\\) over \\(d\\) days; time series operation Min_d(x) d-day Minimum Minimum value of \\(x\\) over \\(d\\) days; time series operation sort_d(x) d-day Rank Rank of \\(x\\) values over \\(d\\) days; time series operation Argsortmin_d(x) d-day Minimum Position Position of the minimum value of \\(x\\) over \\(d\\) days; time series operation Argsortmax_d(x) d-day Maximum Position Position of the maximum value of \\(x\\) over \\(d\\) days; time series operation \\(\\operatorname{inv}(x)\\) Inverse \\( 1 / x\\); element-wise operation Std_d(x) d-day Standard Deviation Standard deviation of \\(x\\) values over \\(d\\) days; time series operation abs(x) Absolute Value \\(\\lvert x\\rvert\\); element-wise operation 4.2.9 Genetic Programming The core idea of Genetic Programming (GP) is to use evolutionary algorithms to automatically \u0026ldquo;evolve\u0026rdquo; factor expressions with strong predictive power in the vast search space composed of operators and basic fields. For factor mining in this paper, the main goal of GP is to search and find those factors that can better predict future stock returns from all possible expressions that can be combined from the basic fields in Table 4.2.7 and the operators in Table 4.2.8. The core process of GP can be divided into the following steps:\nInitialization Define Operator Set and Basic Fields\nOperator set (operators) as shown in Table 4.2.8, including operations such as addition, subtraction, multiplication, division, logarithm, absolute value, delay, moving maximum/minimum, moving correlation coefficient, etc. Basic fields (terminals) as shown in Table 4.2.7, including open price, close price, high price, low price, volume, adjustment factor, etc. These operators and basic fields can be regarded as \u0026ldquo;nodes\u0026rdquo; in the factor expression tree, where basic fields are leaf nodes (terminal nodes), and operators are internal nodes. Randomly Generate Initial Population\nIn the initialization phase, based on the given operator set and field set, a series of factor expressions (which can be represented as several syntax trees or expression trees) are randomly \u0026ldquo;spliced\u0026rdquo; to form an initial population. For example, it may randomly generate \\[ \\text{Factor 1}: \\mathrm{Max\\_5}\\bigl(\\mathrm{add}(\\mathrm{vol}, \\mathrm{close})\\bigr), \\quad \\text{Factor 2}: \\mathrm{sub}\\bigl(\\mathrm{adj\\_factor}, \\mathrm{neg}(\\mathrm{turnover\\_rate})\\bigr), \\dots \\] Each factor expression will correspond to an individual. Fitness Function Measure Factor\u0026rsquo;s Predictive Ability\nFor each expression (individual), we need to evaluate its predictive ability for future returns or other objectives. Specifically, we can calculate the correlation coefficient (IC) or a more comprehensive indicator IR (Information Ratio) between the next period\u0026rsquo;s stock return \\( r^{T+1} \\) and the current period\u0026rsquo;s factor exposure \\( x_k^T \\) to measure it. Set Objective\nIf we want the factor to have a higher correlation (IC), we can set the fitness function to \\(\\lvert \\rho(x_k^T, r^{T+1})\\rvert\\); If we want the factor to have a higher IR, we can set the fitness function to the IR value. The higher the factor IC or IR, the higher the \u0026ldquo;fitness\u0026rdquo; of the expression. \\[ \\text{Fitness} \\bigl(F(x)\\bigr) \\;=\\; \\begin{cases} \\lvert \\rho(x_k^T, r^{T+1})\\rvert \\quad \u0026\\text{(Maximize IC)},\\\\[6pt] \\mathrm{IR}(x_k^T) \\quad \u0026\\text{(Maximize IR)}. \\end{cases} \\] where \\(\\rho(\\cdot)\\) represents the correlation coefficient, and \\(\\mathrm{IR}(\\cdot)\\) is the IR indicator.\nSelection, Crossover, and Mutation Selection\nBased on the results of the fitness function, expressions with high factor fitness are \u0026ldquo;retained\u0026rdquo; or \u0026ldquo;bred\u0026rdquo;, while expressions with lower fitness are eliminated. This is similar to \u0026ldquo;survival of the fittest\u0026rdquo; in biological evolution. Crossover\nRandomly select a part of the \u0026ldquo;nodes\u0026rdquo; of several expressions with higher fitness (parents) for exchange, so as to obtain new expressions (offspring). In the expression tree structure, subtree A and subtree B can be interchanged to generate new offspring expressions. For example, if a subtree of expression tree \\(\\mathrm{FactorA}\\) is exchanged with the corresponding subtree of expression tree \\(\\mathrm{FactorB}\\), two new expressions are generated. Mutation\nRandomly change some nodes of the expression with a certain probability, such as: Replacing the operator of the node (for example, changing \\(\\mathrm{add}\\) to \\(\\mathrm{sub}\\)), Replacing the basic field of the terminal node (for example, changing \\(\\mathrm{vol}\\) to \\(\\mathrm{close}\\)), Or randomly changing operation parameters (such as moving window length, smoothing factor, etc.). Mutation can increase the diversity of the population and avoid premature convergence or falling into local optima. Iterative Evolution Iterative Execution\nRepeatedly execute selection, crossover, and mutation operations for multiple generations. Each generation produces a new population of factor expressions and evaluates their fitness. Convergence and Termination\nWhen evolution reaches a predetermined stopping condition (such as the number of iterations, fitness threshold, etc.), the algorithm terminates. Usually, we will select several factor expressions with higher final fitness and regard them as the evolution results. Mathematical Representation: Searching for Optimal Factor Expressions Abstracting the above process into the following formula, the factor search objective can be simply expressed as:\n\\[ F(x) \\;=\\; \\mathrm{GP}\\bigl(\\{\\text{operators}\\}, \\{\\text{terminals}\\}\\bigr), \\] indicating that a function \\(F(x)\\) is searched through the GP algorithm on a given operator set (operators) and basic field set (terminals). From the perspective of optimization, we hope to find:\n\\[ \\max_{F} \\bigl\\lvert \\rho(F^T, r^{T+1}) \\bigr\\rvert \\quad \\text{or} \\quad \\max_{F} \\; \\mathrm{IR}\\bigl(F\\bigr), \\] where\n\\(\\rho(\\cdot)\\) represents the correlation coefficient (IC) between the factor and the next period\u0026rsquo;s return, \\(\\mathrm{IR}(\\cdot)\\) represents the IR indicator of the factor. In practical applications, we will give a backtesting period, score the candidate factors of each generation (IC/IR evaluation), and continuously \u0026ldquo;evolve\u0026rdquo; better factors through the iterative process of selection, crossover, and mutation.\nThrough the above steps, we can finally automatically mine a batch of factor expressions that have strong predictive power for future returns and good robustness (such as higher IR) in the vast search space of operator combinations and basic field combinations.\n4.2.10 Partially Mined Factors Factor Name Definition 0 Max＿25(add(turnover_rate, vol)) 1 Max＿30(vol) 2 Max＿25(turnover_rate) 3 Max＿35(add(vol, close)) 4 Max＿30(turnover_rate) 5 sub(Min＿20(neg(pre_close)), div(vol, adj_factor)) 6 Max＿60(max(vol, adj_factor)) 7 Max＿50(amount) 8 div(vol, neg(close)) 9 min(ArgSortMin＿25(pre_close), neg(vol)) 10 neg(max(vol, turnover_rate)) 11 mul(amount, neg(turnover_rate)) 12 inv(add(ArgSortMax＿40(change), inv(pct_chg))) 13 Std＿40(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))) 14 div(log(total_mv),amount) 15 div(neg(Max＿5(amount)), Min＿20(ArgSort＿60(high))) 16 Corr＿30(inv(abs(sub(mul(total_mv, change), min(adj_factor, high)))), add(log(Max＿10(pre_close)), high)) 17 ArgSort＿60(neg(turnover_rate)) \u0026hellip; \u0026hellip; These factors are all obtained by combining from the operator list (Table 4.2.8) and the basic field list (Table 4.2.7) through genetic programming and have different mathematical expressions.\nFactor Validity Test After we get the mined factors, we need to test the validity of the factors. Common test indicators are Information Coefficient (IC) and Information Ratio (IR).\nInformation Coefficient (IC) describes the linear correlation between the next period\u0026rsquo;s return rate of the selected stocks and the current period\u0026rsquo;s factor exposure, which can reflect the robustness of the factor in predicting returns. Information Ratio (IR) is the ratio of the mean of excess returns to the standard deviation of excess returns. The information ratio is similar to the Sharpe ratio. The main difference is that the Sharpe ratio uses the risk-free return as a benchmark, while the information ratio uses a risk index as a benchmark. The Sharpe ratio helps to determine the absolute return of a portfolio, and the information ratio helps to determine the relative return of a portfolio. After we calculate the IC, we can calculate the IR based on the IC value. When the IR is greater than 0.5, the factor has a strong ability to stably obtain excess returns. In actual calculation, the \\( \\mathrm{IC} \\) value of factor \\(k\\) generally refers to the correlation coefficient between the exposure \\( x_k^T \\) of factor \\(k\\) in period \\(T\\) of the selected stocks and the return rate \\( r^{T+1} \\) of the selected stocks in period \\(T+1\\); the \\( \\mathrm{IR} \\) value of factor \\(k\\) is the mean of the \\( \\mathrm{IC} \\) of factor \\(k\\) divided by the standard deviation of the \\( \\mathrm{IC} \\) of factor \\(k\\). The calculation formulas are as follows:\n$$ \\begin{gathered} I C=\\rho_{x_{k}^{T}, r^{T+1}}=\\frac{\\operatorname{cov}\\left(x_{k}^{T}, r^{T+1}\\right)}{\\sigma_{x_{k}^{T}} \\sigma_{r^{T+1}}}=\\frac{\\mathrm{E}\\left(x_{k}^{T} * r^{T+1}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right) \\mathrm{E}\\left(r^{T+1}\\right)}{\\sqrt{\\mathrm{E}\\left(\\left(x_{k}^{T}\\right)^{2}\\right)-\\mathrm{E}\\left(x_{k}^{T}\\right)^{2}} \\cdot \\sqrt{\\mathrm{E}\\left(\\left(r^{T+1}\\right)^{2}\\right)-\\mathrm{E}\\left(r^{T+1}\\right)^{2}}} \\\\ I R=\\frac{\\overline{I C}}{\\sigma_{I C}} \\end{gathered} $$Where:\n$x_{k}^{T}$: Exposure of factor $k$ in period $T$ of the selected stocks $r^{T+1}$: Return rate of the selected stocks in period $T+1$ $\\overline{I C}: Mean of IC This paper uses IR to judge the quality of factors. Through \u0026ldquo;screening\u0026rdquo; a large number of different combinations of operators and basic data and IC and IR, this paper obtains the 50 price-volume factors selected in this paper. After IR testing, the table shown in the figure below is obtained by sorting IR from high to low. From the table below, we can see that the IRs of the selected 50 price-volume factors are all greater than 0.5, indicating that these factors have a strong ability to stably obtain excess returns.\n4.2.11 Factor IR Test Table Factor Name IR Factor Name IR 0 3.11 25 2.73 1 2.95 26 2.71 2 2.95 27 2.70 3 2.95 28 2.69 4 2.95 29 2.69 5 2.94 30 2.69 6 2.94 31 2.68 7 2.94 32 2.68 8 2.93 33 2.68 9 2.93 34 2.68 10 2.93 35 2.67 11 2.92 36 2.67 12 2.91 37 2.66 13 2.89 38 2.65 14 2.86 39 2.65 15 2.83 40 2.65 16 2.83 41 2.65 17 2.83 42 2.64 18 2.79 43 2.63 19 2.78 44 2.63 20 2.78 45 2.62 21 2.76 46 2.62 22 2.75 47 2.62 It can be seen from this table that among the screened factors, the IRs of all factors are greater than 0.5, which has a strong and stable ability to obtain excess returns.\n4.3 Factor Cleaning 4.3.1 Factor Missing Value Handling and Outlier Removal Methods for handling missing values of factors include case deletion, mean imputation, regression imputation, and other methods. This paper adopts a relatively simple mean imputation method to handle missing values, that is, using the average value of the factor to replace the missing data. Methods for factor outlier removal include median outlier removal, percentile outlier removal, and $3 \\sigma$ outlier removal. This paper uses the $3 \\sigma$ outlier removal method. This method uses the $3 \\sigma$ principle in statistics to convert outlier factors that are more than three standard deviations away from the mean of the factor to a position that is just three standard deviations away from the mean. The specific calculation formula is as follows:\n$$ X_i^{\\prime}= \\begin{cases} \\bar{X}+3 \\sigma \u0026 \\text{if } X_i \u003e \\bar{X} + 3 \\sigma \\\\ \\bar{X}-3 \\sigma \u0026 \\text{if } X_i \u003c \\bar{X} - 3 \\sigma \\\\ X_i \u0026 \\text{if } \\bar{X} - 3 \\sigma \u003c X_i \u003c \\bar{X} + 3 \\sigma \\end{cases} $$Where:\n$X_{i}$: Value of the factor before processing $\\bar{X}$: Mean of the factor sequence $\\sigma$: Standard deviation of the factor sequence $X_{i}^{\\prime}$: Value of the factor after outlier removal 4.3.2 Factor Standardization In this experiment, multiple factors are selected, and the dimensions of each factor are not completely the same. For the convenience of comparison and regression, we also need to standardize the factors. Currently, common specific standardization methods include Min-Max standardization, Z-score standardization, and Decimal scaling standardization. This paper chooses the Z-score standardization method. The data is standardized through the mean and standard deviation of the original data. The processed data conforms to the standard normal distribution, that is, the mean is 0 and the standard deviation is 1. The standardized numerical value is positive or negative, and a standard normal distribution curve is obtained.\nThe Z-score standardization formula used in this paper is as follows:\n$$ \\tilde{x}=\\frac{x_{i}-u}{\\sigma} $$Where:\n$x_{i}$: Original value of the factor $u$: Mean of the factor sequence $\\sigma$: Standard deviation of the factor sequence $\\tilde{x}$: Standardized factor value 4.3.3 Factor Neutralization Factor neutralization is to eliminate the influence of other factors on our selected factors, so that the stocks selected by our quantitative investment strategy portfolio are more dispersed, rather than concentrated in specific industries or market capitalization stocks. It can better share the risk of the investment portfolio and solve the problem of factor multicollinearity. Market capitalization and industry are the two main independent variables that affect stock returns. Therefore, in the process of factor cleaning, the influence of market capitalization and industry must also be considered. In this empirical study, we adopt the method of only including industry factors and including market factors in industry factors. The single-factor regression model for factors is shown in formula (31). We take the residual term of the following regression model as the new factor value after factor neutralization.\n$$ \\tilde{r}_{j}^{t}=\\sum_{s=1}^{s} X_{j s}^{t} \\tilde{f}_{s}^{t}+X_{j k}^{t} \\tilde{f}_{k}^{t}+\\tilde{u}_{j}^{t} $$Where:\n$\\tilde{r}_{j}^{t}$: Return rate of stock $j$ in period $t$ $X_{j s}^{t}$: Exposure of stock $j$ in industry $s$ in period $t$ $\\tilde{f}_{s}^{t}$: Return rate of the industry in period $t$ $X_{j k}^{t}$: Exposure of stock $j$ on factor $k$ in period $t$ $\\tilde{f}_{k}^{t}$: Return rate of factor $k$ in period $t$ $\\tilde{u}_j^t$: A $0-1$ dummy variable, that is, if stock $j$ belongs to industry $s$, the exposure is 1, otherwise it is 0 In this paper, the industry to which a company belongs is not proportionally split, that is, stock $j$ can only belong to a specific industry $s$, the exposure in industry $s$ is 1, and the exposure in all other industries is 0. This paper uses the Shenwan Hongyuan industry classification standard. The specific classifications are sequentially: agriculture, forestry, animal husbandry and fishery, mining, chemical industry, steel, nonferrous metals, electronic components, household appliances, food and beverage, textile and apparel, light industry manufacturing, pharmaceutical and biological, public utilities, transportation, real estate, commercial trade, catering and tourism, comprehensive, building materials, building decoration, electrical equipment, national defense and military industry, computer, media, communication, banking, non-banking finance, automobile, and mechanical equipment, a total of 28 categories. The table below shows the historical market chart of Shenwan Index Level 1 industries on February 5, 2021.\n4.3.3.1 Historical Market Chart of Shenwan Index Level 1 Industries on February 5, 2021 Index Code Index Name Release Date Open Index High Index Low Index Close Index Volume (100 Million Hands) Turnover (100 Million CNY) Change (%) 801010 Agriculture, Forestry, Animal Husbandry and Fishery 2021/2/5 0:00 4111.43 4271.09 4072.53 4081.81 15.81 307.82 -0.3 801020 Mining 2021/2/5 0:00 2344.62 2357.33 2288.97 2289.41 18.06 115.6 -2.25 801030 Chemical Industry 2021/2/5 0:00 4087.77 4097.59 3910.67 3910.67 55.78 778.85 -3.95 801040 Steel 2021/2/5 0:00 2253.78 2268.17 2243.48 2250.81 11.61 48.39 -1.02 801050 Nonferrous Metals 2021/2/5 0:00 4212.1 4250.59 4035.99 4036.74 45.41 593.92 -4.43 801080 Electronic Components 2021/2/5 0:00 4694.8 4694.8 4561.95 4561.95 52.67 850.79 -2.78 801110 Household Appliances 2021/2/5 0:00 10033.82 10171.26 9968.93 10096.83 8.55 149.18 0.83 801120 Food and Beverage 2021/2/5 0:00 30876.33 31545.02 30649.57 30931.69 11.32 657.11 0.47 801130 Textile and Apparel 2021/2/5 0:00 1614.48 1633.89 1604.68 1607.63 6.28 57.47 -0.39 801140 Light Industry Manufacturing 2021/2/5 0:00 2782.07 2791.88 2735.48 2737.24 15.28 176.16 -1.35 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; Data Source: Shenwan Hongyuan\nThe table below is partial data of the original factors. After sequentially going through the four factor cleaning steps of missing value imputation, factor outlier removal, factor standardization, and factor neutralization mentioned above, partial data of the cleaned factors are obtained as shown in the table.\n4.3.3.2 Original Factor Data trade_date sec_code ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 2005-01-04 600874.SH 0.001684 NaN 9.445412 9.445412 9.445408 -1.00 NaN 12651.124023 \u0026hellip; 2005-01-04 000411.SZ 0.021073 NaN 5.971262 5.971262 5.971313 0.38 NaN 392.124298 \u0026hellip; 2005-01-04 000979.SZ 0.021207 NaN 6.768918 6.768918 6.768815 -1.45 NaN 870.587585 \u0026hellip; 2005-01-04 000498.SZ 0.030220 NaN 8.852752 8.852752 8.852755 0.55 NaN 6994.011719 \u0026hellip; 2005-01-04 600631.SH 0.015699 NaN 9.589897 9.589897 9.589889 -1.70 NaN 14616.806641 \u0026hellip; 4.3.3.3 Cleaned Factor Data sec_code trade_date ret factor_0 factor_1 factor_2 factor_3 factor_4 factor_5 factor_6 \u0026hellip; 000001.SZ 2005-01-04 -1.58653 0.01545 1.38306 1.38306 1.38306 0.13392 0.01545 1.38564 \u0026hellip; 000002.SZ 2005-01-04 1.36761 -0.44814 1.69728 1.69728 1.69728 1.04567 -0.44814 1.69728 \u0026hellip; 000004.SZ 2005-01-04 0.32966 -1.41654 -0.13907 -0.13907 -0.13907 -0.34769 -1.41654 -0.13650 \u0026hellip; 000005.SZ 2005-01-04 0.61297 -1.13066 1.05339 1.05339 1.05339 -1.20020 -1.13066 1.05597 \u0026hellip; 000006.SZ 2005-01-04 -0.35542 1.67667 -0.07726 -0.07726 -0.07726 1.55820 1.67667 -0.07469 \u0026hellip; 4.4 Factor Selection Based on LightGBM 4.4.1 GBDT Gradient Boosting Decision Tree (GBDT), proposed by Friedman (2001)$^{[20]}$, is an iterative regression decision tree. Its main idea is to optimize the model by gradually adding weak classifiers (usually decision trees), so that the overall model can minimize the loss function. The GBDT model can be expressed as:\n$$ \\hat{y} = \\sum_{m=1}^{M} \\gamma_m h_m(\\mathbf{x}) $$Where:\n\\( M \\) is the number of iterations, \\( \\gamma_m \\) is the weight of the \\( m \\)-th weak classifier, \\( h_m(\\mathbf{x}) \\) is the \\( m \\)-th decision tree model. The training process of GBDT minimizes the loss function by gradually fitting the negative gradient direction. The specific update formula is:\n$$ \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{N} L\\left(y_i, \\hat{y}_{i}^{(m-1)} + \\gamma h_m(\\mathbf{x}_i)\\right) $$Where, \\( L \\) is the loss function, \\( y_i \\) is the true value, and \\( \\hat{y}_{i}^{(m-1)} \\) is the predicted value after the \\( (m-1) \\)-th iteration.\n4.4.2 LightGBM Light Gradient Boosting Machine (LightGBM)$^{[21]}$ is an efficient framework for implementing the GBDT algorithm, initially developed by Microsoft as a free and open-source distributed gradient boosting framework. LightGBM is based on decision tree algorithms and is widely used in ranking, classification, and other machine learning tasks. Its development focuses on performance and scalability. Its main advantages include high-efficiency parallel training, faster training speed, lower memory consumption, better accuracy, and support for distributed computing and fast processing of massive data$^{[22]}$.\nThe core algorithm of LightGBM is based on the following optimization objective:\n$$ L = \\sum_{i=1}^{N} l(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(h_m) $$Where, \\( l \\) is the loss function, and \\( \\Omega \\) is the regularization term, used to control model complexity, usually expressed as:\n$$ \\Omega(h_m) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 $$Here, \\( T \\) is the number of leaves in the tree, \\( w_j \\) is the weight of the \\( j \\)-th leaf, and \\( \\gamma \\) and \\( \\lambda \\) are regularization parameters.\nLightGBM uses technologies such as Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB), which significantly improve training efficiency and model performance.\nIn this study, the loss function used during training is Mean Squared Error (MSE), which is defined as:\n$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$Where, \\( y \\) is the true return rate, \\( \\hat{y} \\) is the return rate predicted by the model, and \\( N \\) is the number of samples.\n4.4.3 Algorithm Flow The specific algorithm flow in this section is as follows:\nData Preparation: Use one year\u0026rsquo;s worth of 50 factor data for each stock (A-share market-wide data) and historical future one-month returns as features.\nModel Training: Use Grid Search to optimize the hyperparameters of the LightGBM model and train the model to predict the future one-month return rate. The model training flow is shown in Fig. 4.12.\n$$ \\text{Parameter Optimization:} \\quad \\theta^* = \\arg\\min_\\theta \\sum_{i=1}^{N} L(y_i, \\hat{y}_i(\\theta)) $$Where, \\( \\theta \\) represents the set of model hyperparameters, and \\( \\theta^* \\) is the optimal parameter.\nFactor Importance Calculation: Use LightGBM\u0026rsquo;s feature_importances_ method to calculate the feature importance of each factor. Feature importance is mainly measured by two indicators:\nSplit: The number of times the feature is used for splitting in all trees. Gain: The total gain brought by the feature in all splits (i.e., the amount of reduction in the loss function). The feature importance of a factor can be expressed as:\n$$ \\text{Importance}_{\\text{split}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\mathbb{I}(f \\text{ is used for splitting the } j \\text{-th leaf node}) $$$$ \\text{Importance}_{\\text{gain}}(f) = \\sum_{m=1}^{M} \\sum_{j=1}^{T_m} \\Delta L_{m,j} \\cdot \\mathbb{I}(f \\text{ is used for splitting the } j \\text{-th leaf node}) $$Where, \\( \\mathbb{I} \\) is the indicator function, and \\( \\Delta L_{m,j} \\) is the reduction in loss brought by factor \\( f \\) in the \\( j \\)-th split of the \\( m \\)-th tree.\nFactor Screening: Sort according to the factor importance calculated by the model, and select the top ten factors with the highest importance as the factors used in this cross-sectional analysis. The importance of the selected factors is shown in Table 4.4.4.\n4.4.4 Partial Ranking of Selected Factor Importance importance feature_name trade_date 35 factor_35 2010-08-11 27 factor_27 2010-08-11 33 factor_33 2010-08-11 20 factor_20 2010-08-11 24 factor_24 2010-08-11 45 factor_45 2010-08-11 37 factor_37 2010-08-11 49 factor_49 2010-08-11 19 factor_19 2010-08-11 47 factor_47 2010-08-11 22 factor_22 2010-09-09 20 factor_20 2010-09-09 30 factor_30 2010-09-09 24 factor_24 2010-09-09 4.4.5 Code Implementation Snippet The following is a code snippet used in the training process for factor selection.\nfeature_choice def feature_choice( self, days=21, is_local=False ): if is_local: feature_info = pd.read_hdf(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;)) else: factors = self.get_env().query_data(Factors_Data) factors = factors[ factors[COM_DATE] \u0026gt;= \u0026#39;2010-01-01\u0026#39; ] trade_list = list(set(factors[COM_DATE])) trade_list.sort() if len(trade_list) % days == 0: n = int(len(trade_list) / days) - 7 else: n = int(len(trade_list) / days) - 6 feature_info = pd.DataFrame() begin_index = 147 feature = list(factors.columns) feature.remove(COM_SEC) feature.remove(COM_DATE) feature.remove(Ret) for i in range(n): end_date = days * i + begin_index - 21 begin_date = days * i trade_date = days * i + begin_index print(trade_list[trade_date]) train_data = factors[ (factors[COM_DATE] \u0026lt;= trade_list[end_date]) \u0026amp; (factors[COM_DATE] \u0026gt;= trade_list[begin_date]) ] model = lgb.LGBMRegressor() model.fit(train_data[feature], train_data[Ret]) feature_info_cell = pd.DataFrame(columns=Info_Fields) feature_info_cell[Importance] = model.feature_importances_ feature_info_cell[Feature_Name] = model.feature_name_ feature_info_cell = feature_info_cell.sort_values(by=Importance).tail(10) feature_info_cell[COM_DATE] = trade_list[trade_date] feature_info = pd.concat( [feature_info, feature_info_cell], axis=0 ) h = pd.HDFStore(os.path.join(RESULTS, Feature_Info + \u0026#39;.h5\u0026#39;), \u0026#39;w\u0026#39;) h[\u0026#39;data\u0026#39;] = feature_info h.close() self.get_env().add_data(feature_info, Feature_Info) pass Through the above process, LightGBM is used to efficiently screen out the factors that have the greatest impact on predicting future returns, thereby improving the predictive ability and interpretability of the model.\n4.5 Factor Combination Based on BiLSTM This section uses BiLSTM for factor combination. The specific principle of BiLSTM has been introduced in Chapter 2, and will not be repeated here. First, let\u0026rsquo;s introduce the specific network structure of the model. The network structure of BiLSTM set in this paper through a large number of repeated experiments is shown in Table 4.5.1. The default tanh and linear activation functions of recurrent neural networks are used between layers. Dropout is added to prevent overfitting, but if Dropout uses an excessively large dropout rate, underfitting will occur. Therefore, the dropout rate of Dropout is set to 0.01. The number of neurons in the BiLSTM recurrent layer of the final model is 100. A BiLSTM layer and three fully connected layers are used, and a Dropout is set between the BiLSTM layer and the first fully connected layer.\n4.5.1 BiLSTM Network Structure Layer(type) Output Shape Param# bidirectional_1 (Bidirection) (None, 100) 24400 dropout_1 (Dropout) (None, 100) 0 dense_1 (Dense) (None, 256) 25856 dropout_2 (Dropout) (None, 256) 0 dense_2 (Dense) (None, 64) 16448 dense_3 (Dense) (None, 1) 0 Total params: 66,769\nTrainable params: 66,769\nNon-trainable params: 0\nBecause the amount of data used in this experiment is large, epochs=400 and batch_size=1024 are selected. The loss function of the model is Mean Squared Error (MSE). The optimizer used is Stochastic Gradient Descent (SGD). Stochastic gradient descent has three advantages over gradient descent (GD): it can more effectively use information when information is redundant, and the early iteration effect is excellent, which is suitable for processing large-sample data $^{[23]}$. Since the amount of training data in this experiment is large, if SGD is used, only one sample is used for iteration each time, and the training speed is very fast, which can greatly reduce the time spent on training. The default values in its keras package are used, i.e., lr=0.01, momentum=0.0, decay=0.0, and nesterov=False.\nParameter Explanation:\nlr: Learning rate momentum: Momentum parameter decay: Learning rate decay value after each update nesterov: Determine whether to use Nesterov momentum 4.5.2 Algorithm Flow The specific algorithm flow in this section is as follows:\nUse A-share market-wide data of 10 factors (factors selected by LightGBM) and historical future one-month returns for each stock for one year as features. Take the future one-month return rate of each stock per year as the prediction target, and use BiLSTM for training, as shown in Fig. 12. Fig. 12. Rolling Window\nThe real-time factor data of out-of-sample data for one month is passed through the trained BiLSTM model to obtain the real-time expected return rate of each stock for the next month. The return rate is shown in Table 4.11. 4.5.3 Partial Stock Predicted Return Rate Table sec_code trade_date y_hat 000001.SZ 2011/5/26 0.0424621 000002.SZ 2011/5/26 -0.1632174 000004.SZ 2011/5/26 -0.0642319 000005.SZ 2011/5/26 0.08154649 000006.SZ 2011/5/26 0.00093213 000007.SZ 2011/5/26 -0.073218 000008.SZ 2011/5/26 -0.0464256 000009.SZ 2011/5/26 -0.091549 000010.SZ 2011/5/26 0.08154649 000011.SZ 2011/5/26 -0.1219943 000012.SZ 2011/5/26 -0.1448984 000014.SZ 2011/5/26 0.09038845 000016.SZ 2011/5/26 -0.11225 4.5.4 Code Implementation Snippet The following is a code snippet used in the training process for building the BiLSTM training network.\nbuild_net_blstm def build_net_blstm(self): model = ks.Sequential() model.add( ks.layers.Bidirectional(ks.layers.LSTM( 50 ),input_shape=(11,10)) ) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(256)) model.add( ks.layers.Dropout(0.01) ) model.add(ks.layers.Dense(64)) model.add(ks.layers.Dense(1)) model.compile(optimizer=\u0026#39;sgd\u0026#39;, loss=\u0026#39;mse\u0026#39;) model.summary() self.set_model(model) 4.6 Quantitative Strategy and Strategy Backtesting 4.6.1 Backtesting Metrics First, let\u0026rsquo;s introduce some common backtesting metrics for strategies. Evaluation metrics include Total Rate of Return, Annualized Rate of Return, Annualized volatility, Sharpe ratio, Maximum Drawdown (MDD), Annualized turnover rate, and Annualized transaction cost rate. It is assumed that the stock market is open for 252 days a year, the risk-free rate is defaulted to 0.035, and the commission fee is defaulted to 0.002.\nTotal Rate of Return: Under the same other indicators, the larger the cumulative rate of return, the better the strategy, and the more it can bring greater returns. The formula is as follows: $$ \\text{Total Rate of Return} = r_{p} = \\frac{P_{1} - P_{0}}{P_{0}} $$$P_{1}$: Total value of final stocks and cash\n$P_{0}$: Total value of initial stocks and cash\nAnnualized Rate of Return: It is to convert the cumulative total rate of return into a geometric average rate of return on an annual basis. Under the same other indicators, the larger the annualized rate of return, the better the strategy. The formula is as follows: $$ \\text{Annualized Rate of Return} = R_{p} = \\left(1 + r_{p}\\right)^{\\frac{252}{t}} - 1 $$$r_{p}$: Cumulative rate of return\n$t$: Number of days the investment strategy is executed\nAnnualized volatility: Defined as the standard deviation of the logarithmic value of the annual return rate of the object asset. Annualized volatility is used to measure the risk of a strategy. The greater the volatility, the higher the risk of the strategy. The formula is as follows: $$ \\begin{aligned} \\text{Annualized volatility} = \\sigma_{p} \u0026= \\sqrt{\\frac{252}{t-1} \\sum_{i=1}^{t}\\left(r_{d} - \\bar{r}_{d}\\right)^{2}} \\\\ \\bar{r}_{d} \u0026= \\frac{1}{t} \\sum_{i=1}^{t} r_{d_{i}} \\end{aligned} $$$r_{d_{i}}$: Daily return rate on the $i$-th day\n$\\bar{r}_{d}$: Average daily return rate\n$t$: Number of days the investment strategy is executed\nSharpe ratio: Proposed by Sharpe (1966)$^{[24]}$. It represents the excess return obtained by investors for bearing an extra unit of risk$^{[25]}$. Here is the calculation formula for the annualized Sharpe ratio: $$ S = \\frac{R_{p} - R_{f}}{\\sigma_{p}} $$$R_{p}$: Annualized rate of return\n$R_{f}$: Risk-free rate of return\n$\\sigma_{p}$: Annualized volatility\nMaximum Drawdown (MDD): Indicates the maximum value of the return rate drawdown when the total value of stocks and cash of our strategy portfolio reaches the lowest point during the operation period. Maximum drawdown is used to measure the most extreme possible loss situation of the strategy. $$ MDD = \\frac{\\max \\left(V_{x} - V_{y}\\right)}{V_{x}} $$$V_{x}$ and $V_{y}$ are the total value of stocks and cash of the strategy portfolio on day $x$ and day $y$ respectively, and $x \u0026lt; y$.\nAnnualized turnover rate: Used to measure the frequency of buying and selling stocks in the investment portfolio. The larger the value, the more frequent the portfolio turnover and the greater the transaction cost. $$ \\text{change} = \\frac{N \\times 252}{t} $$$t$: Number of days the investment strategy is executed\n$N$: Total number of buy and sell transactions\nAnnualized transaction cost rate: Used to measure the transaction cost of the investment portfolio strategy. The larger the value, the higher the transaction cost. $$ c = \\left(1 + \\text{commison}\\right)^{\\text{change}} - 1 $$change: Annualized turnover rate\ncommison: Commission fee\n4.6.2 Strategy and Backtesting Results The quantitative trading strategy in this paper adopts position switching every month (i.e., the rebalancing period is 28 trading days). Each time, the strategy adopts an equal-weight stock holding method to buy the 25 stocks with the highest expected return rate predicted by BiLSTM and sell the originally held stocks. The backtesting time and rules in this paper are as follows:\nBacktesting Time: From January 2012 to October 2020. Backtesting Stock Pool: All A-shares, excluding Special Treatment (ST) stocks. Transaction Fee: A brokerage commission of 0.2% is paid when buying, and a brokerage commission of 0.2% is paid when selling. If the commission for a single transaction is less than 5 CNY, the brokerage charges 5 CNY. Buying and Selling Rules: Stocks that hit the upper limit on the opening day cannot be bought, and stocks that hit the lower limit cannot be sold. 4.6.2.1 Strategy Backtesting Results Cumulative Return Annualized Return Annualized Volatility Sharpe Ratio Max Drawdown Annualized Turnover Rate Annualized Transaction Cost Rate Strategy 701.00% 29.18% 33.44% 0.77 51.10% 51.10% 11.35% Benchmark 110.40% 9.70% 26.01% 0.24 58.49% 58.49% 0.00% Fig. 22. Net Profit Curve\nThe backtesting results are shown in the table and Fig. 22 above. My strategy adopts the LightGBM-BiLSTM quantitative strategy introduced in this chapter. The benchmark uses the CSI All Share (000985). From the results above, it can be seen that the cumulative return of this strategy is 701.00%, which is much higher than the benchmark\u0026rsquo;s 110.40%; the annualized return is 29.18%, which is much higher than the benchmark\u0026rsquo;s 9.70%; and the Sharpe ratio is 0.77, which is higher than the benchmark\u0026rsquo;s 0.24. These three backtesting indicators show that the LightGBM-BiLSTM quantitative strategy can indeed bring greater returns to investors. The annualized volatility of this strategy is 33.44%, which is greater than the benchmark\u0026rsquo;s 26.01%, and the maximum drawdown is 51.10%, which is less than the benchmark\u0026rsquo;s 58.49%. These two backtesting indicators show that the LightGBM-BiLSTM quantitative strategy has certain risks, especially it is difficult to resist the impact of systemic risks. The annualized turnover rate is 11.35%, and the annualized transaction cost rate is 2.29%, indicating that our strategy is not a high-frequency trading strategy and the transaction cost is small. It can be seen from the return curve chart that the return rate of the LightGBM-BiLSTM quantitative strategy in the first two years is not much different from the benchmark, and there is no special advantage. However, from around April 2015, the return rate of the LightGBM-BiLSTM quantitative strategy is significantly better than the benchmark\u0026rsquo;s return rate. Overall, the return rate of this LightGBM-BiLSTM quantitative strategy is very considerable, but there are still certain risks.\nChapter 5 Conclusion and Future Directions 5.1 Conclusion This paper first introduced the research background and significance of stock price prediction and quantitative strategy research based on deep learning, and then introduced the domestic and international research status of stock price prediction and quantitative investment strategies respectively. Then, the innovations and research framework of this paper were explained. Then, in the chapter on related theoretical foundations, this paper briefly introduced the deep learning models and the development history of quantitative investment used in this paper. The basic structure, basic principles, and characteristics of the three models LSTM, GRU, and BiLSTM are mainly introduced.\nSubsequently, this paper used the daily frequency data of SPD Bank and IBM, and preprocessed the data through a series of data processing processes and feature extraction. Then, the specific network structure and hyperparameter settings of the three models LSTM, GRU, and BiLSTM were introduced. Then, we used LSTM, GRU, and BiLSTM to predict the closing prices of the two stocks and compare the model evaluations. The experimental results show that for both stocks, the BiLSTM prediction effect is more accurate.\nFinally, in order to further illustrate the application value of BiLSTM in finance, this paper constructed a quantitative investment model based on LightGBM-BiLSTM. Stocks in the entire A-share market and multiple factors were selected for factor cleaning, factor selection based on LightGBM, and factor combination based on LSTM. Then, we constructed a certain investment strategy and compared it with the benchmark holding CSI All Share through evaluation indicators such as cumulative return rate, annualized return rate, annualized volatility, and Sharpe ratio. Through comparison, it was found that the LightGBM-BiLSTM quantitative investment model can bring better returns, indicating the effectiveness of using deep learning to build quantitative investment strategies.\n5.2 Future Directions Although this paper compares the effects of LSTM, GRU, and BiLSTM models in predicting stock closing prices and achieves certain results based on the LightGBM-BiLSTM quantitative investment strategy, there are still some shortcomings in this paper\u0026rsquo;s research. Combining the research results of this paper, the following research and improvements can be further carried out:\nDiversification of Prediction Targets: In terms of predicting stock prices, this paper selects the stock closing price as the prediction target. Although this result is the most intuitive, the Random Walk Hypothesis (RWH) proposed by Bachelier (1900)$^{[26]}$ believes that stock prices follow a random walk and are unpredictable. Although many behavioral economists have since proved that this view is not entirely correct, it also shows that simply predicting stock closing prices is not so strong in terms of difficulty and interpretability $^{[27][28]}$. Therefore, stock volatility prediction, stock price increase/decrease judgment, and stock return rate prediction can be selected as future research directions. Diversified Model Comparison: In terms of predicting stock prices, this paper compares the three recurrent neural network models LSTM, GRU, and BiLSTM and shows that BiLSTM has better prediction effect, but there is still a lack of comparative research with more different models. Therefore, future in-depth research can be conducted on comparisons with Autoregressive Integrated Moving Average (ARIMA), Convolutional Neural Networks (CNNs), Deep Neural Networks (DNNs), CNN-LSTM, Transformer, TimeGPT, and other single or composite models. Factor Diversification: The factors used in this paper to construct quantitative investment strategies are all technical price-volume factors, and the types of factors are single. In the future, different types of factors such as financial factors, sentiment factors, and growth factors can be selected to improve the performance of the strategy. At the same time, future research can also appropriately add timing strategies to increase positions when predicting that the market will rise and reduce positions when predicting that the market will fall to earn beta (\\(\\beta\\)) returns. Investment Portfolio Optimization: The factor combination process in this paper is still imperfect. In the future, quadratic programming methods can be used to optimize the investment portfolio. High-Frequency Trading Strategy Research: The quantitative investment strategy method in this paper adopts a low-frequency trading strategy. In the future, stock tick data can be used to study high-frequency strategies and ultra-high-frequency strategies. References [1] White, H. “Economic prediction using neural networks: The case of IBM daily stock returns.” Proc. of ICNN. 1988, 2: 451-458.\n[2] Kimoto, T., Asakawa, K., Yoda, M., et al. “Stock market prediction system with modular neural networks.” Proc. of 1990 IJCNN International Joint Conference on Neural Networks. IEEE, 1990: 1-6.\n[3] Zhang, G. P. “Time series forecasting using a hybrid ARIMA and neural network model.” Neurocomputing. 2003, 50: 159-175.\n[4] Akita, R., Yoshihara, A., Matsubara, T., et al. “Deep learning for stock prediction using numerical and textual information.” Proc. of 2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS). IEEE, 2016: 1-6.\n[5] 宮崎邦洋, 松尾豊. “Deep Learning を用いた株価予測の分析.” 人工知能学会全国大会論文集 第31回全国大会. 一般社団法人 人工知能学会, 2017: 2D3OS19a3-2D3OS19a3.\n[6] Kim, T., Kim, H. Y. “Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data.” PLoS ONE. 2019, 14(2): e0212320.\n[7] Hochreiter, S., Schmidhuber, J. “Long short-term memory.” Neural Computation. 1997, 9(8): 1735-1780.\n[8] Cho, K., Van Merriënboer, B., Gulcehre, C., et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078. 2014.\n[9] Chung, J., Gulcehre, C., Cho, K. H., et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” arXiv preprint arXiv:1412.3555. 2014.\n[10] Gruber, N., Jockisch, A. “Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?” Frontiers in Artificial Intelligence. 2020, 3(40): 1-6.\n[11] Markowitz, H. “Portfolio Selection.” The Journal of Finance. 1952, 7(1): 77-91. doi:10.2307/2975974.\n[12] Merton, R. C. “An analytic derivation of the efficient portfolio frontier.” Journal of Financial and Quantitative Analysis. 1972: 1851-1872.\n[13] Sharpe, W. F. “Capital asset prices: A theory of market equilibrium under conditions of risk.” The Journal of Finance. 1964, 19(3): 425-442.\n[14] Lintner, J. “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.” Review of Economics and Statistics. 1965, 47(1): 13-37.\n[15] Mossin, J. “Equilibrium in a capital asset market.” Econometrica: Journal of the Econometric Society. 1966: 768-783.\n[16] Ross, S. A. “The arbitrage theory of capital asset pricing.” Journal of Economic Theory. 1976, 13(3): 341-60.\n[17] Fama, E. F., French, K. R. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics. 1993, 33(1): 3-56.\n[18] Fama, E. F., French, K. R. “A five-factor asset pricing model.” Journal of Financial Economics. 2015, 116(1): 1-22.\n[19] Kingma, D. P., Ba, J. “Adam: A method for stochastic optimization.” arXiv preprint arXiv:1412.6980. 2014.\n[20] Friedman, J. H. “Greedy function approximation: A gradient boosting machine.” Annals of Statistics. 2001: 1189-1232.\n[21] Kopitar, L., Kocbek, P., Cilar, L., et al. “Early detection of type 2 diabetes mellitus using machine learning-based prediction models.” Scientific Reports. 2020, 10(1): 1-12.\n[22] Ke, G., Meng, Q., Finley, T., et al. “Lightgbm: A highly efficient gradient boosting decision tree.” Advances in Neural Information Processing Systems. 2017, 30: 3146-3154.\n[23] Bottou, L., Curtis, F. E., Nocedal, J. “Optimization methods for large-scale machine learning.” SIAM Review. 2018, 60(2): 223-311.\n[24] Sharpe, W. F. “Mutual fund performance.” The Journal of Business. 1966, 39(1): 119-138.\n[25] Sharpe, W. F. “The sharpe ratio.” Journal of Portfolio Management. 1994, 21(1): 49-58.\n[26] Bachelier, L. “Théorie de la spéculation.” Annales Scientifiques de l\u0026rsquo;École Normale Supérieure. 1900, 17: 21-86.\n[27] Fromlet, H. “Behavioral finance-theory and practical application: Systematic analysis of departures from the homo oeconomicus paradigm are essential for realistic financial research and analysis.” Business Economics. 2001: 63-69.\n[28] Lo, A. W. “The adaptive markets hypothesis.” The Journal of Portfolio Management. 2004, 30(5): 15-29.\nReference Blog Colah\u0026rsquo;s Blog. (2015, August 27). Understanding LSTM Networks. Citation Citation: For reprint or citation of this article, please indicate the original author and source.\nCited as:\nYue Shui. (Apr 2021). Stock Price Prediction and Quantitative Strategy Based on Deep Learning. https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\nOr\n@article{syhya2021stockprediction, title = \u0026#34;Stock Price Prediction and Quantitative Strategy Based on Deep Learning\u0026#34;, author = \u0026#34;Yue Shui\u0026#34;, journal = \u0026#34;syhya.github.io\u0026#34;, year = \u0026#34;2021\u0026#34;, month = \u0026#34;Apr\u0026#34;, url = \u0026#34;https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/\u0026#34; } ","permalink":"https://syhya.github.io/posts/2021-04-21-deep-learning-stock-prediction/","summary":"\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eThe stock market is a crucial component of the financial market. In recent years, with its vigorous development, research on stock price prediction and quantitative investment strategies has attracted scholars from various fields.  With the advancement of Artificial Intelligence (AI) and Machine Learning (ML) in recent years, researchers have shifted from traditional statistical models to AI algorithms. Particularly after the deep learning boom, neural networks have achieved remarkable results in stock price prediction and quantitative investment strategy research. The objective of deep learning is to learn multi-level features, constructing abstract high-level features by combining low-level ones, thereby mining the distributed feature representations of data. This approach enables complex nonlinear modeling to accomplish prediction tasks. Recurrent Neural Networks (RNNs) have been widely applied to sequential data, such as natural language and speech. Daily stock prices and trading information are sequential data, leading many researchers to use RNNs for stock price prediction. However, basic RNNs suffer from gradient vanishing issues when the number of layers is excessive. The advent of Long Short-Term Memory (LSTM) networks addressed this problem, followed by variants such as Gated Recurrent Units (GRUs), Peephole LSTMs, and Bidirectional LSTMs (BiLSTMs). Traditional stock prediction models often overlook temporal factors or only consider unidirectional temporal relationships. Therefore, this paper employs the BiLSTM model for stock price prediction. From a model principle perspective, the BiLSTM model fully leverages the contextual relationships in both forward and backward temporal directions of time series data. It also avoids gradient vanishing and explosion problems in long sequences, enabling better learning of information with long-term dependencies.\u003c/p\u003e","title":"Stock Price Prediction and Quantitative Strategy Based on Deep Learning"}]