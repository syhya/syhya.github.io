<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Agentic RL | Yue Shui Blog</title><meta name=keywords content="Agentic RL,Reinforcement Learning,LLM,Agent,SWE-bench,verl,ReTool"><meta name=description content="As Large Language Models (LLMs) achieve breakthroughs in natural language processing, their applications continue to expand. However, they also exhibit limitations such as knowledge cutoffs, hallucinations, and deficiencies in complex computation and logical reasoning. To address these challenges, Agentic RL, which combines agents with Reinforcement Learning (RL), is emerging as a key research direction.
Agentic RL enables LLMs to possess capabilities like autonomous planning, decision-making, tool use, and environmental interaction by creating a closed-loop interaction with the external world (e.g., search engines, code interpreters, databases, browsers) and continuously optimizing through reward signals. In practical applications, it not only understands requirements and plans autonomously but also constantly corrects and optimizes within an execution-feedback loop."><meta name=author content="Yue Shui"><link rel=canonical href=https://syhya.github.io/posts/2025-09-30-agentic-rl/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.9271b00776af10d7feb512a59a411af859df11a64bb143b10ab1170a4e8da23f.css integrity="sha256-knGwB3avENf+tRKlmkEa+FnfEaZLsUOxCrEXCk6Noj8=" rel="preload stylesheet" as=style><link rel=icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://syhya.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://syhya.github.io/posts/2025-09-30-agentic-rl/><link rel=alternate hreflang=zh href=https://syhya.github.io/zh/posts/2025-09-30-agentic-rl/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><script>window.MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css><script src=https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-SZ2660B91F"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-SZ2660B91F")}</script><meta property="og:url" content="https://syhya.github.io/posts/2025-09-30-agentic-rl/"><meta property="og:site_name" content="Yue Shui Blog"><meta property="og:title" content="Agentic RL"><meta property="og:description" content="As Large Language Models (LLMs) achieve breakthroughs in natural language processing, their applications continue to expand. However, they also exhibit limitations such as knowledge cutoffs, hallucinations, and deficiencies in complex computation and logical reasoning. To address these challenges, Agentic RL, which combines agents with Reinforcement Learning (RL), is emerging as a key research direction.
Agentic RL enables LLMs to possess capabilities like autonomous planning, decision-making, tool use, and environmental interaction by creating a closed-loop interaction with the external world (e.g., search engines, code interpreters, databases, browsers) and continuously optimizing through reward signals. In practical applications, it not only understands requirements and plans autonomously but also constantly corrects and optimizes within an execution-feedback loop."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-30T12:00:00+08:00"><meta property="article:modified_time" content="2025-09-30T12:00:00+08:00"><meta property="article:tag" content="Agentic RL"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Agent"><meta property="article:tag" content="SWE-Bench"><meta property="article:tag" content="Verl"><meta property="og:image" content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Agentic RL"><meta name=twitter:description content="As Large Language Models (LLMs) achieve breakthroughs in natural language processing, their applications continue to expand. However, they also exhibit limitations such as knowledge cutoffs, hallucinations, and deficiencies in complex computation and logical reasoning. To address these challenges, Agentic RL, which combines agents with Reinforcement Learning (RL), is emerging as a key research direction.
Agentic RL enables LLMs to possess capabilities like autonomous planning, decision-making, tool use, and environmental interaction by creating a closed-loop interaction with the external world (e.g., search engines, code interpreters, databases, browsers) and continuously optimizing through reward signals. In practical applications, it not only understands requirements and plans autonomously but also constantly corrects and optimizes within an execution-feedback loop."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://syhya.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Agentic RL","item":"https://syhya.github.io/posts/2025-09-30-agentic-rl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Agentic RL","name":"Agentic RL","description":"As Large Language Models (LLMs) achieve breakthroughs in natural language processing, their applications continue to expand. However, they also exhibit limitations such as knowledge cutoffs, hallucinations, and deficiencies in complex computation and logical reasoning. To address these challenges, Agentic RL, which combines agents with Reinforcement Learning (RL), is emerging as a key research direction.\nAgentic RL enables LLMs to possess capabilities like autonomous planning, decision-making, tool use, and environmental interaction by creating a closed-loop interaction with the external world (e.g., search engines, code interpreters, databases, browsers) and continuously optimizing through reward signals. In practical applications, it not only understands requirements and plans autonomously but also constantly corrects and optimizes within an execution-feedback loop.\n","keywords":["Agentic RL","Reinforcement Learning","LLM","Agent","SWE-bench","verl","ReTool"],"articleBody":"As Large Language Models (LLMs) achieve breakthroughs in natural language processing, their applications continue to expand. However, they also exhibit limitations such as knowledge cutoffs, hallucinations, and deficiencies in complex computation and logical reasoning. To address these challenges, Agentic RL, which combines agents with Reinforcement Learning (RL), is emerging as a key research direction.\nAgentic RL enables LLMs to possess capabilities like autonomous planning, decision-making, tool use, and environmental interaction by creating a closed-loop interaction with the external world (e.g., search engines, code interpreters, databases, browsers) and continuously optimizing through reward signals. In practical applications, it not only understands requirements and plans autonomously but also constantly corrects and optimizes within an execution-feedback loop.\nIts core value is primarily reflected in two aspects:\nReducing Prompt Dependency: It frees the model from over-reliance on prompts, equipping it with adaptive problem-solving abilities. Enhancing Autonomous Exploration: Through multi-turn reinforcement learning, it improves exploration and reasoning capabilities, thereby compensating for the shortcomings of sparse or repetitive static data distributions. Agentic RL vs. LLM-RL Fig. 1. Paradigm shift from LLM-RL to Agentic RL. (Image source: Zhang et al., 2025)\nAlignment-focused LLM-RL, represented by RLHF, is often approximated in practice as a single-step (sequence-level) decision-making Markov Decision Process (MDP). In contrast, Agentic RL operates in partially observable environments, involving multi-step, long-horizon sequential decision-making, which is better characterized by a Partially Observable Markov Decision Process (POMDP). The table below summarizes the differences between the two.\nFeature Traditional LLM-RL (e.g., RLHF) Agentic RL Decision Process Degenerate single-step MDP: Input prompt → Output complete response → One-time reward, akin to a “single-turn mapping.” Multi-step, long-horizon POMDP: Continuous interaction in a partially observable environment, where each step updates the state and receives feedback. State Space \\(\\mathcal{S}\\) Static, determined solely by the input prompt, and does not evolve during the process. Dynamic, includes interaction history, tool outputs, external environment states, etc., and is constantly updated through interaction. Action Space \\(\\mathcal{A}\\) Single action: generating a text sequence (response). Composite actions: generating a chain of thought (Thought), calling a tool (Tool Call), updating the state, and generating the final answer. Reward \\(\\mathcal{R}\\) Sparse Outcome Reward: Typically given after generation is complete, based on human preference or a model judge. Hybrid Reward Mechanism: Includes both sparse Outcome Rewards and dense Process Rewards (e.g., success/failure of tool calls, sub-task completion). Core Challenges Aligning with human preferences, ensuring safety and usefulness; improving overall generation quality. Long-horizon credit assignment, complex task planning, exploration efficiency, robust tool use, and balancing exploration-exploitation. Evaluation Scientific, comprehensive, and realistic evaluation benchmarks are crucial for measuring and enhancing the capabilities of LLM Agents. “Successful language model evals” (Wei, 2024) summarizes several key traits of successful evaluation benchmarks, which collectively determine whether an eval can be widely accepted by the community and stand the test of time:\nSufficient Sample Size: An eval needs enough examples (at least 1,000 is recommended) to reduce random fluctuations in results. Too few samples can cause scores to fluctuate wildly between model checkpoints, making it painful for researchers to track real performance changes. High-Quality Data: The data in the eval (questions, answers, test cases, etc.) must be accurate. If the eval itself contains many errors, researchers will lose trust in it, especially when a powerful model like GPT-4 disagrees with the ground truth. Simple Single-Number Metric: A successful eval must have a core, easily understandable single-number metric (e.g., accuracy). Overly complex evaluation systems, like the first version of HELM (Liang et al., 2022), can be too comprehensive, making it hard for researchers to focus and hindering quick comparisons and dissemination. Easy to Run and Reproduce: The evaluation process should be as simple and efficient as possible. If running an eval requires complex setup and long wait times, like some subsets of BIG-Bench (Srivastava et al., 2022), it will significantly impede its adoption. Meaningful Task: The eval’s task should be central to intelligence, such as language understanding, math reasoning, or code generation. Tasks that are challenging but not meaningful (e.g., closing parentheses properly) don’t allow for substantive conclusions about a model’s intelligence, even if it performs well. Accurate and Reliable Grading: The automated grading script must be extremely robust and correct. If researchers find that their model’s correct output is graded incorrectly, they will quickly write off the eval. Avoids Premature Saturation: The eval should be difficult enough to ensure that model performance has room to grow over time. Benchmarks like GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) became saturated too quickly, losing their utility as a measure of progress. The concept of Asymmetry of verification (Wei, 2024) points out that for many tasks, it is much easier to verify a solution than to solve it from scratch.\nFig. 2. Improving Verification with Privileged Information. (Image source: Wei, 2024)\nFor example, solving a complex Sudoku puzzle can take hours, but checking a completed grid takes only minutes. Writing the backend code for a large website like Instagram takes a team of engineers years, but any user can quickly verify if the site is working. Similarly, for many information retrieval or open-ended tasks, generating an answer may require extensive trial and error, but verifying a candidate solution against constraints is often much faster.\nWe can significantly reduce the cost of verification by preparing prior information or verification mechanisms. For instance:\nSWE-bench: Verifying code correctness would normally require manual line-by-line review. However, with a pre-existing suite of test cases with ample coverage, one can simply run the tests to determine if a model-generated patch is valid in seconds. AIME Math Competition: The derivation process for math problems is often complex and time-consuming, but once the official answer is released, any proposed solution can be verified against it in seconds. This asymmetry is crucial for AI training because it directly relates to the feasibility of creating an RL environment or a reward model. From this, Jason Wei proposed the Verifier’s Rule:\nThe ease of training AI to solve a task is proportional to how verifiable the task is. All tasks that are possible to solve and easy to verify will be solved by AI.\nA task’s verifiability typically depends on whether it satisfies these five key properties:\nObjective truth: Everyone agrees on what constitutes a good solution (e.g., the unique correct answer to a math problem). Fast to verify: A single solution can be checked in seconds (e.g., running a set of test cases). Scalable to verify: Many candidate solutions can be verified in parallel (e.g., batch-running code tests). Low noise: The verification signal is highly correlated with solution quality, with a low rate of false positives/negatives. Continuous reward: It’s possible to rank the quality of multiple solutions, not just determine correctness, which provides a smoother optimization signal. This rule explains why scenarios like the SWE-bench programming task and AIME math problem-solving have become ideal testbeds for AI capabilities. They naturally meet most of the above criteria, allowing us to efficiently build automated evaluation systems and continuously optimize model performance through large-scale “generate-and-verify” loops.\nSWE-bench Fig. 3. SWE-bench links real GitHub issues with their merged pull request fixes. Given an issue description and a codebase snapshot, models generate a patch that is tested against actual project test cases. (Image source: Jimenez et al., 2024)\nSWE-bench (Jimenez et al., 2024) collects 2,294 real-world development tasks from 12 popular open-source Python projects, sourced directly from GitHub Issues and Pull Requests.\nTo ensure reproducibility and environment independence, SWE-bench constructs an isolated Docker environment for each task, preventing failures due to inconsistencies in Python versions or dependencies. This design also forces the model to learn to generate compatible patches for different environments.\nFor verification, SWE-bench cleverly utilizes the projects’ built-in unit tests to automatically evaluate the correctness of the LLM’s patch. It includes two types of tests:\nFail-to-Pass (F2P): Tests that initially fail but should pass after the correct PR is merged, confirming that the LLM has fixed the target issue. Pass-to-Pass (P2P): Tests that initially pass and must continue to pass after the PR is merged, ensuring the LLM has not broken existing functionality or introduced new bugs. This combination of “real tasks + isolated environments + automated testing” makes SWE-bench a high-fidelity, scalable benchmark that significantly reduces the cost of verifying programming task correctness. However, the original SWE-bench had flaws like unfair test cases, ambiguous problem descriptions, and complex environments, which led to underestimation of model capabilities. Consequently, OpenAI created a high-quality, human-curated subset called SWE-bench Verified (OpenAI, 2024) for more accurate model evaluation.\nBrowseComp Unlike software engineering tasks, web browsing tasks aim to find specific information within the vast expanse of the internet. BrowseComp (Wei et al., 2025) is a simple yet challenging benchmark designed for such tasks.\nDesign Philosophy: BrowseComp follows the principle of being hard to solve, easy to verify. The problems are designed to require persistent and creative browsing of numerous web pages to find the answer, but the answer itself is typically a short, indisputable string that can be easily compared against a reference.\nData Construction: The creators use a reverse questioning method. They first find an obscure fact (e.g., a specific conference paper) and then construct a query with multiple complex constraints around it. For example: “Find a paper published at EMNLP between 2018-2023 whose first author graduated from Dartmouth College and whose fourth author graduated from the University of Pennsylvania.” Verifying this answer is simple, but finding it among thousands of papers is extremely difficult.\nFig. 4. BrowseComp performance of an early version of OpenAI Deep Research scales smoothly with test-time compute. (Image source: Wei et al., 2025)\nBrowseComp measures an agent’s core browsing abilities: factual reasoning, persistent navigation, and creative search. As shown in the figure, the performance of a powerful browsing agent (like OpenAI Deep Research) on this benchmark scales smoothly with test-time compute (i.e., browsing effort), indicating that the eval effectively measures an agent’s deep search and information integration capabilities.\nData High-quality data is the cornerstone of training powerful agents. However, manually annotating the complete decision trajectories of agents in complex tasks is extremely costly and difficult to scale. Therefore, Synthetic Data has become the mainstream solution in this field. Various innovative data generation pipelines have been proposed to create a “data flywheel” that continuously produces high-quality training data.\nAgentFounder Fig. 5. The Agentic Training Pipeline proposed by AgentFounder, incorporating an Agentic CPT stage. (Image source: Su et al., 2025)\nAgentFounder (Su et al., 2025) introduces a new stage between traditional pre-training and post-training called Agentic Continual Pre-training (Agentic CPT). The entire training pipeline consists of three stages:\nGeneral Pre-training: Consistent with standard procedures, a base model with general knowledge is first trained. Agentic Continual Pre-training (Agentic CPT): On top of the general base model, large-scale, diverse synthetic agent behavior data is used for continued “next-word prediction” training. The goal is not to solve specific tasks but to have the model internalize general agent behavior patterns and develop agentic capabilities. Post-training/Task Fine-tuning: On the model that already possesses basic agentic capabilities, SFT or RL is performed to align it with specific tasks. This avoids the optimization conflict that arises when trying to learn capabilities and align with tasks simultaneously during the post-training phase. The key to Agentic CPT is how to synthesize large-scale agent-like data at low cost. To this end, AgentFounder proposes two efficient data generation methods that do not require external tool calls: First-order Action Synthesis (FAS) and Higher-order Action Synthesis (HAS).\nThe core idea of FAS is to generate data about how to think and how to plan the first step at a low cost through deduction. Fig. 6. Illustration of First-order Action Synthesis (FAS). (Image source: Su et al., 2025)\nPlanning Action: Given a problem, have the LLM generate multiple possible analyses and initial tool call plans to help the model learn task decomposition and preliminary planning. Reasoning Action: Given a problem and relevant knowledge snippets, have the LLM generate a complete logical reasoning chain to arrive at the final answer, thereby training its logical deduction and information synthesis abilities. FAS only generates thought processes and planned actions, without involving actual tool calls or environmental interactions. This makes its generation cost extremely low, making it suitable for large-scale data synthesis (up to hundreds of millions of examples).\nThe goal of HAS is to transform existing (even suboptimal) agent trajectories into high-value decision-making learning data. Fig. 7. Illustration of Higher-order Action Synthesis (HAS). (Image source: Su et al., 2025)\nStep-level Expansion: For any step in a trajectory, use an LLM to generate multiple alternative actions, creating a local decision space. Contrastive Learning: Reframe the original choice and the expanded candidate actions as a multiple-choice question with feedback, requiring the model to identify the better decision. Causal Supervision: Append the final outcome (success or failure) to the end of the trajectory to help the model learn the causal link between decisions and outcomes. This method upgrades traditional Imitation Learning to step-level Decision Making. The model not only “walks through a successful path” but also understands how to choose at each critical juncture, improving the signal-to-noise ratio and utilization efficiency of the data.\nDoes Agentic CPT truly alleviate the “optimization conflict”? The paper provides an answer through training loss curves and performance comparison experiments.\nFig. 8. Training loss evolution showing superior convergence of AgentFounder models compared to baseline. (Image source: Su et al., 2025)\nThe experimental results show that models that underwent Agentic CPT converge faster during the SFT phase, with training losses significantly lower than baseline models that did not go through this stage. This indicates that the model has already developed certain agentic capabilities before entering the post-training phase, making it more efficient when learning specific tasks later on.\nWebShaper Traditional data synthesis methods are typically information-driven: information is first crawled from the web, and then questions are generated based on that information. WebShaper (Tao et al., 2025) points out that this approach can lead to a mismatch between the reasoning structure in the generated questions and the structure of the original information, allowing models to find answers through “logical shortcuts” rather than genuine multi-step reasoning. To address this, it proposes a Formalism-Driven paradigm, primarily using knowledge graphs and set theory to formalize problems.\nFig. 9. The formalism-driven data synthesis pipeline of WebShaper. (Image source: Tao et al., 2025)\nKnowledge Projections (KP): WebShaper first formalizes information-seeking tasks based on set theory. A Knowledge Projection \\(R(V)\\) is defined as the set of all entities that have a relation \\(R\\) with the entity set \\(V\\). For example, bornIn({1990s}) represents the set of all entities born in the 1990s. Task Formalization: Complex queries can be rigorously represented as Intersection and Union operations of multiple KPs. For example, the query “Find a player who played for an East German team founded in 1966 during the 2004-05 season and was born in the 90s” can be formalized as the intersection of multiple KPs. Expander Agent: WebShaper uses an agent called Expander, which first generates a formalized query structure (e.g., the intersection of three KPs) and then progressively populates this structure with specific content by calling tools (search, summarization). It uses a “hierarchical expansion” strategy to gradually increase problem complexity, effectively avoiding logical shortcuts and information redundancy. Reward Design The reward function is the soul of RL, defining the agent’s optimization objective.\nVerifiable Rewards: For tasks with clear answers, such as mathematics and coding, this is the most reliable and scalable source of rewards. The reward signal can come directly from unit test pass rates, code compiler feedback, or the correctness of the final answer. This rule-based reward effectively avoids the reward hacking problem that reward models might introduce.\nGenerative Rewards: For open-ended tasks without a single correct answer (e.g., generating a research report), the LLM-as-a-Judge (Zheng et al., 2023) approach uses a powerful LLM as a judge to evaluate the quality of the generated output and provide a score or natural language feedback as the reward signal.\nDense Rewards: Unlike an Outcome Reward Model (ORM), which provides a one-time reward only at the end of a task, a Process Reward Model (PRM) provides feedback for each step or intermediate stage of the agent’s process. This helps solve the credit assignment problem in long-horizon tasks but also increases annotation costs and the risk of being exploited by the model.\nUnsupervised Rewards: To eliminate reliance on external annotations, researchers have explored methods for extracting reward signals from the model’s own behavior, such as constructing rewards based on output consistency (whether multiple generations are consistent) or internal confidence (e.g., the entropy of generation probabilities).\nOptimization Algorithms In recent years, numerous improved algorithms have been developed based on methods like PPO, DPO, and GRPO. Below are three representative examples.\nPPO Proximal Policy Optimization (PPO) (Schulman et al., 2017) is a classic Actor-Critic algorithm that has become a mainstream method for RL fine-tuning of LLMs due to its successful application in InstructGPT (Ouyang et al., 2022). The core idea of PPO is to limit the magnitude of change between the new and old policies during updates, thereby ensuring training stability. It uses a token-level importance ratio and clipping to constrain policy shifts and employs a Critic model to estimate the advantage (decomposing sequence-level rewards to the token-level), which improves stability but introduces additional model and computational overhead.\n$$ \\mathcal{J}_{\\mathrm{PPO}}(\\theta)=\\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta_{\\text {old }}}(\\cdot \\mid x)}\\left[\\frac{1}{|y|} \\sum_{t=1}^{|y|} \\min \\left(w_t(\\theta) \\widehat{A}_t, \\operatorname{clip}\\left(w_t(\\theta), 1-\\varepsilon, 1+\\varepsilon\\right) \\widehat{A}_t\\right)\\right] $$where the importance ratio of the token $y_{t}$ is defined as $w_t(\\theta)=\\frac{\\pi_\\theta\\left(y_t \\mid x, y_{","wordCount":"5072","inLanguage":"en","image":"https://syhya.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-09-30T12:00:00+08:00","dateModified":"2025-09-30T12:00:00+08:00","author":{"@type":"Person","name":"Yue Shui"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://syhya.github.io/posts/2025-09-30-agentic-rl/"},"publisher":{"@type":"Organization","name":"Yue Shui Blog","logo":{"@type":"ImageObject","url":"https://syhya.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://syhya.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://syhya.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://syhya.github.io/zh/ title=简体中文 aria-label=简体中文>Zh</a></li></ul></div></div><ul id=menu><li><a href=https://syhya.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://syhya.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://syhya.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://syhya.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://syhya.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Agentic RL</h1><div class=post-meta><span title='2025-09-30 12:00:00 +0800 +0800'>Created:&nbsp;2025-09-30</span>&nbsp;·&nbsp;Updated:&nbsp;2025-09-30&nbsp;·&nbsp;24 min&nbsp;·&nbsp;5072 words&nbsp;·&nbsp;Yue Shui&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://syhya.github.io/zh/posts/2025-09-30-agentic-rl/>Zh</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#agentic-rl-vs-llm-rl>Agentic RL vs. LLM-RL</a></li><li><a href=#evaluation>Evaluation</a><ul><li><a href=#swe-bench>SWE-bench</a></li><li><a href=#browsecomp>BrowseComp</a></li></ul></li><li><a href=#data>Data</a><ul><li><a href=#agentfounder>AgentFounder</a></li><li><a href=#webshaper>WebShaper</a></li></ul></li><li><a href=#reward-design>Reward Design</a></li><li><a href=#optimization-algorithms>Optimization Algorithms</a><ul><li><a href=#ppo>PPO</a></li><li><a href=#grpo>GRPO</a></li><li><a href=#gspo>GSPO</a></li><li><a href=#frameworks>Frameworks</a></li><li><a href=#verl>verl</a></li></ul></li><li><a href=#case-studies>Case Studies</a><ul><li><a href=#search-r1>Search-R1</a></li><li><a href=#retool>ReTool</a></li></ul></li><li><a href=#references>References</a></li><li><a href=#citation>Citation</a></li></ul></nav></div></details></div><div class=post-content><p>As Large Language Models (LLMs) achieve breakthroughs in natural language processing, their applications continue to expand. However, they also exhibit limitations such as knowledge cutoffs, hallucinations, and deficiencies in complex computation and logical reasoning. To address these challenges, <strong>Agentic RL</strong>, which combines agents with Reinforcement Learning (RL), is emerging as a key research direction.</p><p>Agentic RL enables LLMs to possess capabilities like autonomous planning, decision-making, tool use, and environmental interaction by creating a closed-loop interaction with the external world (e.g., search engines, code interpreters, databases, browsers) and continuously optimizing through reward signals. In practical applications, it not only understands requirements and plans autonomously but also constantly corrects and optimizes within an execution-feedback loop.</p><p>Its core value is primarily reflected in two aspects:</p><ul><li><strong>Reducing Prompt Dependency</strong>: It frees the model from over-reliance on prompts, equipping it with adaptive problem-solving abilities.</li><li><strong>Enhancing Autonomous Exploration</strong>: Through multi-turn reinforcement learning, it improves exploration and reasoning capabilities, thereby compensating for the shortcomings of sparse or repetitive static data distributions.</li></ul><h2 id=agentic-rl-vs-llm-rl>Agentic RL vs. LLM-RL<a hidden class=anchor aria-hidden=true href=#agentic-rl-vs-llm-rl>#</a></h2><figure class=align-center><a href=agentic_rl_survey.png data-fancybox=gallery><img loading=lazy src=agentic_rl_survey.png#center alt="Fig. 1. Paradigm shift from LLM-RL to Agentic RL. (Image source: Zhang et al., 2025)" width=100%></a><figcaption><p>Fig. 1. Paradigm shift from LLM-RL to Agentic RL. (Image source: <a href=https://arxiv.org/abs/2509.02547>Zhang et al., 2025</a>)</p></figcaption></figure><p>Alignment-focused <strong>LLM-RL</strong>, represented by RLHF, is often approximated in practice as a single-step (sequence-level) decision-making <a href=https://en.wikipedia.org/wiki/Markov_decision_process>Markov Decision Process (MDP)</a>. In contrast, <strong>Agentic RL</strong> operates in partially observable environments, involving multi-step, long-horizon sequential decision-making, which is better characterized by a <a href=https://en.wikipedia.org/wiki/Partially_observable_markov_decision_process>Partially Observable Markov Decision Process (POMDP)</a>. The table below summarizes the differences between the two.</p><table><thead><tr><th style=text-align:left>Feature</th><th style=text-align:left>Traditional LLM-RL (e.g., RLHF)</th><th style=text-align:left>Agentic RL</th></tr></thead><tbody><tr><td style=text-align:left><strong>Decision Process</strong></td><td style=text-align:left><strong>Degenerate single-step MDP</strong>: Input prompt → Output complete response → One-time reward, akin to a &ldquo;single-turn mapping.&rdquo;</td><td style=text-align:left><strong>Multi-step, long-horizon POMDP</strong>: Continuous interaction in a partially observable environment, where each step updates the state and receives feedback.</td></tr><tr><td style=text-align:left><strong>State Space</strong> \(\mathcal{S}\)</td><td style=text-align:left>Static, determined solely by the input prompt, and does not evolve during the process.</td><td style=text-align:left>Dynamic, includes interaction history, tool outputs, external environment states, etc., and is constantly updated through interaction.</td></tr><tr><td style=text-align:left><strong>Action Space</strong> \(\mathcal{A}\)</td><td style=text-align:left>Single action: generating a text sequence (response).</td><td style=text-align:left>Composite actions: generating a chain of thought (Thought), calling a tool (Tool Call), updating the state, and generating the final answer.</td></tr><tr><td style=text-align:left><strong>Reward</strong> \(\mathcal{R}\)</td><td style=text-align:left><strong>Sparse Outcome Reward</strong>: Typically given after generation is complete, based on human preference or a model judge.</td><td style=text-align:left><strong>Hybrid Reward Mechanism</strong>: Includes both sparse Outcome Rewards and dense Process Rewards (e.g., success/failure of tool calls, sub-task completion).</td></tr><tr><td style=text-align:left><strong>Core Challenges</strong></td><td style=text-align:left>Aligning with human preferences, ensuring safety and usefulness; improving overall generation quality.</td><td style=text-align:left>Long-horizon credit assignment, complex task planning, exploration efficiency, robust tool use, and balancing exploration-exploitation.</td></tr></tbody></table><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>Scientific, comprehensive, and realistic evaluation benchmarks are crucial for measuring and enhancing the capabilities of LLM Agents. <strong>&ldquo;Successful language model evals&rdquo;</strong> (<a href=https://www.jasonwei.net/blog/evals>Wei, 2024</a>) summarizes several key traits of successful evaluation benchmarks, which collectively determine whether an eval can be widely accepted by the community and stand the test of time:</p><ol><li><strong>Sufficient Sample Size</strong>: An eval needs enough examples (at least 1,000 is recommended) to reduce random fluctuations in results. Too few samples can cause scores to fluctuate wildly between model checkpoints, making it painful for researchers to track real performance changes.</li><li><strong>High-Quality Data</strong>: The data in the eval (questions, answers, test cases, etc.) must be accurate. If the eval itself contains many errors, researchers will lose trust in it, especially when a powerful model like GPT-4 disagrees with the ground truth.</li><li><strong>Simple Single-Number Metric</strong>: A successful eval must have a core, easily understandable single-number metric (e.g., accuracy). Overly complex evaluation systems, like the first version of <strong>HELM</strong> (<a href=https://arxiv.org/abs/2211.09110>Liang et al., 2022</a>), can be too comprehensive, making it hard for researchers to focus and hindering quick comparisons and dissemination.</li><li><strong>Easy to Run and Reproduce</strong>: The evaluation process should be as simple and efficient as possible. If running an eval requires complex setup and long wait times, like some subsets of <strong>BIG-Bench</strong> (<a href=https://arxiv.org/abs/2206.04615>Srivastava et al., 2022</a>), it will significantly impede its adoption.</li><li><strong>Meaningful Task</strong>: The eval&rsquo;s task should be central to intelligence, such as language understanding, math reasoning, or code generation. Tasks that are challenging but not meaningful (e.g., closing parentheses properly) don&rsquo;t allow for substantive conclusions about a model&rsquo;s intelligence, even if it performs well.</li><li><strong>Accurate and Reliable Grading</strong>: The automated grading script must be extremely robust and correct. If researchers find that their model&rsquo;s correct output is graded incorrectly, they will quickly write off the eval.</li><li><strong>Avoids Premature Saturation</strong>: The eval should be difficult enough to ensure that model performance has room to grow over time. Benchmarks like <strong>GLUE</strong> (<a href=https://arxiv.org/abs/1804.07461>Wang et al., 2018</a>) and <strong>SuperGLUE</strong> (<a href=https://arxiv.org/abs/1905.00537>Wang et al., 2019</a>) became saturated too quickly, losing their utility as a measure of progress.</li></ol><p>The concept of <strong>Asymmetry of verification</strong> (<a href=https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law>Wei, 2024</a>) points out that for many tasks, <strong>it is much easier to verify a solution than to solve it from scratch</strong>.</p><figure class=align-center><a href=asymmetry_verification.png data-fancybox=gallery><img loading=lazy src=asymmetry_verification.png#center alt="Fig. 2. Improving Verification with Privileged Information. (Image source: Wei, 2024)" width=100%></a><figcaption><p>Fig. 2. Improving Verification with Privileged Information. (Image source: <a href=https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law>Wei, 2024</a>)</p></figcaption></figure><p>For example, solving a complex Sudoku puzzle can take hours, but checking a completed grid takes only minutes. Writing the backend code for a large website like Instagram takes a team of engineers years, but any user can quickly verify if the site is working. Similarly, for many information retrieval or open-ended tasks, generating an answer may require extensive trial and error, but verifying a candidate solution against constraints is often much faster.</p><p>We can significantly reduce the cost of verification by preparing prior information or verification mechanisms. For instance:</p><ul><li><strong>SWE-bench</strong>: Verifying code correctness would normally require manual line-by-line review. However, with a pre-existing suite of test cases with ample coverage, one can simply run the tests to determine if a model-generated patch is valid in seconds.</li><li><strong>AIME Math Competition</strong>: The derivation process for math problems is often complex and time-consuming, but once the official answer is released, any proposed solution can be verified against it in seconds.</li></ul><p>This asymmetry is crucial for AI training because it directly relates to the feasibility of creating an RL environment or a reward model. From this, Jason Wei proposed the <strong>Verifier&rsquo;s Rule</strong>:</p><blockquote><p><em>The ease of training AI to solve a task is proportional to how verifiable the task is. All tasks that are possible to solve and easy to verify will be solved by AI.</em></p></blockquote><p>A task&rsquo;s verifiability typically depends on whether it satisfies these five key properties:</p><ol><li><strong>Objective truth</strong>: Everyone agrees on what constitutes a good solution (e.g., the unique correct answer to a math problem).</li><li><strong>Fast to verify</strong>: A single solution can be checked in seconds (e.g., running a set of test cases).</li><li><strong>Scalable to verify</strong>: Many candidate solutions can be verified in parallel (e.g., batch-running code tests).</li><li><strong>Low noise</strong>: The verification signal is highly correlated with solution quality, with a low rate of false positives/negatives.</li><li><strong>Continuous reward</strong>: It&rsquo;s possible to rank the quality of multiple solutions, not just determine correctness, which provides a smoother optimization signal.</li></ol><p>This rule explains why scenarios like the <strong>SWE-bench programming task</strong> and <strong>AIME math problem-solving</strong> have become ideal testbeds for AI capabilities. They naturally meet most of the above criteria, allowing us to efficiently build automated evaluation systems and continuously optimize model performance through large-scale &ldquo;generate-and-verify&rdquo; loops.</p><h3 id=swe-bench>SWE-bench<a hidden class=anchor aria-hidden=true href=#swe-bench>#</a></h3><figure class=align-center><a href=swe_bench.png data-fancybox=gallery><img loading=lazy src=swe_bench.png#center alt="Fig. 3. SWE-bench links real GitHub issues with their merged pull request fixes. Given an issue description and a codebase snapshot, models generate a patch that is tested against actual project test cases. (Image source: Jimenez et al., 2024)" width=100%></a><figcaption><p>Fig. 3. SWE-bench links real GitHub issues with their merged pull request fixes. Given an issue description and a codebase snapshot, models generate a patch that is tested against actual project test cases. (Image source: <a href=https://arxiv.org/abs/2310.06770>Jimenez et al., 2024</a>)</p></figcaption></figure><p><strong>SWE-bench</strong> (<a href=https://arxiv.org/abs/2310.06770>Jimenez et al., 2024</a>) collects 2,294 real-world development tasks from 12 popular open-source Python projects, sourced directly from GitHub Issues and Pull Requests.</p><p>To ensure reproducibility and environment independence, SWE-bench constructs an <strong>isolated Docker environment</strong> for each task, preventing failures due to inconsistencies in Python versions or dependencies. This design also forces the model to learn to generate compatible patches for different environments.</p><p>For verification, SWE-bench cleverly utilizes the projects&rsquo; built-in unit tests to <strong>automatically evaluate the correctness of the LLM&rsquo;s patch</strong>. It includes two types of tests:</p><ul><li><strong>Fail-to-Pass (F2P)</strong>: Tests that initially fail but should pass after the correct PR is merged, confirming that the LLM has fixed the target issue.</li><li><strong>Pass-to-Pass (P2P)</strong>: Tests that initially pass and must continue to pass after the PR is merged, ensuring the LLM has not broken existing functionality or introduced new bugs.</li></ul><p>This combination of &ldquo;real tasks + isolated environments + automated testing&rdquo; makes SWE-bench a high-fidelity, scalable benchmark that significantly reduces the cost of verifying programming task correctness. However, the original SWE-bench had flaws like unfair test cases, ambiguous problem descriptions, and complex environments, which led to underestimation of model capabilities. Consequently, OpenAI created a high-quality, human-curated subset called <strong>SWE-bench Verified</strong> (<a href=https://openai.com/index/introducing-swe-bench-verified/>OpenAI, 2024</a>) for more accurate model evaluation.</p><h3 id=browsecomp>BrowseComp<a hidden class=anchor aria-hidden=true href=#browsecomp>#</a></h3><p>Unlike software engineering tasks, web browsing tasks aim to find specific information within the vast expanse of the internet. <strong>BrowseComp</strong> (<a href=https://arxiv.org/abs/2504.12516>Wei et al., 2025</a>) is a simple yet challenging benchmark designed for such tasks.</p><ul><li><p><strong>Design Philosophy</strong>: BrowseComp follows the principle of being <strong>hard to solve, easy to verify</strong>. The problems are designed to require persistent and creative browsing of numerous web pages to find the answer, but the answer itself is typically a short, indisputable string that can be easily compared against a reference.</p></li><li><p><strong>Data Construction</strong>: The creators use a <strong>reverse questioning</strong> method. They first find an obscure fact (e.g., a specific conference paper) and then construct a query with multiple complex constraints around it. For example: &ldquo;Find a paper published at EMNLP between 2018-2023 whose first author graduated from Dartmouth College and whose fourth author graduated from the University of Pennsylvania.&rdquo; Verifying this answer is simple, but finding it among thousands of papers is extremely difficult.</p></li></ul><figure class=align-center><a href=BrowseComp_scale.png data-fancybox=gallery><img loading=lazy src=BrowseComp_scale.png#center alt="Fig. 4. BrowseComp performance of an early version of OpenAI Deep Research scales smoothly with test-time compute. (Image source: Wei et al., 2025)" width=60%></a><figcaption><p>Fig. 4. BrowseComp performance of an early version of OpenAI Deep Research scales smoothly with test-time compute. (Image source: <a href=https://arxiv.org/abs/2504.12516>Wei et al., 2025</a>)</p></figcaption></figure><p>BrowseComp measures an agent&rsquo;s core browsing abilities: factual reasoning, persistent navigation, and creative search. As shown in the figure, the performance of a powerful browsing agent (like OpenAI Deep Research) on this benchmark scales smoothly with test-time compute (i.e., browsing effort), indicating that the eval effectively measures an agent&rsquo;s deep search and information integration capabilities.</p><h2 id=data>Data<a hidden class=anchor aria-hidden=true href=#data>#</a></h2><p>High-quality data is the cornerstone of training powerful agents. However, manually annotating the complete decision trajectories of agents in complex tasks is extremely costly and difficult to scale. Therefore, <strong>Synthetic Data</strong> has become the mainstream solution in this field. Various innovative data generation pipelines have been proposed to create a &ldquo;data flywheel&rdquo; that continuously produces high-quality training data.</p><h3 id=agentfounder>AgentFounder<a hidden class=anchor aria-hidden=true href=#agentfounder>#</a></h3><figure class=align-center><a href=tongyi_agentic_training_pipeline.png data-fancybox=gallery><img loading=lazy src=tongyi_agentic_training_pipeline.png#center alt="Fig. 5. The Agentic Training Pipeline proposed by AgentFounder, incorporating an Agentic CPT stage. (Image source: Su et al., 2025)" width=100%></a><figcaption><p>Fig. 5. The Agentic Training Pipeline proposed by AgentFounder, incorporating an Agentic CPT stage. (Image source: <a href=https://arxiv.org/abs/2509.13310>Su et al., 2025</a>)</p></figcaption></figure><p><strong>AgentFounder</strong> (<a href=https://arxiv.org/abs/2509.13310>Su et al., 2025</a>) introduces a new stage between traditional pre-training and post-training called <strong>Agentic Continual Pre-training (Agentic CPT)</strong>. The entire training pipeline consists of three stages:</p><ol><li><strong>General Pre-training</strong>: Consistent with standard procedures, a base model with general knowledge is first trained.</li><li><strong>Agentic Continual Pre-training (Agentic CPT)</strong>: On top of the general base model, large-scale, diverse synthetic agent behavior data is used for continued &ldquo;next-word prediction&rdquo; training. The goal is not to solve specific tasks but to have the model internalize general agent behavior patterns and develop agentic capabilities.</li><li><strong>Post-training/Task Fine-tuning</strong>: On the model that already possesses basic agentic capabilities, SFT or RL is performed to align it with specific tasks. This avoids the optimization conflict that arises when trying to learn capabilities and align with tasks simultaneously during the post-training phase.</li></ol><p>The key to Agentic CPT is how to <strong>synthesize large-scale agent-like data at low cost</strong>. To this end, AgentFounder proposes two efficient data generation methods that do not require external tool calls: <strong>First-order Action Synthesis (FAS)</strong> and <strong>Higher-order Action Synthesis (HAS)</strong>.</p><ol><li>The core idea of FAS is to generate data about <strong>how to think</strong> and <strong>how to plan the first step</strong> at a low cost through deduction.</li></ol><figure class=align-center><a href=tongyi_fas.png data-fancybox=gallery><img loading=lazy src=tongyi_fas.png#center alt="Fig. 6. Illustration of First-order Action Synthesis (FAS). (Image source: Su et al., 2025)" width=100%></a><figcaption><p>Fig. 6. Illustration of First-order Action Synthesis (FAS). (Image source: <a href=https://arxiv.org/abs/2509.13310>Su et al., 2025</a>)</p></figcaption></figure><ul><li><strong>Planning Action</strong>: Given a problem, have the LLM generate multiple possible analyses and initial tool call plans to help the model learn task decomposition and preliminary planning.</li><li><strong>Reasoning Action</strong>: Given a problem and relevant knowledge snippets, have the LLM generate a complete logical reasoning chain to arrive at the final answer, thereby training its logical deduction and information synthesis abilities.</li></ul><p>FAS only generates thought processes and planned actions, without involving actual tool calls or environmental interactions. This makes its generation cost extremely low, making it suitable for <strong>large-scale data synthesis (up to hundreds of millions of examples)</strong>.</p><ol start=2><li>The goal of HAS is to transform existing (even suboptimal) agent trajectories into high-value <strong>decision-making learning data</strong>.</li></ol><figure class=align-center><a href=tongyi_has.png data-fancybox=gallery><img loading=lazy src=tongyi_has.png#center alt="Fig. 7. Illustration of Higher-order Action Synthesis (HAS). (Image source: Su et al., 2025)" width=100%></a><figcaption><p>Fig. 7. Illustration of Higher-order Action Synthesis (HAS). (Image source: <a href=https://arxiv.org/abs/2509.13310>Su et al., 2025</a>)</p></figcaption></figure><ul><li><strong>Step-level Expansion</strong>: For any step in a trajectory, use an LLM to generate multiple alternative actions, creating a local decision space.</li><li><strong>Contrastive Learning</strong>: Reframe the original choice and the expanded candidate actions as a multiple-choice question with feedback, requiring the model to identify the better decision.</li><li><strong>Causal Supervision</strong>: Append the final outcome (success or failure) to the end of the trajectory to help the model learn the <strong>causal link between decisions and outcomes</strong>.</li></ul><p>This method upgrades traditional <strong>Imitation Learning</strong> to <strong>step-level Decision Making</strong>. The model not only &ldquo;walks through a successful path&rdquo; but also understands <strong>how to choose at each critical juncture</strong>, improving the signal-to-noise ratio and utilization efficiency of the data.</p><p>Does Agentic CPT truly alleviate the &ldquo;optimization conflict&rdquo;? The paper provides an answer through training loss curves and performance comparison experiments.</p><figure class=align-center><a href=agent_founder_loss.png data-fancybox=gallery><img loading=lazy src=agent_founder_loss.png#center alt="Fig. 8. Training loss evolution showing superior convergence of AgentFounder models compared to baseline. (Image source: Su et al., 2025)" width=100%></a><figcaption><p>Fig. 8. Training loss evolution showing superior convergence of AgentFounder models compared to baseline. (Image source: <a href=https://arxiv.org/abs/2509.13310>Su et al., 2025</a>)</p></figcaption></figure><p>The experimental results show that models that underwent Agentic CPT converge faster during the SFT phase, with training losses significantly lower than baseline models that did not go through this stage. This indicates that the model has already developed certain agentic capabilities before entering the post-training phase, making it more efficient when learning specific tasks later on.</p><h3 id=webshaper>WebShaper<a hidden class=anchor aria-hidden=true href=#webshaper>#</a></h3><p>Traditional data synthesis methods are typically information-driven: information is first crawled from the web, and then questions are generated based on that information. <strong>WebShaper</strong> (<a href=https://arxiv.org/abs/2507.15061>Tao et al., 2025</a>) points out that this approach can lead to a mismatch between the reasoning structure in the generated questions and the structure of the original information, allowing models to find answers through &ldquo;logical shortcuts&rdquo; rather than genuine multi-step reasoning. To address this, it proposes a <strong>Formalism-Driven</strong> paradigm, primarily using knowledge graphs and set theory to formalize problems.</p><figure class=align-center><a href=WebShaper.png data-fancybox=gallery><img loading=lazy src=WebShaper.png#center alt="Fig. 9. The formalism-driven data synthesis pipeline of WebShaper. (Image source: Tao et al., 2025)" width=100%></a><figcaption><p>Fig. 9. The formalism-driven data synthesis pipeline of WebShaper. (Image source: <a href=https://arxiv.org/abs/2507.15061>Tao et al., 2025</a>)</p></figcaption></figure><ul><li><strong>Knowledge Projections (KP)</strong>: WebShaper first formalizes information-seeking tasks based on set theory. A <strong>Knowledge Projection</strong> \(R(V)\) is defined as the set of all entities that have a relation \(R\) with the entity set \(V\). For example, <code>bornIn({1990s})</code> represents the set of all entities born in the 1990s.</li><li><strong>Task Formalization</strong>: Complex queries can be rigorously represented as <strong>Intersection</strong> and <strong>Union</strong> operations of multiple KPs. For example, the query &ldquo;Find a player who played for an East German team founded in 1966 during the 2004-05 season and was born in the 90s&rdquo; can be formalized as the intersection of multiple KPs.</li><li><strong>Expander Agent</strong>: WebShaper uses an agent called <strong>Expander</strong>, which first generates a formalized query structure (e.g., the intersection of three KPs) and then progressively populates this structure with specific content by calling tools (search, summarization). It uses a &ldquo;hierarchical expansion&rdquo; strategy to gradually increase problem complexity, effectively avoiding logical shortcuts and information redundancy.</li></ul><h2 id=reward-design>Reward Design<a hidden class=anchor aria-hidden=true href=#reward-design>#</a></h2><p>The reward function is the soul of RL, defining the agent&rsquo;s optimization objective.</p><ul><li><p><strong>Verifiable Rewards</strong>: For tasks with clear answers, such as mathematics and coding, this is the most reliable and scalable source of rewards. The reward signal can come directly from <strong>unit test pass rates</strong>, <strong>code compiler feedback</strong>, or the <strong>correctness of the final answer</strong>. This rule-based reward effectively avoids the reward hacking problem that reward models might introduce.</p></li><li><p><strong>Generative Rewards</strong>: For open-ended tasks without a single correct answer (e.g., generating a research report), the <strong>LLM-as-a-Judge</strong> (<a href=https://arxiv.org/abs/2306.05685>Zheng et al., 2023</a>) approach uses a powerful LLM as a judge to evaluate the quality of the generated output and provide a score or natural language feedback as the reward signal.</p></li><li><p><strong>Dense Rewards</strong>: Unlike an <strong>Outcome Reward Model (ORM)</strong>, which provides a one-time reward only at the end of a task, a <strong>Process Reward Model (PRM)</strong> provides feedback for each step or intermediate stage of the agent&rsquo;s process. This helps solve the credit assignment problem in long-horizon tasks but also increases annotation costs and the risk of being exploited by the model.</p></li><li><p><strong>Unsupervised Rewards</strong>: To eliminate reliance on external annotations, researchers have explored methods for extracting reward signals from the model&rsquo;s own behavior, such as constructing rewards based on <strong>output consistency</strong> (whether multiple generations are consistent) or <strong>internal confidence</strong> (e.g., the entropy of generation probabilities).</p></li></ul><h2 id=optimization-algorithms>Optimization Algorithms<a hidden class=anchor aria-hidden=true href=#optimization-algorithms>#</a></h2><p>In recent years, numerous improved algorithms have been developed based on methods like PPO, DPO, and GRPO. Below are three representative examples.</p><h3 id=ppo>PPO<a hidden class=anchor aria-hidden=true href=#ppo>#</a></h3><p><strong>Proximal Policy Optimization (PPO)</strong> (<a href=https://arxiv.org/abs/1707.06347>Schulman et al., 2017</a>) is a classic Actor-Critic algorithm that has become a mainstream method for RL fine-tuning of LLMs due to its successful application in <strong>InstructGPT</strong> (<a href=https://arxiv.org/abs/2203.02155>Ouyang et al., 2022</a>). The core idea of PPO is to limit the magnitude of change between the new and old policies during updates, thereby ensuring training stability. It uses a <strong>token-level importance ratio</strong> and <strong>clipping</strong> to constrain policy shifts and employs a Critic model to estimate the advantage (decomposing sequence-level rewards to the token-level), which improves stability but introduces additional model and computational overhead.</p>$$
\mathcal{J}_{\mathrm{PPO}}(\theta)=\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta_{\text {old }}}(\cdot \mid x)}\left[\frac{1}{|y|} \sum_{t=1}^{|y|} \min \left(w_t(\theta) \widehat{A}_t, \operatorname{clip}\left(w_t(\theta), 1-\varepsilon, 1+\varepsilon\right) \widehat{A}_t\right)\right]
$$<p>where the importance ratio of the token $y_{t}$ is defined as $w_t(\theta)=\frac{\pi_\theta\left(y_t \mid x, y_{&lt;t}\right)}{\pi_{\theta_{\text {old }}}\left(y_t \mid x, y_{&lt;t}\right)}$, the advantage of $y_{t}$, denoted by $\widehat{A}_{t}$, is estimated by another value model, and $\varepsilon$ is the clipping range of importance ratios.</p><h3 id=grpo>GRPO<a hidden class=anchor aria-hidden=true href=#grpo>#</a></h3><p><strong>Group Relative Policy Optimization (GRPO)</strong> (<a href=https://arxiv.org/abs/2402.03300>Shao, et al. 2024</a>) cleverly removes the Critic model. For each problem, it samples a group of $G$ outputs and calculates the <strong>relative advantage</strong> of each output within the group (i.e., the reward value minus the group mean, divided by the standard deviation) to serve as the advantage function. The advantage is at the sequence-level but is still used for token-level updates. This reduces computational costs and improves training stability. The formula below omits the KL divergence penalty term; for the complete version, refer to the my previous post on <a href=https://syhya.github.io/posts/2025-01-27-deepseek-r1/#grpo>GRPO</a>.</p>$$
\mathcal{J}_{\mathrm{GRPO}}(\theta)=\mathbb{E}_{x \sim \mathcal{D},\left\{y_i\right\}_{i=1}^G \sim \pi_{\theta_{\text {old }}}(\cdot \mid x)}\left[\frac{1}{G} \sum_{i=1}^G \frac{1}{\left|y_i\right|} \sum_{t=1}^{\left|y_i\right|} \min \left(w_{i, t}(\theta) \widehat{A}_{i, t}, \operatorname{clip}\left(w_{i, t}(\theta), 1-\varepsilon, 1+\varepsilon\right) \widehat{A}_{i, t}\right)\right]
$$<p>The importance ratio and advantage for token $y_{i, t}$ are:</p>$$
w_{i, t}(\theta)=\frac{\pi_{\theta}\left(y_{i, t} \mid x, y_{i,&lt;t}\right)}{\pi_{\theta_{\text{old}}}\left(y_{i, t} \mid x, y_{i,&lt;t}\right)}, \quad \widehat{A}_{i, t}=\widehat{A}_{i}=\frac{r\left(x, y_{i}\right)-\operatorname{mean}\left(\left\{r\left(x, y_{i}\right)\right\}_{i=1}^{G}\right)}{\operatorname{std}\left(\left\{r\left(x, y_{i}\right)\right\}_{i=1}^{G}\right)},
$$<p>All tokens within sequence $y_{i}$ share the same advantage $\widehat{A}_{i}$, and $G$ is the number of outputs generated for each query $x$ (i.e., the group size).</p><h3 id=gspo>GSPO<a hidden class=anchor aria-hidden=true href=#gspo>#</a></h3><p><strong>Group Sequence Policy Optimization (GSPO)</strong> (<a href=https://arxiv.org/abs/2507.18071>Zheng et al., 2025</a>) elevates the basic unit of optimization from the token level to the sequence level. Unlike GRPO, which uses token-level importance ratios, GSPO introduces a <strong>sequence-level</strong> importance ratio to align with sequence-level rewards. This avoids the noise accumulated from token ratios in long sequences, reducing variance and improving stability. The Qwen team notes that this not only mitigates large probability fluctuations caused by local routing jitter in MoE models but also naturally aligns with the <strong>sequence-level rewards</strong> common in Agent tasks, making it suitable for long-sequence modeling and routing-sensitive scenarios.</p><p>The objective function for GSPO is:</p>$$
\mathcal{J}_{\mathrm{GSPO}}(\theta)=\mathbb{E}_{x \sim \mathcal{D},\left\{y_{i}\right\}_{i=1}^{G} \sim \pi_{\theta_{\text {old }}}(\cdot \mid x)}\left[\frac{1}{G} \sum_{i=1}^{G} \min \left(s_{i}(\theta) \widehat{A}_{i}, \operatorname{clip}\left(s_{i}(\theta), 1-\varepsilon, 1+\varepsilon\right) \widehat{A}_{i}\right)\right],
$$<p>where the advantage function within the group is defined as:</p>$$
\widehat{A}_{i}=\frac{r\left(x, y_{i}\right)-\operatorname{mean}\left(\left\{r\left(x, y_{i}\right)\right\}_{i=1}^{G}\right)}{\operatorname{std}\left(\left\{r\left(x, y_{i}\right)\right\}_{i=1}^{G}\right)}
$$<p>And the sequence-level importance ratio $s_i(\theta)$ is defined as:</p>$$
s_i(\theta)=\left(\frac{\pi_\theta\left(y_i \mid x\right)}{\pi_{\theta_{\text{old}}}\left(y_i \mid x\right)}\right)^{\frac{1}{\left|y_i\right|}}=\exp \left(\frac{1}{\left|y_i\right|} \sum_{t=1}^{\left|y_i\right|} \log \frac{\pi_\theta\left(y_{i, t} \mid x, y_{i,&lt;t}\right)}{\pi_{\theta_{\text{old}}}\left(y_{i, t} \mid x, y_{i,&lt;t}\right)}\right)
$$<p>It applies clipping to the <strong>entire sequence</strong> rather than individual tokens, keeping the optimization consistent with the sequence-level reward. <strong>Length normalization</strong> is used here to reduce variance and control the numerical range of $s_i(\theta)$, as otherwise, probability changes in a few tokens could cause the ratio to fluctuate dramatically, and different response lengths would lead to inconsistent clipping ranges. It&rsquo;s important to note that because the importance ratio is defined differently, the magnitude of the clipping range in GSPO is typically not the same as in GRPO.</p><h3 id=frameworks>Frameworks<a hidden class=anchor aria-hidden=true href=#frameworks>#</a></h3><p>The training pipeline for Agentic RL is complex, involving multiple stages such as inference (Rollout), training (Training), and reward calculation. It typically requires a distributed framework for efficient coordination.</p><figure class=align-center><a href=dataflow_rlhf_verl.png data-fancybox=gallery><img loading=lazy src=dataflow_rlhf_verl.png#center alt="Fig. 10. Dataflow graph of 3 RLHF algorithms including PPO, Safe-RLHF and ReMax. (Image source: Sheng et al., 2024)" width=100%></a><figcaption><p>Fig. 10. Dataflow graph of 3 RLHF algorithms including PPO, Safe-RLHF and ReMax. (Image source: <a href=https://arxiv.org/abs/2409.19256v2>Sheng et al., 2024</a>)</p></figcaption></figure><p>The figure above shows the complex dataflow during the training process of different algorithms. It involves not only multiple models like Actor, Critic, Reference, and Reward but also intertwines different types of computational workloads such as data generation, inference, and training. Taking the basic <strong>PPO algorithm</strong> as an example, the system involves 4 core models: the Actor generates responses based on the input prompt; the Critic evaluates the results; the Reference serves as a baseline for generation quality; and the Reward provides the reward signal. From a computational perspective, the entire process can be divided into 3 stages:</p><ol><li><strong>Generation</strong>: The Actor generates the response token by token. This process is affected by text length and generation method and is the main consumer of inference resources and time.</li><li><strong>Forward (Rollout)</strong>: The generated result, along with the query, is fed into the 4 models for a forward pass, and the data is stored in a <a href=https://docs.ray.io/en/latest/rllib/rllib-replay-buffers.html>Replay Buffer</a>.</li><li><strong>Training</strong>: Data is sampled from the Buffer to update the Actor and Critic.</li></ol><figure class=align-center><a href=disaggregated_colocated_arch.png data-fancybox=gallery><img loading=lazy src=disaggregated_colocated_arch.png#center alt="Fig. 11. Two representative RL framework architectures. (Image source: Zhong et al., 2025)" width=100%></a><figcaption><p>Fig. 11. Two representative RL framework architectures. (Image source: <a href=https://arxiv.org/abs/2504.15930>Zhong et al., 2025</a>)</p></figcaption></figure><p>As shown in the figure, common distributed scheduling strategies fall into two categories:</p><ol><li><strong>Colocated</strong>: Rollout and Training are deployed on the same set of GPUs and executed alternately using time-slicing. This approach is simple to implement with low communication overhead but suffers from poor stability and cannot leverage heterogeneous hardware.</li><li><strong>Disaggregated</strong>: Rollout and Training are deployed on separate, independent GPU clusters. This architecture is more flexible, has higher stability, and allows for hybrid deployment of heterogeneous hardware, but it may introduce pipeline bubbles, affecting throughput.</li></ol><h3 id=verl>verl<a hidden class=anchor aria-hidden=true href=#verl>#</a></h3><p><strong>verl (Volcano Engine Reinforcement Learning)</strong> (<a href=https://arxiv.org/abs/2409.19256v2>Sheng et al., 2024</a>) is an efficient and general-purpose reinforcement learning framework for LLMs, open-sourced by ByteDance.</p><figure class=align-center><a href=verl_async_system_arch.png data-fancybox=gallery><img loading=lazy src=verl_async_system_arch.png#center alt="Fig. 12. The asynchronous system architecture of verl. (Image source: ByteDance Seed, 2025)" width=100%></a><figcaption><p>Fig. 12. The asynchronous system architecture of verl. (Image source: <a href=https://verl.readthedocs.io/en/latest/start/agentic_rl.html>ByteDance Seed, 2025</a>)</p></figcaption></figure><p>The core of verl is its <strong>asynchronous architecture</strong>, which decouples stages like Rollout, reward calculation, and model optimization, processing them in a pipeline to maximize hardware utilization. Its workflow is as follows:</p><ol><li>The <code>PPOTrainer</code> initiates a PPO iteration, first performing <strong>rollout</strong>, then <strong>train</strong>.</li><li>The <code>AgentLoopManager</code> wakes up/synchronizes the weights of the inference and training engines (vLLM/SGLang ⇄ FSDP/Megatron-LM), splits the batch into chunks, and dispatches them to multiple <code>AgentLoopWorker</code>s for <strong>concurrent</strong> execution.</li><li>Each <code>AgentLoopWorker</code> starts a coroutine for each sample. When generation is needed, it routes the request to the inference instance with the <strong>lowest load</strong> via the <code>AsyncLLMServerManager</code>, naturally supporting <strong>multi-turn dialogue and multi-tool calls</strong>.</li><li>verl natively supports <strong>Loss Masking</strong> for external information like tool outputs. This means these tokens are ignored when calculating the loss, ensuring the model is only responsible for the content it generates. This is a key feature for maintaining stability in Tool RL training.</li></ol><figure class=align-center><a href=verl_agent_loop_worker.png data-fancybox=gallery><img loading=lazy src=verl_agent_loop_worker.png#center alt="Fig. 13. The agent loop worker of verl based on React. (Image source: ByteDance Seed, 2025)" width=100%></a><figcaption><p>Fig. 13. The agent loop worker of verl based on React. (Image source: <a href=https://verl.readthedocs.io/en/latest/start/agentic_rl.html>ByteDance Seed, 2025</a>)</p></figcaption></figure><ol start=5><li>After the rollout is complete, instances are uniformly collected/hibernated to free up GPU memory. Pluggable interfaces allow for customizing <strong>reward functions</strong>, integrating new <strong>tools</strong>, or replacing <strong>RL algorithms</strong> (e.g., deriving a custom Agent from <code>ReactAgentLoop</code>). By default, it uses the LangGraph framework to implement a <a href="https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/?h=react">ReAct Agent</a>.</li></ol><h2 id=case-studies>Case Studies<a hidden class=anchor aria-hidden=true href=#case-studies>#</a></h2><h3 id=search-r1>Search-R1<a hidden class=anchor aria-hidden=true href=#search-r1>#</a></h3><p><strong>Search-R1</strong> (<a href=https://arxiv.org/abs/2503.09516>Jin et al., 2025</a>) trains an LLM to autonomously engage in multi-turn interactions with a search engine during its step-by-step reasoning process, thereby learning when and how to leverage external knowledge.</p><figure class=align-center><a href=search_r1_template.png data-fancybox=gallery><img loading=lazy src=search_r1_template.png#center alt="Fig. 14. Template for SEARCH-R1. question will be replaced with the specific question during training and inference. (Image source: Jin et al., 2025)" width=100%></a><figcaption><p>Fig. 14. Template for SEARCH-R1. <code>question</code> will be replaced with the specific question during training and inference. (Image source: <a href=https://arxiv.org/abs/2503.09516>Jin et al., 2025</a>)</p></figcaption></figure><p>In the RL training trajectory, the model&rsquo;s interaction with the environment follows these steps:</p><ol><li>Conducts <strong>autonomous reasoning</strong> within <code>&lt;think>...&lt;/think></code> tags.</li><li>When it recognizes a knowledge gap, it generates a <code>&lt;search>query&lt;/search></code> tag to <strong>call the search engine</strong>.</li><li>After the environment performs the search, it feeds the retrieved information back to the model wrapped in <code>&lt;information>...&lt;/information></code> tags.</li><li>The model continues its reasoning based on the new information and can perform multiple rounds of searches until it finally provides an answer within <code>&lt;answer>...&lt;/answer></code> tags.</li></ol><figure class=align-center><a href=search_r1_rollout.png data-fancybox=gallery><img loading=lazy src=search_r1_rollout.png#center alt="Fig. 15. LLM Response Rollout with Multi-Turn Search Engine Calls. (Image source: Jin et al., 2025)" width=100%></a><figcaption><p>Fig. 15. LLM Response Rollout with Multi-Turn Search Engine Calls. (Image source: <a href=https://arxiv.org/abs/2503.09516>Jin et al., 2025</a>)</p></figcaption></figure><p>This process is implemented through a loop algorithm: the model generates text until it encounters a <code>&lt;search></code> or <code>&lt;answer></code> tag. If a search request is detected, the system pauses generation, executes the search, and injects the results wrapped in <code>&lt;information></code> tags back into the context for the model to continue reasoning in the next step. This loop continues until the model generates a final answer or reaches the maximum number of interactions.</p><p>The paper uses PPO and GRPO algorithms for training. To ensure training stability, Search-R1 introduces a <strong>retrieved token masking</strong> mechanism: when calculating the RL loss (policy gradient and KL divergence), all retrieved tokens returned by the search engine and wrapped in <code>&lt;information></code> tags are <strong>masked</strong>, and their loss does not contribute to the gradient update. This design forces the model to focus on learning <strong>when and how to reason and search</strong>, rather than mechanically imitating external retrieved content, thus effectively preventing training instability. The results below show that the model trained with this strategy (Qwen2.5-7b-base, trained with PPO) consistently outperforms the version trained without masking.</p><figure class=align-center><a href=search_r1_token_mask.png data-fancybox=gallery><img loading=lazy src=search_r1_token_mask.png#center alt="Fig. 16. The performance of SEARCH-R1 with and without retrieved token loss masking. (Image source: Jin et al., 2025)" width=100%></a><figcaption><p>Fig. 16. The performance of SEARCH-R1 with and without retrieved token loss masking. (Image source: <a href=https://arxiv.org/abs/2503.09516>Jin et al., 2025</a>)</p></figcaption></figure><p>Search-R1 employs a simple outcome-based reward function, scoring only based on the correctness of the final answer, using exact match as the criterion.</p>$$
R(y, \hat{y}_{\text{gold}}) = \text{EM}(y_{\text{answer}}, \hat{y}_{\text{gold}}) =
\begin{cases}
1, & \text{if } y_{\text{answer}} = \hat{y}_{\text{gold}} \\
0, & \text{otherwise}
\end{cases}
$$<p>This simple reward design proved effective enough to guide the model to learn complex search and reasoning behaviors.</p><figure class=align-center><a href=search_r1_result.png data-fancybox=gallery><img loading=lazy src=search_r1_result.png#center alt="Fig. 17. The main results comparing SEARCH-R1 with baseline methods across the seven datasets. (Image source: Jin et al., 2025)" width=100%></a><figcaption><p>Fig. 17. The main results comparing SEARCH-R1 with baseline methods across the seven datasets. (Image source: <a href=https://arxiv.org/abs/2503.09516>Jin et al., 2025</a>)</p></figcaption></figure><p>Experiments show that on the Qwen2.5-7B and Qwen2.5-3B models, Search-R1 achieved average relative performance improvements of <strong>24%</strong> and <strong>20%</strong>, respectively, compared to traditional RAG baselines.</p><h3 id=retool>ReTool<a hidden class=anchor aria-hidden=true href=#retool>#</a></h3><p><strong>ReTool</strong> (<a href=https://arxiv.org/abs/2504.11536>Luo et al., 2025</a>) is a case study on training a model to decide when and how to call a <strong>Code Interpreter (CI)</strong> to solve math problems, based on the verl framework.</p><figure class=align-center><a href=ReTool_arch.png data-fancybox=gallery><img loading=lazy src=ReTool_arch.png#center alt="Fig. 18. The architecture of ReTool. (Image source: Luo et al., 2025)" width=100%></a><figcaption><p>Fig. 18. The architecture of ReTool. (Image source: <a href=https://arxiv.org/abs/2504.11536>Luo et al., 2025</a>)</p></figcaption></figure><p>ReTool adopts a two-stage process: &ldquo;<strong>Cold-start SFT → Tool-augmented RL</strong>&rdquo;:</p><ol><li><strong>Cold-start SFT</strong>: First, a dataset containing code-augmented reasoning trajectories is constructed. SFT is used to equip the model with basic tool-calling capabilities.</li><li><strong>Tool-augmented RL</strong>: During the RL phase, the model can generate <code>&lt;code>...&lt;/code></code> snippets while solving a problem. This code is executed in a sandboxed environment (like <a href=https://github.com/bytedance/SandboxFusion>SandboxFusion</a>), and the execution result (including output or error stack) is fed back to the model wrapped in <code>&lt;interpreter>...&lt;/interpreter></code> tags. The model can then continue reasoning or perform <strong>self-correction</strong> based on the feedback.</li></ol><figure class=align-center><a href=ReTool_self_correction.png data-fancybox=gallery><img loading=lazy src=ReTool_self_correction.png#center alt="Fig. 19. The case of an “aha moment” about code self-correction. (Image source: Luo et al., 2025)" width=100%></a><figcaption><p>Fig. 19. The case of an “aha moment” about code self-correction. (Image source: <a href=https://arxiv.org/abs/2504.11536>Luo et al., 2025</a>)</p></figcaption></figure><p>Its reward function scores only based on the correctness of the final answer, encouraging the model to autonomously explore robust reasoning-execution strategies.</p>$$
R(a,\hat a)=
\begin{cases}
1, & \text{is_equivalent}(a,\hat a) \\
-1, & \text{otherwise}
\end{cases}
$$<p>Training is based on the PPO algorithm. Similar to Search-R1, it applies <strong>full loss masking</strong> to the interpreter feedback <code>&lt;interpreter>...&lt;/interpreter></code>, updating only the model&rsquo;s thoughts and code to prevent gradients from being contaminated by external environmental noise. The results below show that in the AIME 2024/2025 evaluation, ReTool enabled the Qwen2.5-32B-Instruct model to achieve an accuracy of 67.0% / 49.3% with just 400 RL steps, surpassing the text-only RL baseline while reducing the average response length by about 40%.</p><figure class=align-center><a href=ReTool_aime.png data-fancybox=gallery><img loading=lazy src=ReTool_aime.png#center alt="Fig. 20. AIME 2024 & 2025 scores of ReTool and text-based RL baseline on the Qwen2.5-32B-Instruct model. (Image source: Luo et al., 2025)" width=100%></a><figcaption><p>Fig. 20. AIME 2024 & 2025 scores of ReTool and text-based RL baseline on the Qwen2.5-32B-Instruct model. (Image source: <a href=https://arxiv.org/abs/2504.11536>Luo et al., 2025</a>)</p></figcaption></figure><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Zhang, Guibin, et al. <a href=https://arxiv.org/abs/2509.02547>&ldquo;The landscape of agentic reinforcement learning for llms: A survey.&rdquo;</a> arXiv preprint arXiv:2509.02547 (2025).</p><p>[2] Wei, Jason. <a href=https://www.jasonwei.net/blog/evals>&ldquo;Successful language model evals.&rdquo;</a> Blog post, 2024.</p><p>[3] Liang, Percy, et al. <a href=https://arxiv.org/abs/2211.09110>&ldquo;Holistic evaluation of language models.&rdquo;</a> arXiv preprint arXiv:2211.09110 (2022).</p><p>[4] Srivastava, Aarohi, et al. <a href=https://arxiv.org/abs/2206.04615>&ldquo;Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.&rdquo;</a> Transactions on machine learning research (2023).</p><p>[5] Wang, Alex, et al. <a href=https://arxiv.org/abs/1804.07461>&ldquo;GLUE: A multi-task benchmark and analysis platform for natural language understanding.&rdquo;</a> arXiv preprint arXiv:1804.07461 (2018).</p><p>[6] Wang, Alex, et al. <a href=https://arxiv.org/abs/1905.00537>&ldquo;SuperGLUE: A stickier benchmark for general-purpose language understanding systems.&rdquo;</a> Advances in neural information processing systems 32 (2019).</p><p>[7] Wei, Jason. <a href=https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law>&ldquo;Asymmetry of verification and verifier’s rule.&rdquo;</a> Blog post, 2025.</p><p>[8] Jimenez, Carlos E., et al. <a href=https://arxiv.org/abs/2310.06770>&ldquo;SWE-bench: Can language models resolve real-world github issues?.&rdquo;</a> arXiv preprint arXiv:2310.06770 (2023).</p><p>[9] OpenAI. <a href=https://openai.com/index/introducing-swe-bench-verified/>&ldquo;Introducing SWE-bench Verified.&rdquo;</a> OpenAI, 2024 (updated 2025).</p><p>[10] Wei, Jason, et al. <a href=https://arxiv.org/abs/2504.12516>&ldquo;Browsecomp: A simple yet challenging benchmark for browsing agents.&rdquo;</a> arXiv preprint arXiv:2504.12516 (2025).</p><p>[11] Su, Liangcai, et al. <a href=https://arxiv.org/abs/2509.13310>&ldquo;Scaling Agents via Continual Pre-training.&rdquo;</a> arXiv preprint arXiv:2509.13310 (2025).</p><p>[12] Tao, Zhengwei, et al. <a href=https://arxiv.org/abs/2507.15061>&ldquo;Webshaper: Agentically data synthesizing via information-seeking formalization.&rdquo;</a> arXiv preprint arXiv:2507.15061 (2025).</p><p>[13] Zheng, Lianmin, et al. <a href=https://arxiv.org/abs/2306.05685>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena."</a> Advances in neural information processing systems 36 (2023): 46595-46623.</p><p>[14] Schulman, John, et al. <a href=https://arxiv.org/abs/1707.06347>&ldquo;Proximal policy optimization algorithms.&rdquo;</a> arXiv preprint arXiv:1707.06347 (2017).</p><p>[15] Shao, Zhihong, et al. <a href=https://arxiv.org/abs/2402.03300>&ldquo;Deepseekmath: Pushing the limits of mathematical reasoning in open language models.&rdquo;</a> arXiv preprint arXiv:2402.03300 (2024).</p><p>[16] Zheng, Chujie, et al. <a href=https://arxiv.org/abs/2507.18071>&ldquo;Group sequence policy optimization.&rdquo;</a> arXiv preprint arXiv:2507.18071 (2025).</p><p>[17] Sheng, Guangming, et al. <a href=https://arxiv.org/abs/2409.19256v2>&ldquo;Hybridflow: A flexible and efficient rlhf framework.&rdquo;</a> Proceedings of the Twentieth European Conference on Computer Systems. 2025.</p><p>[18] Zhong, Yinmin, et al. <a href=https://arxiv.org/abs/2504.15930>&ldquo;StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation.&rdquo;</a> arXiv preprint arXiv:2504.15930 (2025).</p><p>[19] Jin, Bowen, et al. <a href=https://arxiv.org/abs/2503.09516>&ldquo;Search-r1: Training llms to reason and leverage search engines with reinforcement learning.&rdquo;</a> arXiv preprint arXiv:2503.09516 (2025).</p><p>[20] Feng, Jiazhan, et al. <a href=https://arxiv.org/abs/2504.11536>&ldquo;Retool: Reinforcement learning for strategic tool use in llms.&rdquo;</a> arXiv preprint arXiv:2504.11536 (2025).</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><blockquote><p><strong>Citation</strong>: When reproducing or citing the content of this article, please credit the original author and source.</p></blockquote><p><strong>Cited as:</strong></p><blockquote><p>Yue Shui. (September 2025). Agentic RL
<a href=https://syhya.github.io/en/posts/2025-09-30-agentic-rl/>https://syhya.github.io/en/posts/2025-09-30-agentic-rl/</a></p></blockquote><p>Or</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>yue_shui_agentic_rl_2025</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>   <span class=p>=</span> <span class=s>&#34;Agentic RL&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>  <span class=p>=</span> <span class=s>&#34;Yue Shui&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span> <span class=p>=</span> <span class=s>&#34;syhya.github.io&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>    <span class=p>=</span> <span class=s>&#34;2025&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>month</span>   <span class=p>=</span> <span class=s>&#34;September&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>     <span class=p>=</span> <span class=s>&#34;https://syhya.github.io/en/posts/2025-09-30-agentic-rl/&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://syhya.github.io/tags/agentic-rl/>Agentic RL</a></li><li><a href=https://syhya.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://syhya.github.io/tags/llm/>LLM</a></li><li><a href=https://syhya.github.io/tags/agent/>Agent</a></li><li><a href=https://syhya.github.io/tags/swe-bench/>SWE-Bench</a></li><li><a href=https://syhya.github.io/tags/verl/>Verl</a></li><li><a href=https://syhya.github.io/tags/retool/>ReTool</a></li></ul><nav class=paginav><a class=next href=https://syhya.github.io/posts/2025-08-24-gpt5/><span class=title>Next »</span><br><span>gpt-oss & GPT-5</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Agentic RL on x" href="https://x.com/intent/tweet/?text=Agentic%20RL&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-09-30-agentic-rl%2f&amp;hashtags=AgenticRL%2cReinforcementLearning%2cLLM%2cAgent%2cSWE-bench%2cverl%2cReTool"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Agentic RL on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-09-30-agentic-rl%2f&amp;title=Agentic%20RL&amp;summary=Agentic%20RL&amp;source=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-09-30-agentic-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Agentic RL on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-09-30-agentic-rl%2f&title=Agentic%20RL"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Agentic RL on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-09-30-agentic-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Agentic RL on whatsapp" href="https://api.whatsapp.com/send?text=Agentic%20RL%20-%20https%3a%2f%2fsyhya.github.io%2fposts%2f2025-09-30-agentic-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Agentic RL on telegram" href="https://telegram.me/share/url?text=Agentic%20RL&amp;url=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-09-30-agentic-rl%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Agentic RL on ycombinator" href="https://news.ycombinator.com/submitlink?t=Agentic%20RL&u=https%3a%2f%2fsyhya.github.io%2fposts%2f2025-09-30-agentic-rl%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://syhya.github.io/>Yue Shui Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>